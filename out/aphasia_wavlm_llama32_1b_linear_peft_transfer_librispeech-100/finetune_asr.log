[2025-02-13 19:18:05,066][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-13 19:18:05,067][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-13 19:18:05,067][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100'}
[2025-02-13 19:18:05,067][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-13_19-18-04.txt', 'log_interval': 5}
[2025-02-13 19:18:33,044][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-13 19:18:38,257][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 19:18:38,263][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-13 19:18:38,266][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 19:18:38,267][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-13 19:18:47,421][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 19:18:47,425][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-13 19:18:47,425][slam_llm.models.slam_model][INFO] - setup peft...
[2025-02-13 19:18:47,549][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 19:18:47,551][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-13 19:18:47,641][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-13 19:18:47,642][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-13 19:18:47,642][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_23834_loss_1.1383870840072632/model.pt
[2025-02-13 19:18:47,896][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-13 19:18:47,902][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-13 19:18:49,612][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-13 19:18:54,044][root][INFO] - --> Training Set Length = 28539
[2025-02-13 19:18:54,068][root][INFO] - --> Validation Set Length = 2703
[2025-02-13 19:18:54,069][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 19:18:54,069][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 19:18:56,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:57,749][root][INFO] - Training Epoch: 1/2, step 0/7134 completed (loss: 1.1579816341400146, acc: 0.75)
[2025-02-13 19:18:57,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:58,156][root][INFO] - Training Epoch: 1/2, step 1/7134 completed (loss: 0.9910687208175659, acc: 0.7770700454711914)
[2025-02-13 19:18:58,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:58,744][root][INFO] - Training Epoch: 1/2, step 2/7134 completed (loss: 0.869597852230072, acc: 0.8011363744735718)
[2025-02-13 19:18:58,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:59,214][root][INFO] - Training Epoch: 1/2, step 3/7134 completed (loss: 1.0732008218765259, acc: 0.7790697813034058)
[2025-02-13 19:18:59,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:59,660][root][INFO] - Training Epoch: 1/2, step 4/7134 completed (loss: 1.2136540412902832, acc: 0.7358490824699402)
[2025-02-13 19:18:59,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:00,091][root][INFO] - Training Epoch: 1/2, step 5/7134 completed (loss: 0.9124829769134521, acc: 0.7932960987091064)
[2025-02-13 19:19:00,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:00,519][root][INFO] - Training Epoch: 1/2, step 6/7134 completed (loss: 0.7168205976486206, acc: 0.8450704216957092)
[2025-02-13 19:19:00,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:00,934][root][INFO] - Training Epoch: 1/2, step 7/7134 completed (loss: 1.1315159797668457, acc: 0.7795698642730713)
[2025-02-13 19:19:01,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:01,349][root][INFO] - Training Epoch: 1/2, step 8/7134 completed (loss: 1.100317358970642, acc: 0.75)
[2025-02-13 19:19:01,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:01,742][root][INFO] - Training Epoch: 1/2, step 9/7134 completed (loss: 0.6515158414840698, acc: 0.8675496578216553)
[2025-02-13 19:19:01,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:02,152][root][INFO] - Training Epoch: 1/2, step 10/7134 completed (loss: 1.399608850479126, acc: 0.715976357460022)
[2025-02-13 19:19:02,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:02,587][root][INFO] - Training Epoch: 1/2, step 11/7134 completed (loss: 1.3747658729553223, acc: 0.6916666626930237)
[2025-02-13 19:19:02,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:02,996][root][INFO] - Training Epoch: 1/2, step 12/7134 completed (loss: 0.9549150466918945, acc: 0.7976878881454468)
[2025-02-13 19:19:03,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:03,382][root][INFO] - Training Epoch: 1/2, step 13/7134 completed (loss: 1.0423942804336548, acc: 0.7640449404716492)
[2025-02-13 19:19:03,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:03,794][root][INFO] - Training Epoch: 1/2, step 14/7134 completed (loss: 0.8915500044822693, acc: 0.8040540814399719)
[2025-02-13 19:19:03,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:04,192][root][INFO] - Training Epoch: 1/2, step 15/7134 completed (loss: 0.8156634569168091, acc: 0.8333333134651184)
[2025-02-13 19:19:04,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:04,584][root][INFO] - Training Epoch: 1/2, step 16/7134 completed (loss: 0.501194179058075, acc: 0.8859649300575256)
[2025-02-13 19:19:04,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:04,991][root][INFO] - Training Epoch: 1/2, step 17/7134 completed (loss: 1.0562118291854858, acc: 0.8085106611251831)
[2025-02-13 19:19:05,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:05,404][root][INFO] - Training Epoch: 1/2, step 18/7134 completed (loss: 0.7087686061859131, acc: 0.849397599697113)
[2025-02-13 19:19:05,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:05,825][root][INFO] - Training Epoch: 1/2, step 19/7134 completed (loss: 1.1565136909484863, acc: 0.7514451146125793)
[2025-02-13 19:19:05,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:06,273][root][INFO] - Training Epoch: 1/2, step 20/7134 completed (loss: 0.8011123538017273, acc: 0.8068181872367859)
[2025-02-13 19:19:06,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:06,708][root][INFO] - Training Epoch: 1/2, step 21/7134 completed (loss: 1.0967758893966675, acc: 0.7727272510528564)
[2025-02-13 19:19:06,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:07,087][root][INFO] - Training Epoch: 1/2, step 22/7134 completed (loss: 0.6382497549057007, acc: 0.8650306463241577)
[2025-02-13 19:19:07,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:07,482][root][INFO] - Training Epoch: 1/2, step 23/7134 completed (loss: 0.7131471037864685, acc: 0.8531073331832886)
[2025-02-13 19:19:07,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:07,880][root][INFO] - Training Epoch: 1/2, step 24/7134 completed (loss: 1.0184439420700073, acc: 0.8055555820465088)
[2025-02-13 19:19:08,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:08,296][root][INFO] - Training Epoch: 1/2, step 25/7134 completed (loss: 0.7238438129425049, acc: 0.8520709872245789)
[2025-02-13 19:19:08,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:08,771][root][INFO] - Training Epoch: 1/2, step 26/7134 completed (loss: 0.7843343019485474, acc: 0.8220859169960022)
[2025-02-13 19:19:08,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:09,231][root][INFO] - Training Epoch: 1/2, step 27/7134 completed (loss: 0.6212082505226135, acc: 0.8461538553237915)
[2025-02-13 19:19:09,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:09,686][root][INFO] - Training Epoch: 1/2, step 28/7134 completed (loss: 1.154974341392517, acc: 0.7614213228225708)
[2025-02-13 19:19:09,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:10,114][root][INFO] - Training Epoch: 1/2, step 29/7134 completed (loss: 0.9886778593063354, acc: 0.7692307829856873)
[2025-02-13 19:19:10,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:10,557][root][INFO] - Training Epoch: 1/2, step 30/7134 completed (loss: 1.330649733543396, acc: 0.7101449370384216)
[2025-02-13 19:19:10,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:10,966][root][INFO] - Training Epoch: 1/2, step 31/7134 completed (loss: 1.0220288038253784, acc: 0.7860465049743652)
[2025-02-13 19:19:11,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:11,385][root][INFO] - Training Epoch: 1/2, step 32/7134 completed (loss: 1.480767846107483, acc: 0.739130437374115)
[2025-02-13 19:19:11,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:11,797][root][INFO] - Training Epoch: 1/2, step 33/7134 completed (loss: 0.9624863862991333, acc: 0.800000011920929)
[2025-02-13 19:19:11,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:12,234][root][INFO] - Training Epoch: 1/2, step 34/7134 completed (loss: 0.8747957944869995, acc: 0.8142856955528259)
[2025-02-13 19:19:12,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:12,706][root][INFO] - Training Epoch: 1/2, step 35/7134 completed (loss: 1.006600260734558, acc: 0.8117647171020508)
[2025-02-13 19:19:12,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:13,202][root][INFO] - Training Epoch: 1/2, step 36/7134 completed (loss: 0.9815825819969177, acc: 0.8110598921775818)
[2025-02-13 19:19:13,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:13,617][root][INFO] - Training Epoch: 1/2, step 37/7134 completed (loss: 1.112215518951416, acc: 0.7671957612037659)
[2025-02-13 19:19:13,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:14,062][root][INFO] - Training Epoch: 1/2, step 38/7134 completed (loss: 0.7136710286140442, acc: 0.8301886916160583)
[2025-02-13 19:19:14,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:14,518][root][INFO] - Training Epoch: 1/2, step 39/7134 completed (loss: 0.5405465364456177, acc: 0.8654970526695251)
[2025-02-13 19:19:14,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:14,975][root][INFO] - Training Epoch: 1/2, step 40/7134 completed (loss: 0.8778253197669983, acc: 0.8303571343421936)
[2025-02-13 19:19:15,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:15,440][root][INFO] - Training Epoch: 1/2, step 41/7134 completed (loss: 0.7530835866928101, acc: 0.8723404407501221)
[2025-02-13 19:19:15,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:15,871][root][INFO] - Training Epoch: 1/2, step 42/7134 completed (loss: 0.5869245529174805, acc: 0.8765432238578796)
[2025-02-13 19:19:16,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:16,280][root][INFO] - Training Epoch: 1/2, step 43/7134 completed (loss: 0.5194288492202759, acc: 0.8882681727409363)
[2025-02-13 19:19:16,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:16,676][root][INFO] - Training Epoch: 1/2, step 44/7134 completed (loss: 0.7175498008728027, acc: 0.8395721912384033)
[2025-02-13 19:19:16,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:17,097][root][INFO] - Training Epoch: 1/2, step 45/7134 completed (loss: 0.7169306874275208, acc: 0.8537735939025879)
[2025-02-13 19:19:17,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:17,502][root][INFO] - Training Epoch: 1/2, step 46/7134 completed (loss: 1.0077418088912964, acc: 0.7848837375640869)
[2025-02-13 19:19:17,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:17,910][root][INFO] - Training Epoch: 1/2, step 47/7134 completed (loss: 0.80377197265625, acc: 0.8379888534545898)
[2025-02-13 19:19:18,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:18,331][root][INFO] - Training Epoch: 1/2, step 48/7134 completed (loss: 0.621318519115448, acc: 0.8309178948402405)
[2025-02-13 19:19:18,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:18,767][root][INFO] - Training Epoch: 1/2, step 49/7134 completed (loss: 0.8958303928375244, acc: 0.8294117450714111)
[2025-02-13 19:19:18,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:19,206][root][INFO] - Training Epoch: 1/2, step 50/7134 completed (loss: 0.7388230562210083, acc: 0.8106796145439148)
[2025-02-13 19:19:19,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:19,583][root][INFO] - Training Epoch: 1/2, step 51/7134 completed (loss: 0.566402018070221, acc: 0.8571428656578064)
[2025-02-13 19:19:19,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:19,983][root][INFO] - Training Epoch: 1/2, step 52/7134 completed (loss: 0.4844636619091034, acc: 0.887417197227478)
[2025-02-13 19:19:20,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:20,394][root][INFO] - Training Epoch: 1/2, step 53/7134 completed (loss: 0.5961623191833496, acc: 0.8540540337562561)
[2025-02-13 19:19:20,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:20,815][root][INFO] - Training Epoch: 1/2, step 54/7134 completed (loss: 0.7771532535552979, acc: 0.8428571224212646)
[2025-02-13 19:19:20,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:21,238][root][INFO] - Training Epoch: 1/2, step 55/7134 completed (loss: 0.9448931217193604, acc: 0.7880434989929199)
[2025-02-13 19:19:21,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:21,662][root][INFO] - Training Epoch: 1/2, step 56/7134 completed (loss: 0.5293661952018738, acc: 0.8769230842590332)
[2025-02-13 19:19:21,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22,070][root][INFO] - Training Epoch: 1/2, step 57/7134 completed (loss: 1.2808226346969604, acc: 0.7692307829856873)
[2025-02-13 19:19:22,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22,487][root][INFO] - Training Epoch: 1/2, step 58/7134 completed (loss: 0.9779337644577026, acc: 0.7821229100227356)
[2025-02-13 19:19:22,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22,903][root][INFO] - Training Epoch: 1/2, step 59/7134 completed (loss: 1.1221625804901123, acc: 0.7914438247680664)
[2025-02-13 19:19:23,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:23,329][root][INFO] - Training Epoch: 1/2, step 60/7134 completed (loss: 0.9963467717170715, acc: 0.7721518874168396)
[2025-02-13 19:19:23,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:23,787][root][INFO] - Training Epoch: 1/2, step 61/7134 completed (loss: 0.8901520371437073, acc: 0.832402229309082)
[2025-02-13 19:19:23,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:24,212][root][INFO] - Training Epoch: 1/2, step 62/7134 completed (loss: 1.282824158668518, acc: 0.7303370833396912)
[2025-02-13 19:19:24,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:24,621][root][INFO] - Training Epoch: 1/2, step 63/7134 completed (loss: 0.9094303846359253, acc: 0.8131868243217468)
[2025-02-13 19:19:24,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:25,048][root][INFO] - Training Epoch: 1/2, step 64/7134 completed (loss: 0.8790978193283081, acc: 0.7941176295280457)
[2025-02-13 19:19:25,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:25,488][root][INFO] - Training Epoch: 1/2, step 65/7134 completed (loss: 1.5793614387512207, acc: 0.7234042286872864)
[2025-02-13 19:19:25,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:25,878][root][INFO] - Training Epoch: 1/2, step 66/7134 completed (loss: 1.321089506149292, acc: 0.7651515007019043)
[2025-02-13 19:19:26,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:26,284][root][INFO] - Training Epoch: 1/2, step 67/7134 completed (loss: 1.1522371768951416, acc: 0.7471264600753784)
[2025-02-13 19:19:26,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:26,683][root][INFO] - Training Epoch: 1/2, step 68/7134 completed (loss: 1.342912197113037, acc: 0.7303370833396912)
[2025-02-13 19:19:26,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:27,056][root][INFO] - Training Epoch: 1/2, step 69/7134 completed (loss: 1.2383785247802734, acc: 0.7830687761306763)
[2025-02-13 19:19:27,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:27,463][root][INFO] - Training Epoch: 1/2, step 70/7134 completed (loss: 1.010359525680542, acc: 0.8046875)
[2025-02-13 19:19:27,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:27,863][root][INFO] - Training Epoch: 1/2, step 71/7134 completed (loss: 0.7492308616638184, acc: 0.8363636136054993)
[2025-02-13 19:19:28,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:28,266][root][INFO] - Training Epoch: 1/2, step 72/7134 completed (loss: 1.0655746459960938, acc: 0.7869822382926941)
[2025-02-13 19:19:28,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:28,651][root][INFO] - Training Epoch: 1/2, step 73/7134 completed (loss: 1.4042980670928955, acc: 0.7514451146125793)
[2025-02-13 19:19:28,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:29,071][root][INFO] - Training Epoch: 1/2, step 74/7134 completed (loss: 0.9244768023490906, acc: 0.8720930218696594)
[2025-02-13 19:19:29,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:29,518][root][INFO] - Training Epoch: 1/2, step 75/7134 completed (loss: 1.4599946737289429, acc: 0.7083333134651184)
[2025-02-13 19:19:29,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:29,940][root][INFO] - Training Epoch: 1/2, step 76/7134 completed (loss: 1.1663358211517334, acc: 0.760869562625885)
[2025-02-13 19:19:30,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:30,363][root][INFO] - Training Epoch: 1/2, step 77/7134 completed (loss: 0.8865079879760742, acc: 0.8342541456222534)
[2025-02-13 19:19:30,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:30,802][root][INFO] - Training Epoch: 1/2, step 78/7134 completed (loss: 1.0785491466522217, acc: 0.7748344540596008)
[2025-02-13 19:19:30,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:31,226][root][INFO] - Training Epoch: 1/2, step 79/7134 completed (loss: 1.1482975482940674, acc: 0.7733333110809326)
[2025-02-13 19:19:31,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:31,592][root][INFO] - Training Epoch: 1/2, step 80/7134 completed (loss: 0.5009597539901733, acc: 0.9044944047927856)
[2025-02-13 19:19:31,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:32,014][root][INFO] - Training Epoch: 1/2, step 81/7134 completed (loss: 1.1123602390289307, acc: 0.7513227462768555)
[2025-02-13 19:19:32,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:32,462][root][INFO] - Training Epoch: 1/2, step 82/7134 completed (loss: 0.8429518342018127, acc: 0.7891566157341003)
[2025-02-13 19:19:32,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:32,874][root][INFO] - Training Epoch: 1/2, step 83/7134 completed (loss: 1.2330173254013062, acc: 0.75)
[2025-02-13 19:19:33,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:33,272][root][INFO] - Training Epoch: 1/2, step 84/7134 completed (loss: 0.803117036819458, acc: 0.8057553768157959)
[2025-02-13 19:19:33,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:33,694][root][INFO] - Training Epoch: 1/2, step 85/7134 completed (loss: 1.0804805755615234, acc: 0.8100000023841858)
[2025-02-13 19:19:33,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:34,109][root][INFO] - Training Epoch: 1/2, step 86/7134 completed (loss: 0.9000405073165894, acc: 0.8125)
[2025-02-13 19:19:34,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:34,522][root][INFO] - Training Epoch: 1/2, step 87/7134 completed (loss: 1.1110221147537231, acc: 0.7077922224998474)
[2025-02-13 19:19:34,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:34,938][root][INFO] - Training Epoch: 1/2, step 88/7134 completed (loss: 0.9409828782081604, acc: 0.7710843086242676)
[2025-02-13 19:19:35,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:35,360][root][INFO] - Training Epoch: 1/2, step 89/7134 completed (loss: 0.696330189704895, acc: 0.8571428656578064)
[2025-02-13 19:19:35,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:35,773][root][INFO] - Training Epoch: 1/2, step 90/7134 completed (loss: 0.8703500628471375, acc: 0.8067227005958557)
[2025-02-13 19:19:35,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:36,172][root][INFO] - Training Epoch: 1/2, step 91/7134 completed (loss: 0.9515179991722107, acc: 0.7685950398445129)
[2025-02-13 19:19:36,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:36,593][root][INFO] - Training Epoch: 1/2, step 92/7134 completed (loss: 0.936954140663147, acc: 0.798701286315918)
[2025-02-13 19:19:36,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:37,015][root][INFO] - Training Epoch: 1/2, step 93/7134 completed (loss: 0.8667973875999451, acc: 0.7771428823471069)
[2025-02-13 19:19:37,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:37,409][root][INFO] - Training Epoch: 1/2, step 94/7134 completed (loss: 0.9593728184700012, acc: 0.7924528121948242)
[2025-02-13 19:19:37,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:37,838][root][INFO] - Training Epoch: 1/2, step 95/7134 completed (loss: 0.9003345370292664, acc: 0.8048780560493469)
[2025-02-13 19:19:38,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:38,247][root][INFO] - Training Epoch: 1/2, step 96/7134 completed (loss: 0.5697740316390991, acc: 0.8691588640213013)
[2025-02-13 19:19:38,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:38,717][root][INFO] - Training Epoch: 1/2, step 97/7134 completed (loss: 1.7939740419387817, acc: 0.6666666865348816)
[2025-02-13 19:19:38,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:39,133][root][INFO] - Training Epoch: 1/2, step 98/7134 completed (loss: 1.1117596626281738, acc: 0.7253521084785461)
[2025-02-13 19:19:39,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:39,534][root][INFO] - Training Epoch: 1/2, step 99/7134 completed (loss: 0.6780874133110046, acc: 0.8384615182876587)
[2025-02-13 19:19:39,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:39,970][root][INFO] - Training Epoch: 1/2, step 100/7134 completed (loss: 0.84760981798172, acc: 0.8121547102928162)
[2025-02-13 19:19:40,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:40,384][root][INFO] - Training Epoch: 1/2, step 101/7134 completed (loss: 0.9260473251342773, acc: 0.7890625)
[2025-02-13 19:19:40,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:40,770][root][INFO] - Training Epoch: 1/2, step 102/7134 completed (loss: 0.6025024056434631, acc: 0.868852436542511)
[2025-02-13 19:19:40,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:41,190][root][INFO] - Training Epoch: 1/2, step 103/7134 completed (loss: 0.8670310378074646, acc: 0.7876712083816528)
[2025-02-13 19:19:41,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:41,618][root][INFO] - Training Epoch: 1/2, step 104/7134 completed (loss: 0.5588876605033875, acc: 0.8666666746139526)
[2025-02-13 19:19:41,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:42,068][root][INFO] - Training Epoch: 1/2, step 105/7134 completed (loss: 0.49645182490348816, acc: 0.858208954334259)
[2025-02-13 19:19:42,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:42,479][root][INFO] - Training Epoch: 1/2, step 106/7134 completed (loss: 0.5822944641113281, acc: 0.8633540272712708)
[2025-02-13 19:19:42,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:42,930][root][INFO] - Training Epoch: 1/2, step 107/7134 completed (loss: 1.2932029962539673, acc: 0.7441860437393188)
[2025-02-13 19:19:43,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:43,425][root][INFO] - Training Epoch: 1/2, step 108/7134 completed (loss: 0.9547482132911682, acc: 0.7621621489524841)
[2025-02-13 19:19:43,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:43,826][root][INFO] - Training Epoch: 1/2, step 109/7134 completed (loss: 0.775134265422821, acc: 0.8492063283920288)
[2025-02-13 19:19:43,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:44,217][root][INFO] - Training Epoch: 1/2, step 110/7134 completed (loss: 0.8062340021133423, acc: 0.8156424760818481)
[2025-02-13 19:19:44,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:44,661][root][INFO] - Training Epoch: 1/2, step 111/7134 completed (loss: 0.6842694282531738, acc: 0.8409090638160706)
[2025-02-13 19:19:44,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:45,097][root][INFO] - Training Epoch: 1/2, step 112/7134 completed (loss: 0.601958155632019, acc: 0.8472222089767456)
[2025-02-13 19:19:45,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:45,493][root][INFO] - Training Epoch: 1/2, step 113/7134 completed (loss: 0.4423055946826935, acc: 0.8715083599090576)
[2025-02-13 19:19:45,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:45,965][root][INFO] - Training Epoch: 1/2, step 114/7134 completed (loss: 0.789219081401825, acc: 0.8285714387893677)
[2025-02-13 19:19:46,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:46,388][root][INFO] - Training Epoch: 1/2, step 115/7134 completed (loss: 1.1812604665756226, acc: 0.7227723002433777)
[2025-02-13 19:19:46,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:46,829][root][INFO] - Training Epoch: 1/2, step 116/7134 completed (loss: 1.1696827411651611, acc: 0.7241379022598267)
[2025-02-13 19:19:47,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:47,250][root][INFO] - Training Epoch: 1/2, step 117/7134 completed (loss: 0.7612439393997192, acc: 0.8121212124824524)
[2025-02-13 19:19:47,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:47,728][root][INFO] - Training Epoch: 1/2, step 118/7134 completed (loss: 1.0100853443145752, acc: 0.7611111402511597)
[2025-02-13 19:19:47,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:48,188][root][INFO] - Training Epoch: 1/2, step 119/7134 completed (loss: 0.860557496547699, acc: 0.8085106611251831)
[2025-02-13 19:19:48,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:48,614][root][INFO] - Training Epoch: 1/2, step 120/7134 completed (loss: 0.5947567224502563, acc: 0.8333333134651184)
[2025-02-13 19:19:48,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:49,028][root][INFO] - Training Epoch: 1/2, step 121/7134 completed (loss: 0.6744710206985474, acc: 0.8491619825363159)
[2025-02-13 19:19:49,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:49,469][root][INFO] - Training Epoch: 1/2, step 122/7134 completed (loss: 0.6922733783721924, acc: 0.8791208863258362)
[2025-02-13 19:19:49,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:49,878][root][INFO] - Training Epoch: 1/2, step 123/7134 completed (loss: 0.7027183175086975, acc: 0.8553459048271179)
[2025-02-13 19:19:50,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:50,287][root][INFO] - Training Epoch: 1/2, step 124/7134 completed (loss: 0.7347831130027771, acc: 0.8402777910232544)
[2025-02-13 19:19:50,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:50,755][root][INFO] - Training Epoch: 1/2, step 125/7134 completed (loss: 0.8031057715415955, acc: 0.8186274766921997)
[2025-02-13 19:19:50,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:51,239][root][INFO] - Training Epoch: 1/2, step 126/7134 completed (loss: 0.8545796275138855, acc: 0.835106372833252)
[2025-02-13 19:19:51,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:51,644][root][INFO] - Training Epoch: 1/2, step 127/7134 completed (loss: 0.875531017780304, acc: 0.8541666865348816)
[2025-02-13 19:19:51,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:52,077][root][INFO] - Training Epoch: 1/2, step 128/7134 completed (loss: 0.8856180906295776, acc: 0.8086124658584595)
[2025-02-13 19:19:52,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:52,540][root][INFO] - Training Epoch: 1/2, step 129/7134 completed (loss: 1.0045360326766968, acc: 0.8100558519363403)
[2025-02-13 19:19:52,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:52,943][root][INFO] - Training Epoch: 1/2, step 130/7134 completed (loss: 0.6231512427330017, acc: 0.8529411554336548)
[2025-02-13 19:19:53,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:53,370][root][INFO] - Training Epoch: 1/2, step 131/7134 completed (loss: 0.48057737946510315, acc: 0.8947368264198303)
[2025-02-13 19:19:53,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:53,804][root][INFO] - Training Epoch: 1/2, step 132/7134 completed (loss: 0.6082344055175781, acc: 0.8364779949188232)
[2025-02-13 19:19:53,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:54,279][root][INFO] - Training Epoch: 1/2, step 133/7134 completed (loss: 0.7347307205200195, acc: 0.8161764740943909)
[2025-02-13 19:19:54,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:54,698][root][INFO] - Training Epoch: 1/2, step 134/7134 completed (loss: 0.5117580890655518, acc: 0.8796992301940918)
[2025-02-13 19:19:54,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:55,125][root][INFO] - Training Epoch: 1/2, step 135/7134 completed (loss: 0.7366487979888916, acc: 0.8571428656578064)
[2025-02-13 19:19:55,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:55,512][root][INFO] - Training Epoch: 1/2, step 136/7134 completed (loss: 0.5809131860733032, acc: 0.864130437374115)
[2025-02-13 19:19:55,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:55,935][root][INFO] - Training Epoch: 1/2, step 137/7134 completed (loss: 0.7578109502792358, acc: 0.8542713522911072)
[2025-02-13 19:19:56,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:56,359][root][INFO] - Training Epoch: 1/2, step 138/7134 completed (loss: 0.9140452742576599, acc: 0.7956989407539368)
[2025-02-13 19:19:56,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:56,798][root][INFO] - Training Epoch: 1/2, step 139/7134 completed (loss: 0.8554503917694092, acc: 0.835106372833252)
[2025-02-13 19:19:56,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:57,195][root][INFO] - Training Epoch: 1/2, step 140/7134 completed (loss: 0.5818794965744019, acc: 0.8588957190513611)
[2025-02-13 19:19:57,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:57,607][root][INFO] - Training Epoch: 1/2, step 141/7134 completed (loss: 0.6185181140899658, acc: 0.8700000047683716)
[2025-02-13 19:19:57,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:58,049][root][INFO] - Training Epoch: 1/2, step 142/7134 completed (loss: 0.6379004716873169, acc: 0.8757061958312988)
[2025-02-13 19:19:58,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:58,440][root][INFO] - Training Epoch: 1/2, step 143/7134 completed (loss: 0.6955274939537048, acc: 0.8287292718887329)
[2025-02-13 19:19:58,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:58,878][root][INFO] - Training Epoch: 1/2, step 144/7134 completed (loss: 0.9844940900802612, acc: 0.7905405163764954)
[2025-02-13 19:19:59,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:59,298][root][INFO] - Training Epoch: 1/2, step 145/7134 completed (loss: 1.2179694175720215, acc: 0.7461140155792236)
[2025-02-13 19:19:59,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:59,705][root][INFO] - Training Epoch: 1/2, step 146/7134 completed (loss: 0.799367368221283, acc: 0.7740113139152527)
[2025-02-13 19:19:59,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:00,139][root][INFO] - Training Epoch: 1/2, step 147/7134 completed (loss: 0.9879950284957886, acc: 0.7931034564971924)
[2025-02-13 19:20:00,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:00,552][root][INFO] - Training Epoch: 1/2, step 148/7134 completed (loss: 1.3570102453231812, acc: 0.7096773982048035)
[2025-02-13 19:20:00,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:00,966][root][INFO] - Training Epoch: 1/2, step 149/7134 completed (loss: 1.0037477016448975, acc: 0.7918781638145447)
[2025-02-13 19:20:01,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:01,351][root][INFO] - Training Epoch: 1/2, step 150/7134 completed (loss: 1.0729939937591553, acc: 0.7801046967506409)
[2025-02-13 19:20:01,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:01,748][root][INFO] - Training Epoch: 1/2, step 151/7134 completed (loss: 0.6699702739715576, acc: 0.8602150678634644)
[2025-02-13 19:20:01,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02,129][root][INFO] - Training Epoch: 1/2, step 152/7134 completed (loss: 0.6391641497612, acc: 0.8708133697509766)
[2025-02-13 19:20:02,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02,478][root][INFO] - Training Epoch: 1/2, step 153/7134 completed (loss: 1.0689494609832764, acc: 0.7569060921669006)
[2025-02-13 19:20:02,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02,875][root][INFO] - Training Epoch: 1/2, step 154/7134 completed (loss: 0.8476163744926453, acc: 0.8300653696060181)
[2025-02-13 19:20:03,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:03,273][root][INFO] - Training Epoch: 1/2, step 155/7134 completed (loss: 0.8555500507354736, acc: 0.7978141903877258)
[2025-02-13 19:20:03,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:03,687][root][INFO] - Training Epoch: 1/2, step 156/7134 completed (loss: 0.6803544759750366, acc: 0.8472906351089478)
[2025-02-13 19:20:03,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:04,062][root][INFO] - Training Epoch: 1/2, step 157/7134 completed (loss: 0.8224169015884399, acc: 0.8148148059844971)
[2025-02-13 19:20:04,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:04,471][root][INFO] - Training Epoch: 1/2, step 158/7134 completed (loss: 0.5046972632408142, acc: 0.8947368264198303)
[2025-02-13 19:20:04,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:04,886][root][INFO] - Training Epoch: 1/2, step 159/7134 completed (loss: 0.7799127101898193, acc: 0.8150289058685303)
[2025-02-13 19:20:05,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:05,248][root][INFO] - Training Epoch: 1/2, step 160/7134 completed (loss: 0.6775964498519897, acc: 0.8305084705352783)
[2025-02-13 19:20:05,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:05,636][root][INFO] - Training Epoch: 1/2, step 161/7134 completed (loss: 0.49929025769233704, acc: 0.9054054021835327)
[2025-02-13 19:20:05,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:06,042][root][INFO] - Training Epoch: 1/2, step 162/7134 completed (loss: 0.5975907444953918, acc: 0.8641975522041321)
[2025-02-13 19:20:06,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:06,447][root][INFO] - Training Epoch: 1/2, step 163/7134 completed (loss: 0.6014242768287659, acc: 0.8273381590843201)
[2025-02-13 19:20:06,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:06,841][root][INFO] - Training Epoch: 1/2, step 164/7134 completed (loss: 0.6982408165931702, acc: 0.8383838534355164)
[2025-02-13 19:20:07,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:07,242][root][INFO] - Training Epoch: 1/2, step 165/7134 completed (loss: 0.5590691566467285, acc: 0.8600000143051147)
[2025-02-13 19:20:07,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:07,603][root][INFO] - Training Epoch: 1/2, step 166/7134 completed (loss: 0.4240877330303192, acc: 0.8552631735801697)
[2025-02-13 19:20:07,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:08,064][root][INFO] - Training Epoch: 1/2, step 167/7134 completed (loss: 2.751243829727173, acc: 0.5722891688346863)
[2025-02-13 19:20:08,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:08,461][root][INFO] - Training Epoch: 1/2, step 168/7134 completed (loss: 5.030994415283203, acc: 0.2818181812763214)
[2025-02-13 19:20:08,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:08,908][root][INFO] - Training Epoch: 1/2, step 169/7134 completed (loss: 4.151239395141602, acc: 0.421875)
[2025-02-13 19:20:09,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:09,304][root][INFO] - Training Epoch: 1/2, step 170/7134 completed (loss: 3.2056634426116943, acc: 0.5297619104385376)
[2025-02-13 19:20:09,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:09,741][root][INFO] - Training Epoch: 1/2, step 171/7134 completed (loss: 3.547682762145996, acc: 0.48514851927757263)
[2025-02-13 19:20:09,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:10,206][root][INFO] - Training Epoch: 1/2, step 172/7134 completed (loss: 1.1904696226119995, acc: 0.7286821603775024)
[2025-02-13 19:20:10,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:10,628][root][INFO] - Training Epoch: 1/2, step 173/7134 completed (loss: 1.237549901008606, acc: 0.7120000123977661)
[2025-02-13 19:20:10,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:11,063][root][INFO] - Training Epoch: 1/2, step 174/7134 completed (loss: 1.8226652145385742, acc: 0.6842105388641357)
[2025-02-13 19:20:11,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:11,461][root][INFO] - Training Epoch: 1/2, step 175/7134 completed (loss: 1.5166947841644287, acc: 0.6754966974258423)
[2025-02-13 19:20:11,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:11,849][root][INFO] - Training Epoch: 1/2, step 176/7134 completed (loss: 1.306869626045227, acc: 0.7431694269180298)
[2025-02-13 19:20:11,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:12,247][root][INFO] - Training Epoch: 1/2, step 177/7134 completed (loss: 1.5592082738876343, acc: 0.682634711265564)
[2025-02-13 19:20:12,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:12,617][root][INFO] - Training Epoch: 1/2, step 178/7134 completed (loss: 1.0962125062942505, acc: 0.8051947951316833)
[2025-02-13 19:20:12,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:13,048][root][INFO] - Training Epoch: 1/2, step 179/7134 completed (loss: 0.9239317178726196, acc: 0.7883211970329285)
[2025-02-13 19:20:13,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:13,502][root][INFO] - Training Epoch: 1/2, step 180/7134 completed (loss: 0.7014711499214172, acc: 0.8322147727012634)
[2025-02-13 19:20:13,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:13,924][root][INFO] - Training Epoch: 1/2, step 181/7134 completed (loss: 0.5781344175338745, acc: 0.8413792848587036)
[2025-02-13 19:20:14,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:14,365][root][INFO] - Training Epoch: 1/2, step 182/7134 completed (loss: 0.6298597455024719, acc: 0.850931704044342)
[2025-02-13 19:20:14,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:14,817][root][INFO] - Training Epoch: 1/2, step 183/7134 completed (loss: 0.49698859453201294, acc: 0.8904109597206116)
[2025-02-13 19:20:14,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:15,243][root][INFO] - Training Epoch: 1/2, step 184/7134 completed (loss: 0.5998672842979431, acc: 0.84375)
[2025-02-13 19:20:15,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:15,634][root][INFO] - Training Epoch: 1/2, step 185/7134 completed (loss: 0.25335296988487244, acc: 0.9523809552192688)
[2025-02-13 19:20:15,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:16,092][root][INFO] - Training Epoch: 1/2, step 186/7134 completed (loss: 0.4880503714084625, acc: 0.89552241563797)
[2025-02-13 19:20:16,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:16,505][root][INFO] - Training Epoch: 1/2, step 187/7134 completed (loss: 0.5018497705459595, acc: 0.8758170008659363)
[2025-02-13 19:20:16,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:16,894][root][INFO] - Training Epoch: 1/2, step 188/7134 completed (loss: 0.7950764894485474, acc: 0.8353658318519592)
[2025-02-13 19:20:17,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:17,280][root][INFO] - Training Epoch: 1/2, step 189/7134 completed (loss: 0.5788471698760986, acc: 0.8524590134620667)
[2025-02-13 19:20:17,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:17,671][root][INFO] - Training Epoch: 1/2, step 190/7134 completed (loss: 0.8349127173423767, acc: 0.7985074520111084)
[2025-02-13 19:20:17,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:18,064][root][INFO] - Training Epoch: 1/2, step 191/7134 completed (loss: 0.6507360935211182, acc: 0.8557692170143127)
[2025-02-13 19:20:18,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:18,485][root][INFO] - Training Epoch: 1/2, step 192/7134 completed (loss: 0.4672664701938629, acc: 0.8720930218696594)
[2025-02-13 19:20:18,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:18,852][root][INFO] - Training Epoch: 1/2, step 193/7134 completed (loss: 0.826694667339325, acc: 0.7904191613197327)
[2025-02-13 19:20:19,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:19,247][root][INFO] - Training Epoch: 1/2, step 194/7134 completed (loss: 1.0126128196716309, acc: 0.7861635088920593)
[2025-02-13 19:20:19,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:19,642][root][INFO] - Training Epoch: 1/2, step 195/7134 completed (loss: 0.7287052869796753, acc: 0.8125)
[2025-02-13 19:20:19,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:20,018][root][INFO] - Training Epoch: 1/2, step 196/7134 completed (loss: 0.9241830706596375, acc: 0.7932960987091064)
[2025-02-13 19:20:20,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:20,448][root][INFO] - Training Epoch: 1/2, step 197/7134 completed (loss: 0.6251688003540039, acc: 0.8716216087341309)
[2025-02-13 19:20:20,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:20,892][root][INFO] - Training Epoch: 1/2, step 198/7134 completed (loss: 0.639306902885437, acc: 0.8471337556838989)
[2025-02-13 19:20:21,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:21,327][root][INFO] - Training Epoch: 1/2, step 199/7134 completed (loss: 0.9990988373756409, acc: 0.7941176295280457)
[2025-02-13 19:20:21,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:21,785][root][INFO] - Training Epoch: 1/2, step 200/7134 completed (loss: 0.5394032001495361, acc: 0.884353756904602)
[2025-02-13 19:20:21,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:22,247][root][INFO] - Training Epoch: 1/2, step 201/7134 completed (loss: 0.35694390535354614, acc: 0.8936170339584351)
[2025-02-13 19:20:22,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:22,705][root][INFO] - Training Epoch: 1/2, step 202/7134 completed (loss: 0.4802814722061157, acc: 0.9032257795333862)
[2025-02-13 19:20:22,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:23,167][root][INFO] - Training Epoch: 1/2, step 203/7134 completed (loss: 0.716172456741333, acc: 0.8461538553237915)
[2025-02-13 19:20:23,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:23,604][root][INFO] - Training Epoch: 1/2, step 204/7134 completed (loss: 0.751494288444519, acc: 0.8113207817077637)
[2025-02-13 19:20:23,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:24,018][root][INFO] - Training Epoch: 1/2, step 205/7134 completed (loss: 0.9522903561592102, acc: 0.7900000214576721)
[2025-02-13 19:20:24,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:24,479][root][INFO] - Training Epoch: 1/2, step 206/7134 completed (loss: 0.34913691878318787, acc: 0.8958333134651184)
[2025-02-13 19:20:24,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:24,867][root][INFO] - Training Epoch: 1/2, step 207/7134 completed (loss: 0.6829363107681274, acc: 0.8435373902320862)
[2025-02-13 19:20:25,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:25,298][root][INFO] - Training Epoch: 1/2, step 208/7134 completed (loss: 1.2712219953536987, acc: 0.7642857432365417)
[2025-02-13 19:20:25,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:25,749][root][INFO] - Training Epoch: 1/2, step 209/7134 completed (loss: 1.3640512228012085, acc: 0.6703910827636719)
[2025-02-13 19:20:25,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:26,206][root][INFO] - Training Epoch: 1/2, step 210/7134 completed (loss: 0.8964428305625916, acc: 0.8281938433647156)
[2025-02-13 19:20:26,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:26,646][root][INFO] - Training Epoch: 1/2, step 211/7134 completed (loss: 0.8511673212051392, acc: 0.7837837934494019)
[2025-02-13 19:20:26,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:27,067][root][INFO] - Training Epoch: 1/2, step 212/7134 completed (loss: 1.1799663305282593, acc: 0.7685950398445129)
[2025-02-13 19:20:27,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:27,468][root][INFO] - Training Epoch: 1/2, step 213/7134 completed (loss: 1.2022967338562012, acc: 0.7666666507720947)
[2025-02-13 19:20:27,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:27,871][root][INFO] - Training Epoch: 1/2, step 214/7134 completed (loss: 0.6622641682624817, acc: 0.8415841460227966)
[2025-02-13 19:20:28,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:28,258][root][INFO] - Training Epoch: 1/2, step 215/7134 completed (loss: 0.620978593826294, acc: 0.8285714387893677)
[2025-02-13 19:20:28,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:28,687][root][INFO] - Training Epoch: 1/2, step 216/7134 completed (loss: 0.3911060690879822, acc: 0.8932584524154663)
[2025-02-13 19:20:28,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:29,135][root][INFO] - Training Epoch: 1/2, step 217/7134 completed (loss: 0.4953326880931854, acc: 0.8802083134651184)
[2025-02-13 19:20:29,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:29,563][root][INFO] - Training Epoch: 1/2, step 218/7134 completed (loss: 0.32385703921318054, acc: 0.9144384860992432)
[2025-02-13 19:20:29,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:29,991][root][INFO] - Training Epoch: 1/2, step 219/7134 completed (loss: 0.2648322880268097, acc: 0.9035087823867798)
[2025-02-13 19:20:30,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:30,389][root][INFO] - Training Epoch: 1/2, step 220/7134 completed (loss: 0.35333892703056335, acc: 0.904347836971283)
[2025-02-13 19:20:30,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:30,791][root][INFO] - Training Epoch: 1/2, step 221/7134 completed (loss: 0.523921012878418, acc: 0.893750011920929)
[2025-02-13 19:20:30,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:31,258][root][INFO] - Training Epoch: 1/2, step 222/7134 completed (loss: 0.6135656833648682, acc: 0.8612716794013977)
[2025-02-13 19:20:31,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:31,638][root][INFO] - Training Epoch: 1/2, step 223/7134 completed (loss: 0.8239468336105347, acc: 0.8108108043670654)
[2025-02-13 19:20:31,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:32,018][root][INFO] - Training Epoch: 1/2, step 224/7134 completed (loss: 0.9070250391960144, acc: 0.7932960987091064)
[2025-02-13 19:20:32,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:32,466][root][INFO] - Training Epoch: 1/2, step 225/7134 completed (loss: 0.824986457824707, acc: 0.8047337532043457)
[2025-02-13 19:20:32,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:32,908][root][INFO] - Training Epoch: 1/2, step 226/7134 completed (loss: 0.691920280456543, acc: 0.8514285683631897)
[2025-02-13 19:20:33,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:33,337][root][INFO] - Training Epoch: 1/2, step 227/7134 completed (loss: 0.8746569752693176, acc: 0.75)
[2025-02-13 19:20:33,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:33,776][root][INFO] - Training Epoch: 1/2, step 228/7134 completed (loss: 0.7786930799484253, acc: 0.8083832263946533)
[2025-02-13 19:20:33,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:34,170][root][INFO] - Training Epoch: 1/2, step 229/7134 completed (loss: 0.9760414361953735, acc: 0.7714285850524902)
[2025-02-13 19:20:34,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:34,619][root][INFO] - Training Epoch: 1/2, step 230/7134 completed (loss: 0.7390641570091248, acc: 0.8121547102928162)
[2025-02-13 19:20:34,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:35,037][root][INFO] - Training Epoch: 1/2, step 231/7134 completed (loss: 0.6204975247383118, acc: 0.8421052694320679)
[2025-02-13 19:20:35,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:35,486][root][INFO] - Training Epoch: 1/2, step 232/7134 completed (loss: 0.7278450131416321, acc: 0.8181818127632141)
[2025-02-13 19:20:35,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:35,912][root][INFO] - Training Epoch: 1/2, step 233/7134 completed (loss: 0.5303047895431519, acc: 0.8905109763145447)
[2025-02-13 19:20:36,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:36,347][root][INFO] - Training Epoch: 1/2, step 234/7134 completed (loss: 0.7789466977119446, acc: 0.7957746386528015)
[2025-02-13 19:20:36,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:36,730][root][INFO] - Training Epoch: 1/2, step 235/7134 completed (loss: 0.7931415438652039, acc: 0.8170731663703918)
[2025-02-13 19:20:36,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:37,124][root][INFO] - Training Epoch: 1/2, step 236/7134 completed (loss: 0.5214357972145081, acc: 0.860927164554596)
[2025-02-13 19:20:37,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:37,521][root][INFO] - Training Epoch: 1/2, step 237/7134 completed (loss: 0.6771307587623596, acc: 0.8343949317932129)
[2025-02-13 19:20:37,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:37,939][root][INFO] - Training Epoch: 1/2, step 238/7134 completed (loss: 0.852684736251831, acc: 0.8167939186096191)
[2025-02-13 19:20:38,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:38,364][root][INFO] - Training Epoch: 1/2, step 239/7134 completed (loss: 0.7843424677848816, acc: 0.8207547068595886)
[2025-02-13 19:20:38,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:38,775][root][INFO] - Training Epoch: 1/2, step 240/7134 completed (loss: 0.8917466402053833, acc: 0.8349514603614807)
[2025-02-13 19:20:38,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:39,221][root][INFO] - Training Epoch: 1/2, step 241/7134 completed (loss: 0.9260782599449158, acc: 0.8059701323509216)
[2025-02-13 19:20:39,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:39,667][root][INFO] - Training Epoch: 1/2, step 242/7134 completed (loss: 1.1030482053756714, acc: 0.7583892345428467)
[2025-02-13 19:20:39,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:40,089][root][INFO] - Training Epoch: 1/2, step 243/7134 completed (loss: 1.295562982559204, acc: 0.7241379022598267)
[2025-02-13 19:20:40,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:40,450][root][INFO] - Training Epoch: 1/2, step 244/7134 completed (loss: 0.5104047060012817, acc: 0.8920863270759583)
[2025-02-13 19:20:40,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:40,817][root][INFO] - Training Epoch: 1/2, step 245/7134 completed (loss: 0.7869037389755249, acc: 0.8360655903816223)
[2025-02-13 19:20:40,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:41,255][root][INFO] - Training Epoch: 1/2, step 246/7134 completed (loss: 0.6470817923545837, acc: 0.8405796885490417)
[2025-02-13 19:20:41,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:41,636][root][INFO] - Training Epoch: 1/2, step 247/7134 completed (loss: 0.8403752446174622, acc: 0.8333333134651184)
[2025-02-13 19:20:41,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:42,101][root][INFO] - Training Epoch: 1/2, step 248/7134 completed (loss: 0.6711517572402954, acc: 0.8373983502388)
[2025-02-13 19:20:42,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:42,527][root][INFO] - Training Epoch: 1/2, step 249/7134 completed (loss: 0.6925802826881409, acc: 0.8152173757553101)
[2025-02-13 19:20:42,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:42,988][root][INFO] - Training Epoch: 1/2, step 250/7134 completed (loss: 0.778296172618866, acc: 0.807947039604187)
[2025-02-13 19:20:43,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:43,360][root][INFO] - Training Epoch: 1/2, step 251/7134 completed (loss: 0.7170163989067078, acc: 0.8417266011238098)
[2025-02-13 19:20:43,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:43,730][root][INFO] - Training Epoch: 1/2, step 252/7134 completed (loss: 0.3978358805179596, acc: 0.8920863270759583)
[2025-02-13 19:20:43,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:44,108][root][INFO] - Training Epoch: 1/2, step 253/7134 completed (loss: 0.4943477213382721, acc: 0.8799999952316284)
[2025-02-13 19:20:44,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:44,515][root][INFO] - Training Epoch: 1/2, step 254/7134 completed (loss: 0.682952880859375, acc: 0.8421052694320679)
[2025-02-13 19:20:44,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:44,960][root][INFO] - Training Epoch: 1/2, step 255/7134 completed (loss: 0.4143470823764801, acc: 0.9294871687889099)
[2025-02-13 19:20:45,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:45,387][root][INFO] - Training Epoch: 1/2, step 256/7134 completed (loss: 1.0803349018096924, acc: 0.7567567825317383)
[2025-02-13 19:20:45,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:45,838][root][INFO] - Training Epoch: 1/2, step 257/7134 completed (loss: 0.823875904083252, acc: 0.8414633870124817)
[2025-02-13 19:20:46,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:46,252][root][INFO] - Training Epoch: 1/2, step 258/7134 completed (loss: 0.5704922080039978, acc: 0.8790322542190552)
[2025-02-13 19:20:46,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:46,663][root][INFO] - Training Epoch: 1/2, step 259/7134 completed (loss: 0.6791384816169739, acc: 0.8409090638160706)
[2025-02-13 19:20:46,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:47,102][root][INFO] - Training Epoch: 1/2, step 260/7134 completed (loss: 0.46531450748443604, acc: 0.8947368264198303)
[2025-02-13 19:20:47,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:47,537][root][INFO] - Training Epoch: 1/2, step 261/7134 completed (loss: 0.628335177898407, acc: 0.8888888955116272)
[2025-02-13 19:20:47,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:47,944][root][INFO] - Training Epoch: 1/2, step 262/7134 completed (loss: 0.2870938777923584, acc: 0.9268292784690857)
[2025-02-13 19:20:48,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:48,361][root][INFO] - Training Epoch: 1/2, step 263/7134 completed (loss: 0.32781359553337097, acc: 0.9074074029922485)
[2025-02-13 19:20:48,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:48,768][root][INFO] - Training Epoch: 1/2, step 264/7134 completed (loss: 0.4992958605289459, acc: 0.8764705657958984)
[2025-02-13 19:20:48,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:49,162][root][INFO] - Training Epoch: 1/2, step 265/7134 completed (loss: 0.4907734990119934, acc: 0.8932584524154663)
[2025-02-13 19:20:49,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:49,575][root][INFO] - Training Epoch: 1/2, step 266/7134 completed (loss: 0.40448567271232605, acc: 0.90625)
[2025-02-13 19:20:49,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:49,988][root][INFO] - Training Epoch: 1/2, step 267/7134 completed (loss: 0.5935325026512146, acc: 0.8617021441459656)
[2025-02-13 19:20:50,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:50,438][root][INFO] - Training Epoch: 1/2, step 268/7134 completed (loss: 0.5698181390762329, acc: 0.8606060743331909)
[2025-02-13 19:20:50,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:50,893][root][INFO] - Training Epoch: 1/2, step 269/7134 completed (loss: 0.7691419720649719, acc: 0.8333333134651184)
[2025-02-13 19:20:51,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:51,296][root][INFO] - Training Epoch: 1/2, step 270/7134 completed (loss: 0.536893904209137, acc: 0.893750011920929)
[2025-02-13 19:20:51,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:51,745][root][INFO] - Training Epoch: 1/2, step 271/7134 completed (loss: 0.45367610454559326, acc: 0.9055555462837219)
[2025-02-13 19:20:51,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:52,132][root][INFO] - Training Epoch: 1/2, step 272/7134 completed (loss: 0.5974839925765991, acc: 0.8491619825363159)
[2025-02-13 19:20:52,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:52,528][root][INFO] - Training Epoch: 1/2, step 273/7134 completed (loss: 0.36364153027534485, acc: 0.914893627166748)
[2025-02-13 19:20:52,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:52,917][root][INFO] - Training Epoch: 1/2, step 274/7134 completed (loss: 0.6052497029304504, acc: 0.8830409646034241)
[2025-02-13 19:20:53,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:53,388][root][INFO] - Training Epoch: 1/2, step 275/7134 completed (loss: 0.4072014391422272, acc: 0.8984771370887756)
[2025-02-13 19:20:53,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:53,829][root][INFO] - Training Epoch: 1/2, step 276/7134 completed (loss: 0.4049418866634369, acc: 0.8814433217048645)
[2025-02-13 19:20:54,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:54,267][root][INFO] - Training Epoch: 1/2, step 277/7134 completed (loss: 0.6740380525588989, acc: 0.8540540337562561)
[2025-02-13 19:20:54,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:54,712][root][INFO] - Training Epoch: 1/2, step 278/7134 completed (loss: 0.6564632058143616, acc: 0.859375)
[2025-02-13 19:20:54,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:55,128][root][INFO] - Training Epoch: 1/2, step 279/7134 completed (loss: 0.5904083251953125, acc: 0.8486486673355103)
[2025-02-13 19:20:55,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:55,517][root][INFO] - Training Epoch: 1/2, step 280/7134 completed (loss: 0.7396845817565918, acc: 0.8240000009536743)
[2025-02-13 19:20:55,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:55,933][root][INFO] - Training Epoch: 1/2, step 281/7134 completed (loss: 0.8171733021736145, acc: 0.8135592937469482)
[2025-02-13 19:20:56,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:56,338][root][INFO] - Training Epoch: 1/2, step 282/7134 completed (loss: 0.9795336127281189, acc: 0.797468364238739)
[2025-02-13 19:20:56,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:56,763][root][INFO] - Training Epoch: 1/2, step 283/7134 completed (loss: 0.8945587873458862, acc: 0.8167939186096191)
[2025-02-13 19:20:56,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:57,218][root][INFO] - Training Epoch: 1/2, step 284/7134 completed (loss: 0.5806785821914673, acc: 0.867132842540741)
[2025-02-13 19:20:57,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:57,625][root][INFO] - Training Epoch: 1/2, step 285/7134 completed (loss: 0.7632991075515747, acc: 0.8370370268821716)
[2025-02-13 19:20:57,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:58,041][root][INFO] - Training Epoch: 1/2, step 286/7134 completed (loss: 0.684969961643219, acc: 0.8333333134651184)
[2025-02-13 19:20:58,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:58,448][root][INFO] - Training Epoch: 1/2, step 287/7134 completed (loss: 0.6710459589958191, acc: 0.8255033493041992)
[2025-02-13 19:20:58,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:58,852][root][INFO] - Training Epoch: 1/2, step 288/7134 completed (loss: 0.6884148716926575, acc: 0.8188976645469666)
[2025-02-13 19:20:59,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:59,297][root][INFO] - Training Epoch: 1/2, step 289/7134 completed (loss: 0.4528476595878601, acc: 0.8759689927101135)
[2025-02-13 19:20:59,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:59,702][root][INFO] - Training Epoch: 1/2, step 290/7134 completed (loss: 0.7592332363128662, acc: 0.8376623392105103)
[2025-02-13 19:20:59,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:00,140][root][INFO] - Training Epoch: 1/2, step 291/7134 completed (loss: 0.6382395625114441, acc: 0.8707482814788818)
[2025-02-13 19:21:00,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:00,515][root][INFO] - Training Epoch: 1/2, step 292/7134 completed (loss: 0.9072188138961792, acc: 0.8148148059844971)
[2025-02-13 19:21:00,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:00,967][root][INFO] - Training Epoch: 1/2, step 293/7134 completed (loss: 0.7431196570396423, acc: 0.8672566413879395)
[2025-02-13 19:21:01,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:01,416][root][INFO] - Training Epoch: 1/2, step 294/7134 completed (loss: 0.6836832165718079, acc: 0.8389830589294434)
[2025-02-13 19:21:01,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:01,840][root][INFO] - Training Epoch: 1/2, step 295/7134 completed (loss: 0.966776430606842, acc: 0.8205128312110901)
[2025-02-13 19:21:02,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:02,257][root][INFO] - Training Epoch: 1/2, step 296/7134 completed (loss: 0.46384721994400024, acc: 0.8787878751754761)
[2025-02-13 19:21:02,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:02,659][root][INFO] - Training Epoch: 1/2, step 297/7134 completed (loss: 0.5173471570014954, acc: 0.8760330677032471)
[2025-02-13 19:21:02,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:03,098][root][INFO] - Training Epoch: 1/2, step 298/7134 completed (loss: 0.5753236413002014, acc: 0.8846153616905212)
[2025-02-13 19:21:03,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:03,514][root][INFO] - Training Epoch: 1/2, step 299/7134 completed (loss: 0.2891678512096405, acc: 0.9197080135345459)
[2025-02-13 19:21:03,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:03,929][root][INFO] - Training Epoch: 1/2, step 300/7134 completed (loss: 0.27022236585617065, acc: 0.9202898740768433)
[2025-02-13 19:21:04,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:04,353][root][INFO] - Training Epoch: 1/2, step 301/7134 completed (loss: 0.3656638562679291, acc: 0.9090909361839294)
[2025-02-13 19:21:04,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:04,746][root][INFO] - Training Epoch: 1/2, step 302/7134 completed (loss: 0.45641636848449707, acc: 0.9009901285171509)
[2025-02-13 19:21:04,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:05,161][root][INFO] - Training Epoch: 1/2, step 303/7134 completed (loss: 0.4206976294517517, acc: 0.8738738894462585)
[2025-02-13 19:21:05,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:05,552][root][INFO] - Training Epoch: 1/2, step 304/7134 completed (loss: 0.42743951082229614, acc: 0.8910890817642212)
[2025-02-13 19:21:05,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:05,920][root][INFO] - Training Epoch: 1/2, step 305/7134 completed (loss: 0.38358134031295776, acc: 0.9453125)
[2025-02-13 19:21:06,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:06,331][root][INFO] - Training Epoch: 1/2, step 306/7134 completed (loss: 0.2110164612531662, acc: 0.939130425453186)
[2025-02-13 19:21:06,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:06,816][root][INFO] - Training Epoch: 1/2, step 307/7134 completed (loss: 0.6247552037239075, acc: 0.8130841255187988)
[2025-02-13 19:21:06,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:07,198][root][INFO] - Training Epoch: 1/2, step 308/7134 completed (loss: 0.3515144884586334, acc: 0.9212598204612732)
[2025-02-13 19:21:07,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:07,654][root][INFO] - Training Epoch: 1/2, step 309/7134 completed (loss: 0.3189508318901062, acc: 0.9292035102844238)
[2025-02-13 19:21:07,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:08,090][root][INFO] - Training Epoch: 1/2, step 310/7134 completed (loss: 0.6363824605941772, acc: 0.8604651093482971)
[2025-02-13 19:21:08,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:08,463][root][INFO] - Training Epoch: 1/2, step 311/7134 completed (loss: 0.3155752420425415, acc: 0.902255654335022)
[2025-02-13 19:21:08,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:08,868][root][INFO] - Training Epoch: 1/2, step 312/7134 completed (loss: 0.18663807213306427, acc: 0.9590163826942444)
[2025-02-13 19:21:09,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:09,278][root][INFO] - Training Epoch: 1/2, step 313/7134 completed (loss: 0.36488237977027893, acc: 0.9120000004768372)
[2025-02-13 19:21:09,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:09,684][root][INFO] - Training Epoch: 1/2, step 314/7134 completed (loss: 0.3892887830734253, acc: 0.9185185432434082)
[2025-02-13 19:21:09,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:10,075][root][INFO] - Training Epoch: 1/2, step 315/7134 completed (loss: 0.6142857670783997, acc: 0.8666666746139526)
[2025-02-13 19:21:10,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:10,484][root][INFO] - Training Epoch: 1/2, step 316/7134 completed (loss: 0.49380069971084595, acc: 0.8918918967247009)
[2025-02-13 19:21:10,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:10,907][root][INFO] - Training Epoch: 1/2, step 317/7134 completed (loss: 0.24471025168895721, acc: 0.949367105960846)
[2025-02-13 19:21:11,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:11,309][root][INFO] - Training Epoch: 1/2, step 318/7134 completed (loss: 0.4597260653972626, acc: 0.8870967626571655)
[2025-02-13 19:21:11,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:11,694][root][INFO] - Training Epoch: 1/2, step 319/7134 completed (loss: 0.7006624937057495, acc: 0.8385093212127686)
[2025-02-13 19:21:11,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:12,131][root][INFO] - Training Epoch: 1/2, step 320/7134 completed (loss: 0.6042569279670715, acc: 0.8571428656578064)
[2025-02-13 19:21:12,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:12,569][root][INFO] - Training Epoch: 1/2, step 321/7134 completed (loss: 0.6263619065284729, acc: 0.8505747318267822)
[2025-02-13 19:21:12,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:12,999][root][INFO] - Training Epoch: 1/2, step 322/7134 completed (loss: 0.5240880846977234, acc: 0.8758170008659363)
[2025-02-13 19:21:13,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:13,367][root][INFO] - Training Epoch: 1/2, step 323/7134 completed (loss: 0.9106106758117676, acc: 0.7806122303009033)
[2025-02-13 19:21:13,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:13,760][root][INFO] - Training Epoch: 1/2, step 324/7134 completed (loss: 0.7969889640808105, acc: 0.8240740895271301)
[2025-02-13 19:21:13,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:14,182][root][INFO] - Training Epoch: 1/2, step 325/7134 completed (loss: 0.5197258591651917, acc: 0.8920454382896423)
[2025-02-13 19:21:14,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:14,575][root][INFO] - Training Epoch: 1/2, step 326/7134 completed (loss: 0.7099888324737549, acc: 0.7921348214149475)
[2025-02-13 19:21:14,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:14,987][root][INFO] - Training Epoch: 1/2, step 327/7134 completed (loss: 0.538873016834259, acc: 0.8989899158477783)
[2025-02-13 19:21:15,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:15,446][root][INFO] - Training Epoch: 1/2, step 328/7134 completed (loss: 0.5088526606559753, acc: 0.875)
[2025-02-13 19:21:15,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:15,853][root][INFO] - Training Epoch: 1/2, step 329/7134 completed (loss: 0.5545021295547485, acc: 0.8905472755432129)
[2025-02-13 19:21:16,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:16,256][root][INFO] - Training Epoch: 1/2, step 330/7134 completed (loss: 0.5093284845352173, acc: 0.890350878238678)
[2025-02-13 19:21:16,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:16,668][root][INFO] - Training Epoch: 1/2, step 331/7134 completed (loss: 0.35474076867103577, acc: 0.9166666865348816)
[2025-02-13 19:21:16,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:17,068][root][INFO] - Training Epoch: 1/2, step 332/7134 completed (loss: 0.2192400097846985, acc: 0.9402984976768494)
[2025-02-13 19:21:17,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:17,484][root][INFO] - Training Epoch: 1/2, step 333/7134 completed (loss: 0.525902509689331, acc: 0.8602150678634644)
[2025-02-13 19:21:17,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:17,929][root][INFO] - Training Epoch: 1/2, step 334/7134 completed (loss: 0.4771571457386017, acc: 0.8736263513565063)
[2025-02-13 19:21:18,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:18,362][root][INFO] - Training Epoch: 1/2, step 335/7134 completed (loss: 0.40053239464759827, acc: 0.9256756901741028)
[2025-02-13 19:21:18,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:18,726][root][INFO] - Training Epoch: 1/2, step 336/7134 completed (loss: 0.47581958770751953, acc: 0.8963730335235596)
[2025-02-13 19:21:18,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:19,168][root][INFO] - Training Epoch: 1/2, step 337/7134 completed (loss: 0.4530722200870514, acc: 0.8633540272712708)
[2025-02-13 19:21:19,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:19,590][root][INFO] - Training Epoch: 1/2, step 338/7134 completed (loss: 0.6299040913581848, acc: 0.8795811533927917)
[2025-02-13 19:21:19,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:20,017][root][INFO] - Training Epoch: 1/2, step 339/7134 completed (loss: 0.4774776101112366, acc: 0.8770053386688232)
[2025-02-13 19:21:20,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:20,454][root][INFO] - Training Epoch: 1/2, step 340/7134 completed (loss: 0.3498198091983795, acc: 0.8989899158477783)
[2025-02-13 19:21:20,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:20,875][root][INFO] - Training Epoch: 1/2, step 341/7134 completed (loss: 0.3925597667694092, acc: 0.8928571343421936)
[2025-02-13 19:21:21,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:21,279][root][INFO] - Training Epoch: 1/2, step 342/7134 completed (loss: 0.41849926114082336, acc: 0.893081784248352)
[2025-02-13 19:21:21,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:21,689][root][INFO] - Training Epoch: 1/2, step 343/7134 completed (loss: 0.6451009511947632, acc: 0.8615384697914124)
[2025-02-13 19:21:21,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:22,090][root][INFO] - Training Epoch: 1/2, step 344/7134 completed (loss: 0.3889249861240387, acc: 0.8992248177528381)
[2025-02-13 19:21:22,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:22,508][root][INFO] - Training Epoch: 1/2, step 345/7134 completed (loss: 0.4033287465572357, acc: 0.914893627166748)
[2025-02-13 19:21:22,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:22,931][root][INFO] - Training Epoch: 1/2, step 346/7134 completed (loss: 0.2831243574619293, acc: 0.9342105388641357)
[2025-02-13 19:21:23,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:23,322][root][INFO] - Training Epoch: 1/2, step 347/7134 completed (loss: 0.5996924042701721, acc: 0.856249988079071)
[2025-02-13 19:21:23,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:23,718][root][INFO] - Training Epoch: 1/2, step 348/7134 completed (loss: 0.4131132960319519, acc: 0.8994082808494568)
[2025-02-13 19:21:23,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:24,150][root][INFO] - Training Epoch: 1/2, step 349/7134 completed (loss: 0.5526570677757263, acc: 0.88165682554245)
[2025-02-13 19:21:24,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:24,578][root][INFO] - Training Epoch: 1/2, step 350/7134 completed (loss: 0.45554319024086, acc: 0.8894736766815186)
[2025-02-13 19:21:24,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:24,997][root][INFO] - Training Epoch: 1/2, step 351/7134 completed (loss: 0.39022549986839294, acc: 0.929347813129425)
[2025-02-13 19:21:25,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:25,427][root][INFO] - Training Epoch: 1/2, step 352/7134 completed (loss: 0.37953364849090576, acc: 0.9415204524993896)
[2025-02-13 19:21:25,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:25,862][root][INFO] - Training Epoch: 1/2, step 353/7134 completed (loss: 0.5661580562591553, acc: 0.8711656332015991)
[2025-02-13 19:21:26,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:26,243][root][INFO] - Training Epoch: 1/2, step 354/7134 completed (loss: 0.4955503046512604, acc: 0.8743718862533569)
[2025-02-13 19:21:26,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:26,646][root][INFO] - Training Epoch: 1/2, step 355/7134 completed (loss: 0.4879060983657837, acc: 0.9053254723548889)
[2025-02-13 19:21:26,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:27,060][root][INFO] - Training Epoch: 1/2, step 356/7134 completed (loss: 0.38286489248275757, acc: 0.9041916131973267)
[2025-02-13 19:21:27,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:27,475][root][INFO] - Training Epoch: 1/2, step 357/7134 completed (loss: 0.31325456500053406, acc: 0.9011628031730652)
[2025-02-13 19:21:27,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:27,881][root][INFO] - Training Epoch: 1/2, step 358/7134 completed (loss: 0.32059165835380554, acc: 0.9261363744735718)
[2025-02-13 19:21:28,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:28,288][root][INFO] - Training Epoch: 1/2, step 359/7134 completed (loss: 0.5135911107063293, acc: 0.8882352709770203)
[2025-02-13 19:21:28,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:28,675][root][INFO] - Training Epoch: 1/2, step 360/7134 completed (loss: 0.2563535273075104, acc: 0.9281045794487)
[2025-02-13 19:21:28,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:29,083][root][INFO] - Training Epoch: 1/2, step 361/7134 completed (loss: 0.28483396768569946, acc: 0.9263803958892822)
[2025-02-13 19:21:29,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:29,483][root][INFO] - Training Epoch: 1/2, step 362/7134 completed (loss: 0.39499059319496155, acc: 0.8987341523170471)
[2025-02-13 19:21:29,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:29,901][root][INFO] - Training Epoch: 1/2, step 363/7134 completed (loss: 0.5291725993156433, acc: 0.8852459192276001)
[2025-02-13 19:21:30,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:30,350][root][INFO] - Training Epoch: 1/2, step 364/7134 completed (loss: 0.48359087109565735, acc: 0.8640776872634888)
[2025-02-13 19:21:30,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:30,784][root][INFO] - Training Epoch: 1/2, step 365/7134 completed (loss: 0.31953221559524536, acc: 0.9113923907279968)
[2025-02-13 19:21:30,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:31,207][root][INFO] - Training Epoch: 1/2, step 366/7134 completed (loss: 0.23404176533222198, acc: 0.9490445852279663)
[2025-02-13 19:21:31,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:31,677][root][INFO] - Training Epoch: 1/2, step 367/7134 completed (loss: 0.2942795157432556, acc: 0.9175257682800293)
[2025-02-13 19:21:31,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:32,079][root][INFO] - Training Epoch: 1/2, step 368/7134 completed (loss: 0.22848322987556458, acc: 0.9230769276618958)
[2025-02-13 19:21:32,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:32,477][root][INFO] - Training Epoch: 1/2, step 369/7134 completed (loss: 0.41420778632164, acc: 0.8895705342292786)
[2025-02-13 19:21:32,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:32,903][root][INFO] - Training Epoch: 1/2, step 370/7134 completed (loss: 0.4408978819847107, acc: 0.8860759735107422)
[2025-02-13 19:21:33,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:33,337][root][INFO] - Training Epoch: 1/2, step 371/7134 completed (loss: 0.32993078231811523, acc: 0.9192546606063843)
[2025-02-13 19:21:33,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:33,777][root][INFO] - Training Epoch: 1/2, step 372/7134 completed (loss: 0.42015746235847473, acc: 0.9245283007621765)
[2025-02-13 19:21:33,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:34,200][root][INFO] - Training Epoch: 1/2, step 373/7134 completed (loss: 0.30294305086135864, acc: 0.9215686321258545)
[2025-02-13 19:21:34,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:34,616][root][INFO] - Training Epoch: 1/2, step 374/7134 completed (loss: 0.31853845715522766, acc: 0.9189189076423645)
[2025-02-13 19:21:34,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:35,013][root][INFO] - Training Epoch: 1/2, step 375/7134 completed (loss: 0.47310081124305725, acc: 0.8829787373542786)
[2025-02-13 19:21:35,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:35,368][root][INFO] - Training Epoch: 1/2, step 376/7134 completed (loss: 0.6991567015647888, acc: 0.855555534362793)
[2025-02-13 19:21:35,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:35,783][root][INFO] - Training Epoch: 1/2, step 377/7134 completed (loss: 0.4915759563446045, acc: 0.8826815485954285)
[2025-02-13 19:21:35,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:36,167][root][INFO] - Training Epoch: 1/2, step 378/7134 completed (loss: 0.5382866263389587, acc: 0.8531073331832886)
[2025-02-13 19:21:36,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:36,582][root][INFO] - Training Epoch: 1/2, step 379/7134 completed (loss: 0.15166670083999634, acc: 0.970588207244873)
[2025-02-13 19:21:36,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:37,014][root][INFO] - Training Epoch: 1/2, step 380/7134 completed (loss: 0.7055771946907043, acc: 0.8350515365600586)
[2025-02-13 19:21:37,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:37,398][root][INFO] - Training Epoch: 1/2, step 381/7134 completed (loss: 0.36065223813056946, acc: 0.9315789341926575)
[2025-02-13 19:21:37,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:37,801][root][INFO] - Training Epoch: 1/2, step 382/7134 completed (loss: 0.32620587944984436, acc: 0.9128205180168152)
[2025-02-13 19:21:37,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:38,171][root][INFO] - Training Epoch: 1/2, step 383/7134 completed (loss: 0.3020874857902527, acc: 0.9379844665527344)
[2025-02-13 19:21:38,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:38,566][root][INFO] - Training Epoch: 1/2, step 384/7134 completed (loss: 0.3818246126174927, acc: 0.9447513818740845)
[2025-02-13 19:21:38,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:38,962][root][INFO] - Training Epoch: 1/2, step 385/7134 completed (loss: 0.2542096674442291, acc: 0.9346405267715454)
[2025-02-13 19:21:39,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:39,379][root][INFO] - Training Epoch: 1/2, step 386/7134 completed (loss: 0.3084651827812195, acc: 0.9369369149208069)
[2025-02-13 19:21:39,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:39,769][root][INFO] - Training Epoch: 1/2, step 387/7134 completed (loss: 0.300613135099411, acc: 0.8934911489486694)
[2025-02-13 19:21:39,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:40,132][root][INFO] - Training Epoch: 1/2, step 388/7134 completed (loss: 0.2610839903354645, acc: 0.9186602830886841)
[2025-02-13 19:21:40,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:40,536][root][INFO] - Training Epoch: 1/2, step 389/7134 completed (loss: 0.3025982081890106, acc: 0.9200000166893005)
[2025-02-13 19:21:40,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:40,947][root][INFO] - Training Epoch: 1/2, step 390/7134 completed (loss: 0.32026129961013794, acc: 0.912162184715271)
[2025-02-13 19:21:41,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:41,349][root][INFO] - Training Epoch: 1/2, step 391/7134 completed (loss: 0.2639274597167969, acc: 0.9200000166893005)
[2025-02-13 19:21:41,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:41,762][root][INFO] - Training Epoch: 1/2, step 392/7134 completed (loss: 0.2663424611091614, acc: 0.930232584476471)
[2025-02-13 19:21:41,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:42,146][root][INFO] - Training Epoch: 1/2, step 393/7134 completed (loss: 0.23064912855625153, acc: 0.9197860956192017)
[2025-02-13 19:21:42,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:42,510][root][INFO] - Training Epoch: 1/2, step 394/7134 completed (loss: 0.32826662063598633, acc: 0.9236640930175781)
[2025-02-13 19:21:42,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:42,881][root][INFO] - Training Epoch: 1/2, step 395/7134 completed (loss: 0.3398951292037964, acc: 0.936170220375061)
[2025-02-13 19:21:43,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:43,285][root][INFO] - Training Epoch: 1/2, step 396/7134 completed (loss: 0.2811499834060669, acc: 0.9137930870056152)
[2025-02-13 19:21:43,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:43,708][root][INFO] - Training Epoch: 1/2, step 397/7134 completed (loss: 0.48495930433273315, acc: 0.9130434989929199)
[2025-02-13 19:21:43,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:44,080][root][INFO] - Training Epoch: 1/2, step 398/7134 completed (loss: 0.3939830958843231, acc: 0.9040403962135315)
[2025-02-13 19:21:44,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:44,488][root][INFO] - Training Epoch: 1/2, step 399/7134 completed (loss: 0.3941807448863983, acc: 0.8860759735107422)
[2025-02-13 19:21:44,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:44,876][root][INFO] - Training Epoch: 1/2, step 400/7134 completed (loss: 0.6194868087768555, acc: 0.8734177350997925)
[2025-02-13 19:21:45,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:45,285][root][INFO] - Training Epoch: 1/2, step 401/7134 completed (loss: 0.4649272561073303, acc: 0.8725489974021912)
[2025-02-13 19:21:45,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:45,680][root][INFO] - Training Epoch: 1/2, step 402/7134 completed (loss: 0.39369815587997437, acc: 0.909547746181488)
[2025-02-13 19:21:45,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:46,104][root][INFO] - Training Epoch: 1/2, step 403/7134 completed (loss: 0.36998116970062256, acc: 0.903930127620697)
[2025-02-13 19:21:46,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:46,449][root][INFO] - Training Epoch: 1/2, step 404/7134 completed (loss: 0.40392303466796875, acc: 0.9032257795333862)
[2025-02-13 19:21:46,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:46,834][root][INFO] - Training Epoch: 1/2, step 405/7134 completed (loss: 0.6577260494232178, acc: 0.819767415523529)
[2025-02-13 19:21:46,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:47,247][root][INFO] - Training Epoch: 1/2, step 406/7134 completed (loss: 0.7626093626022339, acc: 0.837837815284729)
[2025-02-13 19:21:47,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:47,656][root][INFO] - Training Epoch: 1/2, step 407/7134 completed (loss: 0.8440098762512207, acc: 0.808917224407196)
[2025-02-13 19:21:47,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:48,049][root][INFO] - Training Epoch: 1/2, step 408/7134 completed (loss: 0.4813176095485687, acc: 0.8659217953681946)
[2025-02-13 19:21:48,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:48,473][root][INFO] - Training Epoch: 1/2, step 409/7134 completed (loss: 0.46066588163375854, acc: 0.8842105269432068)
[2025-02-13 19:21:48,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:48,923][root][INFO] - Training Epoch: 1/2, step 410/7134 completed (loss: 0.46214234828948975, acc: 0.8969696760177612)
[2025-02-13 19:21:49,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:49,323][root][INFO] - Training Epoch: 1/2, step 411/7134 completed (loss: 0.45198264718055725, acc: 0.9144144058227539)
[2025-02-13 19:21:49,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:49,697][root][INFO] - Training Epoch: 1/2, step 412/7134 completed (loss: 0.432922899723053, acc: 0.8834080696105957)
[2025-02-13 19:21:49,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:50,092][root][INFO] - Training Epoch: 1/2, step 413/7134 completed (loss: 0.48874425888061523, acc: 0.8691099286079407)
[2025-02-13 19:21:50,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:50,512][root][INFO] - Training Epoch: 1/2, step 414/7134 completed (loss: 0.44546979665756226, acc: 0.8728323578834534)
[2025-02-13 19:21:50,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:50,892][root][INFO] - Training Epoch: 1/2, step 415/7134 completed (loss: 0.32918795943260193, acc: 0.9095237851142883)
[2025-02-13 19:21:51,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:51,270][root][INFO] - Training Epoch: 1/2, step 416/7134 completed (loss: 0.22830723226070404, acc: 0.9471153616905212)
[2025-02-13 19:21:51,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:51,657][root][INFO] - Training Epoch: 1/2, step 417/7134 completed (loss: 0.5581299066543579, acc: 0.8516483306884766)
[2025-02-13 19:21:51,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:52,061][root][INFO] - Training Epoch: 1/2, step 418/7134 completed (loss: 0.33145609498023987, acc: 0.9275362491607666)
[2025-02-13 19:21:52,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:52,440][root][INFO] - Training Epoch: 1/2, step 419/7134 completed (loss: 0.39770209789276123, acc: 0.905940592288971)
[2025-02-13 19:21:52,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:52,860][root][INFO] - Training Epoch: 1/2, step 420/7134 completed (loss: 0.446768194437027, acc: 0.8857142925262451)
[2025-02-13 19:21:53,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:53,298][root][INFO] - Training Epoch: 1/2, step 421/7134 completed (loss: 0.3444127142429352, acc: 0.9056603908538818)
[2025-02-13 19:21:53,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:53,759][root][INFO] - Training Epoch: 1/2, step 422/7134 completed (loss: 0.3297082483768463, acc: 0.9171597361564636)
[2025-02-13 19:21:53,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:54,152][root][INFO] - Training Epoch: 1/2, step 423/7134 completed (loss: 0.1939970999956131, acc: 0.9606741666793823)
[2025-02-13 19:21:54,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:54,579][root][INFO] - Training Epoch: 1/2, step 424/7134 completed (loss: 0.399098664522171, acc: 0.9012345671653748)
[2025-02-13 19:21:54,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:55,003][root][INFO] - Training Epoch: 1/2, step 425/7134 completed (loss: 0.16403593122959137, acc: 0.9488636255264282)
[2025-02-13 19:21:55,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:55,404][root][INFO] - Training Epoch: 1/2, step 426/7134 completed (loss: 0.23092158138751984, acc: 0.9411764740943909)
[2025-02-13 19:21:55,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:55,805][root][INFO] - Training Epoch: 1/2, step 427/7134 completed (loss: 0.25229936838150024, acc: 0.9433962106704712)
[2025-02-13 19:21:55,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:56,241][root][INFO] - Training Epoch: 1/2, step 428/7134 completed (loss: 0.3646153509616852, acc: 0.916167676448822)
[2025-02-13 19:21:56,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:56,665][root][INFO] - Training Epoch: 1/2, step 429/7134 completed (loss: 0.4688758850097656, acc: 0.8815789222717285)
[2025-02-13 19:21:56,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:57,071][root][INFO] - Training Epoch: 1/2, step 430/7134 completed (loss: 0.24713529646396637, acc: 0.929411768913269)
[2025-02-13 19:21:57,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:57,448][root][INFO] - Training Epoch: 1/2, step 431/7134 completed (loss: 0.25466910004615784, acc: 0.9526627063751221)
[2025-02-13 19:21:57,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:57,866][root][INFO] - Training Epoch: 1/2, step 432/7134 completed (loss: 0.3426354229450226, acc: 0.9166666865348816)
[2025-02-13 19:21:58,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:58,269][root][INFO] - Training Epoch: 1/2, step 433/7134 completed (loss: 0.4199989438056946, acc: 0.9202127456665039)
[2025-02-13 19:21:58,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:58,704][root][INFO] - Training Epoch: 1/2, step 434/7134 completed (loss: 0.2716502547264099, acc: 0.9382715821266174)
[2025-02-13 19:21:58,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59,133][root][INFO] - Training Epoch: 1/2, step 435/7134 completed (loss: 0.22653408348560333, acc: 0.9388889074325562)
[2025-02-13 19:21:59,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59,538][root][INFO] - Training Epoch: 1/2, step 436/7134 completed (loss: 0.3306231200695038, acc: 0.9375)
[2025-02-13 19:21:59,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59,953][root][INFO] - Training Epoch: 1/2, step 437/7134 completed (loss: 0.3287029564380646, acc: 0.9295774698257446)
[2025-02-13 19:22:00,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:00,329][root][INFO] - Training Epoch: 1/2, step 438/7134 completed (loss: 0.11468101292848587, acc: 0.9685039520263672)
[2025-02-13 19:22:00,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:00,754][root][INFO] - Training Epoch: 1/2, step 439/7134 completed (loss: 0.4379507601261139, acc: 0.8908045887947083)
[2025-02-13 19:22:00,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:01,162][root][INFO] - Training Epoch: 1/2, step 440/7134 completed (loss: 0.49657681584358215, acc: 0.8846153616905212)
[2025-02-13 19:22:01,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:01,556][root][INFO] - Training Epoch: 1/2, step 441/7134 completed (loss: 0.18835079669952393, acc: 0.9589040875434875)
[2025-02-13 19:22:01,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:01,989][root][INFO] - Training Epoch: 1/2, step 442/7134 completed (loss: 0.11947913467884064, acc: 0.9689922332763672)
[2025-02-13 19:22:02,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:02,344][root][INFO] - Training Epoch: 1/2, step 443/7134 completed (loss: 0.13852019608020782, acc: 0.9659863710403442)
[2025-02-13 19:22:02,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:02,762][root][INFO] - Training Epoch: 1/2, step 444/7134 completed (loss: 0.10733714699745178, acc: 0.9767441749572754)
[2025-02-13 19:22:02,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:03,115][root][INFO] - Training Epoch: 1/2, step 445/7134 completed (loss: 0.48677119612693787, acc: 0.8805031180381775)
[2025-02-13 19:22:03,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:03,502][root][INFO] - Training Epoch: 1/2, step 446/7134 completed (loss: 0.2748541235923767, acc: 0.9171597361564636)
[2025-02-13 19:22:03,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:03,910][root][INFO] - Training Epoch: 1/2, step 447/7134 completed (loss: 0.19693943858146667, acc: 0.9447852969169617)
[2025-02-13 19:22:04,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:04,307][root][INFO] - Training Epoch: 1/2, step 448/7134 completed (loss: 0.6096428632736206, acc: 0.8888888955116272)
[2025-02-13 19:22:04,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:04,684][root][INFO] - Training Epoch: 1/2, step 449/7134 completed (loss: 0.6188408732414246, acc: 0.835106372833252)
[2025-02-13 19:22:04,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:05,079][root][INFO] - Training Epoch: 1/2, step 450/7134 completed (loss: 0.4709816575050354, acc: 0.8773584961891174)
[2025-02-13 19:22:05,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:05,476][root][INFO] - Training Epoch: 1/2, step 451/7134 completed (loss: 0.9429329037666321, acc: 0.8177965879440308)
[2025-02-13 19:22:05,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:05,853][root][INFO] - Training Epoch: 1/2, step 452/7134 completed (loss: 0.8375905156135559, acc: 0.8426966071128845)
[2025-02-13 19:22:05,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:06,241][root][INFO] - Training Epoch: 1/2, step 453/7134 completed (loss: 0.5282507538795471, acc: 0.8581560254096985)
[2025-02-13 19:22:06,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:06,623][root][INFO] - Training Epoch: 1/2, step 454/7134 completed (loss: 0.7466992139816284, acc: 0.8565400838851929)
[2025-02-13 19:22:06,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:07,013][root][INFO] - Training Epoch: 1/2, step 455/7134 completed (loss: 0.7756345272064209, acc: 0.800000011920929)
[2025-02-13 19:22:07,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:07,427][root][INFO] - Training Epoch: 1/2, step 456/7134 completed (loss: 0.44477179646492004, acc: 0.8895705342292786)
[2025-02-13 19:22:07,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:07,798][root][INFO] - Training Epoch: 1/2, step 457/7134 completed (loss: 0.7040966749191284, acc: 0.8965517282485962)
[2025-02-13 19:22:07,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:08,194][root][INFO] - Training Epoch: 1/2, step 458/7134 completed (loss: 0.44116005301475525, acc: 0.9032257795333862)
[2025-02-13 19:22:08,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:08,598][root][INFO] - Training Epoch: 1/2, step 459/7134 completed (loss: 0.6896243095397949, acc: 0.8253012299537659)
[2025-02-13 19:22:08,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:09,010][root][INFO] - Training Epoch: 1/2, step 460/7134 completed (loss: 0.5721475481987, acc: 0.8733333349227905)
[2025-02-13 19:22:09,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:09,411][root][INFO] - Training Epoch: 1/2, step 461/7134 completed (loss: 0.5030266642570496, acc: 0.8882352709770203)
[2025-02-13 19:22:09,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:09,803][root][INFO] - Training Epoch: 1/2, step 462/7134 completed (loss: 0.43511733412742615, acc: 0.8796296119689941)
[2025-02-13 19:22:10,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:10,224][root][INFO] - Training Epoch: 1/2, step 463/7134 completed (loss: 0.7303907871246338, acc: 0.8507462739944458)
[2025-02-13 19:22:10,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:10,628][root][INFO] - Training Epoch: 1/2, step 464/7134 completed (loss: 0.573192834854126, acc: 0.8693181872367859)
[2025-02-13 19:22:10,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:11,043][root][INFO] - Training Epoch: 1/2, step 465/7134 completed (loss: 0.46192944049835205, acc: 0.8938547372817993)
[2025-02-13 19:22:11,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:11,421][root][INFO] - Training Epoch: 1/2, step 466/7134 completed (loss: 0.5545563697814941, acc: 0.871345043182373)
[2025-02-13 19:22:11,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:11,826][root][INFO] - Training Epoch: 1/2, step 467/7134 completed (loss: 0.7120916843414307, acc: 0.8095238208770752)
[2025-02-13 19:22:11,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:12,216][root][INFO] - Training Epoch: 1/2, step 468/7134 completed (loss: 0.6805780529975891, acc: 0.8533333539962769)
[2025-02-13 19:22:12,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:12,590][root][INFO] - Training Epoch: 1/2, step 469/7134 completed (loss: 0.45442211627960205, acc: 0.8819444179534912)
[2025-02-13 19:22:12,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:12,938][root][INFO] - Training Epoch: 1/2, step 470/7134 completed (loss: 0.6887401342391968, acc: 0.837837815284729)
[2025-02-13 19:22:13,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:13,315][root][INFO] - Training Epoch: 1/2, step 471/7134 completed (loss: 0.6305826306343079, acc: 0.8406593203544617)
[2025-02-13 19:22:13,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:13,726][root][INFO] - Training Epoch: 1/2, step 472/7134 completed (loss: 0.6151256561279297, acc: 0.8733333349227905)
[2025-02-13 19:22:13,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:14,079][root][INFO] - Training Epoch: 1/2, step 473/7134 completed (loss: 0.5028643012046814, acc: 0.8409090638160706)
[2025-02-13 19:22:14,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:14,450][root][INFO] - Training Epoch: 1/2, step 474/7134 completed (loss: 0.6089457869529724, acc: 0.8484848737716675)
[2025-02-13 19:22:14,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:14,821][root][INFO] - Training Epoch: 1/2, step 475/7134 completed (loss: 0.8567658066749573, acc: 0.7972028255462646)
[2025-02-13 19:22:14,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:15,241][root][INFO] - Training Epoch: 1/2, step 476/7134 completed (loss: 0.8621028065681458, acc: 0.831250011920929)
[2025-02-13 19:22:15,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:15,630][root][INFO] - Training Epoch: 1/2, step 477/7134 completed (loss: 0.6636945605278015, acc: 0.8342245817184448)
[2025-02-13 19:22:15,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:16,027][root][INFO] - Training Epoch: 1/2, step 478/7134 completed (loss: 0.827393651008606, acc: 0.8173912763595581)
[2025-02-13 19:22:16,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:16,419][root][INFO] - Training Epoch: 1/2, step 479/7134 completed (loss: 0.6387744545936584, acc: 0.8476821184158325)
[2025-02-13 19:22:16,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:16,795][root][INFO] - Training Epoch: 1/2, step 480/7134 completed (loss: 0.5481510758399963, acc: 0.8687499761581421)
[2025-02-13 19:22:16,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:17,216][root][INFO] - Training Epoch: 1/2, step 481/7134 completed (loss: 0.4845999479293823, acc: 0.8944099545478821)
[2025-02-13 19:22:17,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:17,642][root][INFO] - Training Epoch: 1/2, step 482/7134 completed (loss: 0.3106658458709717, acc: 0.8993710875511169)
[2025-02-13 19:22:17,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:18,042][root][INFO] - Training Epoch: 1/2, step 483/7134 completed (loss: 0.42183631658554077, acc: 0.901098906993866)
[2025-02-13 19:22:18,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:18,441][root][INFO] - Training Epoch: 1/2, step 484/7134 completed (loss: 0.5512004494667053, acc: 0.8571428656578064)
[2025-02-13 19:22:18,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:18,819][root][INFO] - Training Epoch: 1/2, step 485/7134 completed (loss: 0.5851253271102905, acc: 0.9006211161613464)
[2025-02-13 19:22:18,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:19,180][root][INFO] - Training Epoch: 1/2, step 486/7134 completed (loss: 0.7565397024154663, acc: 0.8064516186714172)
[2025-02-13 19:22:19,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:19,622][root][INFO] - Training Epoch: 1/2, step 487/7134 completed (loss: 0.38123711943626404, acc: 0.9197530746459961)
[2025-02-13 19:22:19,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:20,016][root][INFO] - Training Epoch: 1/2, step 488/7134 completed (loss: 0.8178832530975342, acc: 0.8432835936546326)
[2025-02-13 19:22:20,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:20,411][root][INFO] - Training Epoch: 1/2, step 489/7134 completed (loss: 0.81777024269104, acc: 0.8633093237876892)
[2025-02-13 19:22:20,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:20,775][root][INFO] - Training Epoch: 1/2, step 490/7134 completed (loss: 0.5238394737243652, acc: 0.8860759735107422)
[2025-02-13 19:22:20,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:21,120][root][INFO] - Training Epoch: 1/2, step 491/7134 completed (loss: 0.39886701107025146, acc: 0.9027777910232544)
[2025-02-13 19:22:21,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:21,499][root][INFO] - Training Epoch: 1/2, step 492/7134 completed (loss: 0.16871020197868347, acc: 0.9677419066429138)
[2025-02-13 19:22:21,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:21,924][root][INFO] - Training Epoch: 1/2, step 493/7134 completed (loss: 0.4606621563434601, acc: 0.8994082808494568)
[2025-02-13 19:22:22,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:22,337][root][INFO] - Training Epoch: 1/2, step 494/7134 completed (loss: 0.2708034813404083, acc: 0.9466666579246521)
[2025-02-13 19:22:22,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:22,716][root][INFO] - Training Epoch: 1/2, step 495/7134 completed (loss: 0.36440348625183105, acc: 0.93388432264328)
[2025-02-13 19:22:22,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:23,139][root][INFO] - Training Epoch: 1/2, step 496/7134 completed (loss: 0.8829206824302673, acc: 0.8105263113975525)
[2025-02-13 19:22:23,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:23,552][root][INFO] - Training Epoch: 1/2, step 497/7134 completed (loss: 0.47024786472320557, acc: 0.892307698726654)
[2025-02-13 19:22:23,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:23,964][root][INFO] - Training Epoch: 1/2, step 498/7134 completed (loss: 0.4737303555011749, acc: 0.899328887462616)
[2025-02-13 19:22:24,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:24,413][root][INFO] - Training Epoch: 1/2, step 499/7134 completed (loss: 0.33813971281051636, acc: 0.8999999761581421)
[2025-02-13 19:22:24,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:24,817][root][INFO] - Training Epoch: 1/2, step 500/7134 completed (loss: 0.3606661558151245, acc: 0.9069767594337463)
[2025-02-13 19:22:25,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:25,243][root][INFO] - Training Epoch: 1/2, step 501/7134 completed (loss: 0.314119428396225, acc: 0.9306358098983765)
[2025-02-13 19:22:25,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:25,619][root][INFO] - Training Epoch: 1/2, step 502/7134 completed (loss: 0.2245440036058426, acc: 0.948051929473877)
[2025-02-13 19:22:25,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:25,971][root][INFO] - Training Epoch: 1/2, step 503/7134 completed (loss: 0.3298824727535248, acc: 0.9147727489471436)
[2025-02-13 19:22:26,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:26,358][root][INFO] - Training Epoch: 1/2, step 504/7134 completed (loss: 0.2561817765235901, acc: 0.9440993666648865)
[2025-02-13 19:22:26,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:26,773][root][INFO] - Training Epoch: 1/2, step 505/7134 completed (loss: 0.16727446019649506, acc: 0.9523809552192688)
[2025-02-13 19:22:26,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:27,190][root][INFO] - Training Epoch: 1/2, step 506/7134 completed (loss: 0.2878310978412628, acc: 0.9431818127632141)
[2025-02-13 19:22:27,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:27,601][root][INFO] - Training Epoch: 1/2, step 507/7134 completed (loss: 0.32910361886024475, acc: 0.9016393423080444)
[2025-02-13 19:22:27,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:28,005][root][INFO] - Training Epoch: 1/2, step 508/7134 completed (loss: 0.2971750497817993, acc: 0.9411764740943909)
[2025-02-13 19:22:28,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:28,440][root][INFO] - Training Epoch: 1/2, step 509/7134 completed (loss: 0.3725396692752838, acc: 0.9324324131011963)
[2025-02-13 19:22:28,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:28,846][root][INFO] - Training Epoch: 1/2, step 510/7134 completed (loss: 0.6865954399108887, acc: 0.8780487775802612)
[2025-02-13 19:22:28,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:29,239][root][INFO] - Training Epoch: 1/2, step 511/7134 completed (loss: 0.46250975131988525, acc: 0.8857142925262451)
[2025-02-13 19:22:29,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:29,618][root][INFO] - Training Epoch: 1/2, step 512/7134 completed (loss: 0.6813270449638367, acc: 0.8702290058135986)
[2025-02-13 19:22:29,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:30,006][root][INFO] - Training Epoch: 1/2, step 513/7134 completed (loss: 0.48570770025253296, acc: 0.9194630980491638)
[2025-02-13 19:22:30,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:30,402][root][INFO] - Training Epoch: 1/2, step 514/7134 completed (loss: 0.8352805972099304, acc: 0.8611111044883728)
[2025-02-13 19:22:30,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:30,790][root][INFO] - Training Epoch: 1/2, step 515/7134 completed (loss: 0.3464232087135315, acc: 0.9107142686843872)
[2025-02-13 19:22:30,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:31,199][root][INFO] - Training Epoch: 1/2, step 516/7134 completed (loss: 0.39692169427871704, acc: 0.9152542352676392)
[2025-02-13 19:22:31,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:31,578][root][INFO] - Training Epoch: 1/2, step 517/7134 completed (loss: 0.5166209936141968, acc: 0.9057971239089966)
[2025-02-13 19:22:31,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:31,974][root][INFO] - Training Epoch: 1/2, step 518/7134 completed (loss: 0.36226561665534973, acc: 0.9536423683166504)
[2025-02-13 19:22:32,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:32,387][root][INFO] - Training Epoch: 1/2, step 519/7134 completed (loss: 0.36459052562713623, acc: 0.9130434989929199)
[2025-02-13 19:22:32,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:32,760][root][INFO] - Training Epoch: 1/2, step 520/7134 completed (loss: 0.44138050079345703, acc: 0.8815789222717285)
[2025-02-13 19:22:32,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:33,155][root][INFO] - Training Epoch: 1/2, step 521/7134 completed (loss: 0.4084458351135254, acc: 0.9122806787490845)
[2025-02-13 19:22:33,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:33,568][root][INFO] - Training Epoch: 1/2, step 522/7134 completed (loss: 0.3097747564315796, acc: 0.9101123809814453)
[2025-02-13 19:22:33,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:33,991][root][INFO] - Training Epoch: 1/2, step 523/7134 completed (loss: 0.32207030057907104, acc: 0.9244186282157898)
[2025-02-13 19:22:34,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:34,387][root][INFO] - Training Epoch: 1/2, step 524/7134 completed (loss: 0.3177867531776428, acc: 0.9378530979156494)
[2025-02-13 19:22:34,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:34,845][root][INFO] - Training Epoch: 1/2, step 525/7134 completed (loss: 0.4383373260498047, acc: 0.9018405079841614)
[2025-02-13 19:22:34,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:35,243][root][INFO] - Training Epoch: 1/2, step 526/7134 completed (loss: 0.28579550981521606, acc: 0.9204545617103577)
[2025-02-13 19:22:35,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:35,620][root][INFO] - Training Epoch: 1/2, step 527/7134 completed (loss: 0.3523220419883728, acc: 0.9375)
[2025-02-13 19:22:35,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:35,993][root][INFO] - Training Epoch: 1/2, step 528/7134 completed (loss: 0.23213116824626923, acc: 0.9482758641242981)
[2025-02-13 19:22:36,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:36,407][root][INFO] - Training Epoch: 1/2, step 529/7134 completed (loss: 0.368621289730072, acc: 0.8980891704559326)
[2025-02-13 19:22:36,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:36,769][root][INFO] - Training Epoch: 1/2, step 530/7134 completed (loss: 0.3185199201107025, acc: 0.9307692050933838)
[2025-02-13 19:22:36,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:37,165][root][INFO] - Training Epoch: 1/2, step 531/7134 completed (loss: 0.5558915734291077, acc: 0.8835616707801819)
[2025-02-13 19:22:37,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:37,572][root][INFO] - Training Epoch: 1/2, step 532/7134 completed (loss: 0.22117699682712555, acc: 0.9734513163566589)
[2025-02-13 19:22:37,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:37,988][root][INFO] - Training Epoch: 1/2, step 533/7134 completed (loss: 0.6273732781410217, acc: 0.8503401279449463)
[2025-02-13 19:22:38,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:38,384][root][INFO] - Training Epoch: 1/2, step 534/7134 completed (loss: 0.260448157787323, acc: 0.9281437397003174)
[2025-02-13 19:22:38,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:38,819][root][INFO] - Training Epoch: 1/2, step 535/7134 completed (loss: 0.4434986114501953, acc: 0.8978102207183838)
[2025-02-13 19:22:38,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:39,238][root][INFO] - Training Epoch: 1/2, step 536/7134 completed (loss: 0.3175298869609833, acc: 0.9146341681480408)
[2025-02-13 19:22:39,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:39,643][root][INFO] - Training Epoch: 1/2, step 537/7134 completed (loss: 0.3875678479671478, acc: 0.884393036365509)
[2025-02-13 19:22:39,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:40,022][root][INFO] - Training Epoch: 1/2, step 538/7134 completed (loss: 0.2542734146118164, acc: 0.9457831382751465)
[2025-02-13 19:22:40,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:40,406][root][INFO] - Training Epoch: 1/2, step 539/7134 completed (loss: 0.467326283454895, acc: 0.8931297659873962)
[2025-02-13 19:22:40,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:40,758][root][INFO] - Training Epoch: 1/2, step 540/7134 completed (loss: 0.3956427574157715, acc: 0.9056603908538818)
[2025-02-13 19:22:40,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:41,144][root][INFO] - Training Epoch: 1/2, step 541/7134 completed (loss: 0.4881768524646759, acc: 0.8928571343421936)
[2025-02-13 19:22:41,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:41,514][root][INFO] - Training Epoch: 1/2, step 542/7134 completed (loss: 0.795041561126709, acc: 0.8100000023841858)
[2025-02-13 19:22:41,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:41,917][root][INFO] - Training Epoch: 1/2, step 543/7134 completed (loss: 0.7111327648162842, acc: 0.8157894611358643)
[2025-02-13 19:22:42,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:42,292][root][INFO] - Training Epoch: 1/2, step 544/7134 completed (loss: 0.7430999279022217, acc: 0.8357142806053162)
[2025-02-13 19:22:42,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:42,690][root][INFO] - Training Epoch: 1/2, step 545/7134 completed (loss: 0.7061095237731934, acc: 0.8045112490653992)
[2025-02-13 19:22:42,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:43,071][root][INFO] - Training Epoch: 1/2, step 546/7134 completed (loss: 1.010571002960205, acc: 0.7870370149612427)
[2025-02-13 19:22:43,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:43,458][root][INFO] - Training Epoch: 1/2, step 547/7134 completed (loss: 0.4993021488189697, acc: 0.8741722106933594)
[2025-02-13 19:22:43,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:43,845][root][INFO] - Training Epoch: 1/2, step 548/7134 completed (loss: 0.48609432578086853, acc: 0.9210526347160339)
[2025-02-13 19:22:43,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:44,234][root][INFO] - Training Epoch: 1/2, step 549/7134 completed (loss: 1.0273816585540771, acc: 0.7642857432365417)
[2025-02-13 19:22:44,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:44,624][root][INFO] - Training Epoch: 1/2, step 550/7134 completed (loss: 0.4881616234779358, acc: 0.8582677245140076)
[2025-02-13 19:22:44,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:45,012][root][INFO] - Training Epoch: 1/2, step 551/7134 completed (loss: 0.6349056959152222, acc: 0.8203125)
[2025-02-13 19:22:45,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:45,400][root][INFO] - Training Epoch: 1/2, step 552/7134 completed (loss: 0.4670916199684143, acc: 0.884353756904602)
[2025-02-13 19:22:45,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:45,814][root][INFO] - Training Epoch: 1/2, step 553/7134 completed (loss: 0.5176678895950317, acc: 0.8631578683853149)
[2025-02-13 19:22:45,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:46,259][root][INFO] - Training Epoch: 1/2, step 554/7134 completed (loss: 0.7407788038253784, acc: 0.8407643437385559)
[2025-02-13 19:22:46,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:46,641][root][INFO] - Training Epoch: 1/2, step 555/7134 completed (loss: 0.6858254671096802, acc: 0.8571428656578064)
[2025-02-13 19:22:46,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:47,008][root][INFO] - Training Epoch: 1/2, step 556/7134 completed (loss: 0.26744022965431213, acc: 0.9375)
[2025-02-13 19:22:47,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:47,414][root][INFO] - Training Epoch: 1/2, step 557/7134 completed (loss: 0.5913432240486145, acc: 0.8652482032775879)
[2025-02-13 19:22:47,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:47,819][root][INFO] - Training Epoch: 1/2, step 558/7134 completed (loss: 0.595709502696991, acc: 0.8782051205635071)
[2025-02-13 19:22:47,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:48,245][root][INFO] - Training Epoch: 1/2, step 559/7134 completed (loss: 0.609456479549408, acc: 0.8523489832878113)
[2025-02-13 19:22:48,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:48,637][root][INFO] - Training Epoch: 1/2, step 560/7134 completed (loss: 0.2712230086326599, acc: 0.9430894255638123)
[2025-02-13 19:22:48,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:49,067][root][INFO] - Training Epoch: 1/2, step 561/7134 completed (loss: 0.6202837228775024, acc: 0.893750011920929)
[2025-02-13 19:22:49,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:49,434][root][INFO] - Training Epoch: 1/2, step 562/7134 completed (loss: 0.5096132755279541, acc: 0.8571428656578064)
[2025-02-13 19:22:49,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:49,787][root][INFO] - Training Epoch: 1/2, step 563/7134 completed (loss: 0.9109007120132446, acc: 0.811188817024231)
[2025-02-13 19:22:49,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:50,188][root][INFO] - Training Epoch: 1/2, step 564/7134 completed (loss: 0.6195951104164124, acc: 0.847953200340271)
[2025-02-13 19:22:50,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:50,588][root][INFO] - Training Epoch: 1/2, step 565/7134 completed (loss: 0.5259114503860474, acc: 0.874015748500824)
[2025-02-13 19:22:50,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:50,954][root][INFO] - Training Epoch: 1/2, step 566/7134 completed (loss: 0.6720572113990784, acc: 0.7747747898101807)
[2025-02-13 19:22:51,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:51,381][root][INFO] - Training Epoch: 1/2, step 567/7134 completed (loss: 0.4740082025527954, acc: 0.8622754216194153)
[2025-02-13 19:22:51,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:51,752][root][INFO] - Training Epoch: 1/2, step 568/7134 completed (loss: 0.4284698963165283, acc: 0.8736842274665833)
[2025-02-13 19:22:51,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:52,168][root][INFO] - Training Epoch: 1/2, step 569/7134 completed (loss: 0.4444276988506317, acc: 0.9050279259681702)
[2025-02-13 19:22:52,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:52,547][root][INFO] - Training Epoch: 1/2, step 570/7134 completed (loss: 0.516741156578064, acc: 0.8656716346740723)
[2025-02-13 19:22:52,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:52,927][root][INFO] - Training Epoch: 1/2, step 571/7134 completed (loss: 0.2515791058540344, acc: 0.9285714030265808)
[2025-02-13 19:22:53,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:53,280][root][INFO] - Training Epoch: 1/2, step 572/7134 completed (loss: 0.25513124465942383, acc: 0.9314285516738892)
[2025-02-13 19:22:53,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:53,672][root][INFO] - Training Epoch: 1/2, step 573/7134 completed (loss: 0.42190417647361755, acc: 0.8848484754562378)
[2025-02-13 19:22:53,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:54,028][root][INFO] - Training Epoch: 1/2, step 574/7134 completed (loss: 0.3002602756023407, acc: 0.9408283829689026)
[2025-02-13 19:22:54,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:54,493][root][INFO] - Training Epoch: 1/2, step 575/7134 completed (loss: 0.39751774072647095, acc: 0.9122806787490845)
[2025-02-13 19:22:54,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:54,906][root][INFO] - Training Epoch: 1/2, step 576/7134 completed (loss: 0.2903331220149994, acc: 0.9247311949729919)
[2025-02-13 19:22:55,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:55,310][root][INFO] - Training Epoch: 1/2, step 577/7134 completed (loss: 0.48866111040115356, acc: 0.8969696760177612)
[2025-02-13 19:22:55,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:55,722][root][INFO] - Training Epoch: 1/2, step 578/7134 completed (loss: 0.3959231674671173, acc: 0.9004974961280823)
[2025-02-13 19:22:55,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:56,108][root][INFO] - Training Epoch: 1/2, step 579/7134 completed (loss: 0.4907323718070984, acc: 0.8817204236984253)
[2025-02-13 19:22:56,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:56,521][root][INFO] - Training Epoch: 1/2, step 580/7134 completed (loss: 0.731951117515564, acc: 0.8333333134651184)
[2025-02-13 19:22:56,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:56,916][root][INFO] - Training Epoch: 1/2, step 581/7134 completed (loss: 0.2733415365219116, acc: 0.9407894611358643)
[2025-02-13 19:22:57,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:57,287][root][INFO] - Training Epoch: 1/2, step 582/7134 completed (loss: 0.37228360772132874, acc: 0.91847825050354)
[2025-02-13 19:22:57,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:57,663][root][INFO] - Training Epoch: 1/2, step 583/7134 completed (loss: 0.3886251449584961, acc: 0.8982036113739014)
[2025-02-13 19:22:57,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:58,048][root][INFO] - Training Epoch: 1/2, step 584/7134 completed (loss: 0.2510021924972534, acc: 0.9344262480735779)
[2025-02-13 19:22:58,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:58,466][root][INFO] - Training Epoch: 1/2, step 585/7134 completed (loss: 0.29607346653938293, acc: 0.9351351261138916)
[2025-02-13 19:22:58,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:58,871][root][INFO] - Training Epoch: 1/2, step 586/7134 completed (loss: 0.2094608098268509, acc: 0.9308510422706604)
[2025-02-13 19:22:59,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:59,289][root][INFO] - Training Epoch: 1/2, step 587/7134 completed (loss: 0.32885581254959106, acc: 0.9116021990776062)
[2025-02-13 19:22:59,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:59,729][root][INFO] - Training Epoch: 1/2, step 588/7134 completed (loss: 0.3848939538002014, acc: 0.9054054021835327)
[2025-02-13 19:22:59,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:00,127][root][INFO] - Training Epoch: 1/2, step 589/7134 completed (loss: 0.3259648382663727, acc: 0.9141104221343994)
[2025-02-13 19:23:00,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:00,516][root][INFO] - Training Epoch: 1/2, step 590/7134 completed (loss: 0.2173253446817398, acc: 0.9470198750495911)
[2025-02-13 19:23:00,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:00,915][root][INFO] - Training Epoch: 1/2, step 591/7134 completed (loss: 0.26669368147850037, acc: 0.9491525292396545)
[2025-02-13 19:23:01,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:01,312][root][INFO] - Training Epoch: 1/2, step 592/7134 completed (loss: 0.2133440524339676, acc: 0.9602272510528564)
[2025-02-13 19:23:01,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:01,718][root][INFO] - Training Epoch: 1/2, step 593/7134 completed (loss: 0.3805578649044037, acc: 0.9239766001701355)
[2025-02-13 19:23:01,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:02,108][root][INFO] - Training Epoch: 1/2, step 594/7134 completed (loss: 0.49911680817604065, acc: 0.8982036113739014)
[2025-02-13 19:23:02,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:02,511][root][INFO] - Training Epoch: 1/2, step 595/7134 completed (loss: 0.32939404249191284, acc: 0.9301075339317322)
[2025-02-13 19:23:02,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:02,948][root][INFO] - Training Epoch: 1/2, step 596/7134 completed (loss: 0.40299728512763977, acc: 0.894444465637207)
[2025-02-13 19:23:03,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:03,334][root][INFO] - Training Epoch: 1/2, step 597/7134 completed (loss: 0.5139681696891785, acc: 0.8938547372817993)
[2025-02-13 19:23:03,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:03,713][root][INFO] - Training Epoch: 1/2, step 598/7134 completed (loss: 0.686338484287262, acc: 0.864130437374115)
[2025-02-13 19:23:03,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:04,138][root][INFO] - Training Epoch: 1/2, step 599/7134 completed (loss: 0.39168262481689453, acc: 0.9193548560142517)
[2025-02-13 19:23:04,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:04,516][root][INFO] - Training Epoch: 1/2, step 600/7134 completed (loss: 0.3629750609397888, acc: 0.9180327653884888)
[2025-02-13 19:23:04,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:04,908][root][INFO] - Training Epoch: 1/2, step 601/7134 completed (loss: 0.4758223593235016, acc: 0.8806818127632141)
[2025-02-13 19:23:05,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:05,276][root][INFO] - Training Epoch: 1/2, step 602/7134 completed (loss: 0.4163229465484619, acc: 0.9027026891708374)
[2025-02-13 19:23:05,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:05,647][root][INFO] - Training Epoch: 1/2, step 603/7134 completed (loss: 0.47898048162460327, acc: 0.920634925365448)
[2025-02-13 19:23:05,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:05,989][root][INFO] - Training Epoch: 1/2, step 604/7134 completed (loss: 0.4724452495574951, acc: 0.8848484754562378)
[2025-02-13 19:23:06,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:06,378][root][INFO] - Training Epoch: 1/2, step 605/7134 completed (loss: 0.3232417106628418, acc: 0.9304812550544739)
[2025-02-13 19:23:06,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:06,786][root][INFO] - Training Epoch: 1/2, step 606/7134 completed (loss: 0.37839648127555847, acc: 0.9049999713897705)
[2025-02-13 19:23:06,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:07,171][root][INFO] - Training Epoch: 1/2, step 607/7134 completed (loss: 0.3777395784854889, acc: 0.9367815852165222)
[2025-02-13 19:23:07,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:07,596][root][INFO] - Training Epoch: 1/2, step 608/7134 completed (loss: 0.3863612413406372, acc: 0.9027026891708374)
[2025-02-13 19:23:07,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:08,005][root][INFO] - Training Epoch: 1/2, step 609/7134 completed (loss: 0.43092894554138184, acc: 0.892307698726654)
[2025-02-13 19:23:08,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:08,422][root][INFO] - Training Epoch: 1/2, step 610/7134 completed (loss: 0.37946420907974243, acc: 0.9162561297416687)
[2025-02-13 19:23:08,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:08,818][root][INFO] - Training Epoch: 1/2, step 611/7134 completed (loss: 0.3334793746471405, acc: 0.9209039807319641)
[2025-02-13 19:23:09,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:09,257][root][INFO] - Training Epoch: 1/2, step 612/7134 completed (loss: 0.36842167377471924, acc: 0.9021739363670349)
[2025-02-13 19:23:09,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:09,677][root][INFO] - Training Epoch: 1/2, step 613/7134 completed (loss: 0.3523000478744507, acc: 0.9090909361839294)
[2025-02-13 19:23:09,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:10,097][root][INFO] - Training Epoch: 1/2, step 614/7134 completed (loss: 0.2607775628566742, acc: 0.9402173757553101)
[2025-02-13 19:23:10,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:10,491][root][INFO] - Training Epoch: 1/2, step 615/7134 completed (loss: 0.20099115371704102, acc: 0.942105233669281)
[2025-02-13 19:23:10,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:10,872][root][INFO] - Training Epoch: 1/2, step 616/7134 completed (loss: 0.3199051320552826, acc: 0.9308755993843079)
[2025-02-13 19:23:11,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:11,267][root][INFO] - Training Epoch: 1/2, step 617/7134 completed (loss: 0.3965905010700226, acc: 0.9090909361839294)
[2025-02-13 19:23:11,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:11,654][root][INFO] - Training Epoch: 1/2, step 618/7134 completed (loss: 0.2549063265323639, acc: 0.9251337051391602)
[2025-02-13 19:23:11,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:12,061][root][INFO] - Training Epoch: 1/2, step 619/7134 completed (loss: 0.27852991223335266, acc: 0.9426229596138)
[2025-02-13 19:23:12,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:12,481][root][INFO] - Training Epoch: 1/2, step 620/7134 completed (loss: 0.35547563433647156, acc: 0.931506872177124)
[2025-02-13 19:23:12,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:12,863][root][INFO] - Training Epoch: 1/2, step 621/7134 completed (loss: 0.3633665442466736, acc: 0.8933333158493042)
[2025-02-13 19:23:13,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:13,243][root][INFO] - Training Epoch: 1/2, step 622/7134 completed (loss: 0.49760231375694275, acc: 0.875)
[2025-02-13 19:23:13,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:13,644][root][INFO] - Training Epoch: 1/2, step 623/7134 completed (loss: 0.638227641582489, acc: 0.8776978254318237)
[2025-02-13 19:23:13,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:14,026][root][INFO] - Training Epoch: 1/2, step 624/7134 completed (loss: 0.32966798543930054, acc: 0.9343065619468689)
[2025-02-13 19:23:14,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:14,450][root][INFO] - Training Epoch: 1/2, step 625/7134 completed (loss: 0.3981419503688812, acc: 0.8999999761581421)
[2025-02-13 19:23:14,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:14,866][root][INFO] - Training Epoch: 1/2, step 626/7134 completed (loss: 0.4654375910758972, acc: 0.8960000276565552)
[2025-02-13 19:23:15,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:15,259][root][INFO] - Training Epoch: 1/2, step 627/7134 completed (loss: 0.3897542655467987, acc: 0.918367326259613)
[2025-02-13 19:23:15,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:15,662][root][INFO] - Training Epoch: 1/2, step 628/7134 completed (loss: 0.3365691304206848, acc: 0.9387755393981934)
[2025-02-13 19:23:15,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:16,091][root][INFO] - Training Epoch: 1/2, step 629/7134 completed (loss: 0.5649892687797546, acc: 0.8723404407501221)
[2025-02-13 19:23:16,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:16,477][root][INFO] - Training Epoch: 1/2, step 630/7134 completed (loss: 0.5733993053436279, acc: 0.8535031676292419)
[2025-02-13 19:23:16,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:16,856][root][INFO] - Training Epoch: 1/2, step 631/7134 completed (loss: 0.3669203817844391, acc: 0.9285714030265808)
[2025-02-13 19:23:17,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:17,296][root][INFO] - Training Epoch: 1/2, step 632/7134 completed (loss: 0.2640085518360138, acc: 0.9366196990013123)
[2025-02-13 19:23:17,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:17,695][root][INFO] - Training Epoch: 1/2, step 633/7134 completed (loss: 0.2977752089500427, acc: 0.918749988079071)
[2025-02-13 19:23:17,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:18,078][root][INFO] - Training Epoch: 1/2, step 634/7134 completed (loss: 0.3760603368282318, acc: 0.9220778942108154)
[2025-02-13 19:23:18,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:18,486][root][INFO] - Training Epoch: 1/2, step 635/7134 completed (loss: 0.3513469696044922, acc: 0.9155844449996948)
[2025-02-13 19:23:18,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:18,879][root][INFO] - Training Epoch: 1/2, step 636/7134 completed (loss: 0.31264764070510864, acc: 0.940119743347168)
[2025-02-13 19:23:19,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:19,303][root][INFO] - Training Epoch: 1/2, step 637/7134 completed (loss: 0.33783257007598877, acc: 0.9208633303642273)
[2025-02-13 19:23:19,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:19,731][root][INFO] - Training Epoch: 1/2, step 638/7134 completed (loss: 0.33140796422958374, acc: 0.926174521446228)
[2025-02-13 19:23:19,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:20,141][root][INFO] - Training Epoch: 1/2, step 639/7134 completed (loss: 0.3037574887275696, acc: 0.8999999761581421)
[2025-02-13 19:23:20,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:20,573][root][INFO] - Training Epoch: 1/2, step 640/7134 completed (loss: 0.24251048266887665, acc: 0.95652174949646)
[2025-02-13 19:23:20,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:20,977][root][INFO] - Training Epoch: 1/2, step 641/7134 completed (loss: 0.328497976064682, acc: 0.9130434989929199)
[2025-02-13 19:23:21,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:21,429][root][INFO] - Training Epoch: 1/2, step 642/7134 completed (loss: 0.23824669420719147, acc: 0.9411764740943909)
[2025-02-13 19:23:21,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:21,858][root][INFO] - Training Epoch: 1/2, step 643/7134 completed (loss: 0.25018224120140076, acc: 0.932584285736084)
[2025-02-13 19:23:22,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:22,305][root][INFO] - Training Epoch: 1/2, step 644/7134 completed (loss: 0.1580594778060913, acc: 0.96875)
[2025-02-13 19:23:22,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:22,693][root][INFO] - Training Epoch: 1/2, step 645/7134 completed (loss: 0.10814044624567032, acc: 0.9655172228813171)
[2025-02-13 19:23:22,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:23,057][root][INFO] - Training Epoch: 1/2, step 646/7134 completed (loss: 0.22572973370552063, acc: 0.9367088675498962)
[2025-02-13 19:23:23,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:23,454][root][INFO] - Training Epoch: 1/2, step 647/7134 completed (loss: 0.14107073843479156, acc: 0.970370352268219)
[2025-02-13 19:23:23,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:23,853][root][INFO] - Training Epoch: 1/2, step 648/7134 completed (loss: 0.6223656535148621, acc: 0.8461538553237915)
[2025-02-13 19:23:24,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:24,246][root][INFO] - Training Epoch: 1/2, step 649/7134 completed (loss: 0.5589879155158997, acc: 0.8702702522277832)
[2025-02-13 19:23:24,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:24,638][root][INFO] - Training Epoch: 1/2, step 650/7134 completed (loss: 0.6088932752609253, acc: 0.8561643958091736)
[2025-02-13 19:23:24,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:25,020][root][INFO] - Training Epoch: 1/2, step 651/7134 completed (loss: 0.5219681859016418, acc: 0.8636363744735718)
[2025-02-13 19:23:25,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:25,408][root][INFO] - Training Epoch: 1/2, step 652/7134 completed (loss: 0.46889808773994446, acc: 0.8873239159584045)
[2025-02-13 19:23:25,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:25,779][root][INFO] - Training Epoch: 1/2, step 653/7134 completed (loss: 0.24510793387889862, acc: 0.9454545378684998)
[2025-02-13 19:23:25,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:26,142][root][INFO] - Training Epoch: 1/2, step 654/7134 completed (loss: 0.5785871148109436, acc: 0.8646616339683533)
[2025-02-13 19:23:26,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:26,567][root][INFO] - Training Epoch: 1/2, step 655/7134 completed (loss: 0.29398563504219055, acc: 0.9137930870056152)
[2025-02-13 19:23:26,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:26,971][root][INFO] - Training Epoch: 1/2, step 656/7134 completed (loss: 0.4677160382270813, acc: 0.8787878751754761)
[2025-02-13 19:23:27,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:27,352][root][INFO] - Training Epoch: 1/2, step 657/7134 completed (loss: 0.45432141423225403, acc: 0.8802816867828369)
[2025-02-13 19:23:27,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:27,735][root][INFO] - Training Epoch: 1/2, step 658/7134 completed (loss: 0.19544358551502228, acc: 0.9548022747039795)
[2025-02-13 19:23:27,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:28,114][root][INFO] - Training Epoch: 1/2, step 659/7134 completed (loss: 0.1916297823190689, acc: 0.9640718698501587)
[2025-02-13 19:23:28,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:28,498][root][INFO] - Training Epoch: 1/2, step 660/7134 completed (loss: 0.3516520857810974, acc: 0.8999999761581421)
[2025-02-13 19:23:28,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:28,880][root][INFO] - Training Epoch: 1/2, step 661/7134 completed (loss: 0.3071959912776947, acc: 0.9279661178588867)
[2025-02-13 19:23:29,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:29,283][root][INFO] - Training Epoch: 1/2, step 662/7134 completed (loss: 0.3271746039390564, acc: 0.9388889074325562)
[2025-02-13 19:23:29,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:29,692][root][INFO] - Training Epoch: 1/2, step 663/7134 completed (loss: 0.24612760543823242, acc: 0.9368932247161865)
[2025-02-13 19:23:29,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:30,141][root][INFO] - Training Epoch: 1/2, step 664/7134 completed (loss: 0.3274783790111542, acc: 0.9300699234008789)
[2025-02-13 19:23:30,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:30,510][root][INFO] - Training Epoch: 1/2, step 665/7134 completed (loss: 0.5932170152664185, acc: 0.8907103538513184)
[2025-02-13 19:23:30,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:30,901][root][INFO] - Training Epoch: 1/2, step 666/7134 completed (loss: 0.3517271876335144, acc: 0.9314285516738892)
[2025-02-13 19:23:31,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:31,317][root][INFO] - Training Epoch: 1/2, step 667/7134 completed (loss: 0.4209045469760895, acc: 0.8982036113739014)
[2025-02-13 19:23:31,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:31,698][root][INFO] - Training Epoch: 1/2, step 668/7134 completed (loss: 0.16474513709545135, acc: 0.9617486596107483)
[2025-02-13 19:23:31,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:32,083][root][INFO] - Training Epoch: 1/2, step 669/7134 completed (loss: 0.34097009897232056, acc: 0.916201114654541)
[2025-02-13 19:23:32,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:32,532][root][INFO] - Training Epoch: 1/2, step 670/7134 completed (loss: 0.29031726717948914, acc: 0.931506872177124)
[2025-02-13 19:23:32,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:32,932][root][INFO] - Training Epoch: 1/2, step 671/7134 completed (loss: 0.29424813389778137, acc: 0.9049773812294006)
[2025-02-13 19:23:33,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:33,313][root][INFO] - Training Epoch: 1/2, step 672/7134 completed (loss: 0.35064777731895447, acc: 0.9312499761581421)
[2025-02-13 19:23:33,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:33,681][root][INFO] - Training Epoch: 1/2, step 673/7134 completed (loss: 0.3422084450721741, acc: 0.9353233575820923)
[2025-02-13 19:23:33,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:34,051][root][INFO] - Training Epoch: 1/2, step 674/7134 completed (loss: 0.23978987336158752, acc: 0.9512194991111755)
[2025-02-13 19:23:34,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:34,466][root][INFO] - Training Epoch: 1/2, step 675/7134 completed (loss: 0.24479669332504272, acc: 0.939226508140564)
[2025-02-13 19:23:34,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:34,891][root][INFO] - Training Epoch: 1/2, step 676/7134 completed (loss: 0.18505164980888367, acc: 0.9619565010070801)
[2025-02-13 19:23:35,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:35,310][root][INFO] - Training Epoch: 1/2, step 677/7134 completed (loss: 0.13580329716205597, acc: 0.9735449552536011)
[2025-02-13 19:23:35,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:35,707][root][INFO] - Training Epoch: 1/2, step 678/7134 completed (loss: 0.42868852615356445, acc: 0.9207317233085632)
[2025-02-13 19:23:35,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:36,081][root][INFO] - Training Epoch: 1/2, step 679/7134 completed (loss: 0.34067943692207336, acc: 0.932330846786499)
[2025-02-13 19:23:36,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:36,545][root][INFO] - Training Epoch: 1/2, step 680/7134 completed (loss: 0.38933876156806946, acc: 0.909547746181488)
[2025-02-13 19:23:36,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:36,999][root][INFO] - Training Epoch: 1/2, step 681/7134 completed (loss: 0.33343741297721863, acc: 0.9052631855010986)
[2025-02-13 19:23:37,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:37,462][root][INFO] - Training Epoch: 1/2, step 682/7134 completed (loss: 0.2955041527748108, acc: 0.9322034120559692)
[2025-02-13 19:23:37,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:37,924][root][INFO] - Training Epoch: 1/2, step 683/7134 completed (loss: 0.3313806354999542, acc: 0.9189189076423645)
[2025-02-13 19:23:38,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:38,390][root][INFO] - Training Epoch: 1/2, step 684/7134 completed (loss: 0.378589928150177, acc: 0.9111111164093018)
[2025-02-13 19:23:38,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:38,796][root][INFO] - Training Epoch: 1/2, step 685/7134 completed (loss: 0.27192020416259766, acc: 0.909547746181488)
[2025-02-13 19:23:38,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:39,203][root][INFO] - Training Epoch: 1/2, step 686/7134 completed (loss: 0.15418685972690582, acc: 0.9562841653823853)
[2025-02-13 19:23:39,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:39,612][root][INFO] - Training Epoch: 1/2, step 687/7134 completed (loss: 0.23024915158748627, acc: 0.913385808467865)
[2025-02-13 19:23:39,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:40,040][root][INFO] - Training Epoch: 1/2, step 688/7134 completed (loss: 0.15398606657981873, acc: 0.949367105960846)
[2025-02-13 19:23:40,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:40,425][root][INFO] - Training Epoch: 1/2, step 689/7134 completed (loss: 0.17108973860740662, acc: 0.9555555582046509)
[2025-02-13 19:23:40,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:40,817][root][INFO] - Training Epoch: 1/2, step 690/7134 completed (loss: 0.45162221789360046, acc: 0.9096774458885193)
[2025-02-13 19:23:40,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:41,223][root][INFO] - Training Epoch: 1/2, step 691/7134 completed (loss: 0.5041420459747314, acc: 0.8766233921051025)
[2025-02-13 19:23:41,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:41,614][root][INFO] - Training Epoch: 1/2, step 692/7134 completed (loss: 0.7233702540397644, acc: 0.8629441857337952)
[2025-02-13 19:23:41,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:42,014][root][INFO] - Training Epoch: 1/2, step 693/7134 completed (loss: 0.5050904154777527, acc: 0.8581560254096985)
[2025-02-13 19:23:42,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:42,448][root][INFO] - Training Epoch: 1/2, step 694/7134 completed (loss: 0.6448784470558167, acc: 0.8484848737716675)
[2025-02-13 19:23:42,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:42,866][root][INFO] - Training Epoch: 1/2, step 695/7134 completed (loss: 0.6768216490745544, acc: 0.8208954930305481)
[2025-02-13 19:23:43,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:43,316][root][INFO] - Training Epoch: 1/2, step 696/7134 completed (loss: 0.7119496464729309, acc: 0.8427672982215881)
[2025-02-13 19:23:43,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:43,710][root][INFO] - Training Epoch: 1/2, step 697/7134 completed (loss: 0.45016708970069885, acc: 0.8804348111152649)
[2025-02-13 19:23:43,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:44,108][root][INFO] - Training Epoch: 1/2, step 698/7134 completed (loss: 0.40669015049934387, acc: 0.9318181872367859)
[2025-02-13 19:23:44,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:44,557][root][INFO] - Training Epoch: 1/2, step 699/7134 completed (loss: 0.2088543027639389, acc: 0.9589040875434875)
[2025-02-13 19:23:44,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:44,959][root][INFO] - Training Epoch: 1/2, step 700/7134 completed (loss: 0.5718870759010315, acc: 0.8240000009536743)
[2025-02-13 19:23:45,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:45,346][root][INFO] - Training Epoch: 1/2, step 701/7134 completed (loss: 0.6554403305053711, acc: 0.8260869383811951)
[2025-02-13 19:23:45,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:45,768][root][INFO] - Training Epoch: 1/2, step 702/7134 completed (loss: 0.5408781170845032, acc: 0.8476821184158325)
[2025-02-13 19:23:45,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:46,187][root][INFO] - Training Epoch: 1/2, step 703/7134 completed (loss: 0.5635074377059937, acc: 0.8581081032752991)
[2025-02-13 19:23:46,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:46,622][root][INFO] - Training Epoch: 1/2, step 704/7134 completed (loss: 0.48282793164253235, acc: 0.9025974273681641)
[2025-02-13 19:23:46,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:47,065][root][INFO] - Training Epoch: 1/2, step 705/7134 completed (loss: 0.5289908647537231, acc: 0.857798159122467)
[2025-02-13 19:23:47,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:47,474][root][INFO] - Training Epoch: 1/2, step 706/7134 completed (loss: 0.33597707748413086, acc: 0.9103773832321167)
[2025-02-13 19:23:47,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:47,873][root][INFO] - Training Epoch: 1/2, step 707/7134 completed (loss: 0.37260743975639343, acc: 0.9009901285171509)
[2025-02-13 19:23:48,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:48,260][root][INFO] - Training Epoch: 1/2, step 708/7134 completed (loss: 0.28181642293930054, acc: 0.932584285736084)
[2025-02-13 19:23:48,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:48,639][root][INFO] - Training Epoch: 1/2, step 709/7134 completed (loss: 0.3833644688129425, acc: 0.9085714221000671)
[2025-02-13 19:23:48,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:49,048][root][INFO] - Training Epoch: 1/2, step 710/7134 completed (loss: 0.27855509519577026, acc: 0.9114583134651184)
[2025-02-13 19:23:49,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:49,516][root][INFO] - Training Epoch: 1/2, step 711/7134 completed (loss: 0.3022271394729614, acc: 0.9313725233078003)
[2025-02-13 19:23:49,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:49,895][root][INFO] - Training Epoch: 1/2, step 712/7134 completed (loss: 0.18455927073955536, acc: 0.96875)
[2025-02-13 19:23:50,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:50,283][root][INFO] - Training Epoch: 1/2, step 713/7134 completed (loss: 0.38058072328567505, acc: 0.929648220539093)
[2025-02-13 19:23:50,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:50,666][root][INFO] - Training Epoch: 1/2, step 714/7134 completed (loss: 0.35397395491600037, acc: 0.9200000166893005)
[2025-02-13 19:23:50,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:51,107][root][INFO] - Training Epoch: 1/2, step 715/7134 completed (loss: 0.13864651322364807, acc: 0.967391312122345)
[2025-02-13 19:23:51,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:51,484][root][INFO] - Training Epoch: 1/2, step 716/7134 completed (loss: 0.2106875479221344, acc: 0.9558823704719543)
[2025-02-13 19:23:51,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:51,852][root][INFO] - Training Epoch: 1/2, step 717/7134 completed (loss: 0.27227258682250977, acc: 0.9358288645744324)
[2025-02-13 19:23:52,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:52,253][root][INFO] - Training Epoch: 1/2, step 718/7134 completed (loss: 0.20424418151378632, acc: 0.9312169551849365)
[2025-02-13 19:23:52,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:52,685][root][INFO] - Training Epoch: 1/2, step 719/7134 completed (loss: 0.18699133396148682, acc: 0.957446813583374)
[2025-02-13 19:23:52,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:53,073][root][INFO] - Training Epoch: 1/2, step 720/7134 completed (loss: 0.20950505137443542, acc: 0.9405405521392822)
[2025-02-13 19:23:53,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:53,461][root][INFO] - Training Epoch: 1/2, step 721/7134 completed (loss: 0.2035416215658188, acc: 0.942105233669281)
[2025-02-13 19:23:53,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:53,884][root][INFO] - Training Epoch: 1/2, step 722/7134 completed (loss: 0.3119641840457916, acc: 0.9141414165496826)
[2025-02-13 19:23:54,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:54,303][root][INFO] - Training Epoch: 1/2, step 723/7134 completed (loss: 0.1669599413871765, acc: 0.945652186870575)
[2025-02-13 19:23:54,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:54,730][root][INFO] - Training Epoch: 1/2, step 724/7134 completed (loss: 0.32920053601264954, acc: 0.9056603908538818)
[2025-02-13 19:23:54,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:55,137][root][INFO] - Training Epoch: 1/2, step 725/7134 completed (loss: 0.1462378203868866, acc: 0.976190447807312)
[2025-02-13 19:23:55,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:55,543][root][INFO] - Training Epoch: 1/2, step 726/7134 completed (loss: 0.25013983249664307, acc: 0.9553072452545166)
[2025-02-13 19:23:55,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:55,960][root][INFO] - Training Epoch: 1/2, step 727/7134 completed (loss: 0.3482309877872467, acc: 0.9306358098983765)
[2025-02-13 19:23:56,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:56,373][root][INFO] - Training Epoch: 1/2, step 728/7134 completed (loss: 0.1390780806541443, acc: 0.9599999785423279)
[2025-02-13 19:23:56,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:56,793][root][INFO] - Training Epoch: 1/2, step 729/7134 completed (loss: 0.19449667632579803, acc: 0.9620253443717957)
[2025-02-13 19:23:56,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:57,203][root][INFO] - Training Epoch: 1/2, step 730/7134 completed (loss: 0.11420333385467529, acc: 0.978723406791687)
[2025-02-13 19:23:57,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:57,592][root][INFO] - Training Epoch: 1/2, step 731/7134 completed (loss: 0.09651093930006027, acc: 0.9717513918876648)
[2025-02-13 19:23:57,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:58,015][root][INFO] - Training Epoch: 1/2, step 732/7134 completed (loss: 0.45197054743766785, acc: 0.8770492076873779)
[2025-02-13 19:23:58,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:58,475][root][INFO] - Training Epoch: 1/2, step 733/7134 completed (loss: 0.19650278985500336, acc: 0.9461538195610046)
[2025-02-13 19:23:58,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:58,867][root][INFO] - Training Epoch: 1/2, step 734/7134 completed (loss: 0.31488391757011414, acc: 0.9274193644523621)
[2025-02-13 19:23:59,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:59,242][root][INFO] - Training Epoch: 1/2, step 735/7134 completed (loss: 0.34956398606300354, acc: 0.922535240650177)
[2025-02-13 19:23:59,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:59,667][root][INFO] - Training Epoch: 1/2, step 736/7134 completed (loss: 0.707673192024231, acc: 0.8514851331710815)
[2025-02-13 19:23:59,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:00,065][root][INFO] - Training Epoch: 1/2, step 737/7134 completed (loss: 0.4447023570537567, acc: 0.8898305296897888)
[2025-02-13 19:24:00,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:00,459][root][INFO] - Training Epoch: 1/2, step 738/7134 completed (loss: 0.23411080241203308, acc: 0.9622641801834106)
[2025-02-13 19:24:00,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:00,861][root][INFO] - Training Epoch: 1/2, step 739/7134 completed (loss: 0.2840493619441986, acc: 0.939130425453186)
[2025-02-13 19:24:00,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:01,254][root][INFO] - Training Epoch: 1/2, step 740/7134 completed (loss: 0.23033128678798676, acc: 0.9186992049217224)
[2025-02-13 19:24:01,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:01,631][root][INFO] - Training Epoch: 1/2, step 741/7134 completed (loss: 0.3191992938518524, acc: 0.8962963223457336)
[2025-02-13 19:24:01,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:02,028][root][INFO] - Training Epoch: 1/2, step 742/7134 completed (loss: 0.2529509961605072, acc: 0.9368420839309692)
[2025-02-13 19:24:02,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:02,460][root][INFO] - Training Epoch: 1/2, step 743/7134 completed (loss: 0.5199211239814758, acc: 0.8911564350128174)
[2025-02-13 19:24:02,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:02,903][root][INFO] - Training Epoch: 1/2, step 744/7134 completed (loss: 0.2581205666065216, acc: 0.9399999976158142)
[2025-02-13 19:24:03,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:03,304][root][INFO] - Training Epoch: 1/2, step 745/7134 completed (loss: 0.20460520684719086, acc: 0.9411764740943909)
[2025-02-13 19:24:03,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:03,726][root][INFO] - Training Epoch: 1/2, step 746/7134 completed (loss: 0.33902305364608765, acc: 0.9083969593048096)
[2025-02-13 19:24:03,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:04,152][root][INFO] - Training Epoch: 1/2, step 747/7134 completed (loss: 0.47319093346595764, acc: 0.8818897604942322)
[2025-02-13 19:24:04,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:04,630][root][INFO] - Training Epoch: 1/2, step 748/7134 completed (loss: 0.34282514452934265, acc: 0.9029850959777832)
[2025-02-13 19:24:04,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:05,068][root][INFO] - Training Epoch: 1/2, step 749/7134 completed (loss: 0.26056408882141113, acc: 0.9285714030265808)
[2025-02-13 19:24:05,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:05,543][root][INFO] - Training Epoch: 1/2, step 750/7134 completed (loss: 0.3282610774040222, acc: 0.93388432264328)
[2025-02-13 19:24:05,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:05,948][root][INFO] - Training Epoch: 1/2, step 751/7134 completed (loss: 0.5017114281654358, acc: 0.8880000114440918)
[2025-02-13 19:24:06,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:06,379][root][INFO] - Training Epoch: 1/2, step 752/7134 completed (loss: 0.6061716079711914, acc: 0.8918918967247009)
[2025-02-13 19:24:06,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:06,769][root][INFO] - Training Epoch: 1/2, step 753/7134 completed (loss: 0.339893102645874, acc: 0.90625)
[2025-02-13 19:24:06,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:07,153][root][INFO] - Training Epoch: 1/2, step 754/7134 completed (loss: 0.29946014285087585, acc: 0.9420289993286133)
[2025-02-13 19:24:07,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:07,556][root][INFO] - Training Epoch: 1/2, step 755/7134 completed (loss: 0.6017407178878784, acc: 0.8880000114440918)
[2025-02-13 19:24:07,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:07,938][root][INFO] - Training Epoch: 1/2, step 756/7134 completed (loss: 0.27223876118659973, acc: 0.9196428656578064)
[2025-02-13 19:24:08,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:08,403][root][INFO] - Training Epoch: 1/2, step 757/7134 completed (loss: 0.16023680567741394, acc: 0.9807692170143127)
[2025-02-13 19:24:08,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:08,831][root][INFO] - Training Epoch: 1/2, step 758/7134 completed (loss: 0.18871158361434937, acc: 0.954954981803894)
[2025-02-13 19:24:08,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:09,261][root][INFO] - Training Epoch: 1/2, step 759/7134 completed (loss: 0.23327867686748505, acc: 0.9459459185600281)
[2025-02-13 19:24:09,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:09,660][root][INFO] - Training Epoch: 1/2, step 760/7134 completed (loss: 0.28612589836120605, acc: 0.9090909361839294)
[2025-02-13 19:24:09,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:10,099][root][INFO] - Training Epoch: 1/2, step 761/7134 completed (loss: 0.3097420334815979, acc: 0.916167676448822)
[2025-02-13 19:24:10,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:10,529][root][INFO] - Training Epoch: 1/2, step 762/7134 completed (loss: 0.2909093499183655, acc: 0.9275362491607666)
[2025-02-13 19:24:10,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:10,904][root][INFO] - Training Epoch: 1/2, step 763/7134 completed (loss: 0.3867792785167694, acc: 0.875)
[2025-02-13 19:24:11,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:11,275][root][INFO] - Training Epoch: 1/2, step 764/7134 completed (loss: 0.1743737906217575, acc: 0.9644970297813416)
[2025-02-13 19:24:11,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:11,670][root][INFO] - Training Epoch: 1/2, step 765/7134 completed (loss: 0.1730467975139618, acc: 0.955974817276001)
[2025-02-13 19:24:11,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:12,124][root][INFO] - Training Epoch: 1/2, step 766/7134 completed (loss: 0.12374432384967804, acc: 0.9750000238418579)
[2025-02-13 19:24:12,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:12,512][root][INFO] - Training Epoch: 1/2, step 767/7134 completed (loss: 0.30401915311813354, acc: 0.9107142686843872)
[2025-02-13 19:24:12,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:12,917][root][INFO] - Training Epoch: 1/2, step 768/7134 completed (loss: 0.465833455324173, acc: 0.9071038365364075)
[2025-02-13 19:24:13,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:13,337][root][INFO] - Training Epoch: 1/2, step 769/7134 completed (loss: 0.21855415403842926, acc: 0.9440993666648865)
[2025-02-13 19:24:13,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:13,714][root][INFO] - Training Epoch: 1/2, step 770/7134 completed (loss: 0.3420630693435669, acc: 0.9135802388191223)
[2025-02-13 19:24:13,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:14,098][root][INFO] - Training Epoch: 1/2, step 771/7134 completed (loss: 0.20081321895122528, acc: 0.9416058659553528)
[2025-02-13 19:24:14,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:14,464][root][INFO] - Training Epoch: 1/2, step 772/7134 completed (loss: 0.27458465099334717, acc: 0.9047619104385376)
[2025-02-13 19:24:14,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:14,914][root][INFO] - Training Epoch: 1/2, step 773/7134 completed (loss: 0.38519543409347534, acc: 0.9157894849777222)
[2025-02-13 19:24:15,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:15,317][root][INFO] - Training Epoch: 1/2, step 774/7134 completed (loss: 0.250773161649704, acc: 0.9577465057373047)
[2025-02-13 19:24:15,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:15,733][root][INFO] - Training Epoch: 1/2, step 775/7134 completed (loss: 0.14715604484081268, acc: 0.9627659320831299)
[2025-02-13 19:24:15,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:16,159][root][INFO] - Training Epoch: 1/2, step 776/7134 completed (loss: 0.13470207154750824, acc: 0.9681528806686401)
[2025-02-13 19:24:16,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:16,556][root][INFO] - Training Epoch: 1/2, step 777/7134 completed (loss: 0.2107912003993988, acc: 0.9333333373069763)
[2025-02-13 19:24:16,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:16,982][root][INFO] - Training Epoch: 1/2, step 778/7134 completed (loss: 0.20721781253814697, acc: 0.955974817276001)
[2025-02-13 19:24:17,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:17,365][root][INFO] - Training Epoch: 1/2, step 779/7134 completed (loss: 0.18797621130943298, acc: 0.9415204524993896)
[2025-02-13 19:24:17,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:17,858][root][INFO] - Training Epoch: 1/2, step 780/7134 completed (loss: 0.29843804240226746, acc: 0.908108115196228)
[2025-02-13 19:24:18,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:18,263][root][INFO] - Training Epoch: 1/2, step 781/7134 completed (loss: 0.2800408601760864, acc: 0.9411764740943909)
[2025-02-13 19:24:18,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:18,613][root][INFO] - Training Epoch: 1/2, step 782/7134 completed (loss: 0.3150520920753479, acc: 0.9367088675498962)
[2025-02-13 19:24:18,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:18,993][root][INFO] - Training Epoch: 1/2, step 783/7134 completed (loss: 0.30905881524086, acc: 0.9299065470695496)
[2025-02-13 19:24:19,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:19,365][root][INFO] - Training Epoch: 1/2, step 784/7134 completed (loss: 0.25881192088127136, acc: 0.9452736377716064)
[2025-02-13 19:24:19,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:19,748][root][INFO] - Training Epoch: 1/2, step 785/7134 completed (loss: 0.42468392848968506, acc: 0.8933333158493042)
[2025-02-13 19:24:19,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:20,185][root][INFO] - Training Epoch: 1/2, step 786/7134 completed (loss: 0.3266529142856598, acc: 0.9104477763175964)
[2025-02-13 19:24:20,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:20,594][root][INFO] - Training Epoch: 1/2, step 787/7134 completed (loss: 0.14157360792160034, acc: 0.969924807548523)
[2025-02-13 19:24:20,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:20,967][root][INFO] - Training Epoch: 1/2, step 788/7134 completed (loss: 0.4026125967502594, acc: 0.8826815485954285)
[2025-02-13 19:24:21,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:21,360][root][INFO] - Training Epoch: 1/2, step 789/7134 completed (loss: 0.4024011790752411, acc: 0.9166666865348816)
[2025-02-13 19:24:21,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:21,805][root][INFO] - Training Epoch: 1/2, step 790/7134 completed (loss: 0.490553617477417, acc: 0.8648648858070374)
[2025-02-13 19:24:21,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:22,207][root][INFO] - Training Epoch: 1/2, step 791/7134 completed (loss: 0.5386003851890564, acc: 0.8861788511276245)
[2025-02-13 19:24:22,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:22,585][root][INFO] - Training Epoch: 1/2, step 792/7134 completed (loss: 0.30976879596710205, acc: 0.9240506291389465)
[2025-02-13 19:24:22,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:22,985][root][INFO] - Training Epoch: 1/2, step 793/7134 completed (loss: 0.2224487066268921, acc: 0.9437500238418579)
[2025-02-13 19:24:23,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:23,374][root][INFO] - Training Epoch: 1/2, step 794/7134 completed (loss: 0.29764696955680847, acc: 0.931034505367279)
[2025-02-13 19:24:23,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:23,822][root][INFO] - Training Epoch: 1/2, step 795/7134 completed (loss: 0.406473845243454, acc: 0.8734177350997925)
[2025-02-13 19:24:23,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:24,245][root][INFO] - Training Epoch: 1/2, step 796/7134 completed (loss: 0.506883978843689, acc: 0.9047619104385376)
[2025-02-13 19:24:24,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:24,618][root][INFO] - Training Epoch: 1/2, step 797/7134 completed (loss: 0.42153486609458923, acc: 0.9055117964744568)
[2025-02-13 19:24:24,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:25,021][root][INFO] - Training Epoch: 1/2, step 798/7134 completed (loss: 0.34972935914993286, acc: 0.8989899158477783)
[2025-02-13 19:24:25,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:25,419][root][INFO] - Training Epoch: 1/2, step 799/7134 completed (loss: 0.25335606932640076, acc: 0.9256198406219482)
[2025-02-13 19:24:25,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:25,827][root][INFO] - Training Epoch: 1/2, step 800/7134 completed (loss: 0.4484206736087799, acc: 0.8740741014480591)
[2025-02-13 19:24:25,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:26,196][root][INFO] - Training Epoch: 1/2, step 801/7134 completed (loss: 0.5084972977638245, acc: 0.8451613187789917)
[2025-02-13 19:24:26,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:26,626][root][INFO] - Training Epoch: 1/2, step 802/7134 completed (loss: 0.3221873939037323, acc: 0.9219858050346375)
[2025-02-13 19:24:26,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:27,013][root][INFO] - Training Epoch: 1/2, step 803/7134 completed (loss: 0.23382310569286346, acc: 0.9420289993286133)
[2025-02-13 19:24:27,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:27,464][root][INFO] - Training Epoch: 1/2, step 804/7134 completed (loss: 0.5587877631187439, acc: 0.8682170510292053)
[2025-02-13 19:24:27,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:27,872][root][INFO] - Training Epoch: 1/2, step 805/7134 completed (loss: 0.5499478578567505, acc: 0.867132842540741)
[2025-02-13 19:24:28,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:28,271][root][INFO] - Training Epoch: 1/2, step 806/7134 completed (loss: 0.21466274559497833, acc: 0.9506173133850098)
[2025-02-13 19:24:28,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:28,679][root][INFO] - Training Epoch: 1/2, step 807/7134 completed (loss: 0.3799854516983032, acc: 0.8799999952316284)
[2025-02-13 19:24:28,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:29,050][root][INFO] - Training Epoch: 1/2, step 808/7134 completed (loss: 0.2153647243976593, acc: 0.9340101480484009)
[2025-02-13 19:24:29,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:29,427][root][INFO] - Training Epoch: 1/2, step 809/7134 completed (loss: 0.16221065819263458, acc: 0.9666666388511658)
[2025-02-13 19:24:29,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:29,853][root][INFO] - Training Epoch: 1/2, step 810/7134 completed (loss: 0.30056437849998474, acc: 0.9470899701118469)
[2025-02-13 19:24:30,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:30,241][root][INFO] - Training Epoch: 1/2, step 811/7134 completed (loss: 0.5242819786071777, acc: 0.8551723957061768)
[2025-02-13 19:24:30,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:30,601][root][INFO] - Training Epoch: 1/2, step 812/7134 completed (loss: 0.3079768419265747, acc: 0.9210526347160339)
[2025-02-13 19:24:30,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:30,981][root][INFO] - Training Epoch: 1/2, step 813/7134 completed (loss: 0.317072331905365, acc: 0.929411768913269)
[2025-02-13 19:24:31,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:31,384][root][INFO] - Training Epoch: 1/2, step 814/7134 completed (loss: 0.26587244868278503, acc: 0.9152542352676392)
[2025-02-13 19:24:31,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:31,771][root][INFO] - Training Epoch: 1/2, step 815/7134 completed (loss: 0.33851271867752075, acc: 0.9230769276618958)
[2025-02-13 19:24:31,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:32,158][root][INFO] - Training Epoch: 1/2, step 816/7134 completed (loss: 0.4084978699684143, acc: 0.9007092118263245)
[2025-02-13 19:24:32,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:32,543][root][INFO] - Training Epoch: 1/2, step 817/7134 completed (loss: 0.48836347460746765, acc: 0.8920863270759583)
[2025-02-13 19:24:32,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:32,932][root][INFO] - Training Epoch: 1/2, step 818/7134 completed (loss: 0.638123631477356, acc: 0.8562091588973999)
[2025-02-13 19:24:33,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:33,293][root][INFO] - Training Epoch: 1/2, step 819/7134 completed (loss: 0.46568548679351807, acc: 0.9041095972061157)
[2025-02-13 19:24:33,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:33,706][root][INFO] - Training Epoch: 1/2, step 820/7134 completed (loss: 0.41768893599510193, acc: 0.9220778942108154)
[2025-02-13 19:24:33,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:34,123][root][INFO] - Training Epoch: 1/2, step 821/7134 completed (loss: 0.29114311933517456, acc: 0.9421965479850769)
[2025-02-13 19:24:34,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:34,471][root][INFO] - Training Epoch: 1/2, step 822/7134 completed (loss: 0.2543257772922516, acc: 0.9177215099334717)
[2025-02-13 19:24:34,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:34,840][root][INFO] - Training Epoch: 1/2, step 823/7134 completed (loss: 0.16005228459835052, acc: 0.9593495726585388)
[2025-02-13 19:24:34,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:35,213][root][INFO] - Training Epoch: 1/2, step 824/7134 completed (loss: 0.4134584367275238, acc: 0.8863636255264282)
[2025-02-13 19:24:35,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:35,632][root][INFO] - Training Epoch: 1/2, step 825/7134 completed (loss: 0.31517937779426575, acc: 0.9261363744735718)
[2025-02-13 19:24:35,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:36,036][root][INFO] - Training Epoch: 1/2, step 826/7134 completed (loss: 0.27809035778045654, acc: 0.9181286692619324)
[2025-02-13 19:24:36,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:36,405][root][INFO] - Training Epoch: 1/2, step 827/7134 completed (loss: 0.3302685618400574, acc: 0.9122806787490845)
[2025-02-13 19:24:36,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:36,759][root][INFO] - Training Epoch: 1/2, step 828/7134 completed (loss: 0.21475283801555634, acc: 0.9545454382896423)
[2025-02-13 19:24:36,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:37,131][root][INFO] - Training Epoch: 1/2, step 829/7134 completed (loss: 0.3342883884906769, acc: 0.9038461446762085)
[2025-02-13 19:24:37,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:37,512][root][INFO] - Training Epoch: 1/2, step 830/7134 completed (loss: 0.2050982266664505, acc: 0.9496402740478516)
[2025-02-13 19:24:37,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:37,882][root][INFO] - Training Epoch: 1/2, step 831/7134 completed (loss: 0.1709717959165573, acc: 0.9578313231468201)
[2025-02-13 19:24:38,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:38,245][root][INFO] - Training Epoch: 1/2, step 832/7134 completed (loss: 0.47597798705101013, acc: 0.8979591727256775)
[2025-02-13 19:24:38,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:38,619][root][INFO] - Training Epoch: 1/2, step 833/7134 completed (loss: 0.214105486869812, acc: 0.9404761791229248)
[2025-02-13 19:24:38,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:38,996][root][INFO] - Training Epoch: 1/2, step 834/7134 completed (loss: 0.2862825095653534, acc: 0.9352940917015076)
[2025-02-13 19:24:39,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:39,370][root][INFO] - Training Epoch: 1/2, step 835/7134 completed (loss: 0.27295029163360596, acc: 0.9473684430122375)
[2025-02-13 19:24:39,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:39,757][root][INFO] - Training Epoch: 1/2, step 836/7134 completed (loss: 0.2699640393257141, acc: 0.9187816977500916)
[2025-02-13 19:24:39,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:40,150][root][INFO] - Training Epoch: 1/2, step 837/7134 completed (loss: 0.27592551708221436, acc: 0.9435028433799744)
[2025-02-13 19:24:40,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:40,525][root][INFO] - Training Epoch: 1/2, step 838/7134 completed (loss: 0.24554765224456787, acc: 0.9281045794487)
[2025-02-13 19:24:40,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:40,911][root][INFO] - Training Epoch: 1/2, step 839/7134 completed (loss: 0.18980571627616882, acc: 0.9306930899620056)
[2025-02-13 19:24:41,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:41,261][root][INFO] - Training Epoch: 1/2, step 840/7134 completed (loss: 0.23599544167518616, acc: 0.9520958065986633)
[2025-02-13 19:24:41,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:41,631][root][INFO] - Training Epoch: 1/2, step 841/7134 completed (loss: 0.2124488800764084, acc: 0.9473684430122375)
[2025-02-13 19:24:41,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:42,053][root][INFO] - Training Epoch: 1/2, step 842/7134 completed (loss: 0.2085951417684555, acc: 0.9470587968826294)
[2025-02-13 19:24:42,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:42,436][root][INFO] - Training Epoch: 1/2, step 843/7134 completed (loss: 0.18438757956027985, acc: 0.9415204524993896)
[2025-02-13 19:24:42,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:42,816][root][INFO] - Training Epoch: 1/2, step 844/7134 completed (loss: 0.2056383192539215, acc: 0.965753436088562)
[2025-02-13 19:24:42,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:43,198][root][INFO] - Training Epoch: 1/2, step 845/7134 completed (loss: 0.19596897065639496, acc: 0.9333333373069763)
[2025-02-13 19:24:43,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:43,582][root][INFO] - Training Epoch: 1/2, step 846/7134 completed (loss: 0.5185033082962036, acc: 0.8835616707801819)
[2025-02-13 19:24:43,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:43,959][root][INFO] - Training Epoch: 1/2, step 847/7134 completed (loss: 0.4135099947452545, acc: 0.910179615020752)
[2025-02-13 19:24:44,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:44,325][root][INFO] - Training Epoch: 1/2, step 848/7134 completed (loss: 0.30377310514450073, acc: 0.9181286692619324)
[2025-02-13 19:24:44,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:44,829][root][INFO] - Training Epoch: 1/2, step 849/7134 completed (loss: 0.2504889667034149, acc: 0.9207317233085632)
[2025-02-13 19:24:44,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:45,243][root][INFO] - Training Epoch: 1/2, step 850/7134 completed (loss: 0.2512654960155487, acc: 0.926174521446228)
[2025-02-13 19:24:45,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:45,657][root][INFO] - Training Epoch: 1/2, step 851/7134 completed (loss: 0.31797221302986145, acc: 0.9127907156944275)
[2025-02-13 19:24:45,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:46,072][root][INFO] - Training Epoch: 1/2, step 852/7134 completed (loss: 0.19698795676231384, acc: 0.9424083828926086)
[2025-02-13 19:24:46,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:46,494][root][INFO] - Training Epoch: 1/2, step 853/7134 completed (loss: 0.3205595314502716, acc: 0.9371727705001831)
[2025-02-13 19:24:46,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:46,904][root][INFO] - Training Epoch: 1/2, step 854/7134 completed (loss: 0.2967604100704193, acc: 0.9512194991111755)
[2025-02-13 19:24:47,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:47,300][root][INFO] - Training Epoch: 1/2, step 855/7134 completed (loss: 0.2223469465970993, acc: 0.9364162087440491)
[2025-02-13 19:24:47,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:47,680][root][INFO] - Training Epoch: 1/2, step 856/7134 completed (loss: 0.1560976654291153, acc: 0.9672130942344666)
[2025-02-13 19:24:47,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:48,081][root][INFO] - Training Epoch: 1/2, step 857/7134 completed (loss: 0.4087815284729004, acc: 0.9071428775787354)
[2025-02-13 19:24:48,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:48,467][root][INFO] - Training Epoch: 1/2, step 858/7134 completed (loss: 0.4705488085746765, acc: 0.886904776096344)
[2025-02-13 19:24:48,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:48,860][root][INFO] - Training Epoch: 1/2, step 859/7134 completed (loss: 0.4739048480987549, acc: 0.8813559412956238)
[2025-02-13 19:24:49,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:49,254][root][INFO] - Training Epoch: 1/2, step 860/7134 completed (loss: 0.44350680708885193, acc: 0.8476821184158325)
[2025-02-13 19:24:49,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:49,702][root][INFO] - Training Epoch: 1/2, step 861/7134 completed (loss: 0.3577289283275604, acc: 0.9086021780967712)
[2025-02-13 19:24:49,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:50,099][root][INFO] - Training Epoch: 1/2, step 862/7134 completed (loss: 0.4237048923969269, acc: 0.9142857193946838)
[2025-02-13 19:24:50,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:50,506][root][INFO] - Training Epoch: 1/2, step 863/7134 completed (loss: 0.4923470616340637, acc: 0.8797814249992371)
[2025-02-13 19:24:50,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:50,869][root][INFO] - Training Epoch: 1/2, step 864/7134 completed (loss: 0.4975009262561798, acc: 0.9047619104385376)
[2025-02-13 19:24:51,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:51,268][root][INFO] - Training Epoch: 1/2, step 865/7134 completed (loss: 0.5630779266357422, acc: 0.8430232405662537)
[2025-02-13 19:24:51,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:51,682][root][INFO] - Training Epoch: 1/2, step 866/7134 completed (loss: 0.5616747140884399, acc: 0.8642857074737549)
[2025-02-13 19:24:51,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:52,105][root][INFO] - Training Epoch: 1/2, step 867/7134 completed (loss: 0.4673483371734619, acc: 0.887499988079071)
[2025-02-13 19:24:52,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:52,492][root][INFO] - Training Epoch: 1/2, step 868/7134 completed (loss: 0.688216507434845, acc: 0.854651153087616)
[2025-02-13 19:24:52,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:52,900][root][INFO] - Training Epoch: 1/2, step 869/7134 completed (loss: 0.872600793838501, acc: 0.8313252925872803)
[2025-02-13 19:24:53,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:53,298][root][INFO] - Training Epoch: 1/2, step 870/7134 completed (loss: 0.3323519825935364, acc: 0.9064327478408813)
[2025-02-13 19:24:53,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:53,686][root][INFO] - Training Epoch: 1/2, step 871/7134 completed (loss: 0.4362192451953888, acc: 0.9041095972061157)
[2025-02-13 19:24:53,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:54,096][root][INFO] - Training Epoch: 1/2, step 872/7134 completed (loss: 0.18414469063282013, acc: 0.9444444179534912)
[2025-02-13 19:24:54,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:54,469][root][INFO] - Training Epoch: 1/2, step 873/7134 completed (loss: 0.45633774995803833, acc: 0.8969696760177612)
[2025-02-13 19:24:54,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:54,853][root][INFO] - Training Epoch: 1/2, step 874/7134 completed (loss: 0.4312026798725128, acc: 0.8680555820465088)
[2025-02-13 19:24:55,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:55,278][root][INFO] - Training Epoch: 1/2, step 875/7134 completed (loss: 0.24979132413864136, acc: 0.935251772403717)
[2025-02-13 19:24:55,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:55,675][root][INFO] - Training Epoch: 1/2, step 876/7134 completed (loss: 0.288984477519989, acc: 0.9276315569877625)
[2025-02-13 19:24:55,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:56,084][root][INFO] - Training Epoch: 1/2, step 877/7134 completed (loss: 0.46483832597732544, acc: 0.9202454090118408)
[2025-02-13 19:24:56,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:56,454][root][INFO] - Training Epoch: 1/2, step 878/7134 completed (loss: 0.3976288437843323, acc: 0.8999999761581421)
[2025-02-13 19:24:56,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:56,811][root][INFO] - Training Epoch: 1/2, step 879/7134 completed (loss: 0.33715394139289856, acc: 0.9299362897872925)
[2025-02-13 19:24:57,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:57,250][root][INFO] - Training Epoch: 1/2, step 880/7134 completed (loss: 0.22581271827220917, acc: 0.9386503100395203)
[2025-02-13 19:24:57,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:57,627][root][INFO] - Training Epoch: 1/2, step 881/7134 completed (loss: 0.29643505811691284, acc: 0.9024389982223511)
[2025-02-13 19:24:57,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:58,057][root][INFO] - Training Epoch: 1/2, step 882/7134 completed (loss: 0.3214721977710724, acc: 0.9112903475761414)
[2025-02-13 19:24:58,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:58,462][root][INFO] - Training Epoch: 1/2, step 883/7134 completed (loss: 0.49639150500297546, acc: 0.8552631735801697)
[2025-02-13 19:24:58,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:58,836][root][INFO] - Training Epoch: 1/2, step 884/7134 completed (loss: 0.4880983829498291, acc: 0.8802395462989807)
[2025-02-13 19:24:59,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:59,275][root][INFO] - Training Epoch: 1/2, step 885/7134 completed (loss: 0.18519823253154755, acc: 0.9512194991111755)
[2025-02-13 19:24:59,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:59,647][root][INFO] - Training Epoch: 1/2, step 886/7134 completed (loss: 0.36885741353034973, acc: 0.9075630307197571)
[2025-02-13 19:24:59,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:00,066][root][INFO] - Training Epoch: 1/2, step 887/7134 completed (loss: 0.3194096088409424, acc: 0.9477611780166626)
[2025-02-13 19:25:00,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:00,441][root][INFO] - Training Epoch: 1/2, step 888/7134 completed (loss: 0.3319174349308014, acc: 0.9328858852386475)
[2025-02-13 19:25:00,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:00,851][root][INFO] - Training Epoch: 1/2, step 889/7134 completed (loss: 0.3203094005584717, acc: 0.9207921028137207)
[2025-02-13 19:25:01,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:01,281][root][INFO] - Training Epoch: 1/2, step 890/7134 completed (loss: 0.3856070041656494, acc: 0.8926174640655518)
[2025-02-13 19:25:01,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:01,679][root][INFO] - Training Epoch: 1/2, step 891/7134 completed (loss: 0.33412110805511475, acc: 0.9329608678817749)
[2025-02-13 19:25:01,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:02,072][root][INFO] - Training Epoch: 1/2, step 892/7134 completed (loss: 0.24995139241218567, acc: 0.9345238208770752)
[2025-02-13 19:25:02,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:02,485][root][INFO] - Training Epoch: 1/2, step 893/7134 completed (loss: 0.18889755010604858, acc: 0.9607843160629272)
[2025-02-13 19:25:02,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:02,872][root][INFO] - Training Epoch: 1/2, step 894/7134 completed (loss: 0.11817844957113266, acc: 0.9774011373519897)
[2025-02-13 19:25:03,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:03,336][root][INFO] - Training Epoch: 1/2, step 895/7134 completed (loss: 0.14957310259342194, acc: 0.965753436088562)
[2025-02-13 19:25:03,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:03,765][root][INFO] - Training Epoch: 1/2, step 896/7134 completed (loss: 0.12405922263860703, acc: 0.960629940032959)
[2025-02-13 19:25:03,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:04,171][root][INFO] - Training Epoch: 1/2, step 897/7134 completed (loss: 0.2102624475955963, acc: 0.9553072452545166)
[2025-02-13 19:25:04,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:04,545][root][INFO] - Training Epoch: 1/2, step 898/7134 completed (loss: 0.35577645897865295, acc: 0.9204545617103577)
[2025-02-13 19:25:04,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:04,917][root][INFO] - Training Epoch: 1/2, step 899/7134 completed (loss: 0.2874371409416199, acc: 0.9197860956192017)
[2025-02-13 19:25:05,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:05,305][root][INFO] - Training Epoch: 1/2, step 900/7134 completed (loss: 0.14898809790611267, acc: 0.9756097793579102)
[2025-02-13 19:25:05,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:05,736][root][INFO] - Training Epoch: 1/2, step 901/7134 completed (loss: 0.43306881189346313, acc: 0.9107142686843872)
[2025-02-13 19:25:05,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:06,171][root][INFO] - Training Epoch: 1/2, step 902/7134 completed (loss: 0.2710252106189728, acc: 0.9155844449996948)
[2025-02-13 19:25:06,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:06,603][root][INFO] - Training Epoch: 1/2, step 903/7134 completed (loss: 0.09624096751213074, acc: 0.9768785834312439)
[2025-02-13 19:25:06,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:07,024][root][INFO] - Training Epoch: 1/2, step 904/7134 completed (loss: 0.2204420417547226, acc: 0.9717513918876648)
[2025-02-13 19:25:07,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:07,411][root][INFO] - Training Epoch: 1/2, step 905/7134 completed (loss: 0.11498944461345673, acc: 0.9712643623352051)
[2025-02-13 19:25:07,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:07,818][root][INFO] - Training Epoch: 1/2, step 906/7134 completed (loss: 0.2578006386756897, acc: 0.9226519465446472)
[2025-02-13 19:25:07,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:08,246][root][INFO] - Training Epoch: 1/2, step 907/7134 completed (loss: 0.28135520219802856, acc: 0.9354838728904724)
[2025-02-13 19:25:08,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:08,623][root][INFO] - Training Epoch: 1/2, step 908/7134 completed (loss: 0.18588201701641083, acc: 0.9722222089767456)
[2025-02-13 19:25:08,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:08,989][root][INFO] - Training Epoch: 1/2, step 909/7134 completed (loss: 0.15271495282649994, acc: 0.9539473652839661)
[2025-02-13 19:25:09,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:09,451][root][INFO] - Training Epoch: 1/2, step 910/7134 completed (loss: 0.275199830532074, acc: 0.918749988079071)
[2025-02-13 19:25:09,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:09,881][root][INFO] - Training Epoch: 1/2, step 911/7134 completed (loss: 0.24655687808990479, acc: 0.9548386931419373)
[2025-02-13 19:25:10,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:10,275][root][INFO] - Training Epoch: 1/2, step 912/7134 completed (loss: 0.2228337675333023, acc: 0.942105233669281)
[2025-02-13 19:25:10,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:10,653][root][INFO] - Training Epoch: 1/2, step 913/7134 completed (loss: 0.2037504017353058, acc: 0.9505494236946106)
[2025-02-13 19:25:10,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:11,040][root][INFO] - Training Epoch: 1/2, step 914/7134 completed (loss: 0.20800799131393433, acc: 0.939393937587738)
[2025-02-13 19:25:11,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:11,423][root][INFO] - Training Epoch: 1/2, step 915/7134 completed (loss: 0.378691703081131, acc: 0.9071428775787354)
[2025-02-13 19:25:11,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:11,801][root][INFO] - Training Epoch: 1/2, step 916/7134 completed (loss: 0.26002931594848633, acc: 0.9147287011146545)
[2025-02-13 19:25:11,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:12,172][root][INFO] - Training Epoch: 1/2, step 917/7134 completed (loss: 0.37041205167770386, acc: 0.8961039185523987)
[2025-02-13 19:25:12,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:12,540][root][INFO] - Training Epoch: 1/2, step 918/7134 completed (loss: 0.2969154119491577, acc: 0.9337748289108276)
[2025-02-13 19:25:12,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:12,898][root][INFO] - Training Epoch: 1/2, step 919/7134 completed (loss: 0.20556151866912842, acc: 0.9545454382896423)
[2025-02-13 19:25:13,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:13,289][root][INFO] - Training Epoch: 1/2, step 920/7134 completed (loss: 0.3145589232444763, acc: 0.9127907156944275)
[2025-02-13 19:25:13,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:13,677][root][INFO] - Training Epoch: 1/2, step 921/7134 completed (loss: 0.3019148111343384, acc: 0.9298245906829834)
[2025-02-13 19:25:13,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:14,074][root][INFO] - Training Epoch: 1/2, step 922/7134 completed (loss: 0.16648834943771362, acc: 0.9776119589805603)
[2025-02-13 19:25:14,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:14,490][root][INFO] - Training Epoch: 1/2, step 923/7134 completed (loss: 0.1912572979927063, acc: 0.9473684430122375)
[2025-02-13 19:25:14,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:14,926][root][INFO] - Training Epoch: 1/2, step 924/7134 completed (loss: 0.18610399961471558, acc: 0.9548022747039795)
[2025-02-13 19:25:15,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:15,357][root][INFO] - Training Epoch: 1/2, step 925/7134 completed (loss: 0.5124590396881104, acc: 0.8838709592819214)
[2025-02-13 19:25:15,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:15,773][root][INFO] - Training Epoch: 1/2, step 926/7134 completed (loss: 0.2691939175128937, acc: 0.9388889074325562)
[2025-02-13 19:25:15,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:16,171][root][INFO] - Training Epoch: 1/2, step 927/7134 completed (loss: 0.34147223830223083, acc: 0.9136690497398376)
[2025-02-13 19:25:16,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:16,560][root][INFO] - Training Epoch: 1/2, step 928/7134 completed (loss: 0.09208428114652634, acc: 0.9724770784378052)
[2025-02-13 19:25:16,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:16,930][root][INFO] - Training Epoch: 1/2, step 929/7134 completed (loss: 0.25670644640922546, acc: 0.9342105388641357)
[2025-02-13 19:25:17,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:17,278][root][INFO] - Training Epoch: 1/2, step 930/7134 completed (loss: 0.3824317753314972, acc: 0.9032257795333862)
[2025-02-13 19:25:17,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:17,661][root][INFO] - Training Epoch: 1/2, step 931/7134 completed (loss: 0.3000335097312927, acc: 0.9338235259056091)
[2025-02-13 19:25:17,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:18,029][root][INFO] - Training Epoch: 1/2, step 932/7134 completed (loss: 0.2064933329820633, acc: 0.9404761791229248)
[2025-02-13 19:25:18,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:18,385][root][INFO] - Training Epoch: 1/2, step 933/7134 completed (loss: 0.2840624451637268, acc: 0.9341317415237427)
[2025-02-13 19:25:18,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:18,806][root][INFO] - Training Epoch: 1/2, step 934/7134 completed (loss: 0.21087901294231415, acc: 0.9418604373931885)
[2025-02-13 19:25:18,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:19,226][root][INFO] - Training Epoch: 1/2, step 935/7134 completed (loss: 0.13085097074508667, acc: 0.9689922332763672)
[2025-02-13 19:25:19,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:19,667][root][INFO] - Training Epoch: 1/2, step 936/7134 completed (loss: 0.1605440229177475, acc: 0.9527559280395508)
[2025-02-13 19:25:19,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:20,074][root][INFO] - Training Epoch: 1/2, step 937/7134 completed (loss: 0.21491287648677826, acc: 0.9674796462059021)
[2025-02-13 19:25:20,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:20,489][root][INFO] - Training Epoch: 1/2, step 938/7134 completed (loss: 0.28816860914230347, acc: 0.9102563858032227)
[2025-02-13 19:25:20,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:20,848][root][INFO] - Training Epoch: 1/2, step 939/7134 completed (loss: 0.35061126947402954, acc: 0.9207317233085632)
[2025-02-13 19:25:20,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:21,236][root][INFO] - Training Epoch: 1/2, step 940/7134 completed (loss: 0.25527381896972656, acc: 0.9364162087440491)
[2025-02-13 19:25:21,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:21,618][root][INFO] - Training Epoch: 1/2, step 941/7134 completed (loss: 0.2756655514240265, acc: 0.932584285736084)
[2025-02-13 19:25:21,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:21,991][root][INFO] - Training Epoch: 1/2, step 942/7134 completed (loss: 0.3389184772968292, acc: 0.9127907156944275)
[2025-02-13 19:25:22,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:22,366][root][INFO] - Training Epoch: 1/2, step 943/7134 completed (loss: 0.21823325753211975, acc: 0.9509202241897583)
[2025-02-13 19:25:22,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:22,749][root][INFO] - Training Epoch: 1/2, step 944/7134 completed (loss: 0.24805700778961182, acc: 0.9363057613372803)
[2025-02-13 19:25:22,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:23,113][root][INFO] - Training Epoch: 1/2, step 945/7134 completed (loss: 0.2736906409263611, acc: 0.940397322177887)
[2025-02-13 19:25:23,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:23,467][root][INFO] - Training Epoch: 1/2, step 946/7134 completed (loss: 0.22613154351711273, acc: 0.9417475461959839)
[2025-02-13 19:25:23,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:23,833][root][INFO] - Training Epoch: 1/2, step 947/7134 completed (loss: 0.40825751423835754, acc: 0.8970588445663452)
[2025-02-13 19:25:23,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:24,246][root][INFO] - Training Epoch: 1/2, step 948/7134 completed (loss: 0.1334199160337448, acc: 0.9638554453849792)
[2025-02-13 19:25:24,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:24,677][root][INFO] - Training Epoch: 1/2, step 949/7134 completed (loss: 0.2654717266559601, acc: 0.949999988079071)
[2025-02-13 19:25:24,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:25,057][root][INFO] - Training Epoch: 1/2, step 950/7134 completed (loss: 0.2910196781158447, acc: 0.9350649118423462)
[2025-02-13 19:25:25,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:25,428][root][INFO] - Training Epoch: 1/2, step 951/7134 completed (loss: 0.23203250765800476, acc: 0.9312977194786072)
[2025-02-13 19:25:25,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:25,784][root][INFO] - Training Epoch: 1/2, step 952/7134 completed (loss: 0.42509177327156067, acc: 0.8970588445663452)
[2025-02-13 19:25:25,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:26,142][root][INFO] - Training Epoch: 1/2, step 953/7134 completed (loss: 0.3803349733352661, acc: 0.9024389982223511)
[2025-02-13 19:25:26,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:26,514][root][INFO] - Training Epoch: 1/2, step 954/7134 completed (loss: 0.36174628138542175, acc: 0.9379310607910156)
[2025-02-13 19:25:26,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:26,919][root][INFO] - Training Epoch: 1/2, step 955/7134 completed (loss: 0.3232487738132477, acc: 0.9209039807319641)
[2025-02-13 19:25:27,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:27,284][root][INFO] - Training Epoch: 1/2, step 956/7134 completed (loss: 0.3509729206562042, acc: 0.9130434989929199)
[2025-02-13 19:25:27,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:27,657][root][INFO] - Training Epoch: 1/2, step 957/7134 completed (loss: 0.343287855386734, acc: 0.9558823704719543)
[2025-02-13 19:25:27,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:28,033][root][INFO] - Training Epoch: 1/2, step 958/7134 completed (loss: 0.44907835125923157, acc: 0.9029850959777832)
[2025-02-13 19:25:28,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:28,466][root][INFO] - Training Epoch: 1/2, step 959/7134 completed (loss: 0.49144965410232544, acc: 0.8865979313850403)
[2025-02-13 19:25:28,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:28,872][root][INFO] - Training Epoch: 1/2, step 960/7134 completed (loss: 0.6999427080154419, acc: 0.8731343150138855)
[2025-02-13 19:25:29,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:29,298][root][INFO] - Training Epoch: 1/2, step 961/7134 completed (loss: 0.4843631386756897, acc: 0.9120879173278809)
[2025-02-13 19:25:29,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:29,666][root][INFO] - Training Epoch: 1/2, step 962/7134 completed (loss: 0.5111129879951477, acc: 0.9005848169326782)
[2025-02-13 19:25:29,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:30,053][root][INFO] - Training Epoch: 1/2, step 963/7134 completed (loss: 0.46474140882492065, acc: 0.8895348906517029)
[2025-02-13 19:25:30,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:30,446][root][INFO] - Training Epoch: 1/2, step 964/7134 completed (loss: 0.7241948246955872, acc: 0.8475610017776489)
[2025-02-13 19:25:30,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:30,810][root][INFO] - Training Epoch: 1/2, step 965/7134 completed (loss: 0.5108141303062439, acc: 0.903743326663971)
[2025-02-13 19:25:30,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:31,210][root][INFO] - Training Epoch: 1/2, step 966/7134 completed (loss: 0.41229739785194397, acc: 0.907975435256958)
[2025-02-13 19:25:31,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:31,594][root][INFO] - Training Epoch: 1/2, step 967/7134 completed (loss: 0.31426718831062317, acc: 0.936170220375061)
[2025-02-13 19:25:31,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:31,992][root][INFO] - Training Epoch: 1/2, step 968/7134 completed (loss: 0.5395264029502869, acc: 0.8646616339683533)
[2025-02-13 19:25:32,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:32,355][root][INFO] - Training Epoch: 1/2, step 969/7134 completed (loss: 0.366470068693161, acc: 0.9200000166893005)
[2025-02-13 19:25:32,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:32,812][root][INFO] - Training Epoch: 1/2, step 970/7134 completed (loss: 0.426535040140152, acc: 0.904411792755127)
[2025-02-13 19:25:32,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:33,236][root][INFO] - Training Epoch: 1/2, step 971/7134 completed (loss: 0.3554244339466095, acc: 0.9146919250488281)
[2025-02-13 19:25:33,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:33,646][root][INFO] - Training Epoch: 1/2, step 972/7134 completed (loss: 0.4588138163089752, acc: 0.8820512890815735)
[2025-02-13 19:25:33,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:34,057][root][INFO] - Training Epoch: 1/2, step 973/7134 completed (loss: 0.5220045447349548, acc: 0.8571428656578064)
[2025-02-13 19:25:34,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:34,460][root][INFO] - Training Epoch: 1/2, step 974/7134 completed (loss: 0.3546595871448517, acc: 0.9257425665855408)
[2025-02-13 19:25:34,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:34,862][root][INFO] - Training Epoch: 1/2, step 975/7134 completed (loss: 0.423688679933548, acc: 0.9095744490623474)
[2025-02-13 19:25:35,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:35,227][root][INFO] - Training Epoch: 1/2, step 976/7134 completed (loss: 0.30381524562835693, acc: 0.9072847962379456)
[2025-02-13 19:25:35,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:35,588][root][INFO] - Training Epoch: 1/2, step 977/7134 completed (loss: 0.5722055435180664, acc: 0.8595505356788635)
[2025-02-13 19:25:35,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:35,960][root][INFO] - Training Epoch: 1/2, step 978/7134 completed (loss: 0.2560456395149231, acc: 0.9329897165298462)
[2025-02-13 19:25:36,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:36,331][root][INFO] - Training Epoch: 1/2, step 979/7134 completed (loss: 0.3640681803226471, acc: 0.9135135412216187)
[2025-02-13 19:25:36,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:36,700][root][INFO] - Training Epoch: 1/2, step 980/7134 completed (loss: 0.4174373149871826, acc: 0.8826290965080261)
[2025-02-13 19:25:36,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:37,059][root][INFO] - Training Epoch: 1/2, step 981/7134 completed (loss: 0.3632884621620178, acc: 0.9433962106704712)
[2025-02-13 19:25:37,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:37,431][root][INFO] - Training Epoch: 1/2, step 982/7134 completed (loss: 0.610184907913208, acc: 0.9314285516738892)
[2025-02-13 19:25:37,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:37,823][root][INFO] - Training Epoch: 1/2, step 983/7134 completed (loss: 0.34770187735557556, acc: 0.9097744226455688)
[2025-02-13 19:25:37,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:38,192][root][INFO] - Training Epoch: 1/2, step 984/7134 completed (loss: 0.23108872771263123, acc: 0.957446813583374)
[2025-02-13 19:25:38,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:38,559][root][INFO] - Training Epoch: 1/2, step 985/7134 completed (loss: 0.30140408873558044, acc: 0.9388889074325562)
[2025-02-13 19:25:38,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:38,927][root][INFO] - Training Epoch: 1/2, step 986/7134 completed (loss: 0.2552764415740967, acc: 0.9385474920272827)
[2025-02-13 19:25:39,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:39,288][root][INFO] - Training Epoch: 1/2, step 987/7134 completed (loss: 0.2686808705329895, acc: 0.9395973086357117)
[2025-02-13 19:25:39,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:39,650][root][INFO] - Training Epoch: 1/2, step 988/7134 completed (loss: 0.5187931656837463, acc: 0.8757061958312988)
[2025-02-13 19:25:39,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:40,020][root][INFO] - Training Epoch: 1/2, step 989/7134 completed (loss: 0.4965935945510864, acc: 0.8895705342292786)
[2025-02-13 19:25:40,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:40,394][root][INFO] - Training Epoch: 1/2, step 990/7134 completed (loss: 0.43954169750213623, acc: 0.8791208863258362)
[2025-02-13 19:25:40,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:40,769][root][INFO] - Training Epoch: 1/2, step 991/7134 completed (loss: 0.6380183100700378, acc: 0.8679245114326477)
[2025-02-13 19:25:40,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:41,144][root][INFO] - Training Epoch: 1/2, step 992/7134 completed (loss: 0.39305490255355835, acc: 0.9075144529342651)
[2025-02-13 19:25:41,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:41,522][root][INFO] - Training Epoch: 1/2, step 993/7134 completed (loss: 0.30281466245651245, acc: 0.9136690497398376)
[2025-02-13 19:25:41,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:41,926][root][INFO] - Training Epoch: 1/2, step 994/7134 completed (loss: 0.37246373295783997, acc: 0.917475700378418)
[2025-02-13 19:25:42,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:42,316][root][INFO] - Training Epoch: 1/2, step 995/7134 completed (loss: 0.34478917717933655, acc: 0.9251700639724731)
[2025-02-13 19:25:42,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:42,682][root][INFO] - Training Epoch: 1/2, step 996/7134 completed (loss: 0.3728858232498169, acc: 0.9244186282157898)
[2025-02-13 19:25:42,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:43,059][root][INFO] - Training Epoch: 1/2, step 997/7134 completed (loss: 0.1715853363275528, acc: 0.9529411792755127)
[2025-02-13 19:25:43,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:43,443][root][INFO] - Training Epoch: 1/2, step 998/7134 completed (loss: 0.3030177056789398, acc: 0.9221556782722473)
[2025-02-13 19:25:43,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:43,837][root][INFO] - Training Epoch: 1/2, step 999/7134 completed (loss: 0.2492893785238266, acc: 0.9615384340286255)
[2025-02-13 19:25:43,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:44,210][root][INFO] - Training Epoch: 1/2, step 1000/7134 completed (loss: 0.19240622222423553, acc: 0.9485294222831726)
[2025-02-13 19:25:44,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:44,592][root][INFO] - Training Epoch: 1/2, step 1001/7134 completed (loss: 0.2920258045196533, acc: 0.918181836605072)
[2025-02-13 19:25:44,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:44,956][root][INFO] - Training Epoch: 1/2, step 1002/7134 completed (loss: 0.2354208528995514, acc: 0.9622641801834106)
[2025-02-13 19:25:45,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:45,310][root][INFO] - Training Epoch: 1/2, step 1003/7134 completed (loss: 0.1620883345603943, acc: 0.9722222089767456)
[2025-02-13 19:25:45,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:45,696][root][INFO] - Training Epoch: 1/2, step 1004/7134 completed (loss: 0.35422632098197937, acc: 0.9203540086746216)
[2025-02-13 19:25:45,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:46,095][root][INFO] - Training Epoch: 1/2, step 1005/7134 completed (loss: 0.2899073660373688, acc: 0.9432623982429504)
[2025-02-13 19:25:46,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:46,474][root][INFO] - Training Epoch: 1/2, step 1006/7134 completed (loss: 0.31176596879959106, acc: 0.9235668778419495)
[2025-02-13 19:25:46,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:46,874][root][INFO] - Training Epoch: 1/2, step 1007/7134 completed (loss: 0.595691978931427, acc: 0.8661971688270569)
[2025-02-13 19:25:47,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:47,264][root][INFO] - Training Epoch: 1/2, step 1008/7134 completed (loss: 0.30700409412384033, acc: 0.895061731338501)
[2025-02-13 19:25:47,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:47,645][root][INFO] - Training Epoch: 1/2, step 1009/7134 completed (loss: 0.23828716576099396, acc: 0.9534883499145508)
[2025-02-13 19:25:47,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:48,013][root][INFO] - Training Epoch: 1/2, step 1010/7134 completed (loss: 0.34036779403686523, acc: 0.9059829115867615)
[2025-02-13 19:25:48,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:48,386][root][INFO] - Training Epoch: 1/2, step 1011/7134 completed (loss: 0.19486233592033386, acc: 0.955974817276001)
[2025-02-13 19:25:48,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:48,769][root][INFO] - Training Epoch: 1/2, step 1012/7134 completed (loss: 0.6048221588134766, acc: 0.8812500238418579)
[2025-02-13 19:25:48,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:49,155][root][INFO] - Training Epoch: 1/2, step 1013/7134 completed (loss: 0.42258861660957336, acc: 0.9156626462936401)
[2025-02-13 19:25:49,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:49,529][root][INFO] - Training Epoch: 1/2, step 1014/7134 completed (loss: 0.3670729100704193, acc: 0.914893627166748)
[2025-02-13 19:25:49,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:49,908][root][INFO] - Training Epoch: 1/2, step 1015/7134 completed (loss: 0.1516655534505844, acc: 0.9867549538612366)
[2025-02-13 19:25:50,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:50,307][root][INFO] - Training Epoch: 1/2, step 1016/7134 completed (loss: 0.13039530813694, acc: 0.9663865566253662)
[2025-02-13 19:25:50,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:50,696][root][INFO] - Training Epoch: 1/2, step 1017/7134 completed (loss: 0.5776525139808655, acc: 0.9142857193946838)
[2025-02-13 19:25:50,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:51,060][root][INFO] - Training Epoch: 1/2, step 1018/7134 completed (loss: 0.49775412678718567, acc: 0.9068322777748108)
[2025-02-13 19:25:51,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:51,426][root][INFO] - Training Epoch: 1/2, step 1019/7134 completed (loss: 0.46240234375, acc: 0.9155844449996948)
[2025-02-13 19:25:51,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:51,785][root][INFO] - Training Epoch: 1/2, step 1020/7134 completed (loss: 0.20592403411865234, acc: 0.9490445852279663)
[2025-02-13 19:25:51,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:52,159][root][INFO] - Training Epoch: 1/2, step 1021/7134 completed (loss: 0.13755269348621368, acc: 0.9738562107086182)
[2025-02-13 19:25:52,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:52,543][root][INFO] - Training Epoch: 1/2, step 1022/7134 completed (loss: 0.1389075517654419, acc: 0.9595375657081604)
[2025-02-13 19:25:52,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:52,910][root][INFO] - Training Epoch: 1/2, step 1023/7134 completed (loss: 0.2377946525812149, acc: 0.9441340565681458)
[2025-02-13 19:25:53,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:53,280][root][INFO] - Training Epoch: 1/2, step 1024/7134 completed (loss: 0.1983572542667389, acc: 0.9626865386962891)
[2025-02-13 19:25:53,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:53,644][root][INFO] - Training Epoch: 1/2, step 1025/7134 completed (loss: 0.1571359932422638, acc: 0.9503546357154846)
[2025-02-13 19:25:53,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:54,004][root][INFO] - Training Epoch: 1/2, step 1026/7134 completed (loss: 0.25321653485298157, acc: 0.9292035102844238)
[2025-02-13 19:25:54,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:54,387][root][INFO] - Training Epoch: 1/2, step 1027/7134 completed (loss: 0.1596108227968216, acc: 0.9647058844566345)
[2025-02-13 19:25:54,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:54,820][root][INFO] - Training Epoch: 1/2, step 1028/7134 completed (loss: 0.15870118141174316, acc: 0.955974817276001)
[2025-02-13 19:25:54,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:55,189][root][INFO] - Training Epoch: 1/2, step 1029/7134 completed (loss: 0.1933133453130722, acc: 0.9629629850387573)
[2025-02-13 19:25:55,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:55,554][root][INFO] - Training Epoch: 1/2, step 1030/7134 completed (loss: 0.08998747169971466, acc: 0.9712643623352051)
[2025-02-13 19:25:55,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:55,903][root][INFO] - Training Epoch: 1/2, step 1031/7134 completed (loss: 0.32006338238716125, acc: 0.9130434989929199)
[2025-02-13 19:25:56,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:56,332][root][INFO] - Training Epoch: 1/2, step 1032/7134 completed (loss: 0.20569440722465515, acc: 0.9468085169792175)
[2025-02-13 19:25:56,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:56,708][root][INFO] - Training Epoch: 1/2, step 1033/7134 completed (loss: 0.2212870866060257, acc: 0.931506872177124)
[2025-02-13 19:25:56,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:57,054][root][INFO] - Training Epoch: 1/2, step 1034/7134 completed (loss: 0.5913675427436829, acc: 0.8701298832893372)
[2025-02-13 19:25:57,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:57,439][root][INFO] - Training Epoch: 1/2, step 1035/7134 completed (loss: 1.1237854957580566, acc: 0.7972972989082336)
[2025-02-13 19:25:57,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:57,802][root][INFO] - Training Epoch: 1/2, step 1036/7134 completed (loss: 1.2382020950317383, acc: 0.7711864113807678)
[2025-02-13 19:25:57,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:58,188][root][INFO] - Training Epoch: 1/2, step 1037/7134 completed (loss: 0.22846566140651703, acc: 0.936170220375061)
[2025-02-13 19:25:58,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:58,571][root][INFO] - Training Epoch: 1/2, step 1038/7134 completed (loss: 0.35649898648262024, acc: 0.8960000276565552)
[2025-02-13 19:25:58,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:58,989][root][INFO] - Training Epoch: 1/2, step 1039/7134 completed (loss: 0.34877392649650574, acc: 0.8990825414657593)
[2025-02-13 19:25:59,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:59,408][root][INFO] - Training Epoch: 1/2, step 1040/7134 completed (loss: 0.16321897506713867, acc: 0.9492753744125366)
[2025-02-13 19:25:59,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:59,775][root][INFO] - Training Epoch: 1/2, step 1041/7134 completed (loss: 0.1662486493587494, acc: 0.9664429426193237)
[2025-02-13 19:25:59,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:00,195][root][INFO] - Training Epoch: 1/2, step 1042/7134 completed (loss: 0.33101680874824524, acc: 0.9273743033409119)
[2025-02-13 19:26:00,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:00,626][root][INFO] - Training Epoch: 1/2, step 1043/7134 completed (loss: 0.2278059422969818, acc: 0.9504950642585754)
[2025-02-13 19:26:00,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:01,019][root][INFO] - Training Epoch: 1/2, step 1044/7134 completed (loss: 0.13524331152439117, acc: 0.9751552939414978)
[2025-02-13 19:26:01,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:01,405][root][INFO] - Training Epoch: 1/2, step 1045/7134 completed (loss: 0.7417148947715759, acc: 0.7950310707092285)
[2025-02-13 19:26:01,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:01,772][root][INFO] - Training Epoch: 1/2, step 1046/7134 completed (loss: 0.4775391221046448, acc: 0.8947368264198303)
[2025-02-13 19:26:01,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:02,179][root][INFO] - Training Epoch: 1/2, step 1047/7134 completed (loss: 0.6725313663482666, acc: 0.8399999737739563)
[2025-02-13 19:26:02,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:02,602][root][INFO] - Training Epoch: 1/2, step 1048/7134 completed (loss: 0.48854881525039673, acc: 0.9044585824012756)
[2025-02-13 19:26:02,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:02,990][root][INFO] - Training Epoch: 1/2, step 1049/7134 completed (loss: 0.5315558314323425, acc: 0.9144737124443054)
[2025-02-13 19:26:03,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:03,384][root][INFO] - Training Epoch: 1/2, step 1050/7134 completed (loss: 0.5881927013397217, acc: 0.859375)
[2025-02-13 19:26:03,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:03,766][root][INFO] - Training Epoch: 1/2, step 1051/7134 completed (loss: 0.288146048784256, acc: 0.9085714221000671)
[2025-02-13 19:26:03,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:04,156][root][INFO] - Training Epoch: 1/2, step 1052/7134 completed (loss: 0.4656107425689697, acc: 0.8795180916786194)
[2025-02-13 19:26:04,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:04,532][root][INFO] - Training Epoch: 1/2, step 1053/7134 completed (loss: 0.28098881244659424, acc: 0.9281437397003174)
[2025-02-13 19:26:04,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:04,911][root][INFO] - Training Epoch: 1/2, step 1054/7134 completed (loss: 0.2598326504230499, acc: 0.9375)
[2025-02-13 19:26:05,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:05,270][root][INFO] - Training Epoch: 1/2, step 1055/7134 completed (loss: 0.19549888372421265, acc: 0.9465649127960205)
[2025-02-13 19:26:05,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:05,654][root][INFO] - Training Epoch: 1/2, step 1056/7134 completed (loss: 0.44076254963874817, acc: 0.895348846912384)
[2025-02-13 19:26:05,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:06,014][root][INFO] - Training Epoch: 1/2, step 1057/7134 completed (loss: 0.2263757735490799, acc: 0.9430052042007446)
[2025-02-13 19:26:06,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:06,399][root][INFO] - Training Epoch: 1/2, step 1058/7134 completed (loss: 0.22741582989692688, acc: 0.942105233669281)
[2025-02-13 19:26:06,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:06,804][root][INFO] - Training Epoch: 1/2, step 1059/7134 completed (loss: 0.33608171343803406, acc: 0.9383886456489563)
[2025-02-13 19:26:06,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:07,174][root][INFO] - Training Epoch: 1/2, step 1060/7134 completed (loss: 0.26768597960472107, acc: 0.9358974099159241)
[2025-02-13 19:26:07,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:07,618][root][INFO] - Training Epoch: 1/2, step 1061/7134 completed (loss: 0.265076220035553, acc: 0.9432989954948425)
[2025-02-13 19:26:07,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:08,004][root][INFO] - Training Epoch: 1/2, step 1062/7134 completed (loss: 0.2604731619358063, acc: 0.9457831382751465)
[2025-02-13 19:26:08,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:08,399][root][INFO] - Training Epoch: 1/2, step 1063/7134 completed (loss: 0.19100219011306763, acc: 0.9352940917015076)
[2025-02-13 19:26:08,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:08,791][root][INFO] - Training Epoch: 1/2, step 1064/7134 completed (loss: 0.4511376917362213, acc: 0.8918918967247009)
[2025-02-13 19:26:08,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:09,201][root][INFO] - Training Epoch: 1/2, step 1065/7134 completed (loss: 0.31777551770210266, acc: 0.9281768202781677)
[2025-02-13 19:26:09,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:09,594][root][INFO] - Training Epoch: 1/2, step 1066/7134 completed (loss: 0.48349741101264954, acc: 0.9083969593048096)
[2025-02-13 19:26:09,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:09,968][root][INFO] - Training Epoch: 1/2, step 1067/7134 completed (loss: 0.22362391650676727, acc: 0.9346405267715454)
[2025-02-13 19:26:10,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:10,333][root][INFO] - Training Epoch: 1/2, step 1068/7134 completed (loss: 0.3082404136657715, acc: 0.9375)
[2025-02-13 19:26:10,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:10,740][root][INFO] - Training Epoch: 1/2, step 1069/7134 completed (loss: 0.41181135177612305, acc: 0.8888888955116272)
[2025-02-13 19:26:10,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:11,105][root][INFO] - Training Epoch: 1/2, step 1070/7134 completed (loss: 0.24431636929512024, acc: 0.9520547986030579)
[2025-02-13 19:26:11,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:11,521][root][INFO] - Training Epoch: 1/2, step 1071/7134 completed (loss: 0.20740008354187012, acc: 0.9624999761581421)
[2025-02-13 19:26:11,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:11,894][root][INFO] - Training Epoch: 1/2, step 1072/7134 completed (loss: 0.08227984607219696, acc: 0.9756097793579102)
[2025-02-13 19:26:12,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:12,273][root][INFO] - Training Epoch: 1/2, step 1073/7134 completed (loss: 0.13555486500263214, acc: 0.9627329111099243)
[2025-02-13 19:26:12,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:12,637][root][INFO] - Training Epoch: 1/2, step 1074/7134 completed (loss: 0.3079994022846222, acc: 0.9290780425071716)
[2025-02-13 19:26:12,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:13,004][root][INFO] - Training Epoch: 1/2, step 1075/7134 completed (loss: 0.5885145664215088, acc: 0.8686131238937378)
[2025-02-13 19:26:13,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:13,372][root][INFO] - Training Epoch: 1/2, step 1076/7134 completed (loss: 0.5158092379570007, acc: 0.8670212626457214)
[2025-02-13 19:26:13,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:13,747][root][INFO] - Training Epoch: 1/2, step 1077/7134 completed (loss: 0.474982351064682, acc: 0.9115044474601746)
[2025-02-13 19:26:13,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:14,123][root][INFO] - Training Epoch: 1/2, step 1078/7134 completed (loss: 0.6312860250473022, acc: 0.8553459048271179)
[2025-02-13 19:26:14,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:14,503][root][INFO] - Training Epoch: 1/2, step 1079/7134 completed (loss: 0.3140838146209717, acc: 0.921658992767334)
[2025-02-13 19:26:14,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:14,890][root][INFO] - Training Epoch: 1/2, step 1080/7134 completed (loss: 0.26175811886787415, acc: 0.9534883499145508)
[2025-02-13 19:26:15,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:15,265][root][INFO] - Training Epoch: 1/2, step 1081/7134 completed (loss: 0.3276810646057129, acc: 0.9096385836601257)
[2025-02-13 19:26:15,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:15,690][root][INFO] - Training Epoch: 1/2, step 1082/7134 completed (loss: 0.09870875626802444, acc: 0.9775784611701965)
[2025-02-13 19:26:15,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:16,067][root][INFO] - Training Epoch: 1/2, step 1083/7134 completed (loss: 0.31124359369277954, acc: 0.9340101480484009)
[2025-02-13 19:26:16,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:16,445][root][INFO] - Training Epoch: 1/2, step 1084/7134 completed (loss: 0.2310619205236435, acc: 0.9202127456665039)
[2025-02-13 19:26:16,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:16,819][root][INFO] - Training Epoch: 1/2, step 1085/7134 completed (loss: 0.21977481245994568, acc: 0.9245283007621765)
[2025-02-13 19:26:16,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:17,177][root][INFO] - Training Epoch: 1/2, step 1086/7134 completed (loss: 0.5305847525596619, acc: 0.8709677457809448)
[2025-02-13 19:26:17,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:17,558][root][INFO] - Training Epoch: 1/2, step 1087/7134 completed (loss: 0.23006902635097504, acc: 0.9569892287254333)
[2025-02-13 19:26:17,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:17,947][root][INFO] - Training Epoch: 1/2, step 1088/7134 completed (loss: 0.19842447340488434, acc: 0.9545454382896423)
[2025-02-13 19:26:18,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:18,323][root][INFO] - Training Epoch: 1/2, step 1089/7134 completed (loss: 0.4788174629211426, acc: 0.8773006200790405)
[2025-02-13 19:26:18,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:18,712][root][INFO] - Training Epoch: 1/2, step 1090/7134 completed (loss: 0.4166279733181, acc: 0.886904776096344)
[2025-02-13 19:26:18,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:19,075][root][INFO] - Training Epoch: 1/2, step 1091/7134 completed (loss: 0.3702087998390198, acc: 0.9068322777748108)
[2025-02-13 19:26:19,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:19,468][root][INFO] - Training Epoch: 1/2, step 1092/7134 completed (loss: 0.1876433789730072, acc: 0.9580838084220886)
[2025-02-13 19:26:19,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:19,859][root][INFO] - Training Epoch: 1/2, step 1093/7134 completed (loss: 0.33132997155189514, acc: 0.918367326259613)
[2025-02-13 19:26:20,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:20,265][root][INFO] - Training Epoch: 1/2, step 1094/7134 completed (loss: 0.43719595670700073, acc: 0.8796992301940918)
[2025-02-13 19:26:20,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:20,705][root][INFO] - Training Epoch: 1/2, step 1095/7134 completed (loss: 0.27422335743904114, acc: 0.9312169551849365)
[2025-02-13 19:26:20,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:21,100][root][INFO] - Training Epoch: 1/2, step 1096/7134 completed (loss: 0.34302565455436707, acc: 0.9247311949729919)
[2025-02-13 19:26:21,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:21,491][root][INFO] - Training Epoch: 1/2, step 1097/7134 completed (loss: 0.4128792881965637, acc: 0.8961748480796814)
[2025-02-13 19:26:21,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:21,883][root][INFO] - Training Epoch: 1/2, step 1098/7134 completed (loss: 0.2307387888431549, acc: 0.9444444179534912)
[2025-02-13 19:26:22,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:22,273][root][INFO] - Training Epoch: 1/2, step 1099/7134 completed (loss: 0.3537074029445648, acc: 0.9166666865348816)
[2025-02-13 19:26:22,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:22,694][root][INFO] - Training Epoch: 1/2, step 1100/7134 completed (loss: 0.37254372239112854, acc: 0.9064327478408813)
[2025-02-13 19:26:22,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:23,075][root][INFO] - Training Epoch: 1/2, step 1101/7134 completed (loss: 0.34806138277053833, acc: 0.9179487228393555)
[2025-02-13 19:26:23,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:23,453][root][INFO] - Training Epoch: 1/2, step 1102/7134 completed (loss: 0.8272759318351746, acc: 0.8611111044883728)
[2025-02-13 19:26:23,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:23,833][root][INFO] - Training Epoch: 1/2, step 1103/7134 completed (loss: 0.634127140045166, acc: 0.875)
[2025-02-13 19:26:23,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:24,179][root][INFO] - Training Epoch: 1/2, step 1104/7134 completed (loss: 0.3314366340637207, acc: 0.9133333563804626)
[2025-02-13 19:26:24,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:24,550][root][INFO] - Training Epoch: 1/2, step 1105/7134 completed (loss: 0.3664872944355011, acc: 0.9320987462997437)
[2025-02-13 19:26:24,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:24,951][root][INFO] - Training Epoch: 1/2, step 1106/7134 completed (loss: 0.24867244064807892, acc: 0.9271523356437683)
[2025-02-13 19:26:25,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:25,324][root][INFO] - Training Epoch: 1/2, step 1107/7134 completed (loss: 0.22577521204948425, acc: 0.9388889074325562)
[2025-02-13 19:26:25,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:25,699][root][INFO] - Training Epoch: 1/2, step 1108/7134 completed (loss: 0.24113276600837708, acc: 0.9408283829689026)
[2025-02-13 19:26:25,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:26,109][root][INFO] - Training Epoch: 1/2, step 1109/7134 completed (loss: 0.3744068443775177, acc: 0.9017341136932373)
[2025-02-13 19:26:26,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:26,488][root][INFO] - Training Epoch: 1/2, step 1110/7134 completed (loss: 0.3198964595794678, acc: 0.9193548560142517)
[2025-02-13 19:26:26,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:26,871][root][INFO] - Training Epoch: 1/2, step 1111/7134 completed (loss: 0.2706616520881653, acc: 0.9375)
[2025-02-13 19:26:27,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:27,257][root][INFO] - Training Epoch: 1/2, step 1112/7134 completed (loss: 0.2138477861881256, acc: 0.9370629191398621)
[2025-02-13 19:26:27,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:27,623][root][INFO] - Training Epoch: 1/2, step 1113/7134 completed (loss: 0.2907102704048157, acc: 0.9245283007621765)
[2025-02-13 19:26:27,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:28,008][root][INFO] - Training Epoch: 1/2, step 1114/7134 completed (loss: 0.32190370559692383, acc: 0.9259259104728699)
[2025-02-13 19:26:28,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:28,452][root][INFO] - Training Epoch: 1/2, step 1115/7134 completed (loss: 0.2888401746749878, acc: 0.948387086391449)
[2025-02-13 19:26:28,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:28,856][root][INFO] - Training Epoch: 1/2, step 1116/7134 completed (loss: 0.24349910020828247, acc: 0.934959352016449)
[2025-02-13 19:26:28,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:29,271][root][INFO] - Training Epoch: 1/2, step 1117/7134 completed (loss: 0.14936189353466034, acc: 0.9605262875556946)
[2025-02-13 19:26:29,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:29,654][root][INFO] - Training Epoch: 1/2, step 1118/7134 completed (loss: 0.13909180462360382, acc: 0.9541984796524048)
[2025-02-13 19:26:29,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:30,042][root][INFO] - Training Epoch: 1/2, step 1119/7134 completed (loss: 0.17517922818660736, acc: 0.951724112033844)
[2025-02-13 19:26:30,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:30,446][root][INFO] - Training Epoch: 1/2, step 1120/7134 completed (loss: 0.36753156781196594, acc: 0.9083969593048096)
[2025-02-13 19:26:30,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:30,856][root][INFO] - Training Epoch: 1/2, step 1121/7134 completed (loss: 0.18875719606876373, acc: 0.948387086391449)
[2025-02-13 19:26:30,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:31,233][root][INFO] - Training Epoch: 1/2, step 1122/7134 completed (loss: 0.09852461516857147, acc: 0.9769230484962463)
[2025-02-13 19:26:31,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:31,608][root][INFO] - Training Epoch: 1/2, step 1123/7134 completed (loss: 0.18791519105434418, acc: 0.951724112033844)
[2025-02-13 19:26:31,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:31,990][root][INFO] - Training Epoch: 1/2, step 1124/7134 completed (loss: 0.24016927182674408, acc: 0.9189189076423645)
[2025-02-13 19:26:32,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:32,362][root][INFO] - Training Epoch: 1/2, step 1125/7134 completed (loss: 0.15991029143333435, acc: 0.9583333134651184)
[2025-02-13 19:26:32,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:32,731][root][INFO] - Training Epoch: 1/2, step 1126/7134 completed (loss: 0.16651229560375214, acc: 0.9606741666793823)
[2025-02-13 19:26:32,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:33,101][root][INFO] - Training Epoch: 1/2, step 1127/7134 completed (loss: 0.14371474087238312, acc: 0.9779411554336548)
[2025-02-13 19:26:33,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:33,469][root][INFO] - Training Epoch: 1/2, step 1128/7134 completed (loss: 0.22407016158103943, acc: 0.9236640930175781)
[2025-02-13 19:26:33,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:33,851][root][INFO] - Training Epoch: 1/2, step 1129/7134 completed (loss: 0.28533700108528137, acc: 0.9083333611488342)
[2025-02-13 19:26:33,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:34,216][root][INFO] - Training Epoch: 1/2, step 1130/7134 completed (loss: 0.14563339948654175, acc: 0.9640287756919861)
[2025-02-13 19:26:34,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:34,605][root][INFO] - Training Epoch: 1/2, step 1131/7134 completed (loss: 0.3374953866004944, acc: 0.9242424368858337)
[2025-02-13 19:26:34,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:34,980][root][INFO] - Training Epoch: 1/2, step 1132/7134 completed (loss: 0.6618435382843018, acc: 0.8456375598907471)
[2025-02-13 19:26:35,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:35,355][root][INFO] - Training Epoch: 1/2, step 1133/7134 completed (loss: 0.42312684655189514, acc: 0.9075144529342651)
[2025-02-13 19:26:35,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:35,725][root][INFO] - Training Epoch: 1/2, step 1134/7134 completed (loss: 0.42163220047950745, acc: 0.9120879173278809)
[2025-02-13 19:26:35,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:36,115][root][INFO] - Training Epoch: 1/2, step 1135/7134 completed (loss: 0.3740862309932709, acc: 0.899328887462616)
[2025-02-13 19:26:36,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:36,523][root][INFO] - Training Epoch: 1/2, step 1136/7134 completed (loss: 0.48777782917022705, acc: 0.8928571343421936)
[2025-02-13 19:26:36,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:36,929][root][INFO] - Training Epoch: 1/2, step 1137/7134 completed (loss: 0.4106045067310333, acc: 0.8962264060974121)
[2025-02-13 19:26:37,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:37,317][root][INFO] - Training Epoch: 1/2, step 1138/7134 completed (loss: 0.3883349597454071, acc: 0.9150000214576721)
[2025-02-13 19:26:37,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:37,690][root][INFO] - Training Epoch: 1/2, step 1139/7134 completed (loss: 0.43587666749954224, acc: 0.8901098966598511)
[2025-02-13 19:26:37,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:38,058][root][INFO] - Training Epoch: 1/2, step 1140/7134 completed (loss: 0.14351233839988708, acc: 0.9578947424888611)
[2025-02-13 19:26:38,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:38,429][root][INFO] - Training Epoch: 1/2, step 1141/7134 completed (loss: 0.1486557126045227, acc: 0.9818181991577148)
[2025-02-13 19:26:38,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:38,779][root][INFO] - Training Epoch: 1/2, step 1142/7134 completed (loss: 0.20882637798786163, acc: 0.949999988079071)
[2025-02-13 19:26:38,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:39,150][root][INFO] - Training Epoch: 1/2, step 1143/7134 completed (loss: 0.40028175711631775, acc: 0.9277108311653137)
[2025-02-13 19:26:39,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:39,521][root][INFO] - Training Epoch: 1/2, step 1144/7134 completed (loss: 0.35643309354782104, acc: 0.9226190447807312)
[2025-02-13 19:26:39,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:39,896][root][INFO] - Training Epoch: 1/2, step 1145/7134 completed (loss: 0.1710217148065567, acc: 0.9608938694000244)
[2025-02-13 19:26:40,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:40,249][root][INFO] - Training Epoch: 1/2, step 1146/7134 completed (loss: 0.08195342868566513, acc: 0.9924242496490479)
[2025-02-13 19:26:40,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:40,604][root][INFO] - Training Epoch: 1/2, step 1147/7134 completed (loss: 0.19394424557685852, acc: 0.9621621370315552)
[2025-02-13 19:26:40,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:40,971][root][INFO] - Training Epoch: 1/2, step 1148/7134 completed (loss: 0.1343785673379898, acc: 0.971222996711731)
[2025-02-13 19:26:41,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:41,343][root][INFO] - Training Epoch: 1/2, step 1149/7134 completed (loss: 0.41417384147644043, acc: 0.9236111044883728)
[2025-02-13 19:26:41,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:41,704][root][INFO] - Training Epoch: 1/2, step 1150/7134 completed (loss: 0.19323159754276276, acc: 0.9477124214172363)
[2025-02-13 19:26:41,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:42,052][root][INFO] - Training Epoch: 1/2, step 1151/7134 completed (loss: 0.3117009103298187, acc: 0.9327731132507324)
[2025-02-13 19:26:42,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:42,410][root][INFO] - Training Epoch: 1/2, step 1152/7134 completed (loss: 0.19541426002979279, acc: 0.9596773982048035)
[2025-02-13 19:26:42,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:42,765][root][INFO] - Training Epoch: 1/2, step 1153/7134 completed (loss: 0.25951099395751953, acc: 0.9411764740943909)
[2025-02-13 19:26:42,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:43,130][root][INFO] - Training Epoch: 1/2, step 1154/7134 completed (loss: 0.2681899666786194, acc: 0.9609375)
[2025-02-13 19:26:43,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:43,525][root][INFO] - Training Epoch: 1/2, step 1155/7134 completed (loss: 0.5188761353492737, acc: 0.8547008633613586)
[2025-02-13 19:26:43,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:43,903][root][INFO] - Training Epoch: 1/2, step 1156/7134 completed (loss: 0.18260051310062408, acc: 0.9545454382896423)
[2025-02-13 19:26:44,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:44,288][root][INFO] - Training Epoch: 1/2, step 1157/7134 completed (loss: 0.4714239239692688, acc: 0.9172413945198059)
[2025-02-13 19:26:44,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:44,659][root][INFO] - Training Epoch: 1/2, step 1158/7134 completed (loss: 0.393544465303421, acc: 0.939130425453186)
[2025-02-13 19:26:44,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:45,044][root][INFO] - Training Epoch: 1/2, step 1159/7134 completed (loss: 0.24206037819385529, acc: 0.9281437397003174)
[2025-02-13 19:26:45,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:45,430][root][INFO] - Training Epoch: 1/2, step 1160/7134 completed (loss: 0.1429983377456665, acc: 0.9567901492118835)
[2025-02-13 19:26:45,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:45,794][root][INFO] - Training Epoch: 1/2, step 1161/7134 completed (loss: 0.1305006891489029, acc: 0.9855072498321533)
[2025-02-13 19:26:45,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:46,171][root][INFO] - Training Epoch: 1/2, step 1162/7134 completed (loss: 0.29120343923568726, acc: 0.9240506291389465)
[2025-02-13 19:26:46,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:46,544][root][INFO] - Training Epoch: 1/2, step 1163/7134 completed (loss: 0.2364429533481598, acc: 0.931034505367279)
[2025-02-13 19:26:46,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:46,906][root][INFO] - Training Epoch: 1/2, step 1164/7134 completed (loss: 0.24264146387577057, acc: 0.9171270728111267)
[2025-02-13 19:26:47,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:47,291][root][INFO] - Training Epoch: 1/2, step 1165/7134 completed (loss: 0.1330961436033249, acc: 0.9692307710647583)
[2025-02-13 19:26:47,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:47,657][root][INFO] - Training Epoch: 1/2, step 1166/7134 completed (loss: 0.19968348741531372, acc: 0.936170220375061)
[2025-02-13 19:26:47,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:48,052][root][INFO] - Training Epoch: 1/2, step 1167/7134 completed (loss: 0.1306484043598175, acc: 0.9440000057220459)
[2025-02-13 19:26:48,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:48,472][root][INFO] - Training Epoch: 1/2, step 1168/7134 completed (loss: 0.15969650447368622, acc: 0.9788359999656677)
[2025-02-13 19:26:48,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:48,869][root][INFO] - Training Epoch: 1/2, step 1169/7134 completed (loss: 0.26915833353996277, acc: 0.9627659320831299)
[2025-02-13 19:26:49,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:49,274][root][INFO] - Training Epoch: 1/2, step 1170/7134 completed (loss: 0.3116450011730194, acc: 0.9378530979156494)
[2025-02-13 19:26:49,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:49,674][root][INFO] - Training Epoch: 1/2, step 1171/7134 completed (loss: 0.2077884078025818, acc: 0.9399999976158142)
[2025-02-13 19:26:49,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:50,028][root][INFO] - Training Epoch: 1/2, step 1172/7134 completed (loss: 0.1526978760957718, acc: 0.9404761791229248)
[2025-02-13 19:26:50,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:50,393][root][INFO] - Training Epoch: 1/2, step 1173/7134 completed (loss: 0.17518861591815948, acc: 0.9576719403266907)
[2025-02-13 19:26:50,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:50,762][root][INFO] - Training Epoch: 1/2, step 1174/7134 completed (loss: 0.1647452861070633, acc: 0.9570552110671997)
[2025-02-13 19:26:50,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:51,153][root][INFO] - Training Epoch: 1/2, step 1175/7134 completed (loss: 0.21842063963413239, acc: 0.9411764740943909)
[2025-02-13 19:26:51,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:51,522][root][INFO] - Training Epoch: 1/2, step 1176/7134 completed (loss: 0.1514812558889389, acc: 0.9652777910232544)
[2025-02-13 19:26:51,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:51,873][root][INFO] - Training Epoch: 1/2, step 1177/7134 completed (loss: 0.06957422196865082, acc: 0.9846153855323792)
[2025-02-13 19:26:52,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:52,226][root][INFO] - Training Epoch: 1/2, step 1178/7134 completed (loss: 0.12951427698135376, acc: 0.9807692170143127)
[2025-02-13 19:26:52,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:52,610][root][INFO] - Training Epoch: 1/2, step 1179/7134 completed (loss: 0.09424056112766266, acc: 0.9838709831237793)
[2025-02-13 19:26:52,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:53,004][root][INFO] - Training Epoch: 1/2, step 1180/7134 completed (loss: 0.24177120625972748, acc: 0.9527027010917664)
[2025-02-13 19:26:53,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:53,386][root][INFO] - Training Epoch: 1/2, step 1181/7134 completed (loss: 0.16315925121307373, acc: 0.9615384340286255)
[2025-02-13 19:26:53,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:53,749][root][INFO] - Training Epoch: 1/2, step 1182/7134 completed (loss: 0.13472974300384521, acc: 0.9795918464660645)
[2025-02-13 19:26:53,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:54,143][root][INFO] - Training Epoch: 1/2, step 1183/7134 completed (loss: 0.0760934129357338, acc: 0.9826589822769165)
[2025-02-13 19:26:54,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:54,521][root][INFO] - Training Epoch: 1/2, step 1184/7134 completed (loss: 0.1787572056055069, acc: 0.9653179049491882)
[2025-02-13 19:26:54,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:54,936][root][INFO] - Training Epoch: 1/2, step 1185/7134 completed (loss: 0.08788864314556122, acc: 0.9818181991577148)
[2025-02-13 19:26:55,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:55,315][root][INFO] - Training Epoch: 1/2, step 1186/7134 completed (loss: 0.1049603596329689, acc: 0.967391312122345)
[2025-02-13 19:26:55,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:55,691][root][INFO] - Training Epoch: 1/2, step 1187/7134 completed (loss: 0.11537791788578033, acc: 0.9702380895614624)
[2025-02-13 19:26:55,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:56,058][root][INFO] - Training Epoch: 1/2, step 1188/7134 completed (loss: 0.09100900590419769, acc: 0.9677419066429138)
[2025-02-13 19:26:56,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:56,426][root][INFO] - Training Epoch: 1/2, step 1189/7134 completed (loss: 0.13400815427303314, acc: 0.976331353187561)
[2025-02-13 19:26:56,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:56,797][root][INFO] - Training Epoch: 1/2, step 1190/7134 completed (loss: 0.22736483812332153, acc: 0.9666666388511658)
[2025-02-13 19:26:56,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:57,174][root][INFO] - Training Epoch: 1/2, step 1191/7134 completed (loss: 0.27317801117897034, acc: 0.9387755393981934)
[2025-02-13 19:26:57,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:57,540][root][INFO] - Training Epoch: 1/2, step 1192/7134 completed (loss: 0.22672447562217712, acc: 0.9398496150970459)
[2025-02-13 19:26:57,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:57,925][root][INFO] - Training Epoch: 1/2, step 1193/7134 completed (loss: 0.2684759795665741, acc: 0.9448819160461426)
[2025-02-13 19:26:58,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:58,322][root][INFO] - Training Epoch: 1/2, step 1194/7134 completed (loss: 0.16616640985012054, acc: 0.9798657894134521)
[2025-02-13 19:26:58,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:58,722][root][INFO] - Training Epoch: 1/2, step 1195/7134 completed (loss: 0.2710137963294983, acc: 0.8999999761581421)
[2025-02-13 19:26:58,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:59,138][root][INFO] - Training Epoch: 1/2, step 1196/7134 completed (loss: 0.3441654443740845, acc: 0.9290780425071716)
[2025-02-13 19:26:59,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:59,495][root][INFO] - Training Epoch: 1/2, step 1197/7134 completed (loss: 0.26847097277641296, acc: 0.9246575236320496)
[2025-02-13 19:26:59,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:59,862][root][INFO] - Training Epoch: 1/2, step 1198/7134 completed (loss: 0.27104130387306213, acc: 0.925000011920929)
[2025-02-13 19:27:00,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:00,284][root][INFO] - Training Epoch: 1/2, step 1199/7134 completed (loss: 0.2603982388973236, acc: 0.9350649118423462)
[2025-02-13 19:27:00,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:00,682][root][INFO] - Training Epoch: 1/2, step 1200/7134 completed (loss: 0.11766742169857025, acc: 0.9532710313796997)
[2025-02-13 19:27:00,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:01,086][root][INFO] - Training Epoch: 1/2, step 1201/7134 completed (loss: 0.3471482992172241, acc: 0.9182389974594116)
[2025-02-13 19:27:01,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:01,475][root][INFO] - Training Epoch: 1/2, step 1202/7134 completed (loss: 0.41367459297180176, acc: 0.89673912525177)
[2025-02-13 19:27:01,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:01,832][root][INFO] - Training Epoch: 1/2, step 1203/7134 completed (loss: 0.21346963942050934, acc: 0.95333331823349)
[2025-02-13 19:27:01,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:02,174][root][INFO] - Training Epoch: 1/2, step 1204/7134 completed (loss: 0.47396254539489746, acc: 0.904411792755127)
[2025-02-13 19:27:02,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:02,537][root][INFO] - Training Epoch: 1/2, step 1205/7134 completed (loss: 0.47874313592910767, acc: 0.8908045887947083)
[2025-02-13 19:27:02,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:02,919][root][INFO] - Training Epoch: 1/2, step 1206/7134 completed (loss: 0.41460248827934265, acc: 0.9145728349685669)
[2025-02-13 19:27:03,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:03,315][root][INFO] - Training Epoch: 1/2, step 1207/7134 completed (loss: 0.26472288370132446, acc: 0.9247311949729919)
[2025-02-13 19:27:03,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:03,673][root][INFO] - Training Epoch: 1/2, step 1208/7134 completed (loss: 0.4219808876514435, acc: 0.9160305261611938)
[2025-02-13 19:27:03,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:04,062][root][INFO] - Training Epoch: 1/2, step 1209/7134 completed (loss: 0.36741602420806885, acc: 0.9016393423080444)
[2025-02-13 19:27:04,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:04,447][root][INFO] - Training Epoch: 1/2, step 1210/7134 completed (loss: 0.5816527605056763, acc: 0.8533333539962769)
[2025-02-13 19:27:04,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:04,805][root][INFO] - Training Epoch: 1/2, step 1211/7134 completed (loss: 1.0004637241363525, acc: 0.7841726541519165)
[2025-02-13 19:27:04,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:05,149][root][INFO] - Training Epoch: 1/2, step 1212/7134 completed (loss: 0.6314698457717896, acc: 0.8394160866737366)
[2025-02-13 19:27:05,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:05,513][root][INFO] - Training Epoch: 1/2, step 1213/7134 completed (loss: 0.47231197357177734, acc: 0.88165682554245)
[2025-02-13 19:27:05,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:05,873][root][INFO] - Training Epoch: 1/2, step 1214/7134 completed (loss: 0.5533509254455566, acc: 0.846666693687439)
[2025-02-13 19:27:06,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:06,233][root][INFO] - Training Epoch: 1/2, step 1215/7134 completed (loss: 0.5475478172302246, acc: 0.8764705657958984)
[2025-02-13 19:27:06,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:06,604][root][INFO] - Training Epoch: 1/2, step 1216/7134 completed (loss: 0.2785843312740326, acc: 0.9470198750495911)
[2025-02-13 19:27:06,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:06,979][root][INFO] - Training Epoch: 1/2, step 1217/7134 completed (loss: 0.3045932352542877, acc: 0.9276315569877625)
[2025-02-13 19:27:07,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:07,349][root][INFO] - Training Epoch: 1/2, step 1218/7134 completed (loss: 0.5176156163215637, acc: 0.887499988079071)
[2025-02-13 19:27:07,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:07,711][root][INFO] - Training Epoch: 1/2, step 1219/7134 completed (loss: 0.5187327861785889, acc: 0.8666666746139526)
[2025-02-13 19:27:07,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:08,072][root][INFO] - Training Epoch: 1/2, step 1220/7134 completed (loss: 0.3786112070083618, acc: 0.9329897165298462)
[2025-02-13 19:27:08,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:08,449][root][INFO] - Training Epoch: 1/2, step 1221/7134 completed (loss: 0.34093591570854187, acc: 0.9140271544456482)
[2025-02-13 19:27:08,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:08,811][root][INFO] - Training Epoch: 1/2, step 1222/7134 completed (loss: 0.5025856494903564, acc: 0.893203854560852)
[2025-02-13 19:27:08,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:09,168][root][INFO] - Training Epoch: 1/2, step 1223/7134 completed (loss: 0.47109776735305786, acc: 0.8920454382896423)
[2025-02-13 19:27:09,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:09,539][root][INFO] - Training Epoch: 1/2, step 1224/7134 completed (loss: 0.493713915348053, acc: 0.9056603908538818)
[2025-02-13 19:27:09,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:09,939][root][INFO] - Training Epoch: 1/2, step 1225/7134 completed (loss: 0.40229928493499756, acc: 0.9178082346916199)
[2025-02-13 19:27:10,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:10,327][root][INFO] - Training Epoch: 1/2, step 1226/7134 completed (loss: 0.6726662516593933, acc: 0.8492063283920288)
[2025-02-13 19:27:10,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:10,745][root][INFO] - Training Epoch: 1/2, step 1227/7134 completed (loss: 0.9144574999809265, acc: 0.7835051417350769)
[2025-02-13 19:27:10,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:11,122][root][INFO] - Training Epoch: 1/2, step 1228/7134 completed (loss: 0.40175166726112366, acc: 0.895348846912384)
[2025-02-13 19:27:11,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:11,498][root][INFO] - Training Epoch: 1/2, step 1229/7134 completed (loss: 0.3733764886856079, acc: 0.9108911156654358)
[2025-02-13 19:27:11,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:11,884][root][INFO] - Training Epoch: 1/2, step 1230/7134 completed (loss: 0.3790593147277832, acc: 0.8962264060974121)
[2025-02-13 19:27:12,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:12,316][root][INFO] - Training Epoch: 1/2, step 1231/7134 completed (loss: 0.366656094789505, acc: 0.9008620977401733)
[2025-02-13 19:27:12,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:12,760][root][INFO] - Training Epoch: 1/2, step 1232/7134 completed (loss: 0.42533406615257263, acc: 0.8968609571456909)
[2025-02-13 19:27:12,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:13,173][root][INFO] - Training Epoch: 1/2, step 1233/7134 completed (loss: 0.6631256341934204, acc: 0.8552036285400391)
[2025-02-13 19:27:13,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:13,585][root][INFO] - Training Epoch: 1/2, step 1234/7134 completed (loss: 0.2855101227760315, acc: 0.9294871687889099)
[2025-02-13 19:27:13,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:13,978][root][INFO] - Training Epoch: 1/2, step 1235/7134 completed (loss: 0.3706909120082855, acc: 0.9081632494926453)
[2025-02-13 19:27:14,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:14,357][root][INFO] - Training Epoch: 1/2, step 1236/7134 completed (loss: 0.6814921498298645, acc: 0.8529411554336548)
[2025-02-13 19:27:14,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:14,734][root][INFO] - Training Epoch: 1/2, step 1237/7134 completed (loss: 0.4334331750869751, acc: 0.9139072895050049)
[2025-02-13 19:27:14,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:15,102][root][INFO] - Training Epoch: 1/2, step 1238/7134 completed (loss: 1.008974313735962, acc: 0.7767441868782043)
[2025-02-13 19:27:15,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:15,484][root][INFO] - Training Epoch: 1/2, step 1239/7134 completed (loss: 0.5473060607910156, acc: 0.8719512224197388)
[2025-02-13 19:27:15,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:15,854][root][INFO] - Training Epoch: 1/2, step 1240/7134 completed (loss: 0.5094529390335083, acc: 0.8430232405662537)
[2025-02-13 19:27:15,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:16,191][root][INFO] - Training Epoch: 1/2, step 1241/7134 completed (loss: 0.380481481552124, acc: 0.868852436542511)
[2025-02-13 19:27:16,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:16,567][root][INFO] - Training Epoch: 1/2, step 1242/7134 completed (loss: 0.5634238123893738, acc: 0.8638743162155151)
[2025-02-13 19:27:16,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:16,949][root][INFO] - Training Epoch: 1/2, step 1243/7134 completed (loss: 0.474946528673172, acc: 0.8991228342056274)
[2025-02-13 19:27:17,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:17,322][root][INFO] - Training Epoch: 1/2, step 1244/7134 completed (loss: 0.3689460754394531, acc: 0.9014084339141846)
[2025-02-13 19:27:17,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:17,695][root][INFO] - Training Epoch: 1/2, step 1245/7134 completed (loss: 0.40317273139953613, acc: 0.9040403962135315)
[2025-02-13 19:27:17,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:18,061][root][INFO] - Training Epoch: 1/2, step 1246/7134 completed (loss: 0.5535405874252319, acc: 0.8631578683853149)
[2025-02-13 19:27:18,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:18,448][root][INFO] - Training Epoch: 1/2, step 1247/7134 completed (loss: 0.3080207407474518, acc: 0.9277108311653137)
[2025-02-13 19:27:18,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:18,816][root][INFO] - Training Epoch: 1/2, step 1248/7134 completed (loss: 0.47475048899650574, acc: 0.8999999761581421)
[2025-02-13 19:27:18,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:19,185][root][INFO] - Training Epoch: 1/2, step 1249/7134 completed (loss: 0.34872862696647644, acc: 0.9137930870056152)
[2025-02-13 19:27:19,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:19,550][root][INFO] - Training Epoch: 1/2, step 1250/7134 completed (loss: 0.2407550811767578, acc: 0.9466666579246521)
[2025-02-13 19:27:19,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:19,924][root][INFO] - Training Epoch: 1/2, step 1251/7134 completed (loss: 0.43354564905166626, acc: 0.8666666746139526)
[2025-02-13 19:27:20,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:20,356][root][INFO] - Training Epoch: 1/2, step 1252/7134 completed (loss: 0.25395381450653076, acc: 0.9763779640197754)
[2025-02-13 19:27:20,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:20,750][root][INFO] - Training Epoch: 1/2, step 1253/7134 completed (loss: 0.6100517511367798, acc: 0.8461538553237915)
[2025-02-13 19:27:20,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:21,141][root][INFO] - Training Epoch: 1/2, step 1254/7134 completed (loss: 0.4358345568180084, acc: 0.8799999952316284)
[2025-02-13 19:27:21,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:21,543][root][INFO] - Training Epoch: 1/2, step 1255/7134 completed (loss: 0.4429489076137543, acc: 0.8965517282485962)
[2025-02-13 19:27:21,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:21,944][root][INFO] - Training Epoch: 1/2, step 1256/7134 completed (loss: 0.3513909876346588, acc: 0.9083969593048096)
[2025-02-13 19:27:22,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:22,335][root][INFO] - Training Epoch: 1/2, step 1257/7134 completed (loss: 0.30546995997428894, acc: 0.9007633328437805)
[2025-02-13 19:27:22,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:22,702][root][INFO] - Training Epoch: 1/2, step 1258/7134 completed (loss: 0.41402339935302734, acc: 0.9047619104385376)
[2025-02-13 19:27:22,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:23,072][root][INFO] - Training Epoch: 1/2, step 1259/7134 completed (loss: 0.524308979511261, acc: 0.8557692170143127)
[2025-02-13 19:27:23,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:23,441][root][INFO] - Training Epoch: 1/2, step 1260/7134 completed (loss: 0.24607478082180023, acc: 0.9530201554298401)
[2025-02-13 19:27:23,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:23,844][root][INFO] - Training Epoch: 1/2, step 1261/7134 completed (loss: 0.27854013442993164, acc: 0.9256756901741028)
[2025-02-13 19:27:23,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:24,209][root][INFO] - Training Epoch: 1/2, step 1262/7134 completed (loss: 0.09037958830595016, acc: 0.9922480583190918)
[2025-02-13 19:27:24,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:24,573][root][INFO] - Training Epoch: 1/2, step 1263/7134 completed (loss: 0.2156643271446228, acc: 0.9548386931419373)
[2025-02-13 19:27:24,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:24,965][root][INFO] - Training Epoch: 1/2, step 1264/7134 completed (loss: 0.2213158756494522, acc: 0.9481481313705444)
[2025-02-13 19:27:25,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:25,416][root][INFO] - Training Epoch: 1/2, step 1265/7134 completed (loss: 0.24719086289405823, acc: 0.9428571462631226)
[2025-02-13 19:27:25,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:25,830][root][INFO] - Training Epoch: 1/2, step 1266/7134 completed (loss: 0.26278865337371826, acc: 0.9465649127960205)
[2025-02-13 19:27:25,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:26,193][root][INFO] - Training Epoch: 1/2, step 1267/7134 completed (loss: 0.28320005536079407, acc: 0.9285714030265808)
[2025-02-13 19:27:26,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:26,605][root][INFO] - Training Epoch: 1/2, step 1268/7134 completed (loss: 0.34940433502197266, acc: 0.9259259104728699)
[2025-02-13 19:27:26,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:27,010][root][INFO] - Training Epoch: 1/2, step 1269/7134 completed (loss: 0.5620083212852478, acc: 0.8636363744735718)
[2025-02-13 19:27:27,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:27,404][root][INFO] - Training Epoch: 1/2, step 1270/7134 completed (loss: 0.291469007730484, acc: 0.9246575236320496)
[2025-02-13 19:27:27,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:27,788][root][INFO] - Training Epoch: 1/2, step 1271/7134 completed (loss: 0.35344046354293823, acc: 0.925000011920929)
[2025-02-13 19:27:27,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:28,203][root][INFO] - Training Epoch: 1/2, step 1272/7134 completed (loss: 0.18526935577392578, acc: 0.955974817276001)
[2025-02-13 19:27:28,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:28,627][root][INFO] - Training Epoch: 1/2, step 1273/7134 completed (loss: 0.31280601024627686, acc: 0.9037036895751953)
[2025-02-13 19:27:28,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:29,037][root][INFO] - Training Epoch: 1/2, step 1274/7134 completed (loss: 0.09067589044570923, acc: 0.9834710955619812)
[2025-02-13 19:27:29,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:29,422][root][INFO] - Training Epoch: 1/2, step 1275/7134 completed (loss: 0.34931254386901855, acc: 0.9130434989929199)
[2025-02-13 19:27:29,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:29,839][root][INFO] - Training Epoch: 1/2, step 1276/7134 completed (loss: 0.15436670184135437, acc: 0.9696969985961914)
[2025-02-13 19:27:29,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:30,210][root][INFO] - Training Epoch: 1/2, step 1277/7134 completed (loss: 0.12236972898244858, acc: 0.9647887349128723)
[2025-02-13 19:27:30,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:30,596][root][INFO] - Training Epoch: 1/2, step 1278/7134 completed (loss: 0.454601913690567, acc: 0.875)
[2025-02-13 19:27:30,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:30,970][root][INFO] - Training Epoch: 1/2, step 1279/7134 completed (loss: 0.27605944871902466, acc: 0.929347813129425)
[2025-02-13 19:27:31,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:31,353][root][INFO] - Training Epoch: 1/2, step 1280/7134 completed (loss: 0.3307522237300873, acc: 0.9304812550544739)
[2025-02-13 19:27:31,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:31,750][root][INFO] - Training Epoch: 1/2, step 1281/7134 completed (loss: 0.33700767159461975, acc: 0.9015151262283325)
[2025-02-13 19:27:31,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:32,149][root][INFO] - Training Epoch: 1/2, step 1282/7134 completed (loss: 0.6431187987327576, acc: 0.8797814249992371)
[2025-02-13 19:27:32,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:32,535][root][INFO] - Training Epoch: 1/2, step 1283/7134 completed (loss: 0.2574407160282135, acc: 0.9379844665527344)
[2025-02-13 19:27:32,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:32,923][root][INFO] - Training Epoch: 1/2, step 1284/7134 completed (loss: 0.31495121121406555, acc: 0.916167676448822)
[2025-02-13 19:27:33,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:33,327][root][INFO] - Training Epoch: 1/2, step 1285/7134 completed (loss: 0.22988763451576233, acc: 0.9750000238418579)
[2025-02-13 19:27:33,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:33,732][root][INFO] - Training Epoch: 1/2, step 1286/7134 completed (loss: 0.3189316391944885, acc: 0.9329268336296082)
[2025-02-13 19:27:33,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:34,126][root][INFO] - Training Epoch: 1/2, step 1287/7134 completed (loss: 0.2631233334541321, acc: 0.9111111164093018)
[2025-02-13 19:27:34,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:34,490][root][INFO] - Training Epoch: 1/2, step 1288/7134 completed (loss: 0.28545111417770386, acc: 0.9170984625816345)
[2025-02-13 19:27:34,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:34,846][root][INFO] - Training Epoch: 1/2, step 1289/7134 completed (loss: 0.40810877084732056, acc: 0.9379310607910156)
[2025-02-13 19:27:34,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:35,215][root][INFO] - Training Epoch: 1/2, step 1290/7134 completed (loss: 0.1615360826253891, acc: 0.9580838084220886)
[2025-02-13 19:27:35,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:35,606][root][INFO] - Training Epoch: 1/2, step 1291/7134 completed (loss: 0.27141058444976807, acc: 0.9235293865203857)
[2025-02-13 19:27:35,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:35,986][root][INFO] - Training Epoch: 1/2, step 1292/7134 completed (loss: 0.23965680599212646, acc: 0.9395604133605957)
[2025-02-13 19:27:36,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:36,358][root][INFO] - Training Epoch: 1/2, step 1293/7134 completed (loss: 0.2325121909379959, acc: 0.9384615421295166)
[2025-02-13 19:27:36,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:36,727][root][INFO] - Training Epoch: 1/2, step 1294/7134 completed (loss: 0.24301423132419586, acc: 0.9398906826972961)
[2025-02-13 19:27:36,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:37,104][root][INFO] - Training Epoch: 1/2, step 1295/7134 completed (loss: 0.20165131986141205, acc: 0.9626865386962891)
[2025-02-13 19:27:37,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:37,479][root][INFO] - Training Epoch: 1/2, step 1296/7134 completed (loss: 0.19135960936546326, acc: 0.9615384340286255)
[2025-02-13 19:27:37,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:37,862][root][INFO] - Training Epoch: 1/2, step 1297/7134 completed (loss: 0.27853187918663025, acc: 0.9107142686843872)
[2025-02-13 19:27:38,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:38,256][root][INFO] - Training Epoch: 1/2, step 1298/7134 completed (loss: 0.18958064913749695, acc: 0.9669421315193176)
[2025-02-13 19:27:38,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:38,617][root][INFO] - Training Epoch: 1/2, step 1299/7134 completed (loss: 0.19624799489974976, acc: 0.9518072009086609)
[2025-02-13 19:27:38,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:39,039][root][INFO] - Training Epoch: 1/2, step 1300/7134 completed (loss: 0.19356326758861542, acc: 0.9504950642585754)
[2025-02-13 19:27:39,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:39,414][root][INFO] - Training Epoch: 1/2, step 1301/7134 completed (loss: 0.1627458781003952, acc: 0.9473684430122375)
[2025-02-13 19:27:39,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:39,806][root][INFO] - Training Epoch: 1/2, step 1302/7134 completed (loss: 0.23666365444660187, acc: 0.9306930899620056)
[2025-02-13 19:27:39,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:40,223][root][INFO] - Training Epoch: 1/2, step 1303/7134 completed (loss: 0.25402018427848816, acc: 0.9100000262260437)
[2025-02-13 19:27:40,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:40,630][root][INFO] - Training Epoch: 1/2, step 1304/7134 completed (loss: 0.2978517711162567, acc: 0.9435028433799744)
[2025-02-13 19:27:40,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:41,016][root][INFO] - Training Epoch: 1/2, step 1305/7134 completed (loss: 0.3131793439388275, acc: 0.9020618796348572)
[2025-02-13 19:27:41,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:41,440][root][INFO] - Training Epoch: 1/2, step 1306/7134 completed (loss: 0.25640368461608887, acc: 0.9605262875556946)
[2025-02-13 19:27:41,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:41,814][root][INFO] - Training Epoch: 1/2, step 1307/7134 completed (loss: 0.20505847036838531, acc: 0.9496402740478516)
[2025-02-13 19:27:41,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:42,220][root][INFO] - Training Epoch: 1/2, step 1308/7134 completed (loss: 0.27249646186828613, acc: 0.9290322661399841)
[2025-02-13 19:27:42,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:42,597][root][INFO] - Training Epoch: 1/2, step 1309/7134 completed (loss: 0.25486519932746887, acc: 0.9227052927017212)
[2025-02-13 19:27:42,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:42,974][root][INFO] - Training Epoch: 1/2, step 1310/7134 completed (loss: 0.45250988006591797, acc: 0.8674033284187317)
[2025-02-13 19:27:43,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:43,380][root][INFO] - Training Epoch: 1/2, step 1311/7134 completed (loss: 0.23202157020568848, acc: 0.9230769276618958)
[2025-02-13 19:27:43,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:43,818][root][INFO] - Training Epoch: 1/2, step 1312/7134 completed (loss: 0.5176169872283936, acc: 0.9130434989929199)
[2025-02-13 19:27:43,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:44,229][root][INFO] - Training Epoch: 1/2, step 1313/7134 completed (loss: 0.43643757700920105, acc: 0.9197530746459961)
[2025-02-13 19:27:44,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:44,618][root][INFO] - Training Epoch: 1/2, step 1314/7134 completed (loss: 0.3257597088813782, acc: 0.9202898740768433)
[2025-02-13 19:27:44,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:44,985][root][INFO] - Training Epoch: 1/2, step 1315/7134 completed (loss: 0.4913370907306671, acc: 0.8594594597816467)
[2025-02-13 19:27:45,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:45,367][root][INFO] - Training Epoch: 1/2, step 1316/7134 completed (loss: 0.5036015510559082, acc: 0.8861788511276245)
[2025-02-13 19:27:45,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:45,757][root][INFO] - Training Epoch: 1/2, step 1317/7134 completed (loss: 0.445115864276886, acc: 0.89552241563797)
[2025-02-13 19:27:45,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:46,183][root][INFO] - Training Epoch: 1/2, step 1318/7134 completed (loss: 0.2234324812889099, acc: 0.9560439586639404)
[2025-02-13 19:27:46,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:46,550][root][INFO] - Training Epoch: 1/2, step 1319/7134 completed (loss: 0.18477635085582733, acc: 0.9818181991577148)
[2025-02-13 19:27:46,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:46,907][root][INFO] - Training Epoch: 1/2, step 1320/7134 completed (loss: 0.3853474259376526, acc: 0.9139072895050049)
[2025-02-13 19:27:47,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:47,286][root][INFO] - Training Epoch: 1/2, step 1321/7134 completed (loss: 0.3112742304801941, acc: 0.9119170904159546)
[2025-02-13 19:27:47,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:47,690][root][INFO] - Training Epoch: 1/2, step 1322/7134 completed (loss: 0.42523738741874695, acc: 0.8979591727256775)
[2025-02-13 19:27:47,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:48,116][root][INFO] - Training Epoch: 1/2, step 1323/7134 completed (loss: 0.46286848187446594, acc: 0.9154929518699646)
[2025-02-13 19:27:48,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:48,528][root][INFO] - Training Epoch: 1/2, step 1324/7134 completed (loss: 0.3328196108341217, acc: 0.9084967374801636)
[2025-02-13 19:27:48,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:48,898][root][INFO] - Training Epoch: 1/2, step 1325/7134 completed (loss: 0.3314151465892792, acc: 0.8737863898277283)
[2025-02-13 19:27:49,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:49,285][root][INFO] - Training Epoch: 1/2, step 1326/7134 completed (loss: 0.4455793499946594, acc: 0.9086757898330688)
[2025-02-13 19:27:49,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:49,656][root][INFO] - Training Epoch: 1/2, step 1327/7134 completed (loss: 0.23393195867538452, acc: 0.9408283829689026)
[2025-02-13 19:27:49,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:50,024][root][INFO] - Training Epoch: 1/2, step 1328/7134 completed (loss: 0.3236280679702759, acc: 0.9108911156654358)
[2025-02-13 19:27:50,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:50,399][root][INFO] - Training Epoch: 1/2, step 1329/7134 completed (loss: 0.3144740164279938, acc: 0.9399999976158142)
[2025-02-13 19:27:50,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:50,742][root][INFO] - Training Epoch: 1/2, step 1330/7134 completed (loss: 0.2475036233663559, acc: 0.9496402740478516)
[2025-02-13 19:27:50,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:51,112][root][INFO] - Training Epoch: 1/2, step 1331/7134 completed (loss: 0.15130412578582764, acc: 0.9647887349128723)
[2025-02-13 19:27:51,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:51,457][root][INFO] - Training Epoch: 1/2, step 1332/7134 completed (loss: 0.15787965059280396, acc: 0.9473684430122375)
[2025-02-13 19:27:51,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:51,842][root][INFO] - Training Epoch: 1/2, step 1333/7134 completed (loss: 0.3350280523300171, acc: 0.9144737124443054)
[2025-02-13 19:27:51,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:52,218][root][INFO] - Training Epoch: 1/2, step 1334/7134 completed (loss: 0.5719394683837891, acc: 0.8580247163772583)
[2025-02-13 19:27:52,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:52,576][root][INFO] - Training Epoch: 1/2, step 1335/7134 completed (loss: 0.4491385519504547, acc: 0.9162303805351257)
[2025-02-13 19:27:52,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:52,943][root][INFO] - Training Epoch: 1/2, step 1336/7134 completed (loss: 0.3189845085144043, acc: 0.9465240836143494)
[2025-02-13 19:27:53,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:53,325][root][INFO] - Training Epoch: 1/2, step 1337/7134 completed (loss: 0.24960613250732422, acc: 0.9290322661399841)
[2025-02-13 19:27:53,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:53,660][root][INFO] - Training Epoch: 1/2, step 1338/7134 completed (loss: 0.4148581624031067, acc: 0.921875)
[2025-02-13 19:27:53,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:54,016][root][INFO] - Training Epoch: 1/2, step 1339/7134 completed (loss: 0.4715954661369324, acc: 0.8775510191917419)
[2025-02-13 19:27:54,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:54,368][root][INFO] - Training Epoch: 1/2, step 1340/7134 completed (loss: 0.36947205662727356, acc: 0.9285714030265808)
[2025-02-13 19:27:54,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:54,726][root][INFO] - Training Epoch: 1/2, step 1341/7134 completed (loss: 0.15411193668842316, acc: 0.9780219793319702)
[2025-02-13 19:27:54,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:55,114][root][INFO] - Training Epoch: 1/2, step 1342/7134 completed (loss: 0.1864853799343109, acc: 0.9470198750495911)
[2025-02-13 19:27:55,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:55,492][root][INFO] - Training Epoch: 1/2, step 1343/7134 completed (loss: 0.46228376030921936, acc: 0.8823529481887817)
[2025-02-13 19:27:55,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:55,830][root][INFO] - Training Epoch: 1/2, step 1344/7134 completed (loss: 0.19054584205150604, acc: 0.9266055226325989)
[2025-02-13 19:27:55,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:56,156][root][INFO] - Training Epoch: 1/2, step 1345/7134 completed (loss: 0.24970173835754395, acc: 0.9139785170555115)
[2025-02-13 19:27:56,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:56,594][root][INFO] - Training Epoch: 1/2, step 1346/7134 completed (loss: 0.2806916832923889, acc: 0.9180327653884888)
[2025-02-13 19:27:56,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:56,968][root][INFO] - Training Epoch: 1/2, step 1347/7134 completed (loss: 0.21812519431114197, acc: 0.9236111044883728)
[2025-02-13 19:27:57,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:57,341][root][INFO] - Training Epoch: 1/2, step 1348/7134 completed (loss: 0.1081458106637001, acc: 0.9779005646705627)
[2025-02-13 19:27:57,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:57,701][root][INFO] - Training Epoch: 1/2, step 1349/7134 completed (loss: 0.08546330034732819, acc: 0.9716312289237976)
[2025-02-13 19:27:57,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:58,061][root][INFO] - Training Epoch: 1/2, step 1350/7134 completed (loss: 0.12519985437393188, acc: 0.976047933101654)
[2025-02-13 19:27:58,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:58,417][root][INFO] - Training Epoch: 1/2, step 1351/7134 completed (loss: 0.051115963608026505, acc: 0.9916666746139526)
[2025-02-13 19:27:58,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:58,777][root][INFO] - Training Epoch: 1/2, step 1352/7134 completed (loss: 0.16153863072395325, acc: 0.9695122241973877)
[2025-02-13 19:27:58,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:59,179][root][INFO] - Training Epoch: 1/2, step 1353/7134 completed (loss: 0.24891681969165802, acc: 0.9416666626930237)
[2025-02-13 19:27:59,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:59,567][root][INFO] - Training Epoch: 1/2, step 1354/7134 completed (loss: 0.07051212340593338, acc: 0.9800000190734863)
[2025-02-13 19:27:59,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:59,925][root][INFO] - Training Epoch: 1/2, step 1355/7134 completed (loss: 0.04606451094150543, acc: 0.9935483932495117)
[2025-02-13 19:28:00,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:00,304][root][INFO] - Training Epoch: 1/2, step 1356/7134 completed (loss: 0.10159571468830109, acc: 0.9709302186965942)
[2025-02-13 19:28:00,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:00,678][root][INFO] - Training Epoch: 1/2, step 1357/7134 completed (loss: 0.08299200981855392, acc: 0.9788359999656677)
[2025-02-13 19:28:00,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:01,066][root][INFO] - Training Epoch: 1/2, step 1358/7134 completed (loss: 0.13745875656604767, acc: 0.9772727489471436)
[2025-02-13 19:28:01,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:01,428][root][INFO] - Training Epoch: 1/2, step 1359/7134 completed (loss: 0.06366175413131714, acc: 0.9834710955619812)
[2025-02-13 19:28:01,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:01,807][root][INFO] - Training Epoch: 1/2, step 1360/7134 completed (loss: 0.05653534084558487, acc: 0.988095223903656)
[2025-02-13 19:28:01,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:02,255][root][INFO] - Training Epoch: 1/2, step 1361/7134 completed (loss: 0.08783473074436188, acc: 0.976331353187561)
[2025-02-13 19:28:02,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:02,625][root][INFO] - Training Epoch: 1/2, step 1362/7134 completed (loss: 0.20960696041584015, acc: 0.9617486596107483)
[2025-02-13 19:28:02,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:03,017][root][INFO] - Training Epoch: 1/2, step 1363/7134 completed (loss: 0.1612335443496704, acc: 0.959770143032074)
[2025-02-13 19:28:03,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:03,380][root][INFO] - Training Epoch: 1/2, step 1364/7134 completed (loss: 0.06163613870739937, acc: 0.9941176176071167)
[2025-02-13 19:28:03,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:03,738][root][INFO] - Training Epoch: 1/2, step 1365/7134 completed (loss: 0.053256019949913025, acc: 0.9851852059364319)
[2025-02-13 19:28:03,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:04,134][root][INFO] - Training Epoch: 1/2, step 1366/7134 completed (loss: 0.02808430604636669, acc: 1.0)
[2025-02-13 19:28:04,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:04,541][root][INFO] - Training Epoch: 1/2, step 1367/7134 completed (loss: 0.08502891659736633, acc: 0.9852941036224365)
[2025-02-13 19:28:04,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:04,945][root][INFO] - Training Epoch: 1/2, step 1368/7134 completed (loss: 0.1790982186794281, acc: 0.9425287246704102)
[2025-02-13 19:28:05,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:05,335][root][INFO] - Training Epoch: 1/2, step 1369/7134 completed (loss: 0.2231459766626358, acc: 0.9489051103591919)
[2025-02-13 19:28:05,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:05,713][root][INFO] - Training Epoch: 1/2, step 1370/7134 completed (loss: 0.3555029332637787, acc: 0.9591836929321289)
[2025-02-13 19:28:05,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:06,101][root][INFO] - Training Epoch: 1/2, step 1371/7134 completed (loss: 0.19752450287342072, acc: 0.9506173133850098)
[2025-02-13 19:28:06,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:06,489][root][INFO] - Training Epoch: 1/2, step 1372/7134 completed (loss: 0.2253953069448471, acc: 0.9242424368858337)
[2025-02-13 19:28:06,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:06,872][root][INFO] - Training Epoch: 1/2, step 1373/7134 completed (loss: 0.20209035277366638, acc: 0.9379844665527344)
[2025-02-13 19:28:07,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:07,308][root][INFO] - Training Epoch: 1/2, step 1374/7134 completed (loss: 0.2832067310810089, acc: 0.9333333373069763)
[2025-02-13 19:28:07,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:07,659][root][INFO] - Training Epoch: 1/2, step 1375/7134 completed (loss: 0.1673678755760193, acc: 0.9509803652763367)
[2025-02-13 19:28:07,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:08,027][root][INFO] - Training Epoch: 1/2, step 1376/7134 completed (loss: 0.5548428297042847, acc: 0.8839285969734192)
[2025-02-13 19:28:08,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:08,411][root][INFO] - Training Epoch: 1/2, step 1377/7134 completed (loss: 0.47170791029930115, acc: 0.8823529481887817)
[2025-02-13 19:28:08,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:08,793][root][INFO] - Training Epoch: 1/2, step 1378/7134 completed (loss: 0.4917342960834503, acc: 0.8846153616905212)
[2025-02-13 19:28:08,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:09,147][root][INFO] - Training Epoch: 1/2, step 1379/7134 completed (loss: 0.33849984407424927, acc: 0.9009901285171509)
[2025-02-13 19:28:09,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:09,571][root][INFO] - Training Epoch: 1/2, step 1380/7134 completed (loss: 0.4836457073688507, acc: 0.8888888955116272)
[2025-02-13 19:28:09,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:09,969][root][INFO] - Training Epoch: 1/2, step 1381/7134 completed (loss: 0.2085196077823639, acc: 0.9492753744125366)
[2025-02-13 19:28:10,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:10,336][root][INFO] - Training Epoch: 1/2, step 1382/7134 completed (loss: 0.34028196334838867, acc: 0.9307692050933838)
[2025-02-13 19:28:10,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:10,714][root][INFO] - Training Epoch: 1/2, step 1383/7134 completed (loss: 0.5277990102767944, acc: 0.8823529481887817)
[2025-02-13 19:28:10,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:11,131][root][INFO] - Training Epoch: 1/2, step 1384/7134 completed (loss: 0.5017809271812439, acc: 0.8652482032775879)
[2025-02-13 19:28:11,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:11,503][root][INFO] - Training Epoch: 1/2, step 1385/7134 completed (loss: 0.5672925710678101, acc: 0.8560000061988831)
[2025-02-13 19:28:11,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:11,877][root][INFO] - Training Epoch: 1/2, step 1386/7134 completed (loss: 0.3628561198711395, acc: 0.908450722694397)
[2025-02-13 19:28:12,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:12,253][root][INFO] - Training Epoch: 1/2, step 1387/7134 completed (loss: 0.25834882259368896, acc: 0.9370629191398621)
[2025-02-13 19:28:12,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:12,624][root][INFO] - Training Epoch: 1/2, step 1388/7134 completed (loss: 0.3947764039039612, acc: 0.8799999952316284)
[2025-02-13 19:28:12,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:12,995][root][INFO] - Training Epoch: 1/2, step 1389/7134 completed (loss: 0.3426763117313385, acc: 0.9253731369972229)
[2025-02-13 19:28:13,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:13,370][root][INFO] - Training Epoch: 1/2, step 1390/7134 completed (loss: 0.41472166776657104, acc: 0.8978102207183838)
[2025-02-13 19:28:13,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:13,747][root][INFO] - Training Epoch: 1/2, step 1391/7134 completed (loss: 0.5213207602500916, acc: 0.845588207244873)
[2025-02-13 19:28:13,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:14,120][root][INFO] - Training Epoch: 1/2, step 1392/7134 completed (loss: 0.519973874092102, acc: 0.9024389982223511)
[2025-02-13 19:28:14,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:14,488][root][INFO] - Training Epoch: 1/2, step 1393/7134 completed (loss: 0.4783380329608917, acc: 0.84375)
[2025-02-13 19:28:14,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:14,853][root][INFO] - Training Epoch: 1/2, step 1394/7134 completed (loss: 0.41399553418159485, acc: 0.9245283007621765)
[2025-02-13 19:28:14,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:15,260][root][INFO] - Training Epoch: 1/2, step 1395/7134 completed (loss: 0.40583088994026184, acc: 0.9453125)
[2025-02-13 19:28:15,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:15,663][root][INFO] - Training Epoch: 1/2, step 1396/7134 completed (loss: 0.25034403800964355, acc: 0.9279999732971191)
[2025-02-13 19:28:15,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:16,015][root][INFO] - Training Epoch: 1/2, step 1397/7134 completed (loss: 0.5857754945755005, acc: 0.8175675868988037)
[2025-02-13 19:28:16,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:16,402][root][INFO] - Training Epoch: 1/2, step 1398/7134 completed (loss: 0.2458084374666214, acc: 0.9405940771102905)
[2025-02-13 19:28:16,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:16,790][root][INFO] - Training Epoch: 1/2, step 1399/7134 completed (loss: 0.4024572968482971, acc: 0.9280575513839722)
[2025-02-13 19:28:16,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:17,166][root][INFO] - Training Epoch: 1/2, step 1400/7134 completed (loss: 0.25748929381370544, acc: 0.9347826242446899)
[2025-02-13 19:28:17,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:17,541][root][INFO] - Training Epoch: 1/2, step 1401/7134 completed (loss: 0.20382483303546906, acc: 0.9481481313705444)
[2025-02-13 19:28:17,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:17,894][root][INFO] - Training Epoch: 1/2, step 1402/7134 completed (loss: 0.26576030254364014, acc: 0.9253731369972229)
[2025-02-13 19:28:18,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:18,271][root][INFO] - Training Epoch: 1/2, step 1403/7134 completed (loss: 0.22135837376117706, acc: 0.9349112510681152)
[2025-02-13 19:28:18,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:18,644][root][INFO] - Training Epoch: 1/2, step 1404/7134 completed (loss: 0.13742280006408691, acc: 0.9583333134651184)
[2025-02-13 19:28:18,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:19,023][root][INFO] - Training Epoch: 1/2, step 1405/7134 completed (loss: 0.2284507006406784, acc: 0.9642857313156128)
[2025-02-13 19:28:19,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:19,404][root][INFO] - Training Epoch: 1/2, step 1406/7134 completed (loss: 0.16193924844264984, acc: 0.95652174949646)
[2025-02-13 19:28:19,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:19,816][root][INFO] - Training Epoch: 1/2, step 1407/7134 completed (loss: 0.09055311977863312, acc: 0.9767441749572754)
[2025-02-13 19:28:19,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:20,189][root][INFO] - Training Epoch: 1/2, step 1408/7134 completed (loss: 0.11099937558174133, acc: 0.9724137783050537)
[2025-02-13 19:28:20,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:20,569][root][INFO] - Training Epoch: 1/2, step 1409/7134 completed (loss: 0.0591883510351181, acc: 0.9874213933944702)
[2025-02-13 19:28:20,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:20,932][root][INFO] - Training Epoch: 1/2, step 1410/7134 completed (loss: 0.21830520033836365, acc: 0.9613259434700012)
[2025-02-13 19:28:21,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:21,339][root][INFO] - Training Epoch: 1/2, step 1411/7134 completed (loss: 0.10192354768514633, acc: 0.977142870426178)
[2025-02-13 19:28:21,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:21,718][root][INFO] - Training Epoch: 1/2, step 1412/7134 completed (loss: 0.1561654508113861, acc: 0.9684210419654846)
[2025-02-13 19:28:21,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:22,073][root][INFO] - Training Epoch: 1/2, step 1413/7134 completed (loss: 0.09660445898771286, acc: 0.976047933101654)
[2025-02-13 19:28:22,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:22,432][root][INFO] - Training Epoch: 1/2, step 1414/7134 completed (loss: 0.0731196403503418, acc: 0.9801324605941772)
[2025-02-13 19:28:22,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:22,799][root][INFO] - Training Epoch: 1/2, step 1415/7134 completed (loss: 0.18382352590560913, acc: 0.956204354763031)
[2025-02-13 19:28:22,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:23,180][root][INFO] - Training Epoch: 1/2, step 1416/7134 completed (loss: 0.05435590073466301, acc: 0.994350254535675)
[2025-02-13 19:28:23,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:23,541][root][INFO] - Training Epoch: 1/2, step 1417/7134 completed (loss: 0.06786993891000748, acc: 0.9791666865348816)
[2025-02-13 19:28:23,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:23,955][root][INFO] - Training Epoch: 1/2, step 1418/7134 completed (loss: 0.09524016082286835, acc: 0.9689922332763672)
[2025-02-13 19:28:24,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:24,352][root][INFO] - Training Epoch: 1/2, step 1419/7134 completed (loss: 0.11259731650352478, acc: 0.9659090638160706)
[2025-02-13 19:28:24,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:24,723][root][INFO] - Training Epoch: 1/2, step 1420/7134 completed (loss: 0.15733113884925842, acc: 0.9631578922271729)
[2025-02-13 19:28:24,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:25,090][root][INFO] - Training Epoch: 1/2, step 1421/7134 completed (loss: 0.06227216124534607, acc: 0.9865771532058716)
[2025-02-13 19:28:25,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:25,466][root][INFO] - Training Epoch: 1/2, step 1422/7134 completed (loss: 0.4288000166416168, acc: 0.8975903391838074)
[2025-02-13 19:28:25,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:25,829][root][INFO] - Training Epoch: 1/2, step 1423/7134 completed (loss: 0.651077926158905, acc: 0.8549618124961853)
[2025-02-13 19:28:25,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:26,186][root][INFO] - Training Epoch: 1/2, step 1424/7134 completed (loss: 0.39868801832199097, acc: 0.8943089246749878)
[2025-02-13 19:28:26,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:26,562][root][INFO] - Training Epoch: 1/2, step 1425/7134 completed (loss: 0.2829861640930176, acc: 0.9190751314163208)
[2025-02-13 19:28:26,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:26,954][root][INFO] - Training Epoch: 1/2, step 1426/7134 completed (loss: 0.30880168080329895, acc: 0.9175257682800293)
[2025-02-13 19:28:27,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:27,314][root][INFO] - Training Epoch: 1/2, step 1427/7134 completed (loss: 0.34900814294815063, acc: 0.903954803943634)
[2025-02-13 19:28:27,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:27,683][root][INFO] - Training Epoch: 1/2, step 1428/7134 completed (loss: 1.37895667552948, acc: 0.6774193644523621)
[2025-02-13 19:28:27,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:28,054][root][INFO] - Training Epoch: 1/2, step 1429/7134 completed (loss: 0.3445735573768616, acc: 0.9351851940155029)
[2025-02-13 19:28:28,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:28,458][root][INFO] - Training Epoch: 1/2, step 1430/7134 completed (loss: 0.2986803948879242, acc: 0.9136690497398376)
[2025-02-13 19:28:28,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:28,834][root][INFO] - Training Epoch: 1/2, step 1431/7134 completed (loss: 0.16962337493896484, acc: 0.9516128897666931)
[2025-02-13 19:28:28,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:29,215][root][INFO] - Training Epoch: 1/2, step 1432/7134 completed (loss: 0.25634661316871643, acc: 0.8994975090026855)
[2025-02-13 19:28:29,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:29,580][root][INFO] - Training Epoch: 1/2, step 1433/7134 completed (loss: 0.27461761236190796, acc: 0.9313725233078003)
[2025-02-13 19:28:29,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:29,950][root][INFO] - Training Epoch: 1/2, step 1434/7134 completed (loss: 0.30378004908561707, acc: 0.9203979969024658)
[2025-02-13 19:28:30,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:30,357][root][INFO] - Training Epoch: 1/2, step 1435/7134 completed (loss: 0.3377287983894348, acc: 0.9139785170555115)
[2025-02-13 19:28:30,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:30,771][root][INFO] - Training Epoch: 1/2, step 1436/7134 completed (loss: 0.29299530386924744, acc: 0.9305555820465088)
[2025-02-13 19:28:30,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:31,161][root][INFO] - Training Epoch: 1/2, step 1437/7134 completed (loss: 0.2012597620487213, acc: 0.9731183052062988)
[2025-02-13 19:28:31,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:31,545][root][INFO] - Training Epoch: 1/2, step 1438/7134 completed (loss: 0.23339836299419403, acc: 0.9454545378684998)
[2025-02-13 19:28:31,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:31,917][root][INFO] - Training Epoch: 1/2, step 1439/7134 completed (loss: 0.14547070860862732, acc: 0.970370352268219)
[2025-02-13 19:28:32,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:32,286][root][INFO] - Training Epoch: 1/2, step 1440/7134 completed (loss: 0.1675969958305359, acc: 0.9689440727233887)
[2025-02-13 19:28:32,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:32,662][root][INFO] - Training Epoch: 1/2, step 1441/7134 completed (loss: 0.08581945300102234, acc: 0.9655172228813171)
[2025-02-13 19:28:32,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:33,048][root][INFO] - Training Epoch: 1/2, step 1442/7134 completed (loss: 0.06961207091808319, acc: 0.9910714030265808)
[2025-02-13 19:28:33,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:33,421][root][INFO] - Training Epoch: 1/2, step 1443/7134 completed (loss: 0.2448372095823288, acc: 0.9629629850387573)
[2025-02-13 19:28:33,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:33,805][root][INFO] - Training Epoch: 1/2, step 1444/7134 completed (loss: 0.21576973795890808, acc: 0.9444444179534912)
[2025-02-13 19:28:33,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:34,181][root][INFO] - Training Epoch: 1/2, step 1445/7134 completed (loss: 0.237001433968544, acc: 0.9299362897872925)
[2025-02-13 19:28:34,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:34,559][root][INFO] - Training Epoch: 1/2, step 1446/7134 completed (loss: 0.19382694363594055, acc: 0.9444444179534912)
[2025-02-13 19:28:34,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:34,940][root][INFO] - Training Epoch: 1/2, step 1447/7134 completed (loss: 0.1836148202419281, acc: 0.9354838728904724)
[2025-02-13 19:28:35,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:35,308][root][INFO] - Training Epoch: 1/2, step 1448/7134 completed (loss: 0.5444126129150391, acc: 0.89552241563797)
[2025-02-13 19:28:35,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:35,679][root][INFO] - Training Epoch: 1/2, step 1449/7134 completed (loss: 0.4935648739337921, acc: 0.8579235076904297)
[2025-02-13 19:28:35,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:36,054][root][INFO] - Training Epoch: 1/2, step 1450/7134 completed (loss: 0.22580763697624207, acc: 0.9411764740943909)
[2025-02-13 19:28:36,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:36,427][root][INFO] - Training Epoch: 1/2, step 1451/7134 completed (loss: 0.2122737020254135, acc: 0.9476439952850342)
[2025-02-13 19:28:36,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:36,830][root][INFO] - Training Epoch: 1/2, step 1452/7134 completed (loss: 0.23926261067390442, acc: 0.9375)
[2025-02-13 19:28:37,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:37,248][root][INFO] - Training Epoch: 1/2, step 1453/7134 completed (loss: 0.6877111196517944, acc: 0.8488371968269348)
[2025-02-13 19:28:37,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:37,643][root][INFO] - Training Epoch: 1/2, step 1454/7134 completed (loss: 0.3622403144836426, acc: 0.875)
[2025-02-13 19:28:37,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:38,049][root][INFO] - Training Epoch: 1/2, step 1455/7134 completed (loss: 0.5771953463554382, acc: 0.8620689511299133)
[2025-02-13 19:28:38,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:38,447][root][INFO] - Training Epoch: 1/2, step 1456/7134 completed (loss: 0.30892419815063477, acc: 0.9256198406219482)
[2025-02-13 19:28:38,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:38,832][root][INFO] - Training Epoch: 1/2, step 1457/7134 completed (loss: 0.2557327449321747, acc: 0.9130434989929199)
[2025-02-13 19:28:38,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:39,201][root][INFO] - Training Epoch: 1/2, step 1458/7134 completed (loss: 0.16560187935829163, acc: 0.9467455744743347)
[2025-02-13 19:28:39,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:39,555][root][INFO] - Training Epoch: 1/2, step 1459/7134 completed (loss: 0.1525305062532425, acc: 0.9481481313705444)
[2025-02-13 19:28:39,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:39,920][root][INFO] - Training Epoch: 1/2, step 1460/7134 completed (loss: 0.3727886378765106, acc: 0.8934911489486694)
[2025-02-13 19:28:40,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:40,266][root][INFO] - Training Epoch: 1/2, step 1461/7134 completed (loss: 0.3107568919658661, acc: 0.8940397500991821)
[2025-02-13 19:28:40,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:40,641][root][INFO] - Training Epoch: 1/2, step 1462/7134 completed (loss: 0.3270591199398041, acc: 0.9179104566574097)
[2025-02-13 19:28:40,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:41,011][root][INFO] - Training Epoch: 1/2, step 1463/7134 completed (loss: 0.484027236700058, acc: 0.8954248428344727)
[2025-02-13 19:28:41,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:41,399][root][INFO] - Training Epoch: 1/2, step 1464/7134 completed (loss: 0.46726423501968384, acc: 0.9096385836601257)
[2025-02-13 19:28:41,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:41,786][root][INFO] - Training Epoch: 1/2, step 1465/7134 completed (loss: 0.29211679100990295, acc: 0.9358974099159241)
[2025-02-13 19:28:41,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:42,197][root][INFO] - Training Epoch: 1/2, step 1466/7134 completed (loss: 0.10699643194675446, acc: 0.9664804339408875)
[2025-02-13 19:28:42,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:42,570][root][INFO] - Training Epoch: 1/2, step 1467/7134 completed (loss: 0.1343374103307724, acc: 0.9515151381492615)
[2025-02-13 19:28:42,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:42,952][root][INFO] - Training Epoch: 1/2, step 1468/7134 completed (loss: 0.27622613310813904, acc: 0.932584285736084)
[2025-02-13 19:28:43,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:43,338][root][INFO] - Training Epoch: 1/2, step 1469/7134 completed (loss: 0.11181552708148956, acc: 0.9886363744735718)
[2025-02-13 19:28:43,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:43,705][root][INFO] - Training Epoch: 1/2, step 1470/7134 completed (loss: 0.2835773527622223, acc: 0.9171974658966064)
[2025-02-13 19:28:43,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:44,109][root][INFO] - Training Epoch: 1/2, step 1471/7134 completed (loss: 0.4476010799407959, acc: 0.8999999761581421)
[2025-02-13 19:28:44,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:44,495][root][INFO] - Training Epoch: 1/2, step 1472/7134 completed (loss: 0.15209147334098816, acc: 0.9707602262496948)
[2025-02-13 19:28:44,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:44,879][root][INFO] - Training Epoch: 1/2, step 1473/7134 completed (loss: 0.20055241882801056, acc: 0.9454545378684998)
[2025-02-13 19:28:45,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:45,251][root][INFO] - Training Epoch: 1/2, step 1474/7134 completed (loss: 0.20462775230407715, acc: 0.9379310607910156)
[2025-02-13 19:28:45,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:45,626][root][INFO] - Training Epoch: 1/2, step 1475/7134 completed (loss: 0.34297823905944824, acc: 0.9333333373069763)
[2025-02-13 19:28:45,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:45,995][root][INFO] - Training Epoch: 1/2, step 1476/7134 completed (loss: 0.31010082364082336, acc: 0.9133333563804626)
[2025-02-13 19:28:46,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:46,350][root][INFO] - Training Epoch: 1/2, step 1477/7134 completed (loss: 0.21711066365242004, acc: 0.951724112033844)
[2025-02-13 19:28:46,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:46,727][root][INFO] - Training Epoch: 1/2, step 1478/7134 completed (loss: 0.26723742485046387, acc: 0.9539473652839661)
[2025-02-13 19:28:46,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:47,095][root][INFO] - Training Epoch: 1/2, step 1479/7134 completed (loss: 0.13454285264015198, acc: 0.9712643623352051)
[2025-02-13 19:28:47,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:47,472][root][INFO] - Training Epoch: 1/2, step 1480/7134 completed (loss: 0.1185268834233284, acc: 0.9779005646705627)
[2025-02-13 19:28:47,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:47,869][root][INFO] - Training Epoch: 1/2, step 1481/7134 completed (loss: 0.17950835824012756, acc: 0.9666666388511658)
[2025-02-13 19:28:48,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:48,242][root][INFO] - Training Epoch: 1/2, step 1482/7134 completed (loss: 0.3164280354976654, acc: 0.9230769276618958)
[2025-02-13 19:28:48,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:48,639][root][INFO] - Training Epoch: 1/2, step 1483/7134 completed (loss: 0.11273675411939621, acc: 0.9694656729698181)
[2025-02-13 19:28:48,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:49,016][root][INFO] - Training Epoch: 1/2, step 1484/7134 completed (loss: 0.3103700578212738, acc: 0.9337349534034729)
[2025-02-13 19:28:49,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:49,378][root][INFO] - Training Epoch: 1/2, step 1485/7134 completed (loss: 0.21129952371120453, acc: 0.9239766001701355)
[2025-02-13 19:28:49,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:49,754][root][INFO] - Training Epoch: 1/2, step 1486/7134 completed (loss: 0.14631517231464386, acc: 0.9685534834861755)
[2025-02-13 19:28:49,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:50,123][root][INFO] - Training Epoch: 1/2, step 1487/7134 completed (loss: 0.2511017620563507, acc: 0.9351351261138916)
[2025-02-13 19:28:50,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:50,550][root][INFO] - Training Epoch: 1/2, step 1488/7134 completed (loss: 0.23379763960838318, acc: 0.934883713722229)
[2025-02-13 19:28:50,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:50,922][root][INFO] - Training Epoch: 1/2, step 1489/7134 completed (loss: 0.22407656908035278, acc: 0.9336734414100647)
[2025-02-13 19:28:51,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:51,317][root][INFO] - Training Epoch: 1/2, step 1490/7134 completed (loss: 0.24659982323646545, acc: 0.9336283206939697)
[2025-02-13 19:28:51,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:51,677][root][INFO] - Training Epoch: 1/2, step 1491/7134 completed (loss: 0.1690751016139984, acc: 0.961904764175415)
[2025-02-13 19:28:51,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:52,038][root][INFO] - Training Epoch: 1/2, step 1492/7134 completed (loss: 0.342206209897995, acc: 0.9431279897689819)
[2025-02-13 19:28:52,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:52,403][root][INFO] - Training Epoch: 1/2, step 1493/7134 completed (loss: 0.21241378784179688, acc: 0.9532710313796997)
[2025-02-13 19:28:52,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:52,779][root][INFO] - Training Epoch: 1/2, step 1494/7134 completed (loss: 0.19873519241809845, acc: 0.9533678889274597)
[2025-02-13 19:28:52,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:53,143][root][INFO] - Training Epoch: 1/2, step 1495/7134 completed (loss: 0.13884879648685455, acc: 0.9707317352294922)
[2025-02-13 19:28:53,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:53,514][root][INFO] - Training Epoch: 1/2, step 1496/7134 completed (loss: 0.20348885655403137, acc: 0.949999988079071)
[2025-02-13 19:28:53,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:53,907][root][INFO] - Training Epoch: 1/2, step 1497/7134 completed (loss: 0.10050172358751297, acc: 0.9722222089767456)
[2025-02-13 19:28:54,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:54,255][root][INFO] - Training Epoch: 1/2, step 1498/7134 completed (loss: 0.21044692397117615, acc: 0.9405405521392822)
[2025-02-13 19:28:54,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:54,633][root][INFO] - Training Epoch: 1/2, step 1499/7134 completed (loss: 0.1344975084066391, acc: 0.9581395387649536)
[2025-02-13 19:28:54,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:54,991][root][INFO] - Training Epoch: 1/2, step 1500/7134 completed (loss: 0.3455720543861389, acc: 0.9178082346916199)
[2025-02-13 19:28:55,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:55,401][root][INFO] - Training Epoch: 1/2, step 1501/7134 completed (loss: 0.3444829285144806, acc: 0.8994082808494568)
[2025-02-13 19:28:55,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:55,775][root][INFO] - Training Epoch: 1/2, step 1502/7134 completed (loss: 0.23954898118972778, acc: 0.9318181872367859)
[2025-02-13 19:28:55,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:56,141][root][INFO] - Training Epoch: 1/2, step 1503/7134 completed (loss: 0.2934749126434326, acc: 0.9314285516738892)
[2025-02-13 19:28:56,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:56,515][root][INFO] - Training Epoch: 1/2, step 1504/7134 completed (loss: 0.12046222388744354, acc: 0.9682539701461792)
[2025-02-13 19:28:56,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:56,873][root][INFO] - Training Epoch: 1/2, step 1505/7134 completed (loss: 0.22971734404563904, acc: 0.949367105960846)
[2025-02-13 19:28:57,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:57,245][root][INFO] - Training Epoch: 1/2, step 1506/7134 completed (loss: 0.3149023652076721, acc: 0.9119496941566467)
[2025-02-13 19:28:57,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:57,627][root][INFO] - Training Epoch: 1/2, step 1507/7134 completed (loss: 0.3068106472492218, acc: 0.921875)
[2025-02-13 19:28:57,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:57,990][root][INFO] - Training Epoch: 1/2, step 1508/7134 completed (loss: 0.1915666162967682, acc: 0.9532710313796997)
[2025-02-13 19:28:58,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:58,345][root][INFO] - Training Epoch: 1/2, step 1509/7134 completed (loss: 0.1421745866537094, acc: 0.9666666388511658)
[2025-02-13 19:28:58,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:58,716][root][INFO] - Training Epoch: 1/2, step 1510/7134 completed (loss: 0.38018327951431274, acc: 0.9032257795333862)
[2025-02-13 19:28:58,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:59,081][root][INFO] - Training Epoch: 1/2, step 1511/7134 completed (loss: 0.3104633688926697, acc: 0.9273743033409119)
[2025-02-13 19:28:59,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:59,459][root][INFO] - Training Epoch: 1/2, step 1512/7134 completed (loss: 0.10160894691944122, acc: 0.9800000190734863)
[2025-02-13 19:28:59,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:59,818][root][INFO] - Training Epoch: 1/2, step 1513/7134 completed (loss: 0.13280291855335236, acc: 0.9834254384040833)
[2025-02-13 19:28:59,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:00,196][root][INFO] - Training Epoch: 1/2, step 1514/7134 completed (loss: 0.21635660529136658, acc: 0.9430052042007446)
[2025-02-13 19:29:00,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:00,569][root][INFO] - Training Epoch: 1/2, step 1515/7134 completed (loss: 0.21457791328430176, acc: 0.954954981803894)
[2025-02-13 19:29:00,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:00,861][root][INFO] - Training Epoch: 1/2, step 1516/7134 completed (loss: 0.2338021695613861, acc: 0.9624999761581421)
[2025-02-13 19:29:00,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:01,199][root][INFO] - Training Epoch: 1/2, step 1517/7134 completed (loss: 0.4179280400276184, acc: 0.8928571343421936)
[2025-02-13 19:29:01,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:01,546][root][INFO] - Training Epoch: 1/2, step 1518/7134 completed (loss: 0.18701311945915222, acc: 0.969072163105011)
[2025-02-13 19:29:01,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:01,937][root][INFO] - Training Epoch: 1/2, step 1519/7134 completed (loss: 0.19623765349388123, acc: 0.9708737730979919)
[2025-02-13 19:29:02,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:02,302][root][INFO] - Training Epoch: 1/2, step 1520/7134 completed (loss: 0.15130780637264252, acc: 0.970802903175354)
[2025-02-13 19:29:02,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:02,681][root][INFO] - Training Epoch: 1/2, step 1521/7134 completed (loss: 0.8179240226745605, acc: 0.8088235259056091)
[2025-02-13 19:29:02,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:03,035][root][INFO] - Training Epoch: 1/2, step 1522/7134 completed (loss: 0.5463688373565674, acc: 0.8492063283920288)
[2025-02-13 19:29:03,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:03,388][root][INFO] - Training Epoch: 1/2, step 1523/7134 completed (loss: 0.6299624443054199, acc: 0.8289473652839661)
[2025-02-13 19:29:03,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:03,734][root][INFO] - Training Epoch: 1/2, step 1524/7134 completed (loss: 0.38119494915008545, acc: 0.931506872177124)
[2025-02-13 19:29:03,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:04,096][root][INFO] - Training Epoch: 1/2, step 1525/7134 completed (loss: 0.5958740711212158, acc: 0.8899999856948853)
[2025-02-13 19:29:04,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:04,505][root][INFO] - Training Epoch: 1/2, step 1526/7134 completed (loss: 0.5462924242019653, acc: 0.8707482814788818)
[2025-02-13 19:29:04,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:04,900][root][INFO] - Training Epoch: 1/2, step 1527/7134 completed (loss: 0.8416523337364197, acc: 0.8584070801734924)
[2025-02-13 19:29:05,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:05,309][root][INFO] - Training Epoch: 1/2, step 1528/7134 completed (loss: 0.44577398896217346, acc: 0.8913043737411499)
[2025-02-13 19:29:05,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:05,673][root][INFO] - Training Epoch: 1/2, step 1529/7134 completed (loss: 0.21664702892303467, acc: 0.9468085169792175)
[2025-02-13 19:29:05,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:06,043][root][INFO] - Training Epoch: 1/2, step 1530/7134 completed (loss: 0.4140929579734802, acc: 0.9017857313156128)
[2025-02-13 19:29:06,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:06,407][root][INFO] - Training Epoch: 1/2, step 1531/7134 completed (loss: 0.7429521083831787, acc: 0.8181818127632141)
[2025-02-13 19:29:06,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:06,804][root][INFO] - Training Epoch: 1/2, step 1532/7134 completed (loss: 0.2891865372657776, acc: 0.9076923131942749)
[2025-02-13 19:29:06,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:07,161][root][INFO] - Training Epoch: 1/2, step 1533/7134 completed (loss: 0.40518027544021606, acc: 0.891566276550293)
[2025-02-13 19:29:07,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:07,529][root][INFO] - Training Epoch: 1/2, step 1534/7134 completed (loss: 0.384379118680954, acc: 0.9280575513839722)
[2025-02-13 19:29:07,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:07,945][root][INFO] - Training Epoch: 1/2, step 1535/7134 completed (loss: 0.33429980278015137, acc: 0.8971428275108337)
[2025-02-13 19:29:08,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:08,311][root][INFO] - Training Epoch: 1/2, step 1536/7134 completed (loss: 0.24003151059150696, acc: 0.9259259104728699)
[2025-02-13 19:29:08,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:08,676][root][INFO] - Training Epoch: 1/2, step 1537/7134 completed (loss: 0.3462788462638855, acc: 0.9298245906829834)
[2025-02-13 19:29:08,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:09,071][root][INFO] - Training Epoch: 1/2, step 1538/7134 completed (loss: 0.4888131320476532, acc: 0.9107142686843872)
[2025-02-13 19:29:09,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:09,487][root][INFO] - Training Epoch: 1/2, step 1539/7134 completed (loss: 0.3242494761943817, acc: 0.9090909361839294)
[2025-02-13 19:29:09,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:09,890][root][INFO] - Training Epoch: 1/2, step 1540/7134 completed (loss: 0.27568984031677246, acc: 0.9177215099334717)
[2025-02-13 19:29:10,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:10,258][root][INFO] - Training Epoch: 1/2, step 1541/7134 completed (loss: 0.3240124583244324, acc: 0.9052631855010986)
[2025-02-13 19:29:10,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:10,637][root][INFO] - Training Epoch: 1/2, step 1542/7134 completed (loss: 0.21352078020572662, acc: 0.9382022619247437)
[2025-02-13 19:29:10,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:11,013][root][INFO] - Training Epoch: 1/2, step 1543/7134 completed (loss: 0.1766020506620407, acc: 0.9571428298950195)
[2025-02-13 19:29:11,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:11,406][root][INFO] - Training Epoch: 1/2, step 1544/7134 completed (loss: 0.18222719430923462, acc: 0.9350000023841858)
[2025-02-13 19:29:11,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:11,790][root][INFO] - Training Epoch: 1/2, step 1545/7134 completed (loss: 0.2921093702316284, acc: 0.898809552192688)
[2025-02-13 19:29:11,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:12,205][root][INFO] - Training Epoch: 1/2, step 1546/7134 completed (loss: 0.5793969035148621, acc: 0.9058823585510254)
[2025-02-13 19:29:12,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:12,584][root][INFO] - Training Epoch: 1/2, step 1547/7134 completed (loss: 0.4443145990371704, acc: 0.9012345671653748)
[2025-02-13 19:29:12,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:12,957][root][INFO] - Training Epoch: 1/2, step 1548/7134 completed (loss: 0.20628826320171356, acc: 0.9473684430122375)
[2025-02-13 19:29:13,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:13,332][root][INFO] - Training Epoch: 1/2, step 1549/7134 completed (loss: 0.12545807659626007, acc: 0.9714285731315613)
[2025-02-13 19:29:13,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:13,702][root][INFO] - Training Epoch: 1/2, step 1550/7134 completed (loss: 0.19897176325321198, acc: 0.9647058844566345)
[2025-02-13 19:29:13,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:14,075][root][INFO] - Training Epoch: 1/2, step 1551/7134 completed (loss: 0.1632152944803238, acc: 0.9867549538612366)
[2025-02-13 19:29:14,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:14,458][root][INFO] - Training Epoch: 1/2, step 1552/7134 completed (loss: 0.1357102394104004, acc: 0.9642857313156128)
[2025-02-13 19:29:14,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:14,832][root][INFO] - Training Epoch: 1/2, step 1553/7134 completed (loss: 0.26006999611854553, acc: 0.9220778942108154)
[2025-02-13 19:29:14,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:15,215][root][INFO] - Training Epoch: 1/2, step 1554/7134 completed (loss: 0.2629731595516205, acc: 0.9421965479850769)
[2025-02-13 19:29:15,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:15,577][root][INFO] - Training Epoch: 1/2, step 1555/7134 completed (loss: 0.20164982974529266, acc: 0.956250011920929)
[2025-02-13 19:29:15,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:16,014][root][INFO] - Training Epoch: 1/2, step 1556/7134 completed (loss: 0.2933918237686157, acc: 0.909604549407959)
[2025-02-13 19:29:16,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:16,390][root][INFO] - Training Epoch: 1/2, step 1557/7134 completed (loss: 0.16826948523521423, acc: 0.9555555582046509)
[2025-02-13 19:29:16,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:16,826][root][INFO] - Training Epoch: 1/2, step 1558/7134 completed (loss: 0.2774639129638672, acc: 0.931506872177124)
[2025-02-13 19:29:16,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:17,193][root][INFO] - Training Epoch: 1/2, step 1559/7134 completed (loss: 0.2479090839624405, acc: 0.9371428489685059)
[2025-02-13 19:29:17,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:17,560][root][INFO] - Training Epoch: 1/2, step 1560/7134 completed (loss: 0.1569545716047287, acc: 0.9644669890403748)
[2025-02-13 19:29:17,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:17,951][root][INFO] - Training Epoch: 1/2, step 1561/7134 completed (loss: 0.46466541290283203, acc: 0.9051724076271057)
[2025-02-13 19:29:18,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:18,364][root][INFO] - Training Epoch: 1/2, step 1562/7134 completed (loss: 0.29051584005355835, acc: 0.9171597361564636)
[2025-02-13 19:29:18,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:18,782][root][INFO] - Training Epoch: 1/2, step 1563/7134 completed (loss: 0.2898785173892975, acc: 0.9166666865348816)
[2025-02-13 19:29:18,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:19,211][root][INFO] - Training Epoch: 1/2, step 1564/7134 completed (loss: 0.4158754348754883, acc: 0.8811880946159363)
[2025-02-13 19:29:19,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:19,572][root][INFO] - Training Epoch: 1/2, step 1565/7134 completed (loss: 0.40872159600257874, acc: 0.8867924809455872)
[2025-02-13 19:29:19,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:19,938][root][INFO] - Training Epoch: 1/2, step 1566/7134 completed (loss: 0.3215011656284332, acc: 0.9116021990776062)
[2025-02-13 19:29:20,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:20,321][root][INFO] - Training Epoch: 1/2, step 1567/7134 completed (loss: 0.5076205134391785, acc: 0.8975609540939331)
[2025-02-13 19:29:20,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:20,705][root][INFO] - Training Epoch: 1/2, step 1568/7134 completed (loss: 0.44986027479171753, acc: 0.879807710647583)
[2025-02-13 19:29:20,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:21,078][root][INFO] - Training Epoch: 1/2, step 1569/7134 completed (loss: 0.3039463758468628, acc: 0.9090909361839294)
[2025-02-13 19:29:21,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:21,498][root][INFO] - Training Epoch: 1/2, step 1570/7134 completed (loss: 0.6220350861549377, acc: 0.8873239159584045)
[2025-02-13 19:29:21,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:21,873][root][INFO] - Training Epoch: 1/2, step 1571/7134 completed (loss: 0.24208755791187286, acc: 0.9539170265197754)
[2025-02-13 19:29:22,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:22,285][root][INFO] - Training Epoch: 1/2, step 1572/7134 completed (loss: 0.42910072207450867, acc: 0.9295774698257446)
[2025-02-13 19:29:22,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:22,634][root][INFO] - Training Epoch: 1/2, step 1573/7134 completed (loss: 0.1508239507675171, acc: 0.9640718698501587)
[2025-02-13 19:29:22,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:23,043][root][INFO] - Training Epoch: 1/2, step 1574/7134 completed (loss: 0.26763761043548584, acc: 0.9237667918205261)
[2025-02-13 19:29:23,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:23,409][root][INFO] - Training Epoch: 1/2, step 1575/7134 completed (loss: 0.24560897052288055, acc: 0.949999988079071)
[2025-02-13 19:29:23,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:23,849][root][INFO] - Training Epoch: 1/2, step 1576/7134 completed (loss: 0.21666982769966125, acc: 0.9530516266822815)
[2025-02-13 19:29:24,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:24,255][root][INFO] - Training Epoch: 1/2, step 1577/7134 completed (loss: 0.2343008816242218, acc: 0.9468598961830139)
[2025-02-13 19:29:24,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:24,627][root][INFO] - Training Epoch: 1/2, step 1578/7134 completed (loss: 0.23955732583999634, acc: 0.9318181872367859)
[2025-02-13 19:29:24,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:25,050][root][INFO] - Training Epoch: 1/2, step 1579/7134 completed (loss: 0.17898806929588318, acc: 0.9567307829856873)
[2025-02-13 19:29:25,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:25,455][root][INFO] - Training Epoch: 1/2, step 1580/7134 completed (loss: 0.5061156153678894, acc: 0.8599033951759338)
[2025-02-13 19:29:25,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:25,829][root][INFO] - Training Epoch: 1/2, step 1581/7134 completed (loss: 0.6038565635681152, acc: 0.851190447807312)
[2025-02-13 19:29:25,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:26,179][root][INFO] - Training Epoch: 1/2, step 1582/7134 completed (loss: 0.386405885219574, acc: 0.8799999952316284)
[2025-02-13 19:29:26,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:26,550][root][INFO] - Training Epoch: 1/2, step 1583/7134 completed (loss: 0.28364962339401245, acc: 0.9257143139839172)
[2025-02-13 19:29:26,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:26,947][root][INFO] - Training Epoch: 1/2, step 1584/7134 completed (loss: 0.30297935009002686, acc: 0.9369369149208069)
[2025-02-13 19:29:27,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:27,336][root][INFO] - Training Epoch: 1/2, step 1585/7134 completed (loss: 0.18126721680164337, acc: 0.9514563083648682)
[2025-02-13 19:29:27,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:27,722][root][INFO] - Training Epoch: 1/2, step 1586/7134 completed (loss: 0.27056455612182617, acc: 0.9178743958473206)
[2025-02-13 19:29:27,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:28,087][root][INFO] - Training Epoch: 1/2, step 1587/7134 completed (loss: 0.2729290723800659, acc: 0.9617834687232971)
[2025-02-13 19:29:28,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:28,466][root][INFO] - Training Epoch: 1/2, step 1588/7134 completed (loss: 0.191602423787117, acc: 0.9681528806686401)
[2025-02-13 19:29:28,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:28,833][root][INFO] - Training Epoch: 1/2, step 1589/7134 completed (loss: 0.20419514179229736, acc: 0.9451219439506531)
[2025-02-13 19:29:28,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:29,189][root][INFO] - Training Epoch: 1/2, step 1590/7134 completed (loss: 0.22721584141254425, acc: 0.9428571462631226)
[2025-02-13 19:29:29,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:29,614][root][INFO] - Training Epoch: 1/2, step 1591/7134 completed (loss: 0.17350895702838898, acc: 0.9534883499145508)
[2025-02-13 19:29:29,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:29,993][root][INFO] - Training Epoch: 1/2, step 1592/7134 completed (loss: 0.247949481010437, acc: 0.948051929473877)
[2025-02-13 19:29:30,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:30,369][root][INFO] - Training Epoch: 1/2, step 1593/7134 completed (loss: 0.1609165072441101, acc: 0.9679144620895386)
[2025-02-13 19:29:30,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:30,772][root][INFO] - Training Epoch: 1/2, step 1594/7134 completed (loss: 0.16563807427883148, acc: 0.96875)
[2025-02-13 19:29:30,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:31,169][root][INFO] - Training Epoch: 1/2, step 1595/7134 completed (loss: 0.15825322270393372, acc: 0.9579831957817078)
[2025-02-13 19:29:31,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:31,535][root][INFO] - Training Epoch: 1/2, step 1596/7134 completed (loss: 0.3355627655982971, acc: 0.9305555820465088)
[2025-02-13 19:29:31,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:31,898][root][INFO] - Training Epoch: 1/2, step 1597/7134 completed (loss: 0.1292017102241516, acc: 0.9735099077224731)
[2025-02-13 19:29:32,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:32,291][root][INFO] - Training Epoch: 1/2, step 1598/7134 completed (loss: 0.43620479106903076, acc: 0.9166666865348816)
[2025-02-13 19:29:32,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:32,656][root][INFO] - Training Epoch: 1/2, step 1599/7134 completed (loss: 0.42192181944847107, acc: 0.882758617401123)
[2025-02-13 19:29:32,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:33,009][root][INFO] - Training Epoch: 1/2, step 1600/7134 completed (loss: 0.2978895604610443, acc: 0.9285714030265808)
[2025-02-13 19:29:33,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:33,357][root][INFO] - Training Epoch: 1/2, step 1601/7134 completed (loss: 0.14608275890350342, acc: 0.9636363387107849)
[2025-02-13 19:29:33,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:33,797][root][INFO] - Training Epoch: 1/2, step 1602/7134 completed (loss: 0.20128031075000763, acc: 0.9615384340286255)
[2025-02-13 19:29:33,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:34,174][root][INFO] - Training Epoch: 1/2, step 1603/7134 completed (loss: 0.3352857530117035, acc: 0.9277108311653137)
[2025-02-13 19:29:34,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:34,537][root][INFO] - Training Epoch: 1/2, step 1604/7134 completed (loss: 0.15023085474967957, acc: 0.9515151381492615)
[2025-02-13 19:29:34,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:34,936][root][INFO] - Training Epoch: 1/2, step 1605/7134 completed (loss: 0.19359588623046875, acc: 0.9647058844566345)
[2025-02-13 19:29:35,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:35,298][root][INFO] - Training Epoch: 1/2, step 1606/7134 completed (loss: 0.13904647529125214, acc: 0.9591836929321289)
[2025-02-13 19:29:35,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:35,660][root][INFO] - Training Epoch: 1/2, step 1607/7134 completed (loss: 0.09715200960636139, acc: 0.9727272987365723)
[2025-02-13 19:29:35,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:36,069][root][INFO] - Training Epoch: 1/2, step 1608/7134 completed (loss: 0.1552337408065796, acc: 0.960629940032959)
[2025-02-13 19:29:36,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:36,478][root][INFO] - Training Epoch: 1/2, step 1609/7134 completed (loss: 0.24099135398864746, acc: 0.9236111044883728)
[2025-02-13 19:29:36,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:36,878][root][INFO] - Training Epoch: 1/2, step 1610/7134 completed (loss: 0.17667722702026367, acc: 0.9618320465087891)
[2025-02-13 19:29:37,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:37,274][root][INFO] - Training Epoch: 1/2, step 1611/7134 completed (loss: 0.1801927536725998, acc: 0.9567901492118835)
[2025-02-13 19:29:37,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:37,650][root][INFO] - Training Epoch: 1/2, step 1612/7134 completed (loss: 0.2051575630903244, acc: 0.9397590160369873)
[2025-02-13 19:29:37,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:38,025][root][INFO] - Training Epoch: 1/2, step 1613/7134 completed (loss: 0.17876149713993073, acc: 0.95652174949646)
[2025-02-13 19:29:38,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:38,376][root][INFO] - Training Epoch: 1/2, step 1614/7134 completed (loss: 0.19493505358695984, acc: 0.9473684430122375)
[2025-02-13 19:29:38,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:38,768][root][INFO] - Training Epoch: 1/2, step 1615/7134 completed (loss: 0.19727273285388947, acc: 0.9277108311653137)
[2025-02-13 19:29:38,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:39,126][root][INFO] - Training Epoch: 1/2, step 1616/7134 completed (loss: 0.14187520742416382, acc: 0.9612902998924255)
[2025-02-13 19:29:39,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:39,485][root][INFO] - Training Epoch: 1/2, step 1617/7134 completed (loss: 0.15731196105480194, acc: 0.9568345546722412)
[2025-02-13 19:29:39,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:39,849][root][INFO] - Training Epoch: 1/2, step 1618/7134 completed (loss: 0.47287246584892273, acc: 0.9083969593048096)
[2025-02-13 19:29:39,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:40,199][root][INFO] - Training Epoch: 1/2, step 1619/7134 completed (loss: 0.2626473605632782, acc: 0.9268292784690857)
[2025-02-13 19:29:40,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:40,582][root][INFO] - Training Epoch: 1/2, step 1620/7134 completed (loss: 0.3684682846069336, acc: 0.9025974273681641)
[2025-02-13 19:29:40,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:40,969][root][INFO] - Training Epoch: 1/2, step 1621/7134 completed (loss: 0.4297720193862915, acc: 0.915032684803009)
[2025-02-13 19:29:41,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:41,340][root][INFO] - Training Epoch: 1/2, step 1622/7134 completed (loss: 0.26784083247184753, acc: 0.9215686321258545)
[2025-02-13 19:29:41,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:41,744][root][INFO] - Training Epoch: 1/2, step 1623/7134 completed (loss: 0.2282443642616272, acc: 0.9166666865348816)
[2025-02-13 19:29:41,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:42,120][root][INFO] - Training Epoch: 1/2, step 1624/7134 completed (loss: 0.3121221363544464, acc: 0.9415584206581116)
[2025-02-13 19:29:42,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:42,505][root][INFO] - Training Epoch: 1/2, step 1625/7134 completed (loss: 0.2390027940273285, acc: 0.9375)
[2025-02-13 19:29:42,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:42,937][root][INFO] - Training Epoch: 1/2, step 1626/7134 completed (loss: 0.3778947591781616, acc: 0.9278350472450256)
[2025-02-13 19:29:43,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:43,329][root][INFO] - Training Epoch: 1/2, step 1627/7134 completed (loss: 0.22873863577842712, acc: 0.934959352016449)
[2025-02-13 19:29:43,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:43,704][root][INFO] - Training Epoch: 1/2, step 1628/7134 completed (loss: 0.07460656017065048, acc: 0.9814814925193787)
[2025-02-13 19:29:43,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:44,078][root][INFO] - Training Epoch: 1/2, step 1629/7134 completed (loss: 0.20894090831279755, acc: 0.9545454382896423)
[2025-02-13 19:29:44,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:44,443][root][INFO] - Training Epoch: 1/2, step 1630/7134 completed (loss: 0.2892422676086426, acc: 0.9219858050346375)
[2025-02-13 19:29:44,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:44,862][root][INFO] - Training Epoch: 1/2, step 1631/7134 completed (loss: 0.3028806447982788, acc: 0.9280575513839722)
[2025-02-13 19:29:45,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:45,222][root][INFO] - Training Epoch: 1/2, step 1632/7134 completed (loss: 0.17788489162921906, acc: 0.9477611780166626)
[2025-02-13 19:29:45,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:45,589][root][INFO] - Training Epoch: 1/2, step 1633/7134 completed (loss: 0.2994883060455322, acc: 0.9142857193946838)
[2025-02-13 19:29:45,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:45,939][root][INFO] - Training Epoch: 1/2, step 1634/7134 completed (loss: 0.16223397850990295, acc: 0.9496855139732361)
[2025-02-13 19:29:46,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:46,290][root][INFO] - Training Epoch: 1/2, step 1635/7134 completed (loss: 0.16739781200885773, acc: 0.9444444179534912)
[2025-02-13 19:29:46,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:46,672][root][INFO] - Training Epoch: 1/2, step 1636/7134 completed (loss: 0.39237305521965027, acc: 0.9082568883895874)
[2025-02-13 19:29:46,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:47,040][root][INFO] - Training Epoch: 1/2, step 1637/7134 completed (loss: 0.1679696887731552, acc: 0.95652174949646)
[2025-02-13 19:29:47,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:47,421][root][INFO] - Training Epoch: 1/2, step 1638/7134 completed (loss: 0.5775068998336792, acc: 0.8661417365074158)
[2025-02-13 19:29:47,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:47,831][root][INFO] - Training Epoch: 1/2, step 1639/7134 completed (loss: 0.7583154439926147, acc: 0.8550724387168884)
[2025-02-13 19:29:47,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:48,207][root][INFO] - Training Epoch: 1/2, step 1640/7134 completed (loss: 0.3327418863773346, acc: 0.9202898740768433)
[2025-02-13 19:29:48,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:48,573][root][INFO] - Training Epoch: 1/2, step 1641/7134 completed (loss: 0.28948017954826355, acc: 0.9144737124443054)
[2025-02-13 19:29:48,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:48,940][root][INFO] - Training Epoch: 1/2, step 1642/7134 completed (loss: 0.3386542797088623, acc: 0.9119496941566467)
[2025-02-13 19:29:49,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:49,284][root][INFO] - Training Epoch: 1/2, step 1643/7134 completed (loss: 0.5528940558433533, acc: 0.8454545736312866)
[2025-02-13 19:29:49,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:49,656][root][INFO] - Training Epoch: 1/2, step 1644/7134 completed (loss: 0.2703803777694702, acc: 0.9371069073677063)
[2025-02-13 19:29:49,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:50,025][root][INFO] - Training Epoch: 1/2, step 1645/7134 completed (loss: 0.3633309304714203, acc: 0.8896551728248596)
[2025-02-13 19:29:50,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:50,395][root][INFO] - Training Epoch: 1/2, step 1646/7134 completed (loss: 0.4048066735267639, acc: 0.8838709592819214)
[2025-02-13 19:29:50,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:50,751][root][INFO] - Training Epoch: 1/2, step 1647/7134 completed (loss: 0.25896501541137695, acc: 0.9483568072319031)
[2025-02-13 19:29:50,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:51,120][root][INFO] - Training Epoch: 1/2, step 1648/7134 completed (loss: 0.27926722168922424, acc: 0.9402984976768494)
[2025-02-13 19:29:51,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:51,504][root][INFO] - Training Epoch: 1/2, step 1649/7134 completed (loss: 0.23364126682281494, acc: 0.9424778819084167)
[2025-02-13 19:29:51,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:51,883][root][INFO] - Training Epoch: 1/2, step 1650/7134 completed (loss: 0.2496970295906067, acc: 0.9396985173225403)
[2025-02-13 19:29:52,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:52,252][root][INFO] - Training Epoch: 1/2, step 1651/7134 completed (loss: 0.31837373971939087, acc: 0.9545454382896423)
[2025-02-13 19:29:52,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:52,652][root][INFO] - Training Epoch: 1/2, step 1652/7134 completed (loss: 0.25753679871559143, acc: 0.9316239356994629)
[2025-02-13 19:29:52,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:53,022][root][INFO] - Training Epoch: 1/2, step 1653/7134 completed (loss: 0.2809092700481415, acc: 0.8961748480796814)
[2025-02-13 19:29:53,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:53,435][root][INFO] - Training Epoch: 1/2, step 1654/7134 completed (loss: 0.24267594516277313, acc: 0.9227467775344849)
[2025-02-13 19:29:53,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:53,810][root][INFO] - Training Epoch: 1/2, step 1655/7134 completed (loss: 0.2604636251926422, acc: 0.9234693646430969)
[2025-02-13 19:29:53,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:54,166][root][INFO] - Training Epoch: 1/2, step 1656/7134 completed (loss: 0.2985594570636749, acc: 0.9147982001304626)
[2025-02-13 19:29:54,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:54,547][root][INFO] - Training Epoch: 1/2, step 1657/7134 completed (loss: 0.207577183842659, acc: 0.9418604373931885)
[2025-02-13 19:29:54,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:54,898][root][INFO] - Training Epoch: 1/2, step 1658/7134 completed (loss: 0.1693262755870819, acc: 0.954356849193573)
[2025-02-13 19:29:55,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:55,261][root][INFO] - Training Epoch: 1/2, step 1659/7134 completed (loss: 0.25374636054039, acc: 0.9440000057220459)
[2025-02-13 19:29:55,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:55,641][root][INFO] - Training Epoch: 1/2, step 1660/7134 completed (loss: 0.21410122513771057, acc: 0.9471153616905212)
[2025-02-13 19:29:55,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:56,021][root][INFO] - Training Epoch: 1/2, step 1661/7134 completed (loss: 0.2624792158603668, acc: 0.9220778942108154)
[2025-02-13 19:29:56,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:56,381][root][INFO] - Training Epoch: 1/2, step 1662/7134 completed (loss: 0.13073205947875977, acc: 0.9620853066444397)
[2025-02-13 19:29:56,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:56,754][root][INFO] - Training Epoch: 1/2, step 1663/7134 completed (loss: 0.14240583777427673, acc: 0.9531915187835693)
[2025-02-13 19:29:56,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:57,116][root][INFO] - Training Epoch: 1/2, step 1664/7134 completed (loss: 0.17750908434391022, acc: 0.9549999833106995)
[2025-02-13 19:29:57,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:57,489][root][INFO] - Training Epoch: 1/2, step 1665/7134 completed (loss: 0.1698547750711441, acc: 0.9447852969169617)
[2025-02-13 19:29:57,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:57,878][root][INFO] - Training Epoch: 1/2, step 1666/7134 completed (loss: 0.09767935425043106, acc: 0.9747899174690247)
[2025-02-13 19:29:58,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:58,259][root][INFO] - Training Epoch: 1/2, step 1667/7134 completed (loss: 0.1810884028673172, acc: 0.9494949579238892)
[2025-02-13 19:29:58,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:58,651][root][INFO] - Training Epoch: 1/2, step 1668/7134 completed (loss: 0.11118257790803909, acc: 0.9730941653251648)
[2025-02-13 19:29:58,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:59,044][root][INFO] - Training Epoch: 1/2, step 1669/7134 completed (loss: 0.15252776443958282, acc: 0.9656488299369812)
[2025-02-13 19:29:59,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:59,443][root][INFO] - Training Epoch: 1/2, step 1670/7134 completed (loss: 0.32047176361083984, acc: 0.927756667137146)
[2025-02-13 19:29:59,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:59,818][root][INFO] - Training Epoch: 1/2, step 1671/7134 completed (loss: 0.20173968374729156, acc: 0.954081654548645)
[2025-02-13 19:29:59,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:00,200][root][INFO] - Training Epoch: 1/2, step 1672/7134 completed (loss: 0.30720070004463196, acc: 0.9473684430122375)
[2025-02-13 19:30:00,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:00,578][root][INFO] - Training Epoch: 1/2, step 1673/7134 completed (loss: 0.28616979718208313, acc: 0.9503105878829956)
[2025-02-13 19:30:00,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:00,974][root][INFO] - Training Epoch: 1/2, step 1674/7134 completed (loss: 0.5372114777565002, acc: 0.8947368264198303)
[2025-02-13 19:30:01,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:01,414][root][INFO] - Training Epoch: 1/2, step 1675/7134 completed (loss: 0.36952051520347595, acc: 0.8972602486610413)
[2025-02-13 19:30:01,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:01,817][root][INFO] - Training Epoch: 1/2, step 1676/7134 completed (loss: 0.2917960584163666, acc: 0.9358974099159241)
[2025-02-13 19:30:01,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:02,196][root][INFO] - Training Epoch: 1/2, step 1677/7134 completed (loss: 0.5268399119377136, acc: 0.9078013896942139)
[2025-02-13 19:30:02,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:02,568][root][INFO] - Training Epoch: 1/2, step 1678/7134 completed (loss: 0.400794118642807, acc: 0.910179615020752)
[2025-02-13 19:30:02,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:02,936][root][INFO] - Training Epoch: 1/2, step 1679/7134 completed (loss: 0.3789495825767517, acc: 0.9207317233085632)
[2025-02-13 19:30:03,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:03,318][root][INFO] - Training Epoch: 1/2, step 1680/7134 completed (loss: 0.289000928401947, acc: 0.9235293865203857)
[2025-02-13 19:30:03,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:03,699][root][INFO] - Training Epoch: 1/2, step 1681/7134 completed (loss: 0.3927145302295685, acc: 0.8993710875511169)
[2025-02-13 19:30:03,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:04,111][root][INFO] - Training Epoch: 1/2, step 1682/7134 completed (loss: 0.3421233892440796, acc: 0.9246575236320496)
[2025-02-13 19:30:04,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:04,505][root][INFO] - Training Epoch: 1/2, step 1683/7134 completed (loss: 0.25279778242111206, acc: 0.931034505367279)
[2025-02-13 19:30:04,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:04,878][root][INFO] - Training Epoch: 1/2, step 1684/7134 completed (loss: 0.14532750844955444, acc: 0.9858155846595764)
[2025-02-13 19:30:05,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:05,256][root][INFO] - Training Epoch: 1/2, step 1685/7134 completed (loss: 0.21859115362167358, acc: 0.9454545378684998)
[2025-02-13 19:30:05,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:05,640][root][INFO] - Training Epoch: 1/2, step 1686/7134 completed (loss: 0.2997300624847412, acc: 0.9624060392379761)
[2025-02-13 19:30:05,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:06,048][root][INFO] - Training Epoch: 1/2, step 1687/7134 completed (loss: 0.11548654735088348, acc: 0.9557521939277649)
[2025-02-13 19:30:06,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:06,421][root][INFO] - Training Epoch: 1/2, step 1688/7134 completed (loss: 0.20600886642932892, acc: 0.9452054500579834)
[2025-02-13 19:30:06,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:06,836][root][INFO] - Training Epoch: 1/2, step 1689/7134 completed (loss: 0.37966975569725037, acc: 0.9225806593894958)
[2025-02-13 19:30:06,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:07,194][root][INFO] - Training Epoch: 1/2, step 1690/7134 completed (loss: 0.19170072674751282, acc: 0.9292035102844238)
[2025-02-13 19:30:07,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:07,548][root][INFO] - Training Epoch: 1/2, step 1691/7134 completed (loss: 0.1770307421684265, acc: 0.957446813583374)
[2025-02-13 19:30:07,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:07,904][root][INFO] - Training Epoch: 1/2, step 1692/7134 completed (loss: 0.20777064561843872, acc: 0.9430894255638123)
[2025-02-13 19:30:08,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:08,272][root][INFO] - Training Epoch: 1/2, step 1693/7134 completed (loss: 0.1959071308374405, acc: 0.9591836929321289)
[2025-02-13 19:30:08,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:08,664][root][INFO] - Training Epoch: 1/2, step 1694/7134 completed (loss: 0.3392347991466522, acc: 0.9103448390960693)
[2025-02-13 19:30:08,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:09,046][root][INFO] - Training Epoch: 1/2, step 1695/7134 completed (loss: 0.23926463723182678, acc: 0.9327731132507324)
[2025-02-13 19:30:09,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:09,422][root][INFO] - Training Epoch: 1/2, step 1696/7134 completed (loss: 0.288941890001297, acc: 0.9248120188713074)
[2025-02-13 19:30:09,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:09,774][root][INFO] - Training Epoch: 1/2, step 1697/7134 completed (loss: 0.27780723571777344, acc: 0.9650349617004395)
[2025-02-13 19:30:09,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:10,157][root][INFO] - Training Epoch: 1/2, step 1698/7134 completed (loss: 0.06536860764026642, acc: 0.9930070042610168)
[2025-02-13 19:30:10,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:10,545][root][INFO] - Training Epoch: 1/2, step 1699/7134 completed (loss: 0.18613417446613312, acc: 0.9552238583564758)
[2025-02-13 19:30:10,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:10,935][root][INFO] - Training Epoch: 1/2, step 1700/7134 completed (loss: 0.27994370460510254, acc: 0.9253731369972229)
[2025-02-13 19:30:11,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:11,291][root][INFO] - Training Epoch: 1/2, step 1701/7134 completed (loss: 0.31702908873558044, acc: 0.9200000166893005)
[2025-02-13 19:30:11,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:11,667][root][INFO] - Training Epoch: 1/2, step 1702/7134 completed (loss: 0.32019755244255066, acc: 0.9221556782722473)
[2025-02-13 19:30:11,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:12,030][root][INFO] - Training Epoch: 1/2, step 1703/7134 completed (loss: 0.45285889506340027, acc: 0.887417197227478)
[2025-02-13 19:30:12,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:12,381][root][INFO] - Training Epoch: 1/2, step 1704/7134 completed (loss: 0.5495656132698059, acc: 0.8902438879013062)
[2025-02-13 19:30:12,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:12,779][root][INFO] - Training Epoch: 1/2, step 1705/7134 completed (loss: 0.40231338143348694, acc: 0.9200000166893005)
[2025-02-13 19:30:12,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:13,096][root][INFO] - Training Epoch: 1/2, step 1706/7134 completed (loss: 0.2738715708255768, acc: 0.9571428298950195)
[2025-02-13 19:30:13,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:13,470][root][INFO] - Training Epoch: 1/2, step 1707/7134 completed (loss: 0.3457830250263214, acc: 0.931034505367279)
[2025-02-13 19:30:13,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:13,839][root][INFO] - Training Epoch: 1/2, step 1708/7134 completed (loss: 0.23327752947807312, acc: 0.9481481313705444)
[2025-02-13 19:30:13,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:14,200][root][INFO] - Training Epoch: 1/2, step 1709/7134 completed (loss: 0.3957005441188812, acc: 0.8866666555404663)
[2025-02-13 19:30:14,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:14,603][root][INFO] - Training Epoch: 1/2, step 1710/7134 completed (loss: 0.3735823333263397, acc: 0.8986486196517944)
[2025-02-13 19:30:14,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:14,985][root][INFO] - Training Epoch: 1/2, step 1711/7134 completed (loss: 0.33468490839004517, acc: 0.9354838728904724)
[2025-02-13 19:30:15,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:15,354][root][INFO] - Training Epoch: 1/2, step 1712/7134 completed (loss: 0.20232810080051422, acc: 0.9523809552192688)
[2025-02-13 19:30:15,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:15,721][root][INFO] - Training Epoch: 1/2, step 1713/7134 completed (loss: 0.26802167296409607, acc: 0.9047619104385376)
[2025-02-13 19:30:15,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:16,116][root][INFO] - Training Epoch: 1/2, step 1714/7134 completed (loss: 0.21104733645915985, acc: 0.9503546357154846)
[2025-02-13 19:30:16,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:16,517][root][INFO] - Training Epoch: 1/2, step 1715/7134 completed (loss: 0.26953691244125366, acc: 0.9339622855186462)
[2025-02-13 19:30:16,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:16,885][root][INFO] - Training Epoch: 1/2, step 1716/7134 completed (loss: 0.45069965720176697, acc: 0.9210526347160339)
[2025-02-13 19:30:17,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:17,235][root][INFO] - Training Epoch: 1/2, step 1717/7134 completed (loss: 0.241104394197464, acc: 0.9457364082336426)
[2025-02-13 19:30:17,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:17,578][root][INFO] - Training Epoch: 1/2, step 1718/7134 completed (loss: 0.4033969044685364, acc: 0.8974359035491943)
[2025-02-13 19:30:17,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:17,944][root][INFO] - Training Epoch: 1/2, step 1719/7134 completed (loss: 0.32999545335769653, acc: 0.9112426042556763)
[2025-02-13 19:30:18,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:18,334][root][INFO] - Training Epoch: 1/2, step 1720/7134 completed (loss: 0.6040011048316956, acc: 0.8384615182876587)
[2025-02-13 19:30:18,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:18,693][root][INFO] - Training Epoch: 1/2, step 1721/7134 completed (loss: 0.24456624686717987, acc: 0.921875)
[2025-02-13 19:30:18,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:19,135][root][INFO] - Training Epoch: 1/2, step 1722/7134 completed (loss: 0.17619305849075317, acc: 0.9696969985961914)
[2025-02-13 19:30:19,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:19,540][root][INFO] - Training Epoch: 1/2, step 1723/7134 completed (loss: 0.5527806282043457, acc: 0.8888888955116272)
[2025-02-13 19:30:19,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:19,927][root][INFO] - Training Epoch: 1/2, step 1724/7134 completed (loss: 0.3665001392364502, acc: 0.921875)
[2025-02-13 19:30:20,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:20,373][root][INFO] - Training Epoch: 1/2, step 1725/7134 completed (loss: 0.29397881031036377, acc: 0.9402984976768494)
[2025-02-13 19:30:20,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:20,735][root][INFO] - Training Epoch: 1/2, step 1726/7134 completed (loss: 0.1570560336112976, acc: 0.9569892287254333)
[2025-02-13 19:30:20,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:21,089][root][INFO] - Training Epoch: 1/2, step 1727/7134 completed (loss: 0.18902674317359924, acc: 0.9469026327133179)
[2025-02-13 19:30:21,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:21,472][root][INFO] - Training Epoch: 1/2, step 1728/7134 completed (loss: 0.09753499925136566, acc: 0.9846153855323792)
[2025-02-13 19:30:21,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:21,820][root][INFO] - Training Epoch: 1/2, step 1729/7134 completed (loss: 0.3106931746006012, acc: 0.9009009003639221)
[2025-02-13 19:30:21,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:22,188][root][INFO] - Training Epoch: 1/2, step 1730/7134 completed (loss: 0.18025344610214233, acc: 0.9345794320106506)
[2025-02-13 19:30:22,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:22,529][root][INFO] - Training Epoch: 1/2, step 1731/7134 completed (loss: 0.69044429063797, acc: 0.8433734774589539)
[2025-02-13 19:30:22,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:22,865][root][INFO] - Training Epoch: 1/2, step 1732/7134 completed (loss: 0.1774391084909439, acc: 0.9537037014961243)
[2025-02-13 19:30:22,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:23,181][root][INFO] - Training Epoch: 1/2, step 1733/7134 completed (loss: 0.41229677200317383, acc: 0.8990825414657593)
[2025-02-13 19:30:23,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:23,559][root][INFO] - Training Epoch: 1/2, step 1734/7134 completed (loss: 0.385755330324173, acc: 0.9156626462936401)
[2025-02-13 19:30:23,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:23,919][root][INFO] - Training Epoch: 1/2, step 1735/7134 completed (loss: 0.14566093683242798, acc: 0.9735099077224731)
[2025-02-13 19:30:24,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:24,285][root][INFO] - Training Epoch: 1/2, step 1736/7134 completed (loss: 0.3031850755214691, acc: 0.9556962251663208)
[2025-02-13 19:30:24,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:24,652][root][INFO] - Training Epoch: 1/2, step 1737/7134 completed (loss: 0.2802990972995758, acc: 0.9399999976158142)
[2025-02-13 19:30:24,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:25,023][root][INFO] - Training Epoch: 1/2, step 1738/7134 completed (loss: 0.11302898824214935, acc: 0.9720670580863953)
[2025-02-13 19:30:25,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:25,389][root][INFO] - Training Epoch: 1/2, step 1739/7134 completed (loss: 0.27857017517089844, acc: 0.9248120188713074)
[2025-02-13 19:30:25,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:25,755][root][INFO] - Training Epoch: 1/2, step 1740/7134 completed (loss: 0.20077762007713318, acc: 0.9391891956329346)
[2025-02-13 19:30:25,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:26,149][root][INFO] - Training Epoch: 1/2, step 1741/7134 completed (loss: 0.22789257764816284, acc: 0.9254658222198486)
[2025-02-13 19:30:26,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:26,581][root][INFO] - Training Epoch: 1/2, step 1742/7134 completed (loss: 0.250833123922348, acc: 0.940397322177887)
[2025-02-13 19:30:26,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:26,957][root][INFO] - Training Epoch: 1/2, step 1743/7134 completed (loss: 0.21982906758785248, acc: 0.9387755393981934)
[2025-02-13 19:30:27,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:27,322][root][INFO] - Training Epoch: 1/2, step 1744/7134 completed (loss: 0.16332946717739105, acc: 0.9548872113227844)
[2025-02-13 19:30:27,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:27,688][root][INFO] - Training Epoch: 1/2, step 1745/7134 completed (loss: 0.17857472598552704, acc: 0.970588207244873)
[2025-02-13 19:30:27,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:28,035][root][INFO] - Training Epoch: 1/2, step 1746/7134 completed (loss: 0.15109995007514954, acc: 0.957446813583374)
[2025-02-13 19:30:28,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:28,434][root][INFO] - Training Epoch: 1/2, step 1747/7134 completed (loss: 0.25187763571739197, acc: 0.9390243887901306)
[2025-02-13 19:30:28,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:28,818][root][INFO] - Training Epoch: 1/2, step 1748/7134 completed (loss: 0.25350692868232727, acc: 0.9352940917015076)
[2025-02-13 19:30:28,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:29,197][root][INFO] - Training Epoch: 1/2, step 1749/7134 completed (loss: 0.3388075828552246, acc: 0.926701545715332)
[2025-02-13 19:30:29,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:29,589][root][INFO] - Training Epoch: 1/2, step 1750/7134 completed (loss: 0.38329383730888367, acc: 0.9349112510681152)
[2025-02-13 19:30:29,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:29,949][root][INFO] - Training Epoch: 1/2, step 1751/7134 completed (loss: 0.40449029207229614, acc: 0.9226190447807312)
[2025-02-13 19:30:30,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:30,316][root][INFO] - Training Epoch: 1/2, step 1752/7134 completed (loss: 0.4064929485321045, acc: 0.9151515364646912)
[2025-02-13 19:30:30,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:30,662][root][INFO] - Training Epoch: 1/2, step 1753/7134 completed (loss: 0.3298385441303253, acc: 0.9271523356437683)
[2025-02-13 19:30:30,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:31,075][root][INFO] - Training Epoch: 1/2, step 1754/7134 completed (loss: 0.3770199716091156, acc: 0.9084967374801636)
[2025-02-13 19:30:31,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:31,442][root][INFO] - Training Epoch: 1/2, step 1755/7134 completed (loss: 0.16444213688373566, acc: 0.9542483687400818)
[2025-02-13 19:30:31,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:31,805][root][INFO] - Training Epoch: 1/2, step 1756/7134 completed (loss: 0.4059199094772339, acc: 0.8866666555404663)
[2025-02-13 19:30:31,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:32,142][root][INFO] - Training Epoch: 1/2, step 1757/7134 completed (loss: 0.2124512642621994, acc: 0.9586777091026306)
[2025-02-13 19:30:32,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:32,521][root][INFO] - Training Epoch: 1/2, step 1758/7134 completed (loss: 0.1113358587026596, acc: 0.9647058844566345)
[2025-02-13 19:30:32,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:32,902][root][INFO] - Training Epoch: 1/2, step 1759/7134 completed (loss: 0.14369671046733856, acc: 0.9539473652839661)
[2025-02-13 19:30:33,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:33,265][root][INFO] - Training Epoch: 1/2, step 1760/7134 completed (loss: 0.12719537317752838, acc: 0.9754098653793335)
[2025-02-13 19:30:33,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:33,631][root][INFO] - Training Epoch: 1/2, step 1761/7134 completed (loss: 0.2404390275478363, acc: 0.949999988079071)
[2025-02-13 19:30:33,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:33,998][root][INFO] - Training Epoch: 1/2, step 1762/7134 completed (loss: 0.18945856392383575, acc: 0.9417475461959839)
[2025-02-13 19:30:34,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:34,361][root][INFO] - Training Epoch: 1/2, step 1763/7134 completed (loss: 0.24932986497879028, acc: 0.9246575236320496)
[2025-02-13 19:30:34,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:34,722][root][INFO] - Training Epoch: 1/2, step 1764/7134 completed (loss: 0.34055668115615845, acc: 0.9274193644523621)
[2025-02-13 19:30:34,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:35,081][root][INFO] - Training Epoch: 1/2, step 1765/7134 completed (loss: 0.30070960521698, acc: 0.9215686321258545)
[2025-02-13 19:30:35,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:35,438][root][INFO] - Training Epoch: 1/2, step 1766/7134 completed (loss: 0.3700805902481079, acc: 0.9155844449996948)
[2025-02-13 19:30:35,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:35,809][root][INFO] - Training Epoch: 1/2, step 1767/7134 completed (loss: 0.3996141850948334, acc: 0.909604549407959)
[2025-02-13 19:30:35,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:36,178][root][INFO] - Training Epoch: 1/2, step 1768/7134 completed (loss: 0.06635553389787674, acc: 0.9821428656578064)
[2025-02-13 19:30:36,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:36,594][root][INFO] - Training Epoch: 1/2, step 1769/7134 completed (loss: 0.24885477125644684, acc: 0.9399999976158142)
[2025-02-13 19:30:36,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:36,974][root][INFO] - Training Epoch: 1/2, step 1770/7134 completed (loss: 0.18703503906726837, acc: 0.965753436088562)
[2025-02-13 19:30:37,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:37,347][root][INFO] - Training Epoch: 1/2, step 1771/7134 completed (loss: 0.27804821729660034, acc: 0.9285714030265808)
[2025-02-13 19:30:37,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:37,740][root][INFO] - Training Epoch: 1/2, step 1772/7134 completed (loss: 0.22641676664352417, acc: 0.9571428298950195)
[2025-02-13 19:30:37,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:38,132][root][INFO] - Training Epoch: 1/2, step 1773/7134 completed (loss: 0.27434638142585754, acc: 0.936170220375061)
[2025-02-13 19:30:38,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:38,496][root][INFO] - Training Epoch: 1/2, step 1774/7134 completed (loss: 0.04442411661148071, acc: 0.9924242496490479)
[2025-02-13 19:30:38,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:38,848][root][INFO] - Training Epoch: 1/2, step 1775/7134 completed (loss: 0.09933911263942719, acc: 0.9726027250289917)
[2025-02-13 19:30:38,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:39,198][root][INFO] - Training Epoch: 1/2, step 1776/7134 completed (loss: 0.18677477538585663, acc: 0.9448275566101074)
[2025-02-13 19:30:39,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:39,549][root][INFO] - Training Epoch: 1/2, step 1777/7134 completed (loss: 0.07089754939079285, acc: 0.9745222926139832)
[2025-02-13 19:30:39,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:39,960][root][INFO] - Training Epoch: 1/2, step 1778/7134 completed (loss: 0.33693727850914, acc: 0.9245283007621765)
[2025-02-13 19:30:40,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:40,325][root][INFO] - Training Epoch: 1/2, step 1779/7134 completed (loss: 0.27458974719047546, acc: 0.9380530714988708)
[2025-02-13 19:30:40,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:40,759][root][INFO] - Training Epoch: 1/2, step 1780/7134 completed (loss: 0.17479507625102997, acc: 0.9640718698501587)
[2025-02-13 19:30:40,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:41,099][root][INFO] - Training Epoch: 1/2, step 1781/7134 completed (loss: 0.1456967145204544, acc: 0.9731543660163879)
[2025-02-13 19:30:41,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:41,519][root][INFO] - Training Epoch: 1/2, step 1782/7134 completed (loss: 0.13389825820922852, acc: 0.9679487347602844)
[2025-02-13 19:30:42,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:42,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:43,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:43,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:43,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:44,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:44,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:45,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:45,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:45,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:46,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:46,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:46,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:47,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:47,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:48,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:48,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:48,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:49,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:49,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:49,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:50,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:50,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:50,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:51,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:51,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:52,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:52,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:52,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:53,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:53,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:54,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:54,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:54,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:55,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:55,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:55,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:56,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:56,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:56,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:57,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:57,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:58,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:58,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:58,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:59,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:59,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:00,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:00,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:00,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:01,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:01,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:01,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:03,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:03,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:05,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:05,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:05,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:06,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:06,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:06,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:07,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:07,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:07,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:08,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:08,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:08,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:09,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:09,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:09,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:10,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:10,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:10,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:11,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:11,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:11,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:12,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:12,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:12,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:13,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:13,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:14,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:14,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:14,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:16,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:16,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:16,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:17,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:17,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:18,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:18,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:18,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:19,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:19,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:19,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:20,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:20,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:20,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:21,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:21,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:21,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:22,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:22,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:22,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:23,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:23,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:25,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:25,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:25,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:26,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:26,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:26,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:28,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:28,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:28,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:29,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:29,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:29,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:30,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:30,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:30,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:31,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:31,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:31,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:32,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:32,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:33,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:33,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:33,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:34,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:34,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:34,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:35,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:35,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:35,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:36,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:36,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:36,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:37,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:37,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:37,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:38,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:38,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:39,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:39,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:39,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:40,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:40,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:40,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:41,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:41,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:41,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:42,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:42,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:43,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:43,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:43,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:44,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:44,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:44,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:45,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:45,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:45,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:46,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:46,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:46,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:47,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:47,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:47,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:48,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:48,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:48,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:49,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:49,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:49,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:50,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:50,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:50,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:51,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:51,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:51,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:52,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:52,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:52,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:54,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:54,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:54,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:55,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:55,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:56,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:56,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:56,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:57,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:57,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:57,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:58,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:58,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:58,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:59,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:59,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:59,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:00,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:00,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:00,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:01,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:01,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:01,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:02,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:02,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:02,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:03,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:03,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:04,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:04,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:05,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:05,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:05,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:05,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:06,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:06,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:06,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:07,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:07,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:08,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:08,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:08,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:09,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:09,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:09,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:10,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:10,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:11,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:11,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:11,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:12,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:12,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:12,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:13,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:13,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:13,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:14,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:14,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:14,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:15,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:15,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:15,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:16,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:16,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:17,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:17,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:17,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:18,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:18,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:18,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:19,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:19,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:20,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:20,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:21,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:21,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:21,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:22,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:22,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:22,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:23,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:23,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:24,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:24,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:24,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:25,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:25,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:26,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:26,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:26,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:27,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:27,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:27,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:28,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:28,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:28,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:29,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:29,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:29,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:30,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:30,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:30,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:31,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:31,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:31,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:32,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:32,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:32,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:33,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:33,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:34,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:34,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:34,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:34,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:35,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:35,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:35,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:36,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:36,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:37,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:37,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:37,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:38,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:38,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:38,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:39,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:39,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:40,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:40,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:40,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:41,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:41,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:41,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:42,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:42,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:42,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:43,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:43,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:44,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:44,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:44,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:45,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:45,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:45,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:46,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:46,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:46,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:47,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:47,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:47,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:48,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:48,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:49,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:49,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:50,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:50,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:51,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:51,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:51,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:52,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:52,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:52,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:53,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:53,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:54,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:54,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:54,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:55,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:55,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:55,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:56,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:56,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:56,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:57,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:57,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:58,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:58,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:58,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:59,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:59,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:59,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:00,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:00,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:01,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:01,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:01,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:02,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:02,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:02,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:03,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:03,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:03,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:04,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:04,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:04,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:05,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:05,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:05,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:06,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:06,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:06,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:07,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:07,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:07,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:08,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:08,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:09,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:09,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:09,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:11,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:11,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:12,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:12,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:12,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:13,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:13,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:13,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:14,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:14,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:14,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:16,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:16,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:16,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:17,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:17,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:17,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:18,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:18,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:18,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:20,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:20,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:20,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:21,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:21,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:21,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:23,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:23,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:23,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:24,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:24,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:24,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:25,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:25,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:25,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:26,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:26,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:27,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:27,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:27,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:28,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:28,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:28,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:28,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:29,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:29,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:29,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:30,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:30,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:32,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:32,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:33,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:33,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:33,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:34,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:34,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:34,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:35,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:35,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:36,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:36,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:36,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:37,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:37,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:37,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:38,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:38,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:39,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:39,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:39,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:40,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:40,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:40,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:41,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:41,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:41,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:42,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:42,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:42,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:44,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:44,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:44,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:45,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:45,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:46,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:46,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:46,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:47,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:47,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:47,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:48,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:48,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:48,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:49,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:49,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:50,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:50,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:50,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:51,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:51,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:51,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:52,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:52,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:52,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:53,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:53,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:53,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:54,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:54,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:55,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:55,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:55,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:56,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:56,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:56,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:57,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:57,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:57,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:59,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:59,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:59,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:00,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:00,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:00,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:01,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:01,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:02,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:02,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:02,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:03,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:03,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:03,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:04,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:04,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:04,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:05,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:05,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:06,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:06,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:06,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:07,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:07,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:08,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:08,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:08,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:09,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:09,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:09,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:10,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:10,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:10,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:11,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:11,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:11,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:12,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:12,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:12,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:13,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:13,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:13,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:14,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:14,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:14,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:15,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:15,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:15,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:16,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:16,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:16,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:17,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:17,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:17,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:17,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:19,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:19,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:19,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:20,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:21,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:21,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:21,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:22,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:22,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:23,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:23,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:24,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:24,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:24,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:26,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:26,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:26,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:27,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:27,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:27,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:28,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:28,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:29,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:29,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:29,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:30,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:30,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:30,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:31,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:31,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:31,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:32,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:32,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:33,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:33,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:33,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:34,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:34,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:34,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:35,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:35,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:35,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:36,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:36,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:37,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:37,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:37,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:38,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:38,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:39,340][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3824, device='cuda:0') eval_epoch_loss=tensor(0.3238, device='cuda:0') eval_epoch_acc=tensor(0.9234, device='cuda:0')
[2025-02-13 19:34:39,342][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 19:34:39,343][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 19:34:39,588][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_1783_loss_0.3238261938095093/model.pt
[2025-02-13 19:34:39,600][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 19:34:39,602][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.3238261938095093
[2025-02-13 19:34:39,602][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9233985543251038
[2025-02-13 19:34:39,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:40,026][root][INFO] - Training Epoch: 1/2, step 1783/7134 completed (loss: 0.19835111498832703, acc: 0.9479768872261047)
[2025-02-13 19:34:40,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:40,389][root][INFO] - Training Epoch: 1/2, step 1784/7134 completed (loss: 0.14396819472312927, acc: 0.9743589758872986)
[2025-02-13 19:34:40,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:40,784][root][INFO] - Training Epoch: 1/2, step 1785/7134 completed (loss: 0.28862375020980835, acc: 0.9397590160369873)
[2025-02-13 19:34:40,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:41,150][root][INFO] - Training Epoch: 1/2, step 1786/7134 completed (loss: 0.13057075440883636, acc: 0.9612902998924255)
[2025-02-13 19:34:41,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:41,519][root][INFO] - Training Epoch: 1/2, step 1787/7134 completed (loss: 0.34057852625846863, acc: 0.9402984976768494)
[2025-02-13 19:34:41,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:41,870][root][INFO] - Training Epoch: 1/2, step 1788/7134 completed (loss: 0.23146985471248627, acc: 0.931034505367279)
[2025-02-13 19:34:42,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:42,229][root][INFO] - Training Epoch: 1/2, step 1789/7134 completed (loss: 0.12264194339513779, acc: 0.9802631735801697)
[2025-02-13 19:34:42,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:42,617][root][INFO] - Training Epoch: 1/2, step 1790/7134 completed (loss: 0.5978588461875916, acc: 0.8774193525314331)
[2025-02-13 19:34:42,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:42,976][root][INFO] - Training Epoch: 1/2, step 1791/7134 completed (loss: 0.5565981864929199, acc: 0.8650306463241577)
[2025-02-13 19:34:43,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:43,338][root][INFO] - Training Epoch: 1/2, step 1792/7134 completed (loss: 1.018307089805603, acc: 0.8044692873954773)
[2025-02-13 19:34:43,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:43,707][root][INFO] - Training Epoch: 1/2, step 1793/7134 completed (loss: 0.7218692302703857, acc: 0.8461538553237915)
[2025-02-13 19:34:43,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:44,063][root][INFO] - Training Epoch: 1/2, step 1794/7134 completed (loss: 0.48230692744255066, acc: 0.9013158082962036)
[2025-02-13 19:34:44,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:44,438][root][INFO] - Training Epoch: 1/2, step 1795/7134 completed (loss: 0.856410026550293, acc: 0.831250011920929)
[2025-02-13 19:34:44,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:44,800][root][INFO] - Training Epoch: 1/2, step 1796/7134 completed (loss: 0.6500303149223328, acc: 0.8531073331832886)
[2025-02-13 19:34:44,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:45,159][root][INFO] - Training Epoch: 1/2, step 1797/7134 completed (loss: 0.539322555065155, acc: 0.868571400642395)
[2025-02-13 19:34:45,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:45,532][root][INFO] - Training Epoch: 1/2, step 1798/7134 completed (loss: 0.49000054597854614, acc: 0.8837209343910217)
[2025-02-13 19:34:45,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:45,906][root][INFO] - Training Epoch: 1/2, step 1799/7134 completed (loss: 0.39118558168411255, acc: 0.9191918969154358)
[2025-02-13 19:34:46,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:46,270][root][INFO] - Training Epoch: 1/2, step 1800/7134 completed (loss: 0.45847707986831665, acc: 0.8999999761581421)
[2025-02-13 19:34:46,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:46,641][root][INFO] - Training Epoch: 1/2, step 1801/7134 completed (loss: 0.4647686183452606, acc: 0.8918918967247009)
[2025-02-13 19:34:46,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:47,003][root][INFO] - Training Epoch: 1/2, step 1802/7134 completed (loss: 0.3550381362438202, acc: 0.8882978558540344)
[2025-02-13 19:34:47,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:47,361][root][INFO] - Training Epoch: 1/2, step 1803/7134 completed (loss: 0.5033478736877441, acc: 0.8597561120986938)
[2025-02-13 19:34:47,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:47,742][root][INFO] - Training Epoch: 1/2, step 1804/7134 completed (loss: 0.1977996826171875, acc: 0.9417989253997803)
[2025-02-13 19:34:47,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:48,109][root][INFO] - Training Epoch: 1/2, step 1805/7134 completed (loss: 0.2853122651576996, acc: 0.9298245906829834)
[2025-02-13 19:34:48,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:48,485][root][INFO] - Training Epoch: 1/2, step 1806/7134 completed (loss: 0.29788848757743835, acc: 0.9344262480735779)
[2025-02-13 19:34:48,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:48,844][root][INFO] - Training Epoch: 1/2, step 1807/7134 completed (loss: 0.3842543959617615, acc: 0.9216867685317993)
[2025-02-13 19:34:48,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:49,191][root][INFO] - Training Epoch: 1/2, step 1808/7134 completed (loss: 0.1668299436569214, acc: 0.9435028433799744)
[2025-02-13 19:34:49,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:49,566][root][INFO] - Training Epoch: 1/2, step 1809/7134 completed (loss: 0.17013876140117645, acc: 0.9595375657081604)
[2025-02-13 19:34:49,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:49,938][root][INFO] - Training Epoch: 1/2, step 1810/7134 completed (loss: 0.15252836048603058, acc: 0.9560439586639404)
[2025-02-13 19:34:50,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:50,300][root][INFO] - Training Epoch: 1/2, step 1811/7134 completed (loss: 0.3315620720386505, acc: 0.914893627166748)
[2025-02-13 19:34:50,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:50,657][root][INFO] - Training Epoch: 1/2, step 1812/7134 completed (loss: 0.2496166080236435, acc: 0.9491525292396545)
[2025-02-13 19:34:50,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:51,037][root][INFO] - Training Epoch: 1/2, step 1813/7134 completed (loss: 0.34051281213760376, acc: 0.9116021990776062)
[2025-02-13 19:34:51,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:51,421][root][INFO] - Training Epoch: 1/2, step 1814/7134 completed (loss: 0.23171883821487427, acc: 0.9337016344070435)
[2025-02-13 19:34:51,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:51,787][root][INFO] - Training Epoch: 1/2, step 1815/7134 completed (loss: 0.25768646597862244, acc: 0.9299362897872925)
[2025-02-13 19:34:51,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:52,156][root][INFO] - Training Epoch: 1/2, step 1816/7134 completed (loss: 0.37645062804222107, acc: 0.8819444179534912)
[2025-02-13 19:34:52,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:52,525][root][INFO] - Training Epoch: 1/2, step 1817/7134 completed (loss: 0.2716275751590729, acc: 0.9375)
[2025-02-13 19:34:52,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:52,894][root][INFO] - Training Epoch: 1/2, step 1818/7134 completed (loss: 0.22592897713184357, acc: 0.9371727705001831)
[2025-02-13 19:34:53,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:53,256][root][INFO] - Training Epoch: 1/2, step 1819/7134 completed (loss: 0.15117080509662628, acc: 0.9651162624359131)
[2025-02-13 19:34:53,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:53,659][root][INFO] - Training Epoch: 1/2, step 1820/7134 completed (loss: 0.21372836828231812, acc: 0.9433962106704712)
[2025-02-13 19:34:53,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:54,014][root][INFO] - Training Epoch: 1/2, step 1821/7134 completed (loss: 0.20863009989261627, acc: 0.9579439163208008)
[2025-02-13 19:34:54,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:54,363][root][INFO] - Training Epoch: 1/2, step 1822/7134 completed (loss: 0.17254839837551117, acc: 0.9736841917037964)
[2025-02-13 19:34:54,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:54,719][root][INFO] - Training Epoch: 1/2, step 1823/7134 completed (loss: 0.25914466381073, acc: 0.930232584476471)
[2025-02-13 19:34:54,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:55,085][root][INFO] - Training Epoch: 1/2, step 1824/7134 completed (loss: 0.24557626247406006, acc: 0.9617486596107483)
[2025-02-13 19:34:55,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:55,448][root][INFO] - Training Epoch: 1/2, step 1825/7134 completed (loss: 0.19577360153198242, acc: 0.9579439163208008)
[2025-02-13 19:34:55,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:55,806][root][INFO] - Training Epoch: 1/2, step 1826/7134 completed (loss: 0.140126034617424, acc: 0.9590643048286438)
[2025-02-13 19:34:55,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:56,178][root][INFO] - Training Epoch: 1/2, step 1827/7134 completed (loss: 0.21589979529380798, acc: 0.9431279897689819)
[2025-02-13 19:34:56,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:56,537][root][INFO] - Training Epoch: 1/2, step 1828/7134 completed (loss: 0.12068816274404526, acc: 0.9777777791023254)
[2025-02-13 19:34:56,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:56,895][root][INFO] - Training Epoch: 1/2, step 1829/7134 completed (loss: 0.24541963636875153, acc: 0.9462365508079529)
[2025-02-13 19:34:57,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:57,258][root][INFO] - Training Epoch: 1/2, step 1830/7134 completed (loss: 0.2657131850719452, acc: 0.9378238320350647)
[2025-02-13 19:34:57,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:57,600][root][INFO] - Training Epoch: 1/2, step 1831/7134 completed (loss: 0.09205741435289383, acc: 0.9772727489471436)
[2025-02-13 19:34:57,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:57,941][root][INFO] - Training Epoch: 1/2, step 1832/7134 completed (loss: 0.4503030478954315, acc: 0.8914728760719299)
[2025-02-13 19:34:58,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:58,306][root][INFO] - Training Epoch: 1/2, step 1833/7134 completed (loss: 0.7684933543205261, acc: 0.8617886304855347)
[2025-02-13 19:34:58,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:58,685][root][INFO] - Training Epoch: 1/2, step 1834/7134 completed (loss: 0.265635222196579, acc: 0.9358974099159241)
[2025-02-13 19:34:58,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:59,037][root][INFO] - Training Epoch: 1/2, step 1835/7134 completed (loss: 0.41302937269210815, acc: 0.9350649118423462)
[2025-02-13 19:34:59,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:59,420][root][INFO] - Training Epoch: 1/2, step 1836/7134 completed (loss: 0.2784750461578369, acc: 0.9251337051391602)
[2025-02-13 19:34:59,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:59,783][root][INFO] - Training Epoch: 1/2, step 1837/7134 completed (loss: 0.2821633517742157, acc: 0.9340659379959106)
[2025-02-13 19:34:59,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:00,132][root][INFO] - Training Epoch: 1/2, step 1838/7134 completed (loss: 0.23719067871570587, acc: 0.9449541568756104)
[2025-02-13 19:35:00,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:00,514][root][INFO] - Training Epoch: 1/2, step 1839/7134 completed (loss: 0.22713924944400787, acc: 0.9395604133605957)
[2025-02-13 19:35:00,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:00,946][root][INFO] - Training Epoch: 1/2, step 1840/7134 completed (loss: 0.3068574368953705, acc: 0.892405092716217)
[2025-02-13 19:35:01,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:01,300][root][INFO] - Training Epoch: 1/2, step 1841/7134 completed (loss: 0.38317933678627014, acc: 0.918181836605072)
[2025-02-13 19:35:01,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:01,648][root][INFO] - Training Epoch: 1/2, step 1842/7134 completed (loss: 0.20849435031414032, acc: 0.9390243887901306)
[2025-02-13 19:35:01,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:02,004][root][INFO] - Training Epoch: 1/2, step 1843/7134 completed (loss: 0.31653735041618347, acc: 0.9345238208770752)
[2025-02-13 19:35:02,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:02,373][root][INFO] - Training Epoch: 1/2, step 1844/7134 completed (loss: 0.11180984973907471, acc: 0.957446813583374)
[2025-02-13 19:35:02,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:02,746][root][INFO] - Training Epoch: 1/2, step 1845/7134 completed (loss: 0.26800352334976196, acc: 0.9130434989929199)
[2025-02-13 19:35:02,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:03,095][root][INFO] - Training Epoch: 1/2, step 1846/7134 completed (loss: 0.19447046518325806, acc: 0.942307710647583)
[2025-02-13 19:35:03,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:03,468][root][INFO] - Training Epoch: 1/2, step 1847/7134 completed (loss: 0.4176067113876343, acc: 0.8863636255264282)
[2025-02-13 19:35:03,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:03,832][root][INFO] - Training Epoch: 1/2, step 1848/7134 completed (loss: 0.4664630591869354, acc: 0.8829787373542786)
[2025-02-13 19:35:03,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:04,182][root][INFO] - Training Epoch: 1/2, step 1849/7134 completed (loss: 0.24028711020946503, acc: 0.9436619877815247)
[2025-02-13 19:35:04,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:04,539][root][INFO] - Training Epoch: 1/2, step 1850/7134 completed (loss: 0.38534092903137207, acc: 0.9226804375648499)
[2025-02-13 19:35:04,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:04,904][root][INFO] - Training Epoch: 1/2, step 1851/7134 completed (loss: 0.44025924801826477, acc: 0.8871794939041138)
[2025-02-13 19:35:05,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:05,293][root][INFO] - Training Epoch: 1/2, step 1852/7134 completed (loss: 0.49055710434913635, acc: 0.887499988079071)
[2025-02-13 19:35:05,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:05,688][root][INFO] - Training Epoch: 1/2, step 1853/7134 completed (loss: 0.31002792716026306, acc: 0.909547746181488)
[2025-02-13 19:35:05,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:06,050][root][INFO] - Training Epoch: 1/2, step 1854/7134 completed (loss: 0.274863600730896, acc: 0.9276018142700195)
[2025-02-13 19:35:06,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:06,422][root][INFO] - Training Epoch: 1/2, step 1855/7134 completed (loss: 0.23101204633712769, acc: 0.9336493015289307)
[2025-02-13 19:35:06,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:06,781][root][INFO] - Training Epoch: 1/2, step 1856/7134 completed (loss: 0.2350989580154419, acc: 0.9387755393981934)
[2025-02-13 19:35:06,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:07,185][root][INFO] - Training Epoch: 1/2, step 1857/7134 completed (loss: 0.41387277841567993, acc: 0.8970588445663452)
[2025-02-13 19:35:07,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:07,550][root][INFO] - Training Epoch: 1/2, step 1858/7134 completed (loss: 0.21130268275737762, acc: 0.9387755393981934)
[2025-02-13 19:35:07,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:07,916][root][INFO] - Training Epoch: 1/2, step 1859/7134 completed (loss: 0.2401508092880249, acc: 0.9282296895980835)
[2025-02-13 19:35:08,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:08,275][root][INFO] - Training Epoch: 1/2, step 1860/7134 completed (loss: 0.24236741662025452, acc: 0.9398906826972961)
[2025-02-13 19:35:08,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:08,632][root][INFO] - Training Epoch: 1/2, step 1861/7134 completed (loss: 0.21300294995307922, acc: 0.9581151604652405)
[2025-02-13 19:35:08,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:09,002][root][INFO] - Training Epoch: 1/2, step 1862/7134 completed (loss: 0.17082557082176208, acc: 0.9576719403266907)
[2025-02-13 19:35:09,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:09,366][root][INFO] - Training Epoch: 1/2, step 1863/7134 completed (loss: 0.20232194662094116, acc: 0.9605911374092102)
[2025-02-13 19:35:09,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:09,728][root][INFO] - Training Epoch: 1/2, step 1864/7134 completed (loss: 0.7552967071533203, acc: 0.8527919054031372)
[2025-02-13 19:35:09,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:10,077][root][INFO] - Training Epoch: 1/2, step 1865/7134 completed (loss: 0.405992329120636, acc: 0.9127516746520996)
[2025-02-13 19:35:10,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:10,495][root][INFO] - Training Epoch: 1/2, step 1866/7134 completed (loss: 0.10073932260274887, acc: 0.9764705896377563)
[2025-02-13 19:35:10,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:10,863][root][INFO] - Training Epoch: 1/2, step 1867/7134 completed (loss: 0.20704308152198792, acc: 0.9488636255264282)
[2025-02-13 19:35:11,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:11,264][root][INFO] - Training Epoch: 1/2, step 1868/7134 completed (loss: 0.5217465162277222, acc: 0.8834356069564819)
[2025-02-13 19:35:11,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:11,619][root][INFO] - Training Epoch: 1/2, step 1869/7134 completed (loss: 0.3549225926399231, acc: 0.9246575236320496)
[2025-02-13 19:35:11,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:11,965][root][INFO] - Training Epoch: 1/2, step 1870/7134 completed (loss: 0.19977183640003204, acc: 0.9459459185600281)
[2025-02-13 19:35:12,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:12,330][root][INFO] - Training Epoch: 1/2, step 1871/7134 completed (loss: 0.438533216714859, acc: 0.9225806593894958)
[2025-02-13 19:35:12,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:12,652][root][INFO] - Training Epoch: 1/2, step 1872/7134 completed (loss: 2.540863275527954, acc: 0.5652173757553101)
[2025-02-13 19:35:12,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:13,034][root][INFO] - Training Epoch: 1/2, step 1873/7134 completed (loss: 0.8075389862060547, acc: 0.8347107172012329)
[2025-02-13 19:35:13,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:13,396][root][INFO] - Training Epoch: 1/2, step 1874/7134 completed (loss: 0.16086794435977936, acc: 0.977011501789093)
[2025-02-13 19:35:13,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:13,750][root][INFO] - Training Epoch: 1/2, step 1875/7134 completed (loss: 0.11886007338762283, acc: 0.9732142686843872)
[2025-02-13 19:35:13,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:14,115][root][INFO] - Training Epoch: 1/2, step 1876/7134 completed (loss: 0.5729274749755859, acc: 0.8571428656578064)
[2025-02-13 19:35:14,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:14,475][root][INFO] - Training Epoch: 1/2, step 1877/7134 completed (loss: 0.2223803699016571, acc: 0.9491525292396545)
[2025-02-13 19:35:14,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:14,829][root][INFO] - Training Epoch: 1/2, step 1878/7134 completed (loss: 0.20214976370334625, acc: 0.9281045794487)
[2025-02-13 19:35:14,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:15,194][root][INFO] - Training Epoch: 1/2, step 1879/7134 completed (loss: 0.7804937958717346, acc: 0.8271604776382446)
[2025-02-13 19:35:15,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:15,579][root][INFO] - Training Epoch: 1/2, step 1880/7134 completed (loss: 0.5527124404907227, acc: 0.8943089246749878)
[2025-02-13 19:35:15,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:15,939][root][INFO] - Training Epoch: 1/2, step 1881/7134 completed (loss: 0.5733681321144104, acc: 0.8380281925201416)
[2025-02-13 19:35:16,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:16,270][root][INFO] - Training Epoch: 1/2, step 1882/7134 completed (loss: 0.5467978119850159, acc: 0.8804348111152649)
[2025-02-13 19:35:16,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:16,665][root][INFO] - Training Epoch: 1/2, step 1883/7134 completed (loss: 0.49279704689979553, acc: 0.8449612259864807)
[2025-02-13 19:35:16,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:17,028][root][INFO] - Training Epoch: 1/2, step 1884/7134 completed (loss: 0.4800392985343933, acc: 0.8648648858070374)
[2025-02-13 19:35:17,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:17,386][root][INFO] - Training Epoch: 1/2, step 1885/7134 completed (loss: 0.6281322836875916, acc: 0.8591549396514893)
[2025-02-13 19:35:17,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:17,761][root][INFO] - Training Epoch: 1/2, step 1886/7134 completed (loss: 0.5300130248069763, acc: 0.8809523582458496)
[2025-02-13 19:35:17,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:18,122][root][INFO] - Training Epoch: 1/2, step 1887/7134 completed (loss: 0.37385687232017517, acc: 0.9139072895050049)
[2025-02-13 19:35:18,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:18,496][root][INFO] - Training Epoch: 1/2, step 1888/7134 completed (loss: 0.3157050609588623, acc: 0.94017094373703)
[2025-02-13 19:35:18,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:18,890][root][INFO] - Training Epoch: 1/2, step 1889/7134 completed (loss: 0.24460144340991974, acc: 0.9473684430122375)
[2025-02-13 19:35:19,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:19,254][root][INFO] - Training Epoch: 1/2, step 1890/7134 completed (loss: 0.14922496676445007, acc: 0.9536423683166504)
[2025-02-13 19:35:19,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:19,613][root][INFO] - Training Epoch: 1/2, step 1891/7134 completed (loss: 0.47604405879974365, acc: 0.8720930218696594)
[2025-02-13 19:35:19,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:19,973][root][INFO] - Training Epoch: 1/2, step 1892/7134 completed (loss: 0.3373123109340668, acc: 0.9041916131973267)
[2025-02-13 19:35:20,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:20,318][root][INFO] - Training Epoch: 1/2, step 1893/7134 completed (loss: 0.30841192603111267, acc: 0.9135802388191223)
[2025-02-13 19:35:20,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:20,682][root][INFO] - Training Epoch: 1/2, step 1894/7134 completed (loss: 0.3757692575454712, acc: 0.912162184715271)
[2025-02-13 19:35:20,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:21,036][root][INFO] - Training Epoch: 1/2, step 1895/7134 completed (loss: 1.1888240575790405, acc: 0.7415730357170105)
[2025-02-13 19:35:21,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:21,412][root][INFO] - Training Epoch: 1/2, step 1896/7134 completed (loss: 0.39203667640686035, acc: 0.9272727370262146)
[2025-02-13 19:35:21,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:21,772][root][INFO] - Training Epoch: 1/2, step 1897/7134 completed (loss: 0.3554418981075287, acc: 0.9366196990013123)
[2025-02-13 19:35:21,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:22,125][root][INFO] - Training Epoch: 1/2, step 1898/7134 completed (loss: 0.32606467604637146, acc: 0.9103448390960693)
[2025-02-13 19:35:22,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:22,476][root][INFO] - Training Epoch: 1/2, step 1899/7134 completed (loss: 0.4172334372997284, acc: 0.9015151262283325)
[2025-02-13 19:35:22,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:22,851][root][INFO] - Training Epoch: 1/2, step 1900/7134 completed (loss: 0.16146495938301086, acc: 0.9485294222831726)
[2025-02-13 19:35:22,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:23,230][root][INFO] - Training Epoch: 1/2, step 1901/7134 completed (loss: 0.36258724331855774, acc: 0.935251772403717)
[2025-02-13 19:35:23,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:23,598][root][INFO] - Training Epoch: 1/2, step 1902/7134 completed (loss: 0.1744529902935028, acc: 0.9640287756919861)
[2025-02-13 19:35:23,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:23,977][root][INFO] - Training Epoch: 1/2, step 1903/7134 completed (loss: 0.11899010837078094, acc: 0.9803921580314636)
[2025-02-13 19:35:24,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:24,338][root][INFO] - Training Epoch: 1/2, step 1904/7134 completed (loss: 0.2992369830608368, acc: 0.9238095283508301)
[2025-02-13 19:35:24,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:24,706][root][INFO] - Training Epoch: 1/2, step 1905/7134 completed (loss: 0.258003294467926, acc: 0.9503546357154846)
[2025-02-13 19:35:24,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:25,103][root][INFO] - Training Epoch: 1/2, step 1906/7134 completed (loss: 0.22487227618694305, acc: 0.970802903175354)
[2025-02-13 19:35:25,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:25,467][root][INFO] - Training Epoch: 1/2, step 1907/7134 completed (loss: 0.2800794243812561, acc: 0.9115646481513977)
[2025-02-13 19:35:25,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:25,845][root][INFO] - Training Epoch: 1/2, step 1908/7134 completed (loss: 0.29785963892936707, acc: 0.9252336621284485)
[2025-02-13 19:35:25,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:26,218][root][INFO] - Training Epoch: 1/2, step 1909/7134 completed (loss: 0.3274246156215668, acc: 0.9234693646430969)
[2025-02-13 19:35:26,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:26,601][root][INFO] - Training Epoch: 1/2, step 1910/7134 completed (loss: 0.18494470417499542, acc: 0.9674418568611145)
[2025-02-13 19:35:26,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:26,982][root][INFO] - Training Epoch: 1/2, step 1911/7134 completed (loss: 0.1756831258535385, acc: 0.9644669890403748)
[2025-02-13 19:35:27,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:27,345][root][INFO] - Training Epoch: 1/2, step 1912/7134 completed (loss: 0.07329227775335312, acc: 0.9818181991577148)
[2025-02-13 19:35:27,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:27,696][root][INFO] - Training Epoch: 1/2, step 1913/7134 completed (loss: 0.29316261410713196, acc: 0.9448819160461426)
[2025-02-13 19:35:27,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28,083][root][INFO] - Training Epoch: 1/2, step 1914/7134 completed (loss: 0.3170587122440338, acc: 0.9398906826972961)
[2025-02-13 19:35:28,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28,433][root][INFO] - Training Epoch: 1/2, step 1915/7134 completed (loss: 0.14540445804595947, acc: 0.9720279574394226)
[2025-02-13 19:35:28,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28,776][root][INFO] - Training Epoch: 1/2, step 1916/7134 completed (loss: 0.19062237441539764, acc: 0.9560439586639404)
[2025-02-13 19:35:28,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:29,140][root][INFO] - Training Epoch: 1/2, step 1917/7134 completed (loss: 0.2103804349899292, acc: 0.9575757384300232)
[2025-02-13 19:35:29,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:29,488][root][INFO] - Training Epoch: 1/2, step 1918/7134 completed (loss: 0.23341773450374603, acc: 0.9735099077224731)
[2025-02-13 19:35:29,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:29,878][root][INFO] - Training Epoch: 1/2, step 1919/7134 completed (loss: 0.17236372828483582, acc: 0.9634146094322205)
[2025-02-13 19:35:30,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30,239][root][INFO] - Training Epoch: 1/2, step 1920/7134 completed (loss: 0.20292718708515167, acc: 0.9387755393981934)
[2025-02-13 19:35:30,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30,585][root][INFO] - Training Epoch: 1/2, step 1921/7134 completed (loss: 0.2056075632572174, acc: 0.9532163739204407)
[2025-02-13 19:35:30,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30,963][root][INFO] - Training Epoch: 1/2, step 1922/7134 completed (loss: 0.08259914070367813, acc: 0.9795082211494446)
[2025-02-13 19:35:31,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:31,320][root][INFO] - Training Epoch: 1/2, step 1923/7134 completed (loss: 0.15199649333953857, acc: 0.9440559148788452)
[2025-02-13 19:35:31,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:31,669][root][INFO] - Training Epoch: 1/2, step 1924/7134 completed (loss: 0.3124876916408539, acc: 0.921875)
[2025-02-13 19:35:31,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:32,034][root][INFO] - Training Epoch: 1/2, step 1925/7134 completed (loss: 0.32284435629844666, acc: 0.9121338725090027)
[2025-02-13 19:35:32,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:32,389][root][INFO] - Training Epoch: 1/2, step 1926/7134 completed (loss: 0.09877807646989822, acc: 0.9776785969734192)
[2025-02-13 19:35:32,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:32,755][root][INFO] - Training Epoch: 1/2, step 1927/7134 completed (loss: 0.21820491552352905, acc: 0.9669811129570007)
[2025-02-13 19:35:32,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:33,124][root][INFO] - Training Epoch: 1/2, step 1928/7134 completed (loss: 0.33000460267066956, acc: 0.9047619104385376)
[2025-02-13 19:35:33,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:33,514][root][INFO] - Training Epoch: 1/2, step 1929/7134 completed (loss: 0.15690694749355316, acc: 0.9441340565681458)
[2025-02-13 19:35:33,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:33,892][root][INFO] - Training Epoch: 1/2, step 1930/7134 completed (loss: 0.4636530876159668, acc: 0.8792270421981812)
[2025-02-13 19:35:34,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:34,258][root][INFO] - Training Epoch: 1/2, step 1931/7134 completed (loss: 0.36399170756340027, acc: 0.9120370149612427)
[2025-02-13 19:35:34,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:34,601][root][INFO] - Training Epoch: 1/2, step 1932/7134 completed (loss: 0.35425207018852234, acc: 0.9050279259681702)
[2025-02-13 19:35:34,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:34,966][root][INFO] - Training Epoch: 1/2, step 1933/7134 completed (loss: 0.43973737955093384, acc: 0.8844221234321594)
[2025-02-13 19:35:35,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:35,314][root][INFO] - Training Epoch: 1/2, step 1934/7134 completed (loss: 0.19189301133155823, acc: 0.9428571462631226)
[2025-02-13 19:35:35,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:35,681][root][INFO] - Training Epoch: 1/2, step 1935/7134 completed (loss: 0.44355863332748413, acc: 0.8963414430618286)
[2025-02-13 19:35:35,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:36,047][root][INFO] - Training Epoch: 1/2, step 1936/7134 completed (loss: 0.1425478607416153, acc: 0.9738562107086182)
[2025-02-13 19:35:36,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:36,393][root][INFO] - Training Epoch: 1/2, step 1937/7134 completed (loss: 0.24128395318984985, acc: 0.9268292784690857)
[2025-02-13 19:35:36,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:36,756][root][INFO] - Training Epoch: 1/2, step 1938/7134 completed (loss: 0.5446568727493286, acc: 0.8695651888847351)
[2025-02-13 19:35:36,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:37,106][root][INFO] - Training Epoch: 1/2, step 1939/7134 completed (loss: 0.2582278251647949, acc: 0.9396551847457886)
[2025-02-13 19:35:37,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:37,467][root][INFO] - Training Epoch: 1/2, step 1940/7134 completed (loss: 0.5443782210350037, acc: 0.8802395462989807)
[2025-02-13 19:35:37,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:37,825][root][INFO] - Training Epoch: 1/2, step 1941/7134 completed (loss: 0.4657493531703949, acc: 0.8639053106307983)
[2025-02-13 19:35:37,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:38,194][root][INFO] - Training Epoch: 1/2, step 1942/7134 completed (loss: 0.2445225715637207, acc: 0.9351851940155029)
[2025-02-13 19:35:38,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:38,557][root][INFO] - Training Epoch: 1/2, step 1943/7134 completed (loss: 0.47391730546951294, acc: 0.8947368264198303)
[2025-02-13 19:35:38,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:38,918][root][INFO] - Training Epoch: 1/2, step 1944/7134 completed (loss: 0.27223777770996094, acc: 0.9262295365333557)
[2025-02-13 19:35:39,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:39,271][root][INFO] - Training Epoch: 1/2, step 1945/7134 completed (loss: 0.309080570936203, acc: 0.9274193644523621)
[2025-02-13 19:35:39,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:39,623][root][INFO] - Training Epoch: 1/2, step 1946/7134 completed (loss: 0.4322127103805542, acc: 0.8999999761581421)
[2025-02-13 19:35:39,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:39,966][root][INFO] - Training Epoch: 1/2, step 1947/7134 completed (loss: 0.34002482891082764, acc: 0.913294792175293)
[2025-02-13 19:35:40,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:40,315][root][INFO] - Training Epoch: 1/2, step 1948/7134 completed (loss: 0.2195296287536621, acc: 0.9384615421295166)
[2025-02-13 19:35:40,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:40,655][root][INFO] - Training Epoch: 1/2, step 1949/7134 completed (loss: 0.19891126453876495, acc: 0.9350649118423462)
[2025-02-13 19:35:40,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:41,042][root][INFO] - Training Epoch: 1/2, step 1950/7134 completed (loss: 0.2705404758453369, acc: 0.9347826242446899)
[2025-02-13 19:35:41,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:41,437][root][INFO] - Training Epoch: 1/2, step 1951/7134 completed (loss: 0.2767309546470642, acc: 0.932330846786499)
[2025-02-13 19:35:41,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:41,788][root][INFO] - Training Epoch: 1/2, step 1952/7134 completed (loss: 0.33868056535720825, acc: 0.9096385836601257)
[2025-02-13 19:35:41,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:42,147][root][INFO] - Training Epoch: 1/2, step 1953/7134 completed (loss: 0.21656487882137299, acc: 0.9556962251663208)
[2025-02-13 19:35:42,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:42,486][root][INFO] - Training Epoch: 1/2, step 1954/7134 completed (loss: 0.13045966625213623, acc: 0.9509803652763367)
[2025-02-13 19:35:42,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:42,836][root][INFO] - Training Epoch: 1/2, step 1955/7134 completed (loss: 0.24043633043766022, acc: 0.9477611780166626)
[2025-02-13 19:35:42,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:43,146][root][INFO] - Training Epoch: 1/2, step 1956/7134 completed (loss: 0.12957052886486053, acc: 0.989130437374115)
[2025-02-13 19:35:43,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:43,500][root][INFO] - Training Epoch: 1/2, step 1957/7134 completed (loss: 0.2712893784046173, acc: 0.932330846786499)
[2025-02-13 19:35:43,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:43,831][root][INFO] - Training Epoch: 1/2, step 1958/7134 completed (loss: 0.2869897186756134, acc: 0.9222221970558167)
[2025-02-13 19:35:43,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:44,194][root][INFO] - Training Epoch: 1/2, step 1959/7134 completed (loss: 0.3387351632118225, acc: 0.9196428656578064)
[2025-02-13 19:35:44,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:44,575][root][INFO] - Training Epoch: 1/2, step 1960/7134 completed (loss: 0.16968479752540588, acc: 0.9666666388511658)
[2025-02-13 19:35:44,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:44,936][root][INFO] - Training Epoch: 1/2, step 1961/7134 completed (loss: 0.20840616524219513, acc: 0.949367105960846)
[2025-02-13 19:35:45,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:45,310][root][INFO] - Training Epoch: 1/2, step 1962/7134 completed (loss: 0.1615390181541443, acc: 0.9425287246704102)
[2025-02-13 19:35:45,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:45,673][root][INFO] - Training Epoch: 1/2, step 1963/7134 completed (loss: 0.3948360085487366, acc: 0.9072847962379456)
[2025-02-13 19:35:45,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:46,051][root][INFO] - Training Epoch: 1/2, step 1964/7134 completed (loss: 0.36105018854141235, acc: 0.9191918969154358)
[2025-02-13 19:35:46,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:46,428][root][INFO] - Training Epoch: 1/2, step 1965/7134 completed (loss: 0.38929852843284607, acc: 0.9044944047927856)
[2025-02-13 19:35:46,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:46,829][root][INFO] - Training Epoch: 1/2, step 1966/7134 completed (loss: 0.37798401713371277, acc: 0.9235668778419495)
[2025-02-13 19:35:46,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:47,212][root][INFO] - Training Epoch: 1/2, step 1967/7134 completed (loss: 0.2669046223163605, acc: 0.9210526347160339)
[2025-02-13 19:35:47,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:47,643][root][INFO] - Training Epoch: 1/2, step 1968/7134 completed (loss: 0.28287073969841003, acc: 0.9202127456665039)
[2025-02-13 19:35:47,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:48,000][root][INFO] - Training Epoch: 1/2, step 1969/7134 completed (loss: 0.20999695360660553, acc: 0.9435028433799744)
[2025-02-13 19:35:48,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:48,378][root][INFO] - Training Epoch: 1/2, step 1970/7134 completed (loss: 0.21194703876972198, acc: 0.9644970297813416)
[2025-02-13 19:35:48,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:48,778][root][INFO] - Training Epoch: 1/2, step 1971/7134 completed (loss: 0.3820778727531433, acc: 0.9162303805351257)
[2025-02-13 19:35:48,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:49,143][root][INFO] - Training Epoch: 1/2, step 1972/7134 completed (loss: 0.8486520648002625, acc: 0.7894737124443054)
[2025-02-13 19:35:49,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:49,496][root][INFO] - Training Epoch: 1/2, step 1973/7134 completed (loss: 1.0271422863006592, acc: 0.8472222089767456)
[2025-02-13 19:35:49,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:49,863][root][INFO] - Training Epoch: 1/2, step 1974/7134 completed (loss: 0.2243676334619522, acc: 0.9587628841400146)
[2025-02-13 19:35:49,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50,214][root][INFO] - Training Epoch: 1/2, step 1975/7134 completed (loss: 0.14338019490242004, acc: 0.949999988079071)
[2025-02-13 19:35:50,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50,606][root][INFO] - Training Epoch: 1/2, step 1976/7134 completed (loss: 0.3800812065601349, acc: 0.9086538553237915)
[2025-02-13 19:35:50,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50,978][root][INFO] - Training Epoch: 1/2, step 1977/7134 completed (loss: 0.2830345332622528, acc: 0.946107804775238)
[2025-02-13 19:35:51,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:51,328][root][INFO] - Training Epoch: 1/2, step 1978/7134 completed (loss: 0.15158356726169586, acc: 0.9825581312179565)
[2025-02-13 19:35:51,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:51,688][root][INFO] - Training Epoch: 1/2, step 1979/7134 completed (loss: 0.1893533319234848, acc: 0.9560439586639404)
[2025-02-13 19:35:51,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:52,062][root][INFO] - Training Epoch: 1/2, step 1980/7134 completed (loss: 0.1916304975748062, acc: 0.963350772857666)
[2025-02-13 19:35:52,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:52,426][root][INFO] - Training Epoch: 1/2, step 1981/7134 completed (loss: 0.14221924543380737, acc: 0.9725274443626404)
[2025-02-13 19:35:52,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:52,774][root][INFO] - Training Epoch: 1/2, step 1982/7134 completed (loss: 0.1788448691368103, acc: 0.9435897469520569)
[2025-02-13 19:35:52,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:53,132][root][INFO] - Training Epoch: 1/2, step 1983/7134 completed (loss: 0.16397905349731445, acc: 0.9784946441650391)
[2025-02-13 19:35:53,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:53,490][root][INFO] - Training Epoch: 1/2, step 1984/7134 completed (loss: 0.15069547295570374, acc: 0.9709302186965942)
[2025-02-13 19:35:53,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:53,834][root][INFO] - Training Epoch: 1/2, step 1985/7134 completed (loss: 0.3062620460987091, acc: 0.9192546606063843)
[2025-02-13 19:35:53,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:54,202][root][INFO] - Training Epoch: 1/2, step 1986/7134 completed (loss: 0.10300854593515396, acc: 0.9664804339408875)
[2025-02-13 19:35:54,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:54,560][root][INFO] - Training Epoch: 1/2, step 1987/7134 completed (loss: 0.21615801751613617, acc: 0.9550561904907227)
[2025-02-13 19:35:54,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:54,906][root][INFO] - Training Epoch: 1/2, step 1988/7134 completed (loss: 0.19948925077915192, acc: 0.9636363387107849)
[2025-02-13 19:35:55,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:55,278][root][INFO] - Training Epoch: 1/2, step 1989/7134 completed (loss: 0.348177045583725, acc: 0.9178082346916199)
[2025-02-13 19:35:55,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:55,653][root][INFO] - Training Epoch: 1/2, step 1990/7134 completed (loss: 0.21493057906627655, acc: 0.950276255607605)
[2025-02-13 19:35:55,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:56,046][root][INFO] - Training Epoch: 1/2, step 1991/7134 completed (loss: 0.13513001799583435, acc: 0.9677419066429138)
[2025-02-13 19:35:56,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:56,458][root][INFO] - Training Epoch: 1/2, step 1992/7134 completed (loss: 0.4277561902999878, acc: 0.8840579986572266)
[2025-02-13 19:35:56,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:56,824][root][INFO] - Training Epoch: 1/2, step 1993/7134 completed (loss: 0.3048306107521057, acc: 0.9217391014099121)
[2025-02-13 19:35:56,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:57,173][root][INFO] - Training Epoch: 1/2, step 1994/7134 completed (loss: 0.23541602492332458, acc: 0.9224806427955627)
[2025-02-13 19:35:57,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:57,537][root][INFO] - Training Epoch: 1/2, step 1995/7134 completed (loss: 0.09808586537837982, acc: 0.9862068891525269)
[2025-02-13 19:35:57,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:57,878][root][INFO] - Training Epoch: 1/2, step 1996/7134 completed (loss: 0.22844289243221283, acc: 0.9489051103591919)
[2025-02-13 19:35:58,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:58,235][root][INFO] - Training Epoch: 1/2, step 1997/7134 completed (loss: 0.21026667952537537, acc: 0.949999988079071)
[2025-02-13 19:35:58,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:58,598][root][INFO] - Training Epoch: 1/2, step 1998/7134 completed (loss: 0.19455356895923615, acc: 0.9436619877815247)
[2025-02-13 19:35:58,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:58,982][root][INFO] - Training Epoch: 1/2, step 1999/7134 completed (loss: 0.23250971734523773, acc: 0.9370629191398621)
[2025-02-13 19:35:59,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:59,351][root][INFO] - Training Epoch: 1/2, step 2000/7134 completed (loss: 0.17503748834133148, acc: 0.9722222089767456)
[2025-02-13 19:35:59,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:59,750][root][INFO] - Training Epoch: 1/2, step 2001/7134 completed (loss: 0.12715627253055573, acc: 0.9793103337287903)
[2025-02-13 19:35:59,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:00,119][root][INFO] - Training Epoch: 1/2, step 2002/7134 completed (loss: 0.1932455599308014, acc: 0.9624060392379761)
[2025-02-13 19:36:00,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:00,486][root][INFO] - Training Epoch: 1/2, step 2003/7134 completed (loss: 0.16255055367946625, acc: 0.9824561476707458)
[2025-02-13 19:36:00,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:00,855][root][INFO] - Training Epoch: 1/2, step 2004/7134 completed (loss: 0.9622665643692017, acc: 0.8524590134620667)
[2025-02-13 19:36:01,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:01,260][root][INFO] - Training Epoch: 1/2, step 2005/7134 completed (loss: 0.5688861608505249, acc: 0.8877550959587097)
[2025-02-13 19:36:01,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:01,615][root][INFO] - Training Epoch: 1/2, step 2006/7134 completed (loss: 0.11589865386486053, acc: 0.9767441749572754)
[2025-02-13 19:36:01,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:01,981][root][INFO] - Training Epoch: 1/2, step 2007/7134 completed (loss: 0.09210804849863052, acc: 0.9747899174690247)
[2025-02-13 19:36:02,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:02,344][root][INFO] - Training Epoch: 1/2, step 2008/7134 completed (loss: 0.19272802770137787, acc: 0.9520547986030579)
[2025-02-13 19:36:02,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:02,694][root][INFO] - Training Epoch: 1/2, step 2009/7134 completed (loss: 0.14786109328269958, acc: 0.970370352268219)
[2025-02-13 19:36:02,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:03,042][root][INFO] - Training Epoch: 1/2, step 2010/7134 completed (loss: 0.17520637810230255, acc: 0.9489051103591919)
[2025-02-13 19:36:03,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:03,402][root][INFO] - Training Epoch: 1/2, step 2011/7134 completed (loss: 0.09793577343225479, acc: 0.9841269850730896)
[2025-02-13 19:36:03,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:03,752][root][INFO] - Training Epoch: 1/2, step 2012/7134 completed (loss: 0.2077280431985855, acc: 0.9576271176338196)
[2025-02-13 19:36:03,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:04,128][root][INFO] - Training Epoch: 1/2, step 2013/7134 completed (loss: 0.15338054299354553, acc: 0.9754098653793335)
[2025-02-13 19:36:04,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:04,496][root][INFO] - Training Epoch: 1/2, step 2014/7134 completed (loss: 0.14074158668518066, acc: 0.9567901492118835)
[2025-02-13 19:36:04,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:04,856][root][INFO] - Training Epoch: 1/2, step 2015/7134 completed (loss: 0.08072246611118317, acc: 0.987500011920929)
[2025-02-13 19:36:04,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:05,214][root][INFO] - Training Epoch: 1/2, step 2016/7134 completed (loss: 0.289309561252594, acc: 0.9319728016853333)
[2025-02-13 19:36:05,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:05,585][root][INFO] - Training Epoch: 1/2, step 2017/7134 completed (loss: 0.3814297318458557, acc: 0.9072847962379456)
[2025-02-13 19:36:05,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:05,943][root][INFO] - Training Epoch: 1/2, step 2018/7134 completed (loss: 0.07836876809597015, acc: 0.9794520735740662)
[2025-02-13 19:36:06,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:06,346][root][INFO] - Training Epoch: 1/2, step 2019/7134 completed (loss: 0.20059964060783386, acc: 0.940397322177887)
[2025-02-13 19:36:06,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:06,723][root][INFO] - Training Epoch: 1/2, step 2020/7134 completed (loss: 0.17916086316108704, acc: 0.9504132270812988)
[2025-02-13 19:36:06,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:07,115][root][INFO] - Training Epoch: 1/2, step 2021/7134 completed (loss: 0.2728765308856964, acc: 0.9405405521392822)
[2025-02-13 19:36:07,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:07,469][root][INFO] - Training Epoch: 1/2, step 2022/7134 completed (loss: 0.253749281167984, acc: 0.9477611780166626)
[2025-02-13 19:36:07,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:07,844][root][INFO] - Training Epoch: 1/2, step 2023/7134 completed (loss: 0.18781089782714844, acc: 0.9447852969169617)
[2025-02-13 19:36:07,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:08,230][root][INFO] - Training Epoch: 1/2, step 2024/7134 completed (loss: 0.17178577184677124, acc: 0.9593023061752319)
[2025-02-13 19:36:08,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:08,613][root][INFO] - Training Epoch: 1/2, step 2025/7134 completed (loss: 0.23225197196006775, acc: 0.9707602262496948)
[2025-02-13 19:36:08,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:08,975][root][INFO] - Training Epoch: 1/2, step 2026/7134 completed (loss: 0.2118247002363205, acc: 0.9298245906829834)
[2025-02-13 19:36:09,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:09,336][root][INFO] - Training Epoch: 1/2, step 2027/7134 completed (loss: 0.15011271834373474, acc: 0.9473684430122375)
[2025-02-13 19:36:09,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:09,703][root][INFO] - Training Epoch: 1/2, step 2028/7134 completed (loss: 0.19710254669189453, acc: 0.9509202241897583)
[2025-02-13 19:36:09,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:10,101][root][INFO] - Training Epoch: 1/2, step 2029/7134 completed (loss: 0.2568104565143585, acc: 0.929411768913269)
[2025-02-13 19:36:10,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:10,464][root][INFO] - Training Epoch: 1/2, step 2030/7134 completed (loss: 0.25280001759529114, acc: 0.9397590160369873)
[2025-02-13 19:36:10,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:10,853][root][INFO] - Training Epoch: 1/2, step 2031/7134 completed (loss: 0.16340740025043488, acc: 0.9668508172035217)
[2025-02-13 19:36:10,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:11,222][root][INFO] - Training Epoch: 1/2, step 2032/7134 completed (loss: 0.3554377257823944, acc: 0.9005848169326782)
[2025-02-13 19:36:11,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:11,600][root][INFO] - Training Epoch: 1/2, step 2033/7134 completed (loss: 0.13865108788013458, acc: 0.9541984796524048)
[2025-02-13 19:36:11,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:11,986][root][INFO] - Training Epoch: 1/2, step 2034/7134 completed (loss: 0.4096568822860718, acc: 0.9060402512550354)
[2025-02-13 19:36:12,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:12,352][root][INFO] - Training Epoch: 1/2, step 2035/7134 completed (loss: 0.1267661452293396, acc: 0.9731183052062988)
[2025-02-13 19:36:12,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:12,690][root][INFO] - Training Epoch: 1/2, step 2036/7134 completed (loss: 0.24672973155975342, acc: 0.9465649127960205)
[2025-02-13 19:36:12,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:13,067][root][INFO] - Training Epoch: 1/2, step 2037/7134 completed (loss: 0.17203396558761597, acc: 0.9570552110671997)
[2025-02-13 19:36:13,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:13,412][root][INFO] - Training Epoch: 1/2, step 2038/7134 completed (loss: 0.19032374024391174, acc: 0.9505494236946106)
[2025-02-13 19:36:13,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:13,779][root][INFO] - Training Epoch: 1/2, step 2039/7134 completed (loss: 0.12165451049804688, acc: 0.9635036587715149)
[2025-02-13 19:36:13,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:14,138][root][INFO] - Training Epoch: 1/2, step 2040/7134 completed (loss: 0.25964245200157166, acc: 0.9741379022598267)
[2025-02-13 19:36:14,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:14,475][root][INFO] - Training Epoch: 1/2, step 2041/7134 completed (loss: 0.15139304101467133, acc: 0.9636363387107849)
[2025-02-13 19:36:14,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:14,874][root][INFO] - Training Epoch: 1/2, step 2042/7134 completed (loss: 0.28616058826446533, acc: 0.9491525292396545)
[2025-02-13 19:36:15,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:15,260][root][INFO] - Training Epoch: 1/2, step 2043/7134 completed (loss: 0.09346627444028854, acc: 0.9798657894134521)
[2025-02-13 19:36:15,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:15,654][root][INFO] - Training Epoch: 1/2, step 2044/7134 completed (loss: 0.14814305305480957, acc: 0.9599999785423279)
[2025-02-13 19:36:15,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:16,018][root][INFO] - Training Epoch: 1/2, step 2045/7134 completed (loss: 0.27944278717041016, acc: 0.8999999761581421)
[2025-02-13 19:36:16,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:16,363][root][INFO] - Training Epoch: 1/2, step 2046/7134 completed (loss: 0.3828715980052948, acc: 0.9159663915634155)
[2025-02-13 19:36:16,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:16,745][root][INFO] - Training Epoch: 1/2, step 2047/7134 completed (loss: 0.34182846546173096, acc: 0.9172413945198059)
[2025-02-13 19:36:16,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:17,108][root][INFO] - Training Epoch: 1/2, step 2048/7134 completed (loss: 0.41171956062316895, acc: 0.9107142686843872)
[2025-02-13 19:36:17,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:17,470][root][INFO] - Training Epoch: 1/2, step 2049/7134 completed (loss: 0.36641156673431396, acc: 0.918181836605072)
[2025-02-13 19:36:17,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:17,805][root][INFO] - Training Epoch: 1/2, step 2050/7134 completed (loss: 0.2962181270122528, acc: 0.9041095972061157)
[2025-02-13 19:36:17,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:18,164][root][INFO] - Training Epoch: 1/2, step 2051/7134 completed (loss: 0.2875162661075592, acc: 0.9203540086746216)
[2025-02-13 19:36:18,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:18,536][root][INFO] - Training Epoch: 1/2, step 2052/7134 completed (loss: 0.42309442162513733, acc: 0.9166666865348816)
[2025-02-13 19:36:18,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:18,902][root][INFO] - Training Epoch: 1/2, step 2053/7134 completed (loss: 0.47811827063560486, acc: 0.9007633328437805)
[2025-02-13 19:36:19,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:19,273][root][INFO] - Training Epoch: 1/2, step 2054/7134 completed (loss: 0.4439668357372284, acc: 0.9102563858032227)
[2025-02-13 19:36:19,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:19,632][root][INFO] - Training Epoch: 1/2, step 2055/7134 completed (loss: 0.35249724984169006, acc: 0.9178082346916199)
[2025-02-13 19:36:19,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:20,043][root][INFO] - Training Epoch: 1/2, step 2056/7134 completed (loss: 0.4268341362476349, acc: 0.909604549407959)
[2025-02-13 19:36:20,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:20,402][root][INFO] - Training Epoch: 1/2, step 2057/7134 completed (loss: 0.35244059562683105, acc: 0.9044944047927856)
[2025-02-13 19:36:20,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:20,764][root][INFO] - Training Epoch: 1/2, step 2058/7134 completed (loss: 0.48768118023872375, acc: 0.884393036365509)
[2025-02-13 19:36:20,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:21,132][root][INFO] - Training Epoch: 1/2, step 2059/7134 completed (loss: 0.7581766247749329, acc: 0.8368794322013855)
[2025-02-13 19:36:21,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:21,498][root][INFO] - Training Epoch: 1/2, step 2060/7134 completed (loss: 0.2527485191822052, acc: 0.9363057613372803)
[2025-02-13 19:36:21,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:21,869][root][INFO] - Training Epoch: 1/2, step 2061/7134 completed (loss: 0.46172064542770386, acc: 0.898809552192688)
[2025-02-13 19:36:21,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:22,212][root][INFO] - Training Epoch: 1/2, step 2062/7134 completed (loss: 0.4294307827949524, acc: 0.8984375)
[2025-02-13 19:36:22,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:22,562][root][INFO] - Training Epoch: 1/2, step 2063/7134 completed (loss: 0.2389005571603775, acc: 0.9290780425071716)
[2025-02-13 19:36:22,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:22,922][root][INFO] - Training Epoch: 1/2, step 2064/7134 completed (loss: 0.20674404501914978, acc: 0.9632353186607361)
[2025-02-13 19:36:23,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:23,277][root][INFO] - Training Epoch: 1/2, step 2065/7134 completed (loss: 0.2526407837867737, acc: 0.9571428298950195)
[2025-02-13 19:36:23,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:23,640][root][INFO] - Training Epoch: 1/2, step 2066/7134 completed (loss: 0.19533106684684753, acc: 0.9512194991111755)
[2025-02-13 19:36:23,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:24,062][root][INFO] - Training Epoch: 1/2, step 2067/7134 completed (loss: 0.22679643332958221, acc: 0.925000011920929)
[2025-02-13 19:36:24,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:24,449][root][INFO] - Training Epoch: 1/2, step 2068/7134 completed (loss: 0.20953194797039032, acc: 0.9509803652763367)
[2025-02-13 19:36:24,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:24,872][root][INFO] - Training Epoch: 1/2, step 2069/7134 completed (loss: 0.1861310601234436, acc: 0.9597315192222595)
[2025-02-13 19:36:25,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:25,234][root][INFO] - Training Epoch: 1/2, step 2070/7134 completed (loss: 0.1370108723640442, acc: 0.9396551847457886)
[2025-02-13 19:36:25,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:25,594][root][INFO] - Training Epoch: 1/2, step 2071/7134 completed (loss: 0.13843636214733124, acc: 0.9611650705337524)
[2025-02-13 19:36:25,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:25,943][root][INFO] - Training Epoch: 1/2, step 2072/7134 completed (loss: 0.18683156371116638, acc: 0.9509803652763367)
[2025-02-13 19:36:26,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:26,289][root][INFO] - Training Epoch: 1/2, step 2073/7134 completed (loss: 0.16949523985385895, acc: 0.9669421315193176)
[2025-02-13 19:36:26,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:26,649][root][INFO] - Training Epoch: 1/2, step 2074/7134 completed (loss: 0.23685409128665924, acc: 0.963350772857666)
[2025-02-13 19:36:26,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:27,001][root][INFO] - Training Epoch: 1/2, step 2075/7134 completed (loss: 0.4820162355899811, acc: 0.9182389974594116)
[2025-02-13 19:36:27,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:27,368][root][INFO] - Training Epoch: 1/2, step 2076/7134 completed (loss: 0.18675445020198822, acc: 0.9659863710403442)
[2025-02-13 19:36:27,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:27,724][root][INFO] - Training Epoch: 1/2, step 2077/7134 completed (loss: 0.2612268626689911, acc: 0.9435028433799744)
[2025-02-13 19:36:27,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:28,124][root][INFO] - Training Epoch: 1/2, step 2078/7134 completed (loss: 0.2114550918340683, acc: 0.9543147087097168)
[2025-02-13 19:36:28,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:28,486][root][INFO] - Training Epoch: 1/2, step 2079/7134 completed (loss: 0.11825108528137207, acc: 0.9638554453849792)
[2025-02-13 19:36:28,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:28,829][root][INFO] - Training Epoch: 1/2, step 2080/7134 completed (loss: 0.20963646471500397, acc: 0.9607843160629272)
[2025-02-13 19:36:28,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:29,179][root][INFO] - Training Epoch: 1/2, step 2081/7134 completed (loss: 0.11816371977329254, acc: 0.9745222926139832)
[2025-02-13 19:36:29,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:29,540][root][INFO] - Training Epoch: 1/2, step 2082/7134 completed (loss: 0.2544853985309601, acc: 0.9406779408454895)
[2025-02-13 19:36:29,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:29,901][root][INFO] - Training Epoch: 1/2, step 2083/7134 completed (loss: 0.15856124460697174, acc: 0.9767441749572754)
[2025-02-13 19:36:30,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:30,292][root][INFO] - Training Epoch: 1/2, step 2084/7134 completed (loss: 0.18083813786506653, acc: 0.9666666388511658)
[2025-02-13 19:36:30,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:30,676][root][INFO] - Training Epoch: 1/2, step 2085/7134 completed (loss: 0.07510851323604584, acc: 0.9918699264526367)
[2025-02-13 19:36:30,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:31,056][root][INFO] - Training Epoch: 1/2, step 2086/7134 completed (loss: 0.11362423747777939, acc: 0.9901960492134094)
[2025-02-13 19:36:31,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:31,436][root][INFO] - Training Epoch: 1/2, step 2087/7134 completed (loss: 0.08928941190242767, acc: 0.988095223903656)
[2025-02-13 19:36:31,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:31,794][root][INFO] - Training Epoch: 1/2, step 2088/7134 completed (loss: 0.34665510058403015, acc: 0.9526627063751221)
[2025-02-13 19:36:31,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32,175][root][INFO] - Training Epoch: 1/2, step 2089/7134 completed (loss: 0.32434171438217163, acc: 0.9285714030265808)
[2025-02-13 19:36:32,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32,522][root][INFO] - Training Epoch: 1/2, step 2090/7134 completed (loss: 0.09776994585990906, acc: 0.9738562107086182)
[2025-02-13 19:36:32,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32,886][root][INFO] - Training Epoch: 1/2, step 2091/7134 completed (loss: 0.06352178752422333, acc: 0.9862068891525269)
[2025-02-13 19:36:33,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:33,286][root][INFO] - Training Epoch: 1/2, step 2092/7134 completed (loss: 0.08168202638626099, acc: 0.9735099077224731)
[2025-02-13 19:36:33,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:33,703][root][INFO] - Training Epoch: 1/2, step 2093/7134 completed (loss: 0.17939937114715576, acc: 0.9526315927505493)
[2025-02-13 19:36:33,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:34,065][root][INFO] - Training Epoch: 1/2, step 2094/7134 completed (loss: 0.3632236421108246, acc: 0.9328358173370361)
[2025-02-13 19:36:34,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:34,458][root][INFO] - Training Epoch: 1/2, step 2095/7134 completed (loss: 0.2538367807865143, acc: 0.93034827709198)
[2025-02-13 19:36:34,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:34,796][root][INFO] - Training Epoch: 1/2, step 2096/7134 completed (loss: 0.19033659994602203, acc: 0.9440559148788452)
[2025-02-13 19:36:34,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:35,161][root][INFO] - Training Epoch: 1/2, step 2097/7134 completed (loss: 0.12617599964141846, acc: 0.9698795080184937)
[2025-02-13 19:36:35,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:35,557][root][INFO] - Training Epoch: 1/2, step 2098/7134 completed (loss: 0.23299917578697205, acc: 0.9473684430122375)
[2025-02-13 19:36:35,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:35,906][root][INFO] - Training Epoch: 1/2, step 2099/7134 completed (loss: 0.1753247082233429, acc: 0.953125)
[2025-02-13 19:36:36,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:36,271][root][INFO] - Training Epoch: 1/2, step 2100/7134 completed (loss: 0.26429101824760437, acc: 0.9428571462631226)
[2025-02-13 19:36:36,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:36,635][root][INFO] - Training Epoch: 1/2, step 2101/7134 completed (loss: 0.28713634610176086, acc: 0.9285714030265808)
[2025-02-13 19:36:36,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:36,987][root][INFO] - Training Epoch: 1/2, step 2102/7134 completed (loss: 0.23389600217342377, acc: 0.9351351261138916)
[2025-02-13 19:36:37,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:37,335][root][INFO] - Training Epoch: 1/2, step 2103/7134 completed (loss: 0.26614129543304443, acc: 0.918749988079071)
[2025-02-13 19:36:37,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:37,698][root][INFO] - Training Epoch: 1/2, step 2104/7134 completed (loss: 0.12199117243289948, acc: 0.9714285731315613)
[2025-02-13 19:36:37,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:38,044][root][INFO] - Training Epoch: 1/2, step 2105/7134 completed (loss: 0.16346044838428497, acc: 0.9432623982429504)
[2025-02-13 19:36:38,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:38,393][root][INFO] - Training Epoch: 1/2, step 2106/7134 completed (loss: 0.2438245266675949, acc: 0.9342105388641357)
[2025-02-13 19:36:38,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:38,817][root][INFO] - Training Epoch: 1/2, step 2107/7134 completed (loss: 0.20467029511928558, acc: 0.950276255607605)
[2025-02-13 19:36:38,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:39,205][root][INFO] - Training Epoch: 1/2, step 2108/7134 completed (loss: 0.19310936331748962, acc: 0.9503105878829956)
[2025-02-13 19:36:39,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:39,548][root][INFO] - Training Epoch: 1/2, step 2109/7134 completed (loss: 0.3868260979652405, acc: 0.9275362491607666)
[2025-02-13 19:36:39,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:39,931][root][INFO] - Training Epoch: 1/2, step 2110/7134 completed (loss: 0.3672979474067688, acc: 0.9041916131973267)
[2025-02-13 19:36:40,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:40,313][root][INFO] - Training Epoch: 1/2, step 2111/7134 completed (loss: 0.28330832719802856, acc: 0.9170984625816345)
[2025-02-13 19:36:40,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:40,661][root][INFO] - Training Epoch: 1/2, step 2112/7134 completed (loss: 0.1829916536808014, acc: 0.9599999785423279)
[2025-02-13 19:36:40,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:41,029][root][INFO] - Training Epoch: 1/2, step 2113/7134 completed (loss: 0.20959803462028503, acc: 0.9599999785423279)
[2025-02-13 19:36:41,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:41,406][root][INFO] - Training Epoch: 1/2, step 2114/7134 completed (loss: 0.26991236209869385, acc: 0.9289940595626831)
[2025-02-13 19:36:41,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:41,788][root][INFO] - Training Epoch: 1/2, step 2115/7134 completed (loss: 0.29766568541526794, acc: 0.9137930870056152)
[2025-02-13 19:36:41,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:42,151][root][INFO] - Training Epoch: 1/2, step 2116/7134 completed (loss: 0.2835799753665924, acc: 0.9455782175064087)
[2025-02-13 19:36:42,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:42,521][root][INFO] - Training Epoch: 1/2, step 2117/7134 completed (loss: 0.27289801836013794, acc: 0.9378530979156494)
[2025-02-13 19:36:42,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:42,888][root][INFO] - Training Epoch: 1/2, step 2118/7134 completed (loss: 0.1862969547510147, acc: 0.9526627063751221)
[2025-02-13 19:36:43,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:43,256][root][INFO] - Training Epoch: 1/2, step 2119/7134 completed (loss: 0.24158303439617157, acc: 0.9364162087440491)
[2025-02-13 19:36:43,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:43,616][root][INFO] - Training Epoch: 1/2, step 2120/7134 completed (loss: 0.2324542999267578, acc: 0.9506173133850098)
[2025-02-13 19:36:43,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:43,966][root][INFO] - Training Epoch: 1/2, step 2121/7134 completed (loss: 0.35274022817611694, acc: 0.9328858852386475)
[2025-02-13 19:36:44,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:44,343][root][INFO] - Training Epoch: 1/2, step 2122/7134 completed (loss: 0.27420860528945923, acc: 0.9322034120559692)
[2025-02-13 19:36:44,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:44,717][root][INFO] - Training Epoch: 1/2, step 2123/7134 completed (loss: 0.33779165148735046, acc: 0.9276315569877625)
[2025-02-13 19:36:44,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:45,078][root][INFO] - Training Epoch: 1/2, step 2124/7134 completed (loss: 0.17593994736671448, acc: 0.9459459185600281)
[2025-02-13 19:36:45,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:45,437][root][INFO] - Training Epoch: 1/2, step 2125/7134 completed (loss: 0.1223510205745697, acc: 0.970059871673584)
[2025-02-13 19:36:45,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:45,787][root][INFO] - Training Epoch: 1/2, step 2126/7134 completed (loss: 0.3181396722793579, acc: 0.9219858050346375)
[2025-02-13 19:36:45,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:46,140][root][INFO] - Training Epoch: 1/2, step 2127/7134 completed (loss: 0.19337581098079681, acc: 0.9397590160369873)
[2025-02-13 19:36:46,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:46,505][root][INFO] - Training Epoch: 1/2, step 2128/7134 completed (loss: 0.21796536445617676, acc: 0.949999988079071)
[2025-02-13 19:36:46,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:46,866][root][INFO] - Training Epoch: 1/2, step 2129/7134 completed (loss: 0.32393649220466614, acc: 0.9230769276618958)
[2025-02-13 19:36:46,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:47,230][root][INFO] - Training Epoch: 1/2, step 2130/7134 completed (loss: 0.33445748686790466, acc: 0.9189189076423645)
[2025-02-13 19:36:47,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:47,573][root][INFO] - Training Epoch: 1/2, step 2131/7134 completed (loss: 0.12276247888803482, acc: 0.9693251252174377)
[2025-02-13 19:36:47,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:47,932][root][INFO] - Training Epoch: 1/2, step 2132/7134 completed (loss: 0.07924313098192215, acc: 0.9878048896789551)
[2025-02-13 19:36:48,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:48,279][root][INFO] - Training Epoch: 1/2, step 2133/7134 completed (loss: 0.18270651996135712, acc: 0.9473684430122375)
[2025-02-13 19:36:48,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:48,644][root][INFO] - Training Epoch: 1/2, step 2134/7134 completed (loss: 0.15598580241203308, acc: 0.9655172228813171)
[2025-02-13 19:36:48,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49,000][root][INFO] - Training Epoch: 1/2, step 2135/7134 completed (loss: 0.10626666247844696, acc: 0.9668874144554138)
[2025-02-13 19:36:49,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49,368][root][INFO] - Training Epoch: 1/2, step 2136/7134 completed (loss: 0.09968899190425873, acc: 0.9725274443626404)
[2025-02-13 19:36:49,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49,721][root][INFO] - Training Epoch: 1/2, step 2137/7134 completed (loss: 0.23577646911144257, acc: 0.9414893388748169)
[2025-02-13 19:36:49,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:50,063][root][INFO] - Training Epoch: 1/2, step 2138/7134 completed (loss: 0.08120596408843994, acc: 0.976047933101654)
[2025-02-13 19:36:50,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:50,460][root][INFO] - Training Epoch: 1/2, step 2139/7134 completed (loss: 0.1630890816450119, acc: 0.9709302186965942)
[2025-02-13 19:36:50,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:50,839][root][INFO] - Training Epoch: 1/2, step 2140/7134 completed (loss: 0.42722639441490173, acc: 0.903954803943634)
[2025-02-13 19:36:50,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:51,126][root][INFO] - Training Epoch: 1/2, step 2141/7134 completed (loss: 0.36160680651664734, acc: 0.8936170339584351)
[2025-02-13 19:36:51,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:51,495][root][INFO] - Training Epoch: 1/2, step 2142/7134 completed (loss: 0.3386339843273163, acc: 0.9209039807319641)
[2025-02-13 19:36:51,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:51,885][root][INFO] - Training Epoch: 1/2, step 2143/7134 completed (loss: 0.1345677375793457, acc: 0.9655172228813171)
[2025-02-13 19:36:52,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:52,264][root][INFO] - Training Epoch: 1/2, step 2144/7134 completed (loss: 0.12240355461835861, acc: 0.9829545617103577)
[2025-02-13 19:36:52,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:52,663][root][INFO] - Training Epoch: 1/2, step 2145/7134 completed (loss: 0.20224176347255707, acc: 0.9709302186965942)
[2025-02-13 19:36:52,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:53,040][root][INFO] - Training Epoch: 1/2, step 2146/7134 completed (loss: 0.1437263935804367, acc: 0.9527027010917664)
[2025-02-13 19:36:53,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:53,415][root][INFO] - Training Epoch: 1/2, step 2147/7134 completed (loss: 0.22645753622055054, acc: 0.9444444179534912)
[2025-02-13 19:36:53,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:53,770][root][INFO] - Training Epoch: 1/2, step 2148/7134 completed (loss: 0.14861999452114105, acc: 0.9659090638160706)
[2025-02-13 19:36:53,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:54,106][root][INFO] - Training Epoch: 1/2, step 2149/7134 completed (loss: 0.08244065195322037, acc: 0.9824561476707458)
[2025-02-13 19:36:54,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:54,456][root][INFO] - Training Epoch: 1/2, step 2150/7134 completed (loss: 0.13504256308078766, acc: 0.9604519605636597)
[2025-02-13 19:36:54,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:54,820][root][INFO] - Training Epoch: 1/2, step 2151/7134 completed (loss: 0.15896664559841156, acc: 0.9638554453849792)
[2025-02-13 19:36:54,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:55,182][root][INFO] - Training Epoch: 1/2, step 2152/7134 completed (loss: 0.24333690106868744, acc: 0.9354838728904724)
[2025-02-13 19:36:55,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:55,559][root][INFO] - Training Epoch: 1/2, step 2153/7134 completed (loss: 0.2459070235490799, acc: 0.9460784196853638)
[2025-02-13 19:36:55,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:55,917][root][INFO] - Training Epoch: 1/2, step 2154/7134 completed (loss: 0.2321547567844391, acc: 0.954023003578186)
[2025-02-13 19:36:56,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:56,260][root][INFO] - Training Epoch: 1/2, step 2155/7134 completed (loss: 0.1921200305223465, acc: 0.9487179517745972)
[2025-02-13 19:36:56,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:56,594][root][INFO] - Training Epoch: 1/2, step 2156/7134 completed (loss: 0.30945831537246704, acc: 0.9411764740943909)
[2025-02-13 19:36:56,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:56,946][root][INFO] - Training Epoch: 1/2, step 2157/7134 completed (loss: 0.2570105791091919, acc: 0.9242424368858337)
[2025-02-13 19:36:57,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:57,303][root][INFO] - Training Epoch: 1/2, step 2158/7134 completed (loss: 0.27559733390808105, acc: 0.9432989954948425)
[2025-02-13 19:36:57,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:57,649][root][INFO] - Training Epoch: 1/2, step 2159/7134 completed (loss: 0.12401698529720306, acc: 0.9702380895614624)
[2025-02-13 19:36:57,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:57,994][root][INFO] - Training Epoch: 1/2, step 2160/7134 completed (loss: 0.21019086241722107, acc: 0.9390243887901306)
[2025-02-13 19:36:58,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:58,350][root][INFO] - Training Epoch: 1/2, step 2161/7134 completed (loss: 0.2594200372695923, acc: 0.9388889074325562)
[2025-02-13 19:36:58,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:58,703][root][INFO] - Training Epoch: 1/2, step 2162/7134 completed (loss: 0.15905436873435974, acc: 0.948387086391449)
[2025-02-13 19:36:58,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:59,065][root][INFO] - Training Epoch: 1/2, step 2163/7134 completed (loss: 0.23219338059425354, acc: 0.9333333373069763)
[2025-02-13 19:36:59,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:59,405][root][INFO] - Training Epoch: 1/2, step 2164/7134 completed (loss: 0.11234936118125916, acc: 0.9772727489471436)
[2025-02-13 19:36:59,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:59,757][root][INFO] - Training Epoch: 1/2, step 2165/7134 completed (loss: 0.14341790974140167, acc: 0.9817073345184326)
[2025-02-13 19:36:59,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:00,101][root][INFO] - Training Epoch: 1/2, step 2166/7134 completed (loss: 0.13730451464653015, acc: 0.9661017060279846)
[2025-02-13 19:37:00,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:00,461][root][INFO] - Training Epoch: 1/2, step 2167/7134 completed (loss: 0.32345107197761536, acc: 0.9320388436317444)
[2025-02-13 19:37:00,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:00,799][root][INFO] - Training Epoch: 1/2, step 2168/7134 completed (loss: 0.10144919902086258, acc: 0.9784172773361206)
[2025-02-13 19:37:00,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01,188][root][INFO] - Training Epoch: 1/2, step 2169/7134 completed (loss: 0.1859600841999054, acc: 0.9547738432884216)
[2025-02-13 19:37:01,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01,535][root][INFO] - Training Epoch: 1/2, step 2170/7134 completed (loss: 0.12607234716415405, acc: 0.9803921580314636)
[2025-02-13 19:37:01,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01,906][root][INFO] - Training Epoch: 1/2, step 2171/7134 completed (loss: 0.3283681571483612, acc: 0.9243243336677551)
[2025-02-13 19:37:02,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:02,265][root][INFO] - Training Epoch: 1/2, step 2172/7134 completed (loss: 0.1725819855928421, acc: 0.9810426831245422)
[2025-02-13 19:37:02,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:02,610][root][INFO] - Training Epoch: 1/2, step 2173/7134 completed (loss: 0.21891379356384277, acc: 0.965753436088562)
[2025-02-13 19:37:02,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:02,966][root][INFO] - Training Epoch: 1/2, step 2174/7134 completed (loss: 0.1285085678100586, acc: 0.976047933101654)
[2025-02-13 19:37:03,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:03,315][root][INFO] - Training Epoch: 1/2, step 2175/7134 completed (loss: 0.1638406664133072, acc: 0.9696969985961914)
[2025-02-13 19:37:03,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:03,671][root][INFO] - Training Epoch: 1/2, step 2176/7134 completed (loss: 0.13522838056087494, acc: 0.967391312122345)
[2025-02-13 19:37:03,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:04,045][root][INFO] - Training Epoch: 1/2, step 2177/7134 completed (loss: 0.1683197170495987, acc: 0.9672130942344666)
[2025-02-13 19:37:04,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:04,391][root][INFO] - Training Epoch: 1/2, step 2178/7134 completed (loss: 0.1125321164727211, acc: 0.9739583134651184)
[2025-02-13 19:37:04,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:04,780][root][INFO] - Training Epoch: 1/2, step 2179/7134 completed (loss: 0.12244406342506409, acc: 0.982758641242981)
[2025-02-13 19:37:04,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:05,146][root][INFO] - Training Epoch: 1/2, step 2180/7134 completed (loss: 0.18703177571296692, acc: 0.9560439586639404)
[2025-02-13 19:37:05,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:05,540][root][INFO] - Training Epoch: 1/2, step 2181/7134 completed (loss: 0.16625776886940002, acc: 0.95652174949646)
[2025-02-13 19:37:05,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:05,912][root][INFO] - Training Epoch: 1/2, step 2182/7134 completed (loss: 0.2601901888847351, acc: 0.9230769276618958)
[2025-02-13 19:37:06,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:06,308][root][INFO] - Training Epoch: 1/2, step 2183/7134 completed (loss: 0.20536090433597565, acc: 0.9447513818740845)
[2025-02-13 19:37:06,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:06,687][root][INFO] - Training Epoch: 1/2, step 2184/7134 completed (loss: 0.1273707002401352, acc: 0.9577465057373047)
[2025-02-13 19:37:06,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:07,046][root][INFO] - Training Epoch: 1/2, step 2185/7134 completed (loss: 0.33499881625175476, acc: 0.9245283007621765)
[2025-02-13 19:37:07,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:07,419][root][INFO] - Training Epoch: 1/2, step 2186/7134 completed (loss: 0.516697108745575, acc: 0.8799999952316284)
[2025-02-13 19:37:07,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:07,794][root][INFO] - Training Epoch: 1/2, step 2187/7134 completed (loss: 0.3854639232158661, acc: 0.8992805480957031)
[2025-02-13 19:37:07,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:08,179][root][INFO] - Training Epoch: 1/2, step 2188/7134 completed (loss: 0.1813870519399643, acc: 0.9515151381492615)
[2025-02-13 19:37:08,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:08,535][root][INFO] - Training Epoch: 1/2, step 2189/7134 completed (loss: 0.3388281762599945, acc: 0.9025974273681641)
[2025-02-13 19:37:08,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:08,904][root][INFO] - Training Epoch: 1/2, step 2190/7134 completed (loss: 0.24716772139072418, acc: 0.9441340565681458)
[2025-02-13 19:37:09,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:09,254][root][INFO] - Training Epoch: 1/2, step 2191/7134 completed (loss: 0.4929015338420868, acc: 0.8631578683853149)
[2025-02-13 19:37:09,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:09,615][root][INFO] - Training Epoch: 1/2, step 2192/7134 completed (loss: 0.1463378220796585, acc: 0.9629629850387573)
[2025-02-13 19:37:09,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:09,981][root][INFO] - Training Epoch: 1/2, step 2193/7134 completed (loss: 0.27027788758277893, acc: 0.9398906826972961)
[2025-02-13 19:37:10,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:10,356][root][INFO] - Training Epoch: 1/2, step 2194/7134 completed (loss: 0.2276439666748047, acc: 0.9276315569877625)
[2025-02-13 19:37:10,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:10,708][root][INFO] - Training Epoch: 1/2, step 2195/7134 completed (loss: 0.1047687903046608, acc: 0.9753086566925049)
[2025-02-13 19:37:10,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:11,077][root][INFO] - Training Epoch: 1/2, step 2196/7134 completed (loss: 0.15448647737503052, acc: 0.9670329689979553)
[2025-02-13 19:37:11,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:11,433][root][INFO] - Training Epoch: 1/2, step 2197/7134 completed (loss: 0.180043563246727, acc: 0.9291338324546814)
[2025-02-13 19:37:11,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:11,792][root][INFO] - Training Epoch: 1/2, step 2198/7134 completed (loss: 0.13786433637142181, acc: 0.977011501789093)
[2025-02-13 19:37:11,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:12,157][root][INFO] - Training Epoch: 1/2, step 2199/7134 completed (loss: 0.07587206363677979, acc: 0.9810126423835754)
[2025-02-13 19:37:12,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:12,536][root][INFO] - Training Epoch: 1/2, step 2200/7134 completed (loss: 0.1162896677851677, acc: 0.9842519760131836)
[2025-02-13 19:37:12,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:12,910][root][INFO] - Training Epoch: 1/2, step 2201/7134 completed (loss: 0.09277699142694473, acc: 0.9662162065505981)
[2025-02-13 19:37:13,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:13,272][root][INFO] - Training Epoch: 1/2, step 2202/7134 completed (loss: 0.15248920023441315, acc: 0.970802903175354)
[2025-02-13 19:37:13,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:13,651][root][INFO] - Training Epoch: 1/2, step 2203/7134 completed (loss: 0.08367899060249329, acc: 0.9880239367485046)
[2025-02-13 19:37:13,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:14,017][root][INFO] - Training Epoch: 1/2, step 2204/7134 completed (loss: 0.11389409005641937, acc: 0.982758641242981)
[2025-02-13 19:37:14,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:14,377][root][INFO] - Training Epoch: 1/2, step 2205/7134 completed (loss: 0.05790719389915466, acc: 0.9858155846595764)
[2025-02-13 19:37:14,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:14,738][root][INFO] - Training Epoch: 1/2, step 2206/7134 completed (loss: 0.1473315805196762, acc: 0.9798657894134521)
[2025-02-13 19:37:14,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:15,096][root][INFO] - Training Epoch: 1/2, step 2207/7134 completed (loss: 0.06091400235891342, acc: 0.9863945841789246)
[2025-02-13 19:37:15,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:15,460][root][INFO] - Training Epoch: 1/2, step 2208/7134 completed (loss: 0.11209823936223984, acc: 0.9741935729980469)
[2025-02-13 19:37:15,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:15,797][root][INFO] - Training Epoch: 1/2, step 2209/7134 completed (loss: 0.12025942653417587, acc: 0.9695122241973877)
[2025-02-13 19:37:15,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:16,147][root][INFO] - Training Epoch: 1/2, step 2210/7134 completed (loss: 0.14540325105190277, acc: 0.9695122241973877)
[2025-02-13 19:37:16,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:16,502][root][INFO] - Training Epoch: 1/2, step 2211/7134 completed (loss: 0.09463198482990265, acc: 0.9759036302566528)
[2025-02-13 19:37:16,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:16,878][root][INFO] - Training Epoch: 1/2, step 2212/7134 completed (loss: 0.08925487846136093, acc: 0.9828571677207947)
[2025-02-13 19:37:17,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:17,241][root][INFO] - Training Epoch: 1/2, step 2213/7134 completed (loss: 0.17385925352573395, acc: 0.9588235020637512)
[2025-02-13 19:37:17,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:17,601][root][INFO] - Training Epoch: 1/2, step 2214/7134 completed (loss: 0.17911431193351746, acc: 0.9729729890823364)
[2025-02-13 19:37:17,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:17,947][root][INFO] - Training Epoch: 1/2, step 2215/7134 completed (loss: 0.10983184725046158, acc: 0.9746835231781006)
[2025-02-13 19:37:18,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:18,297][root][INFO] - Training Epoch: 1/2, step 2216/7134 completed (loss: 0.12337260693311691, acc: 0.9590643048286438)
[2025-02-13 19:37:18,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:18,649][root][INFO] - Training Epoch: 1/2, step 2217/7134 completed (loss: 0.0945611372590065, acc: 0.976047933101654)
[2025-02-13 19:37:18,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:18,978][root][INFO] - Training Epoch: 1/2, step 2218/7134 completed (loss: 0.19482719898223877, acc: 0.9506173133850098)
[2025-02-13 19:37:19,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:19,318][root][INFO] - Training Epoch: 1/2, step 2219/7134 completed (loss: 0.15712568163871765, acc: 0.9571428298950195)
[2025-02-13 19:37:19,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:19,665][root][INFO] - Training Epoch: 1/2, step 2220/7134 completed (loss: 0.1957554817199707, acc: 0.9555555582046509)
[2025-02-13 19:37:19,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:20,041][root][INFO] - Training Epoch: 1/2, step 2221/7134 completed (loss: 0.21240843832492828, acc: 0.9378882050514221)
[2025-02-13 19:37:20,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:20,396][root][INFO] - Training Epoch: 1/2, step 2222/7134 completed (loss: 0.4356415569782257, acc: 0.9178082346916199)
[2025-02-13 19:37:20,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:20,745][root][INFO] - Training Epoch: 1/2, step 2223/7134 completed (loss: 0.1752994805574417, acc: 0.9530201554298401)
[2025-02-13 19:37:20,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21,099][root][INFO] - Training Epoch: 1/2, step 2224/7134 completed (loss: 0.38171106576919556, acc: 0.9113923907279968)
[2025-02-13 19:37:21,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21,459][root][INFO] - Training Epoch: 1/2, step 2225/7134 completed (loss: 0.23523686826229095, acc: 0.9319728016853333)
[2025-02-13 19:37:21,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21,831][root][INFO] - Training Epoch: 1/2, step 2226/7134 completed (loss: 0.17647594213485718, acc: 0.9463087320327759)
[2025-02-13 19:37:21,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:22,196][root][INFO] - Training Epoch: 1/2, step 2227/7134 completed (loss: 0.0959380567073822, acc: 0.9740259647369385)
[2025-02-13 19:37:22,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:22,536][root][INFO] - Training Epoch: 1/2, step 2228/7134 completed (loss: 0.11734926700592041, acc: 0.9844961166381836)
[2025-02-13 19:37:22,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:22,880][root][INFO] - Training Epoch: 1/2, step 2229/7134 completed (loss: 0.1862010955810547, acc: 0.9611650705337524)
[2025-02-13 19:37:23,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:23,221][root][INFO] - Training Epoch: 1/2, step 2230/7134 completed (loss: 0.04686636105179787, acc: 0.9856114983558655)
[2025-02-13 19:37:23,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:23,587][root][INFO] - Training Epoch: 1/2, step 2231/7134 completed (loss: 0.2909073829650879, acc: 0.9159663915634155)
[2025-02-13 19:37:23,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:23,946][root][INFO] - Training Epoch: 1/2, step 2232/7134 completed (loss: 0.19939012825489044, acc: 0.9367815852165222)
[2025-02-13 19:37:24,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:24,296][root][INFO] - Training Epoch: 1/2, step 2233/7134 completed (loss: 0.20605990290641785, acc: 0.9551281929016113)
[2025-02-13 19:37:24,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:24,677][root][INFO] - Training Epoch: 1/2, step 2234/7134 completed (loss: 0.07630198448896408, acc: 0.9805194735527039)
[2025-02-13 19:37:24,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:25,030][root][INFO] - Training Epoch: 1/2, step 2235/7134 completed (loss: 0.1654784083366394, acc: 0.9720279574394226)
[2025-02-13 19:37:25,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:25,391][root][INFO] - Training Epoch: 1/2, step 2236/7134 completed (loss: 0.17116230726242065, acc: 0.9466666579246521)
[2025-02-13 19:37:25,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:25,753][root][INFO] - Training Epoch: 1/2, step 2237/7134 completed (loss: 0.2132340967655182, acc: 0.9466666579246521)
[2025-02-13 19:37:25,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:26,117][root][INFO] - Training Epoch: 1/2, step 2238/7134 completed (loss: 0.2942745089530945, acc: 0.9382715821266174)
[2025-02-13 19:37:26,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:26,476][root][INFO] - Training Epoch: 1/2, step 2239/7134 completed (loss: 0.5024499893188477, acc: 0.8770053386688232)
[2025-02-13 19:37:26,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:26,836][root][INFO] - Training Epoch: 1/2, step 2240/7134 completed (loss: 0.25808656215667725, acc: 0.9375)
[2025-02-13 19:37:26,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:27,174][root][INFO] - Training Epoch: 1/2, step 2241/7134 completed (loss: 0.40609875321388245, acc: 0.8854166865348816)
[2025-02-13 19:37:27,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:27,513][root][INFO] - Training Epoch: 1/2, step 2242/7134 completed (loss: 0.6953812837600708, acc: 0.8518518805503845)
[2025-02-13 19:37:27,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:27,896][root][INFO] - Training Epoch: 1/2, step 2243/7134 completed (loss: 0.7265747785568237, acc: 0.8258064389228821)
[2025-02-13 19:37:28,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:28,257][root][INFO] - Training Epoch: 1/2, step 2244/7134 completed (loss: 0.34199878573417664, acc: 0.9109588861465454)
[2025-02-13 19:37:28,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:28,604][root][INFO] - Training Epoch: 1/2, step 2245/7134 completed (loss: 0.4826899468898773, acc: 0.8888888955116272)
[2025-02-13 19:37:28,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:28,974][root][INFO] - Training Epoch: 1/2, step 2246/7134 completed (loss: 0.7861081957817078, acc: 0.7978141903877258)
[2025-02-13 19:37:29,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:29,332][root][INFO] - Training Epoch: 1/2, step 2247/7134 completed (loss: 0.6490131616592407, acc: 0.8150289058685303)
[2025-02-13 19:37:29,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:29,688][root][INFO] - Training Epoch: 1/2, step 2248/7134 completed (loss: 0.5632713437080383, acc: 0.8554913401603699)
[2025-02-13 19:37:29,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:30,045][root][INFO] - Training Epoch: 1/2, step 2249/7134 completed (loss: 0.4419706463813782, acc: 0.8839778900146484)
[2025-02-13 19:37:30,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:30,407][root][INFO] - Training Epoch: 1/2, step 2250/7134 completed (loss: 0.39324453473091125, acc: 0.8936170339584351)
[2025-02-13 19:37:30,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:30,765][root][INFO] - Training Epoch: 1/2, step 2251/7134 completed (loss: 0.16553127765655518, acc: 0.9508196711540222)
[2025-02-13 19:37:30,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:31,113][root][INFO] - Training Epoch: 1/2, step 2252/7134 completed (loss: 0.15589340031147003, acc: 0.9619565010070801)
[2025-02-13 19:37:31,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:31,456][root][INFO] - Training Epoch: 1/2, step 2253/7134 completed (loss: 0.28607377409935, acc: 0.9363057613372803)
[2025-02-13 19:37:31,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:31,791][root][INFO] - Training Epoch: 1/2, step 2254/7134 completed (loss: 0.5011842250823975, acc: 0.8548387289047241)
[2025-02-13 19:37:31,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:32,149][root][INFO] - Training Epoch: 1/2, step 2255/7134 completed (loss: 0.15953831374645233, acc: 0.9622641801834106)
[2025-02-13 19:37:32,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:32,478][root][INFO] - Training Epoch: 1/2, step 2256/7134 completed (loss: 0.6649385094642639, acc: 0.8211382031440735)
[2025-02-13 19:37:32,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:32,844][root][INFO] - Training Epoch: 1/2, step 2257/7134 completed (loss: 0.22484567761421204, acc: 0.9444444179534912)
[2025-02-13 19:37:32,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:33,214][root][INFO] - Training Epoch: 1/2, step 2258/7134 completed (loss: 0.2719216048717499, acc: 0.930232584476471)
[2025-02-13 19:37:33,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:33,585][root][INFO] - Training Epoch: 1/2, step 2259/7134 completed (loss: 0.36184799671173096, acc: 0.8974359035491943)
[2025-02-13 19:37:33,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:33,943][root][INFO] - Training Epoch: 1/2, step 2260/7134 completed (loss: 0.5381996631622314, acc: 0.8636363744735718)
[2025-02-13 19:37:34,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:34,330][root][INFO] - Training Epoch: 1/2, step 2261/7134 completed (loss: 0.7692100405693054, acc: 0.8395061492919922)
[2025-02-13 19:37:34,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:34,722][root][INFO] - Training Epoch: 1/2, step 2262/7134 completed (loss: 0.32697248458862305, acc: 0.9248120188713074)
[2025-02-13 19:37:34,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:35,105][root][INFO] - Training Epoch: 1/2, step 2263/7134 completed (loss: 0.38637080788612366, acc: 0.9172932505607605)
[2025-02-13 19:37:35,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:35,489][root][INFO] - Training Epoch: 1/2, step 2264/7134 completed (loss: 0.42369726300239563, acc: 0.8810811042785645)
[2025-02-13 19:37:35,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:35,890][root][INFO] - Training Epoch: 1/2, step 2265/7134 completed (loss: 0.4188338816165924, acc: 0.9025974273681641)
[2025-02-13 19:37:36,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:36,288][root][INFO] - Training Epoch: 1/2, step 2266/7134 completed (loss: 0.29798927903175354, acc: 0.9060773253440857)
[2025-02-13 19:37:36,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:36,670][root][INFO] - Training Epoch: 1/2, step 2267/7134 completed (loss: 0.16796259582042694, acc: 0.9674796462059021)
[2025-02-13 19:37:36,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:37,051][root][INFO] - Training Epoch: 1/2, step 2268/7134 completed (loss: 0.20562182366847992, acc: 0.9520547986030579)
[2025-02-13 19:37:37,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:37,434][root][INFO] - Training Epoch: 1/2, step 2269/7134 completed (loss: 0.21743248403072357, acc: 0.9385474920272827)
[2025-02-13 19:37:37,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:37,822][root][INFO] - Training Epoch: 1/2, step 2270/7134 completed (loss: 0.4033196270465851, acc: 0.90625)
[2025-02-13 19:37:37,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:38,234][root][INFO] - Training Epoch: 1/2, step 2271/7134 completed (loss: 0.3983495831489563, acc: 0.8918918967247009)
[2025-02-13 19:37:38,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:38,614][root][INFO] - Training Epoch: 1/2, step 2272/7134 completed (loss: 0.287432461977005, acc: 0.9301075339317322)
[2025-02-13 19:37:38,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:38,975][root][INFO] - Training Epoch: 1/2, step 2273/7134 completed (loss: 0.12185915559530258, acc: 0.9666666388511658)
[2025-02-13 19:37:39,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:39,366][root][INFO] - Training Epoch: 1/2, step 2274/7134 completed (loss: 0.6408620476722717, acc: 0.8820512890815735)
[2025-02-13 19:37:39,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:39,744][root][INFO] - Training Epoch: 1/2, step 2275/7134 completed (loss: 0.14390987157821655, acc: 0.9754902124404907)
[2025-02-13 19:37:39,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:40,156][root][INFO] - Training Epoch: 1/2, step 2276/7134 completed (loss: 0.24657578766345978, acc: 0.9523809552192688)
[2025-02-13 19:37:40,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:40,545][root][INFO] - Training Epoch: 1/2, step 2277/7134 completed (loss: 0.25215816497802734, acc: 0.9333333373069763)
[2025-02-13 19:37:40,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:40,915][root][INFO] - Training Epoch: 1/2, step 2278/7134 completed (loss: 0.6302587985992432, acc: 0.8709677457809448)
[2025-02-13 19:37:41,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:41,268][root][INFO] - Training Epoch: 1/2, step 2279/7134 completed (loss: 0.16733671724796295, acc: 0.9543147087097168)
[2025-02-13 19:37:41,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:41,631][root][INFO] - Training Epoch: 1/2, step 2280/7134 completed (loss: 0.28799399733543396, acc: 0.9311926364898682)
[2025-02-13 19:37:41,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:41,985][root][INFO] - Training Epoch: 1/2, step 2281/7134 completed (loss: 0.44136667251586914, acc: 0.914893627166748)
[2025-02-13 19:37:42,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:42,348][root][INFO] - Training Epoch: 1/2, step 2282/7134 completed (loss: 0.14236979186534882, acc: 0.9631578922271729)
[2025-02-13 19:37:42,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:42,712][root][INFO] - Training Epoch: 1/2, step 2283/7134 completed (loss: 0.19938844442367554, acc: 0.9567307829856873)
[2025-02-13 19:37:42,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:43,083][root][INFO] - Training Epoch: 1/2, step 2284/7134 completed (loss: 0.16933056712150574, acc: 0.9548386931419373)
[2025-02-13 19:37:43,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:43,474][root][INFO] - Training Epoch: 1/2, step 2285/7134 completed (loss: 0.15371742844581604, acc: 0.9599999785423279)
[2025-02-13 19:37:43,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:43,854][root][INFO] - Training Epoch: 1/2, step 2286/7134 completed (loss: 0.32433024048805237, acc: 0.8965517282485962)
[2025-02-13 19:37:44,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:44,224][root][INFO] - Training Epoch: 1/2, step 2287/7134 completed (loss: 0.27262043952941895, acc: 0.9325153231620789)
[2025-02-13 19:37:44,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:44,602][root][INFO] - Training Epoch: 1/2, step 2288/7134 completed (loss: 0.10137893259525299, acc: 0.9725274443626404)
[2025-02-13 19:37:44,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:44,959][root][INFO] - Training Epoch: 1/2, step 2289/7134 completed (loss: 0.1773950308561325, acc: 0.9465240836143494)
[2025-02-13 19:37:45,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:45,317][root][INFO] - Training Epoch: 1/2, step 2290/7134 completed (loss: 0.21558675169944763, acc: 0.9301075339317322)
[2025-02-13 19:37:45,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:45,686][root][INFO] - Training Epoch: 1/2, step 2291/7134 completed (loss: 0.21266677975654602, acc: 0.9226519465446472)
[2025-02-13 19:37:45,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:46,047][root][INFO] - Training Epoch: 1/2, step 2292/7134 completed (loss: 0.2801567018032074, acc: 0.9242424368858337)
[2025-02-13 19:37:46,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:46,424][root][INFO] - Training Epoch: 1/2, step 2293/7134 completed (loss: 0.18789862096309662, acc: 0.9406392574310303)
[2025-02-13 19:37:46,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:46,784][root][INFO] - Training Epoch: 1/2, step 2294/7134 completed (loss: 0.20534318685531616, acc: 0.9569377899169922)
[2025-02-13 19:37:46,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:47,168][root][INFO] - Training Epoch: 1/2, step 2295/7134 completed (loss: 0.20154082775115967, acc: 0.94921875)
[2025-02-13 19:37:47,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:47,540][root][INFO] - Training Epoch: 1/2, step 2296/7134 completed (loss: 0.2209688276052475, acc: 0.9519230723381042)
[2025-02-13 19:37:47,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:47,906][root][INFO] - Training Epoch: 1/2, step 2297/7134 completed (loss: 0.13221395015716553, acc: 0.9669811129570007)
[2025-02-13 19:37:48,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:48,271][root][INFO] - Training Epoch: 1/2, step 2298/7134 completed (loss: 0.21106207370758057, acc: 0.9476190209388733)
[2025-02-13 19:37:48,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:48,670][root][INFO] - Training Epoch: 1/2, step 2299/7134 completed (loss: 0.25186967849731445, acc: 0.9462365508079529)
[2025-02-13 19:37:48,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:49,048][root][INFO] - Training Epoch: 1/2, step 2300/7134 completed (loss: 0.20667517185211182, acc: 0.9696969985961914)
[2025-02-13 19:37:49,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:49,401][root][INFO] - Training Epoch: 1/2, step 2301/7134 completed (loss: 0.3017156720161438, acc: 0.929347813129425)
[2025-02-13 19:37:49,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:49,757][root][INFO] - Training Epoch: 1/2, step 2302/7134 completed (loss: 0.21455493569374084, acc: 0.9384615421295166)
[2025-02-13 19:37:49,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:50,093][root][INFO] - Training Epoch: 1/2, step 2303/7134 completed (loss: 0.3417392075061798, acc: 0.9078341126441956)
[2025-02-13 19:37:50,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:50,451][root][INFO] - Training Epoch: 1/2, step 2304/7134 completed (loss: 0.3933332860469818, acc: 0.907489001750946)
[2025-02-13 19:37:50,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:50,818][root][INFO] - Training Epoch: 1/2, step 2305/7134 completed (loss: 0.2736617922782898, acc: 0.9292452931404114)
[2025-02-13 19:37:50,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:51,200][root][INFO] - Training Epoch: 1/2, step 2306/7134 completed (loss: 0.12594692409038544, acc: 0.9720930457115173)
[2025-02-13 19:37:51,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:51,613][root][INFO] - Training Epoch: 1/2, step 2307/7134 completed (loss: 0.1590336263179779, acc: 0.9618644118309021)
[2025-02-13 19:37:51,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:51,979][root][INFO] - Training Epoch: 1/2, step 2308/7134 completed (loss: 0.26134178042411804, acc: 0.9295154213905334)
[2025-02-13 19:37:52,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:52,334][root][INFO] - Training Epoch: 1/2, step 2309/7134 completed (loss: 0.08460472524166107, acc: 0.9788359999656677)
[2025-02-13 19:37:52,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:52,700][root][INFO] - Training Epoch: 1/2, step 2310/7134 completed (loss: 0.13984595239162445, acc: 0.961904764175415)
[2025-02-13 19:37:52,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:53,114][root][INFO] - Training Epoch: 1/2, step 2311/7134 completed (loss: 0.20234638452529907, acc: 0.931034505367279)
[2025-02-13 19:37:53,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:53,494][root][INFO] - Training Epoch: 1/2, step 2312/7134 completed (loss: 0.16909687221050262, acc: 0.9624413251876831)
[2025-02-13 19:37:53,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:53,904][root][INFO] - Training Epoch: 1/2, step 2313/7134 completed (loss: 0.2285516858100891, acc: 0.9278846383094788)
[2025-02-13 19:37:54,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:54,289][root][INFO] - Training Epoch: 1/2, step 2314/7134 completed (loss: 0.16617752611637115, acc: 0.9541284441947937)
[2025-02-13 19:37:54,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:54,665][root][INFO] - Training Epoch: 1/2, step 2315/7134 completed (loss: 0.2302325814962387, acc: 0.9255813956260681)
[2025-02-13 19:37:54,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:55,074][root][INFO] - Training Epoch: 1/2, step 2316/7134 completed (loss: 0.10654390603303909, acc: 0.9682539701461792)
[2025-02-13 19:37:55,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:55,467][root][INFO] - Training Epoch: 1/2, step 2317/7134 completed (loss: 0.1997056007385254, acc: 0.9537814855575562)
[2025-02-13 19:37:55,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:55,875][root][INFO] - Training Epoch: 1/2, step 2318/7134 completed (loss: 0.1123894527554512, acc: 0.9737991094589233)
[2025-02-13 19:37:56,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:56,239][root][INFO] - Training Epoch: 1/2, step 2319/7134 completed (loss: 0.13463416695594788, acc: 0.9684684872627258)
[2025-02-13 19:37:56,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:56,606][root][INFO] - Training Epoch: 1/2, step 2320/7134 completed (loss: 0.13527846336364746, acc: 0.9589743614196777)
[2025-02-13 19:37:56,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:56,970][root][INFO] - Training Epoch: 1/2, step 2321/7134 completed (loss: 0.09264025092124939, acc: 0.9882352948188782)
[2025-02-13 19:37:57,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:57,364][root][INFO] - Training Epoch: 1/2, step 2322/7134 completed (loss: 0.2464616894721985, acc: 0.9325153231620789)
[2025-02-13 19:37:57,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:57,752][root][INFO] - Training Epoch: 1/2, step 2323/7134 completed (loss: 0.2944420576095581, acc: 0.9363057613372803)
[2025-02-13 19:37:57,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:58,128][root][INFO] - Training Epoch: 1/2, step 2324/7134 completed (loss: 0.44367915391921997, acc: 0.9578313231468201)
[2025-02-13 19:37:58,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:58,517][root][INFO] - Training Epoch: 1/2, step 2325/7134 completed (loss: 0.30497992038726807, acc: 0.9142857193946838)
[2025-02-13 19:37:58,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:58,887][root][INFO] - Training Epoch: 1/2, step 2326/7134 completed (loss: 0.10134661942720413, acc: 0.9769230484962463)
[2025-02-13 19:37:59,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:59,256][root][INFO] - Training Epoch: 1/2, step 2327/7134 completed (loss: 0.19381169974803925, acc: 0.9530201554298401)
[2025-02-13 19:37:59,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:59,641][root][INFO] - Training Epoch: 1/2, step 2328/7134 completed (loss: 0.2743361294269562, acc: 0.9328858852386475)
[2025-02-13 19:37:59,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:00,037][root][INFO] - Training Epoch: 1/2, step 2329/7134 completed (loss: 0.2233363687992096, acc: 0.9346405267715454)
[2025-02-13 19:38:00,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:00,404][root][INFO] - Training Epoch: 1/2, step 2330/7134 completed (loss: 0.06731405109167099, acc: 0.9757575988769531)
[2025-02-13 19:38:00,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:00,774][root][INFO] - Training Epoch: 1/2, step 2331/7134 completed (loss: 0.07699979096651077, acc: 0.9741935729980469)
[2025-02-13 19:38:00,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:01,145][root][INFO] - Training Epoch: 1/2, step 2332/7134 completed (loss: 0.1333966851234436, acc: 0.9696969985961914)
[2025-02-13 19:38:01,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:01,503][root][INFO] - Training Epoch: 1/2, step 2333/7134 completed (loss: 0.10762527585029602, acc: 0.9756097793579102)
[2025-02-13 19:38:01,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:01,865][root][INFO] - Training Epoch: 1/2, step 2334/7134 completed (loss: 0.26947617530822754, acc: 0.9426751732826233)
[2025-02-13 19:38:02,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:02,271][root][INFO] - Training Epoch: 1/2, step 2335/7134 completed (loss: 0.1252184957265854, acc: 0.949367105960846)
[2025-02-13 19:38:02,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:02,633][root][INFO] - Training Epoch: 1/2, step 2336/7134 completed (loss: 0.10524555295705795, acc: 0.9798657894134521)
[2025-02-13 19:38:02,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:03,005][root][INFO] - Training Epoch: 1/2, step 2337/7134 completed (loss: 0.05820287764072418, acc: 0.9861111044883728)
[2025-02-13 19:38:03,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:03,364][root][INFO] - Training Epoch: 1/2, step 2338/7134 completed (loss: 0.08387035131454468, acc: 0.9805194735527039)
[2025-02-13 19:38:03,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:03,748][root][INFO] - Training Epoch: 1/2, step 2339/7134 completed (loss: 0.18979842960834503, acc: 0.9615384340286255)
[2025-02-13 19:38:03,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:04,116][root][INFO] - Training Epoch: 1/2, step 2340/7134 completed (loss: 0.04709968715906143, acc: 0.9870129823684692)
[2025-02-13 19:38:04,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:04,494][root][INFO] - Training Epoch: 1/2, step 2341/7134 completed (loss: 0.1453305184841156, acc: 0.9752066135406494)
[2025-02-13 19:38:04,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:04,853][root][INFO] - Training Epoch: 1/2, step 2342/7134 completed (loss: 0.08904962986707687, acc: 0.976190447807312)
[2025-02-13 19:38:04,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05,193][root][INFO] - Training Epoch: 1/2, step 2343/7134 completed (loss: 0.09326285123825073, acc: 0.987500011920929)
[2025-02-13 19:38:05,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05,554][root][INFO] - Training Epoch: 1/2, step 2344/7134 completed (loss: 0.10459796339273453, acc: 0.9691358208656311)
[2025-02-13 19:38:05,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05,914][root][INFO] - Training Epoch: 1/2, step 2345/7134 completed (loss: 0.14828674495220184, acc: 0.9425287246704102)
[2025-02-13 19:38:06,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:06,283][root][INFO] - Training Epoch: 1/2, step 2346/7134 completed (loss: 0.08384969085454941, acc: 0.9723756909370422)
[2025-02-13 19:38:06,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:06,658][root][INFO] - Training Epoch: 1/2, step 2347/7134 completed (loss: 0.183216854929924, acc: 0.9507042169570923)
[2025-02-13 19:38:06,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:07,032][root][INFO] - Training Epoch: 1/2, step 2348/7134 completed (loss: 0.563766598701477, acc: 0.8556700944900513)
[2025-02-13 19:38:07,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:07,389][root][INFO] - Training Epoch: 1/2, step 2349/7134 completed (loss: 0.49468064308166504, acc: 0.875)
[2025-02-13 19:38:07,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:07,781][root][INFO] - Training Epoch: 1/2, step 2350/7134 completed (loss: 0.4737685024738312, acc: 0.892405092716217)
[2025-02-13 19:38:07,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:08,160][root][INFO] - Training Epoch: 1/2, step 2351/7134 completed (loss: 0.40398430824279785, acc: 0.8961748480796814)
[2025-02-13 19:38:08,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:08,525][root][INFO] - Training Epoch: 1/2, step 2352/7134 completed (loss: 0.402636855840683, acc: 0.896774172782898)
[2025-02-13 19:38:08,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:08,918][root][INFO] - Training Epoch: 1/2, step 2353/7134 completed (loss: 0.33183327317237854, acc: 0.9161290526390076)
[2025-02-13 19:38:09,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:09,277][root][INFO] - Training Epoch: 1/2, step 2354/7134 completed (loss: 0.1588522344827652, acc: 0.9562841653823853)
[2025-02-13 19:38:09,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:09,678][root][INFO] - Training Epoch: 1/2, step 2355/7134 completed (loss: 0.29607686400413513, acc: 0.9281768202781677)
[2025-02-13 19:38:09,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:10,034][root][INFO] - Training Epoch: 1/2, step 2356/7134 completed (loss: 0.3504182696342468, acc: 0.9057971239089966)
[2025-02-13 19:38:10,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:10,400][root][INFO] - Training Epoch: 1/2, step 2357/7134 completed (loss: 0.3388945460319519, acc: 0.9047619104385376)
[2025-02-13 19:38:10,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:10,768][root][INFO] - Training Epoch: 1/2, step 2358/7134 completed (loss: 0.35092058777809143, acc: 0.918367326259613)
[2025-02-13 19:38:10,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11,131][root][INFO] - Training Epoch: 1/2, step 2359/7134 completed (loss: 0.4617626667022705, acc: 0.8500000238418579)
[2025-02-13 19:38:11,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11,521][root][INFO] - Training Epoch: 1/2, step 2360/7134 completed (loss: 0.2884434461593628, acc: 0.9177215099334717)
[2025-02-13 19:38:11,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11,903][root][INFO] - Training Epoch: 1/2, step 2361/7134 completed (loss: 0.2813636362552643, acc: 0.9285714030265808)
[2025-02-13 19:38:12,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:12,242][root][INFO] - Training Epoch: 1/2, step 2362/7134 completed (loss: 0.3923516571521759, acc: 0.9279279112815857)
[2025-02-13 19:38:12,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:12,655][root][INFO] - Training Epoch: 1/2, step 2363/7134 completed (loss: 0.22242917120456696, acc: 0.9476439952850342)
[2025-02-13 19:38:12,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:13,018][root][INFO] - Training Epoch: 1/2, step 2364/7134 completed (loss: 0.1243412047624588, acc: 0.9738562107086182)
[2025-02-13 19:38:13,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:13,384][root][INFO] - Training Epoch: 1/2, step 2365/7134 completed (loss: 0.13883543014526367, acc: 0.9534883499145508)
[2025-02-13 19:38:13,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:13,771][root][INFO] - Training Epoch: 1/2, step 2366/7134 completed (loss: 0.11041583865880966, acc: 0.9846153855323792)
[2025-02-13 19:38:13,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:14,138][root][INFO] - Training Epoch: 1/2, step 2367/7134 completed (loss: 0.2244313508272171, acc: 0.9595959782600403)
[2025-02-13 19:38:14,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:14,503][root][INFO] - Training Epoch: 1/2, step 2368/7134 completed (loss: 0.3410273790359497, acc: 0.9346733689308167)
[2025-02-13 19:38:14,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:14,860][root][INFO] - Training Epoch: 1/2, step 2369/7134 completed (loss: 0.1987939029932022, acc: 0.9622641801834106)
[2025-02-13 19:38:14,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:15,201][root][INFO] - Training Epoch: 1/2, step 2370/7134 completed (loss: 0.17230771481990814, acc: 0.9666666388511658)
[2025-02-13 19:38:15,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:15,566][root][INFO] - Training Epoch: 1/2, step 2371/7134 completed (loss: 0.26591843366622925, acc: 0.9440993666648865)
[2025-02-13 19:38:15,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:15,925][root][INFO] - Training Epoch: 1/2, step 2372/7134 completed (loss: 0.10235734283924103, acc: 0.9750000238418579)
[2025-02-13 19:38:16,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:16,278][root][INFO] - Training Epoch: 1/2, step 2373/7134 completed (loss: 0.13253483176231384, acc: 0.9792746305465698)
[2025-02-13 19:38:16,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:16,631][root][INFO] - Training Epoch: 1/2, step 2374/7134 completed (loss: 0.18233928084373474, acc: 0.9558011293411255)
[2025-02-13 19:38:16,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:17,022][root][INFO] - Training Epoch: 1/2, step 2375/7134 completed (loss: 0.21274693310260773, acc: 0.9333333373069763)
[2025-02-13 19:38:17,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:17,388][root][INFO] - Training Epoch: 1/2, step 2376/7134 completed (loss: 0.4289277493953705, acc: 0.9019607901573181)
[2025-02-13 19:38:17,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:17,763][root][INFO] - Training Epoch: 1/2, step 2377/7134 completed (loss: 0.2644951641559601, acc: 0.9252336621284485)
[2025-02-13 19:38:17,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:18,120][root][INFO] - Training Epoch: 1/2, step 2378/7134 completed (loss: 0.4211394786834717, acc: 0.9009901285171509)
[2025-02-13 19:38:18,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:18,487][root][INFO] - Training Epoch: 1/2, step 2379/7134 completed (loss: 0.5422975420951843, acc: 0.8520408272743225)
[2025-02-13 19:38:18,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:18,876][root][INFO] - Training Epoch: 1/2, step 2380/7134 completed (loss: 0.32385751605033875, acc: 0.907608687877655)
[2025-02-13 19:38:18,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:19,226][root][INFO] - Training Epoch: 1/2, step 2381/7134 completed (loss: 0.8027171492576599, acc: 0.8260869383811951)
[2025-02-13 19:38:19,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:19,579][root][INFO] - Training Epoch: 1/2, step 2382/7134 completed (loss: 0.2818740904331207, acc: 0.9197530746459961)
[2025-02-13 19:38:19,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:19,936][root][INFO] - Training Epoch: 1/2, step 2383/7134 completed (loss: 0.28345850110054016, acc: 0.9125683307647705)
[2025-02-13 19:38:20,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:20,301][root][INFO] - Training Epoch: 1/2, step 2384/7134 completed (loss: 0.3505062758922577, acc: 0.9195402264595032)
[2025-02-13 19:38:20,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:20,665][root][INFO] - Training Epoch: 1/2, step 2385/7134 completed (loss: 0.2109592705965042, acc: 0.9615384340286255)
[2025-02-13 19:38:20,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:21,020][root][INFO] - Training Epoch: 1/2, step 2386/7134 completed (loss: 0.31812965869903564, acc: 0.9047619104385376)
[2025-02-13 19:38:21,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:21,398][root][INFO] - Training Epoch: 1/2, step 2387/7134 completed (loss: 0.2517494857311249, acc: 0.9285714030265808)
[2025-02-13 19:38:21,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:21,805][root][INFO] - Training Epoch: 1/2, step 2388/7134 completed (loss: 0.29744240641593933, acc: 0.9086538553237915)
[2025-02-13 19:38:22,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:22,218][root][INFO] - Training Epoch: 1/2, step 2389/7134 completed (loss: 0.3714354932308197, acc: 0.9146919250488281)
[2025-02-13 19:38:22,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:22,601][root][INFO] - Training Epoch: 1/2, step 2390/7134 completed (loss: 0.4671007990837097, acc: 0.8909952640533447)
[2025-02-13 19:38:22,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:22,955][root][INFO] - Training Epoch: 1/2, step 2391/7134 completed (loss: 0.31961318850517273, acc: 0.908108115196228)
[2025-02-13 19:38:23,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:23,345][root][INFO] - Training Epoch: 1/2, step 2392/7134 completed (loss: 0.31170138716697693, acc: 0.9234972596168518)
[2025-02-13 19:38:23,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:23,756][root][INFO] - Training Epoch: 1/2, step 2393/7134 completed (loss: 0.2674427926540375, acc: 0.9326424598693848)
[2025-02-13 19:38:23,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:24,139][root][INFO] - Training Epoch: 1/2, step 2394/7134 completed (loss: 0.24768385291099548, acc: 0.9234693646430969)
[2025-02-13 19:38:24,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:24,521][root][INFO] - Training Epoch: 1/2, step 2395/7134 completed (loss: 0.6593132019042969, acc: 0.8285714387893677)
[2025-02-13 19:38:24,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:24,906][root][INFO] - Training Epoch: 1/2, step 2396/7134 completed (loss: 0.6922910213470459, acc: 0.8333333134651184)
[2025-02-13 19:38:25,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:25,248][root][INFO] - Training Epoch: 1/2, step 2397/7134 completed (loss: 0.46976643800735474, acc: 0.8963414430618286)
[2025-02-13 19:38:25,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:25,605][root][INFO] - Training Epoch: 1/2, step 2398/7134 completed (loss: 0.19365809857845306, acc: 0.9459459185600281)
[2025-02-13 19:38:25,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:25,963][root][INFO] - Training Epoch: 1/2, step 2399/7134 completed (loss: 0.5923568606376648, acc: 0.8549222946166992)
[2025-02-13 19:38:26,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:26,337][root][INFO] - Training Epoch: 1/2, step 2400/7134 completed (loss: 0.4731251299381256, acc: 0.8693467378616333)
[2025-02-13 19:38:26,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:26,701][root][INFO] - Training Epoch: 1/2, step 2401/7134 completed (loss: 0.9933773279190063, acc: 0.7596153616905212)
[2025-02-13 19:38:26,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:27,096][root][INFO] - Training Epoch: 1/2, step 2402/7134 completed (loss: 0.47387078404426575, acc: 0.8985507488250732)
[2025-02-13 19:38:27,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:27,481][root][INFO] - Training Epoch: 1/2, step 2403/7134 completed (loss: 0.2693454921245575, acc: 0.9221556782722473)
[2025-02-13 19:38:27,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:27,916][root][INFO] - Training Epoch: 1/2, step 2404/7134 completed (loss: 0.24538180232048035, acc: 0.9342105388641357)
[2025-02-13 19:38:28,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:28,294][root][INFO] - Training Epoch: 1/2, step 2405/7134 completed (loss: 0.4112817049026489, acc: 0.8980891704559326)
[2025-02-13 19:38:28,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:28,655][root][INFO] - Training Epoch: 1/2, step 2406/7134 completed (loss: 0.10216186940670013, acc: 0.976331353187561)
[2025-02-13 19:38:28,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:29,016][root][INFO] - Training Epoch: 1/2, step 2407/7134 completed (loss: 0.24165359139442444, acc: 0.9607843160629272)
[2025-02-13 19:38:29,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:29,373][root][INFO] - Training Epoch: 1/2, step 2408/7134 completed (loss: 0.31993913650512695, acc: 0.9226519465446472)
[2025-02-13 19:38:29,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:29,744][root][INFO] - Training Epoch: 1/2, step 2409/7134 completed (loss: 0.21744661033153534, acc: 0.949999988079071)
[2025-02-13 19:38:29,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:30,096][root][INFO] - Training Epoch: 1/2, step 2410/7134 completed (loss: 0.16717778146266937, acc: 0.9577465057373047)
[2025-02-13 19:38:30,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:30,452][root][INFO] - Training Epoch: 1/2, step 2411/7134 completed (loss: 0.11682385951280594, acc: 0.9613259434700012)
[2025-02-13 19:38:30,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:30,809][root][INFO] - Training Epoch: 1/2, step 2412/7134 completed (loss: 0.11866572499275208, acc: 0.9715909361839294)
[2025-02-13 19:38:30,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:31,176][root][INFO] - Training Epoch: 1/2, step 2413/7134 completed (loss: 0.1393965482711792, acc: 0.9545454382896423)
[2025-02-13 19:38:31,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:31,545][root][INFO] - Training Epoch: 1/2, step 2414/7134 completed (loss: 0.1602986603975296, acc: 0.9617486596107483)
[2025-02-13 19:38:31,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:31,913][root][INFO] - Training Epoch: 1/2, step 2415/7134 completed (loss: 0.5022907257080078, acc: 0.8841463327407837)
[2025-02-13 19:38:32,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:32,267][root][INFO] - Training Epoch: 1/2, step 2416/7134 completed (loss: 0.2119864970445633, acc: 0.955974817276001)
[2025-02-13 19:38:32,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:32,647][root][INFO] - Training Epoch: 1/2, step 2417/7134 completed (loss: 0.20746254920959473, acc: 0.9658536314964294)
[2025-02-13 19:38:32,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:32,999][root][INFO] - Training Epoch: 1/2, step 2418/7134 completed (loss: 0.2723546624183655, acc: 0.9370629191398621)
[2025-02-13 19:38:33,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:33,352][root][INFO] - Training Epoch: 1/2, step 2419/7134 completed (loss: 0.1481454223394394, acc: 0.9815950989723206)
[2025-02-13 19:38:33,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:33,719][root][INFO] - Training Epoch: 1/2, step 2420/7134 completed (loss: 0.20114023983478546, acc: 0.9375)
[2025-02-13 19:38:33,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:34,066][root][INFO] - Training Epoch: 1/2, step 2421/7134 completed (loss: 0.13456667959690094, acc: 0.9621621370315552)
[2025-02-13 19:38:34,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:34,465][root][INFO] - Training Epoch: 1/2, step 2422/7134 completed (loss: 0.1598047912120819, acc: 0.954285740852356)
[2025-02-13 19:38:34,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:34,814][root][INFO] - Training Epoch: 1/2, step 2423/7134 completed (loss: 0.2061033695936203, acc: 0.9523809552192688)
[2025-02-13 19:38:34,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:35,177][root][INFO] - Training Epoch: 1/2, step 2424/7134 completed (loss: 0.21277441084384918, acc: 0.9602272510528564)
[2025-02-13 19:38:35,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:35,541][root][INFO] - Training Epoch: 1/2, step 2425/7134 completed (loss: 0.2609913945198059, acc: 0.9382022619247437)
[2025-02-13 19:38:35,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:35,914][root][INFO] - Training Epoch: 1/2, step 2426/7134 completed (loss: 0.13185139000415802, acc: 0.9655172228813171)
[2025-02-13 19:38:36,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:36,271][root][INFO] - Training Epoch: 1/2, step 2427/7134 completed (loss: 0.09176055341959, acc: 0.96875)
[2025-02-13 19:38:36,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:36,660][root][INFO] - Training Epoch: 1/2, step 2428/7134 completed (loss: 0.13194838166236877, acc: 0.9526315927505493)
[2025-02-13 19:38:36,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:37,011][root][INFO] - Training Epoch: 1/2, step 2429/7134 completed (loss: 0.23695078492164612, acc: 0.9367088675498962)
[2025-02-13 19:38:37,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:37,362][root][INFO] - Training Epoch: 1/2, step 2430/7134 completed (loss: 0.22182396054267883, acc: 0.9399999976158142)
[2025-02-13 19:38:37,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:37,746][root][INFO] - Training Epoch: 1/2, step 2431/7134 completed (loss: 0.2345259040594101, acc: 0.9510489702224731)
[2025-02-13 19:38:37,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:38,117][root][INFO] - Training Epoch: 1/2, step 2432/7134 completed (loss: 0.43680429458618164, acc: 0.9008264541625977)
[2025-02-13 19:38:38,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:38,487][root][INFO] - Training Epoch: 1/2, step 2433/7134 completed (loss: 0.2605191171169281, acc: 0.9424460530281067)
[2025-02-13 19:38:38,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:38,844][root][INFO] - Training Epoch: 1/2, step 2434/7134 completed (loss: 0.3829755187034607, acc: 0.9127516746520996)
[2025-02-13 19:38:38,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:39,208][root][INFO] - Training Epoch: 1/2, step 2435/7134 completed (loss: 0.4070979654788971, acc: 0.9072847962379456)
[2025-02-13 19:38:39,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:39,552][root][INFO] - Training Epoch: 1/2, step 2436/7134 completed (loss: 0.18249428272247314, acc: 0.9492385983467102)
[2025-02-13 19:38:39,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:39,919][root][INFO] - Training Epoch: 1/2, step 2437/7134 completed (loss: 0.2685166895389557, acc: 0.9295774698257446)
[2025-02-13 19:38:40,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:40,285][root][INFO] - Training Epoch: 1/2, step 2438/7134 completed (loss: 0.2733600437641144, acc: 0.9259259104728699)
[2025-02-13 19:38:40,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:40,657][root][INFO] - Training Epoch: 1/2, step 2439/7134 completed (loss: 0.2228868007659912, acc: 0.9358288645744324)
[2025-02-13 19:38:40,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:41,010][root][INFO] - Training Epoch: 1/2, step 2440/7134 completed (loss: 0.19523954391479492, acc: 0.9545454382896423)
[2025-02-13 19:38:41,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:41,353][root][INFO] - Training Epoch: 1/2, step 2441/7134 completed (loss: 0.6369271278381348, acc: 0.8640000224113464)
[2025-02-13 19:38:41,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:41,705][root][INFO] - Training Epoch: 1/2, step 2442/7134 completed (loss: 0.41715681552886963, acc: 0.9289940595626831)
[2025-02-13 19:38:41,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:42,067][root][INFO] - Training Epoch: 1/2, step 2443/7134 completed (loss: 0.3425583243370056, acc: 0.9263803958892822)
[2025-02-13 19:38:42,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:42,487][root][INFO] - Training Epoch: 1/2, step 2444/7134 completed (loss: 0.28353041410446167, acc: 0.9224806427955627)
[2025-02-13 19:38:42,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:42,905][root][INFO] - Training Epoch: 1/2, step 2445/7134 completed (loss: 0.24900656938552856, acc: 0.9554139971733093)
[2025-02-13 19:38:43,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:43,286][root][INFO] - Training Epoch: 1/2, step 2446/7134 completed (loss: 0.442130982875824, acc: 0.8831169009208679)
[2025-02-13 19:38:43,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:43,641][root][INFO] - Training Epoch: 1/2, step 2447/7134 completed (loss: 0.18723517656326294, acc: 0.966292142868042)
[2025-02-13 19:38:43,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:43,986][root][INFO] - Training Epoch: 1/2, step 2448/7134 completed (loss: 0.22252514958381653, acc: 0.9328858852386475)
[2025-02-13 19:38:44,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:44,321][root][INFO] - Training Epoch: 1/2, step 2449/7134 completed (loss: 0.41129156947135925, acc: 0.9127907156944275)
[2025-02-13 19:38:44,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:44,682][root][INFO] - Training Epoch: 1/2, step 2450/7134 completed (loss: 0.19276906549930573, acc: 0.9411764740943909)
[2025-02-13 19:38:44,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:45,050][root][INFO] - Training Epoch: 1/2, step 2451/7134 completed (loss: 0.17865924537181854, acc: 0.9572192430496216)
[2025-02-13 19:38:45,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:45,414][root][INFO] - Training Epoch: 1/2, step 2452/7134 completed (loss: 0.21874934434890747, acc: 0.9301075339317322)
[2025-02-13 19:38:45,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:45,775][root][INFO] - Training Epoch: 1/2, step 2453/7134 completed (loss: 0.23483256995677948, acc: 0.9378238320350647)
[2025-02-13 19:38:45,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:46,159][root][INFO] - Training Epoch: 1/2, step 2454/7134 completed (loss: 0.20241916179656982, acc: 0.936170220375061)
[2025-02-13 19:38:46,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:46,547][root][INFO] - Training Epoch: 1/2, step 2455/7134 completed (loss: 0.103855662047863, acc: 0.9631901979446411)
[2025-02-13 19:38:46,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:46,918][root][INFO] - Training Epoch: 1/2, step 2456/7134 completed (loss: 0.2329992949962616, acc: 0.9430052042007446)
[2025-02-13 19:38:47,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:47,296][root][INFO] - Training Epoch: 1/2, step 2457/7134 completed (loss: 0.39975404739379883, acc: 0.9019607901573181)
[2025-02-13 19:38:47,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:47,653][root][INFO] - Training Epoch: 1/2, step 2458/7134 completed (loss: 0.4659532904624939, acc: 0.90625)
[2025-02-13 19:38:47,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:48,021][root][INFO] - Training Epoch: 1/2, step 2459/7134 completed (loss: 0.09729964286088943, acc: 0.9766082167625427)
[2025-02-13 19:38:48,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:48,376][root][INFO] - Training Epoch: 1/2, step 2460/7134 completed (loss: 0.1908012479543686, acc: 0.9505494236946106)
[2025-02-13 19:38:48,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:48,745][root][INFO] - Training Epoch: 1/2, step 2461/7134 completed (loss: 0.0826021209359169, acc: 0.9748427867889404)
[2025-02-13 19:38:48,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:49,141][root][INFO] - Training Epoch: 1/2, step 2462/7134 completed (loss: 0.3287913203239441, acc: 0.9515151381492615)
[2025-02-13 19:38:49,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:49,513][root][INFO] - Training Epoch: 1/2, step 2463/7134 completed (loss: 0.0909237489104271, acc: 0.9750000238418579)
[2025-02-13 19:38:49,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:49,891][root][INFO] - Training Epoch: 1/2, step 2464/7134 completed (loss: 0.2855232059955597, acc: 0.949999988079071)
[2025-02-13 19:38:50,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:50,285][root][INFO] - Training Epoch: 1/2, step 2465/7134 completed (loss: 0.13585031032562256, acc: 0.9664804339408875)
[2025-02-13 19:38:50,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:50,671][root][INFO] - Training Epoch: 1/2, step 2466/7134 completed (loss: 0.12856319546699524, acc: 0.9640718698501587)
[2025-02-13 19:38:50,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:51,046][root][INFO] - Training Epoch: 1/2, step 2467/7134 completed (loss: 0.35376831889152527, acc: 0.9269663095474243)
[2025-02-13 19:38:51,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:51,390][root][INFO] - Training Epoch: 1/2, step 2468/7134 completed (loss: 0.20491112768650055, acc: 0.9534883499145508)
[2025-02-13 19:38:51,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:51,757][root][INFO] - Training Epoch: 1/2, step 2469/7134 completed (loss: 0.08524753153324127, acc: 0.9883720874786377)
[2025-02-13 19:38:51,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:52,139][root][INFO] - Training Epoch: 1/2, step 2470/7134 completed (loss: 0.24484653770923615, acc: 0.957446813583374)
[2025-02-13 19:38:52,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:52,540][root][INFO] - Training Epoch: 1/2, step 2471/7134 completed (loss: 0.3248991072177887, acc: 0.936170220375061)
[2025-02-13 19:38:52,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:52,892][root][INFO] - Training Epoch: 1/2, step 2472/7134 completed (loss: 0.08643921464681625, acc: 0.9729729890823364)
[2025-02-13 19:38:53,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:53,252][root][INFO] - Training Epoch: 1/2, step 2473/7134 completed (loss: 0.13808423280715942, acc: 0.9719626307487488)
[2025-02-13 19:38:53,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:53,636][root][INFO] - Training Epoch: 1/2, step 2474/7134 completed (loss: 0.06378290057182312, acc: 0.9878787994384766)
[2025-02-13 19:38:53,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:53,996][root][INFO] - Training Epoch: 1/2, step 2475/7134 completed (loss: 0.07471570372581482, acc: 0.9918032884597778)
[2025-02-13 19:38:54,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:54,359][root][INFO] - Training Epoch: 1/2, step 2476/7134 completed (loss: 0.08049177378416061, acc: 0.9824561476707458)
[2025-02-13 19:38:54,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:54,758][root][INFO] - Training Epoch: 1/2, step 2477/7134 completed (loss: 0.07737623900175095, acc: 0.9753086566925049)
[2025-02-13 19:38:54,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:55,126][root][INFO] - Training Epoch: 1/2, step 2478/7134 completed (loss: 0.09664665907621384, acc: 0.9731543660163879)
[2025-02-13 19:38:55,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:55,488][root][INFO] - Training Epoch: 1/2, step 2479/7134 completed (loss: 0.47007179260253906, acc: 0.8775510191917419)
[2025-02-13 19:38:55,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:55,841][root][INFO] - Training Epoch: 1/2, step 2480/7134 completed (loss: 0.10265589505434036, acc: 0.9711538553237915)
[2025-02-13 19:38:55,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:56,200][root][INFO] - Training Epoch: 1/2, step 2481/7134 completed (loss: 0.2174624353647232, acc: 0.9519230723381042)
[2025-02-13 19:38:56,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:56,562][root][INFO] - Training Epoch: 1/2, step 2482/7134 completed (loss: 0.2398284375667572, acc: 0.9220778942108154)
[2025-02-13 19:38:56,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:56,935][root][INFO] - Training Epoch: 1/2, step 2483/7134 completed (loss: 0.0873950943350792, acc: 0.9820359349250793)
[2025-02-13 19:38:57,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:57,296][root][INFO] - Training Epoch: 1/2, step 2484/7134 completed (loss: 0.1925671100616455, acc: 0.9638554453849792)
[2025-02-13 19:38:57,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:57,675][root][INFO] - Training Epoch: 1/2, step 2485/7134 completed (loss: 0.08945802599191666, acc: 0.9851852059364319)
[2025-02-13 19:38:57,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:58,039][root][INFO] - Training Epoch: 1/2, step 2486/7134 completed (loss: 0.12344808131456375, acc: 0.9794520735740662)
[2025-02-13 19:38:58,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:58,387][root][INFO] - Training Epoch: 1/2, step 2487/7134 completed (loss: 0.40020546317100525, acc: 0.9097222089767456)
[2025-02-13 19:38:58,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:58,812][root][INFO] - Training Epoch: 1/2, step 2488/7134 completed (loss: 0.19846634566783905, acc: 0.9575757384300232)
[2025-02-13 19:38:58,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:59,175][root][INFO] - Training Epoch: 1/2, step 2489/7134 completed (loss: 0.12831808626651764, acc: 0.9644970297813416)
[2025-02-13 19:38:59,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:59,556][root][INFO] - Training Epoch: 1/2, step 2490/7134 completed (loss: 0.1109435185790062, acc: 0.9805194735527039)
[2025-02-13 19:38:59,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:59,918][root][INFO] - Training Epoch: 1/2, step 2491/7134 completed (loss: 0.4749061167240143, acc: 0.9029850959777832)
[2025-02-13 19:39:00,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:00,271][root][INFO] - Training Epoch: 1/2, step 2492/7134 completed (loss: 0.20885612070560455, acc: 0.9685534834861755)
[2025-02-13 19:39:00,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:00,639][root][INFO] - Training Epoch: 1/2, step 2493/7134 completed (loss: 0.1819809228181839, acc: 0.9523809552192688)
[2025-02-13 19:39:00,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:01,003][root][INFO] - Training Epoch: 1/2, step 2494/7134 completed (loss: 0.21193630993366241, acc: 0.9320987462997437)
[2025-02-13 19:39:01,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:01,355][root][INFO] - Training Epoch: 1/2, step 2495/7134 completed (loss: 0.2522491216659546, acc: 0.9411764740943909)
[2025-02-13 19:39:01,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:01,712][root][INFO] - Training Epoch: 1/2, step 2496/7134 completed (loss: 0.22099798917770386, acc: 0.949367105960846)
[2025-02-13 19:39:01,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:02,064][root][INFO] - Training Epoch: 1/2, step 2497/7134 completed (loss: 0.17994244396686554, acc: 0.954023003578186)
[2025-02-13 19:39:02,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:02,431][root][INFO] - Training Epoch: 1/2, step 2498/7134 completed (loss: 0.12285122275352478, acc: 0.9714285731315613)
[2025-02-13 19:39:02,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:02,805][root][INFO] - Training Epoch: 1/2, step 2499/7134 completed (loss: 0.1328873485326767, acc: 0.9698795080184937)
[2025-02-13 19:39:02,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:03,166][root][INFO] - Training Epoch: 1/2, step 2500/7134 completed (loss: 0.1463170200586319, acc: 0.9864864945411682)
[2025-02-13 19:39:03,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:03,524][root][INFO] - Training Epoch: 1/2, step 2501/7134 completed (loss: 0.26253437995910645, acc: 0.9438202381134033)
[2025-02-13 19:39:03,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:03,880][root][INFO] - Training Epoch: 1/2, step 2502/7134 completed (loss: 0.15449592471122742, acc: 0.9743589758872986)
[2025-02-13 19:39:04,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:04,221][root][INFO] - Training Epoch: 1/2, step 2503/7134 completed (loss: 0.1231122612953186, acc: 0.95652174949646)
[2025-02-13 19:39:04,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:04,561][root][INFO] - Training Epoch: 1/2, step 2504/7134 completed (loss: 0.23834897577762604, acc: 0.9303797483444214)
[2025-02-13 19:39:04,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:04,909][root][INFO] - Training Epoch: 1/2, step 2505/7134 completed (loss: 0.1409318447113037, acc: 0.9651162624359131)
[2025-02-13 19:39:05,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:05,279][root][INFO] - Training Epoch: 1/2, step 2506/7134 completed (loss: 0.14898957312107086, acc: 0.9508196711540222)
[2025-02-13 19:39:05,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:05,647][root][INFO] - Training Epoch: 1/2, step 2507/7134 completed (loss: 0.06215197592973709, acc: 0.9833333492279053)
[2025-02-13 19:39:05,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:06,018][root][INFO] - Training Epoch: 1/2, step 2508/7134 completed (loss: 0.09697499126195908, acc: 0.9717513918876648)
[2025-02-13 19:39:06,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:06,367][root][INFO] - Training Epoch: 1/2, step 2509/7134 completed (loss: 0.09102603048086166, acc: 0.9764705896377563)
[2025-02-13 19:39:06,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:06,706][root][INFO] - Training Epoch: 1/2, step 2510/7134 completed (loss: 0.07889939844608307, acc: 0.9785714149475098)
[2025-02-13 19:39:06,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:07,070][root][INFO] - Training Epoch: 1/2, step 2511/7134 completed (loss: 0.2202473133802414, acc: 0.9562841653823853)
[2025-02-13 19:39:07,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:07,434][root][INFO] - Training Epoch: 1/2, step 2512/7134 completed (loss: 0.21315303444862366, acc: 0.9235293865203857)
[2025-02-13 19:39:07,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:07,848][root][INFO] - Training Epoch: 1/2, step 2513/7134 completed (loss: 0.14349614083766937, acc: 0.9623655676841736)
[2025-02-13 19:39:07,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:08,211][root][INFO] - Training Epoch: 1/2, step 2514/7134 completed (loss: 0.14852668344974518, acc: 0.9554139971733093)
[2025-02-13 19:39:08,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:08,566][root][INFO] - Training Epoch: 1/2, step 2515/7134 completed (loss: 0.10972481220960617, acc: 0.9870129823684692)
[2025-02-13 19:39:08,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:08,952][root][INFO] - Training Epoch: 1/2, step 2516/7134 completed (loss: 0.17216959595680237, acc: 0.9720670580863953)
[2025-02-13 19:39:09,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:09,307][root][INFO] - Training Epoch: 1/2, step 2517/7134 completed (loss: 0.1831488013267517, acc: 0.9666666388511658)
[2025-02-13 19:39:09,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:09,669][root][INFO] - Training Epoch: 1/2, step 2518/7134 completed (loss: 0.1811748445034027, acc: 0.9477124214172363)
[2025-02-13 19:39:09,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:10,044][root][INFO] - Training Epoch: 1/2, step 2519/7134 completed (loss: 0.05834806337952614, acc: 0.9870967864990234)
[2025-02-13 19:39:10,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:10,415][root][INFO] - Training Epoch: 1/2, step 2520/7134 completed (loss: 0.14376656711101532, acc: 0.9539473652839661)
[2025-02-13 19:39:10,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:10,781][root][INFO] - Training Epoch: 1/2, step 2521/7134 completed (loss: 0.3419989347457886, acc: 0.9285714030265808)
[2025-02-13 19:39:10,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:11,134][root][INFO] - Training Epoch: 1/2, step 2522/7134 completed (loss: 0.1232493445277214, acc: 0.9586206674575806)
[2025-02-13 19:39:11,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:11,495][root][INFO] - Training Epoch: 1/2, step 2523/7134 completed (loss: 0.1866072714328766, acc: 0.9570552110671997)
[2025-02-13 19:39:11,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:11,860][root][INFO] - Training Epoch: 1/2, step 2524/7134 completed (loss: 0.10936824232339859, acc: 0.957317054271698)
[2025-02-13 19:39:12,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:12,220][root][INFO] - Training Epoch: 1/2, step 2525/7134 completed (loss: 0.1517692357301712, acc: 0.9452054500579834)
[2025-02-13 19:39:12,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:12,619][root][INFO] - Training Epoch: 1/2, step 2526/7134 completed (loss: 0.08550114929676056, acc: 0.9898989796638489)
[2025-02-13 19:39:12,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:12,983][root][INFO] - Training Epoch: 1/2, step 2527/7134 completed (loss: 0.08825388550758362, acc: 0.9726775884628296)
[2025-02-13 19:39:13,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:13,343][root][INFO] - Training Epoch: 1/2, step 2528/7134 completed (loss: 0.07738564908504486, acc: 0.9879518151283264)
[2025-02-13 19:39:13,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:13,745][root][INFO] - Training Epoch: 1/2, step 2529/7134 completed (loss: 0.08126863837242126, acc: 0.9821428656578064)
[2025-02-13 19:39:13,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:14,119][root][INFO] - Training Epoch: 1/2, step 2530/7134 completed (loss: 0.10070843994617462, acc: 0.9775280952453613)
[2025-02-13 19:39:14,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:14,472][root][INFO] - Training Epoch: 1/2, step 2531/7134 completed (loss: 0.04874134063720703, acc: 0.9879518151283264)
[2025-02-13 19:39:14,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:14,863][root][INFO] - Training Epoch: 1/2, step 2532/7134 completed (loss: 0.10812395811080933, acc: 0.9733333587646484)
[2025-02-13 19:39:15,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:15,221][root][INFO] - Training Epoch: 1/2, step 2533/7134 completed (loss: 0.15023204684257507, acc: 0.9615384340286255)
[2025-02-13 19:39:15,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:15,580][root][INFO] - Training Epoch: 1/2, step 2534/7134 completed (loss: 0.1446586400270462, acc: 0.9776536226272583)
[2025-02-13 19:39:15,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:15,939][root][INFO] - Training Epoch: 1/2, step 2535/7134 completed (loss: 0.07753638923168182, acc: 0.9830508232116699)
[2025-02-13 19:39:16,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:16,303][root][INFO] - Training Epoch: 1/2, step 2536/7134 completed (loss: 0.23910382390022278, acc: 0.9274611473083496)
[2025-02-13 19:39:16,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:16,657][root][INFO] - Training Epoch: 1/2, step 2537/7134 completed (loss: 0.24045996367931366, acc: 0.931506872177124)
[2025-02-13 19:39:16,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:17,014][root][INFO] - Training Epoch: 1/2, step 2538/7134 completed (loss: 0.23916327953338623, acc: 0.954023003578186)
[2025-02-13 19:39:17,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:17,365][root][INFO] - Training Epoch: 1/2, step 2539/7134 completed (loss: 0.18279747664928436, acc: 0.969924807548523)
[2025-02-13 19:39:17,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:17,763][root][INFO] - Training Epoch: 1/2, step 2540/7134 completed (loss: 0.20161758363246918, acc: 0.9536423683166504)
[2025-02-13 19:39:17,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:18,167][root][INFO] - Training Epoch: 1/2, step 2541/7134 completed (loss: 0.07228175550699234, acc: 1.0)
[2025-02-13 19:39:18,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:18,525][root][INFO] - Training Epoch: 1/2, step 2542/7134 completed (loss: 0.19440676271915436, acc: 0.9437500238418579)
[2025-02-13 19:39:18,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:18,915][root][INFO] - Training Epoch: 1/2, step 2543/7134 completed (loss: 0.2196521908044815, acc: 0.9622641801834106)
[2025-02-13 19:39:19,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:19,294][root][INFO] - Training Epoch: 1/2, step 2544/7134 completed (loss: 0.17503397166728973, acc: 0.9642857313156128)
[2025-02-13 19:39:19,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:19,675][root][INFO] - Training Epoch: 1/2, step 2545/7134 completed (loss: 0.21375501155853271, acc: 0.9179104566574097)
[2025-02-13 19:39:19,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:20,097][root][INFO] - Training Epoch: 1/2, step 2546/7134 completed (loss: 0.40071871876716614, acc: 0.8775510191917419)
[2025-02-13 19:39:20,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:20,468][root][INFO] - Training Epoch: 1/2, step 2547/7134 completed (loss: 0.3084373474121094, acc: 0.9280575513839722)
[2025-02-13 19:39:20,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:20,829][root][INFO] - Training Epoch: 1/2, step 2548/7134 completed (loss: 0.09364496171474457, acc: 0.9734513163566589)
[2025-02-13 19:39:20,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:21,225][root][INFO] - Training Epoch: 1/2, step 2549/7134 completed (loss: 0.1756128966808319, acc: 0.9268292784690857)
[2025-02-13 19:39:21,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:21,583][root][INFO] - Training Epoch: 1/2, step 2550/7134 completed (loss: 0.12176688760519028, acc: 0.965753436088562)
[2025-02-13 19:39:21,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:21,936][root][INFO] - Training Epoch: 1/2, step 2551/7134 completed (loss: 0.24285128712654114, acc: 0.9492753744125366)
[2025-02-13 19:39:22,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:22,270][root][INFO] - Training Epoch: 1/2, step 2552/7134 completed (loss: 0.16868220269680023, acc: 0.949367105960846)
[2025-02-13 19:39:22,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:22,662][root][INFO] - Training Epoch: 1/2, step 2553/7134 completed (loss: 0.25247153639793396, acc: 0.9605262875556946)
[2025-02-13 19:39:22,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:23,028][root][INFO] - Training Epoch: 1/2, step 2554/7134 completed (loss: 0.2659369111061096, acc: 0.9370078444480896)
[2025-02-13 19:39:23,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:23,393][root][INFO] - Training Epoch: 1/2, step 2555/7134 completed (loss: 0.193182110786438, acc: 0.9459459185600281)
[2025-02-13 19:39:23,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:23,759][root][INFO] - Training Epoch: 1/2, step 2556/7134 completed (loss: 0.18393898010253906, acc: 0.9800000190734863)
[2025-02-13 19:39:23,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:24,140][root][INFO] - Training Epoch: 1/2, step 2557/7134 completed (loss: 0.16865725815296173, acc: 0.9727891087532043)
[2025-02-13 19:39:24,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:24,499][root][INFO] - Training Epoch: 1/2, step 2558/7134 completed (loss: 0.06164804846048355, acc: 0.985401451587677)
[2025-02-13 19:39:24,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:24,853][root][INFO] - Training Epoch: 1/2, step 2559/7134 completed (loss: 0.09212352335453033, acc: 0.9750000238418579)
[2025-02-13 19:39:25,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:25,242][root][INFO] - Training Epoch: 1/2, step 2560/7134 completed (loss: 0.15839816629886627, acc: 0.9662162065505981)
[2025-02-13 19:39:25,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:25,598][root][INFO] - Training Epoch: 1/2, step 2561/7134 completed (loss: 0.18091417849063873, acc: 0.9479166865348816)
[2025-02-13 19:39:25,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:25,944][root][INFO] - Training Epoch: 1/2, step 2562/7134 completed (loss: 0.11699200421571732, acc: 0.9599999785423279)
[2025-02-13 19:39:26,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:26,308][root][INFO] - Training Epoch: 1/2, step 2563/7134 completed (loss: 0.15877334773540497, acc: 0.9526627063751221)
[2025-02-13 19:39:26,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:26,669][root][INFO] - Training Epoch: 1/2, step 2564/7134 completed (loss: 0.38738176226615906, acc: 0.9064748287200928)
[2025-02-13 19:39:26,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:27,018][root][INFO] - Training Epoch: 1/2, step 2565/7134 completed (loss: 0.11469938606023788, acc: 0.9640287756919861)
[2025-02-13 19:39:27,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:27,387][root][INFO] - Training Epoch: 1/2, step 2566/7134 completed (loss: 0.15003232657909393, acc: 0.9518072009086609)
[2025-02-13 19:39:27,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:27,725][root][INFO] - Training Epoch: 1/2, step 2567/7134 completed (loss: 0.12058793008327484, acc: 0.9624060392379761)
[2025-02-13 19:39:27,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:28,100][root][INFO] - Training Epoch: 1/2, step 2568/7134 completed (loss: 0.17982730269432068, acc: 0.9551281929016113)
[2025-02-13 19:39:28,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:28,476][root][INFO] - Training Epoch: 1/2, step 2569/7134 completed (loss: 0.08179187774658203, acc: 0.9929577708244324)
[2025-02-13 19:39:28,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:28,833][root][INFO] - Training Epoch: 1/2, step 2570/7134 completed (loss: 0.15242238342761993, acc: 0.9602649211883545)
[2025-02-13 19:39:28,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:29,188][root][INFO] - Training Epoch: 1/2, step 2571/7134 completed (loss: 0.12306076288223267, acc: 0.9754098653793335)
[2025-02-13 19:39:29,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:29,554][root][INFO] - Training Epoch: 1/2, step 2572/7134 completed (loss: 0.11893686652183533, acc: 0.9659863710403442)
[2025-02-13 19:39:29,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:29,927][root][INFO] - Training Epoch: 1/2, step 2573/7134 completed (loss: 0.10885334014892578, acc: 0.9784172773361206)
[2025-02-13 19:39:30,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:30,344][root][INFO] - Training Epoch: 1/2, step 2574/7134 completed (loss: 0.15234801173210144, acc: 0.9595959782600403)
[2025-02-13 19:39:30,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:30,738][root][INFO] - Training Epoch: 1/2, step 2575/7134 completed (loss: 0.08863335102796555, acc: 0.9836065769195557)
[2025-02-13 19:39:30,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:31,093][root][INFO] - Training Epoch: 1/2, step 2576/7134 completed (loss: 0.08190613985061646, acc: 0.9837398529052734)
[2025-02-13 19:39:31,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:31,443][root][INFO] - Training Epoch: 1/2, step 2577/7134 completed (loss: 0.277043879032135, acc: 0.9527559280395508)
[2025-02-13 19:39:31,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:31,795][root][INFO] - Training Epoch: 1/2, step 2578/7134 completed (loss: 0.3508075475692749, acc: 0.9215686321258545)
[2025-02-13 19:39:31,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:32,150][root][INFO] - Training Epoch: 1/2, step 2579/7134 completed (loss: 0.2879163920879364, acc: 0.9230769276618958)
[2025-02-13 19:39:32,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:32,518][root][INFO] - Training Epoch: 1/2, step 2580/7134 completed (loss: 0.23936325311660767, acc: 0.9364162087440491)
[2025-02-13 19:39:32,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:32,869][root][INFO] - Training Epoch: 1/2, step 2581/7134 completed (loss: 0.306776225566864, acc: 0.9209039807319641)
[2025-02-13 19:39:33,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:33,235][root][INFO] - Training Epoch: 1/2, step 2582/7134 completed (loss: 0.3059687912464142, acc: 0.9152542352676392)
[2025-02-13 19:39:33,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:33,597][root][INFO] - Training Epoch: 1/2, step 2583/7134 completed (loss: 0.39184027910232544, acc: 0.9080459475517273)
[2025-02-13 19:39:33,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:33,948][root][INFO] - Training Epoch: 1/2, step 2584/7134 completed (loss: 0.5732783079147339, acc: 0.8936170339584351)
[2025-02-13 19:39:34,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:34,324][root][INFO] - Training Epoch: 1/2, step 2585/7134 completed (loss: 0.29771560430526733, acc: 0.9470198750495911)
[2025-02-13 19:39:34,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:34,713][root][INFO] - Training Epoch: 1/2, step 2586/7134 completed (loss: 0.275528222322464, acc: 0.9230769276618958)
[2025-02-13 19:39:34,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:35,085][root][INFO] - Training Epoch: 1/2, step 2587/7134 completed (loss: 0.15677639842033386, acc: 0.96875)
[2025-02-13 19:39:35,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:35,462][root][INFO] - Training Epoch: 1/2, step 2588/7134 completed (loss: 0.12425979226827621, acc: 0.9591836929321289)
[2025-02-13 19:39:35,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:35,818][root][INFO] - Training Epoch: 1/2, step 2589/7134 completed (loss: 0.4227257966995239, acc: 0.8809523582458496)
[2025-02-13 19:39:35,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:36,172][root][INFO] - Training Epoch: 1/2, step 2590/7134 completed (loss: 0.26904022693634033, acc: 0.9259259104728699)
[2025-02-13 19:39:36,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:36,539][root][INFO] - Training Epoch: 1/2, step 2591/7134 completed (loss: 0.4594612717628479, acc: 0.871345043182373)
[2025-02-13 19:39:36,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:36,917][root][INFO] - Training Epoch: 1/2, step 2592/7134 completed (loss: 0.3171297013759613, acc: 0.9385474920272827)
[2025-02-13 19:39:37,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:37,288][root][INFO] - Training Epoch: 1/2, step 2593/7134 completed (loss: 0.17852036654949188, acc: 0.9817073345184326)
[2025-02-13 19:39:37,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:37,664][root][INFO] - Training Epoch: 1/2, step 2594/7134 completed (loss: 0.3627089262008667, acc: 0.9084967374801636)
[2025-02-13 19:39:37,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:38,030][root][INFO] - Training Epoch: 1/2, step 2595/7134 completed (loss: 0.17495481669902802, acc: 0.957317054271698)
[2025-02-13 19:39:38,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:38,406][root][INFO] - Training Epoch: 1/2, step 2596/7134 completed (loss: 0.23924289643764496, acc: 0.9235293865203857)
[2025-02-13 19:39:38,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:38,762][root][INFO] - Training Epoch: 1/2, step 2597/7134 completed (loss: 0.2769813537597656, acc: 0.9209039807319641)
[2025-02-13 19:39:38,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:39,128][root][INFO] - Training Epoch: 1/2, step 2598/7134 completed (loss: 0.3498481214046478, acc: 0.9006622433662415)
[2025-02-13 19:39:39,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:39,476][root][INFO] - Training Epoch: 1/2, step 2599/7134 completed (loss: 0.053326401859521866, acc: 0.9874213933944702)
[2025-02-13 19:39:39,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:39,852][root][INFO] - Training Epoch: 1/2, step 2600/7134 completed (loss: 0.34264227747917175, acc: 0.949438214302063)
[2025-02-13 19:39:39,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:40,216][root][INFO] - Training Epoch: 1/2, step 2601/7134 completed (loss: 0.31027674674987793, acc: 0.9290322661399841)
[2025-02-13 19:39:40,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:40,570][root][INFO] - Training Epoch: 1/2, step 2602/7134 completed (loss: 0.16662000119686127, acc: 0.9642857313156128)
[2025-02-13 19:39:40,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:40,945][root][INFO] - Training Epoch: 1/2, step 2603/7134 completed (loss: 0.2338581532239914, acc: 0.9281045794487)
[2025-02-13 19:39:41,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:41,312][root][INFO] - Training Epoch: 1/2, step 2604/7134 completed (loss: 0.19301770627498627, acc: 0.9365079402923584)
[2025-02-13 19:39:41,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:41,667][root][INFO] - Training Epoch: 1/2, step 2605/7134 completed (loss: 0.27211740612983704, acc: 0.9469026327133179)
[2025-02-13 19:39:41,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:42,028][root][INFO] - Training Epoch: 1/2, step 2606/7134 completed (loss: 0.2595076560974121, acc: 0.95333331823349)
[2025-02-13 19:39:42,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:42,393][root][INFO] - Training Epoch: 1/2, step 2607/7134 completed (loss: 0.1771782487630844, acc: 0.9382022619247437)
[2025-02-13 19:39:42,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:42,757][root][INFO] - Training Epoch: 1/2, step 2608/7134 completed (loss: 0.48145225644111633, acc: 0.8994709253311157)
[2025-02-13 19:39:42,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:43,112][root][INFO] - Training Epoch: 1/2, step 2609/7134 completed (loss: 0.39859244227409363, acc: 0.9215686321258545)
[2025-02-13 19:39:43,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:43,434][root][INFO] - Training Epoch: 1/2, step 2610/7134 completed (loss: 0.22397756576538086, acc: 0.971222996711731)
[2025-02-13 19:39:43,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:43,798][root][INFO] - Training Epoch: 1/2, step 2611/7134 completed (loss: 0.4896602928638458, acc: 0.910614550113678)
[2025-02-13 19:39:43,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:44,155][root][INFO] - Training Epoch: 1/2, step 2612/7134 completed (loss: 0.3764205873012543, acc: 0.8949999809265137)
[2025-02-13 19:39:44,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:44,524][root][INFO] - Training Epoch: 1/2, step 2613/7134 completed (loss: 0.2941223680973053, acc: 0.9047619104385376)
[2025-02-13 19:39:44,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:44,890][root][INFO] - Training Epoch: 1/2, step 2614/7134 completed (loss: 0.44758254289627075, acc: 0.8670212626457214)
[2025-02-13 19:39:45,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:45,253][root][INFO] - Training Epoch: 1/2, step 2615/7134 completed (loss: 0.2735540568828583, acc: 0.932584285736084)
[2025-02-13 19:39:45,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:45,607][root][INFO] - Training Epoch: 1/2, step 2616/7134 completed (loss: 0.4332154393196106, acc: 0.9327731132507324)
[2025-02-13 19:39:45,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:45,976][root][INFO] - Training Epoch: 1/2, step 2617/7134 completed (loss: 0.49879586696624756, acc: 0.8730964660644531)
[2025-02-13 19:39:46,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:46,317][root][INFO] - Training Epoch: 1/2, step 2618/7134 completed (loss: 0.20298175513744354, acc: 0.9230769276618958)
[2025-02-13 19:39:46,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:46,729][root][INFO] - Training Epoch: 1/2, step 2619/7134 completed (loss: 0.233024463057518, acc: 0.9420289993286133)
[2025-02-13 19:39:46,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:47,085][root][INFO] - Training Epoch: 1/2, step 2620/7134 completed (loss: 0.2856196463108063, acc: 0.9047619104385376)
[2025-02-13 19:39:47,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:47,443][root][INFO] - Training Epoch: 1/2, step 2621/7134 completed (loss: 0.3700210154056549, acc: 0.9090909361839294)
[2025-02-13 19:39:47,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:47,824][root][INFO] - Training Epoch: 1/2, step 2622/7134 completed (loss: 0.26400473713874817, acc: 0.9178082346916199)
[2025-02-13 19:39:47,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:48,191][root][INFO] - Training Epoch: 1/2, step 2623/7134 completed (loss: 0.38390257954597473, acc: 0.9128440618515015)
[2025-02-13 19:39:48,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:48,590][root][INFO] - Training Epoch: 1/2, step 2624/7134 completed (loss: 0.42609626054763794, acc: 0.8952381014823914)
[2025-02-13 19:39:48,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:48,957][root][INFO] - Training Epoch: 1/2, step 2625/7134 completed (loss: 0.29352885484695435, acc: 0.9226190447807312)
[2025-02-13 19:39:49,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:49,317][root][INFO] - Training Epoch: 1/2, step 2626/7134 completed (loss: 0.2361074686050415, acc: 0.9425287246704102)
[2025-02-13 19:39:49,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:49,689][root][INFO] - Training Epoch: 1/2, step 2627/7134 completed (loss: 0.2345832884311676, acc: 0.9509202241897583)
[2025-02-13 19:39:49,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:50,042][root][INFO] - Training Epoch: 1/2, step 2628/7134 completed (loss: 0.27739235758781433, acc: 0.9388889074325562)
[2025-02-13 19:39:50,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:50,450][root][INFO] - Training Epoch: 1/2, step 2629/7134 completed (loss: 0.1569649577140808, acc: 0.9580838084220886)
[2025-02-13 19:39:50,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:50,800][root][INFO] - Training Epoch: 1/2, step 2630/7134 completed (loss: 0.23460905253887177, acc: 0.9275362491607666)
[2025-02-13 19:39:50,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:51,165][root][INFO] - Training Epoch: 1/2, step 2631/7134 completed (loss: 0.21773579716682434, acc: 0.9441624283790588)
[2025-02-13 19:39:51,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:51,546][root][INFO] - Training Epoch: 1/2, step 2632/7134 completed (loss: 0.13394179940223694, acc: 0.9652174115180969)
[2025-02-13 19:39:51,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:51,913][root][INFO] - Training Epoch: 1/2, step 2633/7134 completed (loss: 0.3322930932044983, acc: 0.906593382358551)
[2025-02-13 19:39:52,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:52,346][root][INFO] - Training Epoch: 1/2, step 2634/7134 completed (loss: 0.0996498391032219, acc: 0.976331353187561)
[2025-02-13 19:39:52,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:52,736][root][INFO] - Training Epoch: 1/2, step 2635/7134 completed (loss: 0.4142872989177704, acc: 0.9222221970558167)
[2025-02-13 19:39:52,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:53,101][root][INFO] - Training Epoch: 1/2, step 2636/7134 completed (loss: 0.389389306306839, acc: 0.910614550113678)
[2025-02-13 19:39:53,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:53,466][root][INFO] - Training Epoch: 1/2, step 2637/7134 completed (loss: 0.6181129217147827, acc: 0.8773006200790405)
[2025-02-13 19:39:53,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:53,840][root][INFO] - Training Epoch: 1/2, step 2638/7134 completed (loss: 0.4140138328075409, acc: 0.9054726362228394)
[2025-02-13 19:39:53,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:54,193][root][INFO] - Training Epoch: 1/2, step 2639/7134 completed (loss: 0.6321574449539185, acc: 0.8571428656578064)
[2025-02-13 19:39:54,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:54,561][root][INFO] - Training Epoch: 1/2, step 2640/7134 completed (loss: 0.5099263191223145, acc: 0.9154228568077087)
[2025-02-13 19:39:54,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:54,967][root][INFO] - Training Epoch: 1/2, step 2641/7134 completed (loss: 0.3749549686908722, acc: 0.9142857193946838)
[2025-02-13 19:39:55,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:55,327][root][INFO] - Training Epoch: 1/2, step 2642/7134 completed (loss: 0.812771737575531, acc: 0.8392857313156128)
[2025-02-13 19:39:55,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:55,712][root][INFO] - Training Epoch: 1/2, step 2643/7134 completed (loss: 0.5554894804954529, acc: 0.8837209343910217)
[2025-02-13 19:39:55,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:56,151][root][INFO] - Training Epoch: 1/2, step 2644/7134 completed (loss: 0.4349498450756073, acc: 0.9047619104385376)
[2025-02-13 19:39:56,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:56,532][root][INFO] - Training Epoch: 1/2, step 2645/7134 completed (loss: 0.35871273279190063, acc: 0.9257143139839172)
[2025-02-13 19:39:56,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:56,939][root][INFO] - Training Epoch: 1/2, step 2646/7134 completed (loss: 0.4679754376411438, acc: 0.9006211161613464)
[2025-02-13 19:39:57,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:57,309][root][INFO] - Training Epoch: 1/2, step 2647/7134 completed (loss: 0.318984717130661, acc: 0.91847825050354)
[2025-02-13 19:39:57,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:57,660][root][INFO] - Training Epoch: 1/2, step 2648/7134 completed (loss: 0.450583279132843, acc: 0.9009901285171509)
[2025-02-13 19:39:57,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:58,031][root][INFO] - Training Epoch: 1/2, step 2649/7134 completed (loss: 0.585378110408783, acc: 0.8594594597816467)
[2025-02-13 19:39:58,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:58,357][root][INFO] - Training Epoch: 1/2, step 2650/7134 completed (loss: 0.6020780205726624, acc: 0.841176450252533)
[2025-02-13 19:39:58,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:58,726][root][INFO] - Training Epoch: 1/2, step 2651/7134 completed (loss: 0.22194956243038177, acc: 0.9609755873680115)
[2025-02-13 19:39:58,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:59,092][root][INFO] - Training Epoch: 1/2, step 2652/7134 completed (loss: 0.23616163432598114, acc: 0.949438214302063)
[2025-02-13 19:39:59,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:59,439][root][INFO] - Training Epoch: 1/2, step 2653/7134 completed (loss: 0.28517407178878784, acc: 0.9346405267715454)
[2025-02-13 19:39:59,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:59,821][root][INFO] - Training Epoch: 1/2, step 2654/7134 completed (loss: 0.18425777554512024, acc: 0.9642857313156128)
[2025-02-13 19:39:59,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:00,216][root][INFO] - Training Epoch: 1/2, step 2655/7134 completed (loss: 0.1964903175830841, acc: 0.9503105878829956)
[2025-02-13 19:40:00,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:00,578][root][INFO] - Training Epoch: 1/2, step 2656/7134 completed (loss: 0.38608813285827637, acc: 0.9186046719551086)
[2025-02-13 19:40:00,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:00,962][root][INFO] - Training Epoch: 1/2, step 2657/7134 completed (loss: 0.2958305776119232, acc: 0.9177215099334717)
[2025-02-13 19:40:01,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:01,316][root][INFO] - Training Epoch: 1/2, step 2658/7134 completed (loss: 0.44317206740379333, acc: 0.9141414165496826)
[2025-02-13 19:40:01,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:01,664][root][INFO] - Training Epoch: 1/2, step 2659/7134 completed (loss: 0.38205355405807495, acc: 0.9029850959777832)
[2025-02-13 19:40:01,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:02,036][root][INFO] - Training Epoch: 1/2, step 2660/7134 completed (loss: 0.4784896969795227, acc: 0.8974359035491943)
[2025-02-13 19:40:02,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:02,394][root][INFO] - Training Epoch: 1/2, step 2661/7134 completed (loss: 0.15761488676071167, acc: 0.9644970297813416)
[2025-02-13 19:40:02,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:02,821][root][INFO] - Training Epoch: 1/2, step 2662/7134 completed (loss: 0.2049889713525772, acc: 0.9629629850387573)
[2025-02-13 19:40:02,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:03,177][root][INFO] - Training Epoch: 1/2, step 2663/7134 completed (loss: 0.259597510099411, acc: 0.9326424598693848)
[2025-02-13 19:40:03,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:03,549][root][INFO] - Training Epoch: 1/2, step 2664/7134 completed (loss: 0.2615722417831421, acc: 0.9418604373931885)
[2025-02-13 19:40:03,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:03,888][root][INFO] - Training Epoch: 1/2, step 2665/7134 completed (loss: 0.14816808700561523, acc: 0.9638554453849792)
[2025-02-13 19:40:04,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:04,258][root][INFO] - Training Epoch: 1/2, step 2666/7134 completed (loss: 0.09745126217603683, acc: 0.9883720874786377)
[2025-02-13 19:40:04,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:04,662][root][INFO] - Training Epoch: 1/2, step 2667/7134 completed (loss: 0.07580243796110153, acc: 0.9810126423835754)
[2025-02-13 19:40:04,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:05,062][root][INFO] - Training Epoch: 1/2, step 2668/7134 completed (loss: 0.05959606543183327, acc: 0.9942857027053833)
[2025-02-13 19:40:05,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:05,435][root][INFO] - Training Epoch: 1/2, step 2669/7134 completed (loss: 0.06025715172290802, acc: 0.9900497794151306)
[2025-02-13 19:40:05,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:05,799][root][INFO] - Training Epoch: 1/2, step 2670/7134 completed (loss: 0.12105204910039902, acc: 0.9714285731315613)
[2025-02-13 19:40:05,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:06,195][root][INFO] - Training Epoch: 1/2, step 2671/7134 completed (loss: 0.1381043791770935, acc: 0.954285740852356)
[2025-02-13 19:40:06,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:06,580][root][INFO] - Training Epoch: 1/2, step 2672/7134 completed (loss: 0.05817628279328346, acc: 0.9879518151283264)
[2025-02-13 19:40:06,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:06,951][root][INFO] - Training Epoch: 1/2, step 2673/7134 completed (loss: 0.09318797290325165, acc: 0.9723756909370422)
[2025-02-13 19:40:07,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:07,296][root][INFO] - Training Epoch: 1/2, step 2674/7134 completed (loss: 0.14439649879932404, acc: 0.9629629850387573)
[2025-02-13 19:40:07,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:07,669][root][INFO] - Training Epoch: 1/2, step 2675/7134 completed (loss: 0.04193045198917389, acc: 0.9878787994384766)
[2025-02-13 19:40:07,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:08,016][root][INFO] - Training Epoch: 1/2, step 2676/7134 completed (loss: 0.08550617098808289, acc: 0.9772727489471436)
[2025-02-13 19:40:08,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:08,381][root][INFO] - Training Epoch: 1/2, step 2677/7134 completed (loss: 0.06606126576662064, acc: 0.9693251252174377)
[2025-02-13 19:40:08,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:08,803][root][INFO] - Training Epoch: 1/2, step 2678/7134 completed (loss: 0.06056012585759163, acc: 0.9830508232116699)
[2025-02-13 19:40:08,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:09,175][root][INFO] - Training Epoch: 1/2, step 2679/7134 completed (loss: 0.058907970786094666, acc: 0.9797979593276978)
[2025-02-13 19:40:09,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:09,552][root][INFO] - Training Epoch: 1/2, step 2680/7134 completed (loss: 0.12376932799816132, acc: 0.9670329689979553)
[2025-02-13 19:40:09,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:09,951][root][INFO] - Training Epoch: 1/2, step 2681/7134 completed (loss: 0.029717031866312027, acc: 0.9936708807945251)
[2025-02-13 19:40:10,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:10,317][root][INFO] - Training Epoch: 1/2, step 2682/7134 completed (loss: 0.13946232199668884, acc: 0.9530201554298401)
[2025-02-13 19:40:10,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:10,654][root][INFO] - Training Epoch: 1/2, step 2683/7134 completed (loss: 0.0570811964571476, acc: 0.9801324605941772)
[2025-02-13 19:40:10,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:11,004][root][INFO] - Training Epoch: 1/2, step 2684/7134 completed (loss: 0.292769193649292, acc: 0.9453125)
[2025-02-13 19:40:11,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:11,376][root][INFO] - Training Epoch: 1/2, step 2685/7134 completed (loss: 0.2102707028388977, acc: 0.9704142212867737)
[2025-02-13 19:40:11,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:11,752][root][INFO] - Training Epoch: 1/2, step 2686/7134 completed (loss: 0.2400425672531128, acc: 0.9399999976158142)
[2025-02-13 19:40:11,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:12,132][root][INFO] - Training Epoch: 1/2, step 2687/7134 completed (loss: 0.22739338874816895, acc: 0.9411764740943909)
[2025-02-13 19:40:12,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:12,525][root][INFO] - Training Epoch: 1/2, step 2688/7134 completed (loss: 0.26704350113868713, acc: 0.9097744226455688)
[2025-02-13 19:40:12,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:12,901][root][INFO] - Training Epoch: 1/2, step 2689/7134 completed (loss: 0.23537740111351013, acc: 0.934959352016449)
[2025-02-13 19:40:13,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:13,253][root][INFO] - Training Epoch: 1/2, step 2690/7134 completed (loss: 0.21480923891067505, acc: 0.935251772403717)
[2025-02-13 19:40:13,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:13,658][root][INFO] - Training Epoch: 1/2, step 2691/7134 completed (loss: 0.3862154185771942, acc: 0.9026548862457275)
[2025-02-13 19:40:13,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:14,020][root][INFO] - Training Epoch: 1/2, step 2692/7134 completed (loss: 0.44791173934936523, acc: 0.9008264541625977)
[2025-02-13 19:40:14,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:14,352][root][INFO] - Training Epoch: 1/2, step 2693/7134 completed (loss: 0.52106112241745, acc: 0.8938053250312805)
[2025-02-13 19:40:14,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:14,690][root][INFO] - Training Epoch: 1/2, step 2694/7134 completed (loss: 0.3597911298274994, acc: 0.9337748289108276)
[2025-02-13 19:40:14,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:15,064][root][INFO] - Training Epoch: 1/2, step 2695/7134 completed (loss: 0.2406739443540573, acc: 0.9555555582046509)
[2025-02-13 19:40:15,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:15,421][root][INFO] - Training Epoch: 1/2, step 2696/7134 completed (loss: 0.2666621208190918, acc: 0.9357143044471741)
[2025-02-13 19:40:15,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:15,776][root][INFO] - Training Epoch: 1/2, step 2697/7134 completed (loss: 0.32092946767807007, acc: 0.9155844449996948)
[2025-02-13 19:40:15,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:16,140][root][INFO] - Training Epoch: 1/2, step 2698/7134 completed (loss: 0.28951138257980347, acc: 0.918367326259613)
[2025-02-13 19:40:16,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:16,510][root][INFO] - Training Epoch: 1/2, step 2699/7134 completed (loss: 0.16465900838375092, acc: 0.982758641242981)
[2025-02-13 19:40:16,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:16,878][root][INFO] - Training Epoch: 1/2, step 2700/7134 completed (loss: 0.3413497507572174, acc: 0.8999999761581421)
[2025-02-13 19:40:17,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:17,247][root][INFO] - Training Epoch: 1/2, step 2701/7134 completed (loss: 0.2118075042963028, acc: 0.9398496150970459)
[2025-02-13 19:40:17,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:17,547][root][INFO] - Training Epoch: 1/2, step 2702/7134 completed (loss: 0.6412146687507629, acc: 0.8703703880310059)
[2025-02-13 19:40:17,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:17,969][root][INFO] - Training Epoch: 1/2, step 2703/7134 completed (loss: 0.06973877549171448, acc: 0.9866666793823242)
[2025-02-13 19:40:18,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:18,350][root][INFO] - Training Epoch: 1/2, step 2704/7134 completed (loss: 0.16365492343902588, acc: 0.9464285969734192)
[2025-02-13 19:40:18,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:18,727][root][INFO] - Training Epoch: 1/2, step 2705/7134 completed (loss: 0.3020104765892029, acc: 0.908450722694397)
[2025-02-13 19:40:18,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:19,127][root][INFO] - Training Epoch: 1/2, step 2706/7134 completed (loss: 0.2045440375804901, acc: 0.9383561611175537)
[2025-02-13 19:40:19,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:19,484][root][INFO] - Training Epoch: 1/2, step 2707/7134 completed (loss: 0.29731523990631104, acc: 0.9166666865348816)
[2025-02-13 19:40:19,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:19,847][root][INFO] - Training Epoch: 1/2, step 2708/7134 completed (loss: 0.21351398527622223, acc: 0.9371428489685059)
[2025-02-13 19:40:19,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:20,220][root][INFO] - Training Epoch: 1/2, step 2709/7134 completed (loss: 0.1305369883775711, acc: 0.9647887349128723)
[2025-02-13 19:40:20,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:20,573][root][INFO] - Training Epoch: 1/2, step 2710/7134 completed (loss: 0.29542720317840576, acc: 0.8999999761581421)
[2025-02-13 19:40:20,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:20,929][root][INFO] - Training Epoch: 1/2, step 2711/7134 completed (loss: 0.06881725788116455, acc: 0.9793814420700073)
[2025-02-13 19:40:21,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:21,294][root][INFO] - Training Epoch: 1/2, step 2712/7134 completed (loss: 0.31457987427711487, acc: 0.914893627166748)
[2025-02-13 19:40:21,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:21,661][root][INFO] - Training Epoch: 1/2, step 2713/7134 completed (loss: 0.25905779004096985, acc: 0.9375)
[2025-02-13 19:40:21,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:22,012][root][INFO] - Training Epoch: 1/2, step 2714/7134 completed (loss: 0.09190002828836441, acc: 0.9852941036224365)
[2025-02-13 19:40:22,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:22,365][root][INFO] - Training Epoch: 1/2, step 2715/7134 completed (loss: 0.09969419240951538, acc: 0.9724137783050537)
[2025-02-13 19:40:22,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:22,723][root][INFO] - Training Epoch: 1/2, step 2716/7134 completed (loss: 0.13860833644866943, acc: 0.9651162624359131)
[2025-02-13 19:40:22,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:23,073][root][INFO] - Training Epoch: 1/2, step 2717/7134 completed (loss: 0.2524610161781311, acc: 0.9316239356994629)
[2025-02-13 19:40:23,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:23,437][root][INFO] - Training Epoch: 1/2, step 2718/7134 completed (loss: 0.339600145816803, acc: 0.9236111044883728)
[2025-02-13 19:40:23,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:23,796][root][INFO] - Training Epoch: 1/2, step 2719/7134 completed (loss: 0.20737004280090332, acc: 0.9496402740478516)
[2025-02-13 19:40:23,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:24,153][root][INFO] - Training Epoch: 1/2, step 2720/7134 completed (loss: 0.30475980043411255, acc: 0.9197080135345459)
[2025-02-13 19:40:24,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:24,478][root][INFO] - Training Epoch: 1/2, step 2721/7134 completed (loss: 0.20580846071243286, acc: 0.9735099077224731)
[2025-02-13 19:40:24,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:24,759][root][INFO] - Training Epoch: 1/2, step 2722/7134 completed (loss: 0.18354512751102448, acc: 0.957446813583374)
[2025-02-13 19:40:24,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:25,097][root][INFO] - Training Epoch: 1/2, step 2723/7134 completed (loss: 0.43562522530555725, acc: 0.910179615020752)
[2025-02-13 19:40:25,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:25,483][root][INFO] - Training Epoch: 1/2, step 2724/7134 completed (loss: 0.10839607566595078, acc: 0.9887005686759949)
[2025-02-13 19:40:25,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:25,840][root][INFO] - Training Epoch: 1/2, step 2725/7134 completed (loss: 0.09676044434309006, acc: 0.9701492786407471)
[2025-02-13 19:40:25,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:26,189][root][INFO] - Training Epoch: 1/2, step 2726/7134 completed (loss: 0.2914779782295227, acc: 0.934959352016449)
[2025-02-13 19:40:26,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:26,567][root][INFO] - Training Epoch: 1/2, step 2727/7134 completed (loss: 0.10033717751502991, acc: 0.9736841917037964)
[2025-02-13 19:40:26,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:26,926][root][INFO] - Training Epoch: 1/2, step 2728/7134 completed (loss: 1.0057802200317383, acc: 0.7555555701255798)
[2025-02-13 19:40:27,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:27,287][root][INFO] - Training Epoch: 1/2, step 2729/7134 completed (loss: 0.6585283875465393, acc: 0.8642857074737549)
[2025-02-13 19:40:27,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:27,650][root][INFO] - Training Epoch: 1/2, step 2730/7134 completed (loss: 0.6423873901367188, acc: 0.8495575189590454)
[2025-02-13 19:40:27,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:27,998][root][INFO] - Training Epoch: 1/2, step 2731/7134 completed (loss: 0.44237518310546875, acc: 0.893203854560852)
[2025-02-13 19:40:28,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:28,361][root][INFO] - Training Epoch: 1/2, step 2732/7134 completed (loss: 0.49277639389038086, acc: 0.8774510025978088)
[2025-02-13 19:40:28,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:28,730][root][INFO] - Training Epoch: 1/2, step 2733/7134 completed (loss: 0.3636881411075592, acc: 0.9277777671813965)
[2025-02-13 19:40:28,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:29,073][root][INFO] - Training Epoch: 1/2, step 2734/7134 completed (loss: 0.4310690462589264, acc: 0.9008264541625977)
[2025-02-13 19:40:29,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:29,464][root][INFO] - Training Epoch: 1/2, step 2735/7134 completed (loss: 0.3158183991909027, acc: 0.9512194991111755)
[2025-02-13 19:40:29,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:29,815][root][INFO] - Training Epoch: 1/2, step 2736/7134 completed (loss: 0.5312674045562744, acc: 0.8993710875511169)
[2025-02-13 19:40:29,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:30,168][root][INFO] - Training Epoch: 1/2, step 2737/7134 completed (loss: 0.4607003331184387, acc: 0.9285714030265808)
[2025-02-13 19:40:30,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:30,600][root][INFO] - Training Epoch: 1/2, step 2738/7134 completed (loss: 0.343166708946228, acc: 0.8922155499458313)
[2025-02-13 19:40:30,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:30,959][root][INFO] - Training Epoch: 1/2, step 2739/7134 completed (loss: 0.4446367621421814, acc: 0.8623188138008118)
[2025-02-13 19:40:31,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:31,318][root][INFO] - Training Epoch: 1/2, step 2740/7134 completed (loss: 0.30806291103363037, acc: 0.9251700639724731)
[2025-02-13 19:40:31,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:31,634][root][INFO] - Training Epoch: 1/2, step 2741/7134 completed (loss: 0.21084648370742798, acc: 0.9482758641242981)
[2025-02-13 19:40:31,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:31,994][root][INFO] - Training Epoch: 1/2, step 2742/7134 completed (loss: 0.26422154903411865, acc: 0.9411764740943909)
[2025-02-13 19:40:32,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:32,330][root][INFO] - Training Epoch: 1/2, step 2743/7134 completed (loss: 0.1411830335855484, acc: 0.9468085169792175)
[2025-02-13 19:40:32,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:32,616][root][INFO] - Training Epoch: 1/2, step 2744/7134 completed (loss: 0.13424068689346313, acc: 0.9516128897666931)
[2025-02-13 19:40:32,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:32,951][root][INFO] - Training Epoch: 1/2, step 2745/7134 completed (loss: 0.6413266658782959, acc: 0.851190447807312)
[2025-02-13 19:40:33,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:33,290][root][INFO] - Training Epoch: 1/2, step 2746/7134 completed (loss: 0.43964359164237976, acc: 0.8636363744735718)
[2025-02-13 19:40:33,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:33,658][root][INFO] - Training Epoch: 1/2, step 2747/7134 completed (loss: 0.5357575416564941, acc: 0.9074074029922485)
[2025-02-13 19:40:33,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:34,050][root][INFO] - Training Epoch: 1/2, step 2748/7134 completed (loss: 0.22817827761173248, acc: 0.9610389471054077)
[2025-02-13 19:40:34,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:34,380][root][INFO] - Training Epoch: 1/2, step 2749/7134 completed (loss: 0.27994558215141296, acc: 0.9278350472450256)
[2025-02-13 19:40:34,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:34,724][root][INFO] - Training Epoch: 1/2, step 2750/7134 completed (loss: 0.23188714683055878, acc: 0.9277108311653137)
[2025-02-13 19:40:34,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:35,069][root][INFO] - Training Epoch: 1/2, step 2751/7134 completed (loss: 0.4443642199039459, acc: 0.8828828930854797)
[2025-02-13 19:40:35,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:35,428][root][INFO] - Training Epoch: 1/2, step 2752/7134 completed (loss: 0.29908087849617004, acc: 0.9343065619468689)
[2025-02-13 19:40:35,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:35,781][root][INFO] - Training Epoch: 1/2, step 2753/7134 completed (loss: 0.3349374532699585, acc: 0.924369752407074)
[2025-02-13 19:40:35,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:36,145][root][INFO] - Training Epoch: 1/2, step 2754/7134 completed (loss: 0.3588279187679291, acc: 0.925000011920929)
[2025-02-13 19:40:36,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:36,496][root][INFO] - Training Epoch: 1/2, step 2755/7134 completed (loss: 0.2614383399486542, acc: 0.9453125)
[2025-02-13 19:40:36,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:36,860][root][INFO] - Training Epoch: 1/2, step 2756/7134 completed (loss: 0.32039958238601685, acc: 0.9054054021835327)
[2025-02-13 19:40:36,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:37,154][root][INFO] - Training Epoch: 1/2, step 2757/7134 completed (loss: 0.3479406535625458, acc: 0.8913043737411499)
[2025-02-13 19:40:37,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:37,510][root][INFO] - Training Epoch: 1/2, step 2758/7134 completed (loss: 0.338346004486084, acc: 0.9016393423080444)
[2025-02-13 19:40:37,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:37,852][root][INFO] - Training Epoch: 1/2, step 2759/7134 completed (loss: 0.16183409094810486, acc: 0.9571428298950195)
[2025-02-13 19:40:37,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:38,200][root][INFO] - Training Epoch: 1/2, step 2760/7134 completed (loss: 0.18993277847766876, acc: 0.9555555582046509)
[2025-02-13 19:40:38,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:38,564][root][INFO] - Training Epoch: 1/2, step 2761/7134 completed (loss: 0.42977169156074524, acc: 0.878947377204895)
[2025-02-13 19:40:38,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:38,953][root][INFO] - Training Epoch: 1/2, step 2762/7134 completed (loss: 0.35243651270866394, acc: 0.9261083602905273)
[2025-02-13 19:40:39,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:39,325][root][INFO] - Training Epoch: 1/2, step 2763/7134 completed (loss: 0.22529514133930206, acc: 0.9344262480735779)
[2025-02-13 19:40:39,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:39,745][root][INFO] - Training Epoch: 1/2, step 2764/7134 completed (loss: 0.28575655817985535, acc: 0.9166666865348816)
[2025-02-13 19:40:39,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:40,137][root][INFO] - Training Epoch: 1/2, step 2765/7134 completed (loss: 0.2586045563220978, acc: 0.929411768913269)
[2025-02-13 19:40:40,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:40,542][root][INFO] - Training Epoch: 1/2, step 2766/7134 completed (loss: 0.08070404082536697, acc: 0.9745222926139832)
[2025-02-13 19:40:40,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:40,913][root][INFO] - Training Epoch: 1/2, step 2767/7134 completed (loss: 0.18833065032958984, acc: 0.949999988079071)
[2025-02-13 19:40:41,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:41,252][root][INFO] - Training Epoch: 1/2, step 2768/7134 completed (loss: 0.2409132719039917, acc: 0.9337349534034729)
[2025-02-13 19:40:41,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:41,625][root][INFO] - Training Epoch: 1/2, step 2769/7134 completed (loss: 0.1843906044960022, acc: 0.9368420839309692)
[2025-02-13 19:40:41,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:42,011][root][INFO] - Training Epoch: 1/2, step 2770/7134 completed (loss: 0.12704375386238098, acc: 0.9814814925193787)
[2025-02-13 19:40:42,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:42,399][root][INFO] - Training Epoch: 1/2, step 2771/7134 completed (loss: 0.41396912932395935, acc: 0.9099525809288025)
[2025-02-13 19:40:42,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:42,788][root][INFO] - Training Epoch: 1/2, step 2772/7134 completed (loss: 0.2631213665008545, acc: 0.925000011920929)
[2025-02-13 19:40:42,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:43,156][root][INFO] - Training Epoch: 1/2, step 2773/7134 completed (loss: 0.19980478286743164, acc: 0.9547738432884216)
[2025-02-13 19:40:43,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:43,532][root][INFO] - Training Epoch: 1/2, step 2774/7134 completed (loss: 0.3331416845321655, acc: 0.9268292784690857)
[2025-02-13 19:40:43,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:43,905][root][INFO] - Training Epoch: 1/2, step 2775/7134 completed (loss: 0.23603251576423645, acc: 0.9234972596168518)
[2025-02-13 19:40:44,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:44,297][root][INFO] - Training Epoch: 1/2, step 2776/7134 completed (loss: 0.1622207909822464, acc: 0.9653679728507996)
[2025-02-13 19:40:44,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:44,662][root][INFO] - Training Epoch: 1/2, step 2777/7134 completed (loss: 0.07637207210063934, acc: 0.9828571677207947)
[2025-02-13 19:40:44,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:45,046][root][INFO] - Training Epoch: 1/2, step 2778/7134 completed (loss: 0.28590869903564453, acc: 0.9191918969154358)
[2025-02-13 19:40:45,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:45,423][root][INFO] - Training Epoch: 1/2, step 2779/7134 completed (loss: 0.22914792597293854, acc: 0.9406392574310303)
[2025-02-13 19:40:45,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:45,832][root][INFO] - Training Epoch: 1/2, step 2780/7134 completed (loss: 0.3096996545791626, acc: 0.9203540086746216)
[2025-02-13 19:40:45,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:46,227][root][INFO] - Training Epoch: 1/2, step 2781/7134 completed (loss: 0.1924876570701599, acc: 0.95652174949646)
[2025-02-13 19:40:46,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:46,652][root][INFO] - Training Epoch: 1/2, step 2782/7134 completed (loss: 0.11447356641292572, acc: 0.9732142686843872)
[2025-02-13 19:40:46,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:47,017][root][INFO] - Training Epoch: 1/2, step 2783/7134 completed (loss: 0.11141882091760635, acc: 0.9931972622871399)
[2025-02-13 19:40:47,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:47,380][root][INFO] - Training Epoch: 1/2, step 2784/7134 completed (loss: 0.1566825658082962, acc: 0.9613526463508606)
[2025-02-13 19:40:47,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:47,784][root][INFO] - Training Epoch: 1/2, step 2785/7134 completed (loss: 0.17788080871105194, acc: 0.9611650705337524)
[2025-02-13 19:40:47,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:48,179][root][INFO] - Training Epoch: 1/2, step 2786/7134 completed (loss: 0.32127997279167175, acc: 0.9111111164093018)
[2025-02-13 19:40:48,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:48,584][root][INFO] - Training Epoch: 1/2, step 2787/7134 completed (loss: 0.16929014027118683, acc: 0.9537572264671326)
[2025-02-13 19:40:48,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:48,960][root][INFO] - Training Epoch: 1/2, step 2788/7134 completed (loss: 0.40033531188964844, acc: 0.8980891704559326)
[2025-02-13 19:40:49,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:49,322][root][INFO] - Training Epoch: 1/2, step 2789/7134 completed (loss: 1.5526846647262573, acc: 0.6761904954910278)
[2025-02-13 19:40:49,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:49,691][root][INFO] - Training Epoch: 1/2, step 2790/7134 completed (loss: 1.0582304000854492, acc: 0.7333333492279053)
[2025-02-13 19:40:49,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:50,056][root][INFO] - Training Epoch: 1/2, step 2791/7134 completed (loss: 0.5804245471954346, acc: 0.860927164554596)
[2025-02-13 19:40:50,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:50,429][root][INFO] - Training Epoch: 1/2, step 2792/7134 completed (loss: 0.20068368315696716, acc: 0.9534883499145508)
[2025-02-13 19:40:50,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:50,799][root][INFO] - Training Epoch: 1/2, step 2793/7134 completed (loss: 0.15478499233722687, acc: 0.970059871673584)
[2025-02-13 19:40:50,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:51,144][root][INFO] - Training Epoch: 1/2, step 2794/7134 completed (loss: 0.1591852307319641, acc: 0.939393937587738)
[2025-02-13 19:40:51,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:51,498][root][INFO] - Training Epoch: 1/2, step 2795/7134 completed (loss: 0.5598839521408081, acc: 0.8703703880310059)
[2025-02-13 19:40:51,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:51,819][root][INFO] - Training Epoch: 1/2, step 2796/7134 completed (loss: 0.33792129158973694, acc: 0.9024389982223511)
[2025-02-13 19:40:51,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:52,147][root][INFO] - Training Epoch: 1/2, step 2797/7134 completed (loss: 0.6690587401390076, acc: 0.8048780560493469)
[2025-02-13 19:40:52,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:52,511][root][INFO] - Training Epoch: 1/2, step 2798/7134 completed (loss: 0.3673652708530426, acc: 0.8833333253860474)
[2025-02-13 19:40:52,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:52,849][root][INFO] - Training Epoch: 1/2, step 2799/7134 completed (loss: 0.4465089738368988, acc: 0.875)
[2025-02-13 19:40:52,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:53,160][root][INFO] - Training Epoch: 1/2, step 2800/7134 completed (loss: 0.263324499130249, acc: 0.9270073175430298)
[2025-02-13 19:40:53,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:53,511][root][INFO] - Training Epoch: 1/2, step 2801/7134 completed (loss: 0.31261926889419556, acc: 0.9344262480735779)
[2025-02-13 19:40:53,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:53,878][root][INFO] - Training Epoch: 1/2, step 2802/7134 completed (loss: 0.28442639112472534, acc: 0.9145728349685669)
[2025-02-13 19:40:54,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:54,242][root][INFO] - Training Epoch: 1/2, step 2803/7134 completed (loss: 0.14134302735328674, acc: 0.9691358208656311)
[2025-02-13 19:40:54,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:54,621][root][INFO] - Training Epoch: 1/2, step 2804/7134 completed (loss: 0.18194404244422913, acc: 0.9653179049491882)
[2025-02-13 19:40:54,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:54,983][root][INFO] - Training Epoch: 1/2, step 2805/7134 completed (loss: 0.09735684096813202, acc: 0.9709302186965942)
[2025-02-13 19:40:55,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:55,342][root][INFO] - Training Epoch: 1/2, step 2806/7134 completed (loss: 0.1605713814496994, acc: 0.970588207244873)
[2025-02-13 19:40:55,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:55,732][root][INFO] - Training Epoch: 1/2, step 2807/7134 completed (loss: 0.22499345242977142, acc: 0.9514563083648682)
[2025-02-13 19:40:55,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:56,116][root][INFO] - Training Epoch: 1/2, step 2808/7134 completed (loss: 0.23172222077846527, acc: 0.9246231317520142)
[2025-02-13 19:40:56,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:56,475][root][INFO] - Training Epoch: 1/2, step 2809/7134 completed (loss: 0.2362772524356842, acc: 0.9370078444480896)
[2025-02-13 19:40:56,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:56,858][root][INFO] - Training Epoch: 1/2, step 2810/7134 completed (loss: 0.11421150714159012, acc: 0.9726775884628296)
[2025-02-13 19:40:56,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:57,236][root][INFO] - Training Epoch: 1/2, step 2811/7134 completed (loss: 0.09166324138641357, acc: 0.9779005646705627)
[2025-02-13 19:40:57,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:57,659][root][INFO] - Training Epoch: 1/2, step 2812/7134 completed (loss: 0.0752277821302414, acc: 0.9933775067329407)
[2025-02-13 19:40:57,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:58,033][root][INFO] - Training Epoch: 1/2, step 2813/7134 completed (loss: 0.2014569491147995, acc: 0.9482758641242981)
[2025-02-13 19:40:58,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:58,389][root][INFO] - Training Epoch: 1/2, step 2814/7134 completed (loss: 0.23296023905277252, acc: 0.9418604373931885)
[2025-02-13 19:40:58,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:58,744][root][INFO] - Training Epoch: 1/2, step 2815/7134 completed (loss: 0.1813533753156662, acc: 0.9560439586639404)
[2025-02-13 19:40:58,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:59,120][root][INFO] - Training Epoch: 1/2, step 2816/7134 completed (loss: 0.1898186355829239, acc: 0.9607843160629272)
[2025-02-13 19:40:59,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:59,480][root][INFO] - Training Epoch: 1/2, step 2817/7134 completed (loss: 0.200853168964386, acc: 0.9492385983467102)
[2025-02-13 19:40:59,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:59,841][root][INFO] - Training Epoch: 1/2, step 2818/7134 completed (loss: 0.15996180474758148, acc: 0.9627659320831299)
[2025-02-13 19:40:59,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:00,201][root][INFO] - Training Epoch: 1/2, step 2819/7134 completed (loss: 0.13933168351650238, acc: 0.965753436088562)
[2025-02-13 19:41:00,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:00,605][root][INFO] - Training Epoch: 1/2, step 2820/7134 completed (loss: 0.4297517240047455, acc: 0.9018405079841614)
[2025-02-13 19:41:00,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:00,987][root][INFO] - Training Epoch: 1/2, step 2821/7134 completed (loss: 0.536740243434906, acc: 0.9067357778549194)
[2025-02-13 19:41:01,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:01,347][root][INFO] - Training Epoch: 1/2, step 2822/7134 completed (loss: 0.591106653213501, acc: 0.8918918967247009)
[2025-02-13 19:41:01,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:01,745][root][INFO] - Training Epoch: 1/2, step 2823/7134 completed (loss: 0.37855321168899536, acc: 0.9316770434379578)
[2025-02-13 19:41:01,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:02,121][root][INFO] - Training Epoch: 1/2, step 2824/7134 completed (loss: 0.762563943862915, acc: 0.8553459048271179)
[2025-02-13 19:41:02,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:02,483][root][INFO] - Training Epoch: 1/2, step 2825/7134 completed (loss: 0.2303977906703949, acc: 0.9555555582046509)
[2025-02-13 19:41:02,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:02,853][root][INFO] - Training Epoch: 1/2, step 2826/7134 completed (loss: 0.34065914154052734, acc: 0.8908045887947083)
[2025-02-13 19:41:02,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:03,236][root][INFO] - Training Epoch: 1/2, step 2827/7134 completed (loss: 0.21785710752010345, acc: 0.9617224931716919)
[2025-02-13 19:41:03,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:03,589][root][INFO] - Training Epoch: 1/2, step 2828/7134 completed (loss: 0.22968123853206635, acc: 0.9556962251663208)
[2025-02-13 19:41:03,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:03,973][root][INFO] - Training Epoch: 1/2, step 2829/7134 completed (loss: 0.22958292067050934, acc: 0.9642857313156128)
[2025-02-13 19:41:04,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:04,339][root][INFO] - Training Epoch: 1/2, step 2830/7134 completed (loss: 0.1198538988828659, acc: 0.9552238583564758)
[2025-02-13 19:41:04,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:04,730][root][INFO] - Training Epoch: 1/2, step 2831/7134 completed (loss: 0.18184266984462738, acc: 0.9459459185600281)
[2025-02-13 19:41:04,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:05,115][root][INFO] - Training Epoch: 1/2, step 2832/7134 completed (loss: 0.31534257531166077, acc: 0.9193548560142517)
[2025-02-13 19:41:05,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:05,484][root][INFO] - Training Epoch: 1/2, step 2833/7134 completed (loss: 0.29083698987960815, acc: 0.920634925365448)
[2025-02-13 19:41:05,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:05,860][root][INFO] - Training Epoch: 1/2, step 2834/7134 completed (loss: 0.28979137539863586, acc: 0.9222221970558167)
[2025-02-13 19:41:06,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:06,230][root][INFO] - Training Epoch: 1/2, step 2835/7134 completed (loss: 0.15086577832698822, acc: 0.9671052694320679)
[2025-02-13 19:41:06,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:06,585][root][INFO] - Training Epoch: 1/2, step 2836/7134 completed (loss: 0.36010414361953735, acc: 0.9200000166893005)
[2025-02-13 19:41:06,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:06,957][root][INFO] - Training Epoch: 1/2, step 2837/7134 completed (loss: 0.1404910385608673, acc: 0.9590643048286438)
[2025-02-13 19:41:07,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:07,326][root][INFO] - Training Epoch: 1/2, step 2838/7134 completed (loss: 0.31251731514930725, acc: 0.931034505367279)
[2025-02-13 19:41:07,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:07,679][root][INFO] - Training Epoch: 1/2, step 2839/7134 completed (loss: 0.28185972571372986, acc: 0.9626865386962891)
[2025-02-13 19:41:07,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:08,055][root][INFO] - Training Epoch: 1/2, step 2840/7134 completed (loss: 0.17993195354938507, acc: 0.9647058844566345)
[2025-02-13 19:41:08,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:08,415][root][INFO] - Training Epoch: 1/2, step 2841/7134 completed (loss: 0.1517539620399475, acc: 0.9811320900917053)
[2025-02-13 19:41:08,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:08,788][root][INFO] - Training Epoch: 1/2, step 2842/7134 completed (loss: 0.3109045624732971, acc: 0.9322916865348816)
[2025-02-13 19:41:08,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:09,144][root][INFO] - Training Epoch: 1/2, step 2843/7134 completed (loss: 0.19198939204216003, acc: 0.9496855139732361)
[2025-02-13 19:41:09,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:09,501][root][INFO] - Training Epoch: 1/2, step 2844/7134 completed (loss: 0.42345547676086426, acc: 0.8804348111152649)
[2025-02-13 19:41:09,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:09,894][root][INFO] - Training Epoch: 1/2, step 2845/7134 completed (loss: 0.3343378007411957, acc: 0.9054054021835327)
[2025-02-13 19:41:10,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:10,264][root][INFO] - Training Epoch: 1/2, step 2846/7134 completed (loss: 0.19788306951522827, acc: 0.9542483687400818)
[2025-02-13 19:41:10,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:10,657][root][INFO] - Training Epoch: 1/2, step 2847/7134 completed (loss: 0.16287986934185028, acc: 0.9745222926139832)
[2025-02-13 19:41:10,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:11,033][root][INFO] - Training Epoch: 1/2, step 2848/7134 completed (loss: 0.23283369839191437, acc: 0.9541284441947937)
[2025-02-13 19:41:11,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:11,368][root][INFO] - Training Epoch: 1/2, step 2849/7134 completed (loss: 0.33165979385375977, acc: 0.9200000166893005)
[2025-02-13 19:41:11,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:11,732][root][INFO] - Training Epoch: 1/2, step 2850/7134 completed (loss: 0.5661624073982239, acc: 0.8717948794364929)
[2025-02-13 19:41:11,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:12,107][root][INFO] - Training Epoch: 1/2, step 2851/7134 completed (loss: 0.26350876688957214, acc: 0.9185185432434082)
[2025-02-13 19:41:12,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:12,473][root][INFO] - Training Epoch: 1/2, step 2852/7134 completed (loss: 0.23402108252048492, acc: 0.9512194991111755)
[2025-02-13 19:41:12,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:12,827][root][INFO] - Training Epoch: 1/2, step 2853/7134 completed (loss: 0.1651156097650528, acc: 0.9591836929321289)
[2025-02-13 19:41:12,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:13,186][root][INFO] - Training Epoch: 1/2, step 2854/7134 completed (loss: 0.2589341998100281, acc: 0.918367326259613)
[2025-02-13 19:41:13,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:13,612][root][INFO] - Training Epoch: 1/2, step 2855/7134 completed (loss: 0.20343033969402313, acc: 0.9459459185600281)
[2025-02-13 19:41:13,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:14,025][root][INFO] - Training Epoch: 1/2, step 2856/7134 completed (loss: 0.13149888813495636, acc: 0.9627329111099243)
[2025-02-13 19:41:14,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:14,389][root][INFO] - Training Epoch: 1/2, step 2857/7134 completed (loss: 0.20366127789020538, acc: 0.9580838084220886)
[2025-02-13 19:41:14,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:14,756][root][INFO] - Training Epoch: 1/2, step 2858/7134 completed (loss: 0.20027102530002594, acc: 0.9696969985961914)
[2025-02-13 19:41:14,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:15,129][root][INFO] - Training Epoch: 1/2, step 2859/7134 completed (loss: 0.4262230396270752, acc: 0.9166666865348816)
[2025-02-13 19:41:15,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:15,477][root][INFO] - Training Epoch: 1/2, step 2860/7134 completed (loss: 0.23303747177124023, acc: 0.9375)
[2025-02-13 19:41:15,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:15,836][root][INFO] - Training Epoch: 1/2, step 2861/7134 completed (loss: 0.14873480796813965, acc: 0.9642857313156128)
[2025-02-13 19:41:15,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:16,162][root][INFO] - Training Epoch: 1/2, step 2862/7134 completed (loss: 0.15419381856918335, acc: 0.9774436354637146)
[2025-02-13 19:41:16,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:16,547][root][INFO] - Training Epoch: 1/2, step 2863/7134 completed (loss: 0.1538143903017044, acc: 0.9668508172035217)
[2025-02-13 19:41:16,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:16,939][root][INFO] - Training Epoch: 1/2, step 2864/7134 completed (loss: 0.18994680047035217, acc: 0.9789473414421082)
[2025-02-13 19:41:17,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:17,309][root][INFO] - Training Epoch: 1/2, step 2865/7134 completed (loss: 0.15362532436847687, acc: 0.95652174949646)
[2025-02-13 19:41:17,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:17,703][root][INFO] - Training Epoch: 1/2, step 2866/7134 completed (loss: 0.1481088548898697, acc: 0.9548386931419373)
[2025-02-13 19:41:17,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:18,099][root][INFO] - Training Epoch: 1/2, step 2867/7134 completed (loss: 0.15449613332748413, acc: 0.9259259104728699)
[2025-02-13 19:41:18,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:18,488][root][INFO] - Training Epoch: 1/2, step 2868/7134 completed (loss: 0.1781770884990692, acc: 0.9647058844566345)
[2025-02-13 19:41:18,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:18,895][root][INFO] - Training Epoch: 1/2, step 2869/7134 completed (loss: 0.2563221752643585, acc: 0.9448275566101074)
[2025-02-13 19:41:19,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:19,263][root][INFO] - Training Epoch: 1/2, step 2870/7134 completed (loss: 0.5334166884422302, acc: 0.9090909361839294)
[2025-02-13 19:41:19,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:19,621][root][INFO] - Training Epoch: 1/2, step 2871/7134 completed (loss: 0.22741462290287018, acc: 0.9586206674575806)
[2025-02-13 19:41:19,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:19,980][root][INFO] - Training Epoch: 1/2, step 2872/7134 completed (loss: 0.12176788598299026, acc: 0.9646017551422119)
[2025-02-13 19:41:20,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:20,369][root][INFO] - Training Epoch: 1/2, step 2873/7134 completed (loss: 0.13058346509933472, acc: 0.961240291595459)
[2025-02-13 19:41:20,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:20,741][root][INFO] - Training Epoch: 1/2, step 2874/7134 completed (loss: 0.10060807317495346, acc: 0.9722222089767456)
[2025-02-13 19:41:20,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:21,145][root][INFO] - Training Epoch: 1/2, step 2875/7134 completed (loss: 0.2839714586734772, acc: 0.9285714030265808)
[2025-02-13 19:41:21,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:21,547][root][INFO] - Training Epoch: 1/2, step 2876/7134 completed (loss: 0.40967071056365967, acc: 0.9245283007621765)
[2025-02-13 19:41:21,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:21,921][root][INFO] - Training Epoch: 1/2, step 2877/7134 completed (loss: 0.16021698713302612, acc: 0.9572649598121643)
[2025-02-13 19:41:22,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:22,287][root][INFO] - Training Epoch: 1/2, step 2878/7134 completed (loss: 0.13624809682369232, acc: 0.9736841917037964)
[2025-02-13 19:41:22,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:22,643][root][INFO] - Training Epoch: 1/2, step 2879/7134 completed (loss: 0.0741826519370079, acc: 0.9795918464660645)
[2025-02-13 19:41:22,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:23,043][root][INFO] - Training Epoch: 1/2, step 2880/7134 completed (loss: 0.060063768178224564, acc: 0.9794520735740662)
[2025-02-13 19:41:23,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:23,402][root][INFO] - Training Epoch: 1/2, step 2881/7134 completed (loss: 0.15340298414230347, acc: 0.9523809552192688)
[2025-02-13 19:41:23,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:23,789][root][INFO] - Training Epoch: 1/2, step 2882/7134 completed (loss: 0.35066890716552734, acc: 0.884393036365509)
[2025-02-13 19:41:23,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:24,143][root][INFO] - Training Epoch: 1/2, step 2883/7134 completed (loss: 0.354112446308136, acc: 0.9041916131973267)
[2025-02-13 19:41:24,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:24,502][root][INFO] - Training Epoch: 1/2, step 2884/7134 completed (loss: 0.36726245284080505, acc: 0.8787878751754761)
[2025-02-13 19:41:24,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:24,853][root][INFO] - Training Epoch: 1/2, step 2885/7134 completed (loss: 0.5094114542007446, acc: 0.8545454740524292)
[2025-02-13 19:41:24,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:25,201][root][INFO] - Training Epoch: 1/2, step 2886/7134 completed (loss: 0.4202430844306946, acc: 0.8999999761581421)
[2025-02-13 19:41:25,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:25,577][root][INFO] - Training Epoch: 1/2, step 2887/7134 completed (loss: 0.3596038818359375, acc: 0.9126637578010559)
[2025-02-13 19:41:25,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:25,961][root][INFO] - Training Epoch: 1/2, step 2888/7134 completed (loss: 0.3026024401187897, acc: 0.9263157844543457)
[2025-02-13 19:41:26,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:26,336][root][INFO] - Training Epoch: 1/2, step 2889/7134 completed (loss: 0.24595265090465546, acc: 0.9217877388000488)
[2025-02-13 19:41:26,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:26,717][root][INFO] - Training Epoch: 1/2, step 2890/7134 completed (loss: 0.22397808730602264, acc: 0.9384615421295166)
[2025-02-13 19:41:26,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:27,082][root][INFO] - Training Epoch: 1/2, step 2891/7134 completed (loss: 0.1747121512889862, acc: 0.9333333373069763)
[2025-02-13 19:41:27,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:27,450][root][INFO] - Training Epoch: 1/2, step 2892/7134 completed (loss: 0.36116865277290344, acc: 0.9289940595626831)
[2025-02-13 19:41:27,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:27,826][root][INFO] - Training Epoch: 1/2, step 2893/7134 completed (loss: 0.29875871539115906, acc: 0.9016393423080444)
[2025-02-13 19:41:27,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:28,249][root][INFO] - Training Epoch: 1/2, step 2894/7134 completed (loss: 0.3615244925022125, acc: 0.8888888955116272)
[2025-02-13 19:41:28,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:28,601][root][INFO] - Training Epoch: 1/2, step 2895/7134 completed (loss: 0.10894489288330078, acc: 0.9661017060279846)
[2025-02-13 19:41:28,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:28,969][root][INFO] - Training Epoch: 1/2, step 2896/7134 completed (loss: 0.1922270953655243, acc: 0.9640718698501587)
[2025-02-13 19:41:29,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:29,321][root][INFO] - Training Epoch: 1/2, step 2897/7134 completed (loss: 0.3969905972480774, acc: 0.9172932505607605)
[2025-02-13 19:41:29,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:29,717][root][INFO] - Training Epoch: 1/2, step 2898/7134 completed (loss: 0.20291948318481445, acc: 0.9510869383811951)
[2025-02-13 19:41:29,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:30,144][root][INFO] - Training Epoch: 1/2, step 2899/7134 completed (loss: 0.2572503089904785, acc: 0.9281437397003174)
[2025-02-13 19:41:30,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:30,493][root][INFO] - Training Epoch: 1/2, step 2900/7134 completed (loss: 0.19502174854278564, acc: 0.957446813583374)
[2025-02-13 19:41:30,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:30,874][root][INFO] - Training Epoch: 1/2, step 2901/7134 completed (loss: 0.22479844093322754, acc: 0.9324324131011963)
[2025-02-13 19:41:31,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:31,257][root][INFO] - Training Epoch: 1/2, step 2902/7134 completed (loss: 0.18845510482788086, acc: 0.9572649598121643)
[2025-02-13 19:41:31,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:31,627][root][INFO] - Training Epoch: 1/2, step 2903/7134 completed (loss: 0.15605272352695465, acc: 0.949438214302063)
[2025-02-13 19:41:31,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:32,001][root][INFO] - Training Epoch: 1/2, step 2904/7134 completed (loss: 0.1158701702952385, acc: 0.9783783555030823)
[2025-02-13 19:41:32,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:32,394][root][INFO] - Training Epoch: 1/2, step 2905/7134 completed (loss: 0.19475123286247253, acc: 0.9572192430496216)
[2025-02-13 19:41:32,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:32,788][root][INFO] - Training Epoch: 1/2, step 2906/7134 completed (loss: 0.14876680076122284, acc: 0.9738219976425171)
[2025-02-13 19:41:32,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:33,161][root][INFO] - Training Epoch: 1/2, step 2907/7134 completed (loss: 0.23709797859191895, acc: 0.9281437397003174)
[2025-02-13 19:41:33,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:33,544][root][INFO] - Training Epoch: 1/2, step 2908/7134 completed (loss: 0.16657765209674835, acc: 0.9712643623352051)
[2025-02-13 19:41:33,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:33,920][root][INFO] - Training Epoch: 1/2, step 2909/7134 completed (loss: 0.09596865624189377, acc: 0.9890710115432739)
[2025-02-13 19:41:34,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:34,274][root][INFO] - Training Epoch: 1/2, step 2910/7134 completed (loss: 0.17732997238636017, acc: 0.9625668525695801)
[2025-02-13 19:41:34,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:34,663][root][INFO] - Training Epoch: 1/2, step 2911/7134 completed (loss: 0.1446380466222763, acc: 0.9580419659614563)
[2025-02-13 19:41:34,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:35,050][root][INFO] - Training Epoch: 1/2, step 2912/7134 completed (loss: 0.11243714392185211, acc: 0.965753436088562)
[2025-02-13 19:41:35,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:35,476][root][INFO] - Training Epoch: 1/2, step 2913/7134 completed (loss: 0.23509864509105682, acc: 0.9568345546722412)
[2025-02-13 19:41:35,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:35,902][root][INFO] - Training Epoch: 1/2, step 2914/7134 completed (loss: 0.17281432449817657, acc: 0.9575757384300232)
[2025-02-13 19:41:36,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:36,301][root][INFO] - Training Epoch: 1/2, step 2915/7134 completed (loss: 0.5020059943199158, acc: 0.8965517282485962)
[2025-02-13 19:41:36,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:36,660][root][INFO] - Training Epoch: 1/2, step 2916/7134 completed (loss: 0.2196589857339859, acc: 0.9298245906829834)
[2025-02-13 19:41:36,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:37,015][root][INFO] - Training Epoch: 1/2, step 2917/7134 completed (loss: 0.1387077271938324, acc: 0.9736841917037964)
[2025-02-13 19:41:37,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:37,369][root][INFO] - Training Epoch: 1/2, step 2918/7134 completed (loss: 0.21691934764385223, acc: 0.9415204524993896)
[2025-02-13 19:41:37,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:37,730][root][INFO] - Training Epoch: 1/2, step 2919/7134 completed (loss: 0.15621626377105713, acc: 0.9578313231468201)
[2025-02-13 19:41:37,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:38,118][root][INFO] - Training Epoch: 1/2, step 2920/7134 completed (loss: 0.22269809246063232, acc: 0.955974817276001)
[2025-02-13 19:41:38,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:38,482][root][INFO] - Training Epoch: 1/2, step 2921/7134 completed (loss: 0.18111221492290497, acc: 0.9357143044471741)
[2025-02-13 19:41:38,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:38,836][root][INFO] - Training Epoch: 1/2, step 2922/7134 completed (loss: 0.3599544167518616, acc: 0.9202454090118408)
[2025-02-13 19:41:38,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:39,202][root][INFO] - Training Epoch: 1/2, step 2923/7134 completed (loss: 0.7731761932373047, acc: 0.8802816867828369)
[2025-02-13 19:41:39,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:39,566][root][INFO] - Training Epoch: 1/2, step 2924/7134 completed (loss: 0.2557187080383301, acc: 0.9390243887901306)
[2025-02-13 19:41:39,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:39,948][root][INFO] - Training Epoch: 1/2, step 2925/7134 completed (loss: 0.15234225988388062, acc: 0.9647058844566345)
[2025-02-13 19:41:40,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:40,329][root][INFO] - Training Epoch: 1/2, step 2926/7134 completed (loss: 0.42342710494995117, acc: 0.9195979833602905)
[2025-02-13 19:41:40,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:40,683][root][INFO] - Training Epoch: 1/2, step 2927/7134 completed (loss: 0.15826596319675446, acc: 0.9537572264671326)
[2025-02-13 19:41:40,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:41,045][root][INFO] - Training Epoch: 1/2, step 2928/7134 completed (loss: 0.2491055577993393, acc: 0.9513888955116272)
[2025-02-13 19:41:41,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:41,433][root][INFO] - Training Epoch: 1/2, step 2929/7134 completed (loss: 0.11213003098964691, acc: 0.976190447807312)
[2025-02-13 19:41:41,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:41,801][root][INFO] - Training Epoch: 1/2, step 2930/7134 completed (loss: 0.2584703266620636, acc: 0.9455782175064087)
[2025-02-13 19:41:41,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:42,159][root][INFO] - Training Epoch: 1/2, step 2931/7134 completed (loss: 0.4498836398124695, acc: 0.8910256624221802)
[2025-02-13 19:41:42,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:42,501][root][INFO] - Training Epoch: 1/2, step 2932/7134 completed (loss: 0.18561653792858124, acc: 0.9625668525695801)
[2025-02-13 19:41:42,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:42,870][root][INFO] - Training Epoch: 1/2, step 2933/7134 completed (loss: 0.2100064605474472, acc: 0.9416058659553528)
[2025-02-13 19:41:43,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:43,255][root][INFO] - Training Epoch: 1/2, step 2934/7134 completed (loss: 0.08170115947723389, acc: 0.9928571581840515)
[2025-02-13 19:41:43,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:43,618][root][INFO] - Training Epoch: 1/2, step 2935/7134 completed (loss: 0.08440790325403214, acc: 0.9788359999656677)
[2025-02-13 19:41:43,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:43,996][root][INFO] - Training Epoch: 1/2, step 2936/7134 completed (loss: 0.2370678037405014, acc: 0.9469026327133179)
[2025-02-13 19:41:44,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:44,348][root][INFO] - Training Epoch: 1/2, step 2937/7134 completed (loss: 0.23684841394424438, acc: 0.9411764740943909)
[2025-02-13 19:41:44,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:44,689][root][INFO] - Training Epoch: 1/2, step 2938/7134 completed (loss: 0.2535364031791687, acc: 0.9398906826972961)
[2025-02-13 19:41:44,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:45,031][root][INFO] - Training Epoch: 1/2, step 2939/7134 completed (loss: 0.12370074540376663, acc: 0.9801324605941772)
[2025-02-13 19:41:45,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:45,407][root][INFO] - Training Epoch: 1/2, step 2940/7134 completed (loss: 0.16873407363891602, acc: 0.9508196711540222)
[2025-02-13 19:41:45,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:45,759][root][INFO] - Training Epoch: 1/2, step 2941/7134 completed (loss: 0.20521393418312073, acc: 0.932584285736084)
[2025-02-13 19:41:45,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:46,074][root][INFO] - Training Epoch: 1/2, step 2942/7134 completed (loss: 0.15704993903636932, acc: 0.9696969985961914)
[2025-02-13 19:41:46,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:46,434][root][INFO] - Training Epoch: 1/2, step 2943/7134 completed (loss: 0.24198928475379944, acc: 0.9189189076423645)
[2025-02-13 19:41:46,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:46,798][root][INFO] - Training Epoch: 1/2, step 2944/7134 completed (loss: 0.16352777183055878, acc: 0.9457831382751465)
[2025-02-13 19:41:46,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:47,199][root][INFO] - Training Epoch: 1/2, step 2945/7134 completed (loss: 0.21551862359046936, acc: 0.9328858852386475)
[2025-02-13 19:41:47,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:47,551][root][INFO] - Training Epoch: 1/2, step 2946/7134 completed (loss: 0.27961450815200806, acc: 0.9444444179534912)
[2025-02-13 19:41:47,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:47,929][root][INFO] - Training Epoch: 1/2, step 2947/7134 completed (loss: 0.2824409306049347, acc: 0.9375)
[2025-02-13 19:41:48,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:48,327][root][INFO] - Training Epoch: 1/2, step 2948/7134 completed (loss: 0.3459377884864807, acc: 0.9027777910232544)
[2025-02-13 19:41:48,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:48,695][root][INFO] - Training Epoch: 1/2, step 2949/7134 completed (loss: 0.10517656058073044, acc: 0.9803921580314636)
[2025-02-13 19:41:48,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:49,060][root][INFO] - Training Epoch: 1/2, step 2950/7134 completed (loss: 0.1894155591726303, acc: 0.9497206807136536)
[2025-02-13 19:41:49,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:49,462][root][INFO] - Training Epoch: 1/2, step 2951/7134 completed (loss: 0.1608336716890335, acc: 0.9627329111099243)
[2025-02-13 19:41:49,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:49,856][root][INFO] - Training Epoch: 1/2, step 2952/7134 completed (loss: 0.09920526295900345, acc: 0.9681528806686401)
[2025-02-13 19:41:49,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:50,211][root][INFO] - Training Epoch: 1/2, step 2953/7134 completed (loss: 0.21392807364463806, acc: 0.9387755393981934)
[2025-02-13 19:41:50,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:50,629][root][INFO] - Training Epoch: 1/2, step 2954/7134 completed (loss: 0.21459347009658813, acc: 0.9484536051750183)
[2025-02-13 19:41:50,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:50,986][root][INFO] - Training Epoch: 1/2, step 2955/7134 completed (loss: 0.17890581488609314, acc: 0.9454545378684998)
[2025-02-13 19:41:51,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:51,412][root][INFO] - Training Epoch: 1/2, step 2956/7134 completed (loss: 0.2200249284505844, acc: 0.9447852969169617)
[2025-02-13 19:41:51,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:51,795][root][INFO] - Training Epoch: 1/2, step 2957/7134 completed (loss: 0.13535849750041962, acc: 0.959770143032074)
[2025-02-13 19:41:51,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:52,159][root][INFO] - Training Epoch: 1/2, step 2958/7134 completed (loss: 0.1399458646774292, acc: 0.9652777910232544)
[2025-02-13 19:41:52,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:52,513][root][INFO] - Training Epoch: 1/2, step 2959/7134 completed (loss: 0.12007249891757965, acc: 0.9746835231781006)
[2025-02-13 19:41:52,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:52,885][root][INFO] - Training Epoch: 1/2, step 2960/7134 completed (loss: 0.16907459497451782, acc: 0.9450549483299255)
[2025-02-13 19:41:53,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:53,254][root][INFO] - Training Epoch: 1/2, step 2961/7134 completed (loss: 0.14845815300941467, acc: 0.9520958065986633)
[2025-02-13 19:41:53,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:53,612][root][INFO] - Training Epoch: 1/2, step 2962/7134 completed (loss: 0.12004609405994415, acc: 0.976190447807312)
[2025-02-13 19:41:53,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:53,983][root][INFO] - Training Epoch: 1/2, step 2963/7134 completed (loss: 0.1584315001964569, acc: 0.9329608678817749)
[2025-02-13 19:41:54,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:54,377][root][INFO] - Training Epoch: 1/2, step 2964/7134 completed (loss: 0.08466893434524536, acc: 0.9800000190734863)
[2025-02-13 19:41:54,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:54,802][root][INFO] - Training Epoch: 1/2, step 2965/7134 completed (loss: 0.23309645056724548, acc: 0.9318181872367859)
[2025-02-13 19:41:54,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:55,167][root][INFO] - Training Epoch: 1/2, step 2966/7134 completed (loss: 0.04446036368608475, acc: 0.9914529919624329)
[2025-02-13 19:41:55,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:55,524][root][INFO] - Training Epoch: 1/2, step 2967/7134 completed (loss: 0.3827846050262451, acc: 0.9071038365364075)
[2025-02-13 19:41:55,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:55,893][root][INFO] - Training Epoch: 1/2, step 2968/7134 completed (loss: 0.6142817735671997, acc: 0.862500011920929)
[2025-02-13 19:41:56,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:56,250][root][INFO] - Training Epoch: 1/2, step 2969/7134 completed (loss: 1.3382750749588013, acc: 0.7864077687263489)
[2025-02-13 19:41:56,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:56,595][root][INFO] - Training Epoch: 1/2, step 2970/7134 completed (loss: 1.480136513710022, acc: 0.7238805890083313)
[2025-02-13 19:41:56,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:56,957][root][INFO] - Training Epoch: 1/2, step 2971/7134 completed (loss: 0.3109733760356903, acc: 0.9447513818740845)
[2025-02-13 19:41:57,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:57,321][root][INFO] - Training Epoch: 1/2, step 2972/7134 completed (loss: 0.37156954407691956, acc: 0.9133333563804626)
[2025-02-13 19:41:57,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:57,683][root][INFO] - Training Epoch: 1/2, step 2973/7134 completed (loss: 0.19729362428188324, acc: 0.9454545378684998)
[2025-02-13 19:41:57,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:58,047][root][INFO] - Training Epoch: 1/2, step 2974/7134 completed (loss: 0.3153662085533142, acc: 0.9122806787490845)
[2025-02-13 19:41:58,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:58,458][root][INFO] - Training Epoch: 1/2, step 2975/7134 completed (loss: 0.22609995305538177, acc: 0.9418604373931885)
[2025-02-13 19:41:58,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:58,857][root][INFO] - Training Epoch: 1/2, step 2976/7134 completed (loss: 0.20003844797611237, acc: 0.9528301954269409)
[2025-02-13 19:41:58,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:59,272][root][INFO] - Training Epoch: 1/2, step 2977/7134 completed (loss: 0.23960758745670319, acc: 0.951724112033844)
[2025-02-13 19:41:59,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:59,647][root][INFO] - Training Epoch: 1/2, step 2978/7134 completed (loss: 0.26727351546287537, acc: 0.9457831382751465)
[2025-02-13 19:41:59,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:00,006][root][INFO] - Training Epoch: 1/2, step 2979/7134 completed (loss: 0.2398006021976471, acc: 0.9448275566101074)
[2025-02-13 19:42:00,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:00,391][root][INFO] - Training Epoch: 1/2, step 2980/7134 completed (loss: 0.31942135095596313, acc: 0.9142857193946838)
[2025-02-13 19:42:00,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:00,752][root][INFO] - Training Epoch: 1/2, step 2981/7134 completed (loss: 0.3310401439666748, acc: 0.9195402264595032)
[2025-02-13 19:42:00,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:01,112][root][INFO] - Training Epoch: 1/2, step 2982/7134 completed (loss: 0.4253619611263275, acc: 0.906593382358551)
[2025-02-13 19:42:01,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:01,483][root][INFO] - Training Epoch: 1/2, step 2983/7134 completed (loss: 0.2885776460170746, acc: 0.9202454090118408)
[2025-02-13 19:42:01,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:01,890][root][INFO] - Training Epoch: 1/2, step 2984/7134 completed (loss: 0.34459835290908813, acc: 0.9395604133605957)
[2025-02-13 19:42:02,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:02,323][root][INFO] - Training Epoch: 1/2, step 2985/7134 completed (loss: 0.14896029233932495, acc: 0.9658536314964294)
[2025-02-13 19:42:02,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:02,703][root][INFO] - Training Epoch: 1/2, step 2986/7134 completed (loss: 0.19162854552268982, acc: 0.9536082744598389)
[2025-02-13 19:42:02,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:03,132][root][INFO] - Training Epoch: 1/2, step 2987/7134 completed (loss: 0.2361535131931305, acc: 0.95652174949646)
[2025-02-13 19:42:03,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:03,511][root][INFO] - Training Epoch: 1/2, step 2988/7134 completed (loss: 0.22841013967990875, acc: 0.9513513445854187)
[2025-02-13 19:42:03,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:03,927][root][INFO] - Training Epoch: 1/2, step 2989/7134 completed (loss: 0.22921599447727203, acc: 0.9281437397003174)
[2025-02-13 19:42:04,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:04,289][root][INFO] - Training Epoch: 1/2, step 2990/7134 completed (loss: 0.4784749150276184, acc: 0.8918918967247009)
[2025-02-13 19:42:04,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:04,658][root][INFO] - Training Epoch: 1/2, step 2991/7134 completed (loss: 0.8073005676269531, acc: 0.8229166865348816)
[2025-02-13 19:42:04,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:05,020][root][INFO] - Training Epoch: 1/2, step 2992/7134 completed (loss: 0.4000648856163025, acc: 0.9371069073677063)
[2025-02-13 19:42:05,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:05,388][root][INFO] - Training Epoch: 1/2, step 2993/7134 completed (loss: 0.18425938487052917, acc: 0.9447236061096191)
[2025-02-13 19:42:05,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:05,746][root][INFO] - Training Epoch: 1/2, step 2994/7134 completed (loss: 0.1630207896232605, acc: 0.9365079402923584)
[2025-02-13 19:42:05,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:06,132][root][INFO] - Training Epoch: 1/2, step 2995/7134 completed (loss: 0.19054213166236877, acc: 0.9417475461959839)
[2025-02-13 19:42:06,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:06,487][root][INFO] - Training Epoch: 1/2, step 2996/7134 completed (loss: 0.16363677382469177, acc: 0.9536082744598389)
[2025-02-13 19:42:06,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:06,866][root][INFO] - Training Epoch: 1/2, step 2997/7134 completed (loss: 0.1805257648229599, acc: 0.9528301954269409)
[2025-02-13 19:42:06,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:07,222][root][INFO] - Training Epoch: 1/2, step 2998/7134 completed (loss: 0.14462001621723175, acc: 0.976190447807312)
[2025-02-13 19:42:07,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:07,603][root][INFO] - Training Epoch: 1/2, step 2999/7134 completed (loss: 0.14155498147010803, acc: 0.96875)
[2025-02-13 19:42:07,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:07,979][root][INFO] - Training Epoch: 1/2, step 3000/7134 completed (loss: 0.331569641828537, acc: 0.9027777910232544)
[2025-02-13 19:42:08,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:08,386][root][INFO] - Training Epoch: 1/2, step 3001/7134 completed (loss: 0.20010212063789368, acc: 0.9593908786773682)
[2025-02-13 19:42:08,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:08,776][root][INFO] - Training Epoch: 1/2, step 3002/7134 completed (loss: 0.2675634026527405, acc: 0.9539170265197754)
[2025-02-13 19:42:08,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:09,144][root][INFO] - Training Epoch: 1/2, step 3003/7134 completed (loss: 0.06356091052293777, acc: 0.9847715497016907)
[2025-02-13 19:42:09,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:09,509][root][INFO] - Training Epoch: 1/2, step 3004/7134 completed (loss: 0.0667528361082077, acc: 0.9851484894752502)
[2025-02-13 19:42:09,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:09,889][root][INFO] - Training Epoch: 1/2, step 3005/7134 completed (loss: 0.04787752404808998, acc: 0.984455943107605)
[2025-02-13 19:42:10,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:10,263][root][INFO] - Training Epoch: 1/2, step 3006/7134 completed (loss: 0.0458509624004364, acc: 0.9828571677207947)
[2025-02-13 19:42:10,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:10,651][root][INFO] - Training Epoch: 1/2, step 3007/7134 completed (loss: 0.22319085896015167, acc: 0.9476743936538696)
[2025-02-13 19:42:10,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:11,050][root][INFO] - Training Epoch: 1/2, step 3008/7134 completed (loss: 0.2513478994369507, acc: 0.9387755393981934)
[2025-02-13 19:42:11,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:11,401][root][INFO] - Training Epoch: 1/2, step 3009/7134 completed (loss: 0.2843201756477356, acc: 0.9345238208770752)
[2025-02-13 19:42:11,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:11,760][root][INFO] - Training Epoch: 1/2, step 3010/7134 completed (loss: 0.21483437716960907, acc: 0.9657142758369446)
[2025-02-13 19:42:11,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:12,136][root][INFO] - Training Epoch: 1/2, step 3011/7134 completed (loss: 0.6674959063529968, acc: 0.8484848737716675)
[2025-02-13 19:42:12,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:12,518][root][INFO] - Training Epoch: 1/2, step 3012/7134 completed (loss: 0.465423047542572, acc: 0.8848921060562134)
[2025-02-13 19:42:12,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:12,875][root][INFO] - Training Epoch: 1/2, step 3013/7134 completed (loss: 0.04589279741048813, acc: 1.0)
[2025-02-13 19:42:13,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:13,241][root][INFO] - Training Epoch: 1/2, step 3014/7134 completed (loss: 0.12715403735637665, acc: 0.9746835231781006)
[2025-02-13 19:42:13,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:13,582][root][INFO] - Training Epoch: 1/2, step 3015/7134 completed (loss: 0.12343813478946686, acc: 0.9647887349128723)
[2025-02-13 19:42:13,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:13,950][root][INFO] - Training Epoch: 1/2, step 3016/7134 completed (loss: 0.12379112094640732, acc: 0.9726027250289917)
[2025-02-13 19:42:14,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:14,316][root][INFO] - Training Epoch: 1/2, step 3017/7134 completed (loss: 0.08709544688463211, acc: 0.9752066135406494)
[2025-02-13 19:42:14,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:14,706][root][INFO] - Training Epoch: 1/2, step 3018/7134 completed (loss: 0.1293802261352539, acc: 0.9694656729698181)
[2025-02-13 19:42:14,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:15,106][root][INFO] - Training Epoch: 1/2, step 3019/7134 completed (loss: 0.15088224411010742, acc: 0.9779411554336548)
[2025-02-13 19:42:15,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:15,481][root][INFO] - Training Epoch: 1/2, step 3020/7134 completed (loss: 0.18756544589996338, acc: 0.9496402740478516)
[2025-02-13 19:42:15,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:15,892][root][INFO] - Training Epoch: 1/2, step 3021/7134 completed (loss: 0.15804703533649445, acc: 0.9545454382896423)
[2025-02-13 19:42:16,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:16,271][root][INFO] - Training Epoch: 1/2, step 3022/7134 completed (loss: 0.21043790876865387, acc: 0.9624060392379761)
[2025-02-13 19:42:16,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:16,625][root][INFO] - Training Epoch: 1/2, step 3023/7134 completed (loss: 0.3126692771911621, acc: 0.9153845906257629)
[2025-02-13 19:42:16,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:17,011][root][INFO] - Training Epoch: 1/2, step 3024/7134 completed (loss: 0.11066946387290955, acc: 0.9750000238418579)
[2025-02-13 19:42:17,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:17,377][root][INFO] - Training Epoch: 1/2, step 3025/7134 completed (loss: 0.3703567385673523, acc: 0.9307692050933838)
[2025-02-13 19:42:17,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:17,736][root][INFO] - Training Epoch: 1/2, step 3026/7134 completed (loss: 0.14875754714012146, acc: 0.9754098653793335)
[2025-02-13 19:42:17,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:18,094][root][INFO] - Training Epoch: 1/2, step 3027/7134 completed (loss: 0.26415714621543884, acc: 0.9479166865348816)
[2025-02-13 19:42:18,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:18,435][root][INFO] - Training Epoch: 1/2, step 3028/7134 completed (loss: 0.10983230918645859, acc: 0.969072163105011)
[2025-02-13 19:42:18,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:18,791][root][INFO] - Training Epoch: 1/2, step 3029/7134 completed (loss: 0.2339378297328949, acc: 0.9520547986030579)
[2025-02-13 19:42:18,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:19,174][root][INFO] - Training Epoch: 1/2, step 3030/7134 completed (loss: 0.12966914474964142, acc: 0.9729729890823364)
[2025-02-13 19:42:19,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:19,538][root][INFO] - Training Epoch: 1/2, step 3031/7134 completed (loss: 0.09021160751581192, acc: 0.9797979593276978)
[2025-02-13 19:42:19,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:19,923][root][INFO] - Training Epoch: 1/2, step 3032/7134 completed (loss: 0.23218469321727753, acc: 0.9274193644523621)
[2025-02-13 19:42:20,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:20,288][root][INFO] - Training Epoch: 1/2, step 3033/7134 completed (loss: 0.1528266966342926, acc: 0.96875)
[2025-02-13 19:42:20,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:20,660][root][INFO] - Training Epoch: 1/2, step 3034/7134 completed (loss: 0.1697441190481186, acc: 0.9459459185600281)
[2025-02-13 19:42:20,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:21,019][root][INFO] - Training Epoch: 1/2, step 3035/7134 completed (loss: 0.1520978957414627, acc: 0.9455782175064087)
[2025-02-13 19:42:21,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:21,386][root][INFO] - Training Epoch: 1/2, step 3036/7134 completed (loss: 0.2465507537126541, acc: 0.9440000057220459)
[2025-02-13 19:42:21,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:21,768][root][INFO] - Training Epoch: 1/2, step 3037/7134 completed (loss: 0.16551290452480316, acc: 0.9395973086357117)
[2025-02-13 19:42:21,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:22,145][root][INFO] - Training Epoch: 1/2, step 3038/7134 completed (loss: 0.16099204123020172, acc: 0.9701492786407471)
[2025-02-13 19:42:22,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:22,511][root][INFO] - Training Epoch: 1/2, step 3039/7134 completed (loss: 0.18041378259658813, acc: 0.9729729890823364)
[2025-02-13 19:42:22,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:22,881][root][INFO] - Training Epoch: 1/2, step 3040/7134 completed (loss: 0.20637862384319305, acc: 0.9224806427955627)
[2025-02-13 19:42:23,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:23,283][root][INFO] - Training Epoch: 1/2, step 3041/7134 completed (loss: 0.3331248164176941, acc: 0.9285714030265808)
[2025-02-13 19:42:23,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:23,657][root][INFO] - Training Epoch: 1/2, step 3042/7134 completed (loss: 0.13781973719596863, acc: 0.9551281929016113)
[2025-02-13 19:42:23,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:24,031][root][INFO] - Training Epoch: 1/2, step 3043/7134 completed (loss: 0.09080108255147934, acc: 0.9642857313156128)
[2025-02-13 19:42:24,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:24,386][root][INFO] - Training Epoch: 1/2, step 3044/7134 completed (loss: 0.17976486682891846, acc: 0.9624060392379761)
[2025-02-13 19:42:24,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:24,754][root][INFO] - Training Epoch: 1/2, step 3045/7134 completed (loss: 0.08384396135807037, acc: 0.9844961166381836)
[2025-02-13 19:42:24,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:25,124][root][INFO] - Training Epoch: 1/2, step 3046/7134 completed (loss: 0.13861478865146637, acc: 0.9585798978805542)
[2025-02-13 19:42:25,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:25,481][root][INFO] - Training Epoch: 1/2, step 3047/7134 completed (loss: 0.18121935427188873, acc: 0.9548022747039795)
[2025-02-13 19:42:25,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:25,900][root][INFO] - Training Epoch: 1/2, step 3048/7134 completed (loss: 0.2428968995809555, acc: 0.9210526347160339)
[2025-02-13 19:42:26,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:26,254][root][INFO] - Training Epoch: 1/2, step 3049/7134 completed (loss: 0.23341035842895508, acc: 0.9375)
[2025-02-13 19:42:26,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:26,619][root][INFO] - Training Epoch: 1/2, step 3050/7134 completed (loss: 0.2623198926448822, acc: 0.9340101480484009)
[2025-02-13 19:42:26,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:27,008][root][INFO] - Training Epoch: 1/2, step 3051/7134 completed (loss: 0.2897418439388275, acc: 0.9107142686843872)
[2025-02-13 19:42:27,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:27,430][root][INFO] - Training Epoch: 1/2, step 3052/7134 completed (loss: 0.18097969889640808, acc: 0.9621621370315552)
[2025-02-13 19:42:27,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:27,854][root][INFO] - Training Epoch: 1/2, step 3053/7134 completed (loss: 0.11320096254348755, acc: 0.9800994992256165)
[2025-02-13 19:42:27,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:28,308][root][INFO] - Training Epoch: 1/2, step 3054/7134 completed (loss: 0.0789744183421135, acc: 0.978723406791687)
[2025-02-13 19:42:28,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:28,681][root][INFO] - Training Epoch: 1/2, step 3055/7134 completed (loss: 0.1694759577512741, acc: 0.9594594836235046)
[2025-02-13 19:42:28,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:29,113][root][INFO] - Training Epoch: 1/2, step 3056/7134 completed (loss: 0.11323829740285873, acc: 0.9675675630569458)
[2025-02-13 19:42:29,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:29,535][root][INFO] - Training Epoch: 1/2, step 3057/7134 completed (loss: 0.13162407279014587, acc: 0.9693251252174377)
[2025-02-13 19:42:29,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:29,924][root][INFO] - Training Epoch: 1/2, step 3058/7134 completed (loss: 0.15722568333148956, acc: 0.9684210419654846)
[2025-02-13 19:42:30,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:30,310][root][INFO] - Training Epoch: 1/2, step 3059/7134 completed (loss: 0.049682993441820145, acc: 0.9885714054107666)
[2025-02-13 19:42:30,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:30,697][root][INFO] - Training Epoch: 1/2, step 3060/7134 completed (loss: 0.15894421935081482, acc: 0.963350772857666)
[2025-02-13 19:42:30,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:31,123][root][INFO] - Training Epoch: 1/2, step 3061/7134 completed (loss: 0.09638109803199768, acc: 0.9750000238418579)
[2025-02-13 19:42:31,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:31,546][root][INFO] - Training Epoch: 1/2, step 3062/7134 completed (loss: 0.09526049345731735, acc: 0.9811320900917053)
[2025-02-13 19:42:31,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:31,928][root][INFO] - Training Epoch: 1/2, step 3063/7134 completed (loss: 0.17411670088768005, acc: 0.9431818127632141)
[2025-02-13 19:42:32,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:32,354][root][INFO] - Training Epoch: 1/2, step 3064/7134 completed (loss: 0.07009908556938171, acc: 0.9940828680992126)
[2025-02-13 19:42:32,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:32,738][root][INFO] - Training Epoch: 1/2, step 3065/7134 completed (loss: 0.04411676898598671, acc: 0.9887640476226807)
[2025-02-13 19:42:32,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:33,159][root][INFO] - Training Epoch: 1/2, step 3066/7134 completed (loss: 0.07212550193071365, acc: 0.9933775067329407)
[2025-02-13 19:42:33,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:33,552][root][INFO] - Training Epoch: 1/2, step 3067/7134 completed (loss: 0.11694884300231934, acc: 0.9632353186607361)
[2025-02-13 19:42:33,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:33,937][root][INFO] - Training Epoch: 1/2, step 3068/7134 completed (loss: 0.19747470319271088, acc: 0.9497206807136536)
[2025-02-13 19:42:34,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:34,301][root][INFO] - Training Epoch: 1/2, step 3069/7134 completed (loss: 0.02186655066907406, acc: 1.0)
[2025-02-13 19:42:34,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:34,733][root][INFO] - Training Epoch: 1/2, step 3070/7134 completed (loss: 0.030252132564783096, acc: 1.0)
[2025-02-13 19:42:34,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:35,105][root][INFO] - Training Epoch: 1/2, step 3071/7134 completed (loss: 0.13697797060012817, acc: 0.9777777791023254)
[2025-02-13 19:42:35,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:35,485][root][INFO] - Training Epoch: 1/2, step 3072/7134 completed (loss: 0.05223170295357704, acc: 0.98591548204422)
[2025-02-13 19:42:35,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:35,830][root][INFO] - Training Epoch: 1/2, step 3073/7134 completed (loss: 0.11830728501081467, acc: 0.9774011373519897)
[2025-02-13 19:42:35,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:36,187][root][INFO] - Training Epoch: 1/2, step 3074/7134 completed (loss: 0.034744396805763245, acc: 0.9933775067329407)
[2025-02-13 19:42:36,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:36,534][root][INFO] - Training Epoch: 1/2, step 3075/7134 completed (loss: 0.11774343252182007, acc: 0.9691358208656311)
[2025-02-13 19:42:36,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:36,899][root][INFO] - Training Epoch: 1/2, step 3076/7134 completed (loss: 0.056762080639600754, acc: 0.9892473220825195)
[2025-02-13 19:42:37,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:37,278][root][INFO] - Training Epoch: 1/2, step 3077/7134 completed (loss: 0.28005972504615784, acc: 0.9371727705001831)
[2025-02-13 19:42:37,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:37,652][root][INFO] - Training Epoch: 1/2, step 3078/7134 completed (loss: 0.14980117976665497, acc: 0.9635416865348816)
[2025-02-13 19:42:37,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:38,026][root][INFO] - Training Epoch: 1/2, step 3079/7134 completed (loss: 0.23080192506313324, acc: 0.9576719403266907)
[2025-02-13 19:42:38,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:38,409][root][INFO] - Training Epoch: 1/2, step 3080/7134 completed (loss: 0.30366280674934387, acc: 0.9351851940155029)
[2025-02-13 19:42:38,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:38,746][root][INFO] - Training Epoch: 1/2, step 3081/7134 completed (loss: 0.2119721919298172, acc: 0.9504950642585754)
[2025-02-13 19:42:38,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:39,100][root][INFO] - Training Epoch: 1/2, step 3082/7134 completed (loss: 0.3632192611694336, acc: 0.9054726362228394)
[2025-02-13 19:42:39,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:39,467][root][INFO] - Training Epoch: 1/2, step 3083/7134 completed (loss: 0.18096908926963806, acc: 0.9459459185600281)
[2025-02-13 19:42:39,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:39,822][root][INFO] - Training Epoch: 1/2, step 3084/7134 completed (loss: 0.3033299744129181, acc: 0.9162303805351257)
[2025-02-13 19:42:39,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:40,205][root][INFO] - Training Epoch: 1/2, step 3085/7134 completed (loss: 0.2780536413192749, acc: 0.9290322661399841)
[2025-02-13 19:42:40,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:40,583][root][INFO] - Training Epoch: 1/2, step 3086/7134 completed (loss: 0.2591312825679779, acc: 0.9452054500579834)
[2025-02-13 19:42:40,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:40,953][root][INFO] - Training Epoch: 1/2, step 3087/7134 completed (loss: 0.16644056141376495, acc: 0.9408283829689026)
[2025-02-13 19:42:41,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:41,357][root][INFO] - Training Epoch: 1/2, step 3088/7134 completed (loss: 0.2238394170999527, acc: 0.9158878326416016)
[2025-02-13 19:42:41,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:41,763][root][INFO] - Training Epoch: 1/2, step 3089/7134 completed (loss: 0.17348329722881317, acc: 0.9581395387649536)
[2025-02-13 19:42:41,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:42,138][root][INFO] - Training Epoch: 1/2, step 3090/7134 completed (loss: 0.5017053484916687, acc: 0.8926553726196289)
[2025-02-13 19:42:42,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:42,531][root][INFO] - Training Epoch: 1/2, step 3091/7134 completed (loss: 0.5871865153312683, acc: 0.8497853875160217)
[2025-02-13 19:42:42,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:42,899][root][INFO] - Training Epoch: 1/2, step 3092/7134 completed (loss: 0.3807143270969391, acc: 0.9083969593048096)
[2025-02-13 19:42:43,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:43,292][root][INFO] - Training Epoch: 1/2, step 3093/7134 completed (loss: 0.23619475960731506, acc: 0.9436619877815247)
[2025-02-13 19:42:43,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:43,666][root][INFO] - Training Epoch: 1/2, step 3094/7134 completed (loss: 0.09248194843530655, acc: 0.9826589822769165)
[2025-02-13 19:42:43,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:44,013][root][INFO] - Training Epoch: 1/2, step 3095/7134 completed (loss: 0.2288118153810501, acc: 0.9555555582046509)
[2025-02-13 19:42:44,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:44,376][root][INFO] - Training Epoch: 1/2, step 3096/7134 completed (loss: 0.2251257300376892, acc: 0.9424083828926086)
[2025-02-13 19:42:44,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:44,754][root][INFO] - Training Epoch: 1/2, step 3097/7134 completed (loss: 0.28280773758888245, acc: 0.9207317233085632)
[2025-02-13 19:42:44,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:45,123][root][INFO] - Training Epoch: 1/2, step 3098/7134 completed (loss: 0.18879945576190948, acc: 0.9607843160629272)
[2025-02-13 19:42:45,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:45,494][root][INFO] - Training Epoch: 1/2, step 3099/7134 completed (loss: 0.20748229324817657, acc: 0.9416058659553528)
[2025-02-13 19:42:45,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:45,848][root][INFO] - Training Epoch: 1/2, step 3100/7134 completed (loss: 0.1480158269405365, acc: 0.9551569223403931)
[2025-02-13 19:42:45,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:46,210][root][INFO] - Training Epoch: 1/2, step 3101/7134 completed (loss: 0.19582611322402954, acc: 0.9390863180160522)
[2025-02-13 19:42:46,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:46,579][root][INFO] - Training Epoch: 1/2, step 3102/7134 completed (loss: 0.1126408576965332, acc: 0.987730085849762)
[2025-02-13 19:42:46,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:46,941][root][INFO] - Training Epoch: 1/2, step 3103/7134 completed (loss: 0.14855961501598358, acc: 0.9741379022598267)
[2025-02-13 19:42:47,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:47,284][root][INFO] - Training Epoch: 1/2, step 3104/7134 completed (loss: 0.08481895178556442, acc: 0.9767441749572754)
[2025-02-13 19:42:47,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:47,652][root][INFO] - Training Epoch: 1/2, step 3105/7134 completed (loss: 0.25673234462738037, acc: 0.9434782862663269)
[2025-02-13 19:42:47,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:48,048][root][INFO] - Training Epoch: 1/2, step 3106/7134 completed (loss: 0.13504301011562347, acc: 0.9597315192222595)
[2025-02-13 19:42:48,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:48,408][root][INFO] - Training Epoch: 1/2, step 3107/7134 completed (loss: 0.08213269710540771, acc: 0.9825581312179565)
[2025-02-13 19:42:48,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:48,773][root][INFO] - Training Epoch: 1/2, step 3108/7134 completed (loss: 0.1425846666097641, acc: 0.9795918464660645)
[2025-02-13 19:42:48,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:49,144][root][INFO] - Training Epoch: 1/2, step 3109/7134 completed (loss: 0.2779468894004822, acc: 0.9230769276618958)
[2025-02-13 19:42:49,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:49,516][root][INFO] - Training Epoch: 1/2, step 3110/7134 completed (loss: 0.15934552252292633, acc: 0.9514563083648682)
[2025-02-13 19:42:49,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:49,889][root][INFO] - Training Epoch: 1/2, step 3111/7134 completed (loss: 0.510165810585022, acc: 0.8623188138008118)
[2025-02-13 19:42:50,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:50,246][root][INFO] - Training Epoch: 1/2, step 3112/7134 completed (loss: 0.3678196668624878, acc: 0.9124087691307068)
[2025-02-13 19:42:50,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:50,636][root][INFO] - Training Epoch: 1/2, step 3113/7134 completed (loss: 0.20239941775798798, acc: 0.9512194991111755)
[2025-02-13 19:42:50,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:50,999][root][INFO] - Training Epoch: 1/2, step 3114/7134 completed (loss: 0.15974491834640503, acc: 0.9624060392379761)
[2025-02-13 19:42:51,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:51,353][root][INFO] - Training Epoch: 1/2, step 3115/7134 completed (loss: 0.17733296751976013, acc: 0.9736841917037964)
[2025-02-13 19:42:51,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:51,722][root][INFO] - Training Epoch: 1/2, step 3116/7134 completed (loss: 0.2622120976448059, acc: 0.9491525292396545)
[2025-02-13 19:42:51,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:52,084][root][INFO] - Training Epoch: 1/2, step 3117/7134 completed (loss: 0.2633455991744995, acc: 0.9438202381134033)
[2025-02-13 19:42:52,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:52,443][root][INFO] - Training Epoch: 1/2, step 3118/7134 completed (loss: 0.24460239708423615, acc: 0.9337016344070435)
[2025-02-13 19:42:52,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:52,805][root][INFO] - Training Epoch: 1/2, step 3119/7134 completed (loss: 0.20691244304180145, acc: 0.9576719403266907)
[2025-02-13 19:42:52,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:53,150][root][INFO] - Training Epoch: 1/2, step 3120/7134 completed (loss: 0.3233526051044464, acc: 0.9506173133850098)
[2025-02-13 19:42:53,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:53,511][root][INFO] - Training Epoch: 1/2, step 3121/7134 completed (loss: 0.1644454151391983, acc: 0.9659090638160706)
[2025-02-13 19:42:53,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:53,894][root][INFO] - Training Epoch: 1/2, step 3122/7134 completed (loss: 0.3443126380443573, acc: 0.9263157844543457)
[2025-02-13 19:42:54,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:54,271][root][INFO] - Training Epoch: 1/2, step 3123/7134 completed (loss: 0.29231002926826477, acc: 0.9135135412216187)
[2025-02-13 19:42:54,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:54,691][root][INFO] - Training Epoch: 1/2, step 3124/7134 completed (loss: 0.24130405485630035, acc: 0.9388889074325562)
[2025-02-13 19:42:54,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:55,087][root][INFO] - Training Epoch: 1/2, step 3125/7134 completed (loss: 0.11531390994787216, acc: 0.9795918464660645)
[2025-02-13 19:42:55,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:55,479][root][INFO] - Training Epoch: 1/2, step 3126/7134 completed (loss: 0.2833247184753418, acc: 0.9329608678817749)
[2025-02-13 19:42:55,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:55,849][root][INFO] - Training Epoch: 1/2, step 3127/7134 completed (loss: 0.19525845348834991, acc: 0.9436619877815247)
[2025-02-13 19:42:55,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:56,220][root][INFO] - Training Epoch: 1/2, step 3128/7134 completed (loss: 0.17899377644062042, acc: 0.9572192430496216)
[2025-02-13 19:42:56,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:56,598][root][INFO] - Training Epoch: 1/2, step 3129/7134 completed (loss: 0.14936423301696777, acc: 0.9715909361839294)
[2025-02-13 19:42:56,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:56,994][root][INFO] - Training Epoch: 1/2, step 3130/7134 completed (loss: 0.10536894202232361, acc: 0.9847715497016907)
[2025-02-13 19:42:57,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:57,376][root][INFO] - Training Epoch: 1/2, step 3131/7134 completed (loss: 0.2431706339120865, acc: 0.9435028433799744)
[2025-02-13 19:42:57,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:57,730][root][INFO] - Training Epoch: 1/2, step 3132/7134 completed (loss: 0.13136419653892517, acc: 0.9469026327133179)
[2025-02-13 19:42:57,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:58,105][root][INFO] - Training Epoch: 1/2, step 3133/7134 completed (loss: 0.31470128893852234, acc: 0.9488636255264282)
[2025-02-13 19:42:58,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:58,530][root][INFO] - Training Epoch: 1/2, step 3134/7134 completed (loss: 0.2140807807445526, acc: 0.9453125)
[2025-02-13 19:42:58,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:58,949][root][INFO] - Training Epoch: 1/2, step 3135/7134 completed (loss: 0.32377636432647705, acc: 0.9175257682800293)
[2025-02-13 19:42:59,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:59,312][root][INFO] - Training Epoch: 1/2, step 3136/7134 completed (loss: 0.22914592921733856, acc: 0.9312977194786072)
[2025-02-13 19:42:59,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:59,677][root][INFO] - Training Epoch: 1/2, step 3137/7134 completed (loss: 0.33670279383659363, acc: 0.9078013896942139)
[2025-02-13 19:42:59,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:00,057][root][INFO] - Training Epoch: 1/2, step 3138/7134 completed (loss: 0.3749336302280426, acc: 0.9333333373069763)
[2025-02-13 19:43:00,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:00,395][root][INFO] - Training Epoch: 1/2, step 3139/7134 completed (loss: 0.3445543348789215, acc: 0.9208633303642273)
[2025-02-13 19:43:00,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:00,737][root][INFO] - Training Epoch: 1/2, step 3140/7134 completed (loss: 0.16663812100887299, acc: 0.9589040875434875)
[2025-02-13 19:43:00,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:01,074][root][INFO] - Training Epoch: 1/2, step 3141/7134 completed (loss: 0.27517062425613403, acc: 0.9391891956329346)
[2025-02-13 19:43:01,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:01,408][root][INFO] - Training Epoch: 1/2, step 3142/7134 completed (loss: 0.09194055199623108, acc: 0.9880239367485046)
[2025-02-13 19:43:01,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:01,771][root][INFO] - Training Epoch: 1/2, step 3143/7134 completed (loss: 0.22585074603557587, acc: 0.948387086391449)
[2025-02-13 19:43:01,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:02,189][root][INFO] - Training Epoch: 1/2, step 3144/7134 completed (loss: 0.30947667360305786, acc: 0.9294871687889099)
[2025-02-13 19:43:02,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:02,564][root][INFO] - Training Epoch: 1/2, step 3145/7134 completed (loss: 0.26424187421798706, acc: 0.9369369149208069)
[2025-02-13 19:43:02,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:02,961][root][INFO] - Training Epoch: 1/2, step 3146/7134 completed (loss: 0.3174569010734558, acc: 0.9242424368858337)
[2025-02-13 19:43:03,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:03,323][root][INFO] - Training Epoch: 1/2, step 3147/7134 completed (loss: 0.1286802887916565, acc: 0.9589040875434875)
[2025-02-13 19:43:03,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:03,684][root][INFO] - Training Epoch: 1/2, step 3148/7134 completed (loss: 0.12347674369812012, acc: 0.9608938694000244)
[2025-02-13 19:43:03,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:04,045][root][INFO] - Training Epoch: 1/2, step 3149/7134 completed (loss: 0.09016580879688263, acc: 0.9741935729980469)
[2025-02-13 19:43:04,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:04,409][root][INFO] - Training Epoch: 1/2, step 3150/7134 completed (loss: 1.0165932178497314, acc: 0.8071428537368774)
[2025-02-13 19:43:04,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:04,773][root][INFO] - Training Epoch: 1/2, step 3151/7134 completed (loss: 0.4642640948295593, acc: 0.9007633328437805)
[2025-02-13 19:43:04,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:05,163][root][INFO] - Training Epoch: 1/2, step 3152/7134 completed (loss: 0.2476755827665329, acc: 0.9436619877815247)
[2025-02-13 19:43:05,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:05,533][root][INFO] - Training Epoch: 1/2, step 3153/7134 completed (loss: 0.1942678987979889, acc: 0.949438214302063)
[2025-02-13 19:43:05,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:05,896][root][INFO] - Training Epoch: 1/2, step 3154/7134 completed (loss: 0.2971673011779785, acc: 0.932330846786499)
[2025-02-13 19:43:06,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:06,243][root][INFO] - Training Epoch: 1/2, step 3155/7134 completed (loss: 0.7438459992408752, acc: 0.8721804618835449)
[2025-02-13 19:43:06,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:06,620][root][INFO] - Training Epoch: 1/2, step 3156/7134 completed (loss: 0.42890664935112, acc: 0.9011628031730652)
[2025-02-13 19:43:06,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:06,996][root][INFO] - Training Epoch: 1/2, step 3157/7134 completed (loss: 0.26162752509117126, acc: 0.947826087474823)
[2025-02-13 19:43:07,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:07,341][root][INFO] - Training Epoch: 1/2, step 3158/7134 completed (loss: 0.16377823054790497, acc: 0.978723406791687)
[2025-02-13 19:43:07,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:07,706][root][INFO] - Training Epoch: 1/2, step 3159/7134 completed (loss: 0.25180742144584656, acc: 0.969072163105011)
[2025-02-13 19:43:07,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:08,078][root][INFO] - Training Epoch: 1/2, step 3160/7134 completed (loss: 0.12764866650104523, acc: 0.9743589758872986)
[2025-02-13 19:43:08,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:08,443][root][INFO] - Training Epoch: 1/2, step 3161/7134 completed (loss: 0.21534495055675507, acc: 0.9490445852279663)
[2025-02-13 19:43:08,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:08,805][root][INFO] - Training Epoch: 1/2, step 3162/7134 completed (loss: 0.13545645773410797, acc: 0.9589040875434875)
[2025-02-13 19:43:08,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:09,151][root][INFO] - Training Epoch: 1/2, step 3163/7134 completed (loss: 0.084332175552845, acc: 0.9672130942344666)
[2025-02-13 19:43:09,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:09,509][root][INFO] - Training Epoch: 1/2, step 3164/7134 completed (loss: 0.2495819330215454, acc: 0.9275362491607666)
[2025-02-13 19:43:09,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:09,876][root][INFO] - Training Epoch: 1/2, step 3165/7134 completed (loss: 0.23402775824069977, acc: 0.9440559148788452)
[2025-02-13 19:43:10,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:10,234][root][INFO] - Training Epoch: 1/2, step 3166/7134 completed (loss: 0.40146759152412415, acc: 0.8987341523170471)
[2025-02-13 19:43:10,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:10,613][root][INFO] - Training Epoch: 1/2, step 3167/7134 completed (loss: 0.47868549823760986, acc: 0.895061731338501)
[2025-02-13 19:43:10,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:10,954][root][INFO] - Training Epoch: 1/2, step 3168/7134 completed (loss: 0.24395930767059326, acc: 0.9477124214172363)
[2025-02-13 19:43:11,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:11,306][root][INFO] - Training Epoch: 1/2, step 3169/7134 completed (loss: 0.2947148084640503, acc: 0.9180327653884888)
[2025-02-13 19:43:11,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:11,665][root][INFO] - Training Epoch: 1/2, step 3170/7134 completed (loss: 0.19779019057750702, acc: 0.9411764740943909)
[2025-02-13 19:43:11,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:12,037][root][INFO] - Training Epoch: 1/2, step 3171/7134 completed (loss: 0.2030729204416275, acc: 0.9450549483299255)
[2025-02-13 19:43:12,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:12,392][root][INFO] - Training Epoch: 1/2, step 3172/7134 completed (loss: 0.134139284491539, acc: 0.9841269850730896)
[2025-02-13 19:43:12,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:12,751][root][INFO] - Training Epoch: 1/2, step 3173/7134 completed (loss: 0.22766664624214172, acc: 0.9595959782600403)
[2025-02-13 19:43:12,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:13,116][root][INFO] - Training Epoch: 1/2, step 3174/7134 completed (loss: 0.28903862833976746, acc: 0.9337349534034729)
[2025-02-13 19:43:13,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:13,481][root][INFO] - Training Epoch: 1/2, step 3175/7134 completed (loss: 0.22108730673789978, acc: 0.9476743936538696)
[2025-02-13 19:43:13,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:13,849][root][INFO] - Training Epoch: 1/2, step 3176/7134 completed (loss: 0.48181724548339844, acc: 0.8837209343910217)
[2025-02-13 19:43:13,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:14,203][root][INFO] - Training Epoch: 1/2, step 3177/7134 completed (loss: 0.396882563829422, acc: 0.9012345671653748)
[2025-02-13 19:43:14,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:14,561][root][INFO] - Training Epoch: 1/2, step 3178/7134 completed (loss: 0.2656819820404053, acc: 0.9399999976158142)
[2025-02-13 19:43:14,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:14,942][root][INFO] - Training Epoch: 1/2, step 3179/7134 completed (loss: 0.3695395886898041, acc: 0.9107142686843872)
[2025-02-13 19:43:15,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:15,296][root][INFO] - Training Epoch: 1/2, step 3180/7134 completed (loss: 0.21676424145698547, acc: 0.9444444179534912)
[2025-02-13 19:43:15,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:15,668][root][INFO] - Training Epoch: 1/2, step 3181/7134 completed (loss: 0.3088633716106415, acc: 0.9281768202781677)
[2025-02-13 19:43:15,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:16,019][root][INFO] - Training Epoch: 1/2, step 3182/7134 completed (loss: 0.15314960479736328, acc: 0.9695122241973877)
[2025-02-13 19:43:16,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:16,362][root][INFO] - Training Epoch: 1/2, step 3183/7134 completed (loss: 0.2737310528755188, acc: 0.9343065619468689)
[2025-02-13 19:43:16,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:16,758][root][INFO] - Training Epoch: 1/2, step 3184/7134 completed (loss: 0.21775265038013458, acc: 0.96875)
[2025-02-13 19:43:16,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:17,142][root][INFO] - Training Epoch: 1/2, step 3185/7134 completed (loss: 0.07838719338178635, acc: 0.9874213933944702)
[2025-02-13 19:43:17,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:17,530][root][INFO] - Training Epoch: 1/2, step 3186/7134 completed (loss: 0.1247057318687439, acc: 0.9615384340286255)
[2025-02-13 19:43:17,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:17,886][root][INFO] - Training Epoch: 1/2, step 3187/7134 completed (loss: 0.22347614169120789, acc: 0.9285714030265808)
[2025-02-13 19:43:18,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:18,247][root][INFO] - Training Epoch: 1/2, step 3188/7134 completed (loss: 0.14637115597724915, acc: 0.9497206807136536)
[2025-02-13 19:43:18,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:18,654][root][INFO] - Training Epoch: 1/2, step 3189/7134 completed (loss: 0.153427854180336, acc: 0.9567901492118835)
[2025-02-13 19:43:18,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:19,029][root][INFO] - Training Epoch: 1/2, step 3190/7134 completed (loss: 0.24168165028095245, acc: 0.9437500238418579)
[2025-02-13 19:43:19,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:19,438][root][INFO] - Training Epoch: 1/2, step 3191/7134 completed (loss: 0.1386461853981018, acc: 0.9813664555549622)
[2025-02-13 19:43:19,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:19,897][root][INFO] - Training Epoch: 1/2, step 3192/7134 completed (loss: 0.5943393707275391, acc: 0.8832116723060608)
[2025-02-13 19:43:20,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:20,276][root][INFO] - Training Epoch: 1/2, step 3193/7134 completed (loss: 0.2880772352218628, acc: 0.9135135412216187)
[2025-02-13 19:43:20,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:20,630][root][INFO] - Training Epoch: 1/2, step 3194/7134 completed (loss: 0.1541673094034195, acc: 0.9757575988769531)
[2025-02-13 19:43:20,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:21,007][root][INFO] - Training Epoch: 1/2, step 3195/7134 completed (loss: 0.3050493896007538, acc: 0.9254658222198486)
[2025-02-13 19:43:21,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:21,350][root][INFO] - Training Epoch: 1/2, step 3196/7134 completed (loss: 0.19812092185020447, acc: 0.95652174949646)
[2025-02-13 19:43:21,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:21,701][root][INFO] - Training Epoch: 1/2, step 3197/7134 completed (loss: 0.20446021854877472, acc: 0.940119743347168)
[2025-02-13 19:43:21,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:22,072][root][INFO] - Training Epoch: 1/2, step 3198/7134 completed (loss: 0.18620136380195618, acc: 0.949999988079071)
[2025-02-13 19:43:22,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:22,428][root][INFO] - Training Epoch: 1/2, step 3199/7134 completed (loss: 0.09346985071897507, acc: 0.9709302186965942)
[2025-02-13 19:43:22,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:22,781][root][INFO] - Training Epoch: 1/2, step 3200/7134 completed (loss: 0.2106771618127823, acc: 0.9645389914512634)
[2025-02-13 19:43:22,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:23,158][root][INFO] - Training Epoch: 1/2, step 3201/7134 completed (loss: 0.27401331067085266, acc: 0.9448819160461426)
[2025-02-13 19:43:23,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:23,520][root][INFO] - Training Epoch: 1/2, step 3202/7134 completed (loss: 0.17623429000377655, acc: 0.9438202381134033)
[2025-02-13 19:43:23,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:23,873][root][INFO] - Training Epoch: 1/2, step 3203/7134 completed (loss: 0.16347941756248474, acc: 0.9437500238418579)
[2025-02-13 19:43:24,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:24,215][root][INFO] - Training Epoch: 1/2, step 3204/7134 completed (loss: 0.17941181361675262, acc: 0.9452054500579834)
[2025-02-13 19:43:24,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:24,612][root][INFO] - Training Epoch: 1/2, step 3205/7134 completed (loss: 0.08168277144432068, acc: 0.9645389914512634)
[2025-02-13 19:43:24,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:24,992][root][INFO] - Training Epoch: 1/2, step 3206/7134 completed (loss: 0.13906177878379822, acc: 0.9575757384300232)
[2025-02-13 19:43:25,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:25,351][root][INFO] - Training Epoch: 1/2, step 3207/7134 completed (loss: 0.194206103682518, acc: 0.9386503100395203)
[2025-02-13 19:43:25,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:25,746][root][INFO] - Training Epoch: 1/2, step 3208/7134 completed (loss: 0.1775175780057907, acc: 0.9568345546722412)
[2025-02-13 19:43:25,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:26,117][root][INFO] - Training Epoch: 1/2, step 3209/7134 completed (loss: 0.14216777682304382, acc: 0.9437500238418579)
[2025-02-13 19:43:26,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:26,485][root][INFO] - Training Epoch: 1/2, step 3210/7134 completed (loss: 0.13730907440185547, acc: 0.9548386931419373)
[2025-02-13 19:43:26,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:26,829][root][INFO] - Training Epoch: 1/2, step 3211/7134 completed (loss: 0.18098202347755432, acc: 0.9579831957817078)
[2025-02-13 19:43:26,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:27,227][root][INFO] - Training Epoch: 1/2, step 3212/7134 completed (loss: 0.21592138707637787, acc: 0.9532163739204407)
[2025-02-13 19:43:27,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:27,615][root][INFO] - Training Epoch: 1/2, step 3213/7134 completed (loss: 0.20016790926456451, acc: 0.9404761791229248)
[2025-02-13 19:43:27,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:27,988][root][INFO] - Training Epoch: 1/2, step 3214/7134 completed (loss: 0.16602198779582977, acc: 0.9649122953414917)
[2025-02-13 19:43:28,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:28,351][root][INFO] - Training Epoch: 1/2, step 3215/7134 completed (loss: 0.13782624900341034, acc: 0.9704142212867737)
[2025-02-13 19:43:28,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:28,728][root][INFO] - Training Epoch: 1/2, step 3216/7134 completed (loss: 0.1437569260597229, acc: 0.9629629850387573)
[2025-02-13 19:43:28,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:29,106][root][INFO] - Training Epoch: 1/2, step 3217/7134 completed (loss: 0.15010878443717957, acc: 0.966292142868042)
[2025-02-13 19:43:29,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:29,454][root][INFO] - Training Epoch: 1/2, step 3218/7134 completed (loss: 0.1454363614320755, acc: 0.9612902998924255)
[2025-02-13 19:43:29,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:29,812][root][INFO] - Training Epoch: 1/2, step 3219/7134 completed (loss: 0.22432565689086914, acc: 0.9107142686843872)
[2025-02-13 19:43:29,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:30,167][root][INFO] - Training Epoch: 1/2, step 3220/7134 completed (loss: 0.17535065114498138, acc: 0.9322034120559692)
[2025-02-13 19:43:30,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:30,535][root][INFO] - Training Epoch: 1/2, step 3221/7134 completed (loss: 0.12624211609363556, acc: 0.9677419066429138)
[2025-02-13 19:43:30,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:30,889][root][INFO] - Training Epoch: 1/2, step 3222/7134 completed (loss: 0.38082924485206604, acc: 0.89552241563797)
[2025-02-13 19:43:31,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:31,225][root][INFO] - Training Epoch: 1/2, step 3223/7134 completed (loss: 0.38408222794532776, acc: 0.9386503100395203)
[2025-02-13 19:43:31,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:31,607][root][INFO] - Training Epoch: 1/2, step 3224/7134 completed (loss: 0.3117040991783142, acc: 0.913294792175293)
[2025-02-13 19:43:31,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:31,970][root][INFO] - Training Epoch: 1/2, step 3225/7134 completed (loss: 0.19266293942928314, acc: 0.9399999976158142)
[2025-02-13 19:43:32,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:32,322][root][INFO] - Training Epoch: 1/2, step 3226/7134 completed (loss: 0.28726431727409363, acc: 0.9139072895050049)
[2025-02-13 19:43:32,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:32,708][root][INFO] - Training Epoch: 1/2, step 3227/7134 completed (loss: 0.29470255970954895, acc: 0.9382715821266174)
[2025-02-13 19:43:32,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:33,084][root][INFO] - Training Epoch: 1/2, step 3228/7134 completed (loss: 0.1995062530040741, acc: 0.9343434572219849)
[2025-02-13 19:43:33,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:33,447][root][INFO] - Training Epoch: 1/2, step 3229/7134 completed (loss: 0.24556444585323334, acc: 0.9545454382896423)
[2025-02-13 19:43:33,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:33,836][root][INFO] - Training Epoch: 1/2, step 3230/7134 completed (loss: 0.16762419044971466, acc: 0.956250011920929)
[2025-02-13 19:43:33,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:34,220][root][INFO] - Training Epoch: 1/2, step 3231/7134 completed (loss: 0.2690935730934143, acc: 0.9450549483299255)
[2025-02-13 19:43:34,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:34,581][root][INFO] - Training Epoch: 1/2, step 3232/7134 completed (loss: 0.062149956822395325, acc: 0.9861111044883728)
[2025-02-13 19:43:34,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:34,942][root][INFO] - Training Epoch: 1/2, step 3233/7134 completed (loss: 0.315405935049057, acc: 0.9291338324546814)
[2025-02-13 19:43:35,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:35,305][root][INFO] - Training Epoch: 1/2, step 3234/7134 completed (loss: 0.07759828120470047, acc: 0.9811320900917053)
[2025-02-13 19:43:35,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:35,673][root][INFO] - Training Epoch: 1/2, step 3235/7134 completed (loss: 0.12083700299263, acc: 0.957446813583374)
[2025-02-13 19:43:35,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:36,043][root][INFO] - Training Epoch: 1/2, step 3236/7134 completed (loss: 0.3135015368461609, acc: 0.909604549407959)
[2025-02-13 19:43:36,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:36,457][root][INFO] - Training Epoch: 1/2, step 3237/7134 completed (loss: 0.16738684475421906, acc: 0.948051929473877)
[2025-02-13 19:43:36,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:36,871][root][INFO] - Training Epoch: 1/2, step 3238/7134 completed (loss: 0.24196292459964752, acc: 0.9388889074325562)
[2025-02-13 19:43:37,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:37,242][root][INFO] - Training Epoch: 1/2, step 3239/7134 completed (loss: 0.08199021220207214, acc: 0.9756097793579102)
[2025-02-13 19:43:37,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:37,605][root][INFO] - Training Epoch: 1/2, step 3240/7134 completed (loss: 0.09153804928064346, acc: 0.9776536226272583)
[2025-02-13 19:43:37,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:37,977][root][INFO] - Training Epoch: 1/2, step 3241/7134 completed (loss: 0.15823842585086823, acc: 0.9529411792755127)
[2025-02-13 19:43:38,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:38,339][root][INFO] - Training Epoch: 1/2, step 3242/7134 completed (loss: 0.08030250668525696, acc: 0.9740259647369385)
[2025-02-13 19:43:38,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:38,704][root][INFO] - Training Epoch: 1/2, step 3243/7134 completed (loss: 0.09042396396398544, acc: 0.9679144620895386)
[2025-02-13 19:43:38,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:39,058][root][INFO] - Training Epoch: 1/2, step 3244/7134 completed (loss: 0.23643696308135986, acc: 0.9537572264671326)
[2025-02-13 19:43:39,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:39,476][root][INFO] - Training Epoch: 1/2, step 3245/7134 completed (loss: 0.17307965457439423, acc: 0.9589743614196777)
[2025-02-13 19:43:39,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:39,855][root][INFO] - Training Epoch: 1/2, step 3246/7134 completed (loss: 0.15907993912696838, acc: 0.9743589758872986)
[2025-02-13 19:43:39,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:40,263][root][INFO] - Training Epoch: 1/2, step 3247/7134 completed (loss: 0.13438241183757782, acc: 0.9679487347602844)
[2025-02-13 19:43:40,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:40,657][root][INFO] - Training Epoch: 1/2, step 3248/7134 completed (loss: 0.08721303939819336, acc: 0.985401451587677)
[2025-02-13 19:43:40,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:41,031][root][INFO] - Training Epoch: 1/2, step 3249/7134 completed (loss: 0.11787798255681992, acc: 0.9644970297813416)
[2025-02-13 19:43:41,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:41,394][root][INFO] - Training Epoch: 1/2, step 3250/7134 completed (loss: 0.22216568887233734, acc: 0.940119743347168)
[2025-02-13 19:43:41,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:41,752][root][INFO] - Training Epoch: 1/2, step 3251/7134 completed (loss: 0.17696256935596466, acc: 0.9487179517745972)
[2025-02-13 19:43:41,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:42,118][root][INFO] - Training Epoch: 1/2, step 3252/7134 completed (loss: 0.0499831959605217, acc: 1.0)
[2025-02-13 19:43:42,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:42,486][root][INFO] - Training Epoch: 1/2, step 3253/7134 completed (loss: 0.12082415819168091, acc: 0.9605262875556946)
[2025-02-13 19:43:42,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:42,859][root][INFO] - Training Epoch: 1/2, step 3254/7134 completed (loss: 0.13206692039966583, acc: 0.9740259647369385)
[2025-02-13 19:43:42,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:43,213][root][INFO] - Training Epoch: 1/2, step 3255/7134 completed (loss: 0.18097759783267975, acc: 0.9659090638160706)
[2025-02-13 19:43:43,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:43,577][root][INFO] - Training Epoch: 1/2, step 3256/7134 completed (loss: 0.10706118494272232, acc: 0.9813664555549622)
[2025-02-13 19:43:43,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:43,937][root][INFO] - Training Epoch: 1/2, step 3257/7134 completed (loss: 0.31947943568229675, acc: 0.9575757384300232)
[2025-02-13 19:43:44,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:44,321][root][INFO] - Training Epoch: 1/2, step 3258/7134 completed (loss: 0.49552005529403687, acc: 0.8943089246749878)
[2025-02-13 19:43:44,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:44,687][root][INFO] - Training Epoch: 1/2, step 3259/7134 completed (loss: 0.29167068004608154, acc: 0.9192546606063843)
[2025-02-13 19:43:44,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:45,067][root][INFO] - Training Epoch: 1/2, step 3260/7134 completed (loss: 0.32524412870407104, acc: 0.931034505367279)
[2025-02-13 19:43:45,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:45,498][root][INFO] - Training Epoch: 1/2, step 3261/7134 completed (loss: 0.33305004239082336, acc: 0.8978102207183838)
[2025-02-13 19:43:45,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:45,886][root][INFO] - Training Epoch: 1/2, step 3262/7134 completed (loss: 0.2466227263212204, acc: 0.9567901492118835)
[2025-02-13 19:43:46,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:46,285][root][INFO] - Training Epoch: 1/2, step 3263/7134 completed (loss: 0.266523540019989, acc: 0.9053254723548889)
[2025-02-13 19:43:46,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:46,686][root][INFO] - Training Epoch: 1/2, step 3264/7134 completed (loss: 0.2712072432041168, acc: 0.9329268336296082)
[2025-02-13 19:43:46,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:47,062][root][INFO] - Training Epoch: 1/2, step 3265/7134 completed (loss: 0.2331438958644867, acc: 0.9490445852279663)
[2025-02-13 19:43:47,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:47,428][root][INFO] - Training Epoch: 1/2, step 3266/7134 completed (loss: 0.15032930672168732, acc: 0.951724112033844)
[2025-02-13 19:43:47,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:47,798][root][INFO] - Training Epoch: 1/2, step 3267/7134 completed (loss: 0.2584449052810669, acc: 0.9515151381492615)
[2025-02-13 19:43:47,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:48,149][root][INFO] - Training Epoch: 1/2, step 3268/7134 completed (loss: 0.24760156869888306, acc: 0.9452054500579834)
[2025-02-13 19:43:48,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:48,522][root][INFO] - Training Epoch: 1/2, step 3269/7134 completed (loss: 0.1784513294696808, acc: 0.9521276354789734)
[2025-02-13 19:43:48,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:48,889][root][INFO] - Training Epoch: 1/2, step 3270/7134 completed (loss: 0.11896708607673645, acc: 0.9621621370315552)
[2025-02-13 19:43:48,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:49,190][root][INFO] - Training Epoch: 1/2, step 3271/7134 completed (loss: 0.1364322155714035, acc: 0.98591548204422)
[2025-02-13 19:43:49,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:49,557][root][INFO] - Training Epoch: 1/2, step 3272/7134 completed (loss: 0.21137076616287231, acc: 0.9666666388511658)
[2025-02-13 19:43:49,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:49,925][root][INFO] - Training Epoch: 1/2, step 3273/7134 completed (loss: 0.1260242909193039, acc: 0.954285740852356)
[2025-02-13 19:43:50,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:50,292][root][INFO] - Training Epoch: 1/2, step 3274/7134 completed (loss: 0.12492860853672028, acc: 0.9813664555549622)
[2025-02-13 19:43:50,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:50,663][root][INFO] - Training Epoch: 1/2, step 3275/7134 completed (loss: 0.8554555773735046, acc: 0.8443113565444946)
[2025-02-13 19:43:50,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:51,033][root][INFO] - Training Epoch: 1/2, step 3276/7134 completed (loss: 0.24406889081001282, acc: 0.9607843160629272)
[2025-02-13 19:43:51,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:51,430][root][INFO] - Training Epoch: 1/2, step 3277/7134 completed (loss: 0.30193522572517395, acc: 0.9136690497398376)
[2025-02-13 19:43:51,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:51,837][root][INFO] - Training Epoch: 1/2, step 3278/7134 completed (loss: 0.28653088212013245, acc: 0.9193548560142517)
[2025-02-13 19:43:51,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:52,220][root][INFO] - Training Epoch: 1/2, step 3279/7134 completed (loss: 0.26507794857025146, acc: 0.9176470637321472)
[2025-02-13 19:43:52,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:52,615][root][INFO] - Training Epoch: 1/2, step 3280/7134 completed (loss: 0.5953218340873718, acc: 0.8557692170143127)
[2025-02-13 19:43:52,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:52,991][root][INFO] - Training Epoch: 1/2, step 3281/7134 completed (loss: 0.31608471274375916, acc: 0.9523809552192688)
[2025-02-13 19:43:53,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:53,387][root][INFO] - Training Epoch: 1/2, step 3282/7134 completed (loss: 0.1561470776796341, acc: 0.9465649127960205)
[2025-02-13 19:43:53,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:53,756][root][INFO] - Training Epoch: 1/2, step 3283/7134 completed (loss: 0.1479978710412979, acc: 0.9607843160629272)
[2025-02-13 19:43:53,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:54,114][root][INFO] - Training Epoch: 1/2, step 3284/7134 completed (loss: 0.20923037827014923, acc: 0.9586206674575806)
[2025-02-13 19:43:54,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:54,465][root][INFO] - Training Epoch: 1/2, step 3285/7134 completed (loss: 0.250886470079422, acc: 0.9411764740943909)
[2025-02-13 19:43:54,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:54,830][root][INFO] - Training Epoch: 1/2, step 3286/7134 completed (loss: 0.2140548825263977, acc: 0.9538461565971375)
[2025-02-13 19:43:54,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:55,188][root][INFO] - Training Epoch: 1/2, step 3287/7134 completed (loss: 0.22675926983356476, acc: 0.9333333373069763)
[2025-02-13 19:43:55,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:55,565][root][INFO] - Training Epoch: 1/2, step 3288/7134 completed (loss: 0.2367255687713623, acc: 0.935251772403717)
[2025-02-13 19:43:55,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:55,901][root][INFO] - Training Epoch: 1/2, step 3289/7134 completed (loss: 0.3070347011089325, acc: 0.9264705777168274)
[2025-02-13 19:43:56,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:56,251][root][INFO] - Training Epoch: 1/2, step 3290/7134 completed (loss: 0.27399322390556335, acc: 0.9495798349380493)
[2025-02-13 19:43:56,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:56,611][root][INFO] - Training Epoch: 1/2, step 3291/7134 completed (loss: 0.3148558735847473, acc: 0.9405940771102905)
[2025-02-13 19:43:56,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:56,968][root][INFO] - Training Epoch: 1/2, step 3292/7134 completed (loss: 0.3310348391532898, acc: 0.9411764740943909)
[2025-02-13 19:43:57,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:57,348][root][INFO] - Training Epoch: 1/2, step 3293/7134 completed (loss: 0.20950210094451904, acc: 0.9469696879386902)
[2025-02-13 19:43:57,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:57,713][root][INFO] - Training Epoch: 1/2, step 3294/7134 completed (loss: 0.3191502094268799, acc: 0.9166666865348816)
[2025-02-13 19:43:57,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:58,108][root][INFO] - Training Epoch: 1/2, step 3295/7134 completed (loss: 0.04204783961176872, acc: 1.0)
[2025-02-13 19:43:58,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:58,453][root][INFO] - Training Epoch: 1/2, step 3296/7134 completed (loss: 0.2815437316894531, acc: 0.9307692050933838)
[2025-02-13 19:43:58,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:58,792][root][INFO] - Training Epoch: 1/2, step 3297/7134 completed (loss: 0.344452828168869, acc: 0.8867924809455872)
[2025-02-13 19:43:58,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:59,152][root][INFO] - Training Epoch: 1/2, step 3298/7134 completed (loss: 0.37996208667755127, acc: 0.9248120188713074)
[2025-02-13 19:43:59,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:59,516][root][INFO] - Training Epoch: 1/2, step 3299/7134 completed (loss: 0.1832205206155777, acc: 0.9448819160461426)
[2025-02-13 19:43:59,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:59,877][root][INFO] - Training Epoch: 1/2, step 3300/7134 completed (loss: 0.2560677230358124, acc: 0.932330846786499)
[2025-02-13 19:43:59,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:00,218][root][INFO] - Training Epoch: 1/2, step 3301/7134 completed (loss: 0.18045967817306519, acc: 0.9417475461959839)
[2025-02-13 19:44:00,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:00,553][root][INFO] - Training Epoch: 1/2, step 3302/7134 completed (loss: 0.2575169801712036, acc: 0.9510489702224731)
[2025-02-13 19:44:00,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:00,863][root][INFO] - Training Epoch: 1/2, step 3303/7134 completed (loss: 0.32053813338279724, acc: 0.9603960514068604)
[2025-02-13 19:44:01,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:01,222][root][INFO] - Training Epoch: 1/2, step 3304/7134 completed (loss: 0.11913960427045822, acc: 0.9724770784378052)
[2025-02-13 19:44:01,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:01,597][root][INFO] - Training Epoch: 1/2, step 3305/7134 completed (loss: 0.204752579331398, acc: 0.955974817276001)
[2025-02-13 19:44:01,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:01,975][root][INFO] - Training Epoch: 1/2, step 3306/7134 completed (loss: 0.13670143485069275, acc: 0.959770143032074)
[2025-02-13 19:44:02,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:02,319][root][INFO] - Training Epoch: 1/2, step 3307/7134 completed (loss: 0.10339643061161041, acc: 0.9777777791023254)
[2025-02-13 19:44:02,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:02,698][root][INFO] - Training Epoch: 1/2, step 3308/7134 completed (loss: 0.24454684555530548, acc: 0.9529411792755127)
[2025-02-13 19:44:02,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:03,091][root][INFO] - Training Epoch: 1/2, step 3309/7134 completed (loss: 0.32445600628852844, acc: 0.9038461446762085)
[2025-02-13 19:44:03,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:03,461][root][INFO] - Training Epoch: 1/2, step 3310/7134 completed (loss: 0.19567808508872986, acc: 0.9363636374473572)
[2025-02-13 19:44:03,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:03,865][root][INFO] - Training Epoch: 1/2, step 3311/7134 completed (loss: 0.24562543630599976, acc: 0.9306930899620056)
[2025-02-13 19:44:04,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:04,280][root][INFO] - Training Epoch: 1/2, step 3312/7134 completed (loss: 0.25793495774269104, acc: 0.932330846786499)
[2025-02-13 19:44:04,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:04,678][root][INFO] - Training Epoch: 1/2, step 3313/7134 completed (loss: 0.3670235574245453, acc: 0.8877550959587097)
[2025-02-13 19:44:04,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:05,068][root][INFO] - Training Epoch: 1/2, step 3314/7134 completed (loss: 0.2534165382385254, acc: 0.9357798099517822)
[2025-02-13 19:44:05,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:05,449][root][INFO] - Training Epoch: 1/2, step 3315/7134 completed (loss: 0.24430477619171143, acc: 0.9408283829689026)
[2025-02-13 19:44:05,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:05,818][root][INFO] - Training Epoch: 1/2, step 3316/7134 completed (loss: 0.27180320024490356, acc: 0.9104477763175964)
[2025-02-13 19:44:05,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:06,201][root][INFO] - Training Epoch: 1/2, step 3317/7134 completed (loss: 0.4274963438510895, acc: 0.8842975497245789)
[2025-02-13 19:44:06,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:06,567][root][INFO] - Training Epoch: 1/2, step 3318/7134 completed (loss: 0.34372034668922424, acc: 0.8983050584793091)
[2025-02-13 19:44:06,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:06,937][root][INFO] - Training Epoch: 1/2, step 3319/7134 completed (loss: 0.27498963475227356, acc: 0.9347826242446899)
[2025-02-13 19:44:07,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:07,299][root][INFO] - Training Epoch: 1/2, step 3320/7134 completed (loss: 0.29538100957870483, acc: 0.9378530979156494)
[2025-02-13 19:44:07,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:07,651][root][INFO] - Training Epoch: 1/2, step 3321/7134 completed (loss: 0.3438335061073303, acc: 0.9142857193946838)
[2025-02-13 19:44:07,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:08,022][root][INFO] - Training Epoch: 1/2, step 3322/7134 completed (loss: 0.3010472357273102, acc: 0.9078947305679321)
[2025-02-13 19:44:08,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:08,385][root][INFO] - Training Epoch: 1/2, step 3323/7134 completed (loss: 0.24338550865650177, acc: 0.9239766001701355)
[2025-02-13 19:44:08,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:08,773][root][INFO] - Training Epoch: 1/2, step 3324/7134 completed (loss: 0.347607284784317, acc: 0.9067357778549194)
[2025-02-13 19:44:08,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:09,174][root][INFO] - Training Epoch: 1/2, step 3325/7134 completed (loss: 0.19738280773162842, acc: 0.9520958065986633)
[2025-02-13 19:44:09,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:09,543][root][INFO] - Training Epoch: 1/2, step 3326/7134 completed (loss: 0.29355788230895996, acc: 0.9151515364646912)
[2025-02-13 19:44:09,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:09,953][root][INFO] - Training Epoch: 1/2, step 3327/7134 completed (loss: 0.2606278359889984, acc: 0.9171270728111267)
[2025-02-13 19:44:10,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:10,319][root][INFO] - Training Epoch: 1/2, step 3328/7134 completed (loss: 0.1351752132177353, acc: 0.9680851101875305)
[2025-02-13 19:44:10,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:10,675][root][INFO] - Training Epoch: 1/2, step 3329/7134 completed (loss: 0.24024654924869537, acc: 0.9322916865348816)
[2025-02-13 19:44:10,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:11,031][root][INFO] - Training Epoch: 1/2, step 3330/7134 completed (loss: 0.15105880796909332, acc: 0.9589743614196777)
[2025-02-13 19:44:11,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:11,389][root][INFO] - Training Epoch: 1/2, step 3331/7134 completed (loss: 0.49935248494148254, acc: 0.8520709872245789)
[2025-02-13 19:44:11,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:11,782][root][INFO] - Training Epoch: 1/2, step 3332/7134 completed (loss: 0.31449273228645325, acc: 0.9399999976158142)
[2025-02-13 19:44:11,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:12,162][root][INFO] - Training Epoch: 1/2, step 3333/7134 completed (loss: 0.46438854932785034, acc: 0.904347836971283)
[2025-02-13 19:44:12,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:12,547][root][INFO] - Training Epoch: 1/2, step 3334/7134 completed (loss: 0.2784274220466614, acc: 0.929347813129425)
[2025-02-13 19:44:12,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:12,993][root][INFO] - Training Epoch: 1/2, step 3335/7134 completed (loss: 0.10160192102193832, acc: 0.95333331823349)
[2025-02-13 19:44:13,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:13,391][root][INFO] - Training Epoch: 1/2, step 3336/7134 completed (loss: 0.0838041752576828, acc: 0.9820359349250793)
[2025-02-13 19:44:13,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:13,755][root][INFO] - Training Epoch: 1/2, step 3337/7134 completed (loss: 0.08567333966493607, acc: 0.9831932783126831)
[2025-02-13 19:44:13,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:14,120][root][INFO] - Training Epoch: 1/2, step 3338/7134 completed (loss: 0.102410688996315, acc: 0.9637305736541748)
[2025-02-13 19:44:14,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:14,494][root][INFO] - Training Epoch: 1/2, step 3339/7134 completed (loss: 0.1988723874092102, acc: 0.9323671460151672)
[2025-02-13 19:44:14,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:14,857][root][INFO] - Training Epoch: 1/2, step 3340/7134 completed (loss: 0.28469839692115784, acc: 0.9523809552192688)
[2025-02-13 19:44:14,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:15,207][root][INFO] - Training Epoch: 1/2, step 3341/7134 completed (loss: 0.24787642061710358, acc: 0.9281437397003174)
[2025-02-13 19:44:15,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:15,576][root][INFO] - Training Epoch: 1/2, step 3342/7134 completed (loss: 0.16579197347164154, acc: 0.9520547986030579)
[2025-02-13 19:44:15,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:15,972][root][INFO] - Training Epoch: 1/2, step 3343/7134 completed (loss: 0.1654597669839859, acc: 0.9568965435028076)
[2025-02-13 19:44:16,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:16,334][root][INFO] - Training Epoch: 1/2, step 3344/7134 completed (loss: 0.1699800342321396, acc: 0.9779411554336548)
[2025-02-13 19:44:16,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:16,697][root][INFO] - Training Epoch: 1/2, step 3345/7134 completed (loss: 0.2069089412689209, acc: 0.9510489702224731)
[2025-02-13 19:44:16,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:17,068][root][INFO] - Training Epoch: 1/2, step 3346/7134 completed (loss: 0.1721908450126648, acc: 0.9390243887901306)
[2025-02-13 19:44:17,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:17,424][root][INFO] - Training Epoch: 1/2, step 3347/7134 completed (loss: 0.3785581886768341, acc: 0.9281045794487)
[2025-02-13 19:44:17,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:17,777][root][INFO] - Training Epoch: 1/2, step 3348/7134 completed (loss: 0.14680425822734833, acc: 0.9620253443717957)
[2025-02-13 19:44:17,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:18,132][root][INFO] - Training Epoch: 1/2, step 3349/7134 completed (loss: 0.16968555748462677, acc: 0.961240291595459)
[2025-02-13 19:44:18,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:18,495][root][INFO] - Training Epoch: 1/2, step 3350/7134 completed (loss: 0.16289013624191284, acc: 0.9752066135406494)
[2025-02-13 19:44:18,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:18,848][root][INFO] - Training Epoch: 1/2, step 3351/7134 completed (loss: 0.4053632915019989, acc: 0.9042553305625916)
[2025-02-13 19:44:18,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:19,197][root][INFO] - Training Epoch: 1/2, step 3352/7134 completed (loss: 0.17056222259998322, acc: 0.9629629850387573)
[2025-02-13 19:44:19,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:19,559][root][INFO] - Training Epoch: 1/2, step 3353/7134 completed (loss: 0.17977160215377808, acc: 0.9430894255638123)
[2025-02-13 19:44:19,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:19,951][root][INFO] - Training Epoch: 1/2, step 3354/7134 completed (loss: 0.11023122817277908, acc: 0.9683544039726257)
[2025-02-13 19:44:20,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:20,337][root][INFO] - Training Epoch: 1/2, step 3355/7134 completed (loss: 0.05656914412975311, acc: 0.9939024448394775)
[2025-02-13 19:44:20,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:20,713][root][INFO] - Training Epoch: 1/2, step 3356/7134 completed (loss: 0.027271125465631485, acc: 1.0)
[2025-02-13 19:44:20,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:21,088][root][INFO] - Training Epoch: 1/2, step 3357/7134 completed (loss: 0.1404256969690323, acc: 0.9640718698501587)
[2025-02-13 19:44:21,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:21,452][root][INFO] - Training Epoch: 1/2, step 3358/7134 completed (loss: 0.42970898747444153, acc: 0.8823529481887817)
[2025-02-13 19:44:21,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:21,811][root][INFO] - Training Epoch: 1/2, step 3359/7134 completed (loss: 0.2501711845397949, acc: 0.9178082346916199)
[2025-02-13 19:44:21,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:22,168][root][INFO] - Training Epoch: 1/2, step 3360/7134 completed (loss: 0.23923790454864502, acc: 0.95652174949646)
[2025-02-13 19:44:22,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:22,534][root][INFO] - Training Epoch: 1/2, step 3361/7134 completed (loss: 0.3417380154132843, acc: 0.9017857313156128)
[2025-02-13 19:44:22,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:22,887][root][INFO] - Training Epoch: 1/2, step 3362/7134 completed (loss: 0.2143157720565796, acc: 0.918181836605072)
[2025-02-13 19:44:23,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:23,236][root][INFO] - Training Epoch: 1/2, step 3363/7134 completed (loss: 0.22310571372509003, acc: 0.9345794320106506)
[2025-02-13 19:44:23,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:23,605][root][INFO] - Training Epoch: 1/2, step 3364/7134 completed (loss: 0.44201526045799255, acc: 0.9019607901573181)
[2025-02-13 19:44:23,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:24,006][root][INFO] - Training Epoch: 1/2, step 3365/7134 completed (loss: 0.27889716625213623, acc: 0.9026548862457275)
[2025-02-13 19:44:24,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:24,366][root][INFO] - Training Epoch: 1/2, step 3366/7134 completed (loss: 0.1487938016653061, acc: 0.9629629850387573)
[2025-02-13 19:44:24,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:24,718][root][INFO] - Training Epoch: 1/2, step 3367/7134 completed (loss: 0.2800217270851135, acc: 0.8983957171440125)
[2025-02-13 19:44:24,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:25,086][root][INFO] - Training Epoch: 1/2, step 3368/7134 completed (loss: 0.30686837434768677, acc: 0.9378882050514221)
[2025-02-13 19:44:25,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:25,453][root][INFO] - Training Epoch: 1/2, step 3369/7134 completed (loss: 0.22137956321239471, acc: 0.9459459185600281)
[2025-02-13 19:44:25,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:25,811][root][INFO] - Training Epoch: 1/2, step 3370/7134 completed (loss: 0.17673133313655853, acc: 0.9509202241897583)
[2025-02-13 19:44:25,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:26,161][root][INFO] - Training Epoch: 1/2, step 3371/7134 completed (loss: 0.1618747115135193, acc: 0.9652777910232544)
[2025-02-13 19:44:26,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:26,548][root][INFO] - Training Epoch: 1/2, step 3372/7134 completed (loss: 0.21984972059726715, acc: 0.9319728016853333)
[2025-02-13 19:44:26,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:26,906][root][INFO] - Training Epoch: 1/2, step 3373/7134 completed (loss: 0.15241581201553345, acc: 0.9803921580314636)
[2025-02-13 19:44:27,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:27,285][root][INFO] - Training Epoch: 1/2, step 3374/7134 completed (loss: 0.11833636462688446, acc: 0.9828571677207947)
[2025-02-13 19:44:27,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:27,649][root][INFO] - Training Epoch: 1/2, step 3375/7134 completed (loss: 0.14560970664024353, acc: 0.9640718698501587)
[2025-02-13 19:44:27,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:28,006][root][INFO] - Training Epoch: 1/2, step 3376/7134 completed (loss: 0.05860155448317528, acc: 0.9894179701805115)
[2025-02-13 19:44:28,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:28,392][root][INFO] - Training Epoch: 1/2, step 3377/7134 completed (loss: 0.08805660158395767, acc: 0.9698492288589478)
[2025-02-13 19:44:28,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:28,756][root][INFO] - Training Epoch: 1/2, step 3378/7134 completed (loss: 0.07291658222675323, acc: 0.9802631735801697)
[2025-02-13 19:44:28,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:29,116][root][INFO] - Training Epoch: 1/2, step 3379/7134 completed (loss: 0.18049053847789764, acc: 0.9459459185600281)
[2025-02-13 19:44:29,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:29,472][root][INFO] - Training Epoch: 1/2, step 3380/7134 completed (loss: 0.16194181144237518, acc: 0.9492753744125366)
[2025-02-13 19:44:29,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:29,845][root][INFO] - Training Epoch: 1/2, step 3381/7134 completed (loss: 0.12443078309297562, acc: 0.9756097793579102)
[2025-02-13 19:44:29,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:30,212][root][INFO] - Training Epoch: 1/2, step 3382/7134 completed (loss: 0.11176634579896927, acc: 0.953125)
[2025-02-13 19:44:30,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:30,572][root][INFO] - Training Epoch: 1/2, step 3383/7134 completed (loss: 0.13146205246448517, acc: 0.9860140085220337)
[2025-02-13 19:44:30,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:30,921][root][INFO] - Training Epoch: 1/2, step 3384/7134 completed (loss: 0.26374804973602295, acc: 0.9197080135345459)
[2025-02-13 19:44:31,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:31,287][root][INFO] - Training Epoch: 1/2, step 3385/7134 completed (loss: 0.1006140410900116, acc: 0.9652777910232544)
[2025-02-13 19:44:31,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:31,649][root][INFO] - Training Epoch: 1/2, step 3386/7134 completed (loss: 0.12052768468856812, acc: 0.9644970297813416)
[2025-02-13 19:44:31,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:32,014][root][INFO] - Training Epoch: 1/2, step 3387/7134 completed (loss: 0.24556386470794678, acc: 0.9360465407371521)
[2025-02-13 19:44:32,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:32,365][root][INFO] - Training Epoch: 1/2, step 3388/7134 completed (loss: 0.16628439724445343, acc: 0.9487179517745972)
[2025-02-13 19:44:32,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:32,727][root][INFO] - Training Epoch: 1/2, step 3389/7134 completed (loss: 0.16050882637500763, acc: 0.9617486596107483)
[2025-02-13 19:44:32,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:33,082][root][INFO] - Training Epoch: 1/2, step 3390/7134 completed (loss: 0.16198156774044037, acc: 0.9545454382896423)
[2025-02-13 19:44:33,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:33,415][root][INFO] - Training Epoch: 1/2, step 3391/7134 completed (loss: 0.09958913922309875, acc: 0.9817073345184326)
[2025-02-13 19:44:33,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:33,752][root][INFO] - Training Epoch: 1/2, step 3392/7134 completed (loss: 0.13308443129062653, acc: 0.9770992398262024)
[2025-02-13 19:44:33,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:34,100][root][INFO] - Training Epoch: 1/2, step 3393/7134 completed (loss: 0.19853302836418152, acc: 0.9510489702224731)
[2025-02-13 19:44:34,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:34,467][root][INFO] - Training Epoch: 1/2, step 3394/7134 completed (loss: 0.16612058877944946, acc: 0.949999988079071)
[2025-02-13 19:44:34,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:34,806][root][INFO] - Training Epoch: 1/2, step 3395/7134 completed (loss: 0.34723660349845886, acc: 0.9078947305679321)
[2025-02-13 19:44:34,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:35,164][root][INFO] - Training Epoch: 1/2, step 3396/7134 completed (loss: 0.1576935201883316, acc: 0.9663461446762085)
[2025-02-13 19:44:35,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:35,547][root][INFO] - Training Epoch: 1/2, step 3397/7134 completed (loss: 0.08938591182231903, acc: 0.9850746393203735)
[2025-02-13 19:44:35,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:35,917][root][INFO] - Training Epoch: 1/2, step 3398/7134 completed (loss: 0.17998725175857544, acc: 0.95652174949646)
[2025-02-13 19:44:36,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:36,277][root][INFO] - Training Epoch: 1/2, step 3399/7134 completed (loss: 0.5021689534187317, acc: 0.9097222089767456)
[2025-02-13 19:44:36,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:36,645][root][INFO] - Training Epoch: 1/2, step 3400/7134 completed (loss: 0.26530003547668457, acc: 0.9212598204612732)
[2025-02-13 19:44:36,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:37,005][root][INFO] - Training Epoch: 1/2, step 3401/7134 completed (loss: 0.1908697783946991, acc: 0.9523809552192688)
[2025-02-13 19:44:37,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:37,386][root][INFO] - Training Epoch: 1/2, step 3402/7134 completed (loss: 0.5023402571678162, acc: 0.903743326663971)
[2025-02-13 19:44:37,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:37,788][root][INFO] - Training Epoch: 1/2, step 3403/7134 completed (loss: 0.41041722893714905, acc: 0.8888888955116272)
[2025-02-13 19:44:37,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:38,192][root][INFO] - Training Epoch: 1/2, step 3404/7134 completed (loss: 0.1005377396941185, acc: 0.9811320900917053)
[2025-02-13 19:44:38,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:38,582][root][INFO] - Training Epoch: 1/2, step 3405/7134 completed (loss: 0.3298439383506775, acc: 0.9338235259056091)
[2025-02-13 19:44:38,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:38,952][root][INFO] - Training Epoch: 1/2, step 3406/7134 completed (loss: 0.3942025303840637, acc: 0.9049999713897705)
[2025-02-13 19:44:39,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:39,290][root][INFO] - Training Epoch: 1/2, step 3407/7134 completed (loss: 0.19328953325748444, acc: 0.9386503100395203)
[2025-02-13 19:44:39,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:39,640][root][INFO] - Training Epoch: 1/2, step 3408/7134 completed (loss: 0.1451631486415863, acc: 0.9716981053352356)
[2025-02-13 19:44:39,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:40,006][root][INFO] - Training Epoch: 1/2, step 3409/7134 completed (loss: 0.20874349772930145, acc: 0.959770143032074)
[2025-02-13 19:44:40,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:40,358][root][INFO] - Training Epoch: 1/2, step 3410/7134 completed (loss: 0.24873670935630798, acc: 0.9512194991111755)
[2025-02-13 19:44:40,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:40,722][root][INFO] - Training Epoch: 1/2, step 3411/7134 completed (loss: 0.2274566888809204, acc: 0.9505494236946106)
[2025-02-13 19:44:40,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:41,063][root][INFO] - Training Epoch: 1/2, step 3412/7134 completed (loss: 0.3390018343925476, acc: 0.925000011920929)
[2025-02-13 19:44:41,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:41,420][root][INFO] - Training Epoch: 1/2, step 3413/7134 completed (loss: 0.23336943984031677, acc: 0.9572649598121643)
[2025-02-13 19:44:41,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:41,772][root][INFO] - Training Epoch: 1/2, step 3414/7134 completed (loss: 0.14417214691638947, acc: 0.9513888955116272)
[2025-02-13 19:44:41,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:42,135][root][INFO] - Training Epoch: 1/2, step 3415/7134 completed (loss: 0.19243265688419342, acc: 0.9714285731315613)
[2025-02-13 19:44:42,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:42,518][root][INFO] - Training Epoch: 1/2, step 3416/7134 completed (loss: 0.24316099286079407, acc: 0.9508196711540222)
[2025-02-13 19:44:42,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:42,877][root][INFO] - Training Epoch: 1/2, step 3417/7134 completed (loss: 0.12857526540756226, acc: 0.970059871673584)
[2025-02-13 19:44:43,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:43,240][root][INFO] - Training Epoch: 1/2, step 3418/7134 completed (loss: 0.15032653510570526, acc: 0.9570552110671997)
[2025-02-13 19:44:43,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:43,595][root][INFO] - Training Epoch: 1/2, step 3419/7134 completed (loss: 0.18198856711387634, acc: 0.9698795080184937)
[2025-02-13 19:44:43,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:44,010][root][INFO] - Training Epoch: 1/2, step 3420/7134 completed (loss: 0.0641198679804802, acc: 0.9735099077224731)
[2025-02-13 19:44:44,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:44,379][root][INFO] - Training Epoch: 1/2, step 3421/7134 completed (loss: 0.2057093381881714, acc: 0.9551281929016113)
[2025-02-13 19:44:44,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:44,716][root][INFO] - Training Epoch: 1/2, step 3422/7134 completed (loss: 0.15505026280879974, acc: 0.9632353186607361)
[2025-02-13 19:44:44,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:45,092][root][INFO] - Training Epoch: 1/2, step 3423/7134 completed (loss: 0.039573460817337036, acc: 0.9934210777282715)
[2025-02-13 19:44:45,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:45,457][root][INFO] - Training Epoch: 1/2, step 3424/7134 completed (loss: 0.20176538825035095, acc: 0.9399999976158142)
[2025-02-13 19:44:45,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:45,823][root][INFO] - Training Epoch: 1/2, step 3425/7134 completed (loss: 0.3872661292552948, acc: 0.9263157844543457)
[2025-02-13 19:44:45,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:46,191][root][INFO] - Training Epoch: 1/2, step 3426/7134 completed (loss: 0.23856621980667114, acc: 0.976190447807312)
[2025-02-13 19:44:46,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:46,549][root][INFO] - Training Epoch: 1/2, step 3427/7134 completed (loss: 0.16549408435821533, acc: 0.9740259647369385)
[2025-02-13 19:44:46,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:46,914][root][INFO] - Training Epoch: 1/2, step 3428/7134 completed (loss: 0.15654835104942322, acc: 0.9647058844566345)
[2025-02-13 19:44:47,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:47,292][root][INFO] - Training Epoch: 1/2, step 3429/7134 completed (loss: 0.21305730938911438, acc: 0.9577465057373047)
[2025-02-13 19:44:47,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:47,662][root][INFO] - Training Epoch: 1/2, step 3430/7134 completed (loss: 0.25592923164367676, acc: 0.931506872177124)
[2025-02-13 19:44:47,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:48,024][root][INFO] - Training Epoch: 1/2, step 3431/7134 completed (loss: 0.12102013826370239, acc: 0.97826087474823)
[2025-02-13 19:44:48,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:48,389][root][INFO] - Training Epoch: 1/2, step 3432/7134 completed (loss: 0.15419456362724304, acc: 0.9693251252174377)
[2025-02-13 19:44:48,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:48,802][root][INFO] - Training Epoch: 1/2, step 3433/7134 completed (loss: 0.2409205138683319, acc: 0.9558823704719543)
[2025-02-13 19:44:48,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:49,159][root][INFO] - Training Epoch: 1/2, step 3434/7134 completed (loss: 0.17978836596012115, acc: 0.9357143044471741)
[2025-02-13 19:44:49,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:49,538][root][INFO] - Training Epoch: 1/2, step 3435/7134 completed (loss: 0.15963728725910187, acc: 0.9724137783050537)
[2025-02-13 19:44:49,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:49,922][root][INFO] - Training Epoch: 1/2, step 3436/7134 completed (loss: 0.1751241236925125, acc: 0.9489051103591919)
[2025-02-13 19:44:50,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:50,276][root][INFO] - Training Epoch: 1/2, step 3437/7134 completed (loss: 0.26151585578918457, acc: 0.9256198406219482)
[2025-02-13 19:44:50,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:50,700][root][INFO] - Training Epoch: 1/2, step 3438/7134 completed (loss: 0.2517342269420624, acc: 0.9473684430122375)
[2025-02-13 19:44:50,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:51,056][root][INFO] - Training Epoch: 1/2, step 3439/7134 completed (loss: 0.36638131737709045, acc: 0.920634925365448)
[2025-02-13 19:44:51,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:51,419][root][INFO] - Training Epoch: 1/2, step 3440/7134 completed (loss: 0.4621625542640686, acc: 0.8706896305084229)
[2025-02-13 19:44:51,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:51,773][root][INFO] - Training Epoch: 1/2, step 3441/7134 completed (loss: 0.36836791038513184, acc: 0.9186046719551086)
[2025-02-13 19:44:51,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:52,152][root][INFO] - Training Epoch: 1/2, step 3442/7134 completed (loss: 0.14336514472961426, acc: 0.9645389914512634)
[2025-02-13 19:44:52,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:52,509][root][INFO] - Training Epoch: 1/2, step 3443/7134 completed (loss: 0.2691674828529358, acc: 0.966292142868042)
[2025-02-13 19:44:52,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:52,870][root][INFO] - Training Epoch: 1/2, step 3444/7134 completed (loss: 0.192424938082695, acc: 0.9426751732826233)
[2025-02-13 19:44:52,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:53,214][root][INFO] - Training Epoch: 1/2, step 3445/7134 completed (loss: 0.45056167244911194, acc: 0.9097744226455688)
[2025-02-13 19:44:53,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:53,593][root][INFO] - Training Epoch: 1/2, step 3446/7134 completed (loss: 0.404670774936676, acc: 0.8999999761581421)
[2025-02-13 19:44:53,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:53,974][root][INFO] - Training Epoch: 1/2, step 3447/7134 completed (loss: 0.43682125210762024, acc: 0.9012345671653748)
[2025-02-13 19:44:54,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:54,333][root][INFO] - Training Epoch: 1/2, step 3448/7134 completed (loss: 0.3011390268802643, acc: 0.9306358098983765)
[2025-02-13 19:44:54,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:54,707][root][INFO] - Training Epoch: 1/2, step 3449/7134 completed (loss: 0.19083376228809357, acc: 0.9504950642585754)
[2025-02-13 19:44:54,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:55,073][root][INFO] - Training Epoch: 1/2, step 3450/7134 completed (loss: 0.25434666872024536, acc: 0.9117646813392639)
[2025-02-13 19:44:55,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:55,440][root][INFO] - Training Epoch: 1/2, step 3451/7134 completed (loss: 0.3950847387313843, acc: 0.8794326186180115)
[2025-02-13 19:44:55,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:55,806][root][INFO] - Training Epoch: 1/2, step 3452/7134 completed (loss: 0.5362244248390198, acc: 0.8617886304855347)
[2025-02-13 19:44:55,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:56,173][root][INFO] - Training Epoch: 1/2, step 3453/7134 completed (loss: 0.2684570848941803, acc: 0.9236111044883728)
[2025-02-13 19:44:56,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:56,555][root][INFO] - Training Epoch: 1/2, step 3454/7134 completed (loss: 0.1964682638645172, acc: 0.9578313231468201)
[2025-02-13 19:44:56,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:56,926][root][INFO] - Training Epoch: 1/2, step 3455/7134 completed (loss: 0.08958855271339417, acc: 0.9751552939414978)
[2025-02-13 19:44:57,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:57,293][root][INFO] - Training Epoch: 1/2, step 3456/7134 completed (loss: 0.08907531201839447, acc: 0.9849246144294739)
[2025-02-13 19:44:57,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:57,661][root][INFO] - Training Epoch: 1/2, step 3457/7134 completed (loss: 0.0673542469739914, acc: 0.9929577708244324)
[2025-02-13 19:44:57,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:58,079][root][INFO] - Training Epoch: 1/2, step 3458/7134 completed (loss: 0.27577587962150574, acc: 0.9505494236946106)
[2025-02-13 19:44:58,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:58,442][root][INFO] - Training Epoch: 1/2, step 3459/7134 completed (loss: 0.23771576583385468, acc: 0.9433962106704712)
[2025-02-13 19:44:58,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:58,784][root][INFO] - Training Epoch: 1/2, step 3460/7134 completed (loss: 1.0360114574432373, acc: 0.7878788113594055)
[2025-02-13 19:44:58,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:59,097][root][INFO] - Training Epoch: 1/2, step 3461/7134 completed (loss: 0.8735329508781433, acc: 0.8230769038200378)
[2025-02-13 19:44:59,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:59,457][root][INFO] - Training Epoch: 1/2, step 3462/7134 completed (loss: 0.4007444679737091, acc: 0.9181286692619324)
[2025-02-13 19:44:59,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:59,837][root][INFO] - Training Epoch: 1/2, step 3463/7134 completed (loss: 0.09348117560148239, acc: 0.9814814925193787)
[2025-02-13 19:44:59,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:00,214][root][INFO] - Training Epoch: 1/2, step 3464/7134 completed (loss: 0.18219801783561707, acc: 0.9606741666793823)
[2025-02-13 19:45:00,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:00,575][root][INFO] - Training Epoch: 1/2, step 3465/7134 completed (loss: 0.4009760022163391, acc: 0.9259259104728699)
[2025-02-13 19:45:00,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:00,923][root][INFO] - Training Epoch: 1/2, step 3466/7134 completed (loss: 0.32519519329071045, acc: 0.9197530746459961)
[2025-02-13 19:45:01,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:01,329][root][INFO] - Training Epoch: 1/2, step 3467/7134 completed (loss: 0.14605170488357544, acc: 0.9576271176338196)
[2025-02-13 19:45:01,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:01,736][root][INFO] - Training Epoch: 1/2, step 3468/7134 completed (loss: 0.04957282543182373, acc: 1.0)
[2025-02-13 19:45:01,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:02,126][root][INFO] - Training Epoch: 1/2, step 3469/7134 completed (loss: 0.17998185753822327, acc: 0.9548872113227844)
[2025-02-13 19:45:02,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:02,489][root][INFO] - Training Epoch: 1/2, step 3470/7134 completed (loss: 0.05391966551542282, acc: 0.9814814925193787)
[2025-02-13 19:45:02,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:02,939][root][INFO] - Training Epoch: 1/2, step 3471/7134 completed (loss: 0.21561694145202637, acc: 0.9333333373069763)
[2025-02-13 19:45:03,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:03,314][root][INFO] - Training Epoch: 1/2, step 3472/7134 completed (loss: 0.06493382155895233, acc: 0.987261176109314)
[2025-02-13 19:45:03,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:03,717][root][INFO] - Training Epoch: 1/2, step 3473/7134 completed (loss: 0.12167613208293915, acc: 0.9736841917037964)
[2025-02-13 19:45:03,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:04,061][root][INFO] - Training Epoch: 1/2, step 3474/7134 completed (loss: 0.18778873980045319, acc: 0.9637681245803833)
[2025-02-13 19:45:04,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:04,416][root][INFO] - Training Epoch: 1/2, step 3475/7134 completed (loss: 0.21400828659534454, acc: 0.9545454382896423)
[2025-02-13 19:45:04,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:04,785][root][INFO] - Training Epoch: 1/2, step 3476/7134 completed (loss: 0.3645918369293213, acc: 0.931506872177124)
[2025-02-13 19:45:04,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:05,140][root][INFO] - Training Epoch: 1/2, step 3477/7134 completed (loss: 0.18893064558506012, acc: 0.9594594836235046)
[2025-02-13 19:45:05,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:05,492][root][INFO] - Training Epoch: 1/2, step 3478/7134 completed (loss: 0.05052730813622475, acc: 0.9940119981765747)
[2025-02-13 19:45:05,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:05,854][root][INFO] - Training Epoch: 1/2, step 3479/7134 completed (loss: 0.03293988108634949, acc: 0.9871794581413269)
[2025-02-13 19:45:05,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:06,229][root][INFO] - Training Epoch: 1/2, step 3480/7134 completed (loss: 0.21568788588047028, acc: 0.9725274443626404)
[2025-02-13 19:45:06,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:06,623][root][INFO] - Training Epoch: 1/2, step 3481/7134 completed (loss: 0.13477082550525665, acc: 0.9774436354637146)
[2025-02-13 19:45:06,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:06,997][root][INFO] - Training Epoch: 1/2, step 3482/7134 completed (loss: 0.019742093980312347, acc: 1.0)
[2025-02-13 19:45:07,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:07,370][root][INFO] - Training Epoch: 1/2, step 3483/7134 completed (loss: 0.09216723591089249, acc: 0.9806451797485352)
[2025-02-13 19:45:07,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:07,708][root][INFO] - Training Epoch: 1/2, step 3484/7134 completed (loss: 0.34924811124801636, acc: 0.9642857313156128)
[2025-02-13 19:45:07,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:08,042][root][INFO] - Training Epoch: 1/2, step 3485/7134 completed (loss: 0.1689426153898239, acc: 0.9454545378684998)
[2025-02-13 19:45:08,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:08,398][root][INFO] - Training Epoch: 1/2, step 3486/7134 completed (loss: 0.30482035875320435, acc: 0.9230769276618958)
[2025-02-13 19:45:08,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:08,761][root][INFO] - Training Epoch: 1/2, step 3487/7134 completed (loss: 0.2414354532957077, acc: 0.9457831382751465)
[2025-02-13 19:45:08,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:09,126][root][INFO] - Training Epoch: 1/2, step 3488/7134 completed (loss: 0.14944231510162354, acc: 0.9714285731315613)
[2025-02-13 19:45:09,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:09,472][root][INFO] - Training Epoch: 1/2, step 3489/7134 completed (loss: 0.4009306728839874, acc: 0.8962963223457336)
[2025-02-13 19:45:09,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:09,851][root][INFO] - Training Epoch: 1/2, step 3490/7134 completed (loss: 0.21771492063999176, acc: 0.9650349617004395)
[2025-02-13 19:45:09,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:10,184][root][INFO] - Training Epoch: 1/2, step 3491/7134 completed (loss: 0.41329333186149597, acc: 0.9044585824012756)
[2025-02-13 19:45:10,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:10,525][root][INFO] - Training Epoch: 1/2, step 3492/7134 completed (loss: 0.19887448847293854, acc: 0.9357798099517822)
[2025-02-13 19:45:10,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:10,901][root][INFO] - Training Epoch: 1/2, step 3493/7134 completed (loss: 0.18098214268684387, acc: 0.9506173133850098)
[2025-02-13 19:45:11,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:11,242][root][INFO] - Training Epoch: 1/2, step 3494/7134 completed (loss: 0.16848832368850708, acc: 0.9594594836235046)
[2025-02-13 19:45:11,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:11,603][root][INFO] - Training Epoch: 1/2, step 3495/7134 completed (loss: 0.09831763803958893, acc: 0.9793814420700073)
[2025-02-13 19:45:11,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:11,919][root][INFO] - Training Epoch: 1/2, step 3496/7134 completed (loss: 0.18583691120147705, acc: 0.9509803652763367)
[2025-02-13 19:45:12,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:12,270][root][INFO] - Training Epoch: 1/2, step 3497/7134 completed (loss: 0.18212346732616425, acc: 0.949367105960846)
[2025-02-13 19:45:12,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:12,645][root][INFO] - Training Epoch: 1/2, step 3498/7134 completed (loss: 0.33914080262184143, acc: 0.9303797483444214)
[2025-02-13 19:45:12,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:13,016][root][INFO] - Training Epoch: 1/2, step 3499/7134 completed (loss: 0.2206578403711319, acc: 0.9542483687400818)
[2025-02-13 19:45:13,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:13,375][root][INFO] - Training Epoch: 1/2, step 3500/7134 completed (loss: 0.2979462146759033, acc: 0.9356725215911865)
[2025-02-13 19:45:13,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:13,714][root][INFO] - Training Epoch: 1/2, step 3501/7134 completed (loss: 0.2726130783557892, acc: 0.9242424368858337)
[2025-02-13 19:45:13,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:14,056][root][INFO] - Training Epoch: 1/2, step 3502/7134 completed (loss: 0.4525579512119293, acc: 0.8716216087341309)
[2025-02-13 19:45:14,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:14,424][root][INFO] - Training Epoch: 1/2, step 3503/7134 completed (loss: 0.3676444888114929, acc: 0.9064748287200928)
[2025-02-13 19:45:14,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:14,781][root][INFO] - Training Epoch: 1/2, step 3504/7134 completed (loss: 0.06861452013254166, acc: 0.9733333587646484)
[2025-02-13 19:45:14,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:15,171][root][INFO] - Training Epoch: 1/2, step 3505/7134 completed (loss: 0.19682617485523224, acc: 0.9444444179534912)
[2025-02-13 19:45:15,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:15,558][root][INFO] - Training Epoch: 1/2, step 3506/7134 completed (loss: 0.17117246985435486, acc: 0.961240291595459)
[2025-02-13 19:45:15,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:15,940][root][INFO] - Training Epoch: 1/2, step 3507/7134 completed (loss: 0.08795322477817535, acc: 0.9791666865348816)
[2025-02-13 19:45:16,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:16,305][root][INFO] - Training Epoch: 1/2, step 3508/7134 completed (loss: 0.12068546563386917, acc: 0.9602649211883545)
[2025-02-13 19:45:16,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:16,724][root][INFO] - Training Epoch: 1/2, step 3509/7134 completed (loss: 0.2650493085384369, acc: 0.9693251252174377)
[2025-02-13 19:45:16,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:17,079][root][INFO] - Training Epoch: 1/2, step 3510/7134 completed (loss: 0.10725507885217667, acc: 0.969924807548523)
[2025-02-13 19:45:17,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:17,451][root][INFO] - Training Epoch: 1/2, step 3511/7134 completed (loss: 0.0811898484826088, acc: 0.9852941036224365)
[2025-02-13 19:45:17,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:17,850][root][INFO] - Training Epoch: 1/2, step 3512/7134 completed (loss: 0.249622642993927, acc: 0.9487179517745972)
[2025-02-13 19:45:17,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:18,219][root][INFO] - Training Epoch: 1/2, step 3513/7134 completed (loss: 0.08286544680595398, acc: 0.97826087474823)
[2025-02-13 19:45:18,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:18,590][root][INFO] - Training Epoch: 1/2, step 3514/7134 completed (loss: 0.08894842118024826, acc: 0.9760765433311462)
[2025-02-13 19:45:18,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:18,942][root][INFO] - Training Epoch: 1/2, step 3515/7134 completed (loss: 0.04172910004854202, acc: 0.994350254535675)
[2025-02-13 19:45:19,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:19,385][root][INFO] - Training Epoch: 1/2, step 3516/7134 completed (loss: 0.054754797369241714, acc: 1.0)
[2025-02-13 19:45:19,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:19,831][root][INFO] - Training Epoch: 1/2, step 3517/7134 completed (loss: 0.07079120725393295, acc: 0.9848484992980957)
[2025-02-13 19:45:19,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:20,237][root][INFO] - Training Epoch: 1/2, step 3518/7134 completed (loss: 0.19910597801208496, acc: 0.9407407641410828)
[2025-02-13 19:45:20,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:20,612][root][INFO] - Training Epoch: 1/2, step 3519/7134 completed (loss: 0.09762696176767349, acc: 0.9733333587646484)
[2025-02-13 19:45:20,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:20,997][root][INFO] - Training Epoch: 1/2, step 3520/7134 completed (loss: 0.18609356880187988, acc: 0.9595959782600403)
[2025-02-13 19:45:21,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:21,377][root][INFO] - Training Epoch: 1/2, step 3521/7134 completed (loss: 0.05567897856235504, acc: 0.9820359349250793)
[2025-02-13 19:45:21,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:21,740][root][INFO] - Training Epoch: 1/2, step 3522/7134 completed (loss: 0.058799128979444504, acc: 0.9777777791023254)
[2025-02-13 19:45:21,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:22,108][root][INFO] - Training Epoch: 1/2, step 3523/7134 completed (loss: 0.053394727408885956, acc: 0.9867549538612366)
[2025-02-13 19:45:22,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:22,527][root][INFO] - Training Epoch: 1/2, step 3524/7134 completed (loss: 0.09567303955554962, acc: 0.9835164546966553)
[2025-02-13 19:45:22,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:22,906][root][INFO] - Training Epoch: 1/2, step 3525/7134 completed (loss: 0.08835697174072266, acc: 0.9662162065505981)
[2025-02-13 19:45:23,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:23,274][root][INFO] - Training Epoch: 1/2, step 3526/7134 completed (loss: 0.12175928056240082, acc: 0.9595959782600403)
[2025-02-13 19:45:23,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:23,671][root][INFO] - Training Epoch: 1/2, step 3527/7134 completed (loss: 0.16834889352321625, acc: 0.957446813583374)
[2025-02-13 19:45:23,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:24,032][root][INFO] - Training Epoch: 1/2, step 3528/7134 completed (loss: 0.021595625206828117, acc: 1.0)
[2025-02-13 19:45:24,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:24,380][root][INFO] - Training Epoch: 1/2, step 3529/7134 completed (loss: 0.11904033273458481, acc: 0.9545454382896423)
[2025-02-13 19:45:24,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:24,770][root][INFO] - Training Epoch: 1/2, step 3530/7134 completed (loss: 0.2915956676006317, acc: 0.9159291982650757)
[2025-02-13 19:45:24,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:25,138][root][INFO] - Training Epoch: 1/2, step 3531/7134 completed (loss: 0.1890793740749359, acc: 0.9375)
[2025-02-13 19:45:25,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:25,499][root][INFO] - Training Epoch: 1/2, step 3532/7134 completed (loss: 0.09711561352014542, acc: 0.9729729890823364)
[2025-02-13 19:45:25,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:25,860][root][INFO] - Training Epoch: 1/2, step 3533/7134 completed (loss: 0.13284021615982056, acc: 0.9644444584846497)
[2025-02-13 19:45:26,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:26,229][root][INFO] - Training Epoch: 1/2, step 3534/7134 completed (loss: 0.1363580971956253, acc: 0.9766082167625427)
[2025-02-13 19:45:26,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:26,598][root][INFO] - Training Epoch: 1/2, step 3535/7134 completed (loss: 0.14086845517158508, acc: 0.9640287756919861)
[2025-02-13 19:45:26,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:26,960][root][INFO] - Training Epoch: 1/2, step 3536/7134 completed (loss: 0.232812762260437, acc: 0.9259259104728699)
[2025-02-13 19:45:27,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:27,308][root][INFO] - Training Epoch: 1/2, step 3537/7134 completed (loss: 0.20201586186885834, acc: 0.953125)
[2025-02-13 19:45:27,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:27,680][root][INFO] - Training Epoch: 1/2, step 3538/7134 completed (loss: 0.11886759102344513, acc: 0.9579831957817078)
[2025-02-13 19:45:27,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:28,015][root][INFO] - Training Epoch: 1/2, step 3539/7134 completed (loss: 0.20892448723316193, acc: 0.931034505367279)
[2025-02-13 19:45:28,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:28,366][root][INFO] - Training Epoch: 1/2, step 3540/7134 completed (loss: 0.18522845208644867, acc: 0.9538461565971375)
[2025-02-13 19:45:28,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:28,744][root][INFO] - Training Epoch: 1/2, step 3541/7134 completed (loss: 0.18195182085037231, acc: 0.95333331823349)
[2025-02-13 19:45:28,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:29,094][root][INFO] - Training Epoch: 1/2, step 3542/7134 completed (loss: 0.10440786927938461, acc: 0.9794520735740662)
[2025-02-13 19:45:29,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:29,446][root][INFO] - Training Epoch: 1/2, step 3543/7134 completed (loss: 0.21382978558540344, acc: 0.9154929518699646)
[2025-02-13 19:45:29,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:29,807][root][INFO] - Training Epoch: 1/2, step 3544/7134 completed (loss: 0.2773427963256836, acc: 0.9205297827720642)
[2025-02-13 19:45:29,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:30,182][root][INFO] - Training Epoch: 1/2, step 3545/7134 completed (loss: 0.13390567898750305, acc: 0.9652777910232544)
[2025-02-13 19:45:30,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:30,548][root][INFO] - Training Epoch: 1/2, step 3546/7134 completed (loss: 0.26825204491615295, acc: 0.935251772403717)
[2025-02-13 19:45:30,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:30,927][root][INFO] - Training Epoch: 1/2, step 3547/7134 completed (loss: 0.37445127964019775, acc: 0.903954803943634)
[2025-02-13 19:45:31,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:31,273][root][INFO] - Training Epoch: 1/2, step 3548/7134 completed (loss: 0.20539677143096924, acc: 0.9541284441947937)
[2025-02-13 19:45:31,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:31,638][root][INFO] - Training Epoch: 1/2, step 3549/7134 completed (loss: 0.2505633533000946, acc: 0.9473684430122375)
[2025-02-13 19:45:31,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:32,034][root][INFO] - Training Epoch: 1/2, step 3550/7134 completed (loss: 0.20895260572433472, acc: 0.9343065619468689)
[2025-02-13 19:45:32,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:32,405][root][INFO] - Training Epoch: 1/2, step 3551/7134 completed (loss: 0.45134302973747253, acc: 0.8775510191917419)
[2025-02-13 19:45:32,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:32,805][root][INFO] - Training Epoch: 1/2, step 3552/7134 completed (loss: 0.282133013010025, acc: 0.9212598204612732)
[2025-02-13 19:45:32,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:33,169][root][INFO] - Training Epoch: 1/2, step 3553/7134 completed (loss: 0.25696995854377747, acc: 0.9411764740943909)
[2025-02-13 19:45:33,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:33,548][root][INFO] - Training Epoch: 1/2, step 3554/7134 completed (loss: 0.39093104004859924, acc: 0.9202454090118408)
[2025-02-13 19:45:33,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:33,914][root][INFO] - Training Epoch: 1/2, step 3555/7134 completed (loss: 0.4103107452392578, acc: 0.9096774458885193)
[2025-02-13 19:45:34,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:34,253][root][INFO] - Training Epoch: 1/2, step 3556/7134 completed (loss: 0.25828787684440613, acc: 0.9318181872367859)
[2025-02-13 19:45:34,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:34,638][root][INFO] - Training Epoch: 1/2, step 3557/7134 completed (loss: 0.31790491938591003, acc: 0.936170220375061)
[2025-02-13 19:45:34,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:35,019][root][INFO] - Training Epoch: 1/2, step 3558/7134 completed (loss: 0.35295557975769043, acc: 0.8880000114440918)
[2025-02-13 19:45:35,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:35,393][root][INFO] - Training Epoch: 1/2, step 3559/7134 completed (loss: 0.38133734464645386, acc: 0.8928571343421936)
[2025-02-13 19:45:35,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:35,800][root][INFO] - Training Epoch: 1/2, step 3560/7134 completed (loss: 0.22377066314220428, acc: 0.9398906826972961)
[2025-02-13 19:45:35,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:36,179][root][INFO] - Training Epoch: 1/2, step 3561/7134 completed (loss: 0.255231648683548, acc: 0.9491525292396545)
[2025-02-13 19:45:36,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:36,549][root][INFO] - Training Epoch: 1/2, step 3562/7134 completed (loss: 0.17283666133880615, acc: 0.9485294222831726)
[2025-02-13 19:45:36,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:36,874][root][INFO] - Training Epoch: 1/2, step 3563/7134 completed (loss: 0.22809332609176636, acc: 0.9432623982429504)
[2025-02-13 19:45:37,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:37,248][root][INFO] - Training Epoch: 1/2, step 3564/7134 completed (loss: 0.3182612359523773, acc: 0.9126983880996704)
[2025-02-13 19:45:37,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:37,618][root][INFO] - Training Epoch: 1/2, step 3565/7134 completed (loss: 0.3522421419620514, acc: 0.904411792755127)
[2025-02-13 19:45:38,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:38,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:39,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:39,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:40,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:40,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:40,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:42,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:42,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:42,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:43,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:43,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:43,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:44,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:44,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:45,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:45,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:46,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:46,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:46,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:47,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:47,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:47,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:48,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:48,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:49,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:49,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:50,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:50,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:50,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:51,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:51,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:51,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:52,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:52,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:52,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:53,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:53,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:53,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:54,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:54,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:54,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:55,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:55,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:56,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:56,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:56,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:57,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:57,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:57,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:58,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:58,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:58,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:59,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:59,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:59,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:00,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:00,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:00,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:01,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:01,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:01,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:02,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:02,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:02,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:03,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:03,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:03,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:04,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:04,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:04,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:05,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:05,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:05,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:06,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:06,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:06,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:07,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:07,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:07,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:08,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:08,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:09,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:09,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:09,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:10,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:10,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:10,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:11,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:11,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:11,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:11,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:12,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:12,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:13,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:13,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:13,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:14,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:14,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:14,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:15,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:15,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:15,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:16,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:16,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:16,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:17,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:17,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:18,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:18,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:18,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:19,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:19,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:19,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:20,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:20,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:20,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:21,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:21,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:21,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:22,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:22,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:22,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:23,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:23,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:23,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:24,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:24,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:24,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:25,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:25,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:25,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:26,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:26,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:26,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:27,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:27,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:27,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:28,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:28,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:28,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:29,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:29,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:29,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:31,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:31,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:31,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:32,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:32,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:33,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:33,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:33,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:34,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:34,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:34,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:35,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:35,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:35,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:36,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:36,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:36,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:37,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:37,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:37,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:38,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:38,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:38,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:39,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:39,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:39,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:39,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:40,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:40,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:40,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:41,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:41,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:41,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:42,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:42,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:42,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:43,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:43,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:43,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:44,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:44,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:44,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:45,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:45,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:45,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:46,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:46,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:46,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:46,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:48,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:48,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:48,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:49,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:49,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:49,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:50,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:50,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:50,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:51,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:51,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:51,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:52,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:52,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:52,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:53,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:53,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:53,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:54,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:54,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:54,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:55,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:55,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:55,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:56,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:56,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:56,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:57,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:57,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:57,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:58,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:58,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:59,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:59,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:59,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:00,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:00,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:00,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:00,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:01,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:01,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:02,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:02,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:02,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:03,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:03,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:03,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:04,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:04,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:04,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:05,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:05,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:05,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:06,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:06,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:06,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:07,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:07,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:08,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:08,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:08,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:09,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:09,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:09,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:10,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:10,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:10,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:11,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:11,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:11,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:12,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:12,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:13,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:13,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:13,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:13,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:14,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:14,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:15,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:15,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:15,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:16,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:16,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:17,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:17,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:17,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:18,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:18,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:18,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:19,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:19,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:19,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:20,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:20,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:22,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:22,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:22,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:23,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:23,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:23,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:24,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:24,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:24,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:25,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:25,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:25,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:26,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:26,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:26,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:27,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:27,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:27,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:27,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:28,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:28,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:28,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:29,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:29,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:30,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:30,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:30,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:31,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:31,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:31,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:33,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:33,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:34,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:34,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:34,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:35,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:35,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:35,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:36,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:36,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:37,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:37,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:37,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:38,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:38,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:38,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:39,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:39,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:39,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:39,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:40,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:40,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:40,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:41,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:41,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:42,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:43,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:43,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:43,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:44,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:44,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:44,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:45,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:45,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:45,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:46,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:46,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:46,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:47,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:47,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:47,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:48,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:48,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:48,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:49,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:49,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:50,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:50,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:50,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:51,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:51,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:51,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:52,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:52,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:52,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:52,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:53,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:53,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:53,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:54,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:54,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:54,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:54,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:56,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:56,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:56,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:57,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:57,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:57,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:58,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:58,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:58,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:00,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:00,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:00,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:01,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:01,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:02,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:02,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:02,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:03,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:03,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:03,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:04,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:04,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:04,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:05,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:05,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:05,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:05,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:06,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:06,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:06,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:07,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:07,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:07,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:08,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:08,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:08,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:09,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:09,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:09,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:10,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:10,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:10,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:11,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:11,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:11,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:12,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:12,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:12,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:14,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:14,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:15,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:15,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:15,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:17,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:17,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:17,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:18,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:18,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:18,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:19,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:19,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:19,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:19,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:20,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:20,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:21,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:21,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:21,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:22,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:22,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:22,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:23,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:23,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:23,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:24,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:24,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:24,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:25,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:25,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:25,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:26,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:26,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:26,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:27,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:27,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:28,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:28,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:28,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:29,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:29,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:30,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:30,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:30,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:32,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:32,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:32,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:33,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:34,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:34,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:34,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:35,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:35,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:35,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:36,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:36,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:36,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:37,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:37,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:37,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:38,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:38,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:38,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:38,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:39,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:39,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:41,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:41,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:41,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:43,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:43,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:43,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:44,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:44,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:44,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:45,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:45,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:45,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:46,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:46,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:46,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:47,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:47,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:47,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:47,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:48,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:48,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:48,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:50,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:50,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:50,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:51,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:51,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:51,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:52,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:52,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:52,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:53,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:53,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:53,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:54,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:54,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:55,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:55,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:55,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:56,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:56,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:57,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:57,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:57,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:58,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:58,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:58,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:59,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:59,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:59,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:00,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:00,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:00,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:01,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:01,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:01,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:01,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:02,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:02,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:02,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:03,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:03,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:03,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:04,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:04,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:04,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:04,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:05,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:05,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:07,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:07,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:07,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:08,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:08,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:09,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:09,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:10,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:10,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:10,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:11,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:11,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:11,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:12,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:12,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:12,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:13,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:13,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:13,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:15,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:15,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:15,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:16,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:16,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:17,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:17,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:17,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:18,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:18,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:18,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:19,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:19,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:19,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:20,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:20,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:21,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:21,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:21,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:22,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:22,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:22,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:23,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:23,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:23,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:24,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:24,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:24,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:25,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:25,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:25,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:26,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:26,800][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3001, device='cuda:0') eval_epoch_loss=tensor(0.2624, device='cuda:0') eval_epoch_acc=tensor(0.9366, device='cuda:0')
[2025-02-13 19:49:26,802][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 19:49:26,802][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 19:49:27,095][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_3566_loss_0.26242145895957947/model.pt
[2025-02-13 19:49:27,100][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 19:49:27,100][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.26242145895957947
[2025-02-13 19:49:27,101][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9365698099136353
[2025-02-13 19:49:27,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:27,543][root][INFO] - Training Epoch: 1/2, step 3566/7134 completed (loss: 0.4157836139202118, acc: 0.8823529481887817)
[2025-02-13 19:49:27,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:27,913][root][INFO] - Training Epoch: 1/2, step 3567/7134 completed (loss: 0.2216774970293045, acc: 0.9719101190567017)
[2025-02-13 19:49:28,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:28,297][root][INFO] - Training Epoch: 1/2, step 3568/7134 completed (loss: 0.27243974804878235, acc: 0.9219858050346375)
[2025-02-13 19:49:28,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:28,686][root][INFO] - Training Epoch: 1/2, step 3569/7134 completed (loss: 0.27217093110084534, acc: 0.9333333373069763)
[2025-02-13 19:49:28,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:29,071][root][INFO] - Training Epoch: 1/2, step 3570/7134 completed (loss: 0.43539342284202576, acc: 0.8897058963775635)
[2025-02-13 19:49:29,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:29,498][root][INFO] - Training Epoch: 1/2, step 3571/7134 completed (loss: 0.43513205647468567, acc: 0.915730357170105)
[2025-02-13 19:49:29,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:29,902][root][INFO] - Training Epoch: 1/2, step 3572/7134 completed (loss: 0.1904405653476715, acc: 0.9523809552192688)
[2025-02-13 19:49:30,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:30,243][root][INFO] - Training Epoch: 1/2, step 3573/7134 completed (loss: 0.25835123658180237, acc: 0.9276315569877625)
[2025-02-13 19:49:30,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:30,593][root][INFO] - Training Epoch: 1/2, step 3574/7134 completed (loss: 0.2605101764202118, acc: 0.942148745059967)
[2025-02-13 19:49:30,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:30,969][root][INFO] - Training Epoch: 1/2, step 3575/7134 completed (loss: 0.6439172625541687, acc: 0.8684210777282715)
[2025-02-13 19:49:31,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:31,322][root][INFO] - Training Epoch: 1/2, step 3576/7134 completed (loss: 0.24851815402507782, acc: 0.9104477763175964)
[2025-02-13 19:49:31,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:31,707][root][INFO] - Training Epoch: 1/2, step 3577/7134 completed (loss: 0.2637125253677368, acc: 0.938144326210022)
[2025-02-13 19:49:31,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:32,070][root][INFO] - Training Epoch: 1/2, step 3578/7134 completed (loss: 0.12669846415519714, acc: 0.9518072009086609)
[2025-02-13 19:49:32,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:32,452][root][INFO] - Training Epoch: 1/2, step 3579/7134 completed (loss: 0.29178762435913086, acc: 0.9244186282157898)
[2025-02-13 19:49:32,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:32,829][root][INFO] - Training Epoch: 1/2, step 3580/7134 completed (loss: 0.5197198987007141, acc: 0.8670520186424255)
[2025-02-13 19:49:32,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:33,215][root][INFO] - Training Epoch: 1/2, step 3581/7134 completed (loss: 0.4525062143802643, acc: 0.8647058606147766)
[2025-02-13 19:49:33,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:33,588][root][INFO] - Training Epoch: 1/2, step 3582/7134 completed (loss: 0.4567616581916809, acc: 0.9059829115867615)
[2025-02-13 19:49:33,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:33,961][root][INFO] - Training Epoch: 1/2, step 3583/7134 completed (loss: 0.4249507784843445, acc: 0.8922155499458313)
[2025-02-13 19:49:34,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:34,319][root][INFO] - Training Epoch: 1/2, step 3584/7134 completed (loss: 0.5160701870918274, acc: 0.8510638475418091)
[2025-02-13 19:49:34,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:34,713][root][INFO] - Training Epoch: 1/2, step 3585/7134 completed (loss: 0.31333449482917786, acc: 0.9325153231620789)
[2025-02-13 19:49:34,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:35,113][root][INFO] - Training Epoch: 1/2, step 3586/7134 completed (loss: 0.29622194170951843, acc: 0.9426751732826233)
[2025-02-13 19:49:35,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:35,469][root][INFO] - Training Epoch: 1/2, step 3587/7134 completed (loss: 0.29813626408576965, acc: 0.9230769276618958)
[2025-02-13 19:49:35,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:35,850][root][INFO] - Training Epoch: 1/2, step 3588/7134 completed (loss: 0.24979497492313385, acc: 0.9479768872261047)
[2025-02-13 19:49:36,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:36,231][root][INFO] - Training Epoch: 1/2, step 3589/7134 completed (loss: 0.28895482420921326, acc: 0.9617486596107483)
[2025-02-13 19:49:36,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:36,613][root][INFO] - Training Epoch: 1/2, step 3590/7134 completed (loss: 0.11722257733345032, acc: 0.9666666388511658)
[2025-02-13 19:49:36,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:36,972][root][INFO] - Training Epoch: 1/2, step 3591/7134 completed (loss: 0.2380778044462204, acc: 0.9473684430122375)
[2025-02-13 19:49:37,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:37,311][root][INFO] - Training Epoch: 1/2, step 3592/7134 completed (loss: 0.11310654133558273, acc: 0.949999988079071)
[2025-02-13 19:49:37,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:37,704][root][INFO] - Training Epoch: 1/2, step 3593/7134 completed (loss: 0.3511312007904053, acc: 0.9507042169570923)
[2025-02-13 19:49:37,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:38,066][root][INFO] - Training Epoch: 1/2, step 3594/7134 completed (loss: 0.15565598011016846, acc: 0.9668874144554138)
[2025-02-13 19:49:38,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:38,414][root][INFO] - Training Epoch: 1/2, step 3595/7134 completed (loss: 0.23553933203220367, acc: 0.9346405267715454)
[2025-02-13 19:49:38,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:38,822][root][INFO] - Training Epoch: 1/2, step 3596/7134 completed (loss: 0.2729257345199585, acc: 0.9235293865203857)
[2025-02-13 19:49:38,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:39,202][root][INFO] - Training Epoch: 1/2, step 3597/7134 completed (loss: 0.22199919819831848, acc: 0.9671052694320679)
[2025-02-13 19:49:39,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:39,597][root][INFO] - Training Epoch: 1/2, step 3598/7134 completed (loss: 0.3153175115585327, acc: 0.9052132964134216)
[2025-02-13 19:49:39,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:39,944][root][INFO] - Training Epoch: 1/2, step 3599/7134 completed (loss: 0.3298511505126953, acc: 0.908108115196228)
[2025-02-13 19:49:40,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:40,346][root][INFO] - Training Epoch: 1/2, step 3600/7134 completed (loss: 0.32739606499671936, acc: 0.9222797751426697)
[2025-02-13 19:49:40,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:40,713][root][INFO] - Training Epoch: 1/2, step 3601/7134 completed (loss: 0.761141300201416, acc: 0.8258426785469055)
[2025-02-13 19:49:40,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:41,104][root][INFO] - Training Epoch: 1/2, step 3602/7134 completed (loss: 0.4639393389225006, acc: 0.8930232524871826)
[2025-02-13 19:49:41,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:41,478][root][INFO] - Training Epoch: 1/2, step 3603/7134 completed (loss: 0.3234899640083313, acc: 0.893081784248352)
[2025-02-13 19:49:41,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:41,901][root][INFO] - Training Epoch: 1/2, step 3604/7134 completed (loss: 0.4015430510044098, acc: 0.8897637724876404)
[2025-02-13 19:49:42,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:42,273][root][INFO] - Training Epoch: 1/2, step 3605/7134 completed (loss: 0.2856444716453552, acc: 0.9282511472702026)
[2025-02-13 19:49:42,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:42,678][root][INFO] - Training Epoch: 1/2, step 3606/7134 completed (loss: 0.5066614151000977, acc: 0.9178082346916199)
[2025-02-13 19:49:42,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:43,044][root][INFO] - Training Epoch: 1/2, step 3607/7134 completed (loss: 0.24536104500293732, acc: 0.931506872177124)
[2025-02-13 19:49:43,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:43,413][root][INFO] - Training Epoch: 1/2, step 3608/7134 completed (loss: 0.38258957862854004, acc: 0.917475700378418)
[2025-02-13 19:49:43,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:43,838][root][INFO] - Training Epoch: 1/2, step 3609/7134 completed (loss: 0.2186596542596817, acc: 0.942105233669281)
[2025-02-13 19:49:43,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:44,211][root][INFO] - Training Epoch: 1/2, step 3610/7134 completed (loss: 0.28722578287124634, acc: 0.9230769276618958)
[2025-02-13 19:49:44,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:44,616][root][INFO] - Training Epoch: 1/2, step 3611/7134 completed (loss: 0.33475297689437866, acc: 0.9085714221000671)
[2025-02-13 19:49:44,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:44,969][root][INFO] - Training Epoch: 1/2, step 3612/7134 completed (loss: 0.4320392608642578, acc: 0.8965517282485962)
[2025-02-13 19:49:45,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:45,392][root][INFO] - Training Epoch: 1/2, step 3613/7134 completed (loss: 0.2628304958343506, acc: 0.9508196711540222)
[2025-02-13 19:49:45,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:45,829][root][INFO] - Training Epoch: 1/2, step 3614/7134 completed (loss: 0.2119118720293045, acc: 0.9414893388748169)
[2025-02-13 19:49:45,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:46,194][root][INFO] - Training Epoch: 1/2, step 3615/7134 completed (loss: 0.17458666861057281, acc: 0.9594594836235046)
[2025-02-13 19:49:46,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:46,579][root][INFO] - Training Epoch: 1/2, step 3616/7134 completed (loss: 0.3298493027687073, acc: 0.9006211161613464)
[2025-02-13 19:49:46,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:46,964][root][INFO] - Training Epoch: 1/2, step 3617/7134 completed (loss: 0.2037467062473297, acc: 0.9513513445854187)
[2025-02-13 19:49:47,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:47,382][root][INFO] - Training Epoch: 1/2, step 3618/7134 completed (loss: 0.3237858712673187, acc: 0.9386503100395203)
[2025-02-13 19:49:47,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:47,795][root][INFO] - Training Epoch: 1/2, step 3619/7134 completed (loss: 0.44631797075271606, acc: 0.8733624219894409)
[2025-02-13 19:49:47,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:48,203][root][INFO] - Training Epoch: 1/2, step 3620/7134 completed (loss: 0.25502490997314453, acc: 0.910179615020752)
[2025-02-13 19:49:48,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:48,551][root][INFO] - Training Epoch: 1/2, step 3621/7134 completed (loss: 0.20656666159629822, acc: 0.9363057613372803)
[2025-02-13 19:49:48,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:48,957][root][INFO] - Training Epoch: 1/2, step 3622/7134 completed (loss: 0.21089740097522736, acc: 0.9350649118423462)
[2025-02-13 19:49:49,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:49,339][root][INFO] - Training Epoch: 1/2, step 3623/7134 completed (loss: 0.20076029002666473, acc: 0.9285714030265808)
[2025-02-13 19:49:49,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:49,707][root][INFO] - Training Epoch: 1/2, step 3624/7134 completed (loss: 0.2644796073436737, acc: 0.9424460530281067)
[2025-02-13 19:49:49,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:50,074][root][INFO] - Training Epoch: 1/2, step 3625/7134 completed (loss: 0.464063435792923, acc: 0.8739495873451233)
[2025-02-13 19:49:50,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:50,454][root][INFO] - Training Epoch: 1/2, step 3626/7134 completed (loss: 0.5631890892982483, acc: 0.8651685118675232)
[2025-02-13 19:49:50,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:50,847][root][INFO] - Training Epoch: 1/2, step 3627/7134 completed (loss: 0.5368920564651489, acc: 0.8773584961891174)
[2025-02-13 19:49:50,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:51,237][root][INFO] - Training Epoch: 1/2, step 3628/7134 completed (loss: 0.36844387650489807, acc: 0.9142857193946838)
[2025-02-13 19:49:51,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:51,609][root][INFO] - Training Epoch: 1/2, step 3629/7134 completed (loss: 0.4883270859718323, acc: 0.8870967626571655)
[2025-02-13 19:49:51,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:51,946][root][INFO] - Training Epoch: 1/2, step 3630/7134 completed (loss: 0.2889515161514282, acc: 0.9462365508079529)
[2025-02-13 19:49:52,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:52,327][root][INFO] - Training Epoch: 1/2, step 3631/7134 completed (loss: 0.2915935218334198, acc: 0.8999999761581421)
[2025-02-13 19:49:52,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:52,715][root][INFO] - Training Epoch: 1/2, step 3632/7134 completed (loss: 0.23384135961532593, acc: 0.9672130942344666)
[2025-02-13 19:49:52,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:53,081][root][INFO] - Training Epoch: 1/2, step 3633/7134 completed (loss: 0.29952701926231384, acc: 0.9426751732826233)
[2025-02-13 19:49:53,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:53,440][root][INFO] - Training Epoch: 1/2, step 3634/7134 completed (loss: 0.2692320644855499, acc: 0.9439252614974976)
[2025-02-13 19:49:53,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:53,790][root][INFO] - Training Epoch: 1/2, step 3635/7134 completed (loss: 0.21055684983730316, acc: 0.9396551847457886)
[2025-02-13 19:49:53,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:54,155][root][INFO] - Training Epoch: 1/2, step 3636/7134 completed (loss: 0.1493813842535019, acc: 0.9528301954269409)
[2025-02-13 19:49:54,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:54,537][root][INFO] - Training Epoch: 1/2, step 3637/7134 completed (loss: 0.2717360556125641, acc: 0.9363636374473572)
[2025-02-13 19:49:54,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:54,874][root][INFO] - Training Epoch: 1/2, step 3638/7134 completed (loss: 0.18891841173171997, acc: 0.9230769276618958)
[2025-02-13 19:49:55,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:55,237][root][INFO] - Training Epoch: 1/2, step 3639/7134 completed (loss: 0.3093409836292267, acc: 0.957446813583374)
[2025-02-13 19:49:55,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:55,614][root][INFO] - Training Epoch: 1/2, step 3640/7134 completed (loss: 0.2640930116176605, acc: 0.942148745059967)
[2025-02-13 19:49:55,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:55,964][root][INFO] - Training Epoch: 1/2, step 3641/7134 completed (loss: 0.11205729097127914, acc: 0.975806474685669)
[2025-02-13 19:49:56,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:56,361][root][INFO] - Training Epoch: 1/2, step 3642/7134 completed (loss: 0.06794106960296631, acc: 0.9798657894134521)
[2025-02-13 19:49:56,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:56,759][root][INFO] - Training Epoch: 1/2, step 3643/7134 completed (loss: 0.158453568816185, acc: 0.9453125)
[2025-02-13 19:49:56,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:57,165][root][INFO] - Training Epoch: 1/2, step 3644/7134 completed (loss: 0.27456891536712646, acc: 0.9324324131011963)
[2025-02-13 19:49:57,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:57,554][root][INFO] - Training Epoch: 1/2, step 3645/7134 completed (loss: 0.45984411239624023, acc: 0.8984375)
[2025-02-13 19:49:57,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:57,946][root][INFO] - Training Epoch: 1/2, step 3646/7134 completed (loss: 0.4993935227394104, acc: 0.9015151262283325)
[2025-02-13 19:49:58,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:58,305][root][INFO] - Training Epoch: 1/2, step 3647/7134 completed (loss: 0.17869889736175537, acc: 0.9550561904907227)
[2025-02-13 19:49:58,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:58,691][root][INFO] - Training Epoch: 1/2, step 3648/7134 completed (loss: 0.30359870195388794, acc: 0.9053254723548889)
[2025-02-13 19:49:58,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:59,084][root][INFO] - Training Epoch: 1/2, step 3649/7134 completed (loss: 0.35150378942489624, acc: 0.9194630980491638)
[2025-02-13 19:49:59,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:59,470][root][INFO] - Training Epoch: 1/2, step 3650/7134 completed (loss: 0.20447036623954773, acc: 0.9349112510681152)
[2025-02-13 19:49:59,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:59,863][root][INFO] - Training Epoch: 1/2, step 3651/7134 completed (loss: 0.3794742822647095, acc: 0.891566276550293)
[2025-02-13 19:50:00,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:00,304][root][INFO] - Training Epoch: 1/2, step 3652/7134 completed (loss: 0.266706645488739, acc: 0.9387755393981934)
[2025-02-13 19:50:00,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:00,696][root][INFO] - Training Epoch: 1/2, step 3653/7134 completed (loss: 0.4834686517715454, acc: 0.8711656332015991)
[2025-02-13 19:50:00,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:01,113][root][INFO] - Training Epoch: 1/2, step 3654/7134 completed (loss: 0.0929996445775032, acc: 0.9599999785423279)
[2025-02-13 19:50:01,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:01,508][root][INFO] - Training Epoch: 1/2, step 3655/7134 completed (loss: 0.49696916341781616, acc: 0.8982036113739014)
[2025-02-13 19:50:01,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:01,866][root][INFO] - Training Epoch: 1/2, step 3656/7134 completed (loss: 0.43622809648513794, acc: 0.9202898740768433)
[2025-02-13 19:50:02,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:02,235][root][INFO] - Training Epoch: 1/2, step 3657/7134 completed (loss: 0.17051918804645538, acc: 0.9440559148788452)
[2025-02-13 19:50:02,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:02,604][root][INFO] - Training Epoch: 1/2, step 3658/7134 completed (loss: 0.25555044412612915, acc: 0.9375)
[2025-02-13 19:50:02,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:02,999][root][INFO] - Training Epoch: 1/2, step 3659/7134 completed (loss: 0.29555144906044006, acc: 0.9251700639724731)
[2025-02-13 19:50:03,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:03,371][root][INFO] - Training Epoch: 1/2, step 3660/7134 completed (loss: 0.5706990957260132, acc: 0.8024691343307495)
[2025-02-13 19:50:03,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:03,782][root][INFO] - Training Epoch: 1/2, step 3661/7134 completed (loss: 0.2188548892736435, acc: 0.9428571462631226)
[2025-02-13 19:50:03,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:04,164][root][INFO] - Training Epoch: 1/2, step 3662/7134 completed (loss: 0.4369637072086334, acc: 0.888198733329773)
[2025-02-13 19:50:04,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:04,568][root][INFO] - Training Epoch: 1/2, step 3663/7134 completed (loss: 0.35741615295410156, acc: 0.8984375)
[2025-02-13 19:50:04,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:04,939][root][INFO] - Training Epoch: 1/2, step 3664/7134 completed (loss: 0.45994964241981506, acc: 0.8766233921051025)
[2025-02-13 19:50:05,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:05,318][root][INFO] - Training Epoch: 1/2, step 3665/7134 completed (loss: 0.19239194691181183, acc: 0.9527027010917664)
[2025-02-13 19:50:05,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:05,688][root][INFO] - Training Epoch: 1/2, step 3666/7134 completed (loss: 0.39827197790145874, acc: 0.8972602486610413)
[2025-02-13 19:50:05,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:06,083][root][INFO] - Training Epoch: 1/2, step 3667/7134 completed (loss: 0.847658634185791, acc: 0.8294573426246643)
[2025-02-13 19:50:06,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:06,450][root][INFO] - Training Epoch: 1/2, step 3668/7134 completed (loss: 0.3774353265762329, acc: 0.9103448390960693)
[2025-02-13 19:50:06,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:06,811][root][INFO] - Training Epoch: 1/2, step 3669/7134 completed (loss: 0.13019882142543793, acc: 0.9752066135406494)
[2025-02-13 19:50:06,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:07,223][root][INFO] - Training Epoch: 1/2, step 3670/7134 completed (loss: 0.08883427083492279, acc: 0.9879518151283264)
[2025-02-13 19:50:07,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:07,618][root][INFO] - Training Epoch: 1/2, step 3671/7134 completed (loss: 0.12686489522457123, acc: 0.9683544039726257)
[2025-02-13 19:50:07,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:08,009][root][INFO] - Training Epoch: 1/2, step 3672/7134 completed (loss: 0.16250312328338623, acc: 0.9613259434700012)
[2025-02-13 19:50:08,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:08,372][root][INFO] - Training Epoch: 1/2, step 3673/7134 completed (loss: 0.22051545977592468, acc: 0.931034505367279)
[2025-02-13 19:50:08,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:08,749][root][INFO] - Training Epoch: 1/2, step 3674/7134 completed (loss: 0.08496265113353729, acc: 0.9864864945411682)
[2025-02-13 19:50:08,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:09,104][root][INFO] - Training Epoch: 1/2, step 3675/7134 completed (loss: 0.2062276303768158, acc: 0.950276255607605)
[2025-02-13 19:50:09,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:09,488][root][INFO] - Training Epoch: 1/2, step 3676/7134 completed (loss: 0.21713337302207947, acc: 0.942307710647583)
[2025-02-13 19:50:09,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:09,839][root][INFO] - Training Epoch: 1/2, step 3677/7134 completed (loss: 0.2660823166370392, acc: 0.9261363744735718)
[2025-02-13 19:50:09,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:10,226][root][INFO] - Training Epoch: 1/2, step 3678/7134 completed (loss: 0.430003821849823, acc: 0.9303797483444214)
[2025-02-13 19:50:10,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:10,598][root][INFO] - Training Epoch: 1/2, step 3679/7134 completed (loss: 0.17879697680473328, acc: 0.9538461565971375)
[2025-02-13 19:50:10,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:10,982][root][INFO] - Training Epoch: 1/2, step 3680/7134 completed (loss: 0.2753315269947052, acc: 0.9399999976158142)
[2025-02-13 19:50:11,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:11,358][root][INFO] - Training Epoch: 1/2, step 3681/7134 completed (loss: 0.20878663659095764, acc: 0.9437500238418579)
[2025-02-13 19:50:11,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:11,736][root][INFO] - Training Epoch: 1/2, step 3682/7134 completed (loss: 0.09414522349834442, acc: 0.9642857313156128)
[2025-02-13 19:50:11,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:12,146][root][INFO] - Training Epoch: 1/2, step 3683/7134 completed (loss: 0.10331302881240845, acc: 0.9852941036224365)
[2025-02-13 19:50:12,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:12,530][root][INFO] - Training Epoch: 1/2, step 3684/7134 completed (loss: 0.07015475630760193, acc: 0.9934210777282715)
[2025-02-13 19:50:12,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:12,917][root][INFO] - Training Epoch: 1/2, step 3685/7134 completed (loss: 0.16811126470565796, acc: 0.9430894255638123)
[2025-02-13 19:50:13,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:13,269][root][INFO] - Training Epoch: 1/2, step 3686/7134 completed (loss: 0.13931822776794434, acc: 0.9577465057373047)
[2025-02-13 19:50:13,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:13,671][root][INFO] - Training Epoch: 1/2, step 3687/7134 completed (loss: 0.14523455500602722, acc: 0.95652174949646)
[2025-02-13 19:50:13,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:14,058][root][INFO] - Training Epoch: 1/2, step 3688/7134 completed (loss: 0.08652730286121368, acc: 0.9833333492279053)
[2025-02-13 19:50:14,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:14,431][root][INFO] - Training Epoch: 1/2, step 3689/7134 completed (loss: 0.17383216321468353, acc: 0.9503546357154846)
[2025-02-13 19:50:14,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:14,809][root][INFO] - Training Epoch: 1/2, step 3690/7134 completed (loss: 0.07525627315044403, acc: 0.9873417615890503)
[2025-02-13 19:50:14,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:15,157][root][INFO] - Training Epoch: 1/2, step 3691/7134 completed (loss: 0.18312382698059082, acc: 0.9504950642585754)
[2025-02-13 19:50:15,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:15,518][root][INFO] - Training Epoch: 1/2, step 3692/7134 completed (loss: 0.1537209004163742, acc: 0.9693251252174377)
[2025-02-13 19:50:15,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:15,913][root][INFO] - Training Epoch: 1/2, step 3693/7134 completed (loss: 0.09005177021026611, acc: 0.9924242496490479)
[2025-02-13 19:50:16,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:16,316][root][INFO] - Training Epoch: 1/2, step 3694/7134 completed (loss: 0.2269945591688156, acc: 0.9437500238418579)
[2025-02-13 19:50:16,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:16,677][root][INFO] - Training Epoch: 1/2, step 3695/7134 completed (loss: 0.1407739818096161, acc: 0.9617834687232971)
[2025-02-13 19:50:16,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:17,042][root][INFO] - Training Epoch: 1/2, step 3696/7134 completed (loss: 0.12671013176441193, acc: 0.977011501789093)
[2025-02-13 19:50:17,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:17,392][root][INFO] - Training Epoch: 1/2, step 3697/7134 completed (loss: 0.11962714046239853, acc: 0.9756097793579102)
[2025-02-13 19:50:17,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:17,768][root][INFO] - Training Epoch: 1/2, step 3698/7134 completed (loss: 0.08937563747167587, acc: 0.9724137783050537)
[2025-02-13 19:50:17,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:18,115][root][INFO] - Training Epoch: 1/2, step 3699/7134 completed (loss: 0.08980732411146164, acc: 0.9842519760131836)
[2025-02-13 19:50:18,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:18,508][root][INFO] - Training Epoch: 1/2, step 3700/7134 completed (loss: 0.056269776076078415, acc: 0.9937106966972351)
[2025-02-13 19:50:18,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:18,893][root][INFO] - Training Epoch: 1/2, step 3701/7134 completed (loss: 0.1854456514120102, acc: 0.9635036587715149)
[2025-02-13 19:50:19,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:19,291][root][INFO] - Training Epoch: 1/2, step 3702/7134 completed (loss: 0.1474934071302414, acc: 0.9818181991577148)
[2025-02-13 19:50:19,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:19,666][root][INFO] - Training Epoch: 1/2, step 3703/7134 completed (loss: 0.11709573864936829, acc: 0.9772727489471436)
[2025-02-13 19:50:19,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:20,053][root][INFO] - Training Epoch: 1/2, step 3704/7134 completed (loss: 0.12661056220531464, acc: 0.9801980257034302)
[2025-02-13 19:50:20,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:20,420][root][INFO] - Training Epoch: 1/2, step 3705/7134 completed (loss: 0.32862389087677, acc: 0.9263803958892822)
[2025-02-13 19:50:20,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:20,775][root][INFO] - Training Epoch: 1/2, step 3706/7134 completed (loss: 0.3836601674556732, acc: 0.9108911156654358)
[2025-02-13 19:50:20,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:21,151][root][INFO] - Training Epoch: 1/2, step 3707/7134 completed (loss: 0.10924580693244934, acc: 0.9866666793823242)
[2025-02-13 19:50:21,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:21,530][root][INFO] - Training Epoch: 1/2, step 3708/7134 completed (loss: 0.23977111279964447, acc: 0.9384615421295166)
[2025-02-13 19:50:21,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:21,911][root][INFO] - Training Epoch: 1/2, step 3709/7134 completed (loss: 0.14596319198608398, acc: 0.9583333134651184)
[2025-02-13 19:50:22,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:22,287][root][INFO] - Training Epoch: 1/2, step 3710/7134 completed (loss: 0.18728119134902954, acc: 0.9537037014961243)
[2025-02-13 19:50:22,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:22,652][root][INFO] - Training Epoch: 1/2, step 3711/7134 completed (loss: 0.19958876073360443, acc: 0.9505494236946106)
[2025-02-13 19:50:22,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:23,028][root][INFO] - Training Epoch: 1/2, step 3712/7134 completed (loss: 0.3353438675403595, acc: 0.907608687877655)
[2025-02-13 19:50:23,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:23,405][root][INFO] - Training Epoch: 1/2, step 3713/7134 completed (loss: 0.13579118251800537, acc: 0.9496402740478516)
[2025-02-13 19:50:23,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:23,789][root][INFO] - Training Epoch: 1/2, step 3714/7134 completed (loss: 0.22844399511814117, acc: 0.9378238320350647)
[2025-02-13 19:50:23,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:24,194][root][INFO] - Training Epoch: 1/2, step 3715/7134 completed (loss: 0.14955885708332062, acc: 0.957446813583374)
[2025-02-13 19:50:24,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:24,592][root][INFO] - Training Epoch: 1/2, step 3716/7134 completed (loss: 0.3446536064147949, acc: 0.9238578677177429)
[2025-02-13 19:50:24,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:24,973][root][INFO] - Training Epoch: 1/2, step 3717/7134 completed (loss: 0.14941191673278809, acc: 0.9634146094322205)
[2025-02-13 19:50:25,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:25,395][root][INFO] - Training Epoch: 1/2, step 3718/7134 completed (loss: 0.21016494929790497, acc: 0.953125)
[2025-02-13 19:50:25,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:25,780][root][INFO] - Training Epoch: 1/2, step 3719/7134 completed (loss: 0.22549034655094147, acc: 0.9415204524993896)
[2025-02-13 19:50:25,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:26,152][root][INFO] - Training Epoch: 1/2, step 3720/7134 completed (loss: 0.06356953084468842, acc: 0.9810126423835754)
[2025-02-13 19:50:26,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:26,538][root][INFO] - Training Epoch: 1/2, step 3721/7134 completed (loss: 0.15611301362514496, acc: 0.9479768872261047)
[2025-02-13 19:50:26,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:26,917][root][INFO] - Training Epoch: 1/2, step 3722/7134 completed (loss: 0.16244754195213318, acc: 0.9430052042007446)
[2025-02-13 19:50:27,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:27,301][root][INFO] - Training Epoch: 1/2, step 3723/7134 completed (loss: 0.18688000738620758, acc: 0.9677419066429138)
[2025-02-13 19:50:27,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:27,682][root][INFO] - Training Epoch: 1/2, step 3724/7134 completed (loss: 0.12469418346881866, acc: 0.9759036302566528)
[2025-02-13 19:50:27,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:28,061][root][INFO] - Training Epoch: 1/2, step 3725/7134 completed (loss: 0.3894089162349701, acc: 0.9390243887901306)
[2025-02-13 19:50:28,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:28,437][root][INFO] - Training Epoch: 1/2, step 3726/7134 completed (loss: 0.1327686905860901, acc: 0.9497487545013428)
[2025-02-13 19:50:28,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:28,802][root][INFO] - Training Epoch: 1/2, step 3727/7134 completed (loss: 0.15766094624996185, acc: 0.9653179049491882)
[2025-02-13 19:50:28,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:29,184][root][INFO] - Training Epoch: 1/2, step 3728/7134 completed (loss: 0.08381742238998413, acc: 0.9802955389022827)
[2025-02-13 19:50:29,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:29,575][root][INFO] - Training Epoch: 1/2, step 3729/7134 completed (loss: 0.147000253200531, acc: 0.9622641801834106)
[2025-02-13 19:50:29,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:29,951][root][INFO] - Training Epoch: 1/2, step 3730/7134 completed (loss: 0.1963372677564621, acc: 0.9454545378684998)
[2025-02-13 19:50:30,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:30,321][root][INFO] - Training Epoch: 1/2, step 3731/7134 completed (loss: 0.08286836743354797, acc: 0.9712643623352051)
[2025-02-13 19:50:30,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:30,705][root][INFO] - Training Epoch: 1/2, step 3732/7134 completed (loss: 0.11961537599563599, acc: 0.964102566242218)
[2025-02-13 19:50:30,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:31,080][root][INFO] - Training Epoch: 1/2, step 3733/7134 completed (loss: 0.203172504901886, acc: 0.9503105878829956)
[2025-02-13 19:50:31,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:31,426][root][INFO] - Training Epoch: 1/2, step 3734/7134 completed (loss: 0.3953714966773987, acc: 0.8974359035491943)
[2025-02-13 19:50:31,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:31,805][root][INFO] - Training Epoch: 1/2, step 3735/7134 completed (loss: 0.1943337470293045, acc: 0.9545454382896423)
[2025-02-13 19:50:31,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:32,214][root][INFO] - Training Epoch: 1/2, step 3736/7134 completed (loss: 0.2532126009464264, acc: 0.9411764740943909)
[2025-02-13 19:50:32,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:32,642][root][INFO] - Training Epoch: 1/2, step 3737/7134 completed (loss: 0.21721847355365753, acc: 0.9333333373069763)
[2025-02-13 19:50:32,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:33,047][root][INFO] - Training Epoch: 1/2, step 3738/7134 completed (loss: 0.09669402241706848, acc: 0.9722222089767456)
[2025-02-13 19:50:33,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:33,436][root][INFO] - Training Epoch: 1/2, step 3739/7134 completed (loss: 0.1781357228755951, acc: 0.957446813583374)
[2025-02-13 19:50:33,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:33,911][root][INFO] - Training Epoch: 1/2, step 3740/7134 completed (loss: 0.17607919871807098, acc: 0.9490445852279663)
[2025-02-13 19:50:34,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:34,388][root][INFO] - Training Epoch: 1/2, step 3741/7134 completed (loss: 0.13824625313282013, acc: 0.977011501789093)
[2025-02-13 19:50:34,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:34,805][root][INFO] - Training Epoch: 1/2, step 3742/7134 completed (loss: 0.23225371539592743, acc: 0.9591836929321289)
[2025-02-13 19:50:34,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:35,171][root][INFO] - Training Epoch: 1/2, step 3743/7134 completed (loss: 0.21382610499858856, acc: 0.9352940917015076)
[2025-02-13 19:50:35,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:35,599][root][INFO] - Training Epoch: 1/2, step 3744/7134 completed (loss: 0.24047155678272247, acc: 0.9418604373931885)
[2025-02-13 19:50:35,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:36,012][root][INFO] - Training Epoch: 1/2, step 3745/7134 completed (loss: 0.2250135838985443, acc: 0.9428571462631226)
[2025-02-13 19:50:36,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:36,418][root][INFO] - Training Epoch: 1/2, step 3746/7134 completed (loss: 0.16032278537750244, acc: 0.9604519605636597)
[2025-02-13 19:50:36,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:36,801][root][INFO] - Training Epoch: 1/2, step 3747/7134 completed (loss: 0.17329645156860352, acc: 0.9388889074325562)
[2025-02-13 19:50:36,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:37,162][root][INFO] - Training Epoch: 1/2, step 3748/7134 completed (loss: 0.18978512287139893, acc: 0.9333333373069763)
[2025-02-13 19:50:37,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:37,532][root][INFO] - Training Epoch: 1/2, step 3749/7134 completed (loss: 0.3540635108947754, acc: 0.9107142686843872)
[2025-02-13 19:50:37,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:37,918][root][INFO] - Training Epoch: 1/2, step 3750/7134 completed (loss: 0.07158232480287552, acc: 0.9754601120948792)
[2025-02-13 19:50:38,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:38,362][root][INFO] - Training Epoch: 1/2, step 3751/7134 completed (loss: 0.19723588228225708, acc: 0.9518072009086609)
[2025-02-13 19:50:38,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:38,814][root][INFO] - Training Epoch: 1/2, step 3752/7134 completed (loss: 0.2633703052997589, acc: 0.9371069073677063)
[2025-02-13 19:50:38,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:39,231][root][INFO] - Training Epoch: 1/2, step 3753/7134 completed (loss: 0.3436805009841919, acc: 0.9018405079841614)
[2025-02-13 19:50:39,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:39,600][root][INFO] - Training Epoch: 1/2, step 3754/7134 completed (loss: 0.06976291537284851, acc: 0.9741935729980469)
[2025-02-13 19:50:39,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:39,970][root][INFO] - Training Epoch: 1/2, step 3755/7134 completed (loss: 0.18744125962257385, acc: 0.9504132270812988)
[2025-02-13 19:50:40,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:40,347][root][INFO] - Training Epoch: 1/2, step 3756/7134 completed (loss: 0.161685049533844, acc: 0.9537572264671326)
[2025-02-13 19:50:40,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:40,722][root][INFO] - Training Epoch: 1/2, step 3757/7134 completed (loss: 0.28658097982406616, acc: 0.9285714030265808)
[2025-02-13 19:50:40,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:41,134][root][INFO] - Training Epoch: 1/2, step 3758/7134 completed (loss: 0.2621488869190216, acc: 0.9370078444480896)
[2025-02-13 19:50:41,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:41,503][root][INFO] - Training Epoch: 1/2, step 3759/7134 completed (loss: 0.191448375582695, acc: 0.9440559148788452)
[2025-02-13 19:50:41,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:41,920][root][INFO] - Training Epoch: 1/2, step 3760/7134 completed (loss: 0.22922809422016144, acc: 0.9390243887901306)
[2025-02-13 19:50:42,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:42,289][root][INFO] - Training Epoch: 1/2, step 3761/7134 completed (loss: 0.10782888531684875, acc: 0.9696969985961914)
[2025-02-13 19:50:42,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:42,710][root][INFO] - Training Epoch: 1/2, step 3762/7134 completed (loss: 0.1707163006067276, acc: 0.9342105388641357)
[2025-02-13 19:50:42,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:43,070][root][INFO] - Training Epoch: 1/2, step 3763/7134 completed (loss: 0.07890628278255463, acc: 0.9857142567634583)
[2025-02-13 19:50:43,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:43,446][root][INFO] - Training Epoch: 1/2, step 3764/7134 completed (loss: 0.19970308244228363, acc: 0.946107804775238)
[2025-02-13 19:50:43,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:43,870][root][INFO] - Training Epoch: 1/2, step 3765/7134 completed (loss: 0.12897120416164398, acc: 0.9659090638160706)
[2025-02-13 19:50:43,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:44,256][root][INFO] - Training Epoch: 1/2, step 3766/7134 completed (loss: 0.217833012342453, acc: 0.9395973086357117)
[2025-02-13 19:50:44,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:44,670][root][INFO] - Training Epoch: 1/2, step 3767/7134 completed (loss: 0.17645230889320374, acc: 0.9608938694000244)
[2025-02-13 19:50:44,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:45,086][root][INFO] - Training Epoch: 1/2, step 3768/7134 completed (loss: 0.2234978824853897, acc: 0.942307710647583)
[2025-02-13 19:50:45,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:45,556][root][INFO] - Training Epoch: 1/2, step 3769/7134 completed (loss: 0.1983477920293808, acc: 0.929411768913269)
[2025-02-13 19:50:45,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:45,978][root][INFO] - Training Epoch: 1/2, step 3770/7134 completed (loss: 0.14393529295921326, acc: 0.9597315192222595)
[2025-02-13 19:50:46,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:46,360][root][INFO] - Training Epoch: 1/2, step 3771/7134 completed (loss: 0.06966352462768555, acc: 0.9868420958518982)
[2025-02-13 19:50:46,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:46,741][root][INFO] - Training Epoch: 1/2, step 3772/7134 completed (loss: 0.17310865223407745, acc: 0.949999988079071)
[2025-02-13 19:50:46,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:47,135][root][INFO] - Training Epoch: 1/2, step 3773/7134 completed (loss: 0.1532045155763626, acc: 0.9627659320831299)
[2025-02-13 19:50:47,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:47,513][root][INFO] - Training Epoch: 1/2, step 3774/7134 completed (loss: 0.1894758939743042, acc: 0.9450549483299255)
[2025-02-13 19:50:47,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:47,925][root][INFO] - Training Epoch: 1/2, step 3775/7134 completed (loss: 0.16193687915802002, acc: 0.9440993666648865)
[2025-02-13 19:50:48,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:48,314][root][INFO] - Training Epoch: 1/2, step 3776/7134 completed (loss: 0.2700745165348053, acc: 0.9358974099159241)
[2025-02-13 19:50:48,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:48,681][root][INFO] - Training Epoch: 1/2, step 3777/7134 completed (loss: 0.1312028169631958, acc: 0.970588207244873)
[2025-02-13 19:50:48,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:49,071][root][INFO] - Training Epoch: 1/2, step 3778/7134 completed (loss: 0.19184543192386627, acc: 0.9593023061752319)
[2025-02-13 19:50:49,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:49,452][root][INFO] - Training Epoch: 1/2, step 3779/7134 completed (loss: 0.19516687095165253, acc: 0.9437500238418579)
[2025-02-13 19:50:49,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:49,824][root][INFO] - Training Epoch: 1/2, step 3780/7134 completed (loss: 0.11352987587451935, acc: 0.9757575988769531)
[2025-02-13 19:50:49,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:50,184][root][INFO] - Training Epoch: 1/2, step 3781/7134 completed (loss: 0.30454501509666443, acc: 0.9254658222198486)
[2025-02-13 19:50:50,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:50,639][root][INFO] - Training Epoch: 1/2, step 3782/7134 completed (loss: 0.1801806092262268, acc: 0.955974817276001)
[2025-02-13 19:50:50,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:51,063][root][INFO] - Training Epoch: 1/2, step 3783/7134 completed (loss: 0.23479817807674408, acc: 0.9526627063751221)
[2025-02-13 19:50:51,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:51,511][root][INFO] - Training Epoch: 1/2, step 3784/7134 completed (loss: 0.20147496461868286, acc: 0.9597315192222595)
[2025-02-13 19:50:51,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:51,899][root][INFO] - Training Epoch: 1/2, step 3785/7134 completed (loss: 0.14607509970664978, acc: 0.978723406791687)
[2025-02-13 19:50:52,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:52,342][root][INFO] - Training Epoch: 1/2, step 3786/7134 completed (loss: 0.12240658700466156, acc: 0.9702380895614624)
[2025-02-13 19:50:52,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:52,743][root][INFO] - Training Epoch: 1/2, step 3787/7134 completed (loss: 0.14789526164531708, acc: 0.9538461565971375)
[2025-02-13 19:50:52,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:53,150][root][INFO] - Training Epoch: 1/2, step 3788/7134 completed (loss: 0.31131672859191895, acc: 0.9325153231620789)
[2025-02-13 19:50:53,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:53,523][root][INFO] - Training Epoch: 1/2, step 3789/7134 completed (loss: 0.2711270749568939, acc: 0.929411768913269)
[2025-02-13 19:50:53,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:53,918][root][INFO] - Training Epoch: 1/2, step 3790/7134 completed (loss: 0.395153671503067, acc: 0.9117646813392639)
[2025-02-13 19:50:54,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:54,291][root][INFO] - Training Epoch: 1/2, step 3791/7134 completed (loss: 0.1562625616788864, acc: 0.9595375657081604)
[2025-02-13 19:50:54,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:54,709][root][INFO] - Training Epoch: 1/2, step 3792/7134 completed (loss: 0.18711787462234497, acc: 0.932330846786499)
[2025-02-13 19:50:54,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:55,104][root][INFO] - Training Epoch: 1/2, step 3793/7134 completed (loss: 0.17181265354156494, acc: 0.959770143032074)
[2025-02-13 19:50:55,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:55,458][root][INFO] - Training Epoch: 1/2, step 3794/7134 completed (loss: 0.35548847913742065, acc: 0.9277777671813965)
[2025-02-13 19:50:55,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:55,846][root][INFO] - Training Epoch: 1/2, step 3795/7134 completed (loss: 0.23216789960861206, acc: 0.949999988079071)
[2025-02-13 19:50:55,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:56,209][root][INFO] - Training Epoch: 1/2, step 3796/7134 completed (loss: 0.25098979473114014, acc: 0.9181286692619324)
[2025-02-13 19:50:56,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:56,573][root][INFO] - Training Epoch: 1/2, step 3797/7134 completed (loss: 0.12189875543117523, acc: 0.9575757384300232)
[2025-02-13 19:50:56,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:56,957][root][INFO] - Training Epoch: 1/2, step 3798/7134 completed (loss: 0.4236348867416382, acc: 0.9182389974594116)
[2025-02-13 19:50:57,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:57,399][root][INFO] - Training Epoch: 1/2, step 3799/7134 completed (loss: 0.4032515585422516, acc: 0.9209039807319641)
[2025-02-13 19:50:57,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:57,829][root][INFO] - Training Epoch: 1/2, step 3800/7134 completed (loss: 0.39545053243637085, acc: 0.9172413945198059)
[2025-02-13 19:50:57,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:58,224][root][INFO] - Training Epoch: 1/2, step 3801/7134 completed (loss: 0.1757918894290924, acc: 0.950276255607605)
[2025-02-13 19:50:58,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:58,607][root][INFO] - Training Epoch: 1/2, step 3802/7134 completed (loss: 0.21438287198543549, acc: 0.9427083134651184)
[2025-02-13 19:50:58,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:58,998][root][INFO] - Training Epoch: 1/2, step 3803/7134 completed (loss: 0.286024808883667, acc: 0.9235668778419495)
[2025-02-13 19:50:59,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:59,395][root][INFO] - Training Epoch: 1/2, step 3804/7134 completed (loss: 0.2551328241825104, acc: 0.9647887349128723)
[2025-02-13 19:50:59,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:59,813][root][INFO] - Training Epoch: 1/2, step 3805/7134 completed (loss: 0.22868028283119202, acc: 0.954285740852356)
[2025-02-13 19:50:59,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:00,264][root][INFO] - Training Epoch: 1/2, step 3806/7134 completed (loss: 0.21943353116512299, acc: 0.9444444179534912)
[2025-02-13 19:51:00,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:00,665][root][INFO] - Training Epoch: 1/2, step 3807/7134 completed (loss: 0.14151374995708466, acc: 0.9589040875434875)
[2025-02-13 19:51:00,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:01,049][root][INFO] - Training Epoch: 1/2, step 3808/7134 completed (loss: 0.27522704005241394, acc: 0.9347826242446899)
[2025-02-13 19:51:01,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:01,407][root][INFO] - Training Epoch: 1/2, step 3809/7134 completed (loss: 0.2027355283498764, acc: 0.9651162624359131)
[2025-02-13 19:51:01,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:01,773][root][INFO] - Training Epoch: 1/2, step 3810/7134 completed (loss: 0.18504054844379425, acc: 0.9415584206581116)
[2025-02-13 19:51:01,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:02,143][root][INFO] - Training Epoch: 1/2, step 3811/7134 completed (loss: 0.2971399128437042, acc: 0.940119743347168)
[2025-02-13 19:51:02,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:02,518][root][INFO] - Training Epoch: 1/2, step 3812/7134 completed (loss: 0.4838520884513855, acc: 0.8911564350128174)
[2025-02-13 19:51:02,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:02,861][root][INFO] - Training Epoch: 1/2, step 3813/7134 completed (loss: 0.42301511764526367, acc: 0.9189189076423645)
[2025-02-13 19:51:03,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:03,220][root][INFO] - Training Epoch: 1/2, step 3814/7134 completed (loss: 0.12930840253829956, acc: 0.977142870426178)
[2025-02-13 19:51:03,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:03,549][root][INFO] - Training Epoch: 1/2, step 3815/7134 completed (loss: 0.20097312331199646, acc: 0.9539473652839661)
[2025-02-13 19:51:03,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:03,922][root][INFO] - Training Epoch: 1/2, step 3816/7134 completed (loss: 0.1530488133430481, acc: 0.9626865386962891)
[2025-02-13 19:51:04,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:04,286][root][INFO] - Training Epoch: 1/2, step 3817/7134 completed (loss: 0.28937339782714844, acc: 0.9101123809814453)
[2025-02-13 19:51:04,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:04,689][root][INFO] - Training Epoch: 1/2, step 3818/7134 completed (loss: 0.360821932554245, acc: 0.9210526347160339)
[2025-02-13 19:51:04,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:05,119][root][INFO] - Training Epoch: 1/2, step 3819/7134 completed (loss: 0.32606300711631775, acc: 0.9142857193946838)
[2025-02-13 19:51:05,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:05,508][root][INFO] - Training Epoch: 1/2, step 3820/7134 completed (loss: 0.29126080870628357, acc: 0.9166666865348816)
[2025-02-13 19:51:05,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:05,873][root][INFO] - Training Epoch: 1/2, step 3821/7134 completed (loss: 0.15340949594974518, acc: 0.965753436088562)
[2025-02-13 19:51:06,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:06,224][root][INFO] - Training Epoch: 1/2, step 3822/7134 completed (loss: 0.18588420748710632, acc: 0.9622641801834106)
[2025-02-13 19:51:06,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:06,604][root][INFO] - Training Epoch: 1/2, step 3823/7134 completed (loss: 0.07495784759521484, acc: 0.9932885766029358)
[2025-02-13 19:51:06,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:06,999][root][INFO] - Training Epoch: 1/2, step 3824/7134 completed (loss: 0.2035994827747345, acc: 0.953125)
[2025-02-13 19:51:07,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:07,377][root][INFO] - Training Epoch: 1/2, step 3825/7134 completed (loss: 0.2113228738307953, acc: 0.9351851940155029)
[2025-02-13 19:51:07,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:07,740][root][INFO] - Training Epoch: 1/2, step 3826/7134 completed (loss: 0.17171934247016907, acc: 0.9507042169570923)
[2025-02-13 19:51:07,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:08,121][root][INFO] - Training Epoch: 1/2, step 3827/7134 completed (loss: 0.0934855192899704, acc: 0.9808917045593262)
[2025-02-13 19:51:08,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:08,546][root][INFO] - Training Epoch: 1/2, step 3828/7134 completed (loss: 0.09196050465106964, acc: 0.987500011920929)
[2025-02-13 19:51:08,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:08,927][root][INFO] - Training Epoch: 1/2, step 3829/7134 completed (loss: 0.21537382900714874, acc: 0.9296875)
[2025-02-13 19:51:09,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:09,314][root][INFO] - Training Epoch: 1/2, step 3830/7134 completed (loss: 0.2404545247554779, acc: 0.9629629850387573)
[2025-02-13 19:51:09,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:09,681][root][INFO] - Training Epoch: 1/2, step 3831/7134 completed (loss: 0.28606081008911133, acc: 0.9342105388641357)
[2025-02-13 19:51:09,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:10,077][root][INFO] - Training Epoch: 1/2, step 3832/7134 completed (loss: 0.13122998178005219, acc: 0.9821428656578064)
[2025-02-13 19:51:10,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:10,441][root][INFO] - Training Epoch: 1/2, step 3833/7134 completed (loss: 0.08357851207256317, acc: 0.9726775884628296)
[2025-02-13 19:51:10,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:10,829][root][INFO] - Training Epoch: 1/2, step 3834/7134 completed (loss: 0.039994776248931885, acc: 0.9878048896789551)
[2025-02-13 19:51:10,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:11,198][root][INFO] - Training Epoch: 1/2, step 3835/7134 completed (loss: 0.08497753739356995, acc: 0.9739583134651184)
[2025-02-13 19:51:11,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:11,597][root][INFO] - Training Epoch: 1/2, step 3836/7134 completed (loss: 0.22118045389652252, acc: 0.9629629850387573)
[2025-02-13 19:51:11,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:11,982][root][INFO] - Training Epoch: 1/2, step 3837/7134 completed (loss: 0.15412281453609467, acc: 0.9808917045593262)
[2025-02-13 19:51:12,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:12,361][root][INFO] - Training Epoch: 1/2, step 3838/7134 completed (loss: 0.11740662902593613, acc: 0.9606741666793823)
[2025-02-13 19:51:12,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:12,755][root][INFO] - Training Epoch: 1/2, step 3839/7134 completed (loss: 0.09328114241361618, acc: 0.9689440727233887)
[2025-02-13 19:51:12,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:13,094][root][INFO] - Training Epoch: 1/2, step 3840/7134 completed (loss: 0.11713897436857224, acc: 0.9696969985961914)
[2025-02-13 19:51:13,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:13,469][root][INFO] - Training Epoch: 1/2, step 3841/7134 completed (loss: 0.1962466835975647, acc: 0.9497487545013428)
[2025-02-13 19:51:13,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:13,821][root][INFO] - Training Epoch: 1/2, step 3842/7134 completed (loss: 0.025908714160323143, acc: 1.0)
[2025-02-13 19:51:13,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:14,246][root][INFO] - Training Epoch: 1/2, step 3843/7134 completed (loss: 0.16204231977462769, acc: 0.9684684872627258)
[2025-02-13 19:51:14,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:14,629][root][INFO] - Training Epoch: 1/2, step 3844/7134 completed (loss: 0.279158353805542, acc: 0.9214659929275513)
[2025-02-13 19:51:14,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:14,982][root][INFO] - Training Epoch: 1/2, step 3845/7134 completed (loss: 0.21955889463424683, acc: 0.9714285731315613)
[2025-02-13 19:51:15,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:15,338][root][INFO] - Training Epoch: 1/2, step 3846/7134 completed (loss: 0.08460260182619095, acc: 0.985401451587677)
[2025-02-13 19:51:15,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:15,690][root][INFO] - Training Epoch: 1/2, step 3847/7134 completed (loss: 0.11701562255620956, acc: 0.9716312289237976)
[2025-02-13 19:51:15,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:16,060][root][INFO] - Training Epoch: 1/2, step 3848/7134 completed (loss: 0.13191138207912445, acc: 0.9602272510528564)
[2025-02-13 19:51:16,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:16,433][root][INFO] - Training Epoch: 1/2, step 3849/7134 completed (loss: 0.24707534909248352, acc: 0.9481481313705444)
[2025-02-13 19:51:16,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:16,794][root][INFO] - Training Epoch: 1/2, step 3850/7134 completed (loss: 0.24137747287750244, acc: 0.9398906826972961)
[2025-02-13 19:51:16,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:17,160][root][INFO] - Training Epoch: 1/2, step 3851/7134 completed (loss: 0.21245938539505005, acc: 0.9404761791229248)
[2025-02-13 19:51:17,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:17,518][root][INFO] - Training Epoch: 1/2, step 3852/7134 completed (loss: 0.09688861668109894, acc: 0.9640287756919861)
[2025-02-13 19:51:17,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:17,900][root][INFO] - Training Epoch: 1/2, step 3853/7134 completed (loss: 0.2672056257724762, acc: 0.9385474920272827)
[2025-02-13 19:51:18,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:18,249][root][INFO] - Training Epoch: 1/2, step 3854/7134 completed (loss: 0.18837061524391174, acc: 0.9554139971733093)
[2025-02-13 19:51:18,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:18,632][root][INFO] - Training Epoch: 1/2, step 3855/7134 completed (loss: 0.22315293550491333, acc: 0.946107804775238)
[2025-02-13 19:51:18,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:18,986][root][INFO] - Training Epoch: 1/2, step 3856/7134 completed (loss: 0.17966614663600922, acc: 0.9490445852279663)
[2025-02-13 19:51:19,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:19,331][root][INFO] - Training Epoch: 1/2, step 3857/7134 completed (loss: 0.20678414404392242, acc: 0.9594594836235046)
[2025-02-13 19:51:19,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:19,712][root][INFO] - Training Epoch: 1/2, step 3858/7134 completed (loss: 0.3789708614349365, acc: 0.9125000238418579)
[2025-02-13 19:51:19,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:20,126][root][INFO] - Training Epoch: 1/2, step 3859/7134 completed (loss: 0.25729623436927795, acc: 0.9491525292396545)
[2025-02-13 19:51:20,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:20,494][root][INFO] - Training Epoch: 1/2, step 3860/7134 completed (loss: 0.09867934882640839, acc: 0.9685863852500916)
[2025-02-13 19:51:20,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:20,875][root][INFO] - Training Epoch: 1/2, step 3861/7134 completed (loss: 0.2257375717163086, acc: 0.9281045794487)
[2025-02-13 19:51:21,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:21,310][root][INFO] - Training Epoch: 1/2, step 3862/7134 completed (loss: 0.1607225090265274, acc: 0.9692307710647583)
[2025-02-13 19:51:21,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:21,705][root][INFO] - Training Epoch: 1/2, step 3863/7134 completed (loss: 0.08176583796739578, acc: 0.9767441749572754)
[2025-02-13 19:51:21,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:22,102][root][INFO] - Training Epoch: 1/2, step 3864/7134 completed (loss: 0.1434972882270813, acc: 0.9545454382896423)
[2025-02-13 19:51:22,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:22,480][root][INFO] - Training Epoch: 1/2, step 3865/7134 completed (loss: 0.24413971602916718, acc: 0.9375)
[2025-02-13 19:51:22,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:22,875][root][INFO] - Training Epoch: 1/2, step 3866/7134 completed (loss: 0.2737888991832733, acc: 0.9300000071525574)
[2025-02-13 19:51:23,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23,224][root][INFO] - Training Epoch: 1/2, step 3867/7134 completed (loss: 0.1604158580303192, acc: 0.9545454382896423)
[2025-02-13 19:51:23,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23,589][root][INFO] - Training Epoch: 1/2, step 3868/7134 completed (loss: 0.08397354930639267, acc: 0.9852941036224365)
[2025-02-13 19:51:23,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23,944][root][INFO] - Training Epoch: 1/2, step 3869/7134 completed (loss: 0.17195263504981995, acc: 0.9370629191398621)
[2025-02-13 19:51:24,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:24,365][root][INFO] - Training Epoch: 1/2, step 3870/7134 completed (loss: 0.300677090883255, acc: 0.9477124214172363)
[2025-02-13 19:51:24,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:24,774][root][INFO] - Training Epoch: 1/2, step 3871/7134 completed (loss: 0.18509608507156372, acc: 0.959770143032074)
[2025-02-13 19:51:24,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:25,155][root][INFO] - Training Epoch: 1/2, step 3872/7134 completed (loss: 0.168413445353508, acc: 0.9696969985961914)
[2025-02-13 19:51:25,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:25,550][root][INFO] - Training Epoch: 1/2, step 3873/7134 completed (loss: 0.13926807045936584, acc: 0.9515151381492615)
[2025-02-13 19:51:25,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:25,969][root][INFO] - Training Epoch: 1/2, step 3874/7134 completed (loss: 0.14734859764575958, acc: 0.9590643048286438)
[2025-02-13 19:51:26,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:26,355][root][INFO] - Training Epoch: 1/2, step 3875/7134 completed (loss: 0.24317118525505066, acc: 0.9240506291389465)
[2025-02-13 19:51:26,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:26,748][root][INFO] - Training Epoch: 1/2, step 3876/7134 completed (loss: 0.08998633176088333, acc: 0.984000027179718)
[2025-02-13 19:51:26,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:27,106][root][INFO] - Training Epoch: 1/2, step 3877/7134 completed (loss: 0.19321873784065247, acc: 0.9593023061752319)
[2025-02-13 19:51:27,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:27,493][root][INFO] - Training Epoch: 1/2, step 3878/7134 completed (loss: 0.5622398257255554, acc: 0.8837209343910217)
[2025-02-13 19:51:27,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:27,916][root][INFO] - Training Epoch: 1/2, step 3879/7134 completed (loss: 0.2009725421667099, acc: 0.9402984976768494)
[2025-02-13 19:51:28,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:28,305][root][INFO] - Training Epoch: 1/2, step 3880/7134 completed (loss: 0.20702838897705078, acc: 0.9487179517745972)
[2025-02-13 19:51:28,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:28,717][root][INFO] - Training Epoch: 1/2, step 3881/7134 completed (loss: 0.39741241931915283, acc: 0.9197860956192017)
[2025-02-13 19:51:28,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:29,103][root][INFO] - Training Epoch: 1/2, step 3882/7134 completed (loss: 0.34963878989219666, acc: 0.9175257682800293)
[2025-02-13 19:51:29,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:29,475][root][INFO] - Training Epoch: 1/2, step 3883/7134 completed (loss: 0.2566129267215729, acc: 0.9542483687400818)
[2025-02-13 19:51:29,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:29,844][root][INFO] - Training Epoch: 1/2, step 3884/7134 completed (loss: 0.17050670087337494, acc: 0.954285740852356)
[2025-02-13 19:51:29,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:30,212][root][INFO] - Training Epoch: 1/2, step 3885/7134 completed (loss: 0.07520243525505066, acc: 0.987261176109314)
[2025-02-13 19:51:30,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:30,633][root][INFO] - Training Epoch: 1/2, step 3886/7134 completed (loss: 0.2235092669725418, acc: 0.9523809552192688)
[2025-02-13 19:51:30,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:31,012][root][INFO] - Training Epoch: 1/2, step 3887/7134 completed (loss: 0.20173873007297516, acc: 0.9622641801834106)
[2025-02-13 19:51:31,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:31,376][root][INFO] - Training Epoch: 1/2, step 3888/7134 completed (loss: 0.1831689476966858, acc: 0.9418604373931885)
[2025-02-13 19:51:31,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:31,772][root][INFO] - Training Epoch: 1/2, step 3889/7134 completed (loss: 0.08275254815816879, acc: 0.9774011373519897)
[2025-02-13 19:51:31,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32,157][root][INFO] - Training Epoch: 1/2, step 3890/7134 completed (loss: 0.10662752389907837, acc: 0.9793814420700073)
[2025-02-13 19:51:32,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32,532][root][INFO] - Training Epoch: 1/2, step 3891/7134 completed (loss: 0.0950569286942482, acc: 0.9826589822769165)
[2025-02-13 19:51:32,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32,899][root][INFO] - Training Epoch: 1/2, step 3892/7134 completed (loss: 0.2289622277021408, acc: 0.9466666579246521)
[2025-02-13 19:51:33,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:33,277][root][INFO] - Training Epoch: 1/2, step 3893/7134 completed (loss: 0.27943161129951477, acc: 0.9484536051750183)
[2025-02-13 19:51:33,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:33,625][root][INFO] - Training Epoch: 1/2, step 3894/7134 completed (loss: 0.12209191173315048, acc: 0.9746835231781006)
[2025-02-13 19:51:33,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:33,993][root][INFO] - Training Epoch: 1/2, step 3895/7134 completed (loss: 0.2391243577003479, acc: 0.9447513818740845)
[2025-02-13 19:51:34,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:34,364][root][INFO] - Training Epoch: 1/2, step 3896/7134 completed (loss: 0.14350254833698273, acc: 0.9668246507644653)
[2025-02-13 19:51:34,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:34,735][root][INFO] - Training Epoch: 1/2, step 3897/7134 completed (loss: 0.3499121367931366, acc: 0.9534883499145508)
[2025-02-13 19:51:34,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:35,092][root][INFO] - Training Epoch: 1/2, step 3898/7134 completed (loss: 0.20793041586875916, acc: 0.9655172228813171)
[2025-02-13 19:51:35,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:35,454][root][INFO] - Training Epoch: 1/2, step 3899/7134 completed (loss: 0.17524589598178864, acc: 0.9537572264671326)
[2025-02-13 19:51:35,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:35,851][root][INFO] - Training Epoch: 1/2, step 3900/7134 completed (loss: 0.15919041633605957, acc: 0.9587628841400146)
[2025-02-13 19:51:35,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:36,235][root][INFO] - Training Epoch: 1/2, step 3901/7134 completed (loss: 0.37299853563308716, acc: 0.9337349534034729)
[2025-02-13 19:51:36,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:36,628][root][INFO] - Training Epoch: 1/2, step 3902/7134 completed (loss: 0.3495676517486572, acc: 0.8958333134651184)
[2025-02-13 19:51:36,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:37,001][root][INFO] - Training Epoch: 1/2, step 3903/7134 completed (loss: 0.526369571685791, acc: 0.8601036071777344)
[2025-02-13 19:51:37,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:37,453][root][INFO] - Training Epoch: 1/2, step 3904/7134 completed (loss: 0.3242582082748413, acc: 0.9239766001701355)
[2025-02-13 19:51:37,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:37,848][root][INFO] - Training Epoch: 1/2, step 3905/7134 completed (loss: 0.11205577105283737, acc: 0.9777777791023254)
[2025-02-13 19:51:37,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:38,224][root][INFO] - Training Epoch: 1/2, step 3906/7134 completed (loss: 0.17004285752773285, acc: 0.9438202381134033)
[2025-02-13 19:51:38,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:38,587][root][INFO] - Training Epoch: 1/2, step 3907/7134 completed (loss: 0.2833270728588104, acc: 0.9226804375648499)
[2025-02-13 19:51:38,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:38,952][root][INFO] - Training Epoch: 1/2, step 3908/7134 completed (loss: 0.3362930715084076, acc: 0.9364162087440491)
[2025-02-13 19:51:39,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:39,306][root][INFO] - Training Epoch: 1/2, step 3909/7134 completed (loss: 0.1161658838391304, acc: 0.9685534834861755)
[2025-02-13 19:51:39,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:39,694][root][INFO] - Training Epoch: 1/2, step 3910/7134 completed (loss: 0.11213898658752441, acc: 0.9375)
[2025-02-13 19:51:39,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:40,136][root][INFO] - Training Epoch: 1/2, step 3911/7134 completed (loss: 0.13708826899528503, acc: 0.9496855139732361)
[2025-02-13 19:51:40,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:40,545][root][INFO] - Training Epoch: 1/2, step 3912/7134 completed (loss: 0.0451301671564579, acc: 0.9882352948188782)
[2025-02-13 19:51:40,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:40,967][root][INFO] - Training Epoch: 1/2, step 3913/7134 completed (loss: 0.13189160823822021, acc: 0.9646464586257935)
[2025-02-13 19:51:41,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:41,333][root][INFO] - Training Epoch: 1/2, step 3914/7134 completed (loss: 0.13180656731128693, acc: 0.955974817276001)
[2025-02-13 19:51:41,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:41,691][root][INFO] - Training Epoch: 1/2, step 3915/7134 completed (loss: 0.060968972742557526, acc: 0.9942528605461121)
[2025-02-13 19:51:41,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:42,056][root][INFO] - Training Epoch: 1/2, step 3916/7134 completed (loss: 0.12103316187858582, acc: 0.9693877696990967)
[2025-02-13 19:51:42,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:42,432][root][INFO] - Training Epoch: 1/2, step 3917/7134 completed (loss: 0.08331339061260223, acc: 0.9767441749572754)
[2025-02-13 19:51:42,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:42,787][root][INFO] - Training Epoch: 1/2, step 3918/7134 completed (loss: 0.12843917310237885, acc: 0.9714285731315613)
[2025-02-13 19:51:42,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:43,218][root][INFO] - Training Epoch: 1/2, step 3919/7134 completed (loss: 0.3718271553516388, acc: 0.9230769276618958)
[2025-02-13 19:51:43,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:43,693][root][INFO] - Training Epoch: 1/2, step 3920/7134 completed (loss: 0.3648649752140045, acc: 0.916167676448822)
[2025-02-13 19:51:43,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:44,115][root][INFO] - Training Epoch: 1/2, step 3921/7134 completed (loss: 0.34098121523857117, acc: 0.908450722694397)
[2025-02-13 19:51:44,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:44,505][root][INFO] - Training Epoch: 1/2, step 3922/7134 completed (loss: 0.4869677424430847, acc: 0.8961039185523987)
[2025-02-13 19:51:44,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:44,943][root][INFO] - Training Epoch: 1/2, step 3923/7134 completed (loss: 0.4675055146217346, acc: 0.8928571343421936)
[2025-02-13 19:51:45,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:45,311][root][INFO] - Training Epoch: 1/2, step 3924/7134 completed (loss: 0.22569239139556885, acc: 0.9536082744598389)
[2025-02-13 19:51:45,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:45,674][root][INFO] - Training Epoch: 1/2, step 3925/7134 completed (loss: 0.1940147876739502, acc: 0.950276255607605)
[2025-02-13 19:51:45,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:46,049][root][INFO] - Training Epoch: 1/2, step 3926/7134 completed (loss: 0.21862399578094482, acc: 0.949367105960846)
[2025-02-13 19:51:46,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:46,430][root][INFO] - Training Epoch: 1/2, step 3927/7134 completed (loss: 0.1941521316766739, acc: 0.9515151381492615)
[2025-02-13 19:51:46,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:46,825][root][INFO] - Training Epoch: 1/2, step 3928/7134 completed (loss: 0.4196762144565582, acc: 0.9341317415237427)
[2025-02-13 19:51:46,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:47,189][root][INFO] - Training Epoch: 1/2, step 3929/7134 completed (loss: 0.16282016038894653, acc: 0.9675324559211731)
[2025-02-13 19:51:47,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:47,551][root][INFO] - Training Epoch: 1/2, step 3930/7134 completed (loss: 0.3351879119873047, acc: 0.9491525292396545)
[2025-02-13 19:51:47,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:47,952][root][INFO] - Training Epoch: 1/2, step 3931/7134 completed (loss: 0.14645826816558838, acc: 0.9556962251663208)
[2025-02-13 19:51:48,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:48,365][root][INFO] - Training Epoch: 1/2, step 3932/7134 completed (loss: 0.15961910784244537, acc: 0.9545454382896423)
[2025-02-13 19:51:48,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:48,727][root][INFO] - Training Epoch: 1/2, step 3933/7134 completed (loss: 0.047330092638731, acc: 0.9857142567634583)
[2025-02-13 19:51:48,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:49,125][root][INFO] - Training Epoch: 1/2, step 3934/7134 completed (loss: 0.13027188181877136, acc: 0.9629629850387573)
[2025-02-13 19:51:49,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:49,514][root][INFO] - Training Epoch: 1/2, step 3935/7134 completed (loss: 0.13865125179290771, acc: 0.9589040875434875)
[2025-02-13 19:51:49,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:49,875][root][INFO] - Training Epoch: 1/2, step 3936/7134 completed (loss: 0.14645856618881226, acc: 0.9677419066429138)
[2025-02-13 19:51:50,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:50,236][root][INFO] - Training Epoch: 1/2, step 3937/7134 completed (loss: 0.3185456693172455, acc: 0.9322034120559692)
[2025-02-13 19:51:50,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:50,579][root][INFO] - Training Epoch: 1/2, step 3938/7134 completed (loss: 0.271138995885849, acc: 0.9369369149208069)
[2025-02-13 19:51:50,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:50,939][root][INFO] - Training Epoch: 1/2, step 3939/7134 completed (loss: 0.2388153374195099, acc: 0.9407407641410828)
[2025-02-13 19:51:51,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:51,275][root][INFO] - Training Epoch: 1/2, step 3940/7134 completed (loss: 0.2075941115617752, acc: 0.945652186870575)
[2025-02-13 19:51:51,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:51,618][root][INFO] - Training Epoch: 1/2, step 3941/7134 completed (loss: 0.20853270590305328, acc: 0.9390243887901306)
[2025-02-13 19:51:51,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:51,986][root][INFO] - Training Epoch: 1/2, step 3942/7134 completed (loss: 0.17548659443855286, acc: 0.9599999785423279)
[2025-02-13 19:51:52,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:52,353][root][INFO] - Training Epoch: 1/2, step 3943/7134 completed (loss: 0.21307288110256195, acc: 0.9295774698257446)
[2025-02-13 19:51:52,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:52,724][root][INFO] - Training Epoch: 1/2, step 3944/7134 completed (loss: 0.16813285648822784, acc: 0.9701492786407471)
[2025-02-13 19:51:52,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:53,089][root][INFO] - Training Epoch: 1/2, step 3945/7134 completed (loss: 0.05873531475663185, acc: 0.9934210777282715)
[2025-02-13 19:51:53,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:53,432][root][INFO] - Training Epoch: 1/2, step 3946/7134 completed (loss: 0.11945968121290207, acc: 0.9642857313156128)
[2025-02-13 19:51:53,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:53,774][root][INFO] - Training Epoch: 1/2, step 3947/7134 completed (loss: 0.049651265144348145, acc: 0.9890109896659851)
[2025-02-13 19:51:53,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:54,126][root][INFO] - Training Epoch: 1/2, step 3948/7134 completed (loss: 0.03589554503560066, acc: 0.9900000095367432)
[2025-02-13 19:51:54,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:54,485][root][INFO] - Training Epoch: 1/2, step 3949/7134 completed (loss: 0.13906307518482208, acc: 0.9609375)
[2025-02-13 19:51:54,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:54,797][root][INFO] - Training Epoch: 1/2, step 3950/7134 completed (loss: 0.1787905991077423, acc: 0.9736841917037964)
[2025-02-13 19:51:54,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:55,159][root][INFO] - Training Epoch: 1/2, step 3951/7134 completed (loss: 0.058567967265844345, acc: 0.9793814420700073)
[2025-02-13 19:51:55,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:55,516][root][INFO] - Training Epoch: 1/2, step 3952/7134 completed (loss: 0.18180371820926666, acc: 0.9693877696990967)
[2025-02-13 19:51:55,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:55,870][root][INFO] - Training Epoch: 1/2, step 3953/7134 completed (loss: 0.08000989258289337, acc: 0.9855072498321533)
[2025-02-13 19:51:55,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:56,206][root][INFO] - Training Epoch: 1/2, step 3954/7134 completed (loss: 0.04694893956184387, acc: 1.0)
[2025-02-13 19:51:56,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:56,553][root][INFO] - Training Epoch: 1/2, step 3955/7134 completed (loss: 0.13008932769298553, acc: 0.9685039520263672)
[2025-02-13 19:51:56,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:56,912][root][INFO] - Training Epoch: 1/2, step 3956/7134 completed (loss: 0.19007574021816254, acc: 0.9607843160629272)
[2025-02-13 19:51:57,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:57,262][root][INFO] - Training Epoch: 1/2, step 3957/7134 completed (loss: 0.1243157684803009, acc: 0.9572649598121643)
[2025-02-13 19:51:57,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:57,612][root][INFO] - Training Epoch: 1/2, step 3958/7134 completed (loss: 0.15517084300518036, acc: 0.9404761791229248)
[2025-02-13 19:51:57,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:57,981][root][INFO] - Training Epoch: 1/2, step 3959/7134 completed (loss: 0.15863390266895294, acc: 0.9741379022598267)
[2025-02-13 19:51:58,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:58,352][root][INFO] - Training Epoch: 1/2, step 3960/7134 completed (loss: 0.22865350544452667, acc: 0.9553072452545166)
[2025-02-13 19:51:58,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:58,724][root][INFO] - Training Epoch: 1/2, step 3961/7134 completed (loss: 0.13618648052215576, acc: 0.9753086566925049)
[2025-02-13 19:51:58,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:59,100][root][INFO] - Training Epoch: 1/2, step 3962/7134 completed (loss: 0.13446033000946045, acc: 0.9731543660163879)
[2025-02-13 19:51:59,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:59,468][root][INFO] - Training Epoch: 1/2, step 3963/7134 completed (loss: 0.3819664716720581, acc: 0.929347813129425)
[2025-02-13 19:51:59,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:59,820][root][INFO] - Training Epoch: 1/2, step 3964/7134 completed (loss: 0.3323685824871063, acc: 0.9307692050933838)
[2025-02-13 19:51:59,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:00,178][root][INFO] - Training Epoch: 1/2, step 3965/7134 completed (loss: 0.16666941344738007, acc: 0.9636363387107849)
[2025-02-13 19:52:00,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:00,514][root][INFO] - Training Epoch: 1/2, step 3966/7134 completed (loss: 0.13495668768882751, acc: 0.984375)
[2025-02-13 19:52:00,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:00,871][root][INFO] - Training Epoch: 1/2, step 3967/7134 completed (loss: 0.4289744198322296, acc: 0.9347826242446899)
[2025-02-13 19:52:01,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:01,242][root][INFO] - Training Epoch: 1/2, step 3968/7134 completed (loss: 0.10670330375432968, acc: 0.9776536226272583)
[2025-02-13 19:52:01,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:01,572][root][INFO] - Training Epoch: 1/2, step 3969/7134 completed (loss: 0.42099106311798096, acc: 0.9150943160057068)
[2025-02-13 19:52:01,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:01,931][root][INFO] - Training Epoch: 1/2, step 3970/7134 completed (loss: 0.2819731533527374, acc: 0.9421965479850769)
[2025-02-13 19:52:02,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:02,276][root][INFO] - Training Epoch: 1/2, step 3971/7134 completed (loss: 0.5451908111572266, acc: 0.8820512890815735)
[2025-02-13 19:52:02,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:02,647][root][INFO] - Training Epoch: 1/2, step 3972/7134 completed (loss: 0.8462377190589905, acc: 0.817307710647583)
[2025-02-13 19:52:02,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:03,069][root][INFO] - Training Epoch: 1/2, step 3973/7134 completed (loss: 0.32478320598602295, acc: 0.9295774698257446)
[2025-02-13 19:52:03,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:03,476][root][INFO] - Training Epoch: 1/2, step 3974/7134 completed (loss: 0.398318886756897, acc: 0.8820512890815735)
[2025-02-13 19:52:03,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:03,854][root][INFO] - Training Epoch: 1/2, step 3975/7134 completed (loss: 0.28534114360809326, acc: 0.9200000166893005)
[2025-02-13 19:52:03,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:04,272][root][INFO] - Training Epoch: 1/2, step 3976/7134 completed (loss: 0.45809656381607056, acc: 0.891566276550293)
[2025-02-13 19:52:04,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:04,636][root][INFO] - Training Epoch: 1/2, step 3977/7134 completed (loss: 0.17496603727340698, acc: 0.95652174949646)
[2025-02-13 19:52:04,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:04,988][root][INFO] - Training Epoch: 1/2, step 3978/7134 completed (loss: 0.12648479640483856, acc: 0.9751243591308594)
[2025-02-13 19:52:05,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:05,358][root][INFO] - Training Epoch: 1/2, step 3979/7134 completed (loss: 0.2192043662071228, acc: 0.9635036587715149)
[2025-02-13 19:52:05,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:05,717][root][INFO] - Training Epoch: 1/2, step 3980/7134 completed (loss: 0.11786230653524399, acc: 0.9612902998924255)
[2025-02-13 19:52:05,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:06,070][root][INFO] - Training Epoch: 1/2, step 3981/7134 completed (loss: 0.23074112832546234, acc: 0.9578313231468201)
[2025-02-13 19:52:06,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:06,434][root][INFO] - Training Epoch: 1/2, step 3982/7134 completed (loss: 0.40947040915489197, acc: 0.9186046719551086)
[2025-02-13 19:52:06,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:06,790][root][INFO] - Training Epoch: 1/2, step 3983/7134 completed (loss: 1.0068509578704834, acc: 0.7751938104629517)
[2025-02-13 19:52:06,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:07,152][root][INFO] - Training Epoch: 1/2, step 3984/7134 completed (loss: 0.4612140357494354, acc: 0.9042553305625916)
[2025-02-13 19:52:07,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:07,499][root][INFO] - Training Epoch: 1/2, step 3985/7134 completed (loss: 0.12393784523010254, acc: 0.9692307710647583)
[2025-02-13 19:52:07,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:07,855][root][INFO] - Training Epoch: 1/2, step 3986/7134 completed (loss: 0.6221169829368591, acc: 0.8693181872367859)
[2025-02-13 19:52:07,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:08,206][root][INFO] - Training Epoch: 1/2, step 3987/7134 completed (loss: 0.3059017062187195, acc: 0.89682537317276)
[2025-02-13 19:52:08,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:08,580][root][INFO] - Training Epoch: 1/2, step 3988/7134 completed (loss: 0.3302868902683258, acc: 0.9138755798339844)
[2025-02-13 19:52:08,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:08,954][root][INFO] - Training Epoch: 1/2, step 3989/7134 completed (loss: 0.3610905110836029, acc: 0.9090909361839294)
[2025-02-13 19:52:09,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09,316][root][INFO] - Training Epoch: 1/2, step 3990/7134 completed (loss: 0.38848719000816345, acc: 0.9139072895050049)
[2025-02-13 19:52:09,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09,633][root][INFO] - Training Epoch: 1/2, step 3991/7134 completed (loss: 0.20040203630924225, acc: 0.9466666579246521)
[2025-02-13 19:52:09,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09,974][root][INFO] - Training Epoch: 1/2, step 3992/7134 completed (loss: 0.2884628176689148, acc: 0.9230769276618958)
[2025-02-13 19:52:10,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:10,324][root][INFO] - Training Epoch: 1/2, step 3993/7134 completed (loss: 0.4236276149749756, acc: 0.8796296119689941)
[2025-02-13 19:52:10,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:10,681][root][INFO] - Training Epoch: 1/2, step 3994/7134 completed (loss: 0.285704642534256, acc: 0.9117646813392639)
[2025-02-13 19:52:10,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:11,040][root][INFO] - Training Epoch: 1/2, step 3995/7134 completed (loss: 0.5086140632629395, acc: 0.8682170510292053)
[2025-02-13 19:52:11,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:11,385][root][INFO] - Training Epoch: 1/2, step 3996/7134 completed (loss: 0.24344518780708313, acc: 0.9173553586006165)
[2025-02-13 19:52:11,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:11,716][root][INFO] - Training Epoch: 1/2, step 3997/7134 completed (loss: 0.43534600734710693, acc: 0.8793103694915771)
[2025-02-13 19:52:11,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:12,108][root][INFO] - Training Epoch: 1/2, step 3998/7134 completed (loss: 0.319602906703949, acc: 0.9230769276618958)
[2025-02-13 19:52:12,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:12,477][root][INFO] - Training Epoch: 1/2, step 3999/7134 completed (loss: 0.23028196394443512, acc: 0.9354838728904724)
[2025-02-13 19:52:12,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:12,821][root][INFO] - Training Epoch: 1/2, step 4000/7134 completed (loss: 0.2534814774990082, acc: 0.9268292784690857)
[2025-02-13 19:52:12,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:13,243][root][INFO] - Training Epoch: 1/2, step 4001/7134 completed (loss: 0.5092746615409851, acc: 0.8709677457809448)
[2025-02-13 19:52:13,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:13,582][root][INFO] - Training Epoch: 1/2, step 4002/7134 completed (loss: 0.47768434882164, acc: 0.8785046935081482)
[2025-02-13 19:52:13,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:13,940][root][INFO] - Training Epoch: 1/2, step 4003/7134 completed (loss: 0.47910329699516296, acc: 0.8947368264198303)
[2025-02-13 19:52:14,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:14,282][root][INFO] - Training Epoch: 1/2, step 4004/7134 completed (loss: 0.2901298403739929, acc: 0.9300000071525574)
[2025-02-13 19:52:14,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:14,630][root][INFO] - Training Epoch: 1/2, step 4005/7134 completed (loss: 0.13960367441177368, acc: 0.9542483687400818)
[2025-02-13 19:52:14,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:14,997][root][INFO] - Training Epoch: 1/2, step 4006/7134 completed (loss: 0.10643433034420013, acc: 0.9639639854431152)
[2025-02-13 19:52:15,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:15,326][root][INFO] - Training Epoch: 1/2, step 4007/7134 completed (loss: 0.2228803187608719, acc: 0.9642857313156128)
[2025-02-13 19:52:15,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:15,677][root][INFO] - Training Epoch: 1/2, step 4008/7134 completed (loss: 0.10965116322040558, acc: 0.9730941653251648)
[2025-02-13 19:52:15,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:16,044][root][INFO] - Training Epoch: 1/2, step 4009/7134 completed (loss: 0.07405072450637817, acc: 0.9888268113136292)
[2025-02-13 19:52:16,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:16,409][root][INFO] - Training Epoch: 1/2, step 4010/7134 completed (loss: 0.09223951399326324, acc: 0.9760765433311462)
[2025-02-13 19:52:16,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:16,771][root][INFO] - Training Epoch: 1/2, step 4011/7134 completed (loss: 0.1386609524488449, acc: 0.9485714435577393)
[2025-02-13 19:52:16,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:17,109][root][INFO] - Training Epoch: 1/2, step 4012/7134 completed (loss: 0.1729394942522049, acc: 0.9457831382751465)
[2025-02-13 19:52:17,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:17,461][root][INFO] - Training Epoch: 1/2, step 4013/7134 completed (loss: 0.0593191497027874, acc: 0.9790209531784058)
[2025-02-13 19:52:17,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:17,821][root][INFO] - Training Epoch: 1/2, step 4014/7134 completed (loss: 0.08853381127119064, acc: 0.9800000190734863)
[2025-02-13 19:52:17,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:18,202][root][INFO] - Training Epoch: 1/2, step 4015/7134 completed (loss: 0.03936193883419037, acc: 0.9862068891525269)
[2025-02-13 19:52:18,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:18,580][root][INFO] - Training Epoch: 1/2, step 4016/7134 completed (loss: 0.24932599067687988, acc: 0.9326424598693848)
[2025-02-13 19:52:18,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:18,952][root][INFO] - Training Epoch: 1/2, step 4017/7134 completed (loss: 0.09602595865726471, acc: 0.9639175534248352)
[2025-02-13 19:52:19,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:19,318][root][INFO] - Training Epoch: 1/2, step 4018/7134 completed (loss: 0.16840772330760956, acc: 0.9516907930374146)
[2025-02-13 19:52:19,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:19,669][root][INFO] - Training Epoch: 1/2, step 4019/7134 completed (loss: 0.11627350002527237, acc: 0.9722222089767456)
[2025-02-13 19:52:19,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:20,017][root][INFO] - Training Epoch: 1/2, step 4020/7134 completed (loss: 0.18038113415241241, acc: 0.9431818127632141)
[2025-02-13 19:52:20,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:20,382][root][INFO] - Training Epoch: 1/2, step 4021/7134 completed (loss: 0.11091657727956772, acc: 0.970588207244873)
[2025-02-13 19:52:20,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:20,747][root][INFO] - Training Epoch: 1/2, step 4022/7134 completed (loss: 0.13403022289276123, acc: 0.9567307829856873)
[2025-02-13 19:52:20,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:21,118][root][INFO] - Training Epoch: 1/2, step 4023/7134 completed (loss: 0.18403445184230804, acc: 0.954285740852356)
[2025-02-13 19:52:21,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:21,471][root][INFO] - Training Epoch: 1/2, step 4024/7134 completed (loss: 0.12099763751029968, acc: 0.959770143032074)
[2025-02-13 19:52:21,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:21,824][root][INFO] - Training Epoch: 1/2, step 4025/7134 completed (loss: 0.3193017542362213, acc: 0.9005235433578491)
[2025-02-13 19:52:21,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:22,181][root][INFO] - Training Epoch: 1/2, step 4026/7134 completed (loss: 0.3585261106491089, acc: 0.9068322777748108)
[2025-02-13 19:52:22,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:22,534][root][INFO] - Training Epoch: 1/2, step 4027/7134 completed (loss: 0.24459154903888702, acc: 0.932584285736084)
[2025-02-13 19:52:22,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:22,885][root][INFO] - Training Epoch: 1/2, step 4028/7134 completed (loss: 0.45161372423171997, acc: 0.8818897604942322)
[2025-02-13 19:52:23,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:23,247][root][INFO] - Training Epoch: 1/2, step 4029/7134 completed (loss: 0.09949763864278793, acc: 0.9783783555030823)
[2025-02-13 19:52:23,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:23,605][root][INFO] - Training Epoch: 1/2, step 4030/7134 completed (loss: 0.10632751882076263, acc: 0.9615384340286255)
[2025-02-13 19:52:23,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:23,955][root][INFO] - Training Epoch: 1/2, step 4031/7134 completed (loss: 0.05460115894675255, acc: 0.9937888383865356)
[2025-02-13 19:52:24,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:24,323][root][INFO] - Training Epoch: 1/2, step 4032/7134 completed (loss: 0.23639315366744995, acc: 0.9593023061752319)
[2025-02-13 19:52:24,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:24,684][root][INFO] - Training Epoch: 1/2, step 4033/7134 completed (loss: 0.0724615529179573, acc: 0.9786096215248108)
[2025-02-13 19:52:24,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:25,039][root][INFO] - Training Epoch: 1/2, step 4034/7134 completed (loss: 0.11170589923858643, acc: 0.9795918464660645)
[2025-02-13 19:52:25,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:25,443][root][INFO] - Training Epoch: 1/2, step 4035/7134 completed (loss: 0.33695581555366516, acc: 0.9365853667259216)
[2025-02-13 19:52:25,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:25,806][root][INFO] - Training Epoch: 1/2, step 4036/7134 completed (loss: 0.1517619639635086, acc: 0.9538461565971375)
[2025-02-13 19:52:25,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:26,171][root][INFO] - Training Epoch: 1/2, step 4037/7134 completed (loss: 0.13785314559936523, acc: 0.9583333134651184)
[2025-02-13 19:52:26,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:26,534][root][INFO] - Training Epoch: 1/2, step 4038/7134 completed (loss: 0.20110082626342773, acc: 0.9431818127632141)
[2025-02-13 19:52:26,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:26,886][root][INFO] - Training Epoch: 1/2, step 4039/7134 completed (loss: 0.25370466709136963, acc: 0.9349112510681152)
[2025-02-13 19:52:27,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27,242][root][INFO] - Training Epoch: 1/2, step 4040/7134 completed (loss: 0.13563521206378937, acc: 0.9617486596107483)
[2025-02-13 19:52:27,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27,609][root][INFO] - Training Epoch: 1/2, step 4041/7134 completed (loss: 0.20077794790267944, acc: 0.9653179049491882)
[2025-02-13 19:52:27,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27,965][root][INFO] - Training Epoch: 1/2, step 4042/7134 completed (loss: 0.1863415390253067, acc: 0.9594594836235046)
[2025-02-13 19:52:28,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:28,305][root][INFO] - Training Epoch: 1/2, step 4043/7134 completed (loss: 0.2853844463825226, acc: 0.9367088675498962)
[2025-02-13 19:52:28,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:28,658][root][INFO] - Training Epoch: 1/2, step 4044/7134 completed (loss: 0.20281019806861877, acc: 0.9670329689979553)
[2025-02-13 19:52:28,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:29,017][root][INFO] - Training Epoch: 1/2, step 4045/7134 completed (loss: 0.21546314656734467, acc: 0.9428571462631226)
[2025-02-13 19:52:29,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:29,407][root][INFO] - Training Epoch: 1/2, step 4046/7134 completed (loss: 0.2148895263671875, acc: 0.9415204524993896)
[2025-02-13 19:52:29,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:29,794][root][INFO] - Training Epoch: 1/2, step 4047/7134 completed (loss: 0.2022319883108139, acc: 0.9562841653823853)
[2025-02-13 19:52:29,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:30,160][root][INFO] - Training Epoch: 1/2, step 4048/7134 completed (loss: 0.08108973503112793, acc: 0.9746192693710327)
[2025-02-13 19:52:30,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:30,556][root][INFO] - Training Epoch: 1/2, step 4049/7134 completed (loss: 0.2712169587612152, acc: 0.9353233575820923)
[2025-02-13 19:52:30,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:30,914][root][INFO] - Training Epoch: 1/2, step 4050/7134 completed (loss: 0.26273787021636963, acc: 0.9375)
[2025-02-13 19:52:31,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:31,279][root][INFO] - Training Epoch: 1/2, step 4051/7134 completed (loss: 0.29940420389175415, acc: 0.9385474920272827)
[2025-02-13 19:52:31,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:31,620][root][INFO] - Training Epoch: 1/2, step 4052/7134 completed (loss: 0.15486104786396027, acc: 0.9661017060279846)
[2025-02-13 19:52:31,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:31,984][root][INFO] - Training Epoch: 1/2, step 4053/7134 completed (loss: 0.19344516098499298, acc: 0.9523809552192688)
[2025-02-13 19:52:32,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:32,362][root][INFO] - Training Epoch: 1/2, step 4054/7134 completed (loss: 0.4337767958641052, acc: 0.9130434989929199)
[2025-02-13 19:52:32,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:32,723][root][INFO] - Training Epoch: 1/2, step 4055/7134 completed (loss: 0.16264155507087708, acc: 0.9734042286872864)
[2025-02-13 19:52:32,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:33,091][root][INFO] - Training Epoch: 1/2, step 4056/7134 completed (loss: 0.15710337460041046, acc: 0.9789473414421082)
[2025-02-13 19:52:33,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:33,451][root][INFO] - Training Epoch: 1/2, step 4057/7134 completed (loss: 0.2290680855512619, acc: 0.9470587968826294)
[2025-02-13 19:52:33,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:33,819][root][INFO] - Training Epoch: 1/2, step 4058/7134 completed (loss: 0.2908632159233093, acc: 0.9318181872367859)
[2025-02-13 19:52:33,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:34,173][root][INFO] - Training Epoch: 1/2, step 4059/7134 completed (loss: 0.15650054812431335, acc: 0.9701492786407471)
[2025-02-13 19:52:34,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:34,523][root][INFO] - Training Epoch: 1/2, step 4060/7134 completed (loss: 0.07983291149139404, acc: 0.9729729890823364)
[2025-02-13 19:52:34,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:34,890][root][INFO] - Training Epoch: 1/2, step 4061/7134 completed (loss: 0.04210394620895386, acc: 0.9825581312179565)
[2025-02-13 19:52:35,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:35,241][root][INFO] - Training Epoch: 1/2, step 4062/7134 completed (loss: 0.05585465952754021, acc: 0.9802631735801697)
[2025-02-13 19:52:35,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:35,594][root][INFO] - Training Epoch: 1/2, step 4063/7134 completed (loss: 0.0623060017824173, acc: 1.0)
[2025-02-13 19:52:35,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:35,959][root][INFO] - Training Epoch: 1/2, step 4064/7134 completed (loss: 0.13881655037403107, acc: 0.9869281053543091)
[2025-02-13 19:52:36,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:36,327][root][INFO] - Training Epoch: 1/2, step 4065/7134 completed (loss: 0.07302476465702057, acc: 0.9806451797485352)
[2025-02-13 19:52:36,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:36,714][root][INFO] - Training Epoch: 1/2, step 4066/7134 completed (loss: 0.033657629042863846, acc: 1.0)
[2025-02-13 19:52:36,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:37,079][root][INFO] - Training Epoch: 1/2, step 4067/7134 completed (loss: 0.18888744711875916, acc: 0.9452054500579834)
[2025-02-13 19:52:37,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:37,424][root][INFO] - Training Epoch: 1/2, step 4068/7134 completed (loss: 0.352957546710968, acc: 0.9173553586006165)
[2025-02-13 19:52:37,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:37,784][root][INFO] - Training Epoch: 1/2, step 4069/7134 completed (loss: 0.10573594272136688, acc: 0.9718309640884399)
[2025-02-13 19:52:37,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:38,141][root][INFO] - Training Epoch: 1/2, step 4070/7134 completed (loss: 0.1908724308013916, acc: 0.9536423683166504)
[2025-02-13 19:52:38,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:38,494][root][INFO] - Training Epoch: 1/2, step 4071/7134 completed (loss: 0.10452103614807129, acc: 0.965753436088562)
[2025-02-13 19:52:38,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:38,852][root][INFO] - Training Epoch: 1/2, step 4072/7134 completed (loss: 0.0978325754404068, acc: 0.9756097793579102)
[2025-02-13 19:52:38,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:39,217][root][INFO] - Training Epoch: 1/2, step 4073/7134 completed (loss: 0.042954474687576294, acc: 0.9939758777618408)
[2025-02-13 19:52:39,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:39,581][root][INFO] - Training Epoch: 1/2, step 4074/7134 completed (loss: 0.17168882489204407, acc: 0.9735099077224731)
[2025-02-13 19:52:39,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:39,948][root][INFO] - Training Epoch: 1/2, step 4075/7134 completed (loss: 0.024124516174197197, acc: 1.0)
[2025-02-13 19:52:40,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:40,302][root][INFO] - Training Epoch: 1/2, step 4076/7134 completed (loss: 0.04968727007508278, acc: 1.0)
[2025-02-13 19:52:40,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:40,675][root][INFO] - Training Epoch: 1/2, step 4077/7134 completed (loss: 0.15382665395736694, acc: 0.9727891087532043)
[2025-02-13 19:52:40,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:41,042][root][INFO] - Training Epoch: 1/2, step 4078/7134 completed (loss: 0.10355835407972336, acc: 0.9685534834861755)
[2025-02-13 19:52:41,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:41,373][root][INFO] - Training Epoch: 1/2, step 4079/7134 completed (loss: 0.07880277186632156, acc: 0.9923664331436157)
[2025-02-13 19:52:41,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:41,716][root][INFO] - Training Epoch: 1/2, step 4080/7134 completed (loss: 0.06374258548021317, acc: 0.9793103337287903)
[2025-02-13 19:52:41,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:42,080][root][INFO] - Training Epoch: 1/2, step 4081/7134 completed (loss: 0.026165666058659554, acc: 0.9930555820465088)
[2025-02-13 19:52:42,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:42,430][root][INFO] - Training Epoch: 1/2, step 4082/7134 completed (loss: 0.3823259472846985, acc: 0.9210526347160339)
[2025-02-13 19:52:42,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:42,789][root][INFO] - Training Epoch: 1/2, step 4083/7134 completed (loss: 0.4601515829563141, acc: 0.9005848169326782)
[2025-02-13 19:52:42,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:43,142][root][INFO] - Training Epoch: 1/2, step 4084/7134 completed (loss: 0.4993821084499359, acc: 0.8724831938743591)
[2025-02-13 19:52:43,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:43,501][root][INFO] - Training Epoch: 1/2, step 4085/7134 completed (loss: 0.145169198513031, acc: 0.9473684430122375)
[2025-02-13 19:52:43,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:43,852][root][INFO] - Training Epoch: 1/2, step 4086/7134 completed (loss: 0.33307838439941406, acc: 0.9102563858032227)
[2025-02-13 19:52:43,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:44,213][root][INFO] - Training Epoch: 1/2, step 4087/7134 completed (loss: 0.12183243781328201, acc: 0.9892473220825195)
[2025-02-13 19:52:44,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:44,578][root][INFO] - Training Epoch: 1/2, step 4088/7134 completed (loss: 0.323087215423584, acc: 0.9187816977500916)
[2025-02-13 19:52:44,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:44,945][root][INFO] - Training Epoch: 1/2, step 4089/7134 completed (loss: 0.442585289478302, acc: 0.9015151262283325)
[2025-02-13 19:52:45,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:45,301][root][INFO] - Training Epoch: 1/2, step 4090/7134 completed (loss: 0.4798728823661804, acc: 0.880382776260376)
[2025-02-13 19:52:45,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:45,718][root][INFO] - Training Epoch: 1/2, step 4091/7134 completed (loss: 0.6042240262031555, acc: 0.8707864880561829)
[2025-02-13 19:52:45,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:46,091][root][INFO] - Training Epoch: 1/2, step 4092/7134 completed (loss: 0.24417871236801147, acc: 0.9319371581077576)
[2025-02-13 19:52:46,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:46,430][root][INFO] - Training Epoch: 1/2, step 4093/7134 completed (loss: 0.21493089199066162, acc: 0.9599999785423279)
[2025-02-13 19:52:46,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:46,788][root][INFO] - Training Epoch: 1/2, step 4094/7134 completed (loss: 0.24963250756263733, acc: 0.9390243887901306)
[2025-02-13 19:52:46,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:47,159][root][INFO] - Training Epoch: 1/2, step 4095/7134 completed (loss: 0.16220809519290924, acc: 0.9554139971733093)
[2025-02-13 19:52:47,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:47,507][root][INFO] - Training Epoch: 1/2, step 4096/7134 completed (loss: 0.3029612898826599, acc: 0.8831169009208679)
[2025-02-13 19:52:47,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:47,941][root][INFO] - Training Epoch: 1/2, step 4097/7134 completed (loss: 0.2981204092502594, acc: 0.9133333563804626)
[2025-02-13 19:52:48,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:48,311][root][INFO] - Training Epoch: 1/2, step 4098/7134 completed (loss: 0.094661645591259, acc: 0.977011501789093)
[2025-02-13 19:52:48,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:48,672][root][INFO] - Training Epoch: 1/2, step 4099/7134 completed (loss: 0.12437975406646729, acc: 0.9772727489471436)
[2025-02-13 19:52:48,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:49,052][root][INFO] - Training Epoch: 1/2, step 4100/7134 completed (loss: 0.03286464139819145, acc: 1.0)
[2025-02-13 19:52:49,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:49,401][root][INFO] - Training Epoch: 1/2, step 4101/7134 completed (loss: 0.27251288294792175, acc: 0.9200000166893005)
[2025-02-13 19:52:49,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:49,763][root][INFO] - Training Epoch: 1/2, step 4102/7134 completed (loss: 0.22657078504562378, acc: 0.9674796462059021)
[2025-02-13 19:52:49,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:50,126][root][INFO] - Training Epoch: 1/2, step 4103/7134 completed (loss: 0.09431704878807068, acc: 0.9738562107086182)
[2025-02-13 19:52:50,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:50,488][root][INFO] - Training Epoch: 1/2, step 4104/7134 completed (loss: 0.2081393003463745, acc: 0.9347826242446899)
[2025-02-13 19:52:50,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:50,854][root][INFO] - Training Epoch: 1/2, step 4105/7134 completed (loss: 0.09090516716241837, acc: 0.9800000190734863)
[2025-02-13 19:52:50,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:51,213][root][INFO] - Training Epoch: 1/2, step 4106/7134 completed (loss: 0.13481496274471283, acc: 0.9659863710403442)
[2025-02-13 19:52:51,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:51,566][root][INFO] - Training Epoch: 1/2, step 4107/7134 completed (loss: 0.1870308816432953, acc: 0.9632353186607361)
[2025-02-13 19:52:51,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:51,920][root][INFO] - Training Epoch: 1/2, step 4108/7134 completed (loss: 0.1013115867972374, acc: 0.9727891087532043)
[2025-02-13 19:52:52,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:52,280][root][INFO] - Training Epoch: 1/2, step 4109/7134 completed (loss: 0.10244221985340118, acc: 0.9863945841789246)
[2025-02-13 19:52:52,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:52,630][root][INFO] - Training Epoch: 1/2, step 4110/7134 completed (loss: 0.04652976244688034, acc: 0.9922480583190918)
[2025-02-13 19:52:52,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:52,995][root][INFO] - Training Epoch: 1/2, step 4111/7134 completed (loss: 0.12844914197921753, acc: 0.951724112033844)
[2025-02-13 19:52:53,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:53,345][root][INFO] - Training Epoch: 1/2, step 4112/7134 completed (loss: 0.11082958430051804, acc: 0.9689922332763672)
[2025-02-13 19:52:53,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:53,710][root][INFO] - Training Epoch: 1/2, step 4113/7134 completed (loss: 0.10315114259719849, acc: 0.9720279574394226)
[2025-02-13 19:52:53,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:54,061][root][INFO] - Training Epoch: 1/2, step 4114/7134 completed (loss: 0.4609972834587097, acc: 0.8608695864677429)
[2025-02-13 19:52:54,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:54,412][root][INFO] - Training Epoch: 1/2, step 4115/7134 completed (loss: 0.38243231177330017, acc: 0.9152542352676392)
[2025-02-13 19:52:54,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:54,813][root][INFO] - Training Epoch: 1/2, step 4116/7134 completed (loss: 0.11086973547935486, acc: 0.9618320465087891)
[2025-02-13 19:52:54,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:55,191][root][INFO] - Training Epoch: 1/2, step 4117/7134 completed (loss: 0.09117496013641357, acc: 0.9722222089767456)
[2025-02-13 19:52:55,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:55,542][root][INFO] - Training Epoch: 1/2, step 4118/7134 completed (loss: 0.0811658576130867, acc: 0.9791666865348816)
[2025-02-13 19:52:55,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:55,913][root][INFO] - Training Epoch: 1/2, step 4119/7134 completed (loss: 0.19567663967609406, acc: 0.9527027010917664)
[2025-02-13 19:52:56,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:56,285][root][INFO] - Training Epoch: 1/2, step 4120/7134 completed (loss: 0.14125704765319824, acc: 0.9849624037742615)
[2025-02-13 19:52:56,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:56,635][root][INFO] - Training Epoch: 1/2, step 4121/7134 completed (loss: 0.06653215736150742, acc: 0.9817073345184326)
[2025-02-13 19:52:56,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:57,004][root][INFO] - Training Epoch: 1/2, step 4122/7134 completed (loss: 0.054097291082143784, acc: 0.9831932783126831)
[2025-02-13 19:52:57,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:57,360][root][INFO] - Training Epoch: 1/2, step 4123/7134 completed (loss: 0.05316546559333801, acc: 0.9870129823684692)
[2025-02-13 19:52:57,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:57,728][root][INFO] - Training Epoch: 1/2, step 4124/7134 completed (loss: 0.06336747854948044, acc: 0.987500011920929)
[2025-02-13 19:52:57,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:58,083][root][INFO] - Training Epoch: 1/2, step 4125/7134 completed (loss: 0.08375272154808044, acc: 0.9685534834861755)
[2025-02-13 19:52:58,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:58,442][root][INFO] - Training Epoch: 1/2, step 4126/7134 completed (loss: 0.14000096917152405, acc: 0.9539473652839661)
[2025-02-13 19:52:58,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:58,813][root][INFO] - Training Epoch: 1/2, step 4127/7134 completed (loss: 0.13054244220256805, acc: 0.9731543660163879)
[2025-02-13 19:52:58,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:59,205][root][INFO] - Training Epoch: 1/2, step 4128/7134 completed (loss: 0.049926429986953735, acc: 0.9924812316894531)
[2025-02-13 19:52:59,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:59,600][root][INFO] - Training Epoch: 1/2, step 4129/7134 completed (loss: 0.10441378504037857, acc: 0.9597315192222595)
[2025-02-13 19:52:59,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:59,962][root][INFO] - Training Epoch: 1/2, step 4130/7134 completed (loss: 0.1212066262960434, acc: 0.9482758641242981)
[2025-02-13 19:53:00,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:00,333][root][INFO] - Training Epoch: 1/2, step 4131/7134 completed (loss: 0.12896892428398132, acc: 0.9629629850387573)
[2025-02-13 19:53:00,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:00,703][root][INFO] - Training Epoch: 1/2, step 4132/7134 completed (loss: 0.048533663153648376, acc: 1.0)
[2025-02-13 19:53:00,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:01,066][root][INFO] - Training Epoch: 1/2, step 4133/7134 completed (loss: 0.2822728157043457, acc: 0.9440559148788452)
[2025-02-13 19:53:01,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:01,432][root][INFO] - Training Epoch: 1/2, step 4134/7134 completed (loss: 0.3951198160648346, acc: 0.8999999761581421)
[2025-02-13 19:53:01,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:01,808][root][INFO] - Training Epoch: 1/2, step 4135/7134 completed (loss: 0.059325315058231354, acc: 0.9857142567634583)
[2025-02-13 19:53:01,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:02,210][root][INFO] - Training Epoch: 1/2, step 4136/7134 completed (loss: 0.08198489993810654, acc: 0.969924807548523)
[2025-02-13 19:53:02,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:02,585][root][INFO] - Training Epoch: 1/2, step 4137/7134 completed (loss: 0.08205201476812363, acc: 0.9926470518112183)
[2025-02-13 19:53:02,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:02,947][root][INFO] - Training Epoch: 1/2, step 4138/7134 completed (loss: 0.22097797691822052, acc: 0.9411764740943909)
[2025-02-13 19:53:03,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:03,286][root][INFO] - Training Epoch: 1/2, step 4139/7134 completed (loss: 0.11750946193933487, acc: 0.9714285731315613)
[2025-02-13 19:53:03,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:03,644][root][INFO] - Training Epoch: 1/2, step 4140/7134 completed (loss: 0.36966580152511597, acc: 0.9152542352676392)
[2025-02-13 19:53:03,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:04,042][root][INFO] - Training Epoch: 1/2, step 4141/7134 completed (loss: 0.25666147470474243, acc: 0.9166666865348816)
[2025-02-13 19:53:04,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:04,412][root][INFO] - Training Epoch: 1/2, step 4142/7134 completed (loss: 0.33772528171539307, acc: 0.8999999761581421)
[2025-02-13 19:53:04,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:04,782][root][INFO] - Training Epoch: 1/2, step 4143/7134 completed (loss: 0.16924425959587097, acc: 0.9650654792785645)
[2025-02-13 19:53:04,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:05,166][root][INFO] - Training Epoch: 1/2, step 4144/7134 completed (loss: 0.2801250219345093, acc: 0.9128205180168152)
[2025-02-13 19:53:05,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:05,537][root][INFO] - Training Epoch: 1/2, step 4145/7134 completed (loss: 0.3864874839782715, acc: 0.8813559412956238)
[2025-02-13 19:53:05,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:05,910][root][INFO] - Training Epoch: 1/2, step 4146/7134 completed (loss: 0.4965593218803406, acc: 0.8482142686843872)
[2025-02-13 19:53:06,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:06,287][root][INFO] - Training Epoch: 1/2, step 4147/7134 completed (loss: 0.7127331495285034, acc: 0.8474576473236084)
[2025-02-13 19:53:06,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:06,619][root][INFO] - Training Epoch: 1/2, step 4148/7134 completed (loss: 0.8726903796195984, acc: 0.8120805621147156)
[2025-02-13 19:53:06,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:06,953][root][INFO] - Training Epoch: 1/2, step 4149/7134 completed (loss: 0.6060090661048889, acc: 0.8604651093482971)
[2025-02-13 19:53:07,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:07,310][root][INFO] - Training Epoch: 1/2, step 4150/7134 completed (loss: 0.8532052040100098, acc: 0.8461538553237915)
[2025-02-13 19:53:07,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:07,702][root][INFO] - Training Epoch: 1/2, step 4151/7134 completed (loss: 0.4057948589324951, acc: 0.9256198406219482)
[2025-02-13 19:53:07,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:08,088][root][INFO] - Training Epoch: 1/2, step 4152/7134 completed (loss: 0.20355510711669922, acc: 0.9251700639724731)
[2025-02-13 19:53:08,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:08,485][root][INFO] - Training Epoch: 1/2, step 4153/7134 completed (loss: 0.3586052358150482, acc: 0.9428571462631226)
[2025-02-13 19:53:08,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:08,878][root][INFO] - Training Epoch: 1/2, step 4154/7134 completed (loss: 0.25966987013816833, acc: 0.9138755798339844)
[2025-02-13 19:53:09,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:09,246][root][INFO] - Training Epoch: 1/2, step 4155/7134 completed (loss: 0.26299014687538147, acc: 0.9194630980491638)
[2025-02-13 19:53:09,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:09,621][root][INFO] - Training Epoch: 1/2, step 4156/7134 completed (loss: 0.2809370756149292, acc: 0.9415204524993896)
[2025-02-13 19:53:09,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:09,973][root][INFO] - Training Epoch: 1/2, step 4157/7134 completed (loss: 0.20681509375572205, acc: 0.9411764740943909)
[2025-02-13 19:53:10,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:10,359][root][INFO] - Training Epoch: 1/2, step 4158/7134 completed (loss: 0.5616021752357483, acc: 0.901098906993866)
[2025-02-13 19:53:10,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:10,728][root][INFO] - Training Epoch: 1/2, step 4159/7134 completed (loss: 0.25258514285087585, acc: 0.9548872113227844)
[2025-02-13 19:53:10,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:11,083][root][INFO] - Training Epoch: 1/2, step 4160/7134 completed (loss: 0.3074837923049927, acc: 0.9327731132507324)
[2025-02-13 19:53:11,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:11,475][root][INFO] - Training Epoch: 1/2, step 4161/7134 completed (loss: 0.1446196436882019, acc: 0.982758641242981)
[2025-02-13 19:53:11,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:11,837][root][INFO] - Training Epoch: 1/2, step 4162/7134 completed (loss: 0.23965252935886383, acc: 0.9341317415237427)
[2025-02-13 19:53:11,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:12,212][root][INFO] - Training Epoch: 1/2, step 4163/7134 completed (loss: 0.24460013210773468, acc: 0.9597315192222595)
[2025-02-13 19:53:12,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:12,595][root][INFO] - Training Epoch: 1/2, step 4164/7134 completed (loss: 0.18312808871269226, acc: 0.9503105878829956)
[2025-02-13 19:53:12,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:12,999][root][INFO] - Training Epoch: 1/2, step 4165/7134 completed (loss: 0.22412802278995514, acc: 0.9408602118492126)
[2025-02-13 19:53:13,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:13,359][root][INFO] - Training Epoch: 1/2, step 4166/7134 completed (loss: 0.2971861660480499, acc: 0.9166666865348816)
[2025-02-13 19:53:13,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:13,759][root][INFO] - Training Epoch: 1/2, step 4167/7134 completed (loss: 0.26632627844810486, acc: 0.9268292784690857)
[2025-02-13 19:53:13,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:14,140][root][INFO] - Training Epoch: 1/2, step 4168/7134 completed (loss: 0.25078463554382324, acc: 0.9398496150970459)
[2025-02-13 19:53:14,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:14,514][root][INFO] - Training Epoch: 1/2, step 4169/7134 completed (loss: 0.17557670176029205, acc: 0.9539170265197754)
[2025-02-13 19:53:14,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:14,883][root][INFO] - Training Epoch: 1/2, step 4170/7134 completed (loss: 0.26412779092788696, acc: 0.9428571462631226)
[2025-02-13 19:53:15,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:15,240][root][INFO] - Training Epoch: 1/2, step 4171/7134 completed (loss: 0.11752751469612122, acc: 0.9704433679580688)
[2025-02-13 19:53:15,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:15,592][root][INFO] - Training Epoch: 1/2, step 4172/7134 completed (loss: 0.19442790746688843, acc: 0.9444444179534912)
[2025-02-13 19:53:15,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:15,979][root][INFO] - Training Epoch: 1/2, step 4173/7134 completed (loss: 0.18961213529109955, acc: 0.9351851940155029)
[2025-02-13 19:53:16,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:16,337][root][INFO] - Training Epoch: 1/2, step 4174/7134 completed (loss: 0.024898072704672813, acc: 0.9920634627342224)
[2025-02-13 19:53:16,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:16,708][root][INFO] - Training Epoch: 1/2, step 4175/7134 completed (loss: 0.1894742101430893, acc: 0.9366196990013123)
[2025-02-13 19:53:16,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:17,151][root][INFO] - Training Epoch: 1/2, step 4176/7134 completed (loss: 0.056102924048900604, acc: 0.9935483932495117)
[2025-02-13 19:53:17,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:17,537][root][INFO] - Training Epoch: 1/2, step 4177/7134 completed (loss: 0.13005532324314117, acc: 0.9649122953414917)
[2025-02-13 19:53:17,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:17,935][root][INFO] - Training Epoch: 1/2, step 4178/7134 completed (loss: 0.1577661633491516, acc: 0.9605262875556946)
[2025-02-13 19:53:18,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:18,342][root][INFO] - Training Epoch: 1/2, step 4179/7134 completed (loss: 0.04915718361735344, acc: 1.0)
[2025-02-13 19:53:18,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:18,771][root][INFO] - Training Epoch: 1/2, step 4180/7134 completed (loss: 0.05845290422439575, acc: 0.987500011920929)
[2025-02-13 19:53:18,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:19,150][root][INFO] - Training Epoch: 1/2, step 4181/7134 completed (loss: 0.07571464776992798, acc: 0.9710144996643066)
[2025-02-13 19:53:19,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:19,519][root][INFO] - Training Epoch: 1/2, step 4182/7134 completed (loss: 0.06494951993227005, acc: 0.9870129823684692)
[2025-02-13 19:53:19,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:19,902][root][INFO] - Training Epoch: 1/2, step 4183/7134 completed (loss: 0.1315067857503891, acc: 0.9693251252174377)
[2025-02-13 19:53:20,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:20,275][root][INFO] - Training Epoch: 1/2, step 4184/7134 completed (loss: 0.10367846488952637, acc: 0.970588207244873)
[2025-02-13 19:53:20,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:20,669][root][INFO] - Training Epoch: 1/2, step 4185/7134 completed (loss: 0.1151229739189148, acc: 0.9809523820877075)
[2025-02-13 19:53:20,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:21,055][root][INFO] - Training Epoch: 1/2, step 4186/7134 completed (loss: 0.14565639197826385, acc: 0.9845361113548279)
[2025-02-13 19:53:21,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:21,419][root][INFO] - Training Epoch: 1/2, step 4187/7134 completed (loss: 0.036762550473213196, acc: 0.9919354915618896)
[2025-02-13 19:53:21,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:21,783][root][INFO] - Training Epoch: 1/2, step 4188/7134 completed (loss: 0.055678848177194595, acc: 0.9842105507850647)
[2025-02-13 19:53:21,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:22,129][root][INFO] - Training Epoch: 1/2, step 4189/7134 completed (loss: 0.07447637617588043, acc: 0.9820359349250793)
[2025-02-13 19:53:22,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:22,513][root][INFO] - Training Epoch: 1/2, step 4190/7134 completed (loss: 0.13729329407215118, acc: 0.9649122953414917)
[2025-02-13 19:53:22,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:22,845][root][INFO] - Training Epoch: 1/2, step 4191/7134 completed (loss: 0.13055019080638885, acc: 0.9711538553237915)
[2025-02-13 19:53:22,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:23,184][root][INFO] - Training Epoch: 1/2, step 4192/7134 completed (loss: 0.050928495824337006, acc: 0.9863013625144958)
[2025-02-13 19:53:23,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:23,535][root][INFO] - Training Epoch: 1/2, step 4193/7134 completed (loss: 0.11619234085083008, acc: 0.9720670580863953)
[2025-02-13 19:53:23,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:23,920][root][INFO] - Training Epoch: 1/2, step 4194/7134 completed (loss: 0.036301445215940475, acc: 0.9871794581413269)
[2025-02-13 19:53:24,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:24,308][root][INFO] - Training Epoch: 1/2, step 4195/7134 completed (loss: 0.16164302825927734, acc: 0.9599999785423279)
[2025-02-13 19:53:24,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:24,685][root][INFO] - Training Epoch: 1/2, step 4196/7134 completed (loss: 0.11944472789764404, acc: 0.9784172773361206)
[2025-02-13 19:53:24,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:25,052][root][INFO] - Training Epoch: 1/2, step 4197/7134 completed (loss: 0.11708712577819824, acc: 0.9728260636329651)
[2025-02-13 19:53:25,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:25,415][root][INFO] - Training Epoch: 1/2, step 4198/7134 completed (loss: 0.07862425595521927, acc: 0.9942857027053833)
[2025-02-13 19:53:25,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:25,765][root][INFO] - Training Epoch: 1/2, step 4199/7134 completed (loss: 0.25953733921051025, acc: 0.9507042169570923)
[2025-02-13 19:53:25,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26,147][root][INFO] - Training Epoch: 1/2, step 4200/7134 completed (loss: 0.17996372282505035, acc: 0.9586206674575806)
[2025-02-13 19:53:26,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26,523][root][INFO] - Training Epoch: 1/2, step 4201/7134 completed (loss: 0.06239798292517662, acc: 0.9858155846595764)
[2025-02-13 19:53:26,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26,906][root][INFO] - Training Epoch: 1/2, step 4202/7134 completed (loss: 0.14408889412879944, acc: 0.969924807548523)
[2025-02-13 19:53:27,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:27,304][root][INFO] - Training Epoch: 1/2, step 4203/7134 completed (loss: 0.12146785855293274, acc: 0.9746835231781006)
[2025-02-13 19:53:27,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:27,685][root][INFO] - Training Epoch: 1/2, step 4204/7134 completed (loss: 0.19409114122390747, acc: 0.9599999785423279)
[2025-02-13 19:53:27,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:28,053][root][INFO] - Training Epoch: 1/2, step 4205/7134 completed (loss: 0.19132840633392334, acc: 0.9647058844566345)
[2025-02-13 19:53:28,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:28,424][root][INFO] - Training Epoch: 1/2, step 4206/7134 completed (loss: 0.20363040268421173, acc: 0.9623655676841736)
[2025-02-13 19:53:28,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:28,812][root][INFO] - Training Epoch: 1/2, step 4207/7134 completed (loss: 0.15219035744667053, acc: 0.9811320900917053)
[2025-02-13 19:53:28,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:29,174][root][INFO] - Training Epoch: 1/2, step 4208/7134 completed (loss: 0.16092140972614288, acc: 0.9712643623352051)
[2025-02-13 19:53:29,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:29,536][root][INFO] - Training Epoch: 1/2, step 4209/7134 completed (loss: 0.24218618869781494, acc: 0.9668508172035217)
[2025-02-13 19:53:29,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:29,899][root][INFO] - Training Epoch: 1/2, step 4210/7134 completed (loss: 0.13075418770313263, acc: 0.9561403393745422)
[2025-02-13 19:53:30,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:30,261][root][INFO] - Training Epoch: 1/2, step 4211/7134 completed (loss: 0.07100566476583481, acc: 0.9672130942344666)
[2025-02-13 19:53:30,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:30,661][root][INFO] - Training Epoch: 1/2, step 4212/7134 completed (loss: 0.12876731157302856, acc: 0.9677419066429138)
[2025-02-13 19:53:30,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:31,049][root][INFO] - Training Epoch: 1/2, step 4213/7134 completed (loss: 0.036301389336586, acc: 0.9942528605461121)
[2025-02-13 19:53:31,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:31,421][root][INFO] - Training Epoch: 1/2, step 4214/7134 completed (loss: 0.19196845591068268, acc: 0.9649122953414917)
[2025-02-13 19:53:31,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:31,751][root][INFO] - Training Epoch: 1/2, step 4215/7134 completed (loss: 0.10579090565443039, acc: 0.9763779640197754)
[2025-02-13 19:53:31,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:32,097][root][INFO] - Training Epoch: 1/2, step 4216/7134 completed (loss: 0.10200653225183487, acc: 0.9923076629638672)
[2025-02-13 19:53:32,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:32,443][root][INFO] - Training Epoch: 1/2, step 4217/7134 completed (loss: 0.05474357679486275, acc: 0.9833333492279053)
[2025-02-13 19:53:32,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:32,797][root][INFO] - Training Epoch: 1/2, step 4218/7134 completed (loss: 0.2246534675359726, acc: 0.9701492786407471)
[2025-02-13 19:53:32,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:33,113][root][INFO] - Training Epoch: 1/2, step 4219/7134 completed (loss: 0.07819808274507523, acc: 0.9710144996643066)
[2025-02-13 19:53:33,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:33,495][root][INFO] - Training Epoch: 1/2, step 4220/7134 completed (loss: 0.2664898633956909, acc: 0.9545454382896423)
[2025-02-13 19:53:33,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:33,874][root][INFO] - Training Epoch: 1/2, step 4221/7134 completed (loss: 0.16641288995742798, acc: 0.9662162065505981)
[2025-02-13 19:53:34,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:34,291][root][INFO] - Training Epoch: 1/2, step 4222/7134 completed (loss: 0.5333948135375977, acc: 0.9210526347160339)
[2025-02-13 19:53:34,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:34,674][root][INFO] - Training Epoch: 1/2, step 4223/7134 completed (loss: 0.3134326934814453, acc: 0.909547746181488)
[2025-02-13 19:53:34,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:35,048][root][INFO] - Training Epoch: 1/2, step 4224/7134 completed (loss: 0.3449772000312805, acc: 0.9435483813285828)
[2025-02-13 19:53:35,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:35,427][root][INFO] - Training Epoch: 1/2, step 4225/7134 completed (loss: 0.28411927819252014, acc: 0.9294871687889099)
[2025-02-13 19:53:35,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:35,792][root][INFO] - Training Epoch: 1/2, step 4226/7134 completed (loss: 0.35956284403800964, acc: 0.9086294174194336)
[2025-02-13 19:53:35,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:36,171][root][INFO] - Training Epoch: 1/2, step 4227/7134 completed (loss: 0.49065399169921875, acc: 0.8679245114326477)
[2025-02-13 19:53:36,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:36,541][root][INFO] - Training Epoch: 1/2, step 4228/7134 completed (loss: 0.597571611404419, acc: 0.8589743375778198)
[2025-02-13 19:53:36,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:36,917][root][INFO] - Training Epoch: 1/2, step 4229/7134 completed (loss: 0.4379374086856842, acc: 0.8799999952316284)
[2025-02-13 19:53:37,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:37,289][root][INFO] - Training Epoch: 1/2, step 4230/7134 completed (loss: 0.4297953248023987, acc: 0.930232584476471)
[2025-02-13 19:53:37,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:37,652][root][INFO] - Training Epoch: 1/2, step 4231/7134 completed (loss: 0.621705949306488, acc: 0.8657718300819397)
[2025-02-13 19:53:37,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:38,046][root][INFO] - Training Epoch: 1/2, step 4232/7134 completed (loss: 0.3313846290111542, acc: 0.9111111164093018)
[2025-02-13 19:53:38,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:38,417][root][INFO] - Training Epoch: 1/2, step 4233/7134 completed (loss: 0.2595912516117096, acc: 0.9503546357154846)
[2025-02-13 19:53:38,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:38,839][root][INFO] - Training Epoch: 1/2, step 4234/7134 completed (loss: 0.1366075873374939, acc: 0.9645389914512634)
[2025-02-13 19:53:38,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39,222][root][INFO] - Training Epoch: 1/2, step 4235/7134 completed (loss: 0.14759042859077454, acc: 0.95652174949646)
[2025-02-13 19:53:39,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39,585][root][INFO] - Training Epoch: 1/2, step 4236/7134 completed (loss: 0.09069085866212845, acc: 0.9784946441650391)
[2025-02-13 19:53:39,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39,959][root][INFO] - Training Epoch: 1/2, step 4237/7134 completed (loss: 0.15058697760105133, acc: 0.9626168012619019)
[2025-02-13 19:53:40,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:40,328][root][INFO] - Training Epoch: 1/2, step 4238/7134 completed (loss: 0.18856221437454224, acc: 0.9846153855323792)
[2025-02-13 19:53:40,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:40,673][root][INFO] - Training Epoch: 1/2, step 4239/7134 completed (loss: 0.1505303829908371, acc: 0.9679999947547913)
[2025-02-13 19:53:40,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:41,066][root][INFO] - Training Epoch: 1/2, step 4240/7134 completed (loss: 0.26310721039772034, acc: 0.9366196990013123)
[2025-02-13 19:53:41,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:41,458][root][INFO] - Training Epoch: 1/2, step 4241/7134 completed (loss: 0.06098105385899544, acc: 0.9900000095367432)
[2025-02-13 19:53:41,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:41,835][root][INFO] - Training Epoch: 1/2, step 4242/7134 completed (loss: 0.17062683403491974, acc: 0.9693877696990967)
[2025-02-13 19:53:41,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42,266][root][INFO] - Training Epoch: 1/2, step 4243/7134 completed (loss: 0.1831236332654953, acc: 0.9504132270812988)
[2025-02-13 19:53:42,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42,627][root][INFO] - Training Epoch: 1/2, step 4244/7134 completed (loss: 0.07093024998903275, acc: 0.9905660152435303)
[2025-02-13 19:53:42,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42,980][root][INFO] - Training Epoch: 1/2, step 4245/7134 completed (loss: 0.13632874190807343, acc: 0.9578947424888611)
[2025-02-13 19:53:43,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:43,351][root][INFO] - Training Epoch: 1/2, step 4246/7134 completed (loss: 0.12439074367284775, acc: 0.9589040875434875)
[2025-02-13 19:53:43,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:43,753][root][INFO] - Training Epoch: 1/2, step 4247/7134 completed (loss: 0.09931778162717819, acc: 0.984000027179718)
[2025-02-13 19:53:43,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:44,137][root][INFO] - Training Epoch: 1/2, step 4248/7134 completed (loss: 0.24981997907161713, acc: 0.9520547986030579)
[2025-02-13 19:53:44,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:44,577][root][INFO] - Training Epoch: 1/2, step 4249/7134 completed (loss: 0.280049592256546, acc: 0.9558823704719543)
[2025-02-13 19:53:44,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:44,968][root][INFO] - Training Epoch: 1/2, step 4250/7134 completed (loss: 0.33888304233551025, acc: 0.9454545378684998)
[2025-02-13 19:53:45,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:45,339][root][INFO] - Training Epoch: 1/2, step 4251/7134 completed (loss: 0.15924029052257538, acc: 0.9548872113227844)
[2025-02-13 19:53:45,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:45,717][root][INFO] - Training Epoch: 1/2, step 4252/7134 completed (loss: 0.1859525442123413, acc: 0.9516128897666931)
[2025-02-13 19:53:45,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:46,088][root][INFO] - Training Epoch: 1/2, step 4253/7134 completed (loss: 0.15425415337085724, acc: 0.95652174949646)
[2025-02-13 19:53:46,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:46,444][root][INFO] - Training Epoch: 1/2, step 4254/7134 completed (loss: 0.09491610527038574, acc: 0.9772727489471436)
[2025-02-13 19:53:46,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:46,835][root][INFO] - Training Epoch: 1/2, step 4255/7134 completed (loss: 0.22437232732772827, acc: 0.9461538195610046)
[2025-02-13 19:53:46,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:47,197][root][INFO] - Training Epoch: 1/2, step 4256/7134 completed (loss: 0.11289329081773758, acc: 0.9658119678497314)
[2025-02-13 19:53:47,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:47,614][root][INFO] - Training Epoch: 1/2, step 4257/7134 completed (loss: 0.09066648781299591, acc: 0.9621211886405945)
[2025-02-13 19:53:47,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:47,999][root][INFO] - Training Epoch: 1/2, step 4258/7134 completed (loss: 0.06149899959564209, acc: 0.9913793206214905)
[2025-02-13 19:53:48,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:48,385][root][INFO] - Training Epoch: 1/2, step 4259/7134 completed (loss: 0.28821977972984314, acc: 0.9437500238418579)
[2025-02-13 19:53:48,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:48,789][root][INFO] - Training Epoch: 1/2, step 4260/7134 completed (loss: 0.12581698596477509, acc: 0.9724770784378052)
[2025-02-13 19:53:48,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:49,155][root][INFO] - Training Epoch: 1/2, step 4261/7134 completed (loss: 0.13015708327293396, acc: 0.947826087474823)
[2025-02-13 19:53:49,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:49,556][root][INFO] - Training Epoch: 1/2, step 4262/7134 completed (loss: 0.16415604948997498, acc: 0.9510489702224731)
[2025-02-13 19:53:49,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:49,941][root][INFO] - Training Epoch: 1/2, step 4263/7134 completed (loss: 0.17274431884288788, acc: 0.9629629850387573)
[2025-02-13 19:53:50,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:50,335][root][INFO] - Training Epoch: 1/2, step 4264/7134 completed (loss: 0.31012988090515137, acc: 0.915730357170105)
[2025-02-13 19:53:50,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:50,737][root][INFO] - Training Epoch: 1/2, step 4265/7134 completed (loss: 0.21245789527893066, acc: 0.9360465407371521)
[2025-02-13 19:53:50,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:51,135][root][INFO] - Training Epoch: 1/2, step 4266/7134 completed (loss: 0.10058095306158066, acc: 0.9738219976425171)
[2025-02-13 19:53:51,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:51,516][root][INFO] - Training Epoch: 1/2, step 4267/7134 completed (loss: 0.1338542103767395, acc: 0.9545454382896423)
[2025-02-13 19:53:51,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:51,893][root][INFO] - Training Epoch: 1/2, step 4268/7134 completed (loss: 0.28441762924194336, acc: 0.9268292784690857)
[2025-02-13 19:53:52,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:52,277][root][INFO] - Training Epoch: 1/2, step 4269/7134 completed (loss: 0.3171388804912567, acc: 0.9364162087440491)
[2025-02-13 19:53:52,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:52,650][root][INFO] - Training Epoch: 1/2, step 4270/7134 completed (loss: 0.1989894062280655, acc: 0.9427083134651184)
[2025-02-13 19:53:52,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:53,064][root][INFO] - Training Epoch: 1/2, step 4271/7134 completed (loss: 0.1121753454208374, acc: 0.9621621370315552)
[2025-02-13 19:53:53,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:53,446][root][INFO] - Training Epoch: 1/2, step 4272/7134 completed (loss: 0.1957835853099823, acc: 0.9502487778663635)
[2025-02-13 19:53:53,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:53,829][root][INFO] - Training Epoch: 1/2, step 4273/7134 completed (loss: 0.12762674689292908, acc: 0.9682539701461792)
[2025-02-13 19:53:53,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:54,207][root][INFO] - Training Epoch: 1/2, step 4274/7134 completed (loss: 0.3006672263145447, acc: 0.936274528503418)
[2025-02-13 19:53:54,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:54,582][root][INFO] - Training Epoch: 1/2, step 4275/7134 completed (loss: 0.1298617720603943, acc: 0.9608938694000244)
[2025-02-13 19:53:54,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:54,966][root][INFO] - Training Epoch: 1/2, step 4276/7134 completed (loss: 0.1493014097213745, acc: 0.9729729890823364)
[2025-02-13 19:53:55,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:55,345][root][INFO] - Training Epoch: 1/2, step 4277/7134 completed (loss: 0.06624811887741089, acc: 0.9788732528686523)
[2025-02-13 19:53:55,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:55,721][root][INFO] - Training Epoch: 1/2, step 4278/7134 completed (loss: 0.1236565038561821, acc: 0.9795918464660645)
[2025-02-13 19:53:55,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:56,110][root][INFO] - Training Epoch: 1/2, step 4279/7134 completed (loss: 0.10021655261516571, acc: 0.9751552939414978)
[2025-02-13 19:53:56,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:56,506][root][INFO] - Training Epoch: 1/2, step 4280/7134 completed (loss: 0.049185846000909805, acc: 0.9858155846595764)
[2025-02-13 19:53:56,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:56,933][root][INFO] - Training Epoch: 1/2, step 4281/7134 completed (loss: 0.1623786836862564, acc: 0.9569892287254333)
[2025-02-13 19:53:57,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:57,340][root][INFO] - Training Epoch: 1/2, step 4282/7134 completed (loss: 0.14435021579265594, acc: 0.9396551847457886)
[2025-02-13 19:53:57,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:57,757][root][INFO] - Training Epoch: 1/2, step 4283/7134 completed (loss: 0.11643263697624207, acc: 0.9744898080825806)
[2025-02-13 19:53:57,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:58,162][root][INFO] - Training Epoch: 1/2, step 4284/7134 completed (loss: 0.19247952103614807, acc: 0.9457364082336426)
[2025-02-13 19:53:58,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:58,534][root][INFO] - Training Epoch: 1/2, step 4285/7134 completed (loss: 0.06559215486049652, acc: 0.9844961166381836)
[2025-02-13 19:53:58,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:58,897][root][INFO] - Training Epoch: 1/2, step 4286/7134 completed (loss: 0.08505865186452866, acc: 0.9663865566253662)
[2025-02-13 19:53:59,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:59,282][root][INFO] - Training Epoch: 1/2, step 4287/7134 completed (loss: 0.1855960488319397, acc: 0.967391312122345)
[2025-02-13 19:53:59,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:59,677][root][INFO] - Training Epoch: 1/2, step 4288/7134 completed (loss: 0.13439135253429413, acc: 0.9588235020637512)
[2025-02-13 19:53:59,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:00,063][root][INFO] - Training Epoch: 1/2, step 4289/7134 completed (loss: 0.224356546998024, acc: 0.9386503100395203)
[2025-02-13 19:54:00,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:00,440][root][INFO] - Training Epoch: 1/2, step 4290/7134 completed (loss: 0.124534972012043, acc: 0.9826589822769165)
[2025-02-13 19:54:00,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:00,887][root][INFO] - Training Epoch: 1/2, step 4291/7134 completed (loss: 0.07277676463127136, acc: 0.9768785834312439)
[2025-02-13 19:54:01,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:01,322][root][INFO] - Training Epoch: 1/2, step 4292/7134 completed (loss: 0.165161594748497, acc: 0.9677419066429138)
[2025-02-13 19:54:01,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:01,723][root][INFO] - Training Epoch: 1/2, step 4293/7134 completed (loss: 0.13638237118721008, acc: 0.9666666388511658)
[2025-02-13 19:54:01,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:02,113][root][INFO] - Training Epoch: 1/2, step 4294/7134 completed (loss: 0.2568529546260834, acc: 0.9371428489685059)
[2025-02-13 19:54:02,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:02,497][root][INFO] - Training Epoch: 1/2, step 4295/7134 completed (loss: 0.2251589149236679, acc: 0.949999988079071)
[2025-02-13 19:54:02,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:02,870][root][INFO] - Training Epoch: 1/2, step 4296/7134 completed (loss: 0.0725623145699501, acc: 0.9801980257034302)
[2025-02-13 19:54:03,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:03,279][root][INFO] - Training Epoch: 1/2, step 4297/7134 completed (loss: 0.04343139007687569, acc: 0.9809523820877075)
[2025-02-13 19:54:03,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:03,689][root][INFO] - Training Epoch: 1/2, step 4298/7134 completed (loss: 0.1274213194847107, acc: 0.9659090638160706)
[2025-02-13 19:54:03,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:04,051][root][INFO] - Training Epoch: 1/2, step 4299/7134 completed (loss: 0.19565139710903168, acc: 0.9301075339317322)
[2025-02-13 19:54:04,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:04,466][root][INFO] - Training Epoch: 1/2, step 4300/7134 completed (loss: 0.3200951814651489, acc: 0.9159663915634155)
[2025-02-13 19:54:04,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:04,846][root][INFO] - Training Epoch: 1/2, step 4301/7134 completed (loss: 0.08734317123889923, acc: 0.9718309640884399)
[2025-02-13 19:54:04,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:05,230][root][INFO] - Training Epoch: 1/2, step 4302/7134 completed (loss: 0.28274527192115784, acc: 0.9426751732826233)
[2025-02-13 19:54:05,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:05,616][root][INFO] - Training Epoch: 1/2, step 4303/7134 completed (loss: 0.19956998527050018, acc: 0.9504950642585754)
[2025-02-13 19:54:05,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:06,009][root][INFO] - Training Epoch: 1/2, step 4304/7134 completed (loss: 0.14187024533748627, acc: 0.9585798978805542)
[2025-02-13 19:54:06,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:06,405][root][INFO] - Training Epoch: 1/2, step 4305/7134 completed (loss: 0.12836068868637085, acc: 0.9700000286102295)
[2025-02-13 19:54:06,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:06,840][root][INFO] - Training Epoch: 1/2, step 4306/7134 completed (loss: 0.20191216468811035, acc: 0.9581151604652405)
[2025-02-13 19:54:06,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:07,264][root][INFO] - Training Epoch: 1/2, step 4307/7134 completed (loss: 0.15149487555027008, acc: 0.9607843160629272)
[2025-02-13 19:54:07,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:07,674][root][INFO] - Training Epoch: 1/2, step 4308/7134 completed (loss: 0.17958781123161316, acc: 0.9518716335296631)
[2025-02-13 19:54:07,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:08,064][root][INFO] - Training Epoch: 1/2, step 4309/7134 completed (loss: 0.2264302670955658, acc: 0.9431818127632141)
[2025-02-13 19:54:08,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:08,413][root][INFO] - Training Epoch: 1/2, step 4310/7134 completed (loss: 0.17675304412841797, acc: 0.9593908786773682)
[2025-02-13 19:54:08,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:08,774][root][INFO] - Training Epoch: 1/2, step 4311/7134 completed (loss: 0.10423887521028519, acc: 0.9716312289237976)
[2025-02-13 19:54:08,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:09,125][root][INFO] - Training Epoch: 1/2, step 4312/7134 completed (loss: 0.13903601467609406, acc: 0.9631901979446411)
[2025-02-13 19:54:09,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:09,499][root][INFO] - Training Epoch: 1/2, step 4313/7134 completed (loss: 0.09537577629089355, acc: 0.9636363387107849)
[2025-02-13 19:54:09,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:09,913][root][INFO] - Training Epoch: 1/2, step 4314/7134 completed (loss: 0.13749338686466217, acc: 0.9803921580314636)
[2025-02-13 19:54:10,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:10,278][root][INFO] - Training Epoch: 1/2, step 4315/7134 completed (loss: 0.13032935559749603, acc: 0.9642857313156128)
[2025-02-13 19:54:10,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:10,659][root][INFO] - Training Epoch: 1/2, step 4316/7134 completed (loss: 0.07913918793201447, acc: 0.9820359349250793)
[2025-02-13 19:54:10,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:11,068][root][INFO] - Training Epoch: 1/2, step 4317/7134 completed (loss: 0.07190114259719849, acc: 0.9825581312179565)
[2025-02-13 19:54:11,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:11,484][root][INFO] - Training Epoch: 1/2, step 4318/7134 completed (loss: 0.1826486587524414, acc: 0.954081654548645)
[2025-02-13 19:54:11,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:11,886][root][INFO] - Training Epoch: 1/2, step 4319/7134 completed (loss: 0.09343759715557098, acc: 0.9655172228813171)
[2025-02-13 19:54:12,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:12,271][root][INFO] - Training Epoch: 1/2, step 4320/7134 completed (loss: 0.18602408468723297, acc: 0.942307710647583)
[2025-02-13 19:54:12,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:12,668][root][INFO] - Training Epoch: 1/2, step 4321/7134 completed (loss: 0.49537575244903564, acc: 0.9207921028137207)
[2025-02-13 19:54:12,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:13,042][root][INFO] - Training Epoch: 1/2, step 4322/7134 completed (loss: 0.3808940351009369, acc: 0.9119496941566467)
[2025-02-13 19:54:13,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:13,399][root][INFO] - Training Epoch: 1/2, step 4323/7134 completed (loss: 0.14585094153881073, acc: 0.9473684430122375)
[2025-02-13 19:54:13,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:13,765][root][INFO] - Training Epoch: 1/2, step 4324/7134 completed (loss: 0.118087999522686, acc: 0.9739130139350891)
[2025-02-13 19:54:13,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:14,136][root][INFO] - Training Epoch: 1/2, step 4325/7134 completed (loss: 0.2047613561153412, acc: 0.9383561611175537)
[2025-02-13 19:54:14,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:14,510][root][INFO] - Training Epoch: 1/2, step 4326/7134 completed (loss: 0.2648946940898895, acc: 0.9545454382896423)
[2025-02-13 19:54:14,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:14,882][root][INFO] - Training Epoch: 1/2, step 4327/7134 completed (loss: 0.0908621996641159, acc: 0.9635036587715149)
[2025-02-13 19:54:15,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:15,306][root][INFO] - Training Epoch: 1/2, step 4328/7134 completed (loss: 0.11168409883975983, acc: 0.9765625)
[2025-02-13 19:54:15,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:15,699][root][INFO] - Training Epoch: 1/2, step 4329/7134 completed (loss: 0.18136422336101532, acc: 0.9590643048286438)
[2025-02-13 19:54:15,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:16,072][root][INFO] - Training Epoch: 1/2, step 4330/7134 completed (loss: 0.12554670870304108, acc: 0.9639639854431152)
[2025-02-13 19:54:16,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:16,469][root][INFO] - Training Epoch: 1/2, step 4331/7134 completed (loss: 0.1341872215270996, acc: 0.9629629850387573)
[2025-02-13 19:54:16,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:16,846][root][INFO] - Training Epoch: 1/2, step 4332/7134 completed (loss: 0.22505708038806915, acc: 0.9365079402923584)
[2025-02-13 19:54:16,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:17,256][root][INFO] - Training Epoch: 1/2, step 4333/7134 completed (loss: 0.101314976811409, acc: 0.981249988079071)
[2025-02-13 19:54:17,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:17,646][root][INFO] - Training Epoch: 1/2, step 4334/7134 completed (loss: 0.1932714283466339, acc: 0.9631901979446411)
[2025-02-13 19:54:17,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:18,033][root][INFO] - Training Epoch: 1/2, step 4335/7134 completed (loss: 0.16013038158416748, acc: 0.9834710955619812)
[2025-02-13 19:54:18,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:18,441][root][INFO] - Training Epoch: 1/2, step 4336/7134 completed (loss: 0.054433587938547134, acc: 0.9784172773361206)
[2025-02-13 19:54:18,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:18,832][root][INFO] - Training Epoch: 1/2, step 4337/7134 completed (loss: 0.06073096767067909, acc: 0.9860140085220337)
[2025-02-13 19:54:18,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:19,219][root][INFO] - Training Epoch: 1/2, step 4338/7134 completed (loss: 0.09393902122974396, acc: 0.9818181991577148)
[2025-02-13 19:54:19,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:19,595][root][INFO] - Training Epoch: 1/2, step 4339/7134 completed (loss: 0.19174297153949738, acc: 0.9588235020637512)
[2025-02-13 19:54:19,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:19,999][root][INFO] - Training Epoch: 1/2, step 4340/7134 completed (loss: 0.16749320924282074, acc: 0.951724112033844)
[2025-02-13 19:54:20,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:20,375][root][INFO] - Training Epoch: 1/2, step 4341/7134 completed (loss: 0.11398109048604965, acc: 0.9727891087532043)
[2025-02-13 19:54:20,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:20,756][root][INFO] - Training Epoch: 1/2, step 4342/7134 completed (loss: 0.24401342868804932, acc: 0.9271523356437683)
[2025-02-13 19:54:20,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:21,121][root][INFO] - Training Epoch: 1/2, step 4343/7134 completed (loss: 0.25889456272125244, acc: 0.9481481313705444)
[2025-02-13 19:54:21,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:21,504][root][INFO] - Training Epoch: 1/2, step 4344/7134 completed (loss: 0.186895489692688, acc: 0.95652174949646)
[2025-02-13 19:54:21,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:21,912][root][INFO] - Training Epoch: 1/2, step 4345/7134 completed (loss: 0.10740570724010468, acc: 0.9819819927215576)
[2025-02-13 19:54:22,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:22,266][root][INFO] - Training Epoch: 1/2, step 4346/7134 completed (loss: 0.1469082236289978, acc: 0.9729729890823364)
[2025-02-13 19:54:22,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:22,626][root][INFO] - Training Epoch: 1/2, step 4347/7134 completed (loss: 0.250995397567749, acc: 0.9349112510681152)
[2025-02-13 19:54:22,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:22,993][root][INFO] - Training Epoch: 1/2, step 4348/7134 completed (loss: 0.14771194756031036, acc: 0.9591836929321289)
[2025-02-13 19:54:23,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:23,395][root][INFO] - Training Epoch: 1/2, step 4349/7134 completed (loss: 0.18988825380802155, acc: 0.9583333134651184)
[2025-02-13 19:54:23,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:23,755][root][INFO] - Training Epoch: 1/2, step 4350/7134 completed (loss: 0.14476922154426575, acc: 0.9550561904907227)
[2025-02-13 19:54:23,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:24,122][root][INFO] - Training Epoch: 1/2, step 4351/7134 completed (loss: 0.15865719318389893, acc: 0.9551281929016113)
[2025-02-13 19:54:24,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:24,494][root][INFO] - Training Epoch: 1/2, step 4352/7134 completed (loss: 0.2822190821170807, acc: 0.9316770434379578)
[2025-02-13 19:54:24,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:24,865][root][INFO] - Training Epoch: 1/2, step 4353/7134 completed (loss: 0.17209531366825104, acc: 0.9341317415237427)
[2025-02-13 19:54:25,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:25,256][root][INFO] - Training Epoch: 1/2, step 4354/7134 completed (loss: 0.1619088053703308, acc: 0.9693251252174377)
[2025-02-13 19:54:25,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:25,612][root][INFO] - Training Epoch: 1/2, step 4355/7134 completed (loss: 0.34146347641944885, acc: 0.9254658222198486)
[2025-02-13 19:54:25,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:26,060][root][INFO] - Training Epoch: 1/2, step 4356/7134 completed (loss: 0.12049727141857147, acc: 0.9825581312179565)
[2025-02-13 19:54:26,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:26,466][root][INFO] - Training Epoch: 1/2, step 4357/7134 completed (loss: 0.3095416724681854, acc: 0.9328858852386475)
[2025-02-13 19:54:26,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:26,874][root][INFO] - Training Epoch: 1/2, step 4358/7134 completed (loss: 0.26338568329811096, acc: 0.9268292784690857)
[2025-02-13 19:54:27,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:27,247][root][INFO] - Training Epoch: 1/2, step 4359/7134 completed (loss: 0.1343073695898056, acc: 0.976190447807312)
[2025-02-13 19:54:27,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:27,603][root][INFO] - Training Epoch: 1/2, step 4360/7134 completed (loss: 0.10092483460903168, acc: 0.9745222926139832)
[2025-02-13 19:54:27,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:28,009][root][INFO] - Training Epoch: 1/2, step 4361/7134 completed (loss: 0.15219537913799286, acc: 0.9602649211883545)
[2025-02-13 19:54:28,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:28,377][root][INFO] - Training Epoch: 1/2, step 4362/7134 completed (loss: 0.2326628416776657, acc: 0.9402173757553101)
[2025-02-13 19:54:28,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:28,750][root][INFO] - Training Epoch: 1/2, step 4363/7134 completed (loss: 0.15807947516441345, acc: 0.9696969985961914)
[2025-02-13 19:54:28,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:29,114][root][INFO] - Training Epoch: 1/2, step 4364/7134 completed (loss: 0.1513814628124237, acc: 0.9534883499145508)
[2025-02-13 19:54:29,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:29,489][root][INFO] - Training Epoch: 1/2, step 4365/7134 completed (loss: 0.11296310275793076, acc: 0.971222996711731)
[2025-02-13 19:54:29,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:29,857][root][INFO] - Training Epoch: 1/2, step 4366/7134 completed (loss: 0.21152032911777496, acc: 0.9476743936538696)
[2025-02-13 19:54:29,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:30,215][root][INFO] - Training Epoch: 1/2, step 4367/7134 completed (loss: 0.11266360431909561, acc: 0.9798657894134521)
[2025-02-13 19:54:30,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:30,592][root][INFO] - Training Epoch: 1/2, step 4368/7134 completed (loss: 0.09154344350099564, acc: 0.9940119981765747)
[2025-02-13 19:54:30,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:30,959][root][INFO] - Training Epoch: 1/2, step 4369/7134 completed (loss: 0.11326373368501663, acc: 0.9756097793579102)
[2025-02-13 19:54:31,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:31,333][root][INFO] - Training Epoch: 1/2, step 4370/7134 completed (loss: 0.2457578331232071, acc: 0.9709302186965942)
[2025-02-13 19:54:31,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:31,704][root][INFO] - Training Epoch: 1/2, step 4371/7134 completed (loss: 0.12265966832637787, acc: 0.9817073345184326)
[2025-02-13 19:54:31,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:32,074][root][INFO] - Training Epoch: 1/2, step 4372/7134 completed (loss: 0.08156605809926987, acc: 0.9754098653793335)
[2025-02-13 19:54:32,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:32,433][root][INFO] - Training Epoch: 1/2, step 4373/7134 completed (loss: 0.15586189925670624, acc: 0.9668508172035217)
[2025-02-13 19:54:32,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:32,831][root][INFO] - Training Epoch: 1/2, step 4374/7134 completed (loss: 0.2127869576215744, acc: 0.9512194991111755)
[2025-02-13 19:54:32,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:33,199][root][INFO] - Training Epoch: 1/2, step 4375/7134 completed (loss: 0.22547315061092377, acc: 0.9404761791229248)
[2025-02-13 19:54:33,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:33,534][root][INFO] - Training Epoch: 1/2, step 4376/7134 completed (loss: 0.17439162731170654, acc: 0.9375)
[2025-02-13 19:54:33,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:33,880][root][INFO] - Training Epoch: 1/2, step 4377/7134 completed (loss: 0.2626081705093384, acc: 0.9647058844566345)
[2025-02-13 19:54:34,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:34,237][root][INFO] - Training Epoch: 1/2, step 4378/7134 completed (loss: 0.3148120939731598, acc: 0.9473684430122375)
[2025-02-13 19:54:34,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:34,625][root][INFO] - Training Epoch: 1/2, step 4379/7134 completed (loss: 0.3388163149356842, acc: 0.9117646813392639)
[2025-02-13 19:54:34,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:35,021][root][INFO] - Training Epoch: 1/2, step 4380/7134 completed (loss: 0.2491065412759781, acc: 0.9300699234008789)
[2025-02-13 19:54:35,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:35,445][root][INFO] - Training Epoch: 1/2, step 4381/7134 completed (loss: 0.11467590928077698, acc: 0.9820359349250793)
[2025-02-13 19:54:35,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:35,831][root][INFO] - Training Epoch: 1/2, step 4382/7134 completed (loss: 0.3364894688129425, acc: 0.9161290526390076)
[2025-02-13 19:54:35,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:36,240][root][INFO] - Training Epoch: 1/2, step 4383/7134 completed (loss: 0.45787736773490906, acc: 0.893401026725769)
[2025-02-13 19:54:36,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:36,642][root][INFO] - Training Epoch: 1/2, step 4384/7134 completed (loss: 0.10801887512207031, acc: 0.9829545617103577)
[2025-02-13 19:54:36,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:37,001][root][INFO] - Training Epoch: 1/2, step 4385/7134 completed (loss: 0.3294515907764435, acc: 0.9119170904159546)
[2025-02-13 19:54:37,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:37,375][root][INFO] - Training Epoch: 1/2, step 4386/7134 completed (loss: 0.19875991344451904, acc: 0.9514563083648682)
[2025-02-13 19:54:37,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:37,754][root][INFO] - Training Epoch: 1/2, step 4387/7134 completed (loss: 0.45310673117637634, acc: 0.9058823585510254)
[2025-02-13 19:54:37,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:38,150][root][INFO] - Training Epoch: 1/2, step 4388/7134 completed (loss: 0.3897199034690857, acc: 0.9090909361839294)
[2025-02-13 19:54:38,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:38,556][root][INFO] - Training Epoch: 1/2, step 4389/7134 completed (loss: 0.3953455090522766, acc: 0.8768116235733032)
[2025-02-13 19:54:38,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:38,993][root][INFO] - Training Epoch: 1/2, step 4390/7134 completed (loss: 0.3885650634765625, acc: 0.9018405079841614)
[2025-02-13 19:54:39,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:39,396][root][INFO] - Training Epoch: 1/2, step 4391/7134 completed (loss: 0.47626930475234985, acc: 0.9178082346916199)
[2025-02-13 19:54:39,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:39,790][root][INFO] - Training Epoch: 1/2, step 4392/7134 completed (loss: 0.16329793632030487, acc: 0.9629629850387573)
[2025-02-13 19:54:39,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:40,150][root][INFO] - Training Epoch: 1/2, step 4393/7134 completed (loss: 0.1618037223815918, acc: 0.9599999785423279)
[2025-02-13 19:54:40,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:40,571][root][INFO] - Training Epoch: 1/2, step 4394/7134 completed (loss: 0.16084717214107513, acc: 0.9597315192222595)
[2025-02-13 19:54:40,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:40,925][root][INFO] - Training Epoch: 1/2, step 4395/7134 completed (loss: 0.308736115694046, acc: 0.9378882050514221)
[2025-02-13 19:54:41,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:41,294][root][INFO] - Training Epoch: 1/2, step 4396/7134 completed (loss: 0.5288761258125305, acc: 0.8994709253311157)
[2025-02-13 19:54:41,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:41,653][root][INFO] - Training Epoch: 1/2, step 4397/7134 completed (loss: 0.9592301845550537, acc: 0.8198198080062866)
[2025-02-13 19:54:41,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:42,043][root][INFO] - Training Epoch: 1/2, step 4398/7134 completed (loss: 0.30923277139663696, acc: 0.9555555582046509)
[2025-02-13 19:54:42,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:42,404][root][INFO] - Training Epoch: 1/2, step 4399/7134 completed (loss: 0.1922483742237091, acc: 0.9652777910232544)
[2025-02-13 19:54:42,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:42,850][root][INFO] - Training Epoch: 1/2, step 4400/7134 completed (loss: 0.1840127408504486, acc: 0.9631578922271729)
[2025-02-13 19:54:42,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:43,219][root][INFO] - Training Epoch: 1/2, step 4401/7134 completed (loss: 0.2647796869277954, acc: 0.9419354796409607)
[2025-02-13 19:54:43,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:43,586][root][INFO] - Training Epoch: 1/2, step 4402/7134 completed (loss: 0.5465080142021179, acc: 0.8765432238578796)
[2025-02-13 19:54:43,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:43,969][root][INFO] - Training Epoch: 1/2, step 4403/7134 completed (loss: 0.6534736156463623, acc: 0.8545454740524292)
[2025-02-13 19:54:44,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:44,334][root][INFO] - Training Epoch: 1/2, step 4404/7134 completed (loss: 0.5031823515892029, acc: 0.8608247637748718)
[2025-02-13 19:54:44,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:44,717][root][INFO] - Training Epoch: 1/2, step 4405/7134 completed (loss: 0.2625603973865509, acc: 0.9356435537338257)
[2025-02-13 19:54:44,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:45,100][root][INFO] - Training Epoch: 1/2, step 4406/7134 completed (loss: 0.15175791084766388, acc: 0.9658536314964294)
[2025-02-13 19:54:45,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:45,475][root][INFO] - Training Epoch: 1/2, step 4407/7134 completed (loss: 0.23954372107982635, acc: 0.9489796161651611)
[2025-02-13 19:54:45,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:45,852][root][INFO] - Training Epoch: 1/2, step 4408/7134 completed (loss: 0.3638969659805298, acc: 0.8942307829856873)
[2025-02-13 19:54:45,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:46,307][root][INFO] - Training Epoch: 1/2, step 4409/7134 completed (loss: 0.21860812604427338, acc: 0.9583333134651184)
[2025-02-13 19:54:46,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:46,703][root][INFO] - Training Epoch: 1/2, step 4410/7134 completed (loss: 0.4474559426307678, acc: 0.8951048851013184)
[2025-02-13 19:54:46,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:47,102][root][INFO] - Training Epoch: 1/2, step 4411/7134 completed (loss: 0.4488648474216461, acc: 0.897849440574646)
[2025-02-13 19:54:47,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:47,470][root][INFO] - Training Epoch: 1/2, step 4412/7134 completed (loss: 0.7642192840576172, acc: 0.839195966720581)
[2025-02-13 19:54:47,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:47,841][root][INFO] - Training Epoch: 1/2, step 4413/7134 completed (loss: 0.49215471744537354, acc: 0.9119170904159546)
[2025-02-13 19:54:47,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:48,203][root][INFO] - Training Epoch: 1/2, step 4414/7134 completed (loss: 0.2399045079946518, acc: 0.9408602118492126)
[2025-02-13 19:54:48,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:48,573][root][INFO] - Training Epoch: 1/2, step 4415/7134 completed (loss: 0.2435649037361145, acc: 0.932584285736084)
[2025-02-13 19:54:48,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:48,934][root][INFO] - Training Epoch: 1/2, step 4416/7134 completed (loss: 0.2123723179101944, acc: 0.9430052042007446)
[2025-02-13 19:54:49,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:49,332][root][INFO] - Training Epoch: 1/2, step 4417/7134 completed (loss: 0.41479548811912537, acc: 0.9141414165496826)
[2025-02-13 19:54:49,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:49,737][root][INFO] - Training Epoch: 1/2, step 4418/7134 completed (loss: 0.30833354592323303, acc: 0.9021739363670349)
[2025-02-13 19:54:49,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:50,114][root][INFO] - Training Epoch: 1/2, step 4419/7134 completed (loss: 0.3001425266265869, acc: 0.9414634108543396)
[2025-02-13 19:54:50,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:50,502][root][INFO] - Training Epoch: 1/2, step 4420/7134 completed (loss: 0.337413489818573, acc: 0.9170984625816345)
[2025-02-13 19:54:50,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:50,905][root][INFO] - Training Epoch: 1/2, step 4421/7134 completed (loss: 0.24957884848117828, acc: 0.9449541568756104)
[2025-02-13 19:54:51,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:51,337][root][INFO] - Training Epoch: 1/2, step 4422/7134 completed (loss: 0.35348600149154663, acc: 0.910179615020752)
[2025-02-13 19:54:51,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:51,719][root][INFO] - Training Epoch: 1/2, step 4423/7134 completed (loss: 0.2169402837753296, acc: 0.9419642686843872)
[2025-02-13 19:54:51,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:52,133][root][INFO] - Training Epoch: 1/2, step 4424/7134 completed (loss: 0.18417511880397797, acc: 0.9545454382896423)
[2025-02-13 19:54:52,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:52,531][root][INFO] - Training Epoch: 1/2, step 4425/7134 completed (loss: 0.1635398417711258, acc: 0.9603960514068604)
[2025-02-13 19:54:52,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:52,941][root][INFO] - Training Epoch: 1/2, step 4426/7134 completed (loss: 0.29379045963287354, acc: 0.9279999732971191)
[2025-02-13 19:54:53,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:53,286][root][INFO] - Training Epoch: 1/2, step 4427/7134 completed (loss: 0.6759971976280212, acc: 0.8362069129943848)
[2025-02-13 19:54:53,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:53,660][root][INFO] - Training Epoch: 1/2, step 4428/7134 completed (loss: 0.16200096905231476, acc: 0.9537572264671326)
[2025-02-13 19:54:53,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:54,029][root][INFO] - Training Epoch: 1/2, step 4429/7134 completed (loss: 0.27459752559661865, acc: 0.9142857193946838)
[2025-02-13 19:54:54,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:54,439][root][INFO] - Training Epoch: 1/2, step 4430/7134 completed (loss: 0.2953316867351532, acc: 0.9269663095474243)
[2025-02-13 19:54:54,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:54,810][root][INFO] - Training Epoch: 1/2, step 4431/7134 completed (loss: 0.29363417625427246, acc: 0.9244186282157898)
[2025-02-13 19:54:54,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:55,163][root][INFO] - Training Epoch: 1/2, step 4432/7134 completed (loss: 0.1967749446630478, acc: 0.9617834687232971)
[2025-02-13 19:54:55,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:55,559][root][INFO] - Training Epoch: 1/2, step 4433/7134 completed (loss: 0.187594473361969, acc: 0.961904764175415)
[2025-02-13 19:54:55,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:56,036][root][INFO] - Training Epoch: 1/2, step 4434/7134 completed (loss: 0.33875903487205505, acc: 0.9257143139839172)
[2025-02-13 19:54:56,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:56,472][root][INFO] - Training Epoch: 1/2, step 4435/7134 completed (loss: 0.2697325646877289, acc: 0.9236111044883728)
[2025-02-13 19:54:56,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:56,877][root][INFO] - Training Epoch: 1/2, step 4436/7134 completed (loss: 0.310996949672699, acc: 0.9230769276618958)
[2025-02-13 19:54:57,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:57,271][root][INFO] - Training Epoch: 1/2, step 4437/7134 completed (loss: 0.06340239942073822, acc: 0.9781420826911926)
[2025-02-13 19:54:57,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:57,669][root][INFO] - Training Epoch: 1/2, step 4438/7134 completed (loss: 0.2956882119178772, acc: 0.9402984976768494)
[2025-02-13 19:54:57,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:58,036][root][INFO] - Training Epoch: 1/2, step 4439/7134 completed (loss: 0.05145485699176788, acc: 0.9929078221321106)
[2025-02-13 19:54:58,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:58,398][root][INFO] - Training Epoch: 1/2, step 4440/7134 completed (loss: 0.17228347063064575, acc: 0.9578313231468201)
[2025-02-13 19:54:58,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:58,826][root][INFO] - Training Epoch: 1/2, step 4441/7134 completed (loss: 0.1323452740907669, acc: 0.9748427867889404)
[2025-02-13 19:54:58,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:59,220][root][INFO] - Training Epoch: 1/2, step 4442/7134 completed (loss: 0.3684327304363251, acc: 0.9382022619247437)
[2025-02-13 19:54:59,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:59,612][root][INFO] - Training Epoch: 1/2, step 4443/7134 completed (loss: 0.25048670172691345, acc: 0.9352940917015076)
[2025-02-13 19:54:59,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:59,978][root][INFO] - Training Epoch: 1/2, step 4444/7134 completed (loss: 0.18980206549167633, acc: 0.9489051103591919)
[2025-02-13 19:55:00,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:00,363][root][INFO] - Training Epoch: 1/2, step 4445/7134 completed (loss: 0.35000720620155334, acc: 0.9112426042556763)
[2025-02-13 19:55:00,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:00,741][root][INFO] - Training Epoch: 1/2, step 4446/7134 completed (loss: 0.14204710721969604, acc: 0.9534883499145508)
[2025-02-13 19:55:00,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:01,121][root][INFO] - Training Epoch: 1/2, step 4447/7134 completed (loss: 0.09765208512544632, acc: 0.982758641242981)
[2025-02-13 19:55:01,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:01,467][root][INFO] - Training Epoch: 1/2, step 4448/7134 completed (loss: 0.32524681091308594, acc: 0.9147287011146545)
[2025-02-13 19:55:01,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:01,861][root][INFO] - Training Epoch: 1/2, step 4449/7134 completed (loss: 0.31581056118011475, acc: 0.9576271176338196)
[2025-02-13 19:55:02,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:02,249][root][INFO] - Training Epoch: 1/2, step 4450/7134 completed (loss: 0.2487291842699051, acc: 0.9255319237709045)
[2025-02-13 19:55:02,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:02,656][root][INFO] - Training Epoch: 1/2, step 4451/7134 completed (loss: 0.4578414261341095, acc: 0.9108280539512634)
[2025-02-13 19:55:02,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:03,052][root][INFO] - Training Epoch: 1/2, step 4452/7134 completed (loss: 0.21801413595676422, acc: 0.9407407641410828)
[2025-02-13 19:55:03,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:03,446][root][INFO] - Training Epoch: 1/2, step 4453/7134 completed (loss: 0.18746758997440338, acc: 0.9542483687400818)
[2025-02-13 19:55:03,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:03,851][root][INFO] - Training Epoch: 1/2, step 4454/7134 completed (loss: 0.2551147937774658, acc: 0.9503105878829956)
[2025-02-13 19:55:03,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:04,194][root][INFO] - Training Epoch: 1/2, step 4455/7134 completed (loss: 0.3705854117870331, acc: 0.9444444179534912)
[2025-02-13 19:55:04,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:04,539][root][INFO] - Training Epoch: 1/2, step 4456/7134 completed (loss: 0.07755453884601593, acc: 0.9878787994384766)
[2025-02-13 19:55:04,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:04,895][root][INFO] - Training Epoch: 1/2, step 4457/7134 completed (loss: 0.07608065754175186, acc: 0.9870129823684692)
[2025-02-13 19:55:05,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:05,274][root][INFO] - Training Epoch: 1/2, step 4458/7134 completed (loss: 0.06135889142751694, acc: 0.9860140085220337)
[2025-02-13 19:55:05,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:05,675][root][INFO] - Training Epoch: 1/2, step 4459/7134 completed (loss: 0.08292508870363235, acc: 0.9883720874786377)
[2025-02-13 19:55:05,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:06,072][root][INFO] - Training Epoch: 1/2, step 4460/7134 completed (loss: 0.11340208351612091, acc: 0.9802955389022827)
[2025-02-13 19:55:06,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:06,486][root][INFO] - Training Epoch: 1/2, step 4461/7134 completed (loss: 0.10393927246332169, acc: 0.9739583134651184)
[2025-02-13 19:55:06,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:06,887][root][INFO] - Training Epoch: 1/2, step 4462/7134 completed (loss: 0.12522628903388977, acc: 0.9657142758369446)
[2025-02-13 19:55:07,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:07,296][root][INFO] - Training Epoch: 1/2, step 4463/7134 completed (loss: 0.06422775238752365, acc: 0.9841269850730896)
[2025-02-13 19:55:07,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:07,707][root][INFO] - Training Epoch: 1/2, step 4464/7134 completed (loss: 0.1414695680141449, acc: 0.9589040875434875)
[2025-02-13 19:55:07,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:08,107][root][INFO] - Training Epoch: 1/2, step 4465/7134 completed (loss: 0.10498451441526413, acc: 0.9898989796638489)
[2025-02-13 19:55:08,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:08,485][root][INFO] - Training Epoch: 1/2, step 4466/7134 completed (loss: 0.09740359336137772, acc: 0.9696969985961914)
[2025-02-13 19:55:08,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:08,861][root][INFO] - Training Epoch: 1/2, step 4467/7134 completed (loss: 0.07930923998355865, acc: 0.9679144620895386)
[2025-02-13 19:55:09,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:09,225][root][INFO] - Training Epoch: 1/2, step 4468/7134 completed (loss: 0.03854325786232948, acc: 0.9893048405647278)
[2025-02-13 19:55:09,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:09,613][root][INFO] - Training Epoch: 1/2, step 4469/7134 completed (loss: 0.06890588998794556, acc: 0.9653465151786804)
[2025-02-13 19:55:09,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:10,013][root][INFO] - Training Epoch: 1/2, step 4470/7134 completed (loss: 0.10093583166599274, acc: 0.9711538553237915)
[2025-02-13 19:55:10,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:10,429][root][INFO] - Training Epoch: 1/2, step 4471/7134 completed (loss: 0.05916571989655495, acc: 0.9848484992980957)
[2025-02-13 19:55:10,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:10,792][root][INFO] - Training Epoch: 1/2, step 4472/7134 completed (loss: 0.07563701272010803, acc: 0.9722222089767456)
[2025-02-13 19:55:10,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:11,216][root][INFO] - Training Epoch: 1/2, step 4473/7134 completed (loss: 0.2305898517370224, acc: 0.9349112510681152)
[2025-02-13 19:55:11,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:11,617][root][INFO] - Training Epoch: 1/2, step 4474/7134 completed (loss: 0.21983273327350616, acc: 0.9689440727233887)
[2025-02-13 19:55:11,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:12,027][root][INFO] - Training Epoch: 1/2, step 4475/7134 completed (loss: 0.43395525217056274, acc: 0.8829787373542786)
[2025-02-13 19:55:12,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:12,414][root][INFO] - Training Epoch: 1/2, step 4476/7134 completed (loss: 0.3227284252643585, acc: 0.9175823926925659)
[2025-02-13 19:55:12,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:12,806][root][INFO] - Training Epoch: 1/2, step 4477/7134 completed (loss: 0.07921306788921356, acc: 0.9795918464660645)
[2025-02-13 19:55:12,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:13,200][root][INFO] - Training Epoch: 1/2, step 4478/7134 completed (loss: 0.14348946511745453, acc: 0.9581151604652405)
[2025-02-13 19:55:13,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:13,607][root][INFO] - Training Epoch: 1/2, step 4479/7134 completed (loss: 0.11853718012571335, acc: 0.9611111283302307)
[2025-02-13 19:55:13,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:14,009][root][INFO] - Training Epoch: 1/2, step 4480/7134 completed (loss: 0.2653193771839142, acc: 0.9455445408821106)
[2025-02-13 19:55:14,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:14,381][root][INFO] - Training Epoch: 1/2, step 4481/7134 completed (loss: 0.14619606733322144, acc: 0.9700000286102295)
[2025-02-13 19:55:14,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:14,769][root][INFO] - Training Epoch: 1/2, step 4482/7134 completed (loss: 0.06663284450769424, acc: 0.9851484894752502)
[2025-02-13 19:55:14,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:15,165][root][INFO] - Training Epoch: 1/2, step 4483/7134 completed (loss: 0.21587276458740234, acc: 0.9503105878829956)
[2025-02-13 19:55:15,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:15,596][root][INFO] - Training Epoch: 1/2, step 4484/7134 completed (loss: 0.0966787114739418, acc: 0.9829545617103577)
[2025-02-13 19:55:15,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:15,970][root][INFO] - Training Epoch: 1/2, step 4485/7134 completed (loss: 0.06835466623306274, acc: 0.9869281053543091)
[2025-02-13 19:55:16,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:16,346][root][INFO] - Training Epoch: 1/2, step 4486/7134 completed (loss: 0.07099141925573349, acc: 0.9875776171684265)
[2025-02-13 19:55:16,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:16,730][root][INFO] - Training Epoch: 1/2, step 4487/7134 completed (loss: 0.16100159287452698, acc: 0.9631901979446411)
[2025-02-13 19:55:16,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:17,087][root][INFO] - Training Epoch: 1/2, step 4488/7134 completed (loss: 0.11929981410503387, acc: 0.9767441749572754)
[2025-02-13 19:55:17,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:17,485][root][INFO] - Training Epoch: 1/2, step 4489/7134 completed (loss: 0.08946084976196289, acc: 0.9724137783050537)
[2025-02-13 19:55:17,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:17,865][root][INFO] - Training Epoch: 1/2, step 4490/7134 completed (loss: 0.09467538446187973, acc: 0.9679999947547913)
[2025-02-13 19:55:18,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:18,235][root][INFO] - Training Epoch: 1/2, step 4491/7134 completed (loss: 0.1009574830532074, acc: 0.9770992398262024)
[2025-02-13 19:55:18,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:18,580][root][INFO] - Training Epoch: 1/2, step 4492/7134 completed (loss: 0.1289043426513672, acc: 0.9849624037742615)
[2025-02-13 19:55:18,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:18,955][root][INFO] - Training Epoch: 1/2, step 4493/7134 completed (loss: 0.19436658918857574, acc: 0.9360465407371521)
[2025-02-13 19:55:19,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:19,349][root][INFO] - Training Epoch: 1/2, step 4494/7134 completed (loss: 0.09253817051649094, acc: 0.9775280952453613)
[2025-02-13 19:55:19,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:19,716][root][INFO] - Training Epoch: 1/2, step 4495/7134 completed (loss: 0.0706680566072464, acc: 0.9742268323898315)
[2025-02-13 19:55:19,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:20,083][root][INFO] - Training Epoch: 1/2, step 4496/7134 completed (loss: 0.13503551483154297, acc: 0.9707602262496948)
[2025-02-13 19:55:20,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:20,442][root][INFO] - Training Epoch: 1/2, step 4497/7134 completed (loss: 0.044597283005714417, acc: 0.9894179701805115)
[2025-02-13 19:55:20,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:20,826][root][INFO] - Training Epoch: 1/2, step 4498/7134 completed (loss: 0.044850487262010574, acc: 0.9942196607589722)
[2025-02-13 19:55:20,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:21,194][root][INFO] - Training Epoch: 1/2, step 4499/7134 completed (loss: 0.09619899839162827, acc: 0.978723406791687)
[2025-02-13 19:55:21,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:21,592][root][INFO] - Training Epoch: 1/2, step 4500/7134 completed (loss: 0.08593399077653885, acc: 0.9893048405647278)
[2025-02-13 19:55:21,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:21,963][root][INFO] - Training Epoch: 1/2, step 4501/7134 completed (loss: 0.06074251979589462, acc: 0.9779005646705627)
[2025-02-13 19:55:22,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:22,334][root][INFO] - Training Epoch: 1/2, step 4502/7134 completed (loss: 0.04219554737210274, acc: 0.9941176176071167)
[2025-02-13 19:55:22,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:22,719][root][INFO] - Training Epoch: 1/2, step 4503/7134 completed (loss: 0.05712360516190529, acc: 0.9811320900917053)
[2025-02-13 19:55:22,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:23,117][root][INFO] - Training Epoch: 1/2, step 4504/7134 completed (loss: 0.2041805535554886, acc: 0.95652174949646)
[2025-02-13 19:55:23,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:23,504][root][INFO] - Training Epoch: 1/2, step 4505/7134 completed (loss: 0.19174715876579285, acc: 0.9351351261138916)
[2025-02-13 19:55:23,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:23,879][root][INFO] - Training Epoch: 1/2, step 4506/7134 completed (loss: 0.20588457584381104, acc: 0.9570552110671997)
[2025-02-13 19:55:24,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:24,252][root][INFO] - Training Epoch: 1/2, step 4507/7134 completed (loss: 0.0820327028632164, acc: 0.9740259647369385)
[2025-02-13 19:55:24,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:24,633][root][INFO] - Training Epoch: 1/2, step 4508/7134 completed (loss: 0.1779731661081314, acc: 0.9440993666648865)
[2025-02-13 19:55:24,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:25,023][root][INFO] - Training Epoch: 1/2, step 4509/7134 completed (loss: 0.11484938859939575, acc: 0.9575757384300232)
[2025-02-13 19:55:25,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:25,407][root][INFO] - Training Epoch: 1/2, step 4510/7134 completed (loss: 0.2147817611694336, acc: 0.9285714030265808)
[2025-02-13 19:55:25,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:25,812][root][INFO] - Training Epoch: 1/2, step 4511/7134 completed (loss: 0.21889986097812653, acc: 0.9261363744735718)
[2025-02-13 19:55:25,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:26,188][root][INFO] - Training Epoch: 1/2, step 4512/7134 completed (loss: 0.32252639532089233, acc: 0.9111111164093018)
[2025-02-13 19:55:26,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:26,575][root][INFO] - Training Epoch: 1/2, step 4513/7134 completed (loss: 0.17103956639766693, acc: 0.9649122953414917)
[2025-02-13 19:55:26,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:26,959][root][INFO] - Training Epoch: 1/2, step 4514/7134 completed (loss: 0.3628039062023163, acc: 0.9364162087440491)
[2025-02-13 19:55:27,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:27,329][root][INFO] - Training Epoch: 1/2, step 4515/7134 completed (loss: 0.2533666789531708, acc: 0.9473684430122375)
[2025-02-13 19:55:27,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:27,679][root][INFO] - Training Epoch: 1/2, step 4516/7134 completed (loss: 0.16208136081695557, acc: 0.9411764740943909)
[2025-02-13 19:55:27,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:28,082][root][INFO] - Training Epoch: 1/2, step 4517/7134 completed (loss: 0.15342706441879272, acc: 0.9649999737739563)
[2025-02-13 19:55:28,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:28,458][root][INFO] - Training Epoch: 1/2, step 4518/7134 completed (loss: 0.17848992347717285, acc: 0.9484536051750183)
[2025-02-13 19:55:28,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:28,887][root][INFO] - Training Epoch: 1/2, step 4519/7134 completed (loss: 0.10577962547540665, acc: 0.9701492786407471)
[2025-02-13 19:55:29,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:29,288][root][INFO] - Training Epoch: 1/2, step 4520/7134 completed (loss: 0.1395978480577469, acc: 0.9572192430496216)
[2025-02-13 19:55:29,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:29,679][root][INFO] - Training Epoch: 1/2, step 4521/7134 completed (loss: 0.13933967053890228, acc: 0.9750000238418579)
[2025-02-13 19:55:29,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:30,045][root][INFO] - Training Epoch: 1/2, step 4522/7134 completed (loss: 0.19026802480220795, acc: 0.9573459625244141)
[2025-02-13 19:55:30,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:30,435][root][INFO] - Training Epoch: 1/2, step 4523/7134 completed (loss: 0.05772152170538902, acc: 0.98591548204422)
[2025-02-13 19:55:30,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:30,817][root][INFO] - Training Epoch: 1/2, step 4524/7134 completed (loss: 0.2051527351140976, acc: 0.9319371581077576)
[2025-02-13 19:55:30,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:31,168][root][INFO] - Training Epoch: 1/2, step 4525/7134 completed (loss: 0.19201534986495972, acc: 0.9605911374092102)
[2025-02-13 19:55:31,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:31,514][root][INFO] - Training Epoch: 1/2, step 4526/7134 completed (loss: 0.23975150287151337, acc: 0.9450549483299255)
[2025-02-13 19:55:31,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:31,884][root][INFO] - Training Epoch: 1/2, step 4527/7134 completed (loss: 0.2167738527059555, acc: 0.9372197389602661)
[2025-02-13 19:55:32,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:32,243][root][INFO] - Training Epoch: 1/2, step 4528/7134 completed (loss: 0.21197400987148285, acc: 0.931034505367279)
[2025-02-13 19:55:32,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:32,591][root][INFO] - Training Epoch: 1/2, step 4529/7134 completed (loss: 0.22168336808681488, acc: 0.9319371581077576)
[2025-02-13 19:55:32,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:32,991][root][INFO] - Training Epoch: 1/2, step 4530/7134 completed (loss: 0.062087077647447586, acc: 0.9817073345184326)
[2025-02-13 19:55:33,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:33,402][root][INFO] - Training Epoch: 1/2, step 4531/7134 completed (loss: 0.12569662928581238, acc: 0.953125)
[2025-02-13 19:55:33,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:33,790][root][INFO] - Training Epoch: 1/2, step 4532/7134 completed (loss: 0.18932455778121948, acc: 0.9535865187644958)
[2025-02-13 19:55:33,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:34,145][root][INFO] - Training Epoch: 1/2, step 4533/7134 completed (loss: 0.10948190093040466, acc: 0.9722222089767456)
[2025-02-13 19:55:34,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:34,541][root][INFO] - Training Epoch: 1/2, step 4534/7134 completed (loss: 0.11201860755681992, acc: 0.9746192693710327)
[2025-02-13 19:55:34,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:34,903][root][INFO] - Training Epoch: 1/2, step 4535/7134 completed (loss: 0.07411890476942062, acc: 0.9807692170143127)
[2025-02-13 19:55:35,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:35,282][root][INFO] - Training Epoch: 1/2, step 4536/7134 completed (loss: 0.07470925897359848, acc: 0.9879518151283264)
[2025-02-13 19:55:35,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:35,685][root][INFO] - Training Epoch: 1/2, step 4537/7134 completed (loss: 0.042953964322805405, acc: 0.9954751133918762)
[2025-02-13 19:55:35,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:36,078][root][INFO] - Training Epoch: 1/2, step 4538/7134 completed (loss: 0.10589751601219177, acc: 0.9710144996643066)
[2025-02-13 19:55:36,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:36,447][root][INFO] - Training Epoch: 1/2, step 4539/7134 completed (loss: 0.1223122775554657, acc: 0.9718309640884399)
[2025-02-13 19:55:36,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:36,807][root][INFO] - Training Epoch: 1/2, step 4540/7134 completed (loss: 0.226710706949234, acc: 0.9226190447807312)
[2025-02-13 19:55:36,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:37,228][root][INFO] - Training Epoch: 1/2, step 4541/7134 completed (loss: 0.2941126227378845, acc: 0.8934426307678223)
[2025-02-13 19:55:37,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:37,610][root][INFO] - Training Epoch: 1/2, step 4542/7134 completed (loss: 0.29955190420150757, acc: 0.9122806787490845)
[2025-02-13 19:55:37,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:37,984][root][INFO] - Training Epoch: 1/2, step 4543/7134 completed (loss: 0.3978256583213806, acc: 0.8896104097366333)
[2025-02-13 19:55:38,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:38,363][root][INFO] - Training Epoch: 1/2, step 4544/7134 completed (loss: 0.1472984254360199, acc: 0.9662162065505981)
[2025-02-13 19:55:38,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:38,738][root][INFO] - Training Epoch: 1/2, step 4545/7134 completed (loss: 0.14027853310108185, acc: 0.9629629850387573)
[2025-02-13 19:55:38,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:39,123][root][INFO] - Training Epoch: 1/2, step 4546/7134 completed (loss: 0.14413712918758392, acc: 0.9685039520263672)
[2025-02-13 19:55:39,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:39,520][root][INFO] - Training Epoch: 1/2, step 4547/7134 completed (loss: 0.11976375430822372, acc: 0.9661017060279846)
[2025-02-13 19:55:39,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:39,894][root][INFO] - Training Epoch: 1/2, step 4548/7134 completed (loss: 0.2589428722858429, acc: 0.9370078444480896)
[2025-02-13 19:55:40,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:40,240][root][INFO] - Training Epoch: 1/2, step 4549/7134 completed (loss: 0.24241787195205688, acc: 0.9210526347160339)
[2025-02-13 19:55:40,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:40,636][root][INFO] - Training Epoch: 1/2, step 4550/7134 completed (loss: 0.19565266370773315, acc: 0.9548872113227844)
[2025-02-13 19:55:40,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:41,015][root][INFO] - Training Epoch: 1/2, step 4551/7134 completed (loss: 0.30273643136024475, acc: 0.9391891956329346)
[2025-02-13 19:55:41,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:41,403][root][INFO] - Training Epoch: 1/2, step 4552/7134 completed (loss: 0.21258427202701569, acc: 0.9561403393745422)
[2025-02-13 19:55:41,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:41,753][root][INFO] - Training Epoch: 1/2, step 4553/7134 completed (loss: 0.13786420226097107, acc: 0.9696969985961914)
[2025-02-13 19:55:41,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:42,100][root][INFO] - Training Epoch: 1/2, step 4554/7134 completed (loss: 0.1917608380317688, acc: 0.970802903175354)
[2025-02-13 19:55:42,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:42,461][root][INFO] - Training Epoch: 1/2, step 4555/7134 completed (loss: 0.31394970417022705, acc: 0.9270833134651184)
[2025-02-13 19:55:42,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:42,833][root][INFO] - Training Epoch: 1/2, step 4556/7134 completed (loss: 0.35922786593437195, acc: 0.8838709592819214)
[2025-02-13 19:55:42,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:43,135][root][INFO] - Training Epoch: 1/2, step 4557/7134 completed (loss: 0.09635046869516373, acc: 0.9701492786407471)
[2025-02-13 19:55:43,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:43,479][root][INFO] - Training Epoch: 1/2, step 4558/7134 completed (loss: 0.22353176772594452, acc: 0.9379844665527344)
[2025-02-13 19:55:43,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:43,833][root][INFO] - Training Epoch: 1/2, step 4559/7134 completed (loss: 0.3259549140930176, acc: 0.9090909361839294)
[2025-02-13 19:55:43,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:44,213][root][INFO] - Training Epoch: 1/2, step 4560/7134 completed (loss: 0.14934521913528442, acc: 0.9677419066429138)
[2025-02-13 19:55:44,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:44,618][root][INFO] - Training Epoch: 1/2, step 4561/7134 completed (loss: 0.20604407787322998, acc: 0.9508196711540222)
[2025-02-13 19:55:44,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:44,991][root][INFO] - Training Epoch: 1/2, step 4562/7134 completed (loss: 0.23288452625274658, acc: 0.9512194991111755)
[2025-02-13 19:55:45,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:45,371][root][INFO] - Training Epoch: 1/2, step 4563/7134 completed (loss: 0.12762808799743652, acc: 0.9645389914512634)
[2025-02-13 19:55:45,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:45,770][root][INFO] - Training Epoch: 1/2, step 4564/7134 completed (loss: 0.322329044342041, acc: 0.9285714030265808)
[2025-02-13 19:55:45,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:46,135][root][INFO] - Training Epoch: 1/2, step 4565/7134 completed (loss: 0.24104304611682892, acc: 0.9380530714988708)
[2025-02-13 19:55:46,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:46,507][root][INFO] - Training Epoch: 1/2, step 4566/7134 completed (loss: 0.21827641129493713, acc: 0.932330846786499)
[2025-02-13 19:55:46,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:46,857][root][INFO] - Training Epoch: 1/2, step 4567/7134 completed (loss: 0.05816720053553581, acc: 0.9909909963607788)
[2025-02-13 19:55:46,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:47,227][root][INFO] - Training Epoch: 1/2, step 4568/7134 completed (loss: 0.3541954755783081, acc: 0.9684210419654846)
[2025-02-13 19:55:47,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:47,602][root][INFO] - Training Epoch: 1/2, step 4569/7134 completed (loss: 0.1260686069726944, acc: 0.9646017551422119)
[2025-02-13 19:55:47,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:47,988][root][INFO] - Training Epoch: 1/2, step 4570/7134 completed (loss: 0.1356026828289032, acc: 0.9610389471054077)
[2025-02-13 19:55:48,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:48,381][root][INFO] - Training Epoch: 1/2, step 4571/7134 completed (loss: 0.190032497048378, acc: 0.9482758641242981)
[2025-02-13 19:55:48,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:48,776][root][INFO] - Training Epoch: 1/2, step 4572/7134 completed (loss: 0.10552670061588287, acc: 0.9802631735801697)
[2025-02-13 19:55:48,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:49,160][root][INFO] - Training Epoch: 1/2, step 4573/7134 completed (loss: 0.14104042947292328, acc: 0.9615384340286255)
[2025-02-13 19:55:49,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:49,555][root][INFO] - Training Epoch: 1/2, step 4574/7134 completed (loss: 0.08543241024017334, acc: 0.9848484992980957)
[2025-02-13 19:55:49,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:49,941][root][INFO] - Training Epoch: 1/2, step 4575/7134 completed (loss: 0.1192697063088417, acc: 0.9779005646705627)
[2025-02-13 19:55:50,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:50,311][root][INFO] - Training Epoch: 1/2, step 4576/7134 completed (loss: 0.07674626260995865, acc: 0.9885714054107666)
[2025-02-13 19:55:50,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:50,702][root][INFO] - Training Epoch: 1/2, step 4577/7134 completed (loss: 0.22287794947624207, acc: 0.9360465407371521)
[2025-02-13 19:55:50,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:51,081][root][INFO] - Training Epoch: 1/2, step 4578/7134 completed (loss: 0.07347950339317322, acc: 0.9818181991577148)
[2025-02-13 19:55:51,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:51,484][root][INFO] - Training Epoch: 1/2, step 4579/7134 completed (loss: 0.19386060535907745, acc: 0.9683544039726257)
[2025-02-13 19:55:51,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:51,887][root][INFO] - Training Epoch: 1/2, step 4580/7134 completed (loss: 0.12775373458862305, acc: 0.9620253443717957)
[2025-02-13 19:55:52,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:52,248][root][INFO] - Training Epoch: 1/2, step 4581/7134 completed (loss: 0.03857055678963661, acc: 0.9860140085220337)
[2025-02-13 19:55:52,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:52,632][root][INFO] - Training Epoch: 1/2, step 4582/7134 completed (loss: 0.12930448353290558, acc: 0.9583333134651184)
[2025-02-13 19:55:52,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:52,949][root][INFO] - Training Epoch: 1/2, step 4583/7134 completed (loss: 0.21788115799427032, acc: 0.9599999785423279)
[2025-02-13 19:55:53,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:53,342][root][INFO] - Training Epoch: 1/2, step 4584/7134 completed (loss: 0.1328166276216507, acc: 0.9655172228813171)
[2025-02-13 19:55:53,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:53,704][root][INFO] - Training Epoch: 1/2, step 4585/7134 completed (loss: 0.037566330283880234, acc: 0.9935897588729858)
[2025-02-13 19:55:53,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:54,097][root][INFO] - Training Epoch: 1/2, step 4586/7134 completed (loss: 0.11509159207344055, acc: 0.976331353187561)
[2025-02-13 19:55:54,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:54,510][root][INFO] - Training Epoch: 1/2, step 4587/7134 completed (loss: 0.29891693592071533, acc: 0.9418604373931885)
[2025-02-13 19:55:54,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:54,944][root][INFO] - Training Epoch: 1/2, step 4588/7134 completed (loss: 0.16357925534248352, acc: 0.949999988079071)
[2025-02-13 19:55:55,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:55,334][root][INFO] - Training Epoch: 1/2, step 4589/7134 completed (loss: 0.15100733935832977, acc: 0.9712643623352051)
[2025-02-13 19:55:55,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:55,725][root][INFO] - Training Epoch: 1/2, step 4590/7134 completed (loss: 0.2354062795639038, acc: 0.9285714030265808)
[2025-02-13 19:55:55,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:56,105][root][INFO] - Training Epoch: 1/2, step 4591/7134 completed (loss: 0.21714839339256287, acc: 0.9602272510528564)
[2025-02-13 19:55:56,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:56,490][root][INFO] - Training Epoch: 1/2, step 4592/7134 completed (loss: 0.1845896989107132, acc: 0.9634146094322205)
[2025-02-13 19:55:56,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:56,869][root][INFO] - Training Epoch: 1/2, step 4593/7134 completed (loss: 0.23844817280769348, acc: 0.9210526347160339)
[2025-02-13 19:55:57,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:57,243][root][INFO] - Training Epoch: 1/2, step 4594/7134 completed (loss: 0.3065798580646515, acc: 0.8994709253311157)
[2025-02-13 19:55:57,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:57,623][root][INFO] - Training Epoch: 1/2, step 4595/7134 completed (loss: 0.17128033936023712, acc: 0.949367105960846)
[2025-02-13 19:55:57,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:57,996][root][INFO] - Training Epoch: 1/2, step 4596/7134 completed (loss: 0.1824178248643875, acc: 0.9375)
[2025-02-13 19:55:58,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:58,378][root][INFO] - Training Epoch: 1/2, step 4597/7134 completed (loss: 0.18341763317584991, acc: 0.9487179517745972)
[2025-02-13 19:55:58,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:58,794][root][INFO] - Training Epoch: 1/2, step 4598/7134 completed (loss: 0.27438443899154663, acc: 0.9351351261138916)
[2025-02-13 19:55:58,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:59,170][root][INFO] - Training Epoch: 1/2, step 4599/7134 completed (loss: 0.0507216677069664, acc: 0.9855072498321533)
[2025-02-13 19:55:59,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:59,529][root][INFO] - Training Epoch: 1/2, step 4600/7134 completed (loss: 0.13433438539505005, acc: 0.9685534834861755)
[2025-02-13 19:55:59,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:59,922][root][INFO] - Training Epoch: 1/2, step 4601/7134 completed (loss: 0.11112365871667862, acc: 0.9661017060279846)
[2025-02-13 19:56:00,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:00,316][root][INFO] - Training Epoch: 1/2, step 4602/7134 completed (loss: 0.16674554347991943, acc: 0.9555555582046509)
[2025-02-13 19:56:00,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:00,748][root][INFO] - Training Epoch: 1/2, step 4603/7134 completed (loss: 0.12514172494411469, acc: 0.9651162624359131)
[2025-02-13 19:56:00,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:01,146][root][INFO] - Training Epoch: 1/2, step 4604/7134 completed (loss: 0.128190815448761, acc: 0.9650349617004395)
[2025-02-13 19:56:01,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:01,493][root][INFO] - Training Epoch: 1/2, step 4605/7134 completed (loss: 0.18874536454677582, acc: 0.9363636374473572)
[2025-02-13 19:56:01,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:01,867][root][INFO] - Training Epoch: 1/2, step 4606/7134 completed (loss: 0.13470284640789032, acc: 0.9622641801834106)
[2025-02-13 19:56:02,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:02,242][root][INFO] - Training Epoch: 1/2, step 4607/7134 completed (loss: 0.15234938263893127, acc: 0.957446813583374)
[2025-02-13 19:56:02,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:02,614][root][INFO] - Training Epoch: 1/2, step 4608/7134 completed (loss: 0.11743083596229553, acc: 0.9824561476707458)
[2025-02-13 19:56:02,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:03,045][root][INFO] - Training Epoch: 1/2, step 4609/7134 completed (loss: 0.11064133048057556, acc: 0.9694656729698181)
[2025-02-13 19:56:03,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:03,486][root][INFO] - Training Epoch: 1/2, step 4610/7134 completed (loss: 0.06503460556268692, acc: 0.9832402467727661)
[2025-02-13 19:56:03,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:03,861][root][INFO] - Training Epoch: 1/2, step 4611/7134 completed (loss: 0.07622319459915161, acc: 0.9931034445762634)
[2025-02-13 19:56:03,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:04,277][root][INFO] - Training Epoch: 1/2, step 4612/7134 completed (loss: 0.15355278551578522, acc: 0.9629629850387573)
[2025-02-13 19:56:04,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:04,699][root][INFO] - Training Epoch: 1/2, step 4613/7134 completed (loss: 0.1370965838432312, acc: 0.9814814925193787)
[2025-02-13 19:56:04,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:05,137][root][INFO] - Training Epoch: 1/2, step 4614/7134 completed (loss: 0.13651680946350098, acc: 0.9461538195610046)
[2025-02-13 19:56:05,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:05,537][root][INFO] - Training Epoch: 1/2, step 4615/7134 completed (loss: 0.20832687616348267, acc: 0.9341317415237427)
[2025-02-13 19:56:05,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:05,948][root][INFO] - Training Epoch: 1/2, step 4616/7134 completed (loss: 0.23805268108844757, acc: 0.9186046719551086)
[2025-02-13 19:56:06,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:06,324][root][INFO] - Training Epoch: 1/2, step 4617/7134 completed (loss: 0.470746248960495, acc: 0.9041916131973267)
[2025-02-13 19:56:06,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:06,732][root][INFO] - Training Epoch: 1/2, step 4618/7134 completed (loss: 0.2845272123813629, acc: 0.9069767594337463)
[2025-02-13 19:56:06,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:07,165][root][INFO] - Training Epoch: 1/2, step 4619/7134 completed (loss: 0.2207147181034088, acc: 0.9624999761581421)
[2025-02-13 19:56:07,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:07,581][root][INFO] - Training Epoch: 1/2, step 4620/7134 completed (loss: 0.19289082288742065, acc: 0.9823529124259949)
[2025-02-13 19:56:07,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:07,970][root][INFO] - Training Epoch: 1/2, step 4621/7134 completed (loss: 0.08336217701435089, acc: 0.9878048896789551)
[2025-02-13 19:56:08,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:08,347][root][INFO] - Training Epoch: 1/2, step 4622/7134 completed (loss: 0.09439413994550705, acc: 0.9722222089767456)
[2025-02-13 19:56:08,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:08,723][root][INFO] - Training Epoch: 1/2, step 4623/7134 completed (loss: 0.17001712322235107, acc: 0.970588207244873)
[2025-02-13 19:56:08,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:09,145][root][INFO] - Training Epoch: 1/2, step 4624/7134 completed (loss: 0.28179389238357544, acc: 0.9235293865203857)
[2025-02-13 19:56:09,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:09,520][root][INFO] - Training Epoch: 1/2, step 4625/7134 completed (loss: 0.41952359676361084, acc: 0.8588957190513611)
[2025-02-13 19:56:09,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:09,941][root][INFO] - Training Epoch: 1/2, step 4626/7134 completed (loss: 0.30992409586906433, acc: 0.9130434989929199)
[2025-02-13 19:56:10,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:10,312][root][INFO] - Training Epoch: 1/2, step 4627/7134 completed (loss: 0.2044731229543686, acc: 0.948387086391449)
[2025-02-13 19:56:10,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:10,690][root][INFO] - Training Epoch: 1/2, step 4628/7134 completed (loss: 0.16457761824131012, acc: 0.9634146094322205)
[2025-02-13 19:56:10,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:11,038][root][INFO] - Training Epoch: 1/2, step 4629/7134 completed (loss: 0.20388665795326233, acc: 0.9220778942108154)
[2025-02-13 19:56:11,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:11,397][root][INFO] - Training Epoch: 1/2, step 4630/7134 completed (loss: 0.1539074182510376, acc: 0.9722222089767456)
[2025-02-13 19:56:11,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:11,769][root][INFO] - Training Epoch: 1/2, step 4631/7134 completed (loss: 0.14848195016384125, acc: 0.9674418568611145)
[2025-02-13 19:56:11,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:12,189][root][INFO] - Training Epoch: 1/2, step 4632/7134 completed (loss: 0.1890697479248047, acc: 0.9698795080184937)
[2025-02-13 19:56:12,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:12,572][root][INFO] - Training Epoch: 1/2, step 4633/7134 completed (loss: 0.28876084089279175, acc: 0.931506872177124)
[2025-02-13 19:56:12,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:12,972][root][INFO] - Training Epoch: 1/2, step 4634/7134 completed (loss: 0.13473708927631378, acc: 0.9629629850387573)
[2025-02-13 19:56:13,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:13,364][root][INFO] - Training Epoch: 1/2, step 4635/7134 completed (loss: 0.2244093418121338, acc: 0.9817073345184326)
[2025-02-13 19:56:13,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:13,744][root][INFO] - Training Epoch: 1/2, step 4636/7134 completed (loss: 0.11939525604248047, acc: 0.9680851101875305)
[2025-02-13 19:56:13,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:14,155][root][INFO] - Training Epoch: 1/2, step 4637/7134 completed (loss: 0.2042054533958435, acc: 0.9682539701461792)
[2025-02-13 19:56:14,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:14,561][root][INFO] - Training Epoch: 1/2, step 4638/7134 completed (loss: 0.23812498152256012, acc: 0.9595375657081604)
[2025-02-13 19:56:14,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:14,923][root][INFO] - Training Epoch: 1/2, step 4639/7134 completed (loss: 0.1571480631828308, acc: 0.9675675630569458)
[2025-02-13 19:56:15,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:15,291][root][INFO] - Training Epoch: 1/2, step 4640/7134 completed (loss: 0.18690912425518036, acc: 0.9523809552192688)
[2025-02-13 19:56:15,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:15,642][root][INFO] - Training Epoch: 1/2, step 4641/7134 completed (loss: 0.1565527617931366, acc: 0.9791666865348816)
[2025-02-13 19:56:15,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:16,009][root][INFO] - Training Epoch: 1/2, step 4642/7134 completed (loss: 0.0839894562959671, acc: 0.9717513918876648)
[2025-02-13 19:56:16,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:16,443][root][INFO] - Training Epoch: 1/2, step 4643/7134 completed (loss: 0.1007114052772522, acc: 0.9658536314964294)
[2025-02-13 19:56:16,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:16,865][root][INFO] - Training Epoch: 1/2, step 4644/7134 completed (loss: 0.09172981977462769, acc: 0.9704433679580688)
[2025-02-13 19:56:17,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:17,233][root][INFO] - Training Epoch: 1/2, step 4645/7134 completed (loss: 0.04110347479581833, acc: 0.9939024448394775)
[2025-02-13 19:56:17,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:17,606][root][INFO] - Training Epoch: 1/2, step 4646/7134 completed (loss: 0.1541949361562729, acc: 0.9571428298950195)
[2025-02-13 19:56:17,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:17,997][root][INFO] - Training Epoch: 1/2, step 4647/7134 completed (loss: 0.25587111711502075, acc: 0.9453551769256592)
[2025-02-13 19:56:18,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:18,391][root][INFO] - Training Epoch: 1/2, step 4648/7134 completed (loss: 0.25644880533218384, acc: 0.9554139971733093)
[2025-02-13 19:56:18,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:18,754][root][INFO] - Training Epoch: 1/2, step 4649/7134 completed (loss: 0.2101420909166336, acc: 0.9512194991111755)
[2025-02-13 19:56:18,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:19,128][root][INFO] - Training Epoch: 1/2, step 4650/7134 completed (loss: 0.20489639043807983, acc: 0.9391891956329346)
[2025-02-13 19:56:19,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:19,453][root][INFO] - Training Epoch: 1/2, step 4651/7134 completed (loss: 0.2795332372188568, acc: 0.9497206807136536)
[2025-02-13 19:56:19,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:19,847][root][INFO] - Training Epoch: 1/2, step 4652/7134 completed (loss: 0.13338539004325867, acc: 0.9567901492118835)
[2025-02-13 19:56:19,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:20,245][root][INFO] - Training Epoch: 1/2, step 4653/7134 completed (loss: 0.12515124678611755, acc: 0.9710144996643066)
[2025-02-13 19:56:20,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:20,628][root][INFO] - Training Epoch: 1/2, step 4654/7134 completed (loss: 0.2790561318397522, acc: 0.9243243336677551)
[2025-02-13 19:56:20,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:20,987][root][INFO] - Training Epoch: 1/2, step 4655/7134 completed (loss: 0.3234635591506958, acc: 0.9485714435577393)
[2025-02-13 19:56:21,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:21,384][root][INFO] - Training Epoch: 1/2, step 4656/7134 completed (loss: 0.16583214700222015, acc: 0.9576271176338196)
[2025-02-13 19:56:21,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:21,795][root][INFO] - Training Epoch: 1/2, step 4657/7134 completed (loss: 0.20899562537670135, acc: 0.931034505367279)
[2025-02-13 19:56:21,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:22,177][root][INFO] - Training Epoch: 1/2, step 4658/7134 completed (loss: 0.18055123090744019, acc: 0.9375)
[2025-02-13 19:56:22,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:22,554][root][INFO] - Training Epoch: 1/2, step 4659/7134 completed (loss: 0.2310156673192978, acc: 0.9312169551849365)
[2025-02-13 19:56:22,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:22,918][root][INFO] - Training Epoch: 1/2, step 4660/7134 completed (loss: 0.22563469409942627, acc: 0.942307710647583)
[2025-02-13 19:56:23,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:23,234][root][INFO] - Training Epoch: 1/2, step 4661/7134 completed (loss: 0.1863134801387787, acc: 0.9325153231620789)
[2025-02-13 19:56:23,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:23,582][root][INFO] - Training Epoch: 1/2, step 4662/7134 completed (loss: 0.288095086812973, acc: 0.9290780425071716)
[2025-02-13 19:56:23,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:23,979][root][INFO] - Training Epoch: 1/2, step 4663/7134 completed (loss: 0.30256757140159607, acc: 0.9180327653884888)
[2025-02-13 19:56:24,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:24,390][root][INFO] - Training Epoch: 1/2, step 4664/7134 completed (loss: 0.20585569739341736, acc: 0.9675675630569458)
[2025-02-13 19:56:24,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:24,785][root][INFO] - Training Epoch: 1/2, step 4665/7134 completed (loss: 0.13276813924312592, acc: 0.9505494236946106)
[2025-02-13 19:56:24,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:25,175][root][INFO] - Training Epoch: 1/2, step 4666/7134 completed (loss: 0.35918769240379333, acc: 0.9274611473083496)
[2025-02-13 19:56:25,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:25,535][root][INFO] - Training Epoch: 1/2, step 4667/7134 completed (loss: 0.40985602140426636, acc: 0.912162184715271)
[2025-02-13 19:56:25,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:25,928][root][INFO] - Training Epoch: 1/2, step 4668/7134 completed (loss: 0.15018805861473083, acc: 0.9664429426193237)
[2025-02-13 19:56:26,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:26,300][root][INFO] - Training Epoch: 1/2, step 4669/7134 completed (loss: 0.16713136434555054, acc: 0.9661017060279846)
[2025-02-13 19:56:26,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:26,677][root][INFO] - Training Epoch: 1/2, step 4670/7134 completed (loss: 0.1627798229455948, acc: 0.9701492786407471)
[2025-02-13 19:56:26,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:27,066][root][INFO] - Training Epoch: 1/2, step 4671/7134 completed (loss: 0.13359026610851288, acc: 0.9617486596107483)
[2025-02-13 19:56:27,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:27,422][root][INFO] - Training Epoch: 1/2, step 4672/7134 completed (loss: 0.09899110347032547, acc: 0.9724137783050537)
[2025-02-13 19:56:27,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:27,874][root][INFO] - Training Epoch: 1/2, step 4673/7134 completed (loss: 0.17601357400417328, acc: 0.9622641801834106)
[2025-02-13 19:56:28,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:28,276][root][INFO] - Training Epoch: 1/2, step 4674/7134 completed (loss: 0.1221187487244606, acc: 0.9610389471054077)
[2025-02-13 19:56:28,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:28,686][root][INFO] - Training Epoch: 1/2, step 4675/7134 completed (loss: 0.2057400494813919, acc: 0.9538461565971375)
[2025-02-13 19:56:28,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:29,067][root][INFO] - Training Epoch: 1/2, step 4676/7134 completed (loss: 0.09877283126115799, acc: 0.9805194735527039)
[2025-02-13 19:56:29,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:29,445][root][INFO] - Training Epoch: 1/2, step 4677/7134 completed (loss: 0.1325630396604538, acc: 0.9663865566253662)
[2025-02-13 19:56:29,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:29,852][root][INFO] - Training Epoch: 1/2, step 4678/7134 completed (loss: 0.21948444843292236, acc: 0.9580838084220886)
[2025-02-13 19:56:30,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:30,262][root][INFO] - Training Epoch: 1/2, step 4679/7134 completed (loss: 0.10119088739156723, acc: 0.9884393215179443)
[2025-02-13 19:56:30,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:30,613][root][INFO] - Training Epoch: 1/2, step 4680/7134 completed (loss: 0.24604758620262146, acc: 0.9363636374473572)
[2025-02-13 19:56:30,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:30,976][root][INFO] - Training Epoch: 1/2, step 4681/7134 completed (loss: 0.2684178650379181, acc: 0.9415584206581116)
[2025-02-13 19:56:31,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:31,361][root][INFO] - Training Epoch: 1/2, step 4682/7134 completed (loss: 0.15089067816734314, acc: 0.9668508172035217)
[2025-02-13 19:56:31,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:31,747][root][INFO] - Training Epoch: 1/2, step 4683/7134 completed (loss: 0.1929926872253418, acc: 0.95333331823349)
[2025-02-13 19:56:31,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:32,155][root][INFO] - Training Epoch: 1/2, step 4684/7134 completed (loss: 0.2489921748638153, acc: 0.9536082744598389)
[2025-02-13 19:56:32,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:32,518][root][INFO] - Training Epoch: 1/2, step 4685/7134 completed (loss: 0.10560666769742966, acc: 0.9802955389022827)
[2025-02-13 19:56:32,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:32,920][root][INFO] - Training Epoch: 1/2, step 4686/7134 completed (loss: 0.14139877259731293, acc: 0.9629629850387573)
[2025-02-13 19:56:33,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:33,261][root][INFO] - Training Epoch: 1/2, step 4687/7134 completed (loss: 0.5490275025367737, acc: 0.8478260636329651)
[2025-02-13 19:56:33,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:33,624][root][INFO] - Training Epoch: 1/2, step 4688/7134 completed (loss: 0.7792466282844543, acc: 0.875)
[2025-02-13 19:56:33,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:33,998][root][INFO] - Training Epoch: 1/2, step 4689/7134 completed (loss: 0.39675450325012207, acc: 0.8965517282485962)
[2025-02-13 19:56:34,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:34,382][root][INFO] - Training Epoch: 1/2, step 4690/7134 completed (loss: 0.44508859515190125, acc: 0.9035087823867798)
[2025-02-13 19:56:34,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:34,733][root][INFO] - Training Epoch: 1/2, step 4691/7134 completed (loss: 0.19760024547576904, acc: 0.9538461565971375)
[2025-02-13 19:56:34,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:35,075][root][INFO] - Training Epoch: 1/2, step 4692/7134 completed (loss: 0.7870548963546753, acc: 0.817460298538208)
[2025-02-13 19:56:35,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:35,500][root][INFO] - Training Epoch: 1/2, step 4693/7134 completed (loss: 0.6596878170967102, acc: 0.8571428656578064)
[2025-02-13 19:56:35,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:35,889][root][INFO] - Training Epoch: 1/2, step 4694/7134 completed (loss: 0.579188883304596, acc: 0.8571428656578064)
[2025-02-13 19:56:36,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:36,273][root][INFO] - Training Epoch: 1/2, step 4695/7134 completed (loss: 0.4212992787361145, acc: 0.9133333563804626)
[2025-02-13 19:56:36,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:36,627][root][INFO] - Training Epoch: 1/2, step 4696/7134 completed (loss: 0.10858558863401413, acc: 0.9757575988769531)
[2025-02-13 19:56:36,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:37,011][root][INFO] - Training Epoch: 1/2, step 4697/7134 completed (loss: 0.1609894037246704, acc: 0.9682539701461792)
[2025-02-13 19:56:37,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:37,354][root][INFO] - Training Epoch: 1/2, step 4698/7134 completed (loss: 0.5192720293998718, acc: 0.8857142925262451)
[2025-02-13 19:56:37,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:37,714][root][INFO] - Training Epoch: 1/2, step 4699/7134 completed (loss: 0.4607129395008087, acc: 0.8920863270759583)
[2025-02-13 19:56:37,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:38,147][root][INFO] - Training Epoch: 1/2, step 4700/7134 completed (loss: 0.24242906272411346, acc: 0.9467455744743347)
[2025-02-13 19:56:38,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:38,489][root][INFO] - Training Epoch: 1/2, step 4701/7134 completed (loss: 0.6787872910499573, acc: 0.8500000238418579)
[2025-02-13 19:56:38,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:38,858][root][INFO] - Training Epoch: 1/2, step 4702/7134 completed (loss: 0.15973398089408875, acc: 0.9548386931419373)
[2025-02-13 19:56:38,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:39,220][root][INFO] - Training Epoch: 1/2, step 4703/7134 completed (loss: 0.12303964048624039, acc: 0.970588207244873)
[2025-02-13 19:56:39,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:39,590][root][INFO] - Training Epoch: 1/2, step 4704/7134 completed (loss: 0.1217455342411995, acc: 0.964102566242218)
[2025-02-13 19:56:39,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:39,980][root][INFO] - Training Epoch: 1/2, step 4705/7134 completed (loss: 0.2435709834098816, acc: 0.9408866763114929)
[2025-02-13 19:56:40,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:40,348][root][INFO] - Training Epoch: 1/2, step 4706/7134 completed (loss: 0.3128468692302704, acc: 0.9230769276618958)
[2025-02-13 19:56:40,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:40,712][root][INFO] - Training Epoch: 1/2, step 4707/7134 completed (loss: 0.703307032585144, acc: 0.8805031180381775)
[2025-02-13 19:56:40,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:41,087][root][INFO] - Training Epoch: 1/2, step 4708/7134 completed (loss: 0.1774713099002838, acc: 0.9597989916801453)
[2025-02-13 19:56:41,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:41,451][root][INFO] - Training Epoch: 1/2, step 4709/7134 completed (loss: 0.13763290643692017, acc: 0.9675675630569458)
[2025-02-13 19:56:41,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:41,825][root][INFO] - Training Epoch: 1/2, step 4710/7134 completed (loss: 0.18094931542873383, acc: 0.9515151381492615)
[2025-02-13 19:56:41,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:42,231][root][INFO] - Training Epoch: 1/2, step 4711/7134 completed (loss: 0.22106032073497772, acc: 0.9459459185600281)
[2025-02-13 19:56:42,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:42,586][root][INFO] - Training Epoch: 1/2, step 4712/7134 completed (loss: 0.5116567611694336, acc: 0.9019607901573181)
[2025-02-13 19:56:42,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:42,947][root][INFO] - Training Epoch: 1/2, step 4713/7134 completed (loss: 0.18489019572734833, acc: 0.9402984976768494)
[2025-02-13 19:56:43,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:43,265][root][INFO] - Training Epoch: 1/2, step 4714/7134 completed (loss: 0.3457225263118744, acc: 0.9541984796524048)
[2025-02-13 19:56:43,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:43,631][root][INFO] - Training Epoch: 1/2, step 4715/7134 completed (loss: 0.41296470165252686, acc: 0.9416666626930237)
[2025-02-13 19:56:43,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:43,991][root][INFO] - Training Epoch: 1/2, step 4716/7134 completed (loss: 0.15625396370887756, acc: 0.9612902998924255)
[2025-02-13 19:56:44,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:44,353][root][INFO] - Training Epoch: 1/2, step 4717/7134 completed (loss: 0.11115410178899765, acc: 0.9733333587646484)
[2025-02-13 19:56:44,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:44,711][root][INFO] - Training Epoch: 1/2, step 4718/7134 completed (loss: 0.09077870100736618, acc: 0.9784172773361206)
[2025-02-13 19:56:44,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:45,077][root][INFO] - Training Epoch: 1/2, step 4719/7134 completed (loss: 0.11554406583309174, acc: 0.9704142212867737)
[2025-02-13 19:56:45,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:45,478][root][INFO] - Training Epoch: 1/2, step 4720/7134 completed (loss: 0.04813940450549126, acc: 0.987500011920929)
[2025-02-13 19:56:45,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:45,839][root][INFO] - Training Epoch: 1/2, step 4721/7134 completed (loss: 0.036948129534721375, acc: 0.9938271641731262)
[2025-02-13 19:56:45,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:46,200][root][INFO] - Training Epoch: 1/2, step 4722/7134 completed (loss: 0.2511463165283203, acc: 0.955974817276001)
[2025-02-13 19:56:46,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:46,566][root][INFO] - Training Epoch: 1/2, step 4723/7134 completed (loss: 0.1348009705543518, acc: 0.9745222926139832)
[2025-02-13 19:56:46,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:46,944][root][INFO] - Training Epoch: 1/2, step 4724/7134 completed (loss: 0.1574372500181198, acc: 0.9390243887901306)
[2025-02-13 19:56:47,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:47,305][root][INFO] - Training Epoch: 1/2, step 4725/7134 completed (loss: 0.25321295857429504, acc: 0.9127907156944275)
[2025-02-13 19:56:47,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:47,692][root][INFO] - Training Epoch: 1/2, step 4726/7134 completed (loss: 0.35705816745758057, acc: 0.8965517282485962)
[2025-02-13 19:56:47,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:48,058][root][INFO] - Training Epoch: 1/2, step 4727/7134 completed (loss: 0.1986340880393982, acc: 0.9536423683166504)
[2025-02-13 19:56:48,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:48,431][root][INFO] - Training Epoch: 1/2, step 4728/7134 completed (loss: 0.19162781536579132, acc: 0.9271523356437683)
[2025-02-13 19:56:48,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:48,789][root][INFO] - Training Epoch: 1/2, step 4729/7134 completed (loss: 0.12858819961547852, acc: 0.9710982441902161)
[2025-02-13 19:56:48,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:49,134][root][INFO] - Training Epoch: 1/2, step 4730/7134 completed (loss: 0.20422565937042236, acc: 0.9415204524993896)
[2025-02-13 19:56:49,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:49,490][root][INFO] - Training Epoch: 1/2, step 4731/7134 completed (loss: 0.31565093994140625, acc: 0.9448275566101074)
[2025-02-13 19:56:49,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:49,867][root][INFO] - Training Epoch: 1/2, step 4732/7134 completed (loss: 0.23327897489070892, acc: 0.9411764740943909)
[2025-02-13 19:56:50,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:50,237][root][INFO] - Training Epoch: 1/2, step 4733/7134 completed (loss: 0.21647605299949646, acc: 0.9492753744125366)
[2025-02-13 19:56:50,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:50,617][root][INFO] - Training Epoch: 1/2, step 4734/7134 completed (loss: 0.3834821283817291, acc: 0.9119496941566467)
[2025-02-13 19:56:50,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:51,020][root][INFO] - Training Epoch: 1/2, step 4735/7134 completed (loss: 0.21264494955539703, acc: 0.955974817276001)
[2025-02-13 19:56:51,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:51,399][root][INFO] - Training Epoch: 1/2, step 4736/7134 completed (loss: 0.34339407086372375, acc: 0.925000011920929)
[2025-02-13 19:56:51,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:51,841][root][INFO] - Training Epoch: 1/2, step 4737/7134 completed (loss: 0.23769035935401917, acc: 0.931034505367279)
[2025-02-13 19:56:52,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:52,260][root][INFO] - Training Epoch: 1/2, step 4738/7134 completed (loss: 0.1931716799736023, acc: 0.956250011920929)
[2025-02-13 19:56:52,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:52,633][root][INFO] - Training Epoch: 1/2, step 4739/7134 completed (loss: 0.36992985010147095, acc: 0.9044944047927856)
[2025-02-13 19:56:52,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:53,022][root][INFO] - Training Epoch: 1/2, step 4740/7134 completed (loss: 0.3598502576351166, acc: 0.9333333373069763)
[2025-02-13 19:56:53,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:53,379][root][INFO] - Training Epoch: 1/2, step 4741/7134 completed (loss: 0.21769006550312042, acc: 0.9357143044471741)
[2025-02-13 19:56:53,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:53,734][root][INFO] - Training Epoch: 1/2, step 4742/7134 completed (loss: 0.5674933195114136, acc: 0.8909090757369995)
[2025-02-13 19:56:53,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:54,081][root][INFO] - Training Epoch: 1/2, step 4743/7134 completed (loss: 0.35305356979370117, acc: 0.9144737124443054)
[2025-02-13 19:56:54,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:54,448][root][INFO] - Training Epoch: 1/2, step 4744/7134 completed (loss: 0.1257087141275406, acc: 0.9724770784378052)
[2025-02-13 19:56:54,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:54,835][root][INFO] - Training Epoch: 1/2, step 4745/7134 completed (loss: 0.31674158573150635, acc: 0.9444444179534912)
[2025-02-13 19:56:54,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:55,209][root][INFO] - Training Epoch: 1/2, step 4746/7134 completed (loss: 0.15831170976161957, acc: 0.9844961166381836)
[2025-02-13 19:56:55,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:55,605][root][INFO] - Training Epoch: 1/2, step 4747/7134 completed (loss: 0.16717787086963654, acc: 0.9651162624359131)
[2025-02-13 19:56:55,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:55,996][root][INFO] - Training Epoch: 1/2, step 4748/7134 completed (loss: 0.3465459942817688, acc: 0.9171597361564636)
[2025-02-13 19:56:56,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:56,373][root][INFO] - Training Epoch: 1/2, step 4749/7134 completed (loss: 0.09904993325471878, acc: 0.9651162624359131)
[2025-02-13 19:56:56,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:56,751][root][INFO] - Training Epoch: 1/2, step 4750/7134 completed (loss: 0.1390712857246399, acc: 0.9674796462059021)
[2025-02-13 19:56:56,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:57,123][root][INFO] - Training Epoch: 1/2, step 4751/7134 completed (loss: 0.1117331013083458, acc: 0.9634146094322205)
[2025-02-13 19:56:57,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:57,486][root][INFO] - Training Epoch: 1/2, step 4752/7134 completed (loss: 0.25321561098098755, acc: 0.9378882050514221)
[2025-02-13 19:56:57,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:57,859][root][INFO] - Training Epoch: 1/2, step 4753/7134 completed (loss: 0.32631734013557434, acc: 0.9349112510681152)
[2025-02-13 19:56:58,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:58,206][root][INFO] - Training Epoch: 1/2, step 4754/7134 completed (loss: 0.049021393060684204, acc: 0.9935064911842346)
[2025-02-13 19:56:58,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:58,571][root][INFO] - Training Epoch: 1/2, step 4755/7134 completed (loss: 0.23937149345874786, acc: 0.9615384340286255)
[2025-02-13 19:56:58,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:58,909][root][INFO] - Training Epoch: 1/2, step 4756/7134 completed (loss: 0.2989940941333771, acc: 0.9354838728904724)
[2025-02-13 19:56:59,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:59,284][root][INFO] - Training Epoch: 1/2, step 4757/7134 completed (loss: 0.11006499081850052, acc: 0.9599999785423279)
[2025-02-13 19:56:59,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:59,667][root][INFO] - Training Epoch: 1/2, step 4758/7134 completed (loss: 0.09348241239786148, acc: 0.988950252532959)
[2025-02-13 19:56:59,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:00,025][root][INFO] - Training Epoch: 1/2, step 4759/7134 completed (loss: 0.06133081018924713, acc: 0.978723406791687)
[2025-02-13 19:57:00,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:00,382][root][INFO] - Training Epoch: 1/2, step 4760/7134 completed (loss: 0.12333137542009354, acc: 0.9663865566253662)
[2025-02-13 19:57:00,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:00,738][root][INFO] - Training Epoch: 1/2, step 4761/7134 completed (loss: 0.14335401356220245, acc: 0.9695122241973877)
[2025-02-13 19:57:00,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:01,091][root][INFO] - Training Epoch: 1/2, step 4762/7134 completed (loss: 0.3041573762893677, acc: 0.9395604133605957)
[2025-02-13 19:57:01,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:01,459][root][INFO] - Training Epoch: 1/2, step 4763/7134 completed (loss: 0.15916264057159424, acc: 0.9735099077224731)
[2025-02-13 19:57:01,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:01,838][root][INFO] - Training Epoch: 1/2, step 4764/7134 completed (loss: 0.18753394484519958, acc: 0.9432989954948425)
[2025-02-13 19:57:01,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:02,203][root][INFO] - Training Epoch: 1/2, step 4765/7134 completed (loss: 0.3383714556694031, acc: 0.9333333373069763)
[2025-02-13 19:57:02,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:02,576][root][INFO] - Training Epoch: 1/2, step 4766/7134 completed (loss: 0.15208479762077332, acc: 0.9611111283302307)
[2025-02-13 19:57:02,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:02,916][root][INFO] - Training Epoch: 1/2, step 4767/7134 completed (loss: 0.14808547496795654, acc: 0.9795918464660645)
[2025-02-13 19:57:03,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:03,302][root][INFO] - Training Epoch: 1/2, step 4768/7134 completed (loss: 0.13585422933101654, acc: 0.9647058844566345)
[2025-02-13 19:57:03,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:03,651][root][INFO] - Training Epoch: 1/2, step 4769/7134 completed (loss: 0.23586393892765045, acc: 0.9539473652839661)
[2025-02-13 19:57:03,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:04,030][root][INFO] - Training Epoch: 1/2, step 4770/7134 completed (loss: 0.07365427911281586, acc: 0.984000027179718)
[2025-02-13 19:57:04,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:04,390][root][INFO] - Training Epoch: 1/2, step 4771/7134 completed (loss: 0.29034700989723206, acc: 0.9386503100395203)
[2025-02-13 19:57:04,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:04,749][root][INFO] - Training Epoch: 1/2, step 4772/7134 completed (loss: 0.24783971905708313, acc: 0.9354838728904724)
[2025-02-13 19:57:04,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:05,091][root][INFO] - Training Epoch: 1/2, step 4773/7134 completed (loss: 0.12689389288425446, acc: 0.9615384340286255)
[2025-02-13 19:57:05,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:05,439][root][INFO] - Training Epoch: 1/2, step 4774/7134 completed (loss: 0.14360201358795166, acc: 0.9704142212867737)
[2025-02-13 19:57:05,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:05,791][root][INFO] - Training Epoch: 1/2, step 4775/7134 completed (loss: 0.2635716199874878, acc: 0.9622641801834106)
[2025-02-13 19:57:05,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:06,151][root][INFO] - Training Epoch: 1/2, step 4776/7134 completed (loss: 0.33053985238075256, acc: 0.9154929518699646)
[2025-02-13 19:57:06,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:06,523][root][INFO] - Training Epoch: 1/2, step 4777/7134 completed (loss: 0.23744551837444305, acc: 0.9383561611175537)
[2025-02-13 19:57:06,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:06,895][root][INFO] - Training Epoch: 1/2, step 4778/7134 completed (loss: 0.17117540538311005, acc: 0.9464285969734192)
[2025-02-13 19:57:07,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:07,297][root][INFO] - Training Epoch: 1/2, step 4779/7134 completed (loss: 0.1289440542459488, acc: 0.970370352268219)
[2025-02-13 19:57:07,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:07,693][root][INFO] - Training Epoch: 1/2, step 4780/7134 completed (loss: 0.05930715799331665, acc: 0.9807692170143127)
[2025-02-13 19:57:07,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:08,041][root][INFO] - Training Epoch: 1/2, step 4781/7134 completed (loss: 0.07477570325136185, acc: 0.9838709831237793)
[2025-02-13 19:57:08,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:08,400][root][INFO] - Training Epoch: 1/2, step 4782/7134 completed (loss: 0.12191402912139893, acc: 0.981249988079071)
[2025-02-13 19:57:08,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:08,780][root][INFO] - Training Epoch: 1/2, step 4783/7134 completed (loss: 0.35591602325439453, acc: 0.9202898740768433)
[2025-02-13 19:57:08,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:09,157][root][INFO] - Training Epoch: 1/2, step 4784/7134 completed (loss: 0.20806805789470673, acc: 0.9555555582046509)
[2025-02-13 19:57:09,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:09,584][root][INFO] - Training Epoch: 1/2, step 4785/7134 completed (loss: 0.16111724078655243, acc: 0.9776119589805603)
[2025-02-13 19:57:09,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:09,963][root][INFO] - Training Epoch: 1/2, step 4786/7134 completed (loss: 0.08204342424869537, acc: 0.9691358208656311)
[2025-02-13 19:57:10,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:10,342][root][INFO] - Training Epoch: 1/2, step 4787/7134 completed (loss: 0.10963723063468933, acc: 0.9811320900917053)
[2025-02-13 19:57:10,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:10,756][root][INFO] - Training Epoch: 1/2, step 4788/7134 completed (loss: 0.06740457564592361, acc: 0.9846153855323792)
[2025-02-13 19:57:10,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:11,136][root][INFO] - Training Epoch: 1/2, step 4789/7134 completed (loss: 0.16288459300994873, acc: 0.9679487347602844)
[2025-02-13 19:57:11,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:11,526][root][INFO] - Training Epoch: 1/2, step 4790/7134 completed (loss: 0.16768233478069305, acc: 0.9550561904907227)
[2025-02-13 19:57:11,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:11,911][root][INFO] - Training Epoch: 1/2, step 4791/7134 completed (loss: 0.061186447739601135, acc: 0.9850746393203735)
[2025-02-13 19:57:12,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:12,292][root][INFO] - Training Epoch: 1/2, step 4792/7134 completed (loss: 0.052955832332372665, acc: 0.9932432174682617)
[2025-02-13 19:57:12,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:12,648][root][INFO] - Training Epoch: 1/2, step 4793/7134 completed (loss: 0.37097805738449097, acc: 0.8947368264198303)
[2025-02-13 19:57:12,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:13,028][root][INFO] - Training Epoch: 1/2, step 4794/7134 completed (loss: 0.3767816722393036, acc: 0.9078013896942139)
[2025-02-13 19:57:13,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:13,428][root][INFO] - Training Epoch: 1/2, step 4795/7134 completed (loss: 0.15220579504966736, acc: 0.9509202241897583)
[2025-02-13 19:57:13,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:13,840][root][INFO] - Training Epoch: 1/2, step 4796/7134 completed (loss: 0.255966454744339, acc: 0.9473684430122375)
[2025-02-13 19:57:13,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:14,204][root][INFO] - Training Epoch: 1/2, step 4797/7134 completed (loss: 0.12075113505125046, acc: 0.9672130942344666)
[2025-02-13 19:57:14,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:14,562][root][INFO] - Training Epoch: 1/2, step 4798/7134 completed (loss: 0.14825305342674255, acc: 0.976047933101654)
[2025-02-13 19:57:14,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:14,944][root][INFO] - Training Epoch: 1/2, step 4799/7134 completed (loss: 0.24717235565185547, acc: 0.9431818127632141)
[2025-02-13 19:57:15,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:15,311][root][INFO] - Training Epoch: 1/2, step 4800/7134 completed (loss: 0.1560712307691574, acc: 0.9473684430122375)
[2025-02-13 19:57:15,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:15,686][root][INFO] - Training Epoch: 1/2, step 4801/7134 completed (loss: 0.17664973437786102, acc: 0.9693877696990967)
[2025-02-13 19:57:15,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:16,067][root][INFO] - Training Epoch: 1/2, step 4802/7134 completed (loss: 0.29204127192497253, acc: 0.938144326210022)
[2025-02-13 19:57:16,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:16,413][root][INFO] - Training Epoch: 1/2, step 4803/7134 completed (loss: 0.179923877120018, acc: 0.9636363387107849)
[2025-02-13 19:57:16,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:16,777][root][INFO] - Training Epoch: 1/2, step 4804/7134 completed (loss: 0.2560791075229645, acc: 0.9337349534034729)
[2025-02-13 19:57:16,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:17,140][root][INFO] - Training Epoch: 1/2, step 4805/7134 completed (loss: 0.1305282562971115, acc: 0.9801324605941772)
[2025-02-13 19:57:17,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:17,497][root][INFO] - Training Epoch: 1/2, step 4806/7134 completed (loss: 0.18305692076683044, acc: 0.9790209531784058)
[2025-02-13 19:57:17,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:17,842][root][INFO] - Training Epoch: 1/2, step 4807/7134 completed (loss: 0.1400073766708374, acc: 0.9746835231781006)
[2025-02-13 19:57:17,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:18,198][root][INFO] - Training Epoch: 1/2, step 4808/7134 completed (loss: 0.1981385499238968, acc: 0.9545454382896423)
[2025-02-13 19:57:18,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:18,557][root][INFO] - Training Epoch: 1/2, step 4809/7134 completed (loss: 0.16490773856639862, acc: 0.9586206674575806)
[2025-02-13 19:57:18,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:18,942][root][INFO] - Training Epoch: 1/2, step 4810/7134 completed (loss: 0.1780378669500351, acc: 0.9529411792755127)
[2025-02-13 19:57:19,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:19,300][root][INFO] - Training Epoch: 1/2, step 4811/7134 completed (loss: 0.18096141517162323, acc: 0.9672130942344666)
[2025-02-13 19:57:19,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:19,679][root][INFO] - Training Epoch: 1/2, step 4812/7134 completed (loss: 0.24577993154525757, acc: 0.9604519605636597)
[2025-02-13 19:57:19,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:20,018][root][INFO] - Training Epoch: 1/2, step 4813/7134 completed (loss: 0.05645782873034477, acc: 0.9895833134651184)
[2025-02-13 19:57:20,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:20,390][root][INFO] - Training Epoch: 1/2, step 4814/7134 completed (loss: 0.15868805348873138, acc: 0.95652174949646)
[2025-02-13 19:57:20,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:20,798][root][INFO] - Training Epoch: 1/2, step 4815/7134 completed (loss: 0.018300868570804596, acc: 1.0)
[2025-02-13 19:57:20,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:21,163][root][INFO] - Training Epoch: 1/2, step 4816/7134 completed (loss: 0.03253256902098656, acc: 0.9899497628211975)
[2025-02-13 19:57:21,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:21,522][root][INFO] - Training Epoch: 1/2, step 4817/7134 completed (loss: 0.08589129894971848, acc: 0.9797979593276978)
[2025-02-13 19:57:21,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:21,911][root][INFO] - Training Epoch: 1/2, step 4818/7134 completed (loss: 0.11263912171125412, acc: 0.9659863710403442)
[2025-02-13 19:57:22,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:22,304][root][INFO] - Training Epoch: 1/2, step 4819/7134 completed (loss: 0.04716550186276436, acc: 0.9916666746139526)
[2025-02-13 19:57:22,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:22,684][root][INFO] - Training Epoch: 1/2, step 4820/7134 completed (loss: 0.08321934193372726, acc: 0.9893048405647278)
[2025-02-13 19:57:22,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:23,086][root][INFO] - Training Epoch: 1/2, step 4821/7134 completed (loss: 0.08942253142595291, acc: 0.9710982441902161)
[2025-02-13 19:57:23,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:23,488][root][INFO] - Training Epoch: 1/2, step 4822/7134 completed (loss: 0.07387600094079971, acc: 0.970588207244873)
[2025-02-13 19:57:23,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:23,904][root][INFO] - Training Epoch: 1/2, step 4823/7134 completed (loss: 0.2340213805437088, acc: 0.955974817276001)
[2025-02-13 19:57:24,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:24,273][root][INFO] - Training Epoch: 1/2, step 4824/7134 completed (loss: 0.27761998772621155, acc: 0.9491525292396545)
[2025-02-13 19:57:24,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:24,639][root][INFO] - Training Epoch: 1/2, step 4825/7134 completed (loss: 0.07185196131467819, acc: 0.970588207244873)
[2025-02-13 19:57:24,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:25,034][root][INFO] - Training Epoch: 1/2, step 4826/7134 completed (loss: 0.13544583320617676, acc: 0.9504132270812988)
[2025-02-13 19:57:25,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:25,413][root][INFO] - Training Epoch: 1/2, step 4827/7134 completed (loss: 0.32588592171669006, acc: 0.948387086391449)
[2025-02-13 19:57:25,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:25,781][root][INFO] - Training Epoch: 1/2, step 4828/7134 completed (loss: 0.061975400894880295, acc: 0.9948979616165161)
[2025-02-13 19:57:25,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:26,215][root][INFO] - Training Epoch: 1/2, step 4829/7134 completed (loss: 0.020288942381739616, acc: 0.9950494766235352)
[2025-02-13 19:57:26,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:26,607][root][INFO] - Training Epoch: 1/2, step 4830/7134 completed (loss: 0.04368631914258003, acc: 0.9939393997192383)
[2025-02-13 19:57:26,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:27,061][root][INFO] - Training Epoch: 1/2, step 4831/7134 completed (loss: 0.03023291751742363, acc: 0.9942857027053833)
[2025-02-13 19:57:27,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:27,451][root][INFO] - Training Epoch: 1/2, step 4832/7134 completed (loss: 0.028286872431635857, acc: 0.9934640526771545)
[2025-02-13 19:57:27,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:27,833][root][INFO] - Training Epoch: 1/2, step 4833/7134 completed (loss: 0.05766649916768074, acc: 0.9746192693710327)
[2025-02-13 19:57:27,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:28,199][root][INFO] - Training Epoch: 1/2, step 4834/7134 completed (loss: 0.053364358842372894, acc: 0.9906976819038391)
[2025-02-13 19:57:28,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:28,572][root][INFO] - Training Epoch: 1/2, step 4835/7134 completed (loss: 0.06962589919567108, acc: 0.9743589758872986)
[2025-02-13 19:57:28,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:28,953][root][INFO] - Training Epoch: 1/2, step 4836/7134 completed (loss: 0.057725295424461365, acc: 0.9938271641731262)
[2025-02-13 19:57:29,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:29,316][root][INFO] - Training Epoch: 1/2, step 4837/7134 completed (loss: 0.3504788875579834, acc: 0.9357143044471741)
[2025-02-13 19:57:29,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:29,687][root][INFO] - Training Epoch: 1/2, step 4838/7134 completed (loss: 0.3095933794975281, acc: 0.935251772403717)
[2025-02-13 19:57:29,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:30,100][root][INFO] - Training Epoch: 1/2, step 4839/7134 completed (loss: 0.24646659195423126, acc: 0.961240291595459)
[2025-02-13 19:57:30,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:30,522][root][INFO] - Training Epoch: 1/2, step 4840/7134 completed (loss: 0.16614043712615967, acc: 0.9407894611358643)
[2025-02-13 19:57:30,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:30,884][root][INFO] - Training Epoch: 1/2, step 4841/7134 completed (loss: 0.1584623157978058, acc: 0.9444444179534912)
[2025-02-13 19:57:31,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:31,310][root][INFO] - Training Epoch: 1/2, step 4842/7134 completed (loss: 0.2218962162733078, acc: 0.9555555582046509)
[2025-02-13 19:57:31,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:31,739][root][INFO] - Training Epoch: 1/2, step 4843/7134 completed (loss: 0.23212085664272308, acc: 0.9245283007621765)
[2025-02-13 19:57:31,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:32,142][root][INFO] - Training Epoch: 1/2, step 4844/7134 completed (loss: 0.14792068302631378, acc: 0.9307692050933838)
[2025-02-13 19:57:32,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:32,554][root][INFO] - Training Epoch: 1/2, step 4845/7134 completed (loss: 0.2028369903564453, acc: 0.949999988079071)
[2025-02-13 19:57:32,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:32,938][root][INFO] - Training Epoch: 1/2, step 4846/7134 completed (loss: 0.30802756547927856, acc: 0.918749988079071)
[2025-02-13 19:57:33,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:33,287][root][INFO] - Training Epoch: 1/2, step 4847/7134 completed (loss: 0.1235608235001564, acc: 0.9704142212867737)
[2025-02-13 19:57:33,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:33,642][root][INFO] - Training Epoch: 1/2, step 4848/7134 completed (loss: 0.3037731945514679, acc: 0.9444444179534912)
[2025-02-13 19:57:33,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:33,976][root][INFO] - Training Epoch: 1/2, step 4849/7134 completed (loss: 0.15035705268383026, acc: 0.965753436088562)
[2025-02-13 19:57:34,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:34,341][root][INFO] - Training Epoch: 1/2, step 4850/7134 completed (loss: 0.20421640574932098, acc: 0.9273743033409119)
[2025-02-13 19:57:34,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:34,718][root][INFO] - Training Epoch: 1/2, step 4851/7134 completed (loss: 0.1788337230682373, acc: 0.9459459185600281)
[2025-02-13 19:57:34,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:35,070][root][INFO] - Training Epoch: 1/2, step 4852/7134 completed (loss: 0.28126174211502075, acc: 0.9186046719551086)
[2025-02-13 19:57:35,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:35,485][root][INFO] - Training Epoch: 1/2, step 4853/7134 completed (loss: 0.449971467256546, acc: 0.8802395462989807)
[2025-02-13 19:57:35,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:35,864][root][INFO] - Training Epoch: 1/2, step 4854/7134 completed (loss: 0.1374429315328598, acc: 0.9664429426193237)
[2025-02-13 19:57:36,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:36,270][root][INFO] - Training Epoch: 1/2, step 4855/7134 completed (loss: 0.1780286580324173, acc: 0.9655172228813171)
[2025-02-13 19:57:36,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:36,649][root][INFO] - Training Epoch: 1/2, step 4856/7134 completed (loss: 0.10001204162836075, acc: 0.976190447807312)
[2025-02-13 19:57:36,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:37,034][root][INFO] - Training Epoch: 1/2, step 4857/7134 completed (loss: 0.18197044730186462, acc: 0.9444444179534912)
[2025-02-13 19:57:37,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:37,426][root][INFO] - Training Epoch: 1/2, step 4858/7134 completed (loss: 0.3460830748081207, acc: 0.9320987462997437)
[2025-02-13 19:57:37,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:37,821][root][INFO] - Training Epoch: 1/2, step 4859/7134 completed (loss: 0.12135238945484161, acc: 0.9523809552192688)
[2025-02-13 19:57:37,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:38,220][root][INFO] - Training Epoch: 1/2, step 4860/7134 completed (loss: 0.13916784524917603, acc: 0.9649122953414917)
[2025-02-13 19:57:38,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:38,626][root][INFO] - Training Epoch: 1/2, step 4861/7134 completed (loss: 0.1269102692604065, acc: 0.9585798978805542)
[2025-02-13 19:57:38,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:39,029][root][INFO] - Training Epoch: 1/2, step 4862/7134 completed (loss: 0.16801795363426208, acc: 0.9671052694320679)
[2025-02-13 19:57:39,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:39,426][root][INFO] - Training Epoch: 1/2, step 4863/7134 completed (loss: 0.13737788796424866, acc: 0.9689922332763672)
[2025-02-13 19:57:39,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:39,813][root][INFO] - Training Epoch: 1/2, step 4864/7134 completed (loss: 0.17764022946357727, acc: 0.9554139971733093)
[2025-02-13 19:57:39,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:40,181][root][INFO] - Training Epoch: 1/2, step 4865/7134 completed (loss: 0.1640024483203888, acc: 0.9569892287254333)
[2025-02-13 19:57:40,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:40,569][root][INFO] - Training Epoch: 1/2, step 4866/7134 completed (loss: 0.2348598688840866, acc: 0.9186046719551086)
[2025-02-13 19:57:40,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:40,943][root][INFO] - Training Epoch: 1/2, step 4867/7134 completed (loss: 0.18918322026729584, acc: 0.9608938694000244)
[2025-02-13 19:57:41,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:41,317][root][INFO] - Training Epoch: 1/2, step 4868/7134 completed (loss: 0.18191857635974884, acc: 0.9327731132507324)
[2025-02-13 19:57:41,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:41,696][root][INFO] - Training Epoch: 1/2, step 4869/7134 completed (loss: 0.14830860495567322, acc: 0.9591836929321289)
[2025-02-13 19:57:41,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:42,110][root][INFO] - Training Epoch: 1/2, step 4870/7134 completed (loss: 0.08864282816648483, acc: 0.9729729890823364)
[2025-02-13 19:57:42,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:42,558][root][INFO] - Training Epoch: 1/2, step 4871/7134 completed (loss: 0.14973080158233643, acc: 0.9539473652839661)
[2025-02-13 19:57:42,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:42,956][root][INFO] - Training Epoch: 1/2, step 4872/7134 completed (loss: 0.024634139612317085, acc: 1.0)
[2025-02-13 19:57:43,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:43,376][root][INFO] - Training Epoch: 1/2, step 4873/7134 completed (loss: 0.1786700189113617, acc: 0.9672130942344666)
[2025-02-13 19:57:43,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:43,762][root][INFO] - Training Epoch: 1/2, step 4874/7134 completed (loss: 0.1191229596734047, acc: 0.9707317352294922)
[2025-02-13 19:57:43,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:44,160][root][INFO] - Training Epoch: 1/2, step 4875/7134 completed (loss: 0.11482751369476318, acc: 0.9751552939414978)
[2025-02-13 19:57:44,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:44,589][root][INFO] - Training Epoch: 1/2, step 4876/7134 completed (loss: 0.21199455857276917, acc: 0.9444444179534912)
[2025-02-13 19:57:44,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:44,944][root][INFO] - Training Epoch: 1/2, step 4877/7134 completed (loss: 0.16292715072631836, acc: 0.9692307710647583)
[2025-02-13 19:57:45,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:45,340][root][INFO] - Training Epoch: 1/2, step 4878/7134 completed (loss: 0.14915494620800018, acc: 0.948051929473877)
[2025-02-13 19:57:45,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:45,724][root][INFO] - Training Epoch: 1/2, step 4879/7134 completed (loss: 0.12442567944526672, acc: 0.9551281929016113)
[2025-02-13 19:57:45,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:46,103][root][INFO] - Training Epoch: 1/2, step 4880/7134 completed (loss: 0.0908093973994255, acc: 0.970588207244873)
[2025-02-13 19:57:46,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:46,539][root][INFO] - Training Epoch: 1/2, step 4881/7134 completed (loss: 0.20745708048343658, acc: 0.9597315192222595)
[2025-02-13 19:57:46,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:46,933][root][INFO] - Training Epoch: 1/2, step 4882/7134 completed (loss: 0.14687329530715942, acc: 0.9677419066429138)
[2025-02-13 19:57:47,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:47,311][root][INFO] - Training Epoch: 1/2, step 4883/7134 completed (loss: 0.10439661890268326, acc: 0.9599999785423279)
[2025-02-13 19:57:47,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:47,713][root][INFO] - Training Epoch: 1/2, step 4884/7134 completed (loss: 0.09877313673496246, acc: 0.9673202633857727)
[2025-02-13 19:57:47,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:48,108][root][INFO] - Training Epoch: 1/2, step 4885/7134 completed (loss: 0.09392765909433365, acc: 0.9784946441650391)
[2025-02-13 19:57:48,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:48,506][root][INFO] - Training Epoch: 1/2, step 4886/7134 completed (loss: 0.16284731030464172, acc: 0.9711538553237915)
[2025-02-13 19:57:48,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:48,928][root][INFO] - Training Epoch: 1/2, step 4887/7134 completed (loss: 0.1025552824139595, acc: 0.9738219976425171)
[2025-02-13 19:57:49,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:49,314][root][INFO] - Training Epoch: 1/2, step 4888/7134 completed (loss: 0.09821998327970505, acc: 0.9780219793319702)
[2025-02-13 19:57:49,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:49,754][root][INFO] - Training Epoch: 1/2, step 4889/7134 completed (loss: 0.06571114808320999, acc: 0.9732620120048523)
[2025-02-13 19:57:49,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:50,145][root][INFO] - Training Epoch: 1/2, step 4890/7134 completed (loss: 0.2565942108631134, acc: 0.9433962106704712)
[2025-02-13 19:57:50,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:50,549][root][INFO] - Training Epoch: 1/2, step 4891/7134 completed (loss: 0.15390372276306152, acc: 0.954081654548645)
[2025-02-13 19:57:50,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:50,924][root][INFO] - Training Epoch: 1/2, step 4892/7134 completed (loss: 0.11930832266807556, acc: 0.9805194735527039)
[2025-02-13 19:57:51,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:51,272][root][INFO] - Training Epoch: 1/2, step 4893/7134 completed (loss: 0.6307779550552368, acc: 0.8534482717514038)
[2025-02-13 19:57:51,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:51,628][root][INFO] - Training Epoch: 1/2, step 4894/7134 completed (loss: 0.08500014245510101, acc: 0.9882352948188782)
[2025-02-13 19:57:51,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:52,023][root][INFO] - Training Epoch: 1/2, step 4895/7134 completed (loss: 0.08998271077871323, acc: 0.9744898080825806)
[2025-02-13 19:57:52,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:52,374][root][INFO] - Training Epoch: 1/2, step 4896/7134 completed (loss: 0.07021967321634293, acc: 0.9888268113136292)
[2025-02-13 19:57:52,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:52,738][root][INFO] - Training Epoch: 1/2, step 4897/7134 completed (loss: 0.10750710219144821, acc: 0.9802955389022827)
[2025-02-13 19:57:52,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:53,108][root][INFO] - Training Epoch: 1/2, step 4898/7134 completed (loss: 0.11455881595611572, acc: 0.9731183052062988)
[2025-02-13 19:57:53,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:53,471][root][INFO] - Training Epoch: 1/2, step 4899/7134 completed (loss: 0.09884825348854065, acc: 0.9756097793579102)
[2025-02-13 19:57:53,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:53,837][root][INFO] - Training Epoch: 1/2, step 4900/7134 completed (loss: 0.15718168020248413, acc: 0.9893048405647278)
[2025-02-13 19:57:53,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:54,211][root][INFO] - Training Epoch: 1/2, step 4901/7134 completed (loss: 0.10167281329631805, acc: 0.9756097793579102)
[2025-02-13 19:57:54,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:54,590][root][INFO] - Training Epoch: 1/2, step 4902/7134 completed (loss: 0.06946192681789398, acc: 0.9743589758872986)
[2025-02-13 19:57:54,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:54,953][root][INFO] - Training Epoch: 1/2, step 4903/7134 completed (loss: 0.06468641757965088, acc: 0.985401451587677)
[2025-02-13 19:57:55,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:55,317][root][INFO] - Training Epoch: 1/2, step 4904/7134 completed (loss: 0.05892670527100563, acc: 0.9823529124259949)
[2025-02-13 19:57:55,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:55,666][root][INFO] - Training Epoch: 1/2, step 4905/7134 completed (loss: 0.03402550145983696, acc: 0.9934640526771545)
[2025-02-13 19:57:55,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:56,041][root][INFO] - Training Epoch: 1/2, step 4906/7134 completed (loss: 0.04761166125535965, acc: 0.9849624037742615)
[2025-02-13 19:57:56,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:56,406][root][INFO] - Training Epoch: 1/2, step 4907/7134 completed (loss: 0.0756358727812767, acc: 0.9795918464660645)
[2025-02-13 19:57:56,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:56,786][root][INFO] - Training Epoch: 1/2, step 4908/7134 completed (loss: 0.04193859547376633, acc: 0.9870129823684692)
[2025-02-13 19:57:56,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:57,160][root][INFO] - Training Epoch: 1/2, step 4909/7134 completed (loss: 0.03571052476763725, acc: 0.9932885766029358)
[2025-02-13 19:57:57,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:57,517][root][INFO] - Training Epoch: 1/2, step 4910/7134 completed (loss: 0.14049704372882843, acc: 0.9612902998924255)
[2025-02-13 19:57:57,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:57,883][root][INFO] - Training Epoch: 1/2, step 4911/7134 completed (loss: 0.06738176941871643, acc: 0.9839572310447693)
[2025-02-13 19:57:58,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:58,242][root][INFO] - Training Epoch: 1/2, step 4912/7134 completed (loss: 0.05426141619682312, acc: 0.9900497794151306)
[2025-02-13 19:57:58,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:58,643][root][INFO] - Training Epoch: 1/2, step 4913/7134 completed (loss: 0.21017687022686005, acc: 0.9510489702224731)
[2025-02-13 19:57:58,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:59,014][root][INFO] - Training Epoch: 1/2, step 4914/7134 completed (loss: 0.10369540750980377, acc: 0.9741935729980469)
[2025-02-13 19:57:59,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:59,405][root][INFO] - Training Epoch: 1/2, step 4915/7134 completed (loss: 0.017799004912376404, acc: 0.9938271641731262)
[2025-02-13 19:57:59,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:59,840][root][INFO] - Training Epoch: 1/2, step 4916/7134 completed (loss: 0.21959446370601654, acc: 0.945652186870575)
[2025-02-13 19:57:59,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:00,199][root][INFO] - Training Epoch: 1/2, step 4917/7134 completed (loss: 0.20931097865104675, acc: 0.9554139971733093)
[2025-02-13 19:58:00,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:00,553][root][INFO] - Training Epoch: 1/2, step 4918/7134 completed (loss: 0.028854258358478546, acc: 0.9938271641731262)
[2025-02-13 19:58:00,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:00,979][root][INFO] - Training Epoch: 1/2, step 4919/7134 completed (loss: 0.15751095116138458, acc: 0.9717513918876648)
[2025-02-13 19:58:01,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:01,339][root][INFO] - Training Epoch: 1/2, step 4920/7134 completed (loss: 0.20184963941574097, acc: 0.9316770434379578)
[2025-02-13 19:58:01,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:01,707][root][INFO] - Training Epoch: 1/2, step 4921/7134 completed (loss: 0.1754634976387024, acc: 0.9635036587715149)
[2025-02-13 19:58:01,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:02,121][root][INFO] - Training Epoch: 1/2, step 4922/7134 completed (loss: 0.10901716351509094, acc: 0.9751552939414978)
[2025-02-13 19:58:02,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:02,496][root][INFO] - Training Epoch: 1/2, step 4923/7134 completed (loss: 0.0635799989104271, acc: 0.97826087474823)
[2025-02-13 19:58:02,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:02,856][root][INFO] - Training Epoch: 1/2, step 4924/7134 completed (loss: 0.1097131073474884, acc: 0.9708737730979919)
[2025-02-13 19:58:02,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:03,191][root][INFO] - Training Epoch: 1/2, step 4925/7134 completed (loss: 0.16550731658935547, acc: 0.9463087320327759)
[2025-02-13 19:58:03,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:03,548][root][INFO] - Training Epoch: 1/2, step 4926/7134 completed (loss: 0.03219818323850632, acc: 0.9932885766029358)
[2025-02-13 19:58:03,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:03,927][root][INFO] - Training Epoch: 1/2, step 4927/7134 completed (loss: 0.15919385850429535, acc: 0.9806451797485352)
[2025-02-13 19:58:04,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:04,312][root][INFO] - Training Epoch: 1/2, step 4928/7134 completed (loss: 0.06265993416309357, acc: 0.9842105507850647)
[2025-02-13 19:58:04,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:04,699][root][INFO] - Training Epoch: 1/2, step 4929/7134 completed (loss: 0.045143093913793564, acc: 0.9934210777282715)
[2025-02-13 19:58:04,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:05,077][root][INFO] - Training Epoch: 1/2, step 4930/7134 completed (loss: 0.09310922026634216, acc: 0.9729729890823364)
[2025-02-13 19:58:05,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:05,485][root][INFO] - Training Epoch: 1/2, step 4931/7134 completed (loss: 0.1229526549577713, acc: 0.9570552110671997)
[2025-02-13 19:58:05,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:05,859][root][INFO] - Training Epoch: 1/2, step 4932/7134 completed (loss: 0.05751057341694832, acc: 0.9882352948188782)
[2025-02-13 19:58:05,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:06,226][root][INFO] - Training Epoch: 1/2, step 4933/7134 completed (loss: 0.14807823300361633, acc: 0.963350772857666)
[2025-02-13 19:58:06,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:06,576][root][INFO] - Training Epoch: 1/2, step 4934/7134 completed (loss: 0.09414060413837433, acc: 0.9764705896377563)
[2025-02-13 19:58:06,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:06,959][root][INFO] - Training Epoch: 1/2, step 4935/7134 completed (loss: 0.0783323273062706, acc: 0.970588207244873)
[2025-02-13 19:58:07,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:07,346][root][INFO] - Training Epoch: 1/2, step 4936/7134 completed (loss: 0.03644367679953575, acc: 0.9931972622871399)
[2025-02-13 19:58:07,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:07,721][root][INFO] - Training Epoch: 1/2, step 4937/7134 completed (loss: 0.15852542221546173, acc: 0.9580419659614563)
[2025-02-13 19:58:07,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:08,073][root][INFO] - Training Epoch: 1/2, step 4938/7134 completed (loss: 0.026584722101688385, acc: 0.9938271641731262)
[2025-02-13 19:58:08,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:08,420][root][INFO] - Training Epoch: 1/2, step 4939/7134 completed (loss: 0.13032293319702148, acc: 0.9714285731315613)
[2025-02-13 19:58:08,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:08,798][root][INFO] - Training Epoch: 1/2, step 4940/7134 completed (loss: 0.18031233549118042, acc: 0.9689922332763672)
[2025-02-13 19:58:08,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:09,184][root][INFO] - Training Epoch: 1/2, step 4941/7134 completed (loss: 0.1484922617673874, acc: 0.976047933101654)
[2025-02-13 19:58:09,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:09,543][root][INFO] - Training Epoch: 1/2, step 4942/7134 completed (loss: 0.1571587473154068, acc: 0.949367105960846)
[2025-02-13 19:58:09,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:09,932][root][INFO] - Training Epoch: 1/2, step 4943/7134 completed (loss: 0.15499648451805115, acc: 0.956250011920929)
[2025-02-13 19:58:10,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:10,317][root][INFO] - Training Epoch: 1/2, step 4944/7134 completed (loss: 0.2920845150947571, acc: 0.9411764740943909)
[2025-02-13 19:58:10,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:10,704][root][INFO] - Training Epoch: 1/2, step 4945/7134 completed (loss: 0.281399130821228, acc: 0.939393937587738)
[2025-02-13 19:58:10,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:11,098][root][INFO] - Training Epoch: 1/2, step 4946/7134 completed (loss: 0.24365359544754028, acc: 0.9375)
[2025-02-13 19:58:11,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:11,452][root][INFO] - Training Epoch: 1/2, step 4947/7134 completed (loss: 0.2030511498451233, acc: 0.9652174115180969)
[2025-02-13 19:58:11,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:11,807][root][INFO] - Training Epoch: 1/2, step 4948/7134 completed (loss: 0.2831878066062927, acc: 0.9523809552192688)
[2025-02-13 19:58:11,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:12,192][root][INFO] - Training Epoch: 1/2, step 4949/7134 completed (loss: 0.049964599311351776, acc: 0.9876543283462524)
[2025-02-13 19:58:12,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:12,584][root][INFO] - Training Epoch: 1/2, step 4950/7134 completed (loss: 0.054084789007902145, acc: 0.9795918464660645)
[2025-02-13 19:58:12,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:12,959][root][INFO] - Training Epoch: 1/2, step 4951/7134 completed (loss: 0.1565481424331665, acc: 0.966292142868042)
[2025-02-13 19:58:13,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:13,364][root][INFO] - Training Epoch: 1/2, step 4952/7134 completed (loss: 0.08883029967546463, acc: 0.9662162065505981)
[2025-02-13 19:58:13,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:13,736][root][INFO] - Training Epoch: 1/2, step 4953/7134 completed (loss: 0.15439672768115997, acc: 0.9369369149208069)
[2025-02-13 19:58:13,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:14,107][root][INFO] - Training Epoch: 1/2, step 4954/7134 completed (loss: 0.11105083674192429, acc: 0.9795918464660645)
[2025-02-13 19:58:14,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:14,447][root][INFO] - Training Epoch: 1/2, step 4955/7134 completed (loss: 0.15617604553699493, acc: 0.9583333134651184)
[2025-02-13 19:58:14,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:14,805][root][INFO] - Training Epoch: 1/2, step 4956/7134 completed (loss: 0.25408729910850525, acc: 0.9529411792755127)
[2025-02-13 19:58:14,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:15,135][root][INFO] - Training Epoch: 1/2, step 4957/7134 completed (loss: 0.16983018815517426, acc: 0.957446813583374)
[2025-02-13 19:58:15,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:15,511][root][INFO] - Training Epoch: 1/2, step 4958/7134 completed (loss: 0.13900943100452423, acc: 0.9496855139732361)
[2025-02-13 19:58:15,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:15,895][root][INFO] - Training Epoch: 1/2, step 4959/7134 completed (loss: 0.18553364276885986, acc: 0.9698795080184937)
[2025-02-13 19:58:16,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:16,264][root][INFO] - Training Epoch: 1/2, step 4960/7134 completed (loss: 0.17063525319099426, acc: 0.9558823704719543)
[2025-02-13 19:58:16,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:16,604][root][INFO] - Training Epoch: 1/2, step 4961/7134 completed (loss: 0.14836199581623077, acc: 0.96875)
[2025-02-13 19:58:16,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:16,938][root][INFO] - Training Epoch: 1/2, step 4962/7134 completed (loss: 0.08083128929138184, acc: 0.9817073345184326)
[2025-02-13 19:58:17,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:17,304][root][INFO] - Training Epoch: 1/2, step 4963/7134 completed (loss: 0.06732052564620972, acc: 0.9806451797485352)
[2025-02-13 19:58:17,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:17,643][root][INFO] - Training Epoch: 1/2, step 4964/7134 completed (loss: 0.21902258694171906, acc: 0.935251772403717)
[2025-02-13 19:58:17,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:18,011][root][INFO] - Training Epoch: 1/2, step 4965/7134 completed (loss: 0.03908804431557655, acc: 1.0)
[2025-02-13 19:58:18,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:18,381][root][INFO] - Training Epoch: 1/2, step 4966/7134 completed (loss: 0.22418871521949768, acc: 0.9450549483299255)
[2025-02-13 19:58:18,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:18,700][root][INFO] - Training Epoch: 1/2, step 4967/7134 completed (loss: 0.11836491525173187, acc: 0.9844961166381836)
[2025-02-13 19:58:18,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:19,069][root][INFO] - Training Epoch: 1/2, step 4968/7134 completed (loss: 0.3877868950366974, acc: 0.8936170339584351)
[2025-02-13 19:58:19,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:19,465][root][INFO] - Training Epoch: 1/2, step 4969/7134 completed (loss: 0.15805278718471527, acc: 0.9586206674575806)
[2025-02-13 19:58:19,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:19,828][root][INFO] - Training Epoch: 1/2, step 4970/7134 completed (loss: 0.09085836261510849, acc: 0.9793103337287903)
[2025-02-13 19:58:19,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:20,187][root][INFO] - Training Epoch: 1/2, step 4971/7134 completed (loss: 0.06904102861881256, acc: 0.9735099077224731)
[2025-02-13 19:58:20,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:20,562][root][INFO] - Training Epoch: 1/2, step 4972/7134 completed (loss: 0.15956731140613556, acc: 0.949999988079071)
[2025-02-13 19:58:20,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:20,936][root][INFO] - Training Epoch: 1/2, step 4973/7134 completed (loss: 0.09655551612377167, acc: 0.9772727489471436)
[2025-02-13 19:58:21,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:21,333][root][INFO] - Training Epoch: 1/2, step 4974/7134 completed (loss: 0.19120344519615173, acc: 0.9469696879386902)
[2025-02-13 19:58:21,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:21,680][root][INFO] - Training Epoch: 1/2, step 4975/7134 completed (loss: 0.34603801369667053, acc: 0.9241379499435425)
[2025-02-13 19:58:21,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:22,015][root][INFO] - Training Epoch: 1/2, step 4976/7134 completed (loss: 0.26633644104003906, acc: 0.9473684430122375)
[2025-02-13 19:58:22,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:22,390][root][INFO] - Training Epoch: 1/2, step 4977/7134 completed (loss: 0.27554288506507874, acc: 0.9514563083648682)
[2025-02-13 19:58:22,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:22,745][root][INFO] - Training Epoch: 1/2, step 4978/7134 completed (loss: 0.07655280083417892, acc: 0.9885057210922241)
[2025-02-13 19:58:22,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:23,058][root][INFO] - Training Epoch: 1/2, step 4979/7134 completed (loss: 0.3699915409088135, acc: 0.9300699234008789)
[2025-02-13 19:58:23,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:23,424][root][INFO] - Training Epoch: 1/2, step 4980/7134 completed (loss: 0.284473717212677, acc: 0.949999988079071)
[2025-02-13 19:58:23,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:23,799][root][INFO] - Training Epoch: 1/2, step 4981/7134 completed (loss: 0.19590376317501068, acc: 0.9571428298950195)
[2025-02-13 19:58:23,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:24,187][root][INFO] - Training Epoch: 1/2, step 4982/7134 completed (loss: 0.18504253029823303, acc: 0.9639639854431152)
[2025-02-13 19:58:24,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:24,602][root][INFO] - Training Epoch: 1/2, step 4983/7134 completed (loss: 0.14810830354690552, acc: 0.9784172773361206)
[2025-02-13 19:58:24,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:24,980][root][INFO] - Training Epoch: 1/2, step 4984/7134 completed (loss: 0.25321564078330994, acc: 0.9811320900917053)
[2025-02-13 19:58:25,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:25,333][root][INFO] - Training Epoch: 1/2, step 4985/7134 completed (loss: 0.17766359448432922, acc: 0.9729729890823364)
[2025-02-13 19:58:25,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:25,687][root][INFO] - Training Epoch: 1/2, step 4986/7134 completed (loss: 0.13851165771484375, acc: 0.9720279574394226)
[2025-02-13 19:58:25,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:26,035][root][INFO] - Training Epoch: 1/2, step 4987/7134 completed (loss: 0.06453467905521393, acc: 0.9716981053352356)
[2025-02-13 19:58:26,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:26,388][root][INFO] - Training Epoch: 1/2, step 4988/7134 completed (loss: 0.08287081122398376, acc: 0.982758641242981)
[2025-02-13 19:58:26,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:26,735][root][INFO] - Training Epoch: 1/2, step 4989/7134 completed (loss: 0.07335344702005386, acc: 0.9764705896377563)
[2025-02-13 19:58:26,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:27,090][root][INFO] - Training Epoch: 1/2, step 4990/7134 completed (loss: 0.059810202568769455, acc: 0.9829059839248657)
[2025-02-13 19:58:27,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:27,443][root][INFO] - Training Epoch: 1/2, step 4991/7134 completed (loss: 0.06150442734360695, acc: 0.969924807548523)
[2025-02-13 19:58:27,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:27,816][root][INFO] - Training Epoch: 1/2, step 4992/7134 completed (loss: 0.16826887428760529, acc: 0.9741379022598267)
[2025-02-13 19:58:27,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:28,150][root][INFO] - Training Epoch: 1/2, step 4993/7134 completed (loss: 0.05956922098994255, acc: 0.9800000190734863)
[2025-02-13 19:58:28,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:28,534][root][INFO] - Training Epoch: 1/2, step 4994/7134 completed (loss: 0.07392367720603943, acc: 0.9867549538612366)
[2025-02-13 19:58:28,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:28,860][root][INFO] - Training Epoch: 1/2, step 4995/7134 completed (loss: 0.17884086072444916, acc: 0.9629629850387573)
[2025-02-13 19:58:28,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:29,214][root][INFO] - Training Epoch: 1/2, step 4996/7134 completed (loss: 0.09081749618053436, acc: 0.9750000238418579)
[2025-02-13 19:58:29,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:29,588][root][INFO] - Training Epoch: 1/2, step 4997/7134 completed (loss: 0.05972765386104584, acc: 0.9923664331436157)
[2025-02-13 19:58:29,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:29,916][root][INFO] - Training Epoch: 1/2, step 4998/7134 completed (loss: 0.14313016831874847, acc: 0.9733333587646484)
[2025-02-13 19:58:30,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:30,292][root][INFO] - Training Epoch: 1/2, step 4999/7134 completed (loss: 0.14933954179286957, acc: 0.960629940032959)
[2025-02-13 19:58:30,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:30,699][root][INFO] - Training Epoch: 1/2, step 5000/7134 completed (loss: 0.14365419745445251, acc: 0.969072163105011)
[2025-02-13 19:58:30,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:31,077][root][INFO] - Training Epoch: 1/2, step 5001/7134 completed (loss: 0.2294548898935318, acc: 0.9659863710403442)
[2025-02-13 19:58:31,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:31,446][root][INFO] - Training Epoch: 1/2, step 5002/7134 completed (loss: 0.09714356809854507, acc: 0.9734513163566589)
[2025-02-13 19:58:31,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:31,849][root][INFO] - Training Epoch: 1/2, step 5003/7134 completed (loss: 0.06792740523815155, acc: 0.9851852059364319)
[2025-02-13 19:58:32,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:32,261][root][INFO] - Training Epoch: 1/2, step 5004/7134 completed (loss: 0.07205339521169662, acc: 0.9813084006309509)
[2025-02-13 19:58:32,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:32,652][root][INFO] - Training Epoch: 1/2, step 5005/7134 completed (loss: 0.1577761471271515, acc: 0.9780219793319702)
[2025-02-13 19:58:32,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:33,062][root][INFO] - Training Epoch: 1/2, step 5006/7134 completed (loss: 0.1230364590883255, acc: 0.9802631735801697)
[2025-02-13 19:58:33,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:33,436][root][INFO] - Training Epoch: 1/2, step 5007/7134 completed (loss: 0.06912581622600555, acc: 0.9876543283462524)
[2025-02-13 19:58:33,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:33,839][root][INFO] - Training Epoch: 1/2, step 5008/7134 completed (loss: 0.18253889679908752, acc: 0.9833333492279053)
[2025-02-13 19:58:34,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:34,291][root][INFO] - Training Epoch: 1/2, step 5009/7134 completed (loss: 0.2593044936656952, acc: 0.9484536051750183)
[2025-02-13 19:58:34,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:34,665][root][INFO] - Training Epoch: 1/2, step 5010/7134 completed (loss: 0.1635650247335434, acc: 0.9658119678497314)
[2025-02-13 19:58:34,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:35,073][root][INFO] - Training Epoch: 1/2, step 5011/7134 completed (loss: 0.19674979150295258, acc: 0.931034505367279)
[2025-02-13 19:58:35,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:35,390][root][INFO] - Training Epoch: 1/2, step 5012/7134 completed (loss: 0.05736524611711502, acc: 1.0)
[2025-02-13 19:58:35,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:35,733][root][INFO] - Training Epoch: 1/2, step 5013/7134 completed (loss: 0.05781852453947067, acc: 0.9914529919624329)
[2025-02-13 19:58:35,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:36,132][root][INFO] - Training Epoch: 1/2, step 5014/7134 completed (loss: 0.13203126192092896, acc: 0.9711538553237915)
[2025-02-13 19:58:36,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:36,507][root][INFO] - Training Epoch: 1/2, step 5015/7134 completed (loss: 0.2863108515739441, acc: 0.9386503100395203)
[2025-02-13 19:58:36,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:36,866][root][INFO] - Training Epoch: 1/2, step 5016/7134 completed (loss: 0.41931816935539246, acc: 0.9083969593048096)
[2025-02-13 19:58:36,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:37,231][root][INFO] - Training Epoch: 1/2, step 5017/7134 completed (loss: 0.11421225965023041, acc: 0.9848484992980957)
[2025-02-13 19:58:37,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:37,606][root][INFO] - Training Epoch: 1/2, step 5018/7134 completed (loss: 0.23230215907096863, acc: 0.9398496150970459)
[2025-02-13 19:58:37,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:38,017][root][INFO] - Training Epoch: 1/2, step 5019/7134 completed (loss: 0.40857580304145813, acc: 0.909604549407959)
[2025-02-13 19:58:38,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:38,415][root][INFO] - Training Epoch: 1/2, step 5020/7134 completed (loss: 0.41017767786979675, acc: 0.9047619104385376)
[2025-02-13 19:58:38,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:38,779][root][INFO] - Training Epoch: 1/2, step 5021/7134 completed (loss: 0.11786273866891861, acc: 0.9904761910438538)
[2025-02-13 19:58:38,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:39,152][root][INFO] - Training Epoch: 1/2, step 5022/7134 completed (loss: 0.0839160904288292, acc: 0.9927536249160767)
[2025-02-13 19:58:39,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:39,529][root][INFO] - Training Epoch: 1/2, step 5023/7134 completed (loss: 0.10220909118652344, acc: 0.9741935729980469)
[2025-02-13 19:58:39,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:39,900][root][INFO] - Training Epoch: 1/2, step 5024/7134 completed (loss: 0.08571074903011322, acc: 0.9638554453849792)
[2025-02-13 19:58:40,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:40,268][root][INFO] - Training Epoch: 1/2, step 5025/7134 completed (loss: 0.14622579514980316, acc: 0.9692307710647583)
[2025-02-13 19:58:40,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:40,622][root][INFO] - Training Epoch: 1/2, step 5026/7134 completed (loss: 0.13558663427829742, acc: 0.9541984796524048)
[2025-02-13 19:58:40,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:40,976][root][INFO] - Training Epoch: 1/2, step 5027/7134 completed (loss: 0.08459221571683884, acc: 0.9844961166381836)
[2025-02-13 19:58:41,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:41,336][root][INFO] - Training Epoch: 1/2, step 5028/7134 completed (loss: 0.10858307778835297, acc: 0.9788732528686523)
[2025-02-13 19:58:41,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:41,722][root][INFO] - Training Epoch: 1/2, step 5029/7134 completed (loss: 0.0660259798169136, acc: 0.9846153855323792)
[2025-02-13 19:58:41,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:42,113][root][INFO] - Training Epoch: 1/2, step 5030/7134 completed (loss: 0.1212184801697731, acc: 0.9640718698501587)
[2025-02-13 19:58:42,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:42,492][root][INFO] - Training Epoch: 1/2, step 5031/7134 completed (loss: 0.3068428337574005, acc: 0.9347826242446899)
[2025-02-13 19:58:42,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:42,877][root][INFO] - Training Epoch: 1/2, step 5032/7134 completed (loss: 0.08175364136695862, acc: 0.98591548204422)
[2025-02-13 19:58:43,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:43,255][root][INFO] - Training Epoch: 1/2, step 5033/7134 completed (loss: 0.05966111272573471, acc: 0.98591548204422)
[2025-02-13 19:58:43,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:43,608][root][INFO] - Training Epoch: 1/2, step 5034/7134 completed (loss: 0.06772526353597641, acc: 0.9774436354637146)
[2025-02-13 19:58:43,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:43,971][root][INFO] - Training Epoch: 1/2, step 5035/7134 completed (loss: 0.011186350136995316, acc: 1.0)
[2025-02-13 19:58:44,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:44,343][root][INFO] - Training Epoch: 1/2, step 5036/7134 completed (loss: 0.22601741552352905, acc: 0.9519230723381042)
[2025-02-13 19:58:44,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:44,719][root][INFO] - Training Epoch: 1/2, step 5037/7134 completed (loss: 0.17198945581912994, acc: 0.9588235020637512)
[2025-02-13 19:58:44,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:45,058][root][INFO] - Training Epoch: 1/2, step 5038/7134 completed (loss: 0.42069607973098755, acc: 0.886956512928009)
[2025-02-13 19:58:45,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:45,418][root][INFO] - Training Epoch: 1/2, step 5039/7134 completed (loss: 0.12006081640720367, acc: 0.9503546357154846)
[2025-02-13 19:58:45,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:45,844][root][INFO] - Training Epoch: 1/2, step 5040/7134 completed (loss: 0.112767793238163, acc: 0.9666666388511658)
[2025-02-13 19:58:45,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:46,220][root][INFO] - Training Epoch: 1/2, step 5041/7134 completed (loss: 0.02426530234515667, acc: 1.0)
[2025-02-13 19:58:46,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:46,631][root][INFO] - Training Epoch: 1/2, step 5042/7134 completed (loss: 0.13304711878299713, acc: 0.9708737730979919)
[2025-02-13 19:58:46,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:47,010][root][INFO] - Training Epoch: 1/2, step 5043/7134 completed (loss: 0.10956812649965286, acc: 0.9646017551422119)
[2025-02-13 19:58:47,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:47,432][root][INFO] - Training Epoch: 1/2, step 5044/7134 completed (loss: 0.12407351285219193, acc: 0.9681528806686401)
[2025-02-13 19:58:47,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:47,890][root][INFO] - Training Epoch: 1/2, step 5045/7134 completed (loss: 0.17547136545181274, acc: 0.9621621370315552)
[2025-02-13 19:58:48,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:48,246][root][INFO] - Training Epoch: 1/2, step 5046/7134 completed (loss: 0.2236550748348236, acc: 0.9435483813285828)
[2025-02-13 19:58:48,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:48,670][root][INFO] - Training Epoch: 1/2, step 5047/7134 completed (loss: 0.19235026836395264, acc: 0.9635036587715149)
[2025-02-13 19:58:48,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:49,012][root][INFO] - Training Epoch: 1/2, step 5048/7134 completed (loss: 0.15744544565677643, acc: 0.9642857313156128)
[2025-02-13 19:58:49,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:49,364][root][INFO] - Training Epoch: 1/2, step 5049/7134 completed (loss: 0.20087577402591705, acc: 0.9490445852279663)
[2025-02-13 19:58:49,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:49,764][root][INFO] - Training Epoch: 1/2, step 5050/7134 completed (loss: 0.22781483829021454, acc: 0.949999988079071)
[2025-02-13 19:58:49,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:50,155][root][INFO] - Training Epoch: 1/2, step 5051/7134 completed (loss: 0.22257378697395325, acc: 0.9523809552192688)
[2025-02-13 19:58:50,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:50,553][root][INFO] - Training Epoch: 1/2, step 5052/7134 completed (loss: 0.1991640329360962, acc: 0.9551281929016113)
[2025-02-13 19:58:50,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:50,918][root][INFO] - Training Epoch: 1/2, step 5053/7134 completed (loss: 0.3355085551738739, acc: 0.9083333611488342)
[2025-02-13 19:58:51,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:51,274][root][INFO] - Training Epoch: 1/2, step 5054/7134 completed (loss: 0.27646276354789734, acc: 0.9333333373069763)
[2025-02-13 19:58:51,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:51,609][root][INFO] - Training Epoch: 1/2, step 5055/7134 completed (loss: 0.1937256157398224, acc: 0.9469026327133179)
[2025-02-13 19:58:51,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:52,006][root][INFO] - Training Epoch: 1/2, step 5056/7134 completed (loss: 0.1985546350479126, acc: 0.9530201554298401)
[2025-02-13 19:58:52,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:52,390][root][INFO] - Training Epoch: 1/2, step 5057/7134 completed (loss: 0.29834887385368347, acc: 0.9426751732826233)
[2025-02-13 19:58:52,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:52,750][root][INFO] - Training Epoch: 1/2, step 5058/7134 completed (loss: 0.16609206795692444, acc: 0.9512194991111755)
[2025-02-13 19:58:52,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:53,154][root][INFO] - Training Epoch: 1/2, step 5059/7134 completed (loss: 0.1350162923336029, acc: 0.9580838084220886)
[2025-02-13 19:58:53,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:53,529][root][INFO] - Training Epoch: 1/2, step 5060/7134 completed (loss: 0.17649739980697632, acc: 0.9733333587646484)
[2025-02-13 19:58:53,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:53,865][root][INFO] - Training Epoch: 1/2, step 5061/7134 completed (loss: 0.2846478819847107, acc: 0.9428571462631226)
[2025-02-13 19:58:54,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:54,250][root][INFO] - Training Epoch: 1/2, step 5062/7134 completed (loss: 0.11600451916456223, acc: 0.9629629850387573)
[2025-02-13 19:58:54,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:54,637][root][INFO] - Training Epoch: 1/2, step 5063/7134 completed (loss: 0.18551285564899445, acc: 0.9510489702224731)
[2025-02-13 19:58:54,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:55,008][root][INFO] - Training Epoch: 1/2, step 5064/7134 completed (loss: 0.12548740208148956, acc: 0.9642857313156128)
[2025-02-13 19:58:55,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:55,382][root][INFO] - Training Epoch: 1/2, step 5065/7134 completed (loss: 0.18932265043258667, acc: 0.9622641801834106)
[2025-02-13 19:58:55,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:55,740][root][INFO] - Training Epoch: 1/2, step 5066/7134 completed (loss: 0.16257327795028687, acc: 0.9448275566101074)
[2025-02-13 19:58:55,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:56,088][root][INFO] - Training Epoch: 1/2, step 5067/7134 completed (loss: 0.03985201194882393, acc: 1.0)
[2025-02-13 19:58:56,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:56,463][root][INFO] - Training Epoch: 1/2, step 5068/7134 completed (loss: 0.19457916915416718, acc: 0.9507042169570923)
[2025-02-13 19:58:56,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:56,836][root][INFO] - Training Epoch: 1/2, step 5069/7134 completed (loss: 0.0967140644788742, acc: 0.9729729890823364)
[2025-02-13 19:58:56,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:57,184][root][INFO] - Training Epoch: 1/2, step 5070/7134 completed (loss: 0.026703527197241783, acc: 1.0)
[2025-02-13 19:58:57,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:57,547][root][INFO] - Training Epoch: 1/2, step 5071/7134 completed (loss: 0.09962230175733566, acc: 0.9764705896377563)
[2025-02-13 19:58:57,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:57,905][root][INFO] - Training Epoch: 1/2, step 5072/7134 completed (loss: 0.2365829348564148, acc: 0.9450549483299255)
[2025-02-13 19:58:58,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:58,292][root][INFO] - Training Epoch: 1/2, step 5073/7134 completed (loss: 0.14953075349330902, acc: 0.9541984796524048)
[2025-02-13 19:58:58,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:58,735][root][INFO] - Training Epoch: 1/2, step 5074/7134 completed (loss: 0.21327389776706696, acc: 0.9457364082336426)
[2025-02-13 19:58:58,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:59,143][root][INFO] - Training Epoch: 1/2, step 5075/7134 completed (loss: 0.18541941046714783, acc: 0.95652174949646)
[2025-02-13 19:58:59,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:59,525][root][INFO] - Training Epoch: 1/2, step 5076/7134 completed (loss: 0.14506877958774567, acc: 0.9462365508079529)
[2025-02-13 19:58:59,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:59,889][root][INFO] - Training Epoch: 1/2, step 5077/7134 completed (loss: 0.0920478031039238, acc: 0.970588207244873)
[2025-02-13 19:59:00,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:00,258][root][INFO] - Training Epoch: 1/2, step 5078/7134 completed (loss: 0.3323652148246765, acc: 0.9333333373069763)
[2025-02-13 19:59:00,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:00,657][root][INFO] - Training Epoch: 1/2, step 5079/7134 completed (loss: 0.14303290843963623, acc: 0.9666666388511658)
[2025-02-13 19:59:00,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:01,032][root][INFO] - Training Epoch: 1/2, step 5080/7134 completed (loss: 0.05629601329565048, acc: 0.9803921580314636)
[2025-02-13 19:59:01,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:01,390][root][INFO] - Training Epoch: 1/2, step 5081/7134 completed (loss: 0.08693845570087433, acc: 0.9653179049491882)
[2025-02-13 19:59:01,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:01,765][root][INFO] - Training Epoch: 1/2, step 5082/7134 completed (loss: 0.11148037016391754, acc: 0.9793103337287903)
[2025-02-13 19:59:01,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:02,136][root][INFO] - Training Epoch: 1/2, step 5083/7134 completed (loss: 0.18360236287117004, acc: 0.9644970297813416)
[2025-02-13 19:59:02,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:02,514][root][INFO] - Training Epoch: 1/2, step 5084/7134 completed (loss: 0.135285422205925, acc: 0.9653465151786804)
[2025-02-13 19:59:02,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:02,902][root][INFO] - Training Epoch: 1/2, step 5085/7134 completed (loss: 0.1358935683965683, acc: 0.969072163105011)
[2025-02-13 19:59:03,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:03,273][root][INFO] - Training Epoch: 1/2, step 5086/7134 completed (loss: 0.09973353892564774, acc: 0.9668508172035217)
[2025-02-13 19:59:03,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:03,641][root][INFO] - Training Epoch: 1/2, step 5087/7134 completed (loss: 0.063070148229599, acc: 0.9837837815284729)
[2025-02-13 19:59:03,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:04,023][root][INFO] - Training Epoch: 1/2, step 5088/7134 completed (loss: 0.023820316419005394, acc: 1.0)
[2025-02-13 19:59:04,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:04,420][root][INFO] - Training Epoch: 1/2, step 5089/7134 completed (loss: 0.08180397748947144, acc: 0.9897959232330322)
[2025-02-13 19:59:04,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:04,787][root][INFO] - Training Epoch: 1/2, step 5090/7134 completed (loss: 0.03866216167807579, acc: 0.9902912378311157)
[2025-02-13 19:59:04,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:05,154][root][INFO] - Training Epoch: 1/2, step 5091/7134 completed (loss: 0.0985216274857521, acc: 0.9615384340286255)
[2025-02-13 19:59:05,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:05,518][root][INFO] - Training Epoch: 1/2, step 5092/7134 completed (loss: 0.1247665286064148, acc: 0.9728260636329651)
[2025-02-13 19:59:05,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:05,931][root][INFO] - Training Epoch: 1/2, step 5093/7134 completed (loss: 0.109598807990551, acc: 0.9659090638160706)
[2025-02-13 19:59:06,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:06,321][root][INFO] - Training Epoch: 1/2, step 5094/7134 completed (loss: 0.08140664547681808, acc: 0.9923076629638672)
[2025-02-13 19:59:06,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:06,699][root][INFO] - Training Epoch: 1/2, step 5095/7134 completed (loss: 0.2778457701206207, acc: 0.9299362897872925)
[2025-02-13 19:59:06,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:07,086][root][INFO] - Training Epoch: 1/2, step 5096/7134 completed (loss: 0.1916474550962448, acc: 0.9414893388748169)
[2025-02-13 19:59:07,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:07,468][root][INFO] - Training Epoch: 1/2, step 5097/7134 completed (loss: 0.14704324305057526, acc: 0.9679144620895386)
[2025-02-13 19:59:07,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:07,836][root][INFO] - Training Epoch: 1/2, step 5098/7134 completed (loss: 0.17559412121772766, acc: 0.9489796161651611)
[2025-02-13 19:59:07,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:08,215][root][INFO] - Training Epoch: 1/2, step 5099/7134 completed (loss: 0.155069962143898, acc: 0.9629629850387573)
[2025-02-13 19:59:08,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:08,589][root][INFO] - Training Epoch: 1/2, step 5100/7134 completed (loss: 0.10816076397895813, acc: 0.9607843160629272)
[2025-02-13 19:59:08,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:08,995][root][INFO] - Training Epoch: 1/2, step 5101/7134 completed (loss: 0.07161920517683029, acc: 0.9738219976425171)
[2025-02-13 19:59:09,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:09,354][root][INFO] - Training Epoch: 1/2, step 5102/7134 completed (loss: 0.16104881465435028, acc: 0.9512194991111755)
[2025-02-13 19:59:09,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:09,729][root][INFO] - Training Epoch: 1/2, step 5103/7134 completed (loss: 0.11077211797237396, acc: 0.976190447807312)
[2025-02-13 19:59:09,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:10,093][root][INFO] - Training Epoch: 1/2, step 5104/7134 completed (loss: 0.08687278628349304, acc: 0.9736841917037964)
[2025-02-13 19:59:10,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:10,487][root][INFO] - Training Epoch: 1/2, step 5105/7134 completed (loss: 0.17677819728851318, acc: 0.9668874144554138)
[2025-02-13 19:59:10,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:10,853][root][INFO] - Training Epoch: 1/2, step 5106/7134 completed (loss: 0.19535984098911285, acc: 0.9520547986030579)
[2025-02-13 19:59:10,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:11,262][root][INFO] - Training Epoch: 1/2, step 5107/7134 completed (loss: 0.34089571237564087, acc: 0.9047619104385376)
[2025-02-13 19:59:11,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:11,664][root][INFO] - Training Epoch: 1/2, step 5108/7134 completed (loss: 0.43110713362693787, acc: 0.8723404407501221)
[2025-02-13 19:59:11,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:12,064][root][INFO] - Training Epoch: 1/2, step 5109/7134 completed (loss: 0.4451909363269806, acc: 0.8894472122192383)
[2025-02-13 19:59:12,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:12,476][root][INFO] - Training Epoch: 1/2, step 5110/7134 completed (loss: 0.2507777810096741, acc: 0.9395604133605957)
[2025-02-13 19:59:12,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:12,818][root][INFO] - Training Epoch: 1/2, step 5111/7134 completed (loss: 0.2215823531150818, acc: 0.931034505367279)
[2025-02-13 19:59:12,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:13,215][root][INFO] - Training Epoch: 1/2, step 5112/7134 completed (loss: 0.2760053873062134, acc: 0.9459459185600281)
[2025-02-13 19:59:13,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:13,583][root][INFO] - Training Epoch: 1/2, step 5113/7134 completed (loss: 0.3526427745819092, acc: 0.9139072895050049)
[2025-02-13 19:59:13,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:13,997][root][INFO] - Training Epoch: 1/2, step 5114/7134 completed (loss: 0.11583001166582108, acc: 0.9668874144554138)
[2025-02-13 19:59:14,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:14,438][root][INFO] - Training Epoch: 1/2, step 5115/7134 completed (loss: 0.13254211843013763, acc: 0.9576719403266907)
[2025-02-13 19:59:14,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:14,825][root][INFO] - Training Epoch: 1/2, step 5116/7134 completed (loss: 0.12332332134246826, acc: 0.9735449552536011)
[2025-02-13 19:59:14,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:15,189][root][INFO] - Training Epoch: 1/2, step 5117/7134 completed (loss: 0.10342299938201904, acc: 0.9723756909370422)
[2025-02-13 19:59:15,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:15,530][root][INFO] - Training Epoch: 1/2, step 5118/7134 completed (loss: 0.1398245245218277, acc: 0.9759036302566528)
[2025-02-13 19:59:15,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:15,923][root][INFO] - Training Epoch: 1/2, step 5119/7134 completed (loss: 0.09430162608623505, acc: 0.9885057210922241)
[2025-02-13 19:59:16,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:16,274][root][INFO] - Training Epoch: 1/2, step 5120/7134 completed (loss: 0.1872926950454712, acc: 0.9365079402923584)
[2025-02-13 19:59:16,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:16,630][root][INFO] - Training Epoch: 1/2, step 5121/7134 completed (loss: 0.06588643044233322, acc: 0.9818181991577148)
[2025-02-13 19:59:16,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:17,063][root][INFO] - Training Epoch: 1/2, step 5122/7134 completed (loss: 0.25175583362579346, acc: 0.9127907156944275)
[2025-02-13 19:59:17,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:17,431][root][INFO] - Training Epoch: 1/2, step 5123/7134 completed (loss: 0.09935632348060608, acc: 0.9801980257034302)
[2025-02-13 19:59:17,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:17,790][root][INFO] - Training Epoch: 1/2, step 5124/7134 completed (loss: 0.1659686267375946, acc: 0.9556650519371033)
[2025-02-13 19:59:17,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:18,156][root][INFO] - Training Epoch: 1/2, step 5125/7134 completed (loss: 0.05249428749084473, acc: 0.9803921580314636)
[2025-02-13 19:59:18,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:18,525][root][INFO] - Training Epoch: 1/2, step 5126/7134 completed (loss: 0.14854364097118378, acc: 0.9793103337287903)
[2025-02-13 19:59:18,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:18,918][root][INFO] - Training Epoch: 1/2, step 5127/7134 completed (loss: 0.3649542033672333, acc: 0.931034505367279)
[2025-02-13 19:59:19,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:19,306][root][INFO] - Training Epoch: 1/2, step 5128/7134 completed (loss: 0.08611894398927689, acc: 0.9784482717514038)
[2025-02-13 19:59:19,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:19,636][root][INFO] - Training Epoch: 1/2, step 5129/7134 completed (loss: 0.23405465483665466, acc: 0.9534883499145508)
[2025-02-13 19:59:19,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:20,034][root][INFO] - Training Epoch: 1/2, step 5130/7134 completed (loss: 0.0762118324637413, acc: 0.9805194735527039)
[2025-02-13 19:59:20,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:20,418][root][INFO] - Training Epoch: 1/2, step 5131/7134 completed (loss: 0.25065264105796814, acc: 0.9551281929016113)
[2025-02-13 19:59:20,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:20,792][root][INFO] - Training Epoch: 1/2, step 5132/7134 completed (loss: 0.20627552270889282, acc: 0.9453125)
[2025-02-13 19:59:20,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:21,145][root][INFO] - Training Epoch: 1/2, step 5133/7134 completed (loss: 0.19116073846817017, acc: 0.9451219439506531)
[2025-02-13 19:59:21,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:21,502][root][INFO] - Training Epoch: 1/2, step 5134/7134 completed (loss: 0.14090466499328613, acc: 0.9492753744125366)
[2025-02-13 19:59:21,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:21,839][root][INFO] - Training Epoch: 1/2, step 5135/7134 completed (loss: 0.23367172479629517, acc: 0.9768785834312439)
[2025-02-13 19:59:21,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:22,208][root][INFO] - Training Epoch: 1/2, step 5136/7134 completed (loss: 0.11047077924013138, acc: 0.9677419066429138)
[2025-02-13 19:59:22,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:22,568][root][INFO] - Training Epoch: 1/2, step 5137/7134 completed (loss: 0.316546767950058, acc: 0.9492753744125366)
[2025-02-13 19:59:22,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:22,933][root][INFO] - Training Epoch: 1/2, step 5138/7134 completed (loss: 0.371698260307312, acc: 0.9468085169792175)
[2025-02-13 19:59:23,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:23,273][root][INFO] - Training Epoch: 1/2, step 5139/7134 completed (loss: 0.18096359074115753, acc: 0.9529411792755127)
[2025-02-13 19:59:23,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:23,662][root][INFO] - Training Epoch: 1/2, step 5140/7134 completed (loss: 0.25954899191856384, acc: 0.9375)
[2025-02-13 19:59:23,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:24,081][root][INFO] - Training Epoch: 1/2, step 5141/7134 completed (loss: 0.23887787759304047, acc: 0.9508196711540222)
[2025-02-13 19:59:24,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:24,452][root][INFO] - Training Epoch: 1/2, step 5142/7134 completed (loss: 0.10919032245874405, acc: 0.9800994992256165)
[2025-02-13 19:59:24,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:24,819][root][INFO] - Training Epoch: 1/2, step 5143/7134 completed (loss: 0.16799508035182953, acc: 0.9552238583564758)
[2025-02-13 19:59:24,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:25,185][root][INFO] - Training Epoch: 1/2, step 5144/7134 completed (loss: 0.08942798525094986, acc: 0.9735449552536011)
[2025-02-13 19:59:25,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:25,554][root][INFO] - Training Epoch: 1/2, step 5145/7134 completed (loss: 0.08770812302827835, acc: 0.9791666865348816)
[2025-02-13 19:59:25,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:25,916][root][INFO] - Training Epoch: 1/2, step 5146/7134 completed (loss: 0.14401815831661224, acc: 0.9726775884628296)
[2025-02-13 19:59:26,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:26,274][root][INFO] - Training Epoch: 1/2, step 5147/7134 completed (loss: 0.08992976695299149, acc: 0.9679487347602844)
[2025-02-13 19:59:26,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:26,608][root][INFO] - Training Epoch: 1/2, step 5148/7134 completed (loss: 0.23710551857948303, acc: 0.9437500238418579)
[2025-02-13 19:59:26,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:26,978][root][INFO] - Training Epoch: 1/2, step 5149/7134 completed (loss: 0.10222739726305008, acc: 0.9728506803512573)
[2025-02-13 19:59:27,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:27,327][root][INFO] - Training Epoch: 1/2, step 5150/7134 completed (loss: 0.19164063036441803, acc: 0.9640718698501587)
[2025-02-13 19:59:27,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:27,706][root][INFO] - Training Epoch: 1/2, step 5151/7134 completed (loss: 0.20665381848812103, acc: 0.954285740852356)
[2025-02-13 19:59:27,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:28,074][root][INFO] - Training Epoch: 1/2, step 5152/7134 completed (loss: 0.09478852897882462, acc: 0.9764705896377563)
[2025-02-13 19:59:28,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:28,454][root][INFO] - Training Epoch: 1/2, step 5153/7134 completed (loss: 0.15133124589920044, acc: 0.9542483687400818)
[2025-02-13 19:59:28,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:28,817][root][INFO] - Training Epoch: 1/2, step 5154/7134 completed (loss: 0.09024964272975922, acc: 0.9750000238418579)
[2025-02-13 19:59:28,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:29,185][root][INFO] - Training Epoch: 1/2, step 5155/7134 completed (loss: 0.1863512396812439, acc: 0.9635036587715149)
[2025-02-13 19:59:29,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:29,558][root][INFO] - Training Epoch: 1/2, step 5156/7134 completed (loss: 0.12157929688692093, acc: 0.9722222089767456)
[2025-02-13 19:59:29,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:29,926][root][INFO] - Training Epoch: 1/2, step 5157/7134 completed (loss: 0.12351493537425995, acc: 0.9772727489471436)
[2025-02-13 19:59:30,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:30,303][root][INFO] - Training Epoch: 1/2, step 5158/7134 completed (loss: 0.2514243423938751, acc: 0.9358974099159241)
[2025-02-13 19:59:30,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:30,677][root][INFO] - Training Epoch: 1/2, step 5159/7134 completed (loss: 0.17235618829727173, acc: 0.9595959782600403)
[2025-02-13 19:59:30,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:31,046][root][INFO] - Training Epoch: 1/2, step 5160/7134 completed (loss: 0.22790637612342834, acc: 0.9388889074325562)
[2025-02-13 19:59:31,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:31,416][root][INFO] - Training Epoch: 1/2, step 5161/7134 completed (loss: 0.28606918454170227, acc: 0.9193548560142517)
[2025-02-13 19:59:31,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:31,778][root][INFO] - Training Epoch: 1/2, step 5162/7134 completed (loss: 0.256680965423584, acc: 0.9200000166893005)
[2025-02-13 19:59:31,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:32,142][root][INFO] - Training Epoch: 1/2, step 5163/7134 completed (loss: 0.1280234009027481, acc: 0.9677419066429138)
[2025-02-13 19:59:32,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:32,504][root][INFO] - Training Epoch: 1/2, step 5164/7134 completed (loss: 0.09073061496019363, acc: 0.9806451797485352)
[2025-02-13 19:59:32,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:32,880][root][INFO] - Training Epoch: 1/2, step 5165/7134 completed (loss: 0.18399813771247864, acc: 0.9575757384300232)
[2025-02-13 19:59:33,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:33,240][root][INFO] - Training Epoch: 1/2, step 5166/7134 completed (loss: 0.19464848935604095, acc: 0.9320987462997437)
[2025-02-13 19:59:33,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:33,607][root][INFO] - Training Epoch: 1/2, step 5167/7134 completed (loss: 0.24008680880069733, acc: 0.9447513818740845)
[2025-02-13 19:59:33,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:33,942][root][INFO] - Training Epoch: 1/2, step 5168/7134 completed (loss: 0.35115107893943787, acc: 0.9244186282157898)
[2025-02-13 19:59:34,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:34,325][root][INFO] - Training Epoch: 1/2, step 5169/7134 completed (loss: 0.2068374902009964, acc: 0.9512194991111755)
[2025-02-13 19:59:34,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:34,711][root][INFO] - Training Epoch: 1/2, step 5170/7134 completed (loss: 0.3273073732852936, acc: 0.9274611473083496)
[2025-02-13 19:59:34,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:35,096][root][INFO] - Training Epoch: 1/2, step 5171/7134 completed (loss: 0.15278086066246033, acc: 0.9756097793579102)
[2025-02-13 19:59:35,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:35,480][root][INFO] - Training Epoch: 1/2, step 5172/7134 completed (loss: 0.44831857085227966, acc: 0.8963414430618286)
[2025-02-13 19:59:35,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:35,815][root][INFO] - Training Epoch: 1/2, step 5173/7134 completed (loss: 0.18563398718833923, acc: 0.9570552110671997)
[2025-02-13 19:59:35,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:36,214][root][INFO] - Training Epoch: 1/2, step 5174/7134 completed (loss: 0.10309139639139175, acc: 0.9710982441902161)
[2025-02-13 19:59:36,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:36,584][root][INFO] - Training Epoch: 1/2, step 5175/7134 completed (loss: 0.14200732111930847, acc: 0.9640718698501587)
[2025-02-13 19:59:36,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:36,951][root][INFO] - Training Epoch: 1/2, step 5176/7134 completed (loss: 0.13225026428699493, acc: 0.9605262875556946)
[2025-02-13 19:59:37,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:37,336][root][INFO] - Training Epoch: 1/2, step 5177/7134 completed (loss: 0.24910610914230347, acc: 0.9599999785423279)
[2025-02-13 19:59:37,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:37,696][root][INFO] - Training Epoch: 1/2, step 5178/7134 completed (loss: 0.4452834129333496, acc: 0.9333333373069763)
[2025-02-13 19:59:37,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:38,061][root][INFO] - Training Epoch: 1/2, step 5179/7134 completed (loss: 0.2287018895149231, acc: 0.9464285969734192)
[2025-02-13 19:59:38,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:38,434][root][INFO] - Training Epoch: 1/2, step 5180/7134 completed (loss: 0.2904911935329437, acc: 0.936170220375061)
[2025-02-13 19:59:38,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:38,832][root][INFO] - Training Epoch: 1/2, step 5181/7134 completed (loss: 0.4928050637245178, acc: 0.8876404762268066)
[2025-02-13 19:59:38,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:39,227][root][INFO] - Training Epoch: 1/2, step 5182/7134 completed (loss: 0.6839260458946228, acc: 0.8897637724876404)
[2025-02-13 19:59:39,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:39,623][root][INFO] - Training Epoch: 1/2, step 5183/7134 completed (loss: 0.26121270656585693, acc: 0.9485294222831726)
[2025-02-13 19:59:39,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:39,972][root][INFO] - Training Epoch: 1/2, step 5184/7134 completed (loss: 0.1500035673379898, acc: 0.9711538553237915)
[2025-02-13 19:59:40,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:40,325][root][INFO] - Training Epoch: 1/2, step 5185/7134 completed (loss: 0.2154318243265152, acc: 0.9504950642585754)
[2025-02-13 19:59:40,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:40,688][root][INFO] - Training Epoch: 1/2, step 5186/7134 completed (loss: 0.1613103449344635, acc: 0.960629940032959)
[2025-02-13 19:59:40,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:41,049][root][INFO] - Training Epoch: 1/2, step 5187/7134 completed (loss: 0.09217587858438492, acc: 0.984000027179718)
[2025-02-13 19:59:41,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:41,402][root][INFO] - Training Epoch: 1/2, step 5188/7134 completed (loss: 0.24172843992710114, acc: 0.9420289993286133)
[2025-02-13 19:59:41,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:41,707][root][INFO] - Training Epoch: 1/2, step 5189/7134 completed (loss: 0.2718783915042877, acc: 0.9702970385551453)
[2025-02-13 19:59:41,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:42,055][root][INFO] - Training Epoch: 1/2, step 5190/7134 completed (loss: 0.14527517557144165, acc: 0.9636363387107849)
[2025-02-13 19:59:42,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:42,463][root][INFO] - Training Epoch: 1/2, step 5191/7134 completed (loss: 0.24468129873275757, acc: 0.9320987462997437)
[2025-02-13 19:59:42,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:42,842][root][INFO] - Training Epoch: 1/2, step 5192/7134 completed (loss: 0.1794721484184265, acc: 0.9696969985961914)
[2025-02-13 19:59:42,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:43,196][root][INFO] - Training Epoch: 1/2, step 5193/7134 completed (loss: 0.1541270613670349, acc: 0.9583333134651184)
[2025-02-13 19:59:43,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:43,570][root][INFO] - Training Epoch: 1/2, step 5194/7134 completed (loss: 0.46454405784606934, acc: 0.9108280539512634)
[2025-02-13 19:59:43,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:43,923][root][INFO] - Training Epoch: 1/2, step 5195/7134 completed (loss: 0.2945726811885834, acc: 0.9351851940155029)
[2025-02-13 19:59:44,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:44,274][root][INFO] - Training Epoch: 1/2, step 5196/7134 completed (loss: 0.3412362337112427, acc: 0.9300000071525574)
[2025-02-13 19:59:44,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:44,633][root][INFO] - Training Epoch: 1/2, step 5197/7134 completed (loss: 0.23272605240345, acc: 0.920634925365448)
[2025-02-13 19:59:44,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:44,989][root][INFO] - Training Epoch: 1/2, step 5198/7134 completed (loss: 0.18199265003204346, acc: 0.9672130942344666)
[2025-02-13 19:59:45,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:45,355][root][INFO] - Training Epoch: 1/2, step 5199/7134 completed (loss: 0.3599420487880707, acc: 0.9186046719551086)
[2025-02-13 19:59:45,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:45,707][root][INFO] - Training Epoch: 1/2, step 5200/7134 completed (loss: 0.15608185529708862, acc: 0.9603174328804016)
[2025-02-13 19:59:45,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:46,098][root][INFO] - Training Epoch: 1/2, step 5201/7134 completed (loss: 0.3295234739780426, acc: 0.9323671460151672)
[2025-02-13 19:59:46,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:46,456][root][INFO] - Training Epoch: 1/2, step 5202/7134 completed (loss: 0.15479221940040588, acc: 0.9739583134651184)
[2025-02-13 19:59:46,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:46,807][root][INFO] - Training Epoch: 1/2, step 5203/7134 completed (loss: 0.10015557706356049, acc: 0.9725274443626404)
[2025-02-13 19:59:46,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:47,153][root][INFO] - Training Epoch: 1/2, step 5204/7134 completed (loss: 0.05226786434650421, acc: 1.0)
[2025-02-13 19:59:47,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:47,544][root][INFO] - Training Epoch: 1/2, step 5205/7134 completed (loss: 0.12965716421604156, acc: 0.9638554453849792)
[2025-02-13 19:59:47,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:47,951][root][INFO] - Training Epoch: 1/2, step 5206/7134 completed (loss: 0.17206168174743652, acc: 0.9590643048286438)
[2025-02-13 19:59:48,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:48,313][root][INFO] - Training Epoch: 1/2, step 5207/7134 completed (loss: 0.5643322467803955, acc: 0.89682537317276)
[2025-02-13 19:59:48,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:48,696][root][INFO] - Training Epoch: 1/2, step 5208/7134 completed (loss: 0.13074727356433868, acc: 0.9655172228813171)
[2025-02-13 19:59:48,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:49,057][root][INFO] - Training Epoch: 1/2, step 5209/7134 completed (loss: 0.26705801486968994, acc: 0.9248120188713074)
[2025-02-13 19:59:49,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:49,408][root][INFO] - Training Epoch: 1/2, step 5210/7134 completed (loss: 0.1136443018913269, acc: 0.9644970297813416)
[2025-02-13 19:59:49,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:49,773][root][INFO] - Training Epoch: 1/2, step 5211/7134 completed (loss: 0.17917466163635254, acc: 0.9603174328804016)
[2025-02-13 19:59:49,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:50,190][root][INFO] - Training Epoch: 1/2, step 5212/7134 completed (loss: 0.21922430396080017, acc: 0.9220778942108154)
[2025-02-13 19:59:50,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:50,568][root][INFO] - Training Epoch: 1/2, step 5213/7134 completed (loss: 0.18977537751197815, acc: 0.9629629850387573)
[2025-02-13 19:59:50,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:50,939][root][INFO] - Training Epoch: 1/2, step 5214/7134 completed (loss: 0.36341559886932373, acc: 0.9159663915634155)
[2025-02-13 19:59:51,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:51,336][root][INFO] - Training Epoch: 1/2, step 5215/7134 completed (loss: 0.3414076864719391, acc: 0.9306930899620056)
[2025-02-13 19:59:51,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:51,646][root][INFO] - Training Epoch: 1/2, step 5216/7134 completed (loss: 0.23834262788295746, acc: 0.9305555820465088)
[2025-02-13 19:59:51,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:52,081][root][INFO] - Training Epoch: 1/2, step 5217/7134 completed (loss: 0.417949914932251, acc: 0.9239130616188049)
[2025-02-13 19:59:52,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:52,477][root][INFO] - Training Epoch: 1/2, step 5218/7134 completed (loss: 0.30559778213500977, acc: 0.9207921028137207)
[2025-02-13 19:59:52,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:52,886][root][INFO] - Training Epoch: 1/2, step 5219/7134 completed (loss: 0.24023644626140594, acc: 0.9444444179534912)
[2025-02-13 19:59:53,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:53,334][root][INFO] - Training Epoch: 1/2, step 5220/7134 completed (loss: 0.15500590205192566, acc: 0.9383561611175537)
[2025-02-13 19:59:53,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:53,666][root][INFO] - Training Epoch: 1/2, step 5221/7134 completed (loss: 0.149346262216568, acc: 0.9691358208656311)
[2025-02-13 19:59:53,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:53,989][root][INFO] - Training Epoch: 1/2, step 5222/7134 completed (loss: 0.09558278322219849, acc: 0.976190447807312)
[2025-02-13 19:59:54,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:54,333][root][INFO] - Training Epoch: 1/2, step 5223/7134 completed (loss: 0.1717001348733902, acc: 0.9557521939277649)
[2025-02-13 19:59:54,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:54,685][root][INFO] - Training Epoch: 1/2, step 5224/7134 completed (loss: 0.16271597146987915, acc: 0.9629629850387573)
[2025-02-13 19:59:54,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:55,043][root][INFO] - Training Epoch: 1/2, step 5225/7134 completed (loss: 0.15537965297698975, acc: 0.9671052694320679)
[2025-02-13 19:59:55,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:55,395][root][INFO] - Training Epoch: 1/2, step 5226/7134 completed (loss: 0.24145081639289856, acc: 0.942307710647583)
[2025-02-13 19:59:55,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:55,767][root][INFO] - Training Epoch: 1/2, step 5227/7134 completed (loss: 0.1832149773836136, acc: 0.9487179517745972)
[2025-02-13 19:59:55,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:56,132][root][INFO] - Training Epoch: 1/2, step 5228/7134 completed (loss: 0.1088380292057991, acc: 0.9599999785423279)
[2025-02-13 19:59:56,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:56,510][root][INFO] - Training Epoch: 1/2, step 5229/7134 completed (loss: 0.05359853431582451, acc: 0.9806451797485352)
[2025-02-13 19:59:56,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:56,909][root][INFO] - Training Epoch: 1/2, step 5230/7134 completed (loss: 0.07962597906589508, acc: 0.9723756909370422)
[2025-02-13 19:59:57,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:57,319][root][INFO] - Training Epoch: 1/2, step 5231/7134 completed (loss: 0.18043962121009827, acc: 0.9639175534248352)
[2025-02-13 19:59:57,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:57,727][root][INFO] - Training Epoch: 1/2, step 5232/7134 completed (loss: 0.12655967473983765, acc: 0.9661017060279846)
[2025-02-13 19:59:57,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:58,111][root][INFO] - Training Epoch: 1/2, step 5233/7134 completed (loss: 0.14987313747406006, acc: 0.9541284441947937)
[2025-02-13 19:59:58,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:58,488][root][INFO] - Training Epoch: 1/2, step 5234/7134 completed (loss: 0.10667350143194199, acc: 0.9736841917037964)
[2025-02-13 19:59:58,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:58,842][root][INFO] - Training Epoch: 1/2, step 5235/7134 completed (loss: 0.03252488374710083, acc: 1.0)
[2025-02-13 19:59:58,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:59,193][root][INFO] - Training Epoch: 1/2, step 5236/7134 completed (loss: 0.02337292954325676, acc: 0.9927007555961609)
[2025-02-13 19:59:59,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:59,591][root][INFO] - Training Epoch: 1/2, step 5237/7134 completed (loss: 0.06833066046237946, acc: 0.98591548204422)
[2025-02-13 19:59:59,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:59,962][root][INFO] - Training Epoch: 1/2, step 5238/7134 completed (loss: 0.06521356850862503, acc: 0.9925373196601868)
[2025-02-13 20:00:00,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:00,279][root][INFO] - Training Epoch: 1/2, step 5239/7134 completed (loss: 0.19895726442337036, acc: 0.9411764740943909)
[2025-02-13 20:00:00,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:00,635][root][INFO] - Training Epoch: 1/2, step 5240/7134 completed (loss: 0.07343266904354095, acc: 0.9941860437393188)
[2025-02-13 20:00:00,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:01,000][root][INFO] - Training Epoch: 1/2, step 5241/7134 completed (loss: 0.21937470138072968, acc: 0.9548386931419373)
[2025-02-13 20:00:01,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:01,376][root][INFO] - Training Epoch: 1/2, step 5242/7134 completed (loss: 0.09069322794675827, acc: 0.9736841917037964)
[2025-02-13 20:00:01,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:01,734][root][INFO] - Training Epoch: 1/2, step 5243/7134 completed (loss: 0.03763395547866821, acc: 0.9928571581840515)
[2025-02-13 20:00:01,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:02,083][root][INFO] - Training Epoch: 1/2, step 5244/7134 completed (loss: 0.16254280507564545, acc: 0.9593023061752319)
[2025-02-13 20:00:02,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:02,476][root][INFO] - Training Epoch: 1/2, step 5245/7134 completed (loss: 0.08279795199632645, acc: 0.9920634627342224)
[2025-02-13 20:00:02,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:02,830][root][INFO] - Training Epoch: 1/2, step 5246/7134 completed (loss: 0.13800010085105896, acc: 0.949367105960846)
[2025-02-13 20:00:02,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:03,188][root][INFO] - Training Epoch: 1/2, step 5247/7134 completed (loss: 0.1450243890285492, acc: 0.9629629850387573)
[2025-02-13 20:00:03,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:03,553][root][INFO] - Training Epoch: 1/2, step 5248/7134 completed (loss: 0.11067257076501846, acc: 0.9923664331436157)
[2025-02-13 20:00:03,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:03,919][root][INFO] - Training Epoch: 1/2, step 5249/7134 completed (loss: 0.10224958509206772, acc: 0.9745222926139832)
[2025-02-13 20:00:04,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:04,293][root][INFO] - Training Epoch: 1/2, step 5250/7134 completed (loss: 0.10358886420726776, acc: 0.9728260636329651)
[2025-02-13 20:00:04,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:04,653][root][INFO] - Training Epoch: 1/2, step 5251/7134 completed (loss: 0.11529656499624252, acc: 0.9635416865348816)
[2025-02-13 20:00:04,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:05,023][root][INFO] - Training Epoch: 1/2, step 5252/7134 completed (loss: 0.21882645785808563, acc: 0.9469026327133179)
[2025-02-13 20:00:05,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:05,382][root][INFO] - Training Epoch: 1/2, step 5253/7134 completed (loss: 0.06288638710975647, acc: 0.9861111044883728)
[2025-02-13 20:00:05,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:05,745][root][INFO] - Training Epoch: 1/2, step 5254/7134 completed (loss: 0.3493320345878601, acc: 0.9090909361839294)
[2025-02-13 20:00:05,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:06,108][root][INFO] - Training Epoch: 1/2, step 5255/7134 completed (loss: 0.1062302216887474, acc: 0.9655172228813171)
[2025-02-13 20:00:06,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:06,464][root][INFO] - Training Epoch: 1/2, step 5256/7134 completed (loss: 0.24258778989315033, acc: 0.9466666579246521)
[2025-02-13 20:00:06,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:06,849][root][INFO] - Training Epoch: 1/2, step 5257/7134 completed (loss: 0.36220476031303406, acc: 0.9281437397003174)
[2025-02-13 20:00:06,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:07,218][root][INFO] - Training Epoch: 1/2, step 5258/7134 completed (loss: 0.25612685084342957, acc: 0.9417989253997803)
[2025-02-13 20:00:07,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:07,583][root][INFO] - Training Epoch: 1/2, step 5259/7134 completed (loss: 0.2864392399787903, acc: 0.935251772403717)
[2025-02-13 20:00:07,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:07,943][root][INFO] - Training Epoch: 1/2, step 5260/7134 completed (loss: 0.15460394322872162, acc: 0.9576719403266907)
[2025-02-13 20:00:08,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:08,310][root][INFO] - Training Epoch: 1/2, step 5261/7134 completed (loss: 0.2447260022163391, acc: 0.9580419659614563)
[2025-02-13 20:00:08,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:08,686][root][INFO] - Training Epoch: 1/2, step 5262/7134 completed (loss: 0.1039809063076973, acc: 0.9638554453849792)
[2025-02-13 20:00:08,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:09,044][root][INFO] - Training Epoch: 1/2, step 5263/7134 completed (loss: 0.15006254613399506, acc: 0.9647887349128723)
[2025-02-13 20:00:09,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:09,411][root][INFO] - Training Epoch: 1/2, step 5264/7134 completed (loss: 0.11920209974050522, acc: 0.9693877696990967)
[2025-02-13 20:00:09,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:09,772][root][INFO] - Training Epoch: 1/2, step 5265/7134 completed (loss: 0.19432595372200012, acc: 0.9562841653823853)
[2025-02-13 20:00:09,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:10,126][root][INFO] - Training Epoch: 1/2, step 5266/7134 completed (loss: 0.04305518418550491, acc: 0.9876543283462524)
[2025-02-13 20:00:10,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:10,475][root][INFO] - Training Epoch: 1/2, step 5267/7134 completed (loss: 0.3937220871448517, acc: 0.9135802388191223)
[2025-02-13 20:00:10,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:10,833][root][INFO] - Training Epoch: 1/2, step 5268/7134 completed (loss: 0.24607731401920319, acc: 0.9457364082336426)
[2025-02-13 20:00:10,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:11,198][root][INFO] - Training Epoch: 1/2, step 5269/7134 completed (loss: 0.24964414536952972, acc: 0.9571428298950195)
[2025-02-13 20:00:11,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:11,569][root][INFO] - Training Epoch: 1/2, step 5270/7134 completed (loss: 0.13214002549648285, acc: 0.9655172228813171)
[2025-02-13 20:00:11,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:11,935][root][INFO] - Training Epoch: 1/2, step 5271/7134 completed (loss: 0.13948030769824982, acc: 0.9640287756919861)
[2025-02-13 20:00:12,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:12,291][root][INFO] - Training Epoch: 1/2, step 5272/7134 completed (loss: 0.11735114455223083, acc: 0.9826589822769165)
[2025-02-13 20:00:12,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:12,666][root][INFO] - Training Epoch: 1/2, step 5273/7134 completed (loss: 0.18267017602920532, acc: 0.9583333134651184)
[2025-02-13 20:00:12,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:13,038][root][INFO] - Training Epoch: 1/2, step 5274/7134 completed (loss: 0.20760905742645264, acc: 0.9551281929016113)
[2025-02-13 20:00:13,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:13,395][root][INFO] - Training Epoch: 1/2, step 5275/7134 completed (loss: 0.11579284071922302, acc: 0.9756097793579102)
[2025-02-13 20:00:13,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:13,746][root][INFO] - Training Epoch: 1/2, step 5276/7134 completed (loss: 0.18757911026477814, acc: 0.9671052694320679)
[2025-02-13 20:00:13,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:14,098][root][INFO] - Training Epoch: 1/2, step 5277/7134 completed (loss: 0.10951049625873566, acc: 0.9745222926139832)
[2025-02-13 20:00:14,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:14,457][root][INFO] - Training Epoch: 1/2, step 5278/7134 completed (loss: 0.27710145711898804, acc: 0.9333333373069763)
[2025-02-13 20:00:14,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:14,819][root][INFO] - Training Epoch: 1/2, step 5279/7134 completed (loss: 0.03961953520774841, acc: 0.9918699264526367)
[2025-02-13 20:00:14,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:15,197][root][INFO] - Training Epoch: 1/2, step 5280/7134 completed (loss: 0.16151869297027588, acc: 0.9714285731315613)
[2025-02-13 20:00:15,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:15,592][root][INFO] - Training Epoch: 1/2, step 5281/7134 completed (loss: 0.13149182498455048, acc: 0.9637681245803833)
[2025-02-13 20:00:15,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:15,967][root][INFO] - Training Epoch: 1/2, step 5282/7134 completed (loss: 0.2223929762840271, acc: 0.9593023061752319)
[2025-02-13 20:00:16,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:16,341][root][INFO] - Training Epoch: 1/2, step 5283/7134 completed (loss: 0.1528107225894928, acc: 0.9508196711540222)
[2025-02-13 20:00:16,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:16,696][root][INFO] - Training Epoch: 1/2, step 5284/7134 completed (loss: 0.23403827846050262, acc: 0.96875)
[2025-02-13 20:00:16,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:17,062][root][INFO] - Training Epoch: 1/2, step 5285/7134 completed (loss: 0.17282241582870483, acc: 0.9677419066429138)
[2025-02-13 20:00:17,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:17,427][root][INFO] - Training Epoch: 1/2, step 5286/7134 completed (loss: 0.22292795777320862, acc: 0.9798657894134521)
[2025-02-13 20:00:17,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:17,860][root][INFO] - Training Epoch: 1/2, step 5287/7134 completed (loss: 0.1993815153837204, acc: 0.9545454382896423)
[2025-02-13 20:00:17,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:18,230][root][INFO] - Training Epoch: 1/2, step 5288/7134 completed (loss: 0.11782141029834747, acc: 0.96875)
[2025-02-13 20:00:18,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:18,604][root][INFO] - Training Epoch: 1/2, step 5289/7134 completed (loss: 0.089118592441082, acc: 0.9715909361839294)
[2025-02-13 20:00:18,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:19,011][root][INFO] - Training Epoch: 1/2, step 5290/7134 completed (loss: 0.22927923500537872, acc: 0.9576719403266907)
[2025-02-13 20:00:19,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:19,383][root][INFO] - Training Epoch: 1/2, step 5291/7134 completed (loss: 0.06633210927248001, acc: 0.9837837815284729)
[2025-02-13 20:00:19,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:19,738][root][INFO] - Training Epoch: 1/2, step 5292/7134 completed (loss: 0.09561122953891754, acc: 0.9830508232116699)
[2025-02-13 20:00:19,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:20,105][root][INFO] - Training Epoch: 1/2, step 5293/7134 completed (loss: 0.10837607085704803, acc: 0.9681528806686401)
[2025-02-13 20:00:20,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:20,507][root][INFO] - Training Epoch: 1/2, step 5294/7134 completed (loss: 0.1618628203868866, acc: 0.9526627063751221)
[2025-02-13 20:00:20,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:20,871][root][INFO] - Training Epoch: 1/2, step 5295/7134 completed (loss: 0.1149350181221962, acc: 0.9767441749572754)
[2025-02-13 20:00:21,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:21,254][root][INFO] - Training Epoch: 1/2, step 5296/7134 completed (loss: 0.1554153859615326, acc: 0.9661017060279846)
[2025-02-13 20:00:21,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:21,617][root][INFO] - Training Epoch: 1/2, step 5297/7134 completed (loss: 0.11982555687427521, acc: 0.9638554453849792)
[2025-02-13 20:00:21,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:22,019][root][INFO] - Training Epoch: 1/2, step 5298/7134 completed (loss: 0.13176967203617096, acc: 0.9503546357154846)
[2025-02-13 20:00:22,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:22,371][root][INFO] - Training Epoch: 1/2, step 5299/7134 completed (loss: 0.044831134378910065, acc: 1.0)
[2025-02-13 20:00:22,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:22,725][root][INFO] - Training Epoch: 1/2, step 5300/7134 completed (loss: 0.032860707491636276, acc: 0.9939758777618408)
[2025-02-13 20:00:22,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:23,088][root][INFO] - Training Epoch: 1/2, step 5301/7134 completed (loss: 0.07861509919166565, acc: 0.9817073345184326)
[2025-02-13 20:00:23,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:23,474][root][INFO] - Training Epoch: 1/2, step 5302/7134 completed (loss: 0.22921310365200043, acc: 0.9497206807136536)
[2025-02-13 20:00:23,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:23,859][root][INFO] - Training Epoch: 1/2, step 5303/7134 completed (loss: 0.4107203185558319, acc: 0.9166666865348816)
[2025-02-13 20:00:24,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:24,221][root][INFO] - Training Epoch: 1/2, step 5304/7134 completed (loss: 0.47022745013237, acc: 0.8835616707801819)
[2025-02-13 20:00:24,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:24,570][root][INFO] - Training Epoch: 1/2, step 5305/7134 completed (loss: 0.2811133563518524, acc: 0.9375)
[2025-02-13 20:00:24,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:24,931][root][INFO] - Training Epoch: 1/2, step 5306/7134 completed (loss: 0.10864368826150894, acc: 0.9833333492279053)
[2025-02-13 20:00:25,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:25,312][root][INFO] - Training Epoch: 1/2, step 5307/7134 completed (loss: 0.4266263246536255, acc: 0.9239766001701355)
[2025-02-13 20:00:25,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:25,675][root][INFO] - Training Epoch: 1/2, step 5308/7134 completed (loss: 0.1825554519891739, acc: 0.9629629850387573)
[2025-02-13 20:00:25,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:26,012][root][INFO] - Training Epoch: 1/2, step 5309/7134 completed (loss: 0.5427207946777344, acc: 0.9020978808403015)
[2025-02-13 20:00:26,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:26,392][root][INFO] - Training Epoch: 1/2, step 5310/7134 completed (loss: 0.37829160690307617, acc: 0.9034090638160706)
[2025-02-13 20:00:26,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:26,760][root][INFO] - Training Epoch: 1/2, step 5311/7134 completed (loss: 0.31266510486602783, acc: 0.9192546606063843)
[2025-02-13 20:00:26,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:27,114][root][INFO] - Training Epoch: 1/2, step 5312/7134 completed (loss: 0.21813835203647614, acc: 0.9444444179534912)
[2025-02-13 20:00:27,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:27,481][root][INFO] - Training Epoch: 1/2, step 5313/7134 completed (loss: 0.16557271778583527, acc: 0.9590163826942444)
[2025-02-13 20:00:27,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:27,842][root][INFO] - Training Epoch: 1/2, step 5314/7134 completed (loss: 0.2673893868923187, acc: 0.9285714030265808)
[2025-02-13 20:00:27,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:28,216][root][INFO] - Training Epoch: 1/2, step 5315/7134 completed (loss: 0.1857001930475235, acc: 0.9640287756919861)
[2025-02-13 20:00:28,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:28,592][root][INFO] - Training Epoch: 1/2, step 5316/7134 completed (loss: 0.13662835955619812, acc: 0.95333331823349)
[2025-02-13 20:00:28,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:28,950][root][INFO] - Training Epoch: 1/2, step 5317/7134 completed (loss: 0.15783365070819855, acc: 0.9523809552192688)
[2025-02-13 20:00:29,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:29,306][root][INFO] - Training Epoch: 1/2, step 5318/7134 completed (loss: 0.0687464252114296, acc: 0.9808917045593262)
[2025-02-13 20:00:29,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:29,677][root][INFO] - Training Epoch: 1/2, step 5319/7134 completed (loss: 0.2654714584350586, acc: 0.9248120188713074)
[2025-02-13 20:00:29,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:30,049][root][INFO] - Training Epoch: 1/2, step 5320/7134 completed (loss: 0.25617074966430664, acc: 0.9371069073677063)
[2025-02-13 20:00:30,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:30,420][root][INFO] - Training Epoch: 1/2, step 5321/7134 completed (loss: 0.19160231947898865, acc: 0.9610389471054077)
[2025-02-13 20:00:30,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:30,794][root][INFO] - Training Epoch: 1/2, step 5322/7134 completed (loss: 0.1145268902182579, acc: 0.9599999785423279)
[2025-02-13 20:00:30,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:31,205][root][INFO] - Training Epoch: 1/2, step 5323/7134 completed (loss: 0.19784855842590332, acc: 0.9194630980491638)
[2025-02-13 20:00:31,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:31,570][root][INFO] - Training Epoch: 1/2, step 5324/7134 completed (loss: 0.1551819145679474, acc: 0.9671052694320679)
[2025-02-13 20:00:31,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:31,930][root][INFO] - Training Epoch: 1/2, step 5325/7134 completed (loss: 0.12878482043743134, acc: 0.9710144996643066)
[2025-02-13 20:00:32,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:32,302][root][INFO] - Training Epoch: 1/2, step 5326/7134 completed (loss: 0.25211721658706665, acc: 0.951724112033844)
[2025-02-13 20:00:32,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:32,649][root][INFO] - Training Epoch: 1/2, step 5327/7134 completed (loss: 0.1574295610189438, acc: 0.9419354796409607)
[2025-02-13 20:00:32,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:33,004][root][INFO] - Training Epoch: 1/2, step 5328/7134 completed (loss: 0.14032262563705444, acc: 0.969924807548523)
[2025-02-13 20:00:33,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:33,364][root][INFO] - Training Epoch: 1/2, step 5329/7134 completed (loss: 0.16280950605869293, acc: 0.9599999785423279)
[2025-02-13 20:00:33,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:33,724][root][INFO] - Training Epoch: 1/2, step 5330/7134 completed (loss: 0.2868383228778839, acc: 0.9240506291389465)
[2025-02-13 20:00:33,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:34,102][root][INFO] - Training Epoch: 1/2, step 5331/7134 completed (loss: 0.16222667694091797, acc: 0.9777777791023254)
[2025-02-13 20:00:34,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:34,488][root][INFO] - Training Epoch: 1/2, step 5332/7134 completed (loss: 0.1554465889930725, acc: 0.9520547986030579)
[2025-02-13 20:00:34,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:34,832][root][INFO] - Training Epoch: 1/2, step 5333/7134 completed (loss: 0.09819459170103073, acc: 0.9671052694320679)
[2025-02-13 20:00:34,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:35,194][root][INFO] - Training Epoch: 1/2, step 5334/7134 completed (loss: 0.21214161813259125, acc: 0.949999988079071)
[2025-02-13 20:00:35,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:35,572][root][INFO] - Training Epoch: 1/2, step 5335/7134 completed (loss: 0.10823682695627213, acc: 0.9798657894134521)
[2025-02-13 20:00:35,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:35,930][root][INFO] - Training Epoch: 1/2, step 5336/7134 completed (loss: 0.16042187809944153, acc: 0.9577465057373047)
[2025-02-13 20:00:36,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:36,288][root][INFO] - Training Epoch: 1/2, step 5337/7134 completed (loss: 0.23672083020210266, acc: 0.9485981464385986)
[2025-02-13 20:00:36,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:36,668][root][INFO] - Training Epoch: 1/2, step 5338/7134 completed (loss: 0.11774855107069016, acc: 0.9620853066444397)
[2025-02-13 20:00:36,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:37,021][root][INFO] - Training Epoch: 1/2, step 5339/7134 completed (loss: 0.15466120839118958, acc: 0.9560439586639404)
[2025-02-13 20:00:37,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:37,386][root][INFO] - Training Epoch: 1/2, step 5340/7134 completed (loss: 0.3465062975883484, acc: 0.899328887462616)
[2025-02-13 20:00:37,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:37,737][root][INFO] - Training Epoch: 1/2, step 5341/7134 completed (loss: 0.14289888739585876, acc: 0.9649999737739563)
[2025-02-13 20:00:37,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:38,134][root][INFO] - Training Epoch: 1/2, step 5342/7134 completed (loss: 0.3973751664161682, acc: 0.8736842274665833)
[2025-02-13 20:00:38,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:38,504][root][INFO] - Training Epoch: 1/2, step 5343/7134 completed (loss: 0.20371077954769135, acc: 0.9513513445854187)
[2025-02-13 20:00:38,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:38,868][root][INFO] - Training Epoch: 1/2, step 5344/7134 completed (loss: 0.262997031211853, acc: 0.9298245906829834)
[2025-02-13 20:00:39,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:39,223][root][INFO] - Training Epoch: 1/2, step 5345/7134 completed (loss: 0.2094283401966095, acc: 0.9545454382896423)
[2025-02-13 20:00:39,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:39,581][root][INFO] - Training Epoch: 1/2, step 5346/7134 completed (loss: 0.18825426697731018, acc: 0.9587156176567078)
[2025-02-13 20:00:39,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:39,937][root][INFO] - Training Epoch: 1/2, step 5347/7134 completed (loss: 0.18987177312374115, acc: 0.9516128897666931)
[2025-02-13 20:00:40,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:40,298][root][INFO] - Training Epoch: 1/2, step 5348/7134 completed (loss: 0.25381630659103394, acc: 0.9216867685317993)
[2025-02-13 20:00:41,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:41,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:43,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:43,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:43,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:44,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:44,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:44,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:45,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:45,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:45,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:46,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:46,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:46,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:47,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:47,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:47,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:48,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:48,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:48,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:49,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:49,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:49,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:50,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:50,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:51,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:51,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:51,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:52,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:52,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:52,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:53,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:53,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:53,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:54,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:54,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:54,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:55,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:55,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:55,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:56,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:56,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:57,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:57,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:57,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:58,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:58,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:58,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:59,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:59,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:59,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:00,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:00,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:00,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:01,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:01,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:01,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:02,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:02,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:02,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:03,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:03,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:03,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:04,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:04,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:04,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:05,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:05,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:05,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:06,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:06,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:07,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:07,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:07,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:09,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:09,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:09,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:10,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:10,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:10,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:11,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:11,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:11,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:12,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:12,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:12,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:13,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:13,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:13,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:14,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:14,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:14,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:15,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:15,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:15,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:16,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:16,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:16,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:17,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:17,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:17,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:18,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:18,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:18,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:19,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:19,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:19,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:21,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:21,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:21,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:22,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:22,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:22,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:23,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:23,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:23,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:24,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:24,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:24,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:25,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:25,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:26,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:26,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:26,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:27,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:27,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:27,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:28,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:28,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:29,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:29,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:29,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:30,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:30,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:30,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:31,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:31,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:31,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:32,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:32,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:32,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:33,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:33,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:34,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:34,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:34,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:35,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:35,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:35,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:36,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:36,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:36,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:37,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:37,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:38,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:38,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:38,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:39,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:39,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:39,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:40,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:40,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:40,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:41,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:41,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:41,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:42,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:42,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:43,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:43,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:43,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:44,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:44,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:44,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:45,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:45,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:45,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:46,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:46,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:46,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:47,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:47,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:47,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:48,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:48,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:49,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:49,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:49,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:49,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:50,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:50,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:50,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:51,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:51,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:51,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:52,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:52,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:52,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:53,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:53,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:53,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:53,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:54,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:54,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:54,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:55,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:55,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:55,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:56,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:56,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:56,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:57,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:57,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:57,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:59,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:59,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:59,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:01,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:01,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:03,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:03,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:03,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:04,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:04,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:05,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:05,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:05,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:06,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:06,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:06,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:07,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:07,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:07,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:08,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:08,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:08,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:09,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:09,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:09,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:10,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:10,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:11,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:11,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:11,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:12,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:12,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:12,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:13,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:13,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:14,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:14,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:14,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:14,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:15,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:15,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:15,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:16,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:16,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:17,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:17,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:17,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:18,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:18,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:18,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:19,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:19,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:20,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:20,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:20,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:21,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:21,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:21,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:22,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:22,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:23,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:23,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:23,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:24,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:24,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:24,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:25,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:25,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:25,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:26,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:26,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:27,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:27,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:27,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:28,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:28,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:29,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:29,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:29,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:30,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:30,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:30,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:31,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:31,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:31,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:32,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:32,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:33,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:33,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:33,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:34,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:34,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:34,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:35,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:35,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:35,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:36,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:36,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:36,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:37,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:37,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:37,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:38,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:38,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:38,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:39,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:39,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:40,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:40,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:40,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:41,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:41,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:41,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:42,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:42,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:42,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:43,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:43,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:43,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:44,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:44,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:44,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:45,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:45,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:46,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:46,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:47,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:47,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:48,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:48,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:48,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:48,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:49,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:49,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:49,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:50,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:50,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:50,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:51,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:51,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:51,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:52,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:52,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:52,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:53,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:53,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:53,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:54,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:54,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:54,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:55,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:55,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:55,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:55,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:56,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:56,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:56,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:58,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:58,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:58,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:00,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:00,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:00,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:01,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:01,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:01,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:02,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:02,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:02,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:02,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:03,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:03,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:04,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:04,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:04,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:05,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:05,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:05,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:06,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:06,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:06,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:07,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:07,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:07,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:08,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:08,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:08,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:10,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:10,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:10,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:11,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:11,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:11,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:12,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:12,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:12,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:13,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:13,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:13,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:15,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:15,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:15,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:16,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:16,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:16,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:17,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:17,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:17,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:17,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:18,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:18,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:18,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:19,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:19,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:19,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:20,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:20,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:20,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:21,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:21,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:21,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:23,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:23,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:24,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:24,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:24,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:25,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:25,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:25,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:25,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:26,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:26,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:26,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:27,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:27,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:27,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:28,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:28,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:28,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:29,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:29,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:29,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:30,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:30,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:32,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:32,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:33,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:33,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:33,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:34,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:34,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:35,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:35,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:35,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:36,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:36,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:36,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:37,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:37,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:38,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:38,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:38,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:39,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:39,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:39,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:40,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:40,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:41,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:41,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:41,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:42,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:42,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:42,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:43,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:43,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:43,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:44,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:44,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:44,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:44,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:45,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:45,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:46,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:46,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:46,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:47,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:47,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:47,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:48,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:48,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:50,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:50,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:50,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:51,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:51,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:51,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:52,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:52,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:53,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:53,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:53,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:54,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:54,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:54,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:55,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:55,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:55,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:56,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:56,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:56,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:57,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:57,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:57,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:58,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:58,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:59,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:59,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:00,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:00,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:00,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:01,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:01,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:02,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:02,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:02,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:03,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:03,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:04,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:04,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:04,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:04,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:05,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:05,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:05,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:06,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:06,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:06,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:07,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:07,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:07,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:08,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:08,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:08,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:09,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:09,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:09,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:10,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:10,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:10,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:11,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:11,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:11,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:12,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:12,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:12,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:13,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:13,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:13,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:14,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:14,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:15,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:15,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:16,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:16,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:16,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:17,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:17,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:18,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:18,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:18,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:19,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:19,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:19,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:20,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:20,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:20,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:21,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:21,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:21,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:22,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:22,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:23,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:23,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:23,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:24,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:24,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:24,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:25,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:25,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:25,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:26,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:26,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:26,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:27,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:27,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:27,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:28,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:28,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:28,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:29,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:29,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:29,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:30,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:30,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:30,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:31,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:31,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:32,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:32,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:33,147][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2793, device='cuda:0') eval_epoch_loss=tensor(0.2463, device='cuda:0') eval_epoch_acc=tensor(0.9418, device='cuda:0')
[2025-02-13 20:04:33,148][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:04:33,149][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:04:33,393][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_5349_loss_0.2462950050830841/model.pt
[2025-02-13 20:04:33,398][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:04:33,398][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.2462950050830841
[2025-02-13 20:04:33,399][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9417964220046997
[2025-02-13 20:04:33,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:33,807][root][INFO] - Training Epoch: 1/2, step 5349/7134 completed (loss: 0.4334340989589691, acc: 0.8971428275108337)
[2025-02-13 20:04:33,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:34,187][root][INFO] - Training Epoch: 1/2, step 5350/7134 completed (loss: 0.1777186393737793, acc: 0.9411764740943909)
[2025-02-13 20:04:34,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:34,610][root][INFO] - Training Epoch: 1/2, step 5351/7134 completed (loss: 0.2618526220321655, acc: 0.929729700088501)
[2025-02-13 20:04:34,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:34,947][root][INFO] - Training Epoch: 1/2, step 5352/7134 completed (loss: 0.2123231738805771, acc: 0.9580838084220886)
[2025-02-13 20:04:35,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:35,337][root][INFO] - Training Epoch: 1/2, step 5353/7134 completed (loss: 0.1638754904270172, acc: 0.9545454382896423)
[2025-02-13 20:04:35,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:35,728][root][INFO] - Training Epoch: 1/2, step 5354/7134 completed (loss: 0.2642911374568939, acc: 0.9305555820465088)
[2025-02-13 20:04:35,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:36,146][root][INFO] - Training Epoch: 1/2, step 5355/7134 completed (loss: 0.2841666042804718, acc: 0.9270833134651184)
[2025-02-13 20:04:36,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:36,530][root][INFO] - Training Epoch: 1/2, step 5356/7134 completed (loss: 0.24883690476417542, acc: 0.9308510422706604)
[2025-02-13 20:04:36,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:36,892][root][INFO] - Training Epoch: 1/2, step 5357/7134 completed (loss: 0.17961421608924866, acc: 0.9503546357154846)
[2025-02-13 20:04:37,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:37,311][root][INFO] - Training Epoch: 1/2, step 5358/7134 completed (loss: 0.434775710105896, acc: 0.9175823926925659)
[2025-02-13 20:04:37,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:37,694][root][INFO] - Training Epoch: 1/2, step 5359/7134 completed (loss: 0.14544373750686646, acc: 0.9753694534301758)
[2025-02-13 20:04:37,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:38,060][root][INFO] - Training Epoch: 1/2, step 5360/7134 completed (loss: 0.09728045016527176, acc: 0.9712643623352051)
[2025-02-13 20:04:38,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:38,436][root][INFO] - Training Epoch: 1/2, step 5361/7134 completed (loss: 0.31130000948905945, acc: 0.9289340376853943)
[2025-02-13 20:04:38,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:38,804][root][INFO] - Training Epoch: 1/2, step 5362/7134 completed (loss: 0.33387473225593567, acc: 0.9189189076423645)
[2025-02-13 20:04:38,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:39,164][root][INFO] - Training Epoch: 1/2, step 5363/7134 completed (loss: 0.24564838409423828, acc: 0.9329268336296082)
[2025-02-13 20:04:39,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:39,543][root][INFO] - Training Epoch: 1/2, step 5364/7134 completed (loss: 0.16352583467960358, acc: 0.9595375657081604)
[2025-02-13 20:04:39,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:39,941][root][INFO] - Training Epoch: 1/2, step 5365/7134 completed (loss: 0.11937527358531952, acc: 0.9621211886405945)
[2025-02-13 20:04:40,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:40,296][root][INFO] - Training Epoch: 1/2, step 5366/7134 completed (loss: 0.27829691767692566, acc: 0.9626865386962891)
[2025-02-13 20:04:40,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:40,698][root][INFO] - Training Epoch: 1/2, step 5367/7134 completed (loss: 0.2078057825565338, acc: 0.967391312122345)
[2025-02-13 20:04:40,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:41,085][root][INFO] - Training Epoch: 1/2, step 5368/7134 completed (loss: 0.2275143265724182, acc: 0.9457364082336426)
[2025-02-13 20:04:41,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:41,447][root][INFO] - Training Epoch: 1/2, step 5369/7134 completed (loss: 0.16117703914642334, acc: 0.9745762944221497)
[2025-02-13 20:04:41,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:41,851][root][INFO] - Training Epoch: 1/2, step 5370/7134 completed (loss: 0.2834119200706482, acc: 0.9545454382896423)
[2025-02-13 20:04:42,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:42,282][root][INFO] - Training Epoch: 1/2, step 5371/7134 completed (loss: 0.34870949387550354, acc: 0.9266666769981384)
[2025-02-13 20:04:42,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:42,661][root][INFO] - Training Epoch: 1/2, step 5372/7134 completed (loss: 0.077053502202034, acc: 0.9937106966972351)
[2025-02-13 20:04:42,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:42,992][root][INFO] - Training Epoch: 1/2, step 5373/7134 completed (loss: 0.10946066677570343, acc: 0.976190447807312)
[2025-02-13 20:04:43,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:43,366][root][INFO] - Training Epoch: 1/2, step 5374/7134 completed (loss: 0.08552171289920807, acc: 0.976190447807312)
[2025-02-13 20:04:43,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:43,758][root][INFO] - Training Epoch: 1/2, step 5375/7134 completed (loss: 0.09523443877696991, acc: 0.9922480583190918)
[2025-02-13 20:04:43,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:44,142][root][INFO] - Training Epoch: 1/2, step 5376/7134 completed (loss: 0.1100185364484787, acc: 0.9662162065505981)
[2025-02-13 20:04:44,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:44,540][root][INFO] - Training Epoch: 1/2, step 5377/7134 completed (loss: 0.11477525532245636, acc: 0.9753086566925049)
[2025-02-13 20:04:44,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:44,990][root][INFO] - Training Epoch: 1/2, step 5378/7134 completed (loss: 0.05298641696572304, acc: 0.9894737005233765)
[2025-02-13 20:04:45,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:45,403][root][INFO] - Training Epoch: 1/2, step 5379/7134 completed (loss: 0.07902941107749939, acc: 0.9895833134651184)
[2025-02-13 20:04:45,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:45,761][root][INFO] - Training Epoch: 1/2, step 5380/7134 completed (loss: 0.15331080555915833, acc: 0.970370352268219)
[2025-02-13 20:04:45,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:46,125][root][INFO] - Training Epoch: 1/2, step 5381/7134 completed (loss: 0.07939408719539642, acc: 0.9717513918876648)
[2025-02-13 20:04:46,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:46,523][root][INFO] - Training Epoch: 1/2, step 5382/7134 completed (loss: 0.0674930214881897, acc: 0.9927536249160767)
[2025-02-13 20:04:46,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:46,915][root][INFO] - Training Epoch: 1/2, step 5383/7134 completed (loss: 0.05687804892659187, acc: 0.9838709831237793)
[2025-02-13 20:04:47,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:47,302][root][INFO] - Training Epoch: 1/2, step 5384/7134 completed (loss: 0.08218413591384888, acc: 0.9796954393386841)
[2025-02-13 20:04:47,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:47,756][root][INFO] - Training Epoch: 1/2, step 5385/7134 completed (loss: 0.10776527225971222, acc: 0.9709302186965942)
[2025-02-13 20:04:47,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:48,167][root][INFO] - Training Epoch: 1/2, step 5386/7134 completed (loss: 0.0455535426735878, acc: 0.9890109896659851)
[2025-02-13 20:04:48,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:48,588][root][INFO] - Training Epoch: 1/2, step 5387/7134 completed (loss: 0.05484702065587044, acc: 0.9754098653793335)
[2025-02-13 20:04:48,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:49,015][root][INFO] - Training Epoch: 1/2, step 5388/7134 completed (loss: 0.048235226422548294, acc: 0.9888268113136292)
[2025-02-13 20:04:49,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:49,408][root][INFO] - Training Epoch: 1/2, step 5389/7134 completed (loss: 0.03117034211754799, acc: 0.9931972622871399)
[2025-02-13 20:04:49,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:49,775][root][INFO] - Training Epoch: 1/2, step 5390/7134 completed (loss: 0.20243506133556366, acc: 0.9496855139732361)
[2025-02-13 20:04:49,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:50,113][root][INFO] - Training Epoch: 1/2, step 5391/7134 completed (loss: 0.1415717601776123, acc: 0.9640287756919861)
[2025-02-13 20:04:50,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:50,472][root][INFO] - Training Epoch: 1/2, step 5392/7134 completed (loss: 0.20493732392787933, acc: 0.9659090638160706)
[2025-02-13 20:04:50,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:50,834][root][INFO] - Training Epoch: 1/2, step 5393/7134 completed (loss: 0.18020179867744446, acc: 0.9666666388511658)
[2025-02-13 20:04:50,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:51,197][root][INFO] - Training Epoch: 1/2, step 5394/7134 completed (loss: 0.507511556148529, acc: 0.8828125)
[2025-02-13 20:04:51,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:51,586][root][INFO] - Training Epoch: 1/2, step 5395/7134 completed (loss: 0.10461235791444778, acc: 0.9864864945411682)
[2025-02-13 20:04:51,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:51,970][root][INFO] - Training Epoch: 1/2, step 5396/7134 completed (loss: 0.10490693897008896, acc: 0.9802631735801697)
[2025-02-13 20:04:52,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:52,383][root][INFO] - Training Epoch: 1/2, step 5397/7134 completed (loss: 0.09852370619773865, acc: 0.978723406791687)
[2025-02-13 20:04:52,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:52,783][root][INFO] - Training Epoch: 1/2, step 5398/7134 completed (loss: 0.12310097366571426, acc: 0.9698795080184937)
[2025-02-13 20:04:52,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:53,153][root][INFO] - Training Epoch: 1/2, step 5399/7134 completed (loss: 0.11179248243570328, acc: 0.9485714435577393)
[2025-02-13 20:04:53,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:53,536][root][INFO] - Training Epoch: 1/2, step 5400/7134 completed (loss: 0.3144122362136841, acc: 0.9448275566101074)
[2025-02-13 20:04:53,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:53,906][root][INFO] - Training Epoch: 1/2, step 5401/7134 completed (loss: 0.10576921701431274, acc: 0.9722222089767456)
[2025-02-13 20:04:54,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:54,262][root][INFO] - Training Epoch: 1/2, step 5402/7134 completed (loss: 0.18866342306137085, acc: 0.9520547986030579)
[2025-02-13 20:04:54,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:54,656][root][INFO] - Training Epoch: 1/2, step 5403/7134 completed (loss: 0.20156995952129364, acc: 0.9590163826942444)
[2025-02-13 20:04:54,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:55,089][root][INFO] - Training Epoch: 1/2, step 5404/7134 completed (loss: 0.32006654143333435, acc: 0.926174521446228)
[2025-02-13 20:04:55,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:55,462][root][INFO] - Training Epoch: 1/2, step 5405/7134 completed (loss: 0.14586526155471802, acc: 0.95652174949646)
[2025-02-13 20:04:55,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:55,827][root][INFO] - Training Epoch: 1/2, step 5406/7134 completed (loss: 0.16822469234466553, acc: 0.9639639854431152)
[2025-02-13 20:04:55,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:56,181][root][INFO] - Training Epoch: 1/2, step 5407/7134 completed (loss: 0.23479460179805756, acc: 0.9230769276618958)
[2025-02-13 20:04:56,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:56,565][root][INFO] - Training Epoch: 1/2, step 5408/7134 completed (loss: 0.3102799952030182, acc: 0.9290780425071716)
[2025-02-13 20:04:56,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:56,934][root][INFO] - Training Epoch: 1/2, step 5409/7134 completed (loss: 0.30892330408096313, acc: 0.9510489702224731)
[2025-02-13 20:04:57,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:57,302][root][INFO] - Training Epoch: 1/2, step 5410/7134 completed (loss: 0.0761217400431633, acc: 0.9759036302566528)
[2025-02-13 20:04:57,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:57,663][root][INFO] - Training Epoch: 1/2, step 5411/7134 completed (loss: 0.25548404455184937, acc: 0.9507042169570923)
[2025-02-13 20:04:57,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:58,028][root][INFO] - Training Epoch: 1/2, step 5412/7134 completed (loss: 0.2342882752418518, acc: 0.9734513163566589)
[2025-02-13 20:04:58,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:58,412][root][INFO] - Training Epoch: 1/2, step 5413/7134 completed (loss: 0.37618887424468994, acc: 0.8888888955116272)
[2025-02-13 20:04:58,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:58,815][root][INFO] - Training Epoch: 1/2, step 5414/7134 completed (loss: 0.4430983364582062, acc: 0.8922155499458313)
[2025-02-13 20:04:58,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:59,201][root][INFO] - Training Epoch: 1/2, step 5415/7134 completed (loss: 0.35434797406196594, acc: 0.9350649118423462)
[2025-02-13 20:04:59,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:59,594][root][INFO] - Training Epoch: 1/2, step 5416/7134 completed (loss: 0.2784789502620697, acc: 0.9411764740943909)
[2025-02-13 20:04:59,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:59,972][root][INFO] - Training Epoch: 1/2, step 5417/7134 completed (loss: 0.14619478583335876, acc: 0.9615384340286255)
[2025-02-13 20:05:00,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:00,401][root][INFO] - Training Epoch: 1/2, step 5418/7134 completed (loss: 0.34124821424484253, acc: 0.9236111044883728)
[2025-02-13 20:05:00,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:00,821][root][INFO] - Training Epoch: 1/2, step 5419/7134 completed (loss: 0.6249405741691589, acc: 0.8467153310775757)
[2025-02-13 20:05:00,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:01,231][root][INFO] - Training Epoch: 1/2, step 5420/7134 completed (loss: 0.6145024299621582, acc: 0.8270676732063293)
[2025-02-13 20:05:01,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:01,653][root][INFO] - Training Epoch: 1/2, step 5421/7134 completed (loss: 0.5109944343566895, acc: 0.8812500238418579)
[2025-02-13 20:05:01,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:02,057][root][INFO] - Training Epoch: 1/2, step 5422/7134 completed (loss: 0.21214911341667175, acc: 0.9513888955116272)
[2025-02-13 20:05:02,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:02,504][root][INFO] - Training Epoch: 1/2, step 5423/7134 completed (loss: 0.18693216145038605, acc: 0.9750000238418579)
[2025-02-13 20:05:02,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:02,896][root][INFO] - Training Epoch: 1/2, step 5424/7134 completed (loss: 0.2193240374326706, acc: 0.9461538195610046)
[2025-02-13 20:05:03,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:03,242][root][INFO] - Training Epoch: 1/2, step 5425/7134 completed (loss: 0.15663769841194153, acc: 0.953125)
[2025-02-13 20:05:03,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:03,607][root][INFO] - Training Epoch: 1/2, step 5426/7134 completed (loss: 0.252259224653244, acc: 0.9590163826942444)
[2025-02-13 20:05:03,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:03,973][root][INFO] - Training Epoch: 1/2, step 5427/7134 completed (loss: 0.12054426968097687, acc: 0.9803921580314636)
[2025-02-13 20:05:04,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:04,327][root][INFO] - Training Epoch: 1/2, step 5428/7134 completed (loss: 0.15837769210338593, acc: 0.951724112033844)
[2025-02-13 20:05:04,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:04,722][root][INFO] - Training Epoch: 1/2, step 5429/7134 completed (loss: 0.1877700686454773, acc: 0.9597315192222595)
[2025-02-13 20:05:04,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:05,133][root][INFO] - Training Epoch: 1/2, step 5430/7134 completed (loss: 0.27532878518104553, acc: 0.9545454382896423)
[2025-02-13 20:05:05,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:05,566][root][INFO] - Training Epoch: 1/2, step 5431/7134 completed (loss: 0.24729633331298828, acc: 0.9097744226455688)
[2025-02-13 20:05:05,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:05,935][root][INFO] - Training Epoch: 1/2, step 5432/7134 completed (loss: 0.12529967725276947, acc: 0.96875)
[2025-02-13 20:05:06,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:06,363][root][INFO] - Training Epoch: 1/2, step 5433/7134 completed (loss: 0.34005722403526306, acc: 0.9411764740943909)
[2025-02-13 20:05:06,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:06,752][root][INFO] - Training Epoch: 1/2, step 5434/7134 completed (loss: 0.26243987679481506, acc: 0.9354838728904724)
[2025-02-13 20:05:06,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:07,137][root][INFO] - Training Epoch: 1/2, step 5435/7134 completed (loss: 0.2233961671590805, acc: 0.9520000219345093)
[2025-02-13 20:05:07,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:07,525][root][INFO] - Training Epoch: 1/2, step 5436/7134 completed (loss: 0.19445137679576874, acc: 0.9548386931419373)
[2025-02-13 20:05:07,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:07,917][root][INFO] - Training Epoch: 1/2, step 5437/7134 completed (loss: 0.19388684630393982, acc: 0.969924807548523)
[2025-02-13 20:05:08,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:08,287][root][INFO] - Training Epoch: 1/2, step 5438/7134 completed (loss: 0.26554521918296814, acc: 0.9277777671813965)
[2025-02-13 20:05:08,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:08,688][root][INFO] - Training Epoch: 1/2, step 5439/7134 completed (loss: 0.3666648864746094, acc: 0.9151515364646912)
[2025-02-13 20:05:08,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:09,115][root][INFO] - Training Epoch: 1/2, step 5440/7134 completed (loss: 0.20429539680480957, acc: 0.9166666865348816)
[2025-02-13 20:05:09,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:09,472][root][INFO] - Training Epoch: 1/2, step 5441/7134 completed (loss: 0.26160213351249695, acc: 0.9189189076423645)
[2025-02-13 20:05:09,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:09,885][root][INFO] - Training Epoch: 1/2, step 5442/7134 completed (loss: 0.14201200008392334, acc: 0.976190447807312)
[2025-02-13 20:05:10,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:10,323][root][INFO] - Training Epoch: 1/2, step 5443/7134 completed (loss: 0.14701470732688904, acc: 0.9634146094322205)
[2025-02-13 20:05:10,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:10,723][root][INFO] - Training Epoch: 1/2, step 5444/7134 completed (loss: 0.1637915074825287, acc: 0.9558823704719543)
[2025-02-13 20:05:10,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:11,132][root][INFO] - Training Epoch: 1/2, step 5445/7134 completed (loss: 0.2807001769542694, acc: 0.9264705777168274)
[2025-02-13 20:05:11,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:11,522][root][INFO] - Training Epoch: 1/2, step 5446/7134 completed (loss: 0.148493692278862, acc: 0.9509202241897583)
[2025-02-13 20:05:11,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:11,875][root][INFO] - Training Epoch: 1/2, step 5447/7134 completed (loss: 0.26496028900146484, acc: 0.9178082346916199)
[2025-02-13 20:05:12,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:12,260][root][INFO] - Training Epoch: 1/2, step 5448/7134 completed (loss: 0.21742112934589386, acc: 0.9340659379959106)
[2025-02-13 20:05:12,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:12,634][root][INFO] - Training Epoch: 1/2, step 5449/7134 completed (loss: 0.21884191036224365, acc: 0.9672130942344666)
[2025-02-13 20:05:12,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:12,995][root][INFO] - Training Epoch: 1/2, step 5450/7134 completed (loss: 0.1588025987148285, acc: 0.9611111283302307)
[2025-02-13 20:05:13,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:13,367][root][INFO] - Training Epoch: 1/2, step 5451/7134 completed (loss: 0.09500932693481445, acc: 0.9704142212867737)
[2025-02-13 20:05:13,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:13,731][root][INFO] - Training Epoch: 1/2, step 5452/7134 completed (loss: 0.1308816820383072, acc: 0.9644970297813416)
[2025-02-13 20:05:13,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:14,126][root][INFO] - Training Epoch: 1/2, step 5453/7134 completed (loss: 0.2617757022380829, acc: 0.9277777671813965)
[2025-02-13 20:05:14,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:14,530][root][INFO] - Training Epoch: 1/2, step 5454/7134 completed (loss: 0.1447826623916626, acc: 0.9836956262588501)
[2025-02-13 20:05:14,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:14,905][root][INFO] - Training Epoch: 1/2, step 5455/7134 completed (loss: 0.17652344703674316, acc: 0.9572192430496216)
[2025-02-13 20:05:15,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:15,291][root][INFO] - Training Epoch: 1/2, step 5456/7134 completed (loss: 0.14376287162303925, acc: 0.9569892287254333)
[2025-02-13 20:05:15,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:15,690][root][INFO] - Training Epoch: 1/2, step 5457/7134 completed (loss: 0.18686193227767944, acc: 0.954023003578186)
[2025-02-13 20:05:15,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:16,095][root][INFO] - Training Epoch: 1/2, step 5458/7134 completed (loss: 0.13015177845954895, acc: 0.9613259434700012)
[2025-02-13 20:05:16,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:16,466][root][INFO] - Training Epoch: 1/2, step 5459/7134 completed (loss: 0.03549497574567795, acc: 0.987730085849762)
[2025-02-13 20:05:16,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:16,825][root][INFO] - Training Epoch: 1/2, step 5460/7134 completed (loss: 0.10110890120267868, acc: 0.9822485446929932)
[2025-02-13 20:05:16,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:17,172][root][INFO] - Training Epoch: 1/2, step 5461/7134 completed (loss: 0.2469615936279297, acc: 0.9174311757087708)
[2025-02-13 20:05:17,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:17,543][root][INFO] - Training Epoch: 1/2, step 5462/7134 completed (loss: 0.0758855938911438, acc: 0.9754601120948792)
[2025-02-13 20:05:17,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:17,920][root][INFO] - Training Epoch: 1/2, step 5463/7134 completed (loss: 0.24218401312828064, acc: 0.9567901492118835)
[2025-02-13 20:05:18,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:18,284][root][INFO] - Training Epoch: 1/2, step 5464/7134 completed (loss: 0.2199886292219162, acc: 0.9453551769256592)
[2025-02-13 20:05:18,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:18,652][root][INFO] - Training Epoch: 1/2, step 5465/7134 completed (loss: 0.08148057758808136, acc: 0.987500011920929)
[2025-02-13 20:05:18,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:19,020][root][INFO] - Training Epoch: 1/2, step 5466/7134 completed (loss: 0.1284460425376892, acc: 0.96517413854599)
[2025-02-13 20:05:19,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:19,408][root][INFO] - Training Epoch: 1/2, step 5467/7134 completed (loss: 0.08958166837692261, acc: 0.9701492786407471)
[2025-02-13 20:05:19,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:19,773][root][INFO] - Training Epoch: 1/2, step 5468/7134 completed (loss: 0.06494484841823578, acc: 0.9842932224273682)
[2025-02-13 20:05:19,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:20,168][root][INFO] - Training Epoch: 1/2, step 5469/7134 completed (loss: 0.2018340826034546, acc: 0.9417989253997803)
[2025-02-13 20:05:20,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:20,580][root][INFO] - Training Epoch: 1/2, step 5470/7134 completed (loss: 0.052020855247974396, acc: 0.9900497794151306)
[2025-02-13 20:05:20,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:20,955][root][INFO] - Training Epoch: 1/2, step 5471/7134 completed (loss: 0.044256582856178284, acc: 0.9882352948188782)
[2025-02-13 20:05:21,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:21,358][root][INFO] - Training Epoch: 1/2, step 5472/7134 completed (loss: 0.09287410974502563, acc: 0.9832402467727661)
[2025-02-13 20:05:21,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:21,731][root][INFO] - Training Epoch: 1/2, step 5473/7134 completed (loss: 0.02956099435687065, acc: 1.0)
[2025-02-13 20:05:21,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:22,115][root][INFO] - Training Epoch: 1/2, step 5474/7134 completed (loss: 0.15184786915779114, acc: 0.9636363387107849)
[2025-02-13 20:05:22,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:22,492][root][INFO] - Training Epoch: 1/2, step 5475/7134 completed (loss: 0.07054683566093445, acc: 0.9814814925193787)
[2025-02-13 20:05:22,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:22,872][root][INFO] - Training Epoch: 1/2, step 5476/7134 completed (loss: 0.017070526257157326, acc: 0.9935064911842346)
[2025-02-13 20:05:23,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:23,242][root][INFO] - Training Epoch: 1/2, step 5477/7134 completed (loss: 0.10778900980949402, acc: 0.9846938848495483)
[2025-02-13 20:05:23,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:23,644][root][INFO] - Training Epoch: 1/2, step 5478/7134 completed (loss: 0.10546965897083282, acc: 0.9777777791023254)
[2025-02-13 20:05:23,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:24,033][root][INFO] - Training Epoch: 1/2, step 5479/7134 completed (loss: 0.2210438996553421, acc: 0.9459459185600281)
[2025-02-13 20:05:24,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:24,468][root][INFO] - Training Epoch: 1/2, step 5480/7134 completed (loss: 0.21800515055656433, acc: 0.9404761791229248)
[2025-02-13 20:05:24,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:24,882][root][INFO] - Training Epoch: 1/2, step 5481/7134 completed (loss: 0.169908344745636, acc: 0.9754098653793335)
[2025-02-13 20:05:25,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:25,497][root][INFO] - Training Epoch: 1/2, step 5482/7134 completed (loss: 0.4595659077167511, acc: 0.9020618796348572)
[2025-02-13 20:05:25,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:25,914][root][INFO] - Training Epoch: 1/2, step 5483/7134 completed (loss: 0.2008650153875351, acc: 0.939393937587738)
[2025-02-13 20:05:26,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:26,279][root][INFO] - Training Epoch: 1/2, step 5484/7134 completed (loss: 0.17643868923187256, acc: 0.9506173133850098)
[2025-02-13 20:05:26,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:26,663][root][INFO] - Training Epoch: 1/2, step 5485/7134 completed (loss: 0.21781831979751587, acc: 0.9354838728904724)
[2025-02-13 20:05:26,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:27,039][root][INFO] - Training Epoch: 1/2, step 5486/7134 completed (loss: 0.2790546119213104, acc: 0.9318181872367859)
[2025-02-13 20:05:27,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:27,405][root][INFO] - Training Epoch: 1/2, step 5487/7134 completed (loss: 0.17154286801815033, acc: 0.9523809552192688)
[2025-02-13 20:05:27,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:27,819][root][INFO] - Training Epoch: 1/2, step 5488/7134 completed (loss: 0.307009220123291, acc: 0.9281437397003174)
[2025-02-13 20:05:27,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:28,237][root][INFO] - Training Epoch: 1/2, step 5489/7134 completed (loss: 0.516281247138977, acc: 0.8536585569381714)
[2025-02-13 20:05:28,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:28,585][root][INFO] - Training Epoch: 1/2, step 5490/7134 completed (loss: 0.20558792352676392, acc: 0.939393937587738)
[2025-02-13 20:05:28,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:29,016][root][INFO] - Training Epoch: 1/2, step 5491/7134 completed (loss: 0.14175164699554443, acc: 0.9590643048286438)
[2025-02-13 20:05:29,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:29,377][root][INFO] - Training Epoch: 1/2, step 5492/7134 completed (loss: 0.19801278412342072, acc: 0.9552238583564758)
[2025-02-13 20:05:29,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:29,738][root][INFO] - Training Epoch: 1/2, step 5493/7134 completed (loss: 0.3081459105014801, acc: 0.8958333134651184)
[2025-02-13 20:05:29,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:30,102][root][INFO] - Training Epoch: 1/2, step 5494/7134 completed (loss: 0.16572394967079163, acc: 0.955974817276001)
[2025-02-13 20:05:30,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:30,508][root][INFO] - Training Epoch: 1/2, step 5495/7134 completed (loss: 0.14096814393997192, acc: 0.9548872113227844)
[2025-02-13 20:05:30,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:30,936][root][INFO] - Training Epoch: 1/2, step 5496/7134 completed (loss: 0.25091540813446045, acc: 0.9682539701461792)
[2025-02-13 20:05:31,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:31,338][root][INFO] - Training Epoch: 1/2, step 5497/7134 completed (loss: 0.26296839118003845, acc: 0.9235293865203857)
[2025-02-13 20:05:31,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:31,716][root][INFO] - Training Epoch: 1/2, step 5498/7134 completed (loss: 0.24037089943885803, acc: 0.9468085169792175)
[2025-02-13 20:05:31,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:32,134][root][INFO] - Training Epoch: 1/2, step 5499/7134 completed (loss: 0.251129150390625, acc: 0.9279279112815857)
[2025-02-13 20:05:32,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:32,503][root][INFO] - Training Epoch: 1/2, step 5500/7134 completed (loss: 0.12785986065864563, acc: 0.9689119458198547)
[2025-02-13 20:05:32,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:32,877][root][INFO] - Training Epoch: 1/2, step 5501/7134 completed (loss: 0.23004741966724396, acc: 0.9343434572219849)
[2025-02-13 20:05:33,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:33,245][root][INFO] - Training Epoch: 1/2, step 5502/7134 completed (loss: 0.16477851569652557, acc: 0.9604519605636597)
[2025-02-13 20:05:33,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:33,634][root][INFO] - Training Epoch: 1/2, step 5503/7134 completed (loss: 0.41702955961227417, acc: 0.9230769276618958)
[2025-02-13 20:05:33,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:34,033][root][INFO] - Training Epoch: 1/2, step 5504/7134 completed (loss: 0.21969084441661835, acc: 0.9277108311653137)
[2025-02-13 20:05:34,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:34,456][root][INFO] - Training Epoch: 1/2, step 5505/7134 completed (loss: 0.3931456506252289, acc: 0.8863636255264282)
[2025-02-13 20:05:34,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:34,851][root][INFO] - Training Epoch: 1/2, step 5506/7134 completed (loss: 0.26541727781295776, acc: 0.9242424368858337)
[2025-02-13 20:05:34,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:35,222][root][INFO] - Training Epoch: 1/2, step 5507/7134 completed (loss: 0.22141148149967194, acc: 0.9298245906829834)
[2025-02-13 20:05:35,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:35,618][root][INFO] - Training Epoch: 1/2, step 5508/7134 completed (loss: 0.18827837705612183, acc: 0.9425837397575378)
[2025-02-13 20:05:35,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:36,012][root][INFO] - Training Epoch: 1/2, step 5509/7134 completed (loss: 0.26682403683662415, acc: 0.9478672742843628)
[2025-02-13 20:05:36,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:36,383][root][INFO] - Training Epoch: 1/2, step 5510/7134 completed (loss: 0.2935502529144287, acc: 0.9247311949729919)
[2025-02-13 20:05:36,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:36,787][root][INFO] - Training Epoch: 1/2, step 5511/7134 completed (loss: 0.2266090363264084, acc: 0.9417475461959839)
[2025-02-13 20:05:36,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:37,217][root][INFO] - Training Epoch: 1/2, step 5512/7134 completed (loss: 0.28649431467056274, acc: 0.9230769276618958)
[2025-02-13 20:05:37,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:37,581][root][INFO] - Training Epoch: 1/2, step 5513/7134 completed (loss: 0.2971961200237274, acc: 0.9053254723548889)
[2025-02-13 20:05:37,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:37,982][root][INFO] - Training Epoch: 1/2, step 5514/7134 completed (loss: 0.36085185408592224, acc: 0.8981481194496155)
[2025-02-13 20:05:38,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:38,347][root][INFO] - Training Epoch: 1/2, step 5515/7134 completed (loss: 0.26078635454177856, acc: 0.9404761791229248)
[2025-02-13 20:05:38,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:38,754][root][INFO] - Training Epoch: 1/2, step 5516/7134 completed (loss: 0.19186481833457947, acc: 0.977011501789093)
[2025-02-13 20:05:38,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:39,161][root][INFO] - Training Epoch: 1/2, step 5517/7134 completed (loss: 0.19738814234733582, acc: 0.9553072452545166)
[2025-02-13 20:05:39,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:39,550][root][INFO] - Training Epoch: 1/2, step 5518/7134 completed (loss: 0.21086302399635315, acc: 0.9333333373069763)
[2025-02-13 20:05:39,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:39,930][root][INFO] - Training Epoch: 1/2, step 5519/7134 completed (loss: 0.2096220850944519, acc: 0.9382715821266174)
[2025-02-13 20:05:40,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:40,311][root][INFO] - Training Epoch: 1/2, step 5520/7134 completed (loss: 0.22265665233135223, acc: 0.9336493015289307)
[2025-02-13 20:05:40,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:40,703][root][INFO] - Training Epoch: 1/2, step 5521/7134 completed (loss: 0.28677499294281006, acc: 0.9005235433578491)
[2025-02-13 20:05:40,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:41,085][root][INFO] - Training Epoch: 1/2, step 5522/7134 completed (loss: 0.3009406328201294, acc: 0.8952879309654236)
[2025-02-13 20:05:41,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:41,431][root][INFO] - Training Epoch: 1/2, step 5523/7134 completed (loss: 0.21645691990852356, acc: 0.9285714030265808)
[2025-02-13 20:05:41,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:41,777][root][INFO] - Training Epoch: 1/2, step 5524/7134 completed (loss: 0.31326112151145935, acc: 0.9113923907279968)
[2025-02-13 20:05:41,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:42,145][root][INFO] - Training Epoch: 1/2, step 5525/7134 completed (loss: 0.1279694139957428, acc: 0.9634146094322205)
[2025-02-13 20:05:42,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:42,510][root][INFO] - Training Epoch: 1/2, step 5526/7134 completed (loss: 0.17023880779743195, acc: 0.95652174949646)
[2025-02-13 20:05:42,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:42,878][root][INFO] - Training Epoch: 1/2, step 5527/7134 completed (loss: 0.13661527633666992, acc: 0.9720279574394226)
[2025-02-13 20:05:43,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:43,265][root][INFO] - Training Epoch: 1/2, step 5528/7134 completed (loss: 0.1796754151582718, acc: 0.9545454382896423)
[2025-02-13 20:05:43,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:43,655][root][INFO] - Training Epoch: 1/2, step 5529/7134 completed (loss: 0.15065449476242065, acc: 0.9636363387107849)
[2025-02-13 20:05:43,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:44,043][root][INFO] - Training Epoch: 1/2, step 5530/7134 completed (loss: 0.09276608377695084, acc: 0.9727891087532043)
[2025-02-13 20:05:44,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:44,414][root][INFO] - Training Epoch: 1/2, step 5531/7134 completed (loss: 0.1353265643119812, acc: 0.9759036302566528)
[2025-02-13 20:05:44,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:44,791][root][INFO] - Training Epoch: 1/2, step 5532/7134 completed (loss: 0.19583271443843842, acc: 0.9435028433799744)
[2025-02-13 20:05:44,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:45,167][root][INFO] - Training Epoch: 1/2, step 5533/7134 completed (loss: 0.05701589584350586, acc: 0.9944444298744202)
[2025-02-13 20:05:45,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:45,540][root][INFO] - Training Epoch: 1/2, step 5534/7134 completed (loss: 0.3132345378398895, acc: 0.9382022619247437)
[2025-02-13 20:05:45,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:45,918][root][INFO] - Training Epoch: 1/2, step 5535/7134 completed (loss: 0.12447647005319595, acc: 0.984455943107605)
[2025-02-13 20:05:46,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:46,277][root][INFO] - Training Epoch: 1/2, step 5536/7134 completed (loss: 0.13700369000434875, acc: 0.9726775884628296)
[2025-02-13 20:05:46,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:46,629][root][INFO] - Training Epoch: 1/2, step 5537/7134 completed (loss: 0.17859439551830292, acc: 0.959770143032074)
[2025-02-13 20:05:46,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:46,993][root][INFO] - Training Epoch: 1/2, step 5538/7134 completed (loss: 0.14584824442863464, acc: 0.9685863852500916)
[2025-02-13 20:05:47,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:47,385][root][INFO] - Training Epoch: 1/2, step 5539/7134 completed (loss: 0.15805578231811523, acc: 0.9617486596107483)
[2025-02-13 20:05:47,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:47,758][root][INFO] - Training Epoch: 1/2, step 5540/7134 completed (loss: 0.19230249524116516, acc: 0.9518072009086609)
[2025-02-13 20:05:47,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:48,132][root][INFO] - Training Epoch: 1/2, step 5541/7134 completed (loss: 0.11317285150289536, acc: 0.9883720874786377)
[2025-02-13 20:05:48,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:48,497][root][INFO] - Training Epoch: 1/2, step 5542/7134 completed (loss: 0.05263623967766762, acc: 0.9806451797485352)
[2025-02-13 20:05:48,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:48,869][root][INFO] - Training Epoch: 1/2, step 5543/7134 completed (loss: 0.10608688741922379, acc: 0.9839572310447693)
[2025-02-13 20:05:49,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:49,238][root][INFO] - Training Epoch: 1/2, step 5544/7134 completed (loss: 0.0761144757270813, acc: 0.9890710115432739)
[2025-02-13 20:05:49,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:49,595][root][INFO] - Training Epoch: 1/2, step 5545/7134 completed (loss: 0.12603861093521118, acc: 0.9642857313156128)
[2025-02-13 20:05:49,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:49,956][root][INFO] - Training Epoch: 1/2, step 5546/7134 completed (loss: 0.10781259834766388, acc: 0.976047933101654)
[2025-02-13 20:05:50,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:50,325][root][INFO] - Training Epoch: 1/2, step 5547/7134 completed (loss: 0.11995340883731842, acc: 0.9723756909370422)
[2025-02-13 20:05:50,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:50,692][root][INFO] - Training Epoch: 1/2, step 5548/7134 completed (loss: 0.16368065774440765, acc: 0.970588207244873)
[2025-02-13 20:05:50,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:51,057][root][INFO] - Training Epoch: 1/2, step 5549/7134 completed (loss: 0.13428747653961182, acc: 0.9597315192222595)
[2025-02-13 20:05:51,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:51,424][root][INFO] - Training Epoch: 1/2, step 5550/7134 completed (loss: 0.08838321268558502, acc: 0.983146071434021)
[2025-02-13 20:05:51,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:51,772][root][INFO] - Training Epoch: 1/2, step 5551/7134 completed (loss: 0.30417925119400024, acc: 0.9354838728904724)
[2025-02-13 20:05:51,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:52,145][root][INFO] - Training Epoch: 1/2, step 5552/7134 completed (loss: 0.04246950149536133, acc: 0.9938650131225586)
[2025-02-13 20:05:52,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:52,521][root][INFO] - Training Epoch: 1/2, step 5553/7134 completed (loss: 0.1758890599012375, acc: 0.9644970297813416)
[2025-02-13 20:05:52,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:52,869][root][INFO] - Training Epoch: 1/2, step 5554/7134 completed (loss: 0.11038419604301453, acc: 0.9808917045593262)
[2025-02-13 20:05:52,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:53,222][root][INFO] - Training Epoch: 1/2, step 5555/7134 completed (loss: 0.12985241413116455, acc: 0.976190447807312)
[2025-02-13 20:05:53,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:53,600][root][INFO] - Training Epoch: 1/2, step 5556/7134 completed (loss: 0.24893981218338013, acc: 0.9553072452545166)
[2025-02-13 20:05:53,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:53,989][root][INFO] - Training Epoch: 1/2, step 5557/7134 completed (loss: 0.17834946513175964, acc: 0.95652174949646)
[2025-02-13 20:05:54,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:54,390][root][INFO] - Training Epoch: 1/2, step 5558/7134 completed (loss: 0.163113534450531, acc: 0.9444444179534912)
[2025-02-13 20:05:54,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:54,780][root][INFO] - Training Epoch: 1/2, step 5559/7134 completed (loss: 0.24259012937545776, acc: 0.9608938694000244)
[2025-02-13 20:05:54,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:55,143][root][INFO] - Training Epoch: 1/2, step 5560/7134 completed (loss: 0.3283454179763794, acc: 0.9388889074325562)
[2025-02-13 20:05:55,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:55,517][root][INFO] - Training Epoch: 1/2, step 5561/7134 completed (loss: 0.21319179236888885, acc: 0.9590163826942444)
[2025-02-13 20:05:55,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:55,886][root][INFO] - Training Epoch: 1/2, step 5562/7134 completed (loss: 0.24829623103141785, acc: 0.9496402740478516)
[2025-02-13 20:05:56,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:56,249][root][INFO] - Training Epoch: 1/2, step 5563/7134 completed (loss: 0.11396785825490952, acc: 0.9722222089767456)
[2025-02-13 20:05:56,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:56,602][root][INFO] - Training Epoch: 1/2, step 5564/7134 completed (loss: 0.06986065208911896, acc: 0.9750000238418579)
[2025-02-13 20:05:56,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:56,971][root][INFO] - Training Epoch: 1/2, step 5565/7134 completed (loss: 0.06257082521915436, acc: 0.9800000190734863)
[2025-02-13 20:05:57,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:57,338][root][INFO] - Training Epoch: 1/2, step 5566/7134 completed (loss: 0.06030897796154022, acc: 0.989847719669342)
[2025-02-13 20:05:57,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:57,726][root][INFO] - Training Epoch: 1/2, step 5567/7134 completed (loss: 0.056099146604537964, acc: 0.9850746393203735)
[2025-02-13 20:05:57,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:58,086][root][INFO] - Training Epoch: 1/2, step 5568/7134 completed (loss: 0.05488944426178932, acc: 0.9890109896659851)
[2025-02-13 20:05:58,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:58,465][root][INFO] - Training Epoch: 1/2, step 5569/7134 completed (loss: 0.05536344647407532, acc: 0.9895833134651184)
[2025-02-13 20:05:58,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:58,832][root][INFO] - Training Epoch: 1/2, step 5570/7134 completed (loss: 0.09840424358844757, acc: 0.9836956262588501)
[2025-02-13 20:05:58,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:59,190][root][INFO] - Training Epoch: 1/2, step 5571/7134 completed (loss: 0.07227562367916107, acc: 0.9873417615890503)
[2025-02-13 20:05:59,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:59,559][root][INFO] - Training Epoch: 1/2, step 5572/7134 completed (loss: 0.08381354063749313, acc: 0.987730085849762)
[2025-02-13 20:05:59,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:59,943][root][INFO] - Training Epoch: 1/2, step 5573/7134 completed (loss: 0.10257529467344284, acc: 0.9803921580314636)
[2025-02-13 20:06:00,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:00,310][root][INFO] - Training Epoch: 1/2, step 5574/7134 completed (loss: 0.06433363258838654, acc: 0.9930555820465088)
[2025-02-13 20:06:00,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:00,691][root][INFO] - Training Epoch: 1/2, step 5575/7134 completed (loss: 0.27361300587654114, acc: 0.9399999976158142)
[2025-02-13 20:06:00,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:01,056][root][INFO] - Training Epoch: 1/2, step 5576/7134 completed (loss: 0.07826937735080719, acc: 0.988095223903656)
[2025-02-13 20:06:01,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:01,424][root][INFO] - Training Epoch: 1/2, step 5577/7134 completed (loss: 0.30081337690353394, acc: 0.9111111164093018)
[2025-02-13 20:06:01,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:01,790][root][INFO] - Training Epoch: 1/2, step 5578/7134 completed (loss: 0.18678615987300873, acc: 0.9746835231781006)
[2025-02-13 20:06:01,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:02,155][root][INFO] - Training Epoch: 1/2, step 5579/7134 completed (loss: 0.1428452879190445, acc: 0.9693251252174377)
[2025-02-13 20:06:02,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:02,508][root][INFO] - Training Epoch: 1/2, step 5580/7134 completed (loss: 0.11589976400136948, acc: 0.9777777791023254)
[2025-02-13 20:06:02,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:02,874][root][INFO] - Training Epoch: 1/2, step 5581/7134 completed (loss: 0.12710578739643097, acc: 0.9757575988769531)
[2025-02-13 20:06:03,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:03,248][root][INFO] - Training Epoch: 1/2, step 5582/7134 completed (loss: 0.13364443182945251, acc: 0.9613259434700012)
[2025-02-13 20:06:03,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:03,619][root][INFO] - Training Epoch: 1/2, step 5583/7134 completed (loss: 0.09689353406429291, acc: 0.9648241400718689)
[2025-02-13 20:06:03,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:03,986][root][INFO] - Training Epoch: 1/2, step 5584/7134 completed (loss: 0.14281073212623596, acc: 0.9683544039726257)
[2025-02-13 20:06:04,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:04,365][root][INFO] - Training Epoch: 1/2, step 5585/7134 completed (loss: 0.15320084989070892, acc: 0.9658536314964294)
[2025-02-13 20:06:04,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:04,745][root][INFO] - Training Epoch: 1/2, step 5586/7134 completed (loss: 0.11155778169631958, acc: 0.9863945841789246)
[2025-02-13 20:06:04,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:05,123][root][INFO] - Training Epoch: 1/2, step 5587/7134 completed (loss: 0.08138803392648697, acc: 0.9714285731315613)
[2025-02-13 20:06:05,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:05,502][root][INFO] - Training Epoch: 1/2, step 5588/7134 completed (loss: 0.05164959281682968, acc: 0.9938271641731262)
[2025-02-13 20:06:05,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:05,872][root][INFO] - Training Epoch: 1/2, step 5589/7134 completed (loss: 0.18750494718551636, acc: 0.9689440727233887)
[2025-02-13 20:06:06,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:06,226][root][INFO] - Training Epoch: 1/2, step 5590/7134 completed (loss: 0.17651912569999695, acc: 0.9653179049491882)
[2025-02-13 20:06:06,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:06,581][root][INFO] - Training Epoch: 1/2, step 5591/7134 completed (loss: 0.0562526173889637, acc: 0.9826589822769165)
[2025-02-13 20:06:06,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:06,934][root][INFO] - Training Epoch: 1/2, step 5592/7134 completed (loss: 0.08614551275968552, acc: 0.9825581312179565)
[2025-02-13 20:06:07,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:07,301][root][INFO] - Training Epoch: 1/2, step 5593/7134 completed (loss: 0.15917880833148956, acc: 0.967391312122345)
[2025-02-13 20:06:07,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:07,654][root][INFO] - Training Epoch: 1/2, step 5594/7134 completed (loss: 0.07141364365816116, acc: 0.9943181872367859)
[2025-02-13 20:06:07,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:08,018][root][INFO] - Training Epoch: 1/2, step 5595/7134 completed (loss: 0.1451493501663208, acc: 0.9772727489471436)
[2025-02-13 20:06:08,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:08,376][root][INFO] - Training Epoch: 1/2, step 5596/7134 completed (loss: 0.05514540523290634, acc: 0.9940119981765747)
[2025-02-13 20:06:08,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:08,750][root][INFO] - Training Epoch: 1/2, step 5597/7134 completed (loss: 0.10577176511287689, acc: 0.9879518151283264)
[2025-02-13 20:06:08,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:09,111][root][INFO] - Training Epoch: 1/2, step 5598/7134 completed (loss: 0.10121409595012665, acc: 0.9826589822769165)
[2025-02-13 20:06:09,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:09,485][root][INFO] - Training Epoch: 1/2, step 5599/7134 completed (loss: 0.1819917857646942, acc: 0.9591836929321289)
[2025-02-13 20:06:09,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:09,855][root][INFO] - Training Epoch: 1/2, step 5600/7134 completed (loss: 0.18946582078933716, acc: 0.9571428298950195)
[2025-02-13 20:06:09,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:10,202][root][INFO] - Training Epoch: 1/2, step 5601/7134 completed (loss: 0.12834422290325165, acc: 0.9541984796524048)
[2025-02-13 20:06:10,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:10,575][root][INFO] - Training Epoch: 1/2, step 5602/7134 completed (loss: 0.20083484053611755, acc: 0.9507042169570923)
[2025-02-13 20:06:10,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:10,936][root][INFO] - Training Epoch: 1/2, step 5603/7134 completed (loss: 0.152133509516716, acc: 0.9508196711540222)
[2025-02-13 20:06:11,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:11,282][root][INFO] - Training Epoch: 1/2, step 5604/7134 completed (loss: 0.24640479683876038, acc: 0.9430894255638123)
[2025-02-13 20:06:11,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:11,637][root][INFO] - Training Epoch: 1/2, step 5605/7134 completed (loss: 0.23139740526676178, acc: 0.9291338324546814)
[2025-02-13 20:06:11,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:12,002][root][INFO] - Training Epoch: 1/2, step 5606/7134 completed (loss: 0.2903675436973572, acc: 0.9345794320106506)
[2025-02-13 20:06:12,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:12,364][root][INFO] - Training Epoch: 1/2, step 5607/7134 completed (loss: 0.08416116237640381, acc: 0.9784172773361206)
[2025-02-13 20:06:12,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:12,755][root][INFO] - Training Epoch: 1/2, step 5608/7134 completed (loss: 0.17208582162857056, acc: 0.9534883499145508)
[2025-02-13 20:06:12,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:13,118][root][INFO] - Training Epoch: 1/2, step 5609/7134 completed (loss: 0.23362095654010773, acc: 0.9166666865348816)
[2025-02-13 20:06:13,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:13,467][root][INFO] - Training Epoch: 1/2, step 5610/7134 completed (loss: 0.0781744197010994, acc: 0.9727272987365723)
[2025-02-13 20:06:13,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:13,843][root][INFO] - Training Epoch: 1/2, step 5611/7134 completed (loss: 0.15597201883792877, acc: 0.9626865386962891)
[2025-02-13 20:06:13,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:14,240][root][INFO] - Training Epoch: 1/2, step 5612/7134 completed (loss: 0.23122964799404144, acc: 0.9507042169570923)
[2025-02-13 20:06:14,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:14,598][root][INFO] - Training Epoch: 1/2, step 5613/7134 completed (loss: 0.24493266642093658, acc: 0.9553571343421936)
[2025-02-13 20:06:14,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:14,949][root][INFO] - Training Epoch: 1/2, step 5614/7134 completed (loss: 0.31860220432281494, acc: 0.9463087320327759)
[2025-02-13 20:06:15,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:15,311][root][INFO] - Training Epoch: 1/2, step 5615/7134 completed (loss: 0.12728917598724365, acc: 0.9714285731315613)
[2025-02-13 20:06:15,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:15,663][root][INFO] - Training Epoch: 1/2, step 5616/7134 completed (loss: 0.13793249428272247, acc: 0.9629629850387573)
[2025-02-13 20:06:15,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:16,020][root][INFO] - Training Epoch: 1/2, step 5617/7134 completed (loss: 0.06719893962144852, acc: 0.9844961166381836)
[2025-02-13 20:06:16,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:16,390][root][INFO] - Training Epoch: 1/2, step 5618/7134 completed (loss: 0.14623229205608368, acc: 0.970588207244873)
[2025-02-13 20:06:16,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:16,734][root][INFO] - Training Epoch: 1/2, step 5619/7134 completed (loss: 0.18587028980255127, acc: 0.9327731132507324)
[2025-02-13 20:06:16,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:17,091][root][INFO] - Training Epoch: 1/2, step 5620/7134 completed (loss: 0.13292519748210907, acc: 0.95652174949646)
[2025-02-13 20:06:17,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:17,460][root][INFO] - Training Epoch: 1/2, step 5621/7134 completed (loss: 0.10752485692501068, acc: 0.9806451797485352)
[2025-02-13 20:06:17,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:17,833][root][INFO] - Training Epoch: 1/2, step 5622/7134 completed (loss: 0.15987400710582733, acc: 0.9518716335296631)
[2025-02-13 20:06:17,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:18,180][root][INFO] - Training Epoch: 1/2, step 5623/7134 completed (loss: 0.1660805642604828, acc: 0.9513888955116272)
[2025-02-13 20:06:18,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:18,555][root][INFO] - Training Epoch: 1/2, step 5624/7134 completed (loss: 0.10216239839792252, acc: 0.9738562107086182)
[2025-02-13 20:06:18,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:18,917][root][INFO] - Training Epoch: 1/2, step 5625/7134 completed (loss: 0.054169122129678726, acc: 0.9901960492134094)
[2025-02-13 20:06:19,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:19,275][root][INFO] - Training Epoch: 1/2, step 5626/7134 completed (loss: 0.08095338940620422, acc: 0.9724137783050537)
[2025-02-13 20:06:19,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:19,629][root][INFO] - Training Epoch: 1/2, step 5627/7134 completed (loss: 0.271420955657959, acc: 0.9583333134651184)
[2025-02-13 20:06:19,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:20,007][root][INFO] - Training Epoch: 1/2, step 5628/7134 completed (loss: 0.15354730188846588, acc: 0.9732142686843872)
[2025-02-13 20:06:20,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:20,383][root][INFO] - Training Epoch: 1/2, step 5629/7134 completed (loss: 0.1706048548221588, acc: 0.9508196711540222)
[2025-02-13 20:06:20,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:20,749][root][INFO] - Training Epoch: 1/2, step 5630/7134 completed (loss: 0.2957043945789337, acc: 0.9411764740943909)
[2025-02-13 20:06:20,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:21,102][root][INFO] - Training Epoch: 1/2, step 5631/7134 completed (loss: 0.08858340978622437, acc: 0.9918699264526367)
[2025-02-13 20:06:21,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:21,464][root][INFO] - Training Epoch: 1/2, step 5632/7134 completed (loss: 0.2296581268310547, acc: 0.9519230723381042)
[2025-02-13 20:06:21,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:21,823][root][INFO] - Training Epoch: 1/2, step 5633/7134 completed (loss: 0.11333408206701279, acc: 0.9732142686843872)
[2025-02-13 20:06:21,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:22,197][root][INFO] - Training Epoch: 1/2, step 5634/7134 completed (loss: 0.10422339290380478, acc: 0.9557521939277649)
[2025-02-13 20:06:22,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:22,573][root][INFO] - Training Epoch: 1/2, step 5635/7134 completed (loss: 0.07518851011991501, acc: 0.984375)
[2025-02-13 20:06:22,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:22,947][root][INFO] - Training Epoch: 1/2, step 5636/7134 completed (loss: 0.1292867213487625, acc: 0.9692307710647583)
[2025-02-13 20:06:23,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:23,315][root][INFO] - Training Epoch: 1/2, step 5637/7134 completed (loss: 0.12126310169696808, acc: 0.9772727489471436)
[2025-02-13 20:06:23,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:23,672][root][INFO] - Training Epoch: 1/2, step 5638/7134 completed (loss: 0.13266275823116302, acc: 0.9736841917037964)
[2025-02-13 20:06:23,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:24,046][root][INFO] - Training Epoch: 1/2, step 5639/7134 completed (loss: 0.10148116201162338, acc: 0.9740259647369385)
[2025-02-13 20:06:24,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:24,401][root][INFO] - Training Epoch: 1/2, step 5640/7134 completed (loss: 0.16199155151844025, acc: 0.9503546357154846)
[2025-02-13 20:06:24,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:24,740][root][INFO] - Training Epoch: 1/2, step 5641/7134 completed (loss: 0.16362611949443817, acc: 0.9550561904907227)
[2025-02-13 20:06:24,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:25,089][root][INFO] - Training Epoch: 1/2, step 5642/7134 completed (loss: 0.19617204368114471, acc: 0.9510489702224731)
[2025-02-13 20:06:25,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:25,439][root][INFO] - Training Epoch: 1/2, step 5643/7134 completed (loss: 0.2370678186416626, acc: 0.9368420839309692)
[2025-02-13 20:06:25,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:25,808][root][INFO] - Training Epoch: 1/2, step 5644/7134 completed (loss: 0.19015862047672272, acc: 0.9428571462631226)
[2025-02-13 20:06:25,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:26,165][root][INFO] - Training Epoch: 1/2, step 5645/7134 completed (loss: 0.10009317100048065, acc: 0.9652777910232544)
[2025-02-13 20:06:26,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:26,524][root][INFO] - Training Epoch: 1/2, step 5646/7134 completed (loss: 0.0872952863574028, acc: 0.9785714149475098)
[2025-02-13 20:06:26,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:26,896][root][INFO] - Training Epoch: 1/2, step 5647/7134 completed (loss: 0.17128294706344604, acc: 0.9513888955116272)
[2025-02-13 20:06:27,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:27,264][root][INFO] - Training Epoch: 1/2, step 5648/7134 completed (loss: 0.26874715089797974, acc: 0.9333333373069763)
[2025-02-13 20:06:27,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:27,617][root][INFO] - Training Epoch: 1/2, step 5649/7134 completed (loss: 0.2901812791824341, acc: 0.912162184715271)
[2025-02-13 20:06:27,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:27,991][root][INFO] - Training Epoch: 1/2, step 5650/7134 completed (loss: 0.19713851809501648, acc: 0.9435483813285828)
[2025-02-13 20:06:28,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:28,372][root][INFO] - Training Epoch: 1/2, step 5651/7134 completed (loss: 0.09487069398164749, acc: 0.9795918464660645)
[2025-02-13 20:06:28,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:28,767][root][INFO] - Training Epoch: 1/2, step 5652/7134 completed (loss: 0.2033998817205429, acc: 0.9280575513839722)
[2025-02-13 20:06:28,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:29,134][root][INFO] - Training Epoch: 1/2, step 5653/7134 completed (loss: 0.1906169354915619, acc: 0.948051929473877)
[2025-02-13 20:06:29,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:29,500][root][INFO] - Training Epoch: 1/2, step 5654/7134 completed (loss: 0.05622067302465439, acc: 0.9921875)
[2025-02-13 20:06:29,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:29,914][root][INFO] - Training Epoch: 1/2, step 5655/7134 completed (loss: 0.15999850630760193, acc: 0.9719626307487488)
[2025-02-13 20:06:30,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:30,277][root][INFO] - Training Epoch: 1/2, step 5656/7134 completed (loss: 0.14694543182849884, acc: 0.9672130942344666)
[2025-02-13 20:06:30,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:30,628][root][INFO] - Training Epoch: 1/2, step 5657/7134 completed (loss: 0.2101825624704361, acc: 0.9602649211883545)
[2025-02-13 20:06:30,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:31,002][root][INFO] - Training Epoch: 1/2, step 5658/7134 completed (loss: 0.14834997057914734, acc: 0.9607843160629272)
[2025-02-13 20:06:31,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:31,359][root][INFO] - Training Epoch: 1/2, step 5659/7134 completed (loss: 0.1415873020887375, acc: 0.9428571462631226)
[2025-02-13 20:06:31,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:31,721][root][INFO] - Training Epoch: 1/2, step 5660/7134 completed (loss: 0.20514144003391266, acc: 0.9453125)
[2025-02-13 20:06:31,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:32,094][root][INFO] - Training Epoch: 1/2, step 5661/7134 completed (loss: 0.13808004558086395, acc: 0.9747899174690247)
[2025-02-13 20:06:32,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:32,470][root][INFO] - Training Epoch: 1/2, step 5662/7134 completed (loss: 0.15290382504463196, acc: 0.9640287756919861)
[2025-02-13 20:06:32,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:32,791][root][INFO] - Training Epoch: 1/2, step 5663/7134 completed (loss: 0.057120345532894135, acc: 0.9927536249160767)
[2025-02-13 20:06:32,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:33,156][root][INFO] - Training Epoch: 1/2, step 5664/7134 completed (loss: 0.3165273666381836, acc: 0.9383561611175537)
[2025-02-13 20:06:33,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:33,520][root][INFO] - Training Epoch: 1/2, step 5665/7134 completed (loss: 0.06934812664985657, acc: 0.9931972622871399)
[2025-02-13 20:06:33,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:33,871][root][INFO] - Training Epoch: 1/2, step 5666/7134 completed (loss: 0.0928390622138977, acc: 0.9803921580314636)
[2025-02-13 20:06:33,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:34,219][root][INFO] - Training Epoch: 1/2, step 5667/7134 completed (loss: 0.22351449728012085, acc: 0.9512194991111755)
[2025-02-13 20:06:34,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:34,568][root][INFO] - Training Epoch: 1/2, step 5668/7134 completed (loss: 0.18717224895954132, acc: 0.970370352268219)
[2025-02-13 20:06:34,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:34,915][root][INFO] - Training Epoch: 1/2, step 5669/7134 completed (loss: 0.20785170793533325, acc: 0.9617834687232971)
[2025-02-13 20:06:35,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:35,324][root][INFO] - Training Epoch: 1/2, step 5670/7134 completed (loss: 0.06737776845693588, acc: 0.9841269850730896)
[2025-02-13 20:06:35,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:35,711][root][INFO] - Training Epoch: 1/2, step 5671/7134 completed (loss: 0.11807554960250854, acc: 0.966292142868042)
[2025-02-13 20:06:35,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:36,143][root][INFO] - Training Epoch: 1/2, step 5672/7134 completed (loss: 0.11952316761016846, acc: 0.9777777791023254)
[2025-02-13 20:06:36,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:36,494][root][INFO] - Training Epoch: 1/2, step 5673/7134 completed (loss: 0.11349393427371979, acc: 0.9794520735740662)
[2025-02-13 20:06:36,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:36,868][root][INFO] - Training Epoch: 1/2, step 5674/7134 completed (loss: 0.1795782744884491, acc: 0.9591836929321289)
[2025-02-13 20:06:37,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:37,242][root][INFO] - Training Epoch: 1/2, step 5675/7134 completed (loss: 0.2945833206176758, acc: 0.9066666960716248)
[2025-02-13 20:06:37,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:37,614][root][INFO] - Training Epoch: 1/2, step 5676/7134 completed (loss: 0.3346406817436218, acc: 0.9022988677024841)
[2025-02-13 20:06:37,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:37,991][root][INFO] - Training Epoch: 1/2, step 5677/7134 completed (loss: 0.31753212213516235, acc: 0.9120879173278809)
[2025-02-13 20:06:38,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:38,363][root][INFO] - Training Epoch: 1/2, step 5678/7134 completed (loss: 0.14137309789657593, acc: 0.9691358208656311)
[2025-02-13 20:06:38,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:38,787][root][INFO] - Training Epoch: 1/2, step 5679/7134 completed (loss: 0.46859100461006165, acc: 0.8860759735107422)
[2025-02-13 20:06:38,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:39,165][root][INFO] - Training Epoch: 1/2, step 5680/7134 completed (loss: 0.5585342049598694, acc: 0.8913043737411499)
[2025-02-13 20:06:39,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:39,553][root][INFO] - Training Epoch: 1/2, step 5681/7134 completed (loss: 0.41438597440719604, acc: 0.8787878751754761)
[2025-02-13 20:06:39,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:39,934][root][INFO] - Training Epoch: 1/2, step 5682/7134 completed (loss: 0.2532518208026886, acc: 0.9337349534034729)
[2025-02-13 20:06:40,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:40,367][root][INFO] - Training Epoch: 1/2, step 5683/7134 completed (loss: 0.3404325246810913, acc: 0.9222221970558167)
[2025-02-13 20:06:40,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:40,764][root][INFO] - Training Epoch: 1/2, step 5684/7134 completed (loss: 0.4715718924999237, acc: 0.8902438879013062)
[2025-02-13 20:06:40,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:41,141][root][INFO] - Training Epoch: 1/2, step 5685/7134 completed (loss: 0.2695091962814331, acc: 0.9212598204612732)
[2025-02-13 20:06:41,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:41,514][root][INFO] - Training Epoch: 1/2, step 5686/7134 completed (loss: 0.2897931635379791, acc: 0.9064748287200928)
[2025-02-13 20:06:41,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:41,896][root][INFO] - Training Epoch: 1/2, step 5687/7134 completed (loss: 0.30521026253700256, acc: 0.9242424368858337)
[2025-02-13 20:06:42,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:42,272][root][INFO] - Training Epoch: 1/2, step 5688/7134 completed (loss: 0.4676159620285034, acc: 0.9277777671813965)
[2025-02-13 20:06:42,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:42,678][root][INFO] - Training Epoch: 1/2, step 5689/7134 completed (loss: 0.19359742105007172, acc: 0.9634146094322205)
[2025-02-13 20:06:42,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:43,067][root][INFO] - Training Epoch: 1/2, step 5690/7134 completed (loss: 0.13452361524105072, acc: 0.9742268323898315)
[2025-02-13 20:06:43,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:43,455][root][INFO] - Training Epoch: 1/2, step 5691/7134 completed (loss: 0.25063881278038025, acc: 0.9453551769256592)
[2025-02-13 20:06:43,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:43,836][root][INFO] - Training Epoch: 1/2, step 5692/7134 completed (loss: 0.19727230072021484, acc: 0.9599999785423279)
[2025-02-13 20:06:43,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:44,231][root][INFO] - Training Epoch: 1/2, step 5693/7134 completed (loss: 0.06845200806856155, acc: 0.9874213933944702)
[2025-02-13 20:06:44,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:44,583][root][INFO] - Training Epoch: 1/2, step 5694/7134 completed (loss: 0.10281403362751007, acc: 0.9825581312179565)
[2025-02-13 20:06:44,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:44,980][root][INFO] - Training Epoch: 1/2, step 5695/7134 completed (loss: 0.06864862143993378, acc: 0.987730085849762)
[2025-02-13 20:06:45,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:45,418][root][INFO] - Training Epoch: 1/2, step 5696/7134 completed (loss: 0.10780554264783859, acc: 0.9801980257034302)
[2025-02-13 20:06:45,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:45,821][root][INFO] - Training Epoch: 1/2, step 5697/7134 completed (loss: 0.25454092025756836, acc: 0.9575757384300232)
[2025-02-13 20:06:45,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:46,219][root][INFO] - Training Epoch: 1/2, step 5698/7134 completed (loss: 0.09059974551200867, acc: 0.9886363744735718)
[2025-02-13 20:06:46,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:46,611][root][INFO] - Training Epoch: 1/2, step 5699/7134 completed (loss: 0.3328492045402527, acc: 0.9195402264595032)
[2025-02-13 20:06:46,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:47,032][root][INFO] - Training Epoch: 1/2, step 5700/7134 completed (loss: 0.17352727055549622, acc: 0.9681528806686401)
[2025-02-13 20:06:47,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:47,439][root][INFO] - Training Epoch: 1/2, step 5701/7134 completed (loss: 0.1857536882162094, acc: 0.9621621370315552)
[2025-02-13 20:06:47,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:47,854][root][INFO] - Training Epoch: 1/2, step 5702/7134 completed (loss: 0.1268504559993744, acc: 0.9825581312179565)
[2025-02-13 20:06:48,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:48,292][root][INFO] - Training Epoch: 1/2, step 5703/7134 completed (loss: 0.03828717768192291, acc: 0.9937106966972351)
[2025-02-13 20:06:48,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:48,666][root][INFO] - Training Epoch: 1/2, step 5704/7134 completed (loss: 0.16424666345119476, acc: 0.9470587968826294)
[2025-02-13 20:06:48,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:49,080][root][INFO] - Training Epoch: 1/2, step 5705/7134 completed (loss: 0.07064275443553925, acc: 0.9852941036224365)
[2025-02-13 20:06:49,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:49,465][root][INFO] - Training Epoch: 1/2, step 5706/7134 completed (loss: 0.10723107308149338, acc: 0.9702970385551453)
[2025-02-13 20:06:49,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:49,817][root][INFO] - Training Epoch: 1/2, step 5707/7134 completed (loss: 0.1525418907403946, acc: 0.9760000109672546)
[2025-02-13 20:06:49,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:50,225][root][INFO] - Training Epoch: 1/2, step 5708/7134 completed (loss: 0.04284342750906944, acc: 0.9871794581413269)
[2025-02-13 20:06:50,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:50,629][root][INFO] - Training Epoch: 1/2, step 5709/7134 completed (loss: 0.22232863306999207, acc: 0.95652174949646)
[2025-02-13 20:06:50,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:51,026][root][INFO] - Training Epoch: 1/2, step 5710/7134 completed (loss: 0.19919681549072266, acc: 0.9428571462631226)
[2025-02-13 20:06:51,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:51,414][root][INFO] - Training Epoch: 1/2, step 5711/7134 completed (loss: 0.21297603845596313, acc: 0.9408602118492126)
[2025-02-13 20:06:51,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:51,801][root][INFO] - Training Epoch: 1/2, step 5712/7134 completed (loss: 0.0874280259013176, acc: 0.9677419066429138)
[2025-02-13 20:06:51,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52,197][root][INFO] - Training Epoch: 1/2, step 5713/7134 completed (loss: 0.09551414847373962, acc: 0.9775280952453613)
[2025-02-13 20:06:52,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52,609][root][INFO] - Training Epoch: 1/2, step 5714/7134 completed (loss: 0.08442520350217819, acc: 0.97826087474823)
[2025-02-13 20:06:52,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52,993][root][INFO] - Training Epoch: 1/2, step 5715/7134 completed (loss: 0.2934240996837616, acc: 0.9485294222831726)
[2025-02-13 20:06:53,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:53,381][root][INFO] - Training Epoch: 1/2, step 5716/7134 completed (loss: 0.2992182970046997, acc: 0.9305555820465088)
[2025-02-13 20:06:53,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:53,766][root][INFO] - Training Epoch: 1/2, step 5717/7134 completed (loss: 0.431259423494339, acc: 0.9181286692619324)
[2025-02-13 20:06:53,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:54,138][root][INFO] - Training Epoch: 1/2, step 5718/7134 completed (loss: 0.15803959965705872, acc: 0.9539473652839661)
[2025-02-13 20:06:54,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:54,536][root][INFO] - Training Epoch: 1/2, step 5719/7134 completed (loss: 0.223782479763031, acc: 0.9235293865203857)
[2025-02-13 20:06:54,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:54,972][root][INFO] - Training Epoch: 1/2, step 5720/7134 completed (loss: 0.2566436231136322, acc: 0.9294871687889099)
[2025-02-13 20:06:55,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:55,366][root][INFO] - Training Epoch: 1/2, step 5721/7134 completed (loss: 0.19463253021240234, acc: 0.9326424598693848)
[2025-02-13 20:06:55,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:55,722][root][INFO] - Training Epoch: 1/2, step 5722/7134 completed (loss: 0.05912930890917778, acc: 0.9818181991577148)
[2025-02-13 20:06:55,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:56,104][root][INFO] - Training Epoch: 1/2, step 5723/7134 completed (loss: 0.19800561666488647, acc: 0.9576719403266907)
[2025-02-13 20:06:56,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:56,464][root][INFO] - Training Epoch: 1/2, step 5724/7134 completed (loss: 0.24554714560508728, acc: 0.939393937587738)
[2025-02-13 20:06:56,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:56,873][root][INFO] - Training Epoch: 1/2, step 5725/7134 completed (loss: 0.17531242966651917, acc: 0.954285740852356)
[2025-02-13 20:06:57,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:57,232][root][INFO] - Training Epoch: 1/2, step 5726/7134 completed (loss: 0.24533405900001526, acc: 0.946107804775238)
[2025-02-13 20:06:57,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:57,594][root][INFO] - Training Epoch: 1/2, step 5727/7134 completed (loss: 0.06742201745510101, acc: 0.979899525642395)
[2025-02-13 20:06:57,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:57,979][root][INFO] - Training Epoch: 1/2, step 5728/7134 completed (loss: 0.022270387038588524, acc: 1.0)
[2025-02-13 20:06:58,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:58,339][root][INFO] - Training Epoch: 1/2, step 5729/7134 completed (loss: 0.07764722406864166, acc: 0.9805194735527039)
[2025-02-13 20:06:58,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:58,701][root][INFO] - Training Epoch: 1/2, step 5730/7134 completed (loss: 0.17948278784751892, acc: 0.9481481313705444)
[2025-02-13 20:06:58,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:59,086][root][INFO] - Training Epoch: 1/2, step 5731/7134 completed (loss: 0.23267099261283875, acc: 0.9375)
[2025-02-13 20:06:59,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:59,442][root][INFO] - Training Epoch: 1/2, step 5732/7134 completed (loss: 0.8787912726402283, acc: 0.7891566157341003)
[2025-02-13 20:06:59,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:59,824][root][INFO] - Training Epoch: 1/2, step 5733/7134 completed (loss: 0.29972097277641296, acc: 0.9240506291389465)
[2025-02-13 20:06:59,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:00,195][root][INFO] - Training Epoch: 1/2, step 5734/7134 completed (loss: 0.3173983097076416, acc: 0.9107142686843872)
[2025-02-13 20:07:00,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:00,567][root][INFO] - Training Epoch: 1/2, step 5735/7134 completed (loss: 0.33960390090942383, acc: 0.940397322177887)
[2025-02-13 20:07:00,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:00,950][root][INFO] - Training Epoch: 1/2, step 5736/7134 completed (loss: 0.3575877845287323, acc: 0.9171270728111267)
[2025-02-13 20:07:01,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:01,322][root][INFO] - Training Epoch: 1/2, step 5737/7134 completed (loss: 0.137833371758461, acc: 0.9473684430122375)
[2025-02-13 20:07:01,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:01,703][root][INFO] - Training Epoch: 1/2, step 5738/7134 completed (loss: 0.11955419927835464, acc: 0.9595959782600403)
[2025-02-13 20:07:01,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:02,089][root][INFO] - Training Epoch: 1/2, step 5739/7134 completed (loss: 0.24710462987422943, acc: 0.9416666626930237)
[2025-02-13 20:07:02,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:02,472][root][INFO] - Training Epoch: 1/2, step 5740/7134 completed (loss: 0.12701423466205597, acc: 0.9583333134651184)
[2025-02-13 20:07:02,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:02,843][root][INFO] - Training Epoch: 1/2, step 5741/7134 completed (loss: 0.12044143676757812, acc: 0.9659090638160706)
[2025-02-13 20:07:02,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:03,207][root][INFO] - Training Epoch: 1/2, step 5742/7134 completed (loss: 0.07118429243564606, acc: 0.9887640476226807)
[2025-02-13 20:07:03,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:03,587][root][INFO] - Training Epoch: 1/2, step 5743/7134 completed (loss: 0.09342432022094727, acc: 0.9739583134651184)
[2025-02-13 20:07:03,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:03,923][root][INFO] - Training Epoch: 1/2, step 5744/7134 completed (loss: 0.042602475732564926, acc: 1.0)
[2025-02-13 20:07:04,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:04,303][root][INFO] - Training Epoch: 1/2, step 5745/7134 completed (loss: 0.22540028393268585, acc: 0.9550561904907227)
[2025-02-13 20:07:04,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:04,650][root][INFO] - Training Epoch: 1/2, step 5746/7134 completed (loss: 0.13120201230049133, acc: 0.9629629850387573)
[2025-02-13 20:07:04,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:05,021][root][INFO] - Training Epoch: 1/2, step 5747/7134 completed (loss: 0.14604046940803528, acc: 0.9454545378684998)
[2025-02-13 20:07:05,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:05,398][root][INFO] - Training Epoch: 1/2, step 5748/7134 completed (loss: 0.06602463126182556, acc: 0.9763779640197754)
[2025-02-13 20:07:05,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:05,763][root][INFO] - Training Epoch: 1/2, step 5749/7134 completed (loss: 0.2083018571138382, acc: 0.9644970297813416)
[2025-02-13 20:07:05,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:06,141][root][INFO] - Training Epoch: 1/2, step 5750/7134 completed (loss: 0.1712222397327423, acc: 0.9497206807136536)
[2025-02-13 20:07:06,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:06,510][root][INFO] - Training Epoch: 1/2, step 5751/7134 completed (loss: 0.21962468326091766, acc: 0.940397322177887)
[2025-02-13 20:07:06,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:06,937][root][INFO] - Training Epoch: 1/2, step 5752/7134 completed (loss: 0.7302670478820801, acc: 0.8244274854660034)
[2025-02-13 20:07:07,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:07,312][root][INFO] - Training Epoch: 1/2, step 5753/7134 completed (loss: 0.24100223183631897, acc: 0.926174521446228)
[2025-02-13 20:07:07,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:07,756][root][INFO] - Training Epoch: 1/2, step 5754/7134 completed (loss: 0.2601117491722107, acc: 0.9367815852165222)
[2025-02-13 20:07:07,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08,152][root][INFO] - Training Epoch: 1/2, step 5755/7134 completed (loss: 0.3362273871898651, acc: 0.9285714030265808)
[2025-02-13 20:07:08,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08,527][root][INFO] - Training Epoch: 1/2, step 5756/7134 completed (loss: 0.31792184710502625, acc: 0.9011628031730652)
[2025-02-13 20:07:08,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08,903][root][INFO] - Training Epoch: 1/2, step 5757/7134 completed (loss: 0.35154056549072266, acc: 0.9213483333587646)
[2025-02-13 20:07:09,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:09,287][root][INFO] - Training Epoch: 1/2, step 5758/7134 completed (loss: 0.4085327982902527, acc: 0.8888888955116272)
[2025-02-13 20:07:09,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:09,722][root][INFO] - Training Epoch: 1/2, step 5759/7134 completed (loss: 0.6394093632698059, acc: 0.8834951519966125)
[2025-02-13 20:07:09,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:10,093][root][INFO] - Training Epoch: 1/2, step 5760/7134 completed (loss: 0.2004946768283844, acc: 0.9610389471054077)
[2025-02-13 20:07:10,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:10,472][root][INFO] - Training Epoch: 1/2, step 5761/7134 completed (loss: 0.1711927205324173, acc: 0.9440000057220459)
[2025-02-13 20:07:10,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:10,828][root][INFO] - Training Epoch: 1/2, step 5762/7134 completed (loss: 0.188357874751091, acc: 0.961240291595459)
[2025-02-13 20:07:10,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:11,199][root][INFO] - Training Epoch: 1/2, step 5763/7134 completed (loss: 0.15720753371715546, acc: 0.9562841653823853)
[2025-02-13 20:07:11,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:11,563][root][INFO] - Training Epoch: 1/2, step 5764/7134 completed (loss: 0.03604760766029358, acc: 0.9923664331436157)
[2025-02-13 20:07:11,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:11,940][root][INFO] - Training Epoch: 1/2, step 5765/7134 completed (loss: 0.19895121455192566, acc: 0.955974817276001)
[2025-02-13 20:07:12,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:12,305][root][INFO] - Training Epoch: 1/2, step 5766/7134 completed (loss: 0.09627090394496918, acc: 0.9870967864990234)
[2025-02-13 20:07:12,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:12,661][root][INFO] - Training Epoch: 1/2, step 5767/7134 completed (loss: 0.07738662511110306, acc: 0.9875776171684265)
[2025-02-13 20:07:12,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:13,038][root][INFO] - Training Epoch: 1/2, step 5768/7134 completed (loss: 0.0788561999797821, acc: 0.9750000238418579)
[2025-02-13 20:07:13,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:13,418][root][INFO] - Training Epoch: 1/2, step 5769/7134 completed (loss: 0.06159265339374542, acc: 0.9928571581840515)
[2025-02-13 20:07:13,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:13,797][root][INFO] - Training Epoch: 1/2, step 5770/7134 completed (loss: 0.11093632131814957, acc: 0.9783783555030823)
[2025-02-13 20:07:13,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:14,164][root][INFO] - Training Epoch: 1/2, step 5771/7134 completed (loss: 0.24557946622371674, acc: 0.9386503100395203)
[2025-02-13 20:07:14,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:14,536][root][INFO] - Training Epoch: 1/2, step 5772/7134 completed (loss: 0.053398486226797104, acc: 0.9833333492279053)
[2025-02-13 20:07:14,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:14,906][root][INFO] - Training Epoch: 1/2, step 5773/7134 completed (loss: 0.12197573482990265, acc: 0.969924807548523)
[2025-02-13 20:07:15,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:15,294][root][INFO] - Training Epoch: 1/2, step 5774/7134 completed (loss: 0.09963548928499222, acc: 0.9753086566925049)
[2025-02-13 20:07:15,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:15,642][root][INFO] - Training Epoch: 1/2, step 5775/7134 completed (loss: 0.07961621135473251, acc: 0.9800000190734863)
[2025-02-13 20:07:15,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:16,006][root][INFO] - Training Epoch: 1/2, step 5776/7134 completed (loss: 0.1654794067144394, acc: 0.9550561904907227)
[2025-02-13 20:07:16,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:16,385][root][INFO] - Training Epoch: 1/2, step 5777/7134 completed (loss: 0.21672777831554413, acc: 0.9579439163208008)
[2025-02-13 20:07:16,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:16,757][root][INFO] - Training Epoch: 1/2, step 5778/7134 completed (loss: 0.36583152413368225, acc: 0.9146341681480408)
[2025-02-13 20:07:16,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17,134][root][INFO] - Training Epoch: 1/2, step 5779/7134 completed (loss: 0.2580965757369995, acc: 0.9588235020637512)
[2025-02-13 20:07:17,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17,511][root][INFO] - Training Epoch: 1/2, step 5780/7134 completed (loss: 0.20033077895641327, acc: 0.9552238583564758)
[2025-02-13 20:07:17,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17,865][root][INFO] - Training Epoch: 1/2, step 5781/7134 completed (loss: 0.24927639961242676, acc: 0.9479768872261047)
[2025-02-13 20:07:18,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:18,234][root][INFO] - Training Epoch: 1/2, step 5782/7134 completed (loss: 0.12018270045518875, acc: 0.9779005646705627)
[2025-02-13 20:07:18,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:18,606][root][INFO] - Training Epoch: 1/2, step 5783/7134 completed (loss: 0.16389600932598114, acc: 0.9653179049491882)
[2025-02-13 20:07:18,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:18,972][root][INFO] - Training Epoch: 1/2, step 5784/7134 completed (loss: 0.15601544082164764, acc: 0.9604519605636597)
[2025-02-13 20:07:19,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:19,330][root][INFO] - Training Epoch: 1/2, step 5785/7134 completed (loss: 0.22717644274234772, acc: 0.9513513445854187)
[2025-02-13 20:07:19,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:19,699][root][INFO] - Training Epoch: 1/2, step 5786/7134 completed (loss: 0.17269496619701385, acc: 0.9523809552192688)
[2025-02-13 20:07:19,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:20,067][root][INFO] - Training Epoch: 1/2, step 5787/7134 completed (loss: 0.13818161189556122, acc: 0.9581151604652405)
[2025-02-13 20:07:20,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:20,421][root][INFO] - Training Epoch: 1/2, step 5788/7134 completed (loss: 0.07357699424028397, acc: 0.9868420958518982)
[2025-02-13 20:07:20,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:20,797][root][INFO] - Training Epoch: 1/2, step 5789/7134 completed (loss: 0.11633875221014023, acc: 0.9646464586257935)
[2025-02-13 20:07:20,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:21,161][root][INFO] - Training Epoch: 1/2, step 5790/7134 completed (loss: 0.21849384903907776, acc: 0.9642857313156128)
[2025-02-13 20:07:21,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:21,518][root][INFO] - Training Epoch: 1/2, step 5791/7134 completed (loss: 0.10712303221225739, acc: 0.9626865386962891)
[2025-02-13 20:07:21,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:21,890][root][INFO] - Training Epoch: 1/2, step 5792/7134 completed (loss: 0.44569867849349976, acc: 0.8590604066848755)
[2025-02-13 20:07:22,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:22,253][root][INFO] - Training Epoch: 1/2, step 5793/7134 completed (loss: 0.30317941308021545, acc: 0.9263803958892822)
[2025-02-13 20:07:22,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:22,634][root][INFO] - Training Epoch: 1/2, step 5794/7134 completed (loss: 0.38246044516563416, acc: 0.9171974658966064)
[2025-02-13 20:07:22,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23,062][root][INFO] - Training Epoch: 1/2, step 5795/7134 completed (loss: 0.5039778351783752, acc: 0.8947368264198303)
[2025-02-13 20:07:23,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23,453][root][INFO] - Training Epoch: 1/2, step 5796/7134 completed (loss: 0.3669492304325104, acc: 0.9083969593048096)
[2025-02-13 20:07:23,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23,812][root][INFO] - Training Epoch: 1/2, step 5797/7134 completed (loss: 0.23709551990032196, acc: 0.9395973086357117)
[2025-02-13 20:07:23,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:24,195][root][INFO] - Training Epoch: 1/2, step 5798/7134 completed (loss: 0.9440882205963135, acc: 0.8662790656089783)
[2025-02-13 20:07:24,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:24,564][root][INFO] - Training Epoch: 1/2, step 5799/7134 completed (loss: 0.23562951385974884, acc: 0.9448819160461426)
[2025-02-13 20:07:24,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:24,936][root][INFO] - Training Epoch: 1/2, step 5800/7134 completed (loss: 0.43640947341918945, acc: 0.9064748287200928)
[2025-02-13 20:07:25,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:25,310][root][INFO] - Training Epoch: 1/2, step 5801/7134 completed (loss: 0.30611371994018555, acc: 0.9259259104728699)
[2025-02-13 20:07:25,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:25,738][root][INFO] - Training Epoch: 1/2, step 5802/7134 completed (loss: 0.166427344083786, acc: 0.9558823704719543)
[2025-02-13 20:07:25,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:26,134][root][INFO] - Training Epoch: 1/2, step 5803/7134 completed (loss: 0.07639476656913757, acc: 0.982758641242981)
[2025-02-13 20:07:26,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:26,562][root][INFO] - Training Epoch: 1/2, step 5804/7134 completed (loss: 0.3269825875759125, acc: 0.9044585824012756)
[2025-02-13 20:07:26,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:26,977][root][INFO] - Training Epoch: 1/2, step 5805/7134 completed (loss: 0.2455967515707016, acc: 0.940397322177887)
[2025-02-13 20:07:27,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:27,412][root][INFO] - Training Epoch: 1/2, step 5806/7134 completed (loss: 0.15541458129882812, acc: 0.9636363387107849)
[2025-02-13 20:07:27,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:27,811][root][INFO] - Training Epoch: 1/2, step 5807/7134 completed (loss: 0.2546252906322479, acc: 0.9507042169570923)
[2025-02-13 20:07:27,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:28,174][root][INFO] - Training Epoch: 1/2, step 5808/7134 completed (loss: 0.13669556379318237, acc: 0.9683544039726257)
[2025-02-13 20:07:28,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:28,559][root][INFO] - Training Epoch: 1/2, step 5809/7134 completed (loss: 0.13320474326610565, acc: 0.9509803652763367)
[2025-02-13 20:07:28,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:28,951][root][INFO] - Training Epoch: 1/2, step 5810/7134 completed (loss: 0.1072879433631897, acc: 0.9669421315193176)
[2025-02-13 20:07:29,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:29,317][root][INFO] - Training Epoch: 1/2, step 5811/7134 completed (loss: 0.11755713820457458, acc: 0.9780219793319702)
[2025-02-13 20:07:29,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:29,677][root][INFO] - Training Epoch: 1/2, step 5812/7134 completed (loss: 0.08783150464296341, acc: 0.9779411554336548)
[2025-02-13 20:07:29,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:30,034][root][INFO] - Training Epoch: 1/2, step 5813/7134 completed (loss: 0.11183309555053711, acc: 0.9836065769195557)
[2025-02-13 20:07:30,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:30,432][root][INFO] - Training Epoch: 1/2, step 5814/7134 completed (loss: 0.139595165848732, acc: 0.9520000219345093)
[2025-02-13 20:07:30,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:30,807][root][INFO] - Training Epoch: 1/2, step 5815/7134 completed (loss: 0.1620028167963028, acc: 0.9482758641242981)
[2025-02-13 20:07:30,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:31,187][root][INFO] - Training Epoch: 1/2, step 5816/7134 completed (loss: 0.029371144250035286, acc: 0.9946523904800415)
[2025-02-13 20:07:31,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:31,563][root][INFO] - Training Epoch: 1/2, step 5817/7134 completed (loss: 0.1652171015739441, acc: 0.9714285731315613)
[2025-02-13 20:07:31,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:31,989][root][INFO] - Training Epoch: 1/2, step 5818/7134 completed (loss: 0.09025432914495468, acc: 0.9673202633857727)
[2025-02-13 20:07:32,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:32,386][root][INFO] - Training Epoch: 1/2, step 5819/7134 completed (loss: 0.0760447084903717, acc: 0.9929577708244324)
[2025-02-13 20:07:32,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:32,793][root][INFO] - Training Epoch: 1/2, step 5820/7134 completed (loss: 0.10995364189147949, acc: 0.9834710955619812)
[2025-02-13 20:07:32,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:33,217][root][INFO] - Training Epoch: 1/2, step 5821/7134 completed (loss: 0.10590507090091705, acc: 0.9862068891525269)
[2025-02-13 20:07:33,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:33,627][root][INFO] - Training Epoch: 1/2, step 5822/7134 completed (loss: 0.07066918909549713, acc: 0.9865771532058716)
[2025-02-13 20:07:33,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:33,972][root][INFO] - Training Epoch: 1/2, step 5823/7134 completed (loss: 0.06424444913864136, acc: 0.9659863710403442)
[2025-02-13 20:07:34,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:34,339][root][INFO] - Training Epoch: 1/2, step 5824/7134 completed (loss: 0.049535688012838364, acc: 0.9939393997192383)
[2025-02-13 20:07:34,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:34,751][root][INFO] - Training Epoch: 1/2, step 5825/7134 completed (loss: 0.1787680834531784, acc: 0.9541984796524048)
[2025-02-13 20:07:34,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:35,137][root][INFO] - Training Epoch: 1/2, step 5826/7134 completed (loss: 0.07233034074306488, acc: 0.9767441749572754)
[2025-02-13 20:07:35,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:35,516][root][INFO] - Training Epoch: 1/2, step 5827/7134 completed (loss: 0.06947759538888931, acc: 1.0)
[2025-02-13 20:07:35,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:35,885][root][INFO] - Training Epoch: 1/2, step 5828/7134 completed (loss: 0.06678907573223114, acc: 0.9803921580314636)
[2025-02-13 20:07:36,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:36,282][root][INFO] - Training Epoch: 1/2, step 5829/7134 completed (loss: 0.06360267102718353, acc: 0.9817073345184326)
[2025-02-13 20:07:36,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:36,632][root][INFO] - Training Epoch: 1/2, step 5830/7134 completed (loss: 0.22191734611988068, acc: 0.9440000057220459)
[2025-02-13 20:07:36,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:37,018][root][INFO] - Training Epoch: 1/2, step 5831/7134 completed (loss: 0.11344525218009949, acc: 0.9591836929321289)
[2025-02-13 20:07:37,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:37,403][root][INFO] - Training Epoch: 1/2, step 5832/7134 completed (loss: 0.24032944440841675, acc: 0.9389312863349915)
[2025-02-13 20:07:37,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:37,803][root][INFO] - Training Epoch: 1/2, step 5833/7134 completed (loss: 0.059159766882658005, acc: 0.9849624037742615)
[2025-02-13 20:07:37,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:38,242][root][INFO] - Training Epoch: 1/2, step 5834/7134 completed (loss: 0.21799473464488983, acc: 0.9298245906829834)
[2025-02-13 20:07:38,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:38,621][root][INFO] - Training Epoch: 1/2, step 5835/7134 completed (loss: 0.1831480711698532, acc: 0.9599999785423279)
[2025-02-13 20:07:38,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:38,987][root][INFO] - Training Epoch: 1/2, step 5836/7134 completed (loss: 0.2714309096336365, acc: 0.9490445852279663)
[2025-02-13 20:07:39,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:39,352][root][INFO] - Training Epoch: 1/2, step 5837/7134 completed (loss: 0.5381166934967041, acc: 0.899328887462616)
[2025-02-13 20:07:39,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:39,760][root][INFO] - Training Epoch: 1/2, step 5838/7134 completed (loss: 0.20164552330970764, acc: 0.9640287756919861)
[2025-02-13 20:07:39,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:40,127][root][INFO] - Training Epoch: 1/2, step 5839/7134 completed (loss: 0.3436780869960785, acc: 0.9202898740768433)
[2025-02-13 20:07:40,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:40,517][root][INFO] - Training Epoch: 1/2, step 5840/7134 completed (loss: 0.01886547915637493, acc: 1.0)
[2025-02-13 20:07:40,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:40,894][root][INFO] - Training Epoch: 1/2, step 5841/7134 completed (loss: 0.31546634435653687, acc: 0.925000011920929)
[2025-02-13 20:07:41,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:41,304][root][INFO] - Training Epoch: 1/2, step 5842/7134 completed (loss: 0.09646874666213989, acc: 0.9586777091026306)
[2025-02-13 20:07:41,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:41,713][root][INFO] - Training Epoch: 1/2, step 5843/7134 completed (loss: 0.22803974151611328, acc: 0.9702380895614624)
[2025-02-13 20:07:41,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:42,086][root][INFO] - Training Epoch: 1/2, step 5844/7134 completed (loss: 0.2159424126148224, acc: 0.9638554453849792)
[2025-02-13 20:07:42,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:42,461][root][INFO] - Training Epoch: 1/2, step 5845/7134 completed (loss: 0.14193589985370636, acc: 0.9545454382896423)
[2025-02-13 20:07:42,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:42,856][root][INFO] - Training Epoch: 1/2, step 5846/7134 completed (loss: 0.17087145149707794, acc: 0.9702380895614624)
[2025-02-13 20:07:43,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:43,245][root][INFO] - Training Epoch: 1/2, step 5847/7134 completed (loss: 0.08545353263616562, acc: 0.9759036302566528)
[2025-02-13 20:07:43,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:43,637][root][INFO] - Training Epoch: 1/2, step 5848/7134 completed (loss: 0.11059214919805527, acc: 0.9797297120094299)
[2025-02-13 20:07:43,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:43,997][root][INFO] - Training Epoch: 1/2, step 5849/7134 completed (loss: 0.21533916890621185, acc: 0.9415584206581116)
[2025-02-13 20:07:44,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:44,401][root][INFO] - Training Epoch: 1/2, step 5850/7134 completed (loss: 0.13126157224178314, acc: 0.9577465057373047)
[2025-02-13 20:07:44,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:44,745][root][INFO] - Training Epoch: 1/2, step 5851/7134 completed (loss: 0.2388746738433838, acc: 0.9452054500579834)
[2025-02-13 20:07:44,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:45,090][root][INFO] - Training Epoch: 1/2, step 5852/7134 completed (loss: 0.1511729508638382, acc: 0.9650349617004395)
[2025-02-13 20:07:45,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:45,509][root][INFO] - Training Epoch: 1/2, step 5853/7134 completed (loss: 0.2086811065673828, acc: 0.9213483333587646)
[2025-02-13 20:07:45,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:45,881][root][INFO] - Training Epoch: 1/2, step 5854/7134 completed (loss: 0.16303899884223938, acc: 0.9769230484962463)
[2025-02-13 20:07:46,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:46,279][root][INFO] - Training Epoch: 1/2, step 5855/7134 completed (loss: 0.1954634189605713, acc: 0.9661017060279846)
[2025-02-13 20:07:46,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:46,681][root][INFO] - Training Epoch: 1/2, step 5856/7134 completed (loss: 0.2814454138278961, acc: 0.9154929518699646)
[2025-02-13 20:07:46,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:47,048][root][INFO] - Training Epoch: 1/2, step 5857/7134 completed (loss: 0.23044072091579437, acc: 0.9398496150970459)
[2025-02-13 20:07:47,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:47,440][root][INFO] - Training Epoch: 1/2, step 5858/7134 completed (loss: 0.12561854720115662, acc: 0.963302731513977)
[2025-02-13 20:07:47,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:47,885][root][INFO] - Training Epoch: 1/2, step 5859/7134 completed (loss: 0.14402484893798828, acc: 0.9568965435028076)
[2025-02-13 20:07:48,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:48,311][root][INFO] - Training Epoch: 1/2, step 5860/7134 completed (loss: 0.1667432188987732, acc: 0.9545454382896423)
[2025-02-13 20:07:48,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:48,695][root][INFO] - Training Epoch: 1/2, step 5861/7134 completed (loss: 0.10369652509689331, acc: 0.961904764175415)
[2025-02-13 20:07:48,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:49,064][root][INFO] - Training Epoch: 1/2, step 5862/7134 completed (loss: 0.22082923352718353, acc: 0.9406779408454895)
[2025-02-13 20:07:49,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:49,415][root][INFO] - Training Epoch: 1/2, step 5863/7134 completed (loss: 0.31320735812187195, acc: 0.9462365508079529)
[2025-02-13 20:07:49,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:49,814][root][INFO] - Training Epoch: 1/2, step 5864/7134 completed (loss: 0.3708745539188385, acc: 0.9298245906829834)
[2025-02-13 20:07:49,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:50,212][root][INFO] - Training Epoch: 1/2, step 5865/7134 completed (loss: 0.1758987307548523, acc: 0.9652777910232544)
[2025-02-13 20:07:50,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:50,614][root][INFO] - Training Epoch: 1/2, step 5866/7134 completed (loss: 0.08586081117391586, acc: 0.9835164546966553)
[2025-02-13 20:07:50,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:51,030][root][INFO] - Training Epoch: 1/2, step 5867/7134 completed (loss: 0.14470790326595306, acc: 0.9662162065505981)
[2025-02-13 20:07:51,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:51,425][root][INFO] - Training Epoch: 1/2, step 5868/7134 completed (loss: 0.25877130031585693, acc: 0.9281045794487)
[2025-02-13 20:07:51,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:51,812][root][INFO] - Training Epoch: 1/2, step 5869/7134 completed (loss: 0.149249866604805, acc: 0.9580838084220886)
[2025-02-13 20:07:51,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:52,209][root][INFO] - Training Epoch: 1/2, step 5870/7134 completed (loss: 0.14337904751300812, acc: 0.9629629850387573)
[2025-02-13 20:07:52,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:52,606][root][INFO] - Training Epoch: 1/2, step 5871/7134 completed (loss: 0.08940953016281128, acc: 0.9724770784378052)
[2025-02-13 20:07:52,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:52,997][root][INFO] - Training Epoch: 1/2, step 5872/7134 completed (loss: 0.16708025336265564, acc: 0.9627329111099243)
[2025-02-13 20:07:53,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:53,425][root][INFO] - Training Epoch: 1/2, step 5873/7134 completed (loss: 0.1942470371723175, acc: 0.9572192430496216)
[2025-02-13 20:07:53,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:53,833][root][INFO] - Training Epoch: 1/2, step 5874/7134 completed (loss: 0.09264271706342697, acc: 0.9682539701461792)
[2025-02-13 20:07:53,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:54,250][root][INFO] - Training Epoch: 1/2, step 5875/7134 completed (loss: 0.20955424010753632, acc: 0.9166666865348816)
[2025-02-13 20:07:54,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:54,617][root][INFO] - Training Epoch: 1/2, step 5876/7134 completed (loss: 0.07726271450519562, acc: 0.966292142868042)
[2025-02-13 20:07:54,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:55,003][root][INFO] - Training Epoch: 1/2, step 5877/7134 completed (loss: 0.1174808144569397, acc: 0.9553072452545166)
[2025-02-13 20:07:55,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:55,370][root][INFO] - Training Epoch: 1/2, step 5878/7134 completed (loss: 0.1672239601612091, acc: 0.9545454382896423)
[2025-02-13 20:07:55,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:55,736][root][INFO] - Training Epoch: 1/2, step 5879/7134 completed (loss: 0.07671378552913666, acc: 0.9777777791023254)
[2025-02-13 20:07:55,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:56,184][root][INFO] - Training Epoch: 1/2, step 5880/7134 completed (loss: 0.15302038192749023, acc: 0.9642857313156128)
[2025-02-13 20:07:56,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:56,563][root][INFO] - Training Epoch: 1/2, step 5881/7134 completed (loss: 0.11102336645126343, acc: 0.9658119678497314)
[2025-02-13 20:07:56,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:56,920][root][INFO] - Training Epoch: 1/2, step 5882/7134 completed (loss: 0.06181083619594574, acc: 0.9803921580314636)
[2025-02-13 20:07:57,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:57,280][root][INFO] - Training Epoch: 1/2, step 5883/7134 completed (loss: 0.06644289940595627, acc: 0.9769230484962463)
[2025-02-13 20:07:57,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:57,720][root][INFO] - Training Epoch: 1/2, step 5884/7134 completed (loss: 0.26374098658561707, acc: 0.920634925365448)
[2025-02-13 20:07:57,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:58,108][root][INFO] - Training Epoch: 1/2, step 5885/7134 completed (loss: 0.4158583879470825, acc: 0.9313725233078003)
[2025-02-13 20:07:58,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:58,437][root][INFO] - Training Epoch: 1/2, step 5886/7134 completed (loss: 0.06988025456666946, acc: 0.976190447807312)
[2025-02-13 20:07:58,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:58,787][root][INFO] - Training Epoch: 1/2, step 5887/7134 completed (loss: 0.04240823909640312, acc: 0.9916666746139526)
[2025-02-13 20:07:58,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:59,146][root][INFO] - Training Epoch: 1/2, step 5888/7134 completed (loss: 0.3154487609863281, acc: 0.9290780425071716)
[2025-02-13 20:07:59,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:59,440][root][INFO] - Training Epoch: 1/2, step 5889/7134 completed (loss: 0.15035419166088104, acc: 0.9807692170143127)
[2025-02-13 20:07:59,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:59,806][root][INFO] - Training Epoch: 1/2, step 5890/7134 completed (loss: 0.05291453003883362, acc: 0.9864864945411682)
[2025-02-13 20:07:59,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:00,168][root][INFO] - Training Epoch: 1/2, step 5891/7134 completed (loss: 0.2400178611278534, acc: 0.9571428298950195)
[2025-02-13 20:08:00,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:00,600][root][INFO] - Training Epoch: 1/2, step 5892/7134 completed (loss: 0.14158709347248077, acc: 0.9767441749572754)
[2025-02-13 20:08:00,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:01,010][root][INFO] - Training Epoch: 1/2, step 5893/7134 completed (loss: 0.08697731792926788, acc: 0.9754601120948792)
[2025-02-13 20:08:01,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:01,359][root][INFO] - Training Epoch: 1/2, step 5894/7134 completed (loss: 0.2043270468711853, acc: 0.9594594836235046)
[2025-02-13 20:08:01,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:01,696][root][INFO] - Training Epoch: 1/2, step 5895/7134 completed (loss: 0.10798248648643494, acc: 0.96875)
[2025-02-13 20:08:01,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:02,066][root][INFO] - Training Epoch: 1/2, step 5896/7134 completed (loss: 0.10858874022960663, acc: 0.9748427867889404)
[2025-02-13 20:08:02,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:02,425][root][INFO] - Training Epoch: 1/2, step 5897/7134 completed (loss: 0.11144619435071945, acc: 0.9772727489471436)
[2025-02-13 20:08:02,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:02,782][root][INFO] - Training Epoch: 1/2, step 5898/7134 completed (loss: 0.12417346984148026, acc: 0.9642857313156128)
[2025-02-13 20:08:02,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:03,103][root][INFO] - Training Epoch: 1/2, step 5899/7134 completed (loss: 0.14613890647888184, acc: 0.9743589758872986)
[2025-02-13 20:08:03,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:03,430][root][INFO] - Training Epoch: 1/2, step 5900/7134 completed (loss: 0.1004386693239212, acc: 0.9629629850387573)
[2025-02-13 20:08:03,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:03,761][root][INFO] - Training Epoch: 1/2, step 5901/7134 completed (loss: 0.1855054497718811, acc: 0.969072163105011)
[2025-02-13 20:08:03,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:04,117][root][INFO] - Training Epoch: 1/2, step 5902/7134 completed (loss: 0.06853366643190384, acc: 0.9897959232330322)
[2025-02-13 20:08:04,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:04,464][root][INFO] - Training Epoch: 1/2, step 5903/7134 completed (loss: 0.11896080523729324, acc: 0.9734513163566589)
[2025-02-13 20:08:04,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:04,809][root][INFO] - Training Epoch: 1/2, step 5904/7134 completed (loss: 0.19395986199378967, acc: 0.9577465057373047)
[2025-02-13 20:08:04,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:05,125][root][INFO] - Training Epoch: 1/2, step 5905/7134 completed (loss: 0.18853116035461426, acc: 0.9527027010917664)
[2025-02-13 20:08:05,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:05,493][root][INFO] - Training Epoch: 1/2, step 5906/7134 completed (loss: 0.1402900367975235, acc: 0.9673202633857727)
[2025-02-13 20:08:05,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:05,907][root][INFO] - Training Epoch: 1/2, step 5907/7134 completed (loss: 0.10690630227327347, acc: 0.9791666865348816)
[2025-02-13 20:08:06,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:06,298][root][INFO] - Training Epoch: 1/2, step 5908/7134 completed (loss: 0.12452229857444763, acc: 0.9740259647369385)
[2025-02-13 20:08:06,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:06,781][root][INFO] - Training Epoch: 1/2, step 5909/7134 completed (loss: 0.25714242458343506, acc: 0.956204354763031)
[2025-02-13 20:08:06,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:07,183][root][INFO] - Training Epoch: 1/2, step 5910/7134 completed (loss: 0.27178603410720825, acc: 0.9337748289108276)
[2025-02-13 20:08:07,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:07,543][root][INFO] - Training Epoch: 1/2, step 5911/7134 completed (loss: 0.11882534623146057, acc: 0.95652174949646)
[2025-02-13 20:08:07,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:07,885][root][INFO] - Training Epoch: 1/2, step 5912/7134 completed (loss: 0.08191754668951035, acc: 0.984375)
[2025-02-13 20:08:08,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:08,289][root][INFO] - Training Epoch: 1/2, step 5913/7134 completed (loss: 0.18167683482170105, acc: 0.9526627063751221)
[2025-02-13 20:08:08,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:08,667][root][INFO] - Training Epoch: 1/2, step 5914/7134 completed (loss: 0.1990443468093872, acc: 0.9642857313156128)
[2025-02-13 20:08:08,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:09,050][root][INFO] - Training Epoch: 1/2, step 5915/7134 completed (loss: 0.17264099419116974, acc: 0.9694656729698181)
[2025-02-13 20:08:09,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:09,461][root][INFO] - Training Epoch: 1/2, step 5916/7134 completed (loss: 0.1728696972131729, acc: 0.9536423683166504)
[2025-02-13 20:08:09,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:09,822][root][INFO] - Training Epoch: 1/2, step 5917/7134 completed (loss: 0.056221846491098404, acc: 0.9760000109672546)
[2025-02-13 20:08:09,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:10,243][root][INFO] - Training Epoch: 1/2, step 5918/7134 completed (loss: 0.23765483498573303, acc: 0.9440559148788452)
[2025-02-13 20:08:10,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:10,614][root][INFO] - Training Epoch: 1/2, step 5919/7134 completed (loss: 0.06898008286952972, acc: 0.9701492786407471)
[2025-02-13 20:08:10,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:10,982][root][INFO] - Training Epoch: 1/2, step 5920/7134 completed (loss: 0.04883400350809097, acc: 1.0)
[2025-02-13 20:08:11,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:11,344][root][INFO] - Training Epoch: 1/2, step 5921/7134 completed (loss: 0.15882830321788788, acc: 0.9482758641242981)
[2025-02-13 20:08:11,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:11,780][root][INFO] - Training Epoch: 1/2, step 5922/7134 completed (loss: 0.6632906198501587, acc: 0.8653846383094788)
[2025-02-13 20:08:11,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:12,164][root][INFO] - Training Epoch: 1/2, step 5923/7134 completed (loss: 0.24305397272109985, acc: 0.9556962251663208)
[2025-02-13 20:08:12,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:12,565][root][INFO] - Training Epoch: 1/2, step 5924/7134 completed (loss: 0.16785341501235962, acc: 0.942307710647583)
[2025-02-13 20:08:12,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:12,950][root][INFO] - Training Epoch: 1/2, step 5925/7134 completed (loss: 0.8365897536277771, acc: 0.8333333134651184)
[2025-02-13 20:08:13,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:13,323][root][INFO] - Training Epoch: 1/2, step 5926/7134 completed (loss: 0.36711302399635315, acc: 0.8881579041481018)
[2025-02-13 20:08:13,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:13,717][root][INFO] - Training Epoch: 1/2, step 5927/7134 completed (loss: 0.27022698521614075, acc: 0.9107142686843872)
[2025-02-13 20:08:13,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:14,079][root][INFO] - Training Epoch: 1/2, step 5928/7134 completed (loss: 1.1044992208480835, acc: 0.837837815284729)
[2025-02-13 20:08:14,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:14,441][root][INFO] - Training Epoch: 1/2, step 5929/7134 completed (loss: 0.2876260280609131, acc: 0.9354838728904724)
[2025-02-13 20:08:14,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:14,856][root][INFO] - Training Epoch: 1/2, step 5930/7134 completed (loss: 0.7428902387619019, acc: 0.8429751992225647)
[2025-02-13 20:08:15,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:15,241][root][INFO] - Training Epoch: 1/2, step 5931/7134 completed (loss: 0.23968231678009033, acc: 0.9230769276618958)
[2025-02-13 20:08:15,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:15,601][root][INFO] - Training Epoch: 1/2, step 5932/7134 completed (loss: 0.5341504216194153, acc: 0.8848921060562134)
[2025-02-13 20:08:15,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:15,942][root][INFO] - Training Epoch: 1/2, step 5933/7134 completed (loss: 0.5308579206466675, acc: 0.8823529481887817)
[2025-02-13 20:08:16,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:16,304][root][INFO] - Training Epoch: 1/2, step 5934/7134 completed (loss: 0.27137506008148193, acc: 0.9285714030265808)
[2025-02-13 20:08:16,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:16,685][root][INFO] - Training Epoch: 1/2, step 5935/7134 completed (loss: 0.07662349194288254, acc: 0.9754098653793335)
[2025-02-13 20:08:16,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:17,029][root][INFO] - Training Epoch: 1/2, step 5936/7134 completed (loss: 0.5393194556236267, acc: 0.89570552110672)
[2025-02-13 20:08:17,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:17,402][root][INFO] - Training Epoch: 1/2, step 5937/7134 completed (loss: 0.11534324288368225, acc: 0.9647887349128723)
[2025-02-13 20:08:17,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:17,784][root][INFO] - Training Epoch: 1/2, step 5938/7134 completed (loss: 0.22311154007911682, acc: 0.9473684430122375)
[2025-02-13 20:08:17,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:18,158][root][INFO] - Training Epoch: 1/2, step 5939/7134 completed (loss: 1.0043809413909912, acc: 0.8017241358757019)
[2025-02-13 20:08:18,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:18,552][root][INFO] - Training Epoch: 1/2, step 5940/7134 completed (loss: 0.21387404203414917, acc: 0.9503105878829956)
[2025-02-13 20:08:18,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:18,923][root][INFO] - Training Epoch: 1/2, step 5941/7134 completed (loss: 0.2548893988132477, acc: 0.9166666865348816)
[2025-02-13 20:08:19,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:19,328][root][INFO] - Training Epoch: 1/2, step 5942/7134 completed (loss: 0.057648833841085434, acc: 0.9679999947547913)
[2025-02-13 20:08:19,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:19,716][root][INFO] - Training Epoch: 1/2, step 5943/7134 completed (loss: 0.07697568088769913, acc: 0.9815950989723206)
[2025-02-13 20:08:19,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:20,113][root][INFO] - Training Epoch: 1/2, step 5944/7134 completed (loss: 0.1457025557756424, acc: 0.9642857313156128)
[2025-02-13 20:08:20,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:20,560][root][INFO] - Training Epoch: 1/2, step 5945/7134 completed (loss: 0.0719941183924675, acc: 0.9822485446929932)
[2025-02-13 20:08:20,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:20,931][root][INFO] - Training Epoch: 1/2, step 5946/7134 completed (loss: 0.14085423946380615, acc: 0.9513888955116272)
[2025-02-13 20:08:21,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:21,310][root][INFO] - Training Epoch: 1/2, step 5947/7134 completed (loss: 0.22805795073509216, acc: 0.9375)
[2025-02-13 20:08:21,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:21,679][root][INFO] - Training Epoch: 1/2, step 5948/7134 completed (loss: 0.3288740813732147, acc: 0.934959352016449)
[2025-02-13 20:08:21,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:22,053][root][INFO] - Training Epoch: 1/2, step 5949/7134 completed (loss: 0.08494681119918823, acc: 0.9888888597488403)
[2025-02-13 20:08:22,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:22,452][root][INFO] - Training Epoch: 1/2, step 5950/7134 completed (loss: 0.2958437204360962, acc: 0.9306930899620056)
[2025-02-13 20:08:22,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:22,848][root][INFO] - Training Epoch: 1/2, step 5951/7134 completed (loss: 0.277648389339447, acc: 0.9404761791229248)
[2025-02-13 20:08:23,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:23,244][root][INFO] - Training Epoch: 1/2, step 5952/7134 completed (loss: 0.6247614026069641, acc: 0.9210526347160339)
[2025-02-13 20:08:23,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:23,645][root][INFO] - Training Epoch: 1/2, step 5953/7134 completed (loss: 0.20334182679653168, acc: 0.9528301954269409)
[2025-02-13 20:08:23,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:24,059][root][INFO] - Training Epoch: 1/2, step 5954/7134 completed (loss: 0.04752545803785324, acc: 0.9900990128517151)
[2025-02-13 20:08:24,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:24,463][root][INFO] - Training Epoch: 1/2, step 5955/7134 completed (loss: 0.2740793824195862, acc: 0.9629629850387573)
[2025-02-13 20:08:24,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:24,842][root][INFO] - Training Epoch: 1/2, step 5956/7134 completed (loss: 0.27694541215896606, acc: 0.9416666626930237)
[2025-02-13 20:08:24,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:25,228][root][INFO] - Training Epoch: 1/2, step 5957/7134 completed (loss: 0.1406119465827942, acc: 0.9756097793579102)
[2025-02-13 20:08:25,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:25,632][root][INFO] - Training Epoch: 1/2, step 5958/7134 completed (loss: 0.17844633758068085, acc: 0.9599999785423279)
[2025-02-13 20:08:25,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:26,030][root][INFO] - Training Epoch: 1/2, step 5959/7134 completed (loss: 0.1988268345594406, acc: 0.9468085169792175)
[2025-02-13 20:08:26,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:26,447][root][INFO] - Training Epoch: 1/2, step 5960/7134 completed (loss: 0.034447044134140015, acc: 0.988095223903656)
[2025-02-13 20:08:26,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:26,848][root][INFO] - Training Epoch: 1/2, step 5961/7134 completed (loss: 0.2307378202676773, acc: 0.9259259104728699)
[2025-02-13 20:08:26,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:27,250][root][INFO] - Training Epoch: 1/2, step 5962/7134 completed (loss: 0.09739470481872559, acc: 0.9793103337287903)
[2025-02-13 20:08:27,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:27,631][root][INFO] - Training Epoch: 1/2, step 5963/7134 completed (loss: 0.06861773133277893, acc: 0.9885714054107666)
[2025-02-13 20:08:27,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:27,988][root][INFO] - Training Epoch: 1/2, step 5964/7134 completed (loss: 0.05216198042035103, acc: 0.9921259880065918)
[2025-02-13 20:08:28,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:28,410][root][INFO] - Training Epoch: 1/2, step 5965/7134 completed (loss: 0.145377054810524, acc: 0.9615384340286255)
[2025-02-13 20:08:28,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:28,810][root][INFO] - Training Epoch: 1/2, step 5966/7134 completed (loss: 0.1594649702310562, acc: 0.967391312122345)
[2025-02-13 20:08:28,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:29,189][root][INFO] - Training Epoch: 1/2, step 5967/7134 completed (loss: 0.1006758064031601, acc: 0.9583333134651184)
[2025-02-13 20:08:29,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:29,535][root][INFO] - Training Epoch: 1/2, step 5968/7134 completed (loss: 0.20654210448265076, acc: 0.9599999785423279)
[2025-02-13 20:08:29,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:29,897][root][INFO] - Training Epoch: 1/2, step 5969/7134 completed (loss: 0.16594631969928741, acc: 0.948387086391449)
[2025-02-13 20:08:30,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:30,258][root][INFO] - Training Epoch: 1/2, step 5970/7134 completed (loss: 0.15771332383155823, acc: 0.984375)
[2025-02-13 20:08:30,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:30,635][root][INFO] - Training Epoch: 1/2, step 5971/7134 completed (loss: 0.0793304517865181, acc: 0.9631578922271729)
[2025-02-13 20:08:30,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:31,014][root][INFO] - Training Epoch: 1/2, step 5972/7134 completed (loss: 0.2594528794288635, acc: 0.9263803958892822)
[2025-02-13 20:08:31,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:31,430][root][INFO] - Training Epoch: 1/2, step 5973/7134 completed (loss: 0.31543225049972534, acc: 0.9428571462631226)
[2025-02-13 20:08:31,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:31,897][root][INFO] - Training Epoch: 1/2, step 5974/7134 completed (loss: 0.08770452439785004, acc: 0.9926470518112183)
[2025-02-13 20:08:32,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:32,291][root][INFO] - Training Epoch: 1/2, step 5975/7134 completed (loss: 0.05918837711215019, acc: 0.9817073345184326)
[2025-02-13 20:08:32,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:32,722][root][INFO] - Training Epoch: 1/2, step 5976/7134 completed (loss: 0.17338939011096954, acc: 0.9673202633857727)
[2025-02-13 20:08:32,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:33,089][root][INFO] - Training Epoch: 1/2, step 5977/7134 completed (loss: 0.12188360840082169, acc: 0.9750000238418579)
[2025-02-13 20:08:33,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:33,459][root][INFO] - Training Epoch: 1/2, step 5978/7134 completed (loss: 0.1691017746925354, acc: 0.9813664555549622)
[2025-02-13 20:08:33,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:33,862][root][INFO] - Training Epoch: 1/2, step 5979/7134 completed (loss: 0.21841783821582794, acc: 0.9473684430122375)
[2025-02-13 20:08:34,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:34,229][root][INFO] - Training Epoch: 1/2, step 5980/7134 completed (loss: 0.12012174725532532, acc: 0.9619565010070801)
[2025-02-13 20:08:34,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:34,610][root][INFO] - Training Epoch: 1/2, step 5981/7134 completed (loss: 0.09519579261541367, acc: 0.989847719669342)
[2025-02-13 20:08:34,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:34,959][root][INFO] - Training Epoch: 1/2, step 5982/7134 completed (loss: 0.0958128422498703, acc: 0.975806474685669)
[2025-02-13 20:08:35,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:35,298][root][INFO] - Training Epoch: 1/2, step 5983/7134 completed (loss: 0.12876777350902557, acc: 0.9712643623352051)
[2025-02-13 20:08:35,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:35,675][root][INFO] - Training Epoch: 1/2, step 5984/7134 completed (loss: 0.13992831110954285, acc: 0.9696969985961914)
[2025-02-13 20:08:35,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:36,055][root][INFO] - Training Epoch: 1/2, step 5985/7134 completed (loss: 0.11986272782087326, acc: 0.9719101190567017)
[2025-02-13 20:08:36,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:36,439][root][INFO] - Training Epoch: 1/2, step 5986/7134 completed (loss: 0.20400074124336243, acc: 0.9523809552192688)
[2025-02-13 20:08:36,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:36,811][root][INFO] - Training Epoch: 1/2, step 5987/7134 completed (loss: 0.14829352498054504, acc: 0.9658536314964294)
[2025-02-13 20:08:36,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:37,160][root][INFO] - Training Epoch: 1/2, step 5988/7134 completed (loss: 0.08375503122806549, acc: 0.9833333492279053)
[2025-02-13 20:08:37,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:37,522][root][INFO] - Training Epoch: 1/2, step 5989/7134 completed (loss: 0.12945939600467682, acc: 0.9769230484962463)
[2025-02-13 20:08:37,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:37,903][root][INFO] - Training Epoch: 1/2, step 5990/7134 completed (loss: 0.14683246612548828, acc: 0.9613259434700012)
[2025-02-13 20:08:37,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:38,223][root][INFO] - Training Epoch: 1/2, step 5991/7134 completed (loss: 0.2464710772037506, acc: 0.9431818127632141)
[2025-02-13 20:08:38,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:38,579][root][INFO] - Training Epoch: 1/2, step 5992/7134 completed (loss: 0.08156727254390717, acc: 0.9846938848495483)
[2025-02-13 20:08:38,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:38,922][root][INFO] - Training Epoch: 1/2, step 5993/7134 completed (loss: 0.10694196075201035, acc: 0.9775280952453613)
[2025-02-13 20:08:39,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:39,265][root][INFO] - Training Epoch: 1/2, step 5994/7134 completed (loss: 0.07615364342927933, acc: 0.9752066135406494)
[2025-02-13 20:08:39,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:39,631][root][INFO] - Training Epoch: 1/2, step 5995/7134 completed (loss: 0.08445325493812561, acc: 0.9710982441902161)
[2025-02-13 20:08:39,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:39,997][root][INFO] - Training Epoch: 1/2, step 5996/7134 completed (loss: 0.16056916117668152, acc: 0.9599999785423279)
[2025-02-13 20:08:40,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:40,355][root][INFO] - Training Epoch: 1/2, step 5997/7134 completed (loss: 0.1371123194694519, acc: 0.9715909361839294)
[2025-02-13 20:08:40,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:40,712][root][INFO] - Training Epoch: 1/2, step 5998/7134 completed (loss: 0.09858492761850357, acc: 0.9682539701461792)
[2025-02-13 20:08:40,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41,070][root][INFO] - Training Epoch: 1/2, step 5999/7134 completed (loss: 0.08004472404718399, acc: 0.9806763529777527)
[2025-02-13 20:08:41,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41,425][root][INFO] - Training Epoch: 1/2, step 6000/7134 completed (loss: 0.069303497672081, acc: 0.9929577708244324)
[2025-02-13 20:08:41,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41,795][root][INFO] - Training Epoch: 1/2, step 6001/7134 completed (loss: 0.06192293390631676, acc: 0.987730085849762)
[2025-02-13 20:08:41,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:42,159][root][INFO] - Training Epoch: 1/2, step 6002/7134 completed (loss: 0.17492873966693878, acc: 0.9504132270812988)
[2025-02-13 20:08:42,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:42,548][root][INFO] - Training Epoch: 1/2, step 6003/7134 completed (loss: 0.1510852724313736, acc: 0.9545454382896423)
[2025-02-13 20:08:42,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:42,926][root][INFO] - Training Epoch: 1/2, step 6004/7134 completed (loss: 0.4729452431201935, acc: 0.9017341136932373)
[2025-02-13 20:08:43,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:43,282][root][INFO] - Training Epoch: 1/2, step 6005/7134 completed (loss: 0.23995471000671387, acc: 0.9470198750495911)
[2025-02-13 20:08:43,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:43,664][root][INFO] - Training Epoch: 1/2, step 6006/7134 completed (loss: 0.2404845803976059, acc: 0.9200000166893005)
[2025-02-13 20:08:43,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:44,038][root][INFO] - Training Epoch: 1/2, step 6007/7134 completed (loss: 0.16187328100204468, acc: 0.9558823704719543)
[2025-02-13 20:08:44,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:44,400][root][INFO] - Training Epoch: 1/2, step 6008/7134 completed (loss: 0.17444607615470886, acc: 0.9440559148788452)
[2025-02-13 20:08:44,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:44,753][root][INFO] - Training Epoch: 1/2, step 6009/7134 completed (loss: 0.18484754860401154, acc: 0.9548872113227844)
[2025-02-13 20:08:44,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:45,114][root][INFO] - Training Epoch: 1/2, step 6010/7134 completed (loss: 0.27563151717185974, acc: 0.9240506291389465)
[2025-02-13 20:08:45,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:45,490][root][INFO] - Training Epoch: 1/2, step 6011/7134 completed (loss: 0.29723745584487915, acc: 0.9285714030265808)
[2025-02-13 20:08:45,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:45,870][root][INFO] - Training Epoch: 1/2, step 6012/7134 completed (loss: 0.14464497566223145, acc: 0.9454545378684998)
[2025-02-13 20:08:46,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:46,237][root][INFO] - Training Epoch: 1/2, step 6013/7134 completed (loss: 0.2281351238489151, acc: 0.9382022619247437)
[2025-02-13 20:08:46,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:46,590][root][INFO] - Training Epoch: 1/2, step 6014/7134 completed (loss: 0.27379366755485535, acc: 0.9370078444480896)
[2025-02-13 20:08:46,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:46,976][root][INFO] - Training Epoch: 1/2, step 6015/7134 completed (loss: 0.30252277851104736, acc: 0.9099099040031433)
[2025-02-13 20:08:47,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:47,335][root][INFO] - Training Epoch: 1/2, step 6016/7134 completed (loss: 0.3424403965473175, acc: 0.9189189076423645)
[2025-02-13 20:08:47,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:47,666][root][INFO] - Training Epoch: 1/2, step 6017/7134 completed (loss: 0.2816299796104431, acc: 0.9230769276618958)
[2025-02-13 20:08:47,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:48,027][root][INFO] - Training Epoch: 1/2, step 6018/7134 completed (loss: 0.20200663805007935, acc: 0.9615384340286255)
[2025-02-13 20:08:48,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:48,352][root][INFO] - Training Epoch: 1/2, step 6019/7134 completed (loss: 0.26336967945098877, acc: 0.9166666865348816)
[2025-02-13 20:08:48,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:48,701][root][INFO] - Training Epoch: 1/2, step 6020/7134 completed (loss: 0.047284286469221115, acc: 0.9841269850730896)
[2025-02-13 20:08:48,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:49,093][root][INFO] - Training Epoch: 1/2, step 6021/7134 completed (loss: 0.17477349936962128, acc: 0.9512194991111755)
[2025-02-13 20:08:49,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:49,458][root][INFO] - Training Epoch: 1/2, step 6022/7134 completed (loss: 0.16321709752082825, acc: 0.9605262875556946)
[2025-02-13 20:08:49,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:49,837][root][INFO] - Training Epoch: 1/2, step 6023/7134 completed (loss: 0.0694563090801239, acc: 0.9659090638160706)
[2025-02-13 20:08:49,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:50,234][root][INFO] - Training Epoch: 1/2, step 6024/7134 completed (loss: 0.01853403076529503, acc: 1.0)
[2025-02-13 20:08:50,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:50,612][root][INFO] - Training Epoch: 1/2, step 6025/7134 completed (loss: 0.1509360671043396, acc: 0.9508196711540222)
[2025-02-13 20:08:50,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:50,993][root][INFO] - Training Epoch: 1/2, step 6026/7134 completed (loss: 0.5343902111053467, acc: 0.8474576473236084)
[2025-02-13 20:08:51,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:51,396][root][INFO] - Training Epoch: 1/2, step 6027/7134 completed (loss: 0.08762016147375107, acc: 0.9775280952453613)
[2025-02-13 20:08:51,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:51,798][root][INFO] - Training Epoch: 1/2, step 6028/7134 completed (loss: 0.21539945900440216, acc: 0.949999988079071)
[2025-02-13 20:08:51,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:52,221][root][INFO] - Training Epoch: 1/2, step 6029/7134 completed (loss: 0.13033559918403625, acc: 0.9577465057373047)
[2025-02-13 20:08:52,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:52,691][root][INFO] - Training Epoch: 1/2, step 6030/7134 completed (loss: 0.13540418446063995, acc: 0.9696969985961914)
[2025-02-13 20:08:52,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:53,086][root][INFO] - Training Epoch: 1/2, step 6031/7134 completed (loss: 0.14943164587020874, acc: 0.9651162624359131)
[2025-02-13 20:08:53,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:53,481][root][INFO] - Training Epoch: 1/2, step 6032/7134 completed (loss: 0.09056088328361511, acc: 0.9801980257034302)
[2025-02-13 20:08:53,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:53,967][root][INFO] - Training Epoch: 1/2, step 6033/7134 completed (loss: 0.21798738837242126, acc: 0.9545454382896423)
[2025-02-13 20:08:54,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:54,359][root][INFO] - Training Epoch: 1/2, step 6034/7134 completed (loss: 0.32234686613082886, acc: 0.89552241563797)
[2025-02-13 20:08:54,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:54,738][root][INFO] - Training Epoch: 1/2, step 6035/7134 completed (loss: 0.40775904059410095, acc: 0.9061033129692078)
[2025-02-13 20:08:54,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:55,117][root][INFO] - Training Epoch: 1/2, step 6036/7134 completed (loss: 0.33774110674858093, acc: 0.9269663095474243)
[2025-02-13 20:08:55,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:55,499][root][INFO] - Training Epoch: 1/2, step 6037/7134 completed (loss: 0.21612657606601715, acc: 0.936170220375061)
[2025-02-13 20:08:55,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:55,873][root][INFO] - Training Epoch: 1/2, step 6038/7134 completed (loss: 0.1633269190788269, acc: 0.9607843160629272)
[2025-02-13 20:08:56,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:56,261][root][INFO] - Training Epoch: 1/2, step 6039/7134 completed (loss: 0.23570914566516876, acc: 0.9512194991111755)
[2025-02-13 20:08:56,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:56,680][root][INFO] - Training Epoch: 1/2, step 6040/7134 completed (loss: 0.12301359325647354, acc: 0.9823788404464722)
[2025-02-13 20:08:56,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57,052][root][INFO] - Training Epoch: 1/2, step 6041/7134 completed (loss: 0.2995060980319977, acc: 0.9193548560142517)
[2025-02-13 20:08:57,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57,460][root][INFO] - Training Epoch: 1/2, step 6042/7134 completed (loss: 0.10580302029848099, acc: 0.9770641922950745)
[2025-02-13 20:08:57,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57,894][root][INFO] - Training Epoch: 1/2, step 6043/7134 completed (loss: 0.15734265744686127, acc: 0.9462365508079529)
[2025-02-13 20:08:58,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:58,288][root][INFO] - Training Epoch: 1/2, step 6044/7134 completed (loss: 0.09516637027263641, acc: 0.9666666388511658)
[2025-02-13 20:08:58,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:58,750][root][INFO] - Training Epoch: 1/2, step 6045/7134 completed (loss: 0.18306653201580048, acc: 0.9425837397575378)
[2025-02-13 20:08:58,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:59,168][root][INFO] - Training Epoch: 1/2, step 6046/7134 completed (loss: 0.2505585849285126, acc: 0.931506872177124)
[2025-02-13 20:08:59,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:59,570][root][INFO] - Training Epoch: 1/2, step 6047/7134 completed (loss: 0.3822845220565796, acc: 0.918367326259613)
[2025-02-13 20:08:59,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:59,974][root][INFO] - Training Epoch: 1/2, step 6048/7134 completed (loss: 0.2697239816188812, acc: 0.9186992049217224)
[2025-02-13 20:09:00,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:00,353][root][INFO] - Training Epoch: 1/2, step 6049/7134 completed (loss: 0.1412087231874466, acc: 0.9580419659614563)
[2025-02-13 20:09:00,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:00,767][root][INFO] - Training Epoch: 1/2, step 6050/7134 completed (loss: 0.32687780261039734, acc: 0.9017341136932373)
[2025-02-13 20:09:00,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:01,217][root][INFO] - Training Epoch: 1/2, step 6051/7134 completed (loss: 0.4377618730068207, acc: 0.8920454382896423)
[2025-02-13 20:09:01,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:01,601][root][INFO] - Training Epoch: 1/2, step 6052/7134 completed (loss: 0.3739434480667114, acc: 0.8866666555404663)
[2025-02-13 20:09:01,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:01,979][root][INFO] - Training Epoch: 1/2, step 6053/7134 completed (loss: 0.2755297124385834, acc: 0.9292035102844238)
[2025-02-13 20:09:02,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:02,360][root][INFO] - Training Epoch: 1/2, step 6054/7134 completed (loss: 0.16817182302474976, acc: 0.9481865167617798)
[2025-02-13 20:09:02,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:02,741][root][INFO] - Training Epoch: 1/2, step 6055/7134 completed (loss: 0.24511010944843292, acc: 0.9415584206581116)
[2025-02-13 20:09:02,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:03,110][root][INFO] - Training Epoch: 1/2, step 6056/7134 completed (loss: 0.1749694049358368, acc: 0.9279279112815857)
[2025-02-13 20:09:03,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:03,505][root][INFO] - Training Epoch: 1/2, step 6057/7134 completed (loss: 0.3032850921154022, acc: 0.9350649118423462)
[2025-02-13 20:09:03,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:03,918][root][INFO] - Training Epoch: 1/2, step 6058/7134 completed (loss: 0.17149946093559265, acc: 0.9491525292396545)
[2025-02-13 20:09:04,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:04,293][root][INFO] - Training Epoch: 1/2, step 6059/7134 completed (loss: 0.14166170358657837, acc: 0.9589040875434875)
[2025-02-13 20:09:04,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:04,663][root][INFO] - Training Epoch: 1/2, step 6060/7134 completed (loss: 0.17366814613342285, acc: 0.9567901492118835)
[2025-02-13 20:09:04,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:05,062][root][INFO] - Training Epoch: 1/2, step 6061/7134 completed (loss: 0.2822880148887634, acc: 0.922535240650177)
[2025-02-13 20:09:05,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:05,424][root][INFO] - Training Epoch: 1/2, step 6062/7134 completed (loss: 0.30233460664749146, acc: 0.920634925365448)
[2025-02-13 20:09:05,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:05,847][root][INFO] - Training Epoch: 1/2, step 6063/7134 completed (loss: 0.26149022579193115, acc: 0.9491525292396545)
[2025-02-13 20:09:06,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:06,309][root][INFO] - Training Epoch: 1/2, step 6064/7134 completed (loss: 0.2691181004047394, acc: 0.9378882050514221)
[2025-02-13 20:09:06,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:06,691][root][INFO] - Training Epoch: 1/2, step 6065/7134 completed (loss: 0.18774452805519104, acc: 0.9492385983467102)
[2025-02-13 20:09:06,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:07,052][root][INFO] - Training Epoch: 1/2, step 6066/7134 completed (loss: 0.39138883352279663, acc: 0.8888888955116272)
[2025-02-13 20:09:07,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:07,424][root][INFO] - Training Epoch: 1/2, step 6067/7134 completed (loss: 0.44795456528663635, acc: 0.8947368264198303)
[2025-02-13 20:09:07,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:07,858][root][INFO] - Training Epoch: 1/2, step 6068/7134 completed (loss: 0.5164062976837158, acc: 0.8216215968132019)
[2025-02-13 20:09:07,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:08,217][root][INFO] - Training Epoch: 1/2, step 6069/7134 completed (loss: 0.32476094365119934, acc: 0.9058823585510254)
[2025-02-13 20:09:08,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:08,602][root][INFO] - Training Epoch: 1/2, step 6070/7134 completed (loss: 0.2343740016222, acc: 0.9257143139839172)
[2025-02-13 20:09:08,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:08,951][root][INFO] - Training Epoch: 1/2, step 6071/7134 completed (loss: 0.38519471883773804, acc: 0.8994082808494568)
[2025-02-13 20:09:09,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:09,311][root][INFO] - Training Epoch: 1/2, step 6072/7134 completed (loss: 0.4251069128513336, acc: 0.8583333492279053)
[2025-02-13 20:09:09,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:09,685][root][INFO] - Training Epoch: 1/2, step 6073/7134 completed (loss: 0.2429903894662857, acc: 0.9299362897872925)
[2025-02-13 20:09:09,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:10,082][root][INFO] - Training Epoch: 1/2, step 6074/7134 completed (loss: 0.38700151443481445, acc: 0.9354838728904724)
[2025-02-13 20:09:10,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:10,523][root][INFO] - Training Epoch: 1/2, step 6075/7134 completed (loss: 0.2168910950422287, acc: 0.9368420839309692)
[2025-02-13 20:09:10,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:10,923][root][INFO] - Training Epoch: 1/2, step 6076/7134 completed (loss: 0.3923982083797455, acc: 0.8877005577087402)
[2025-02-13 20:09:11,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:11,328][root][INFO] - Training Epoch: 1/2, step 6077/7134 completed (loss: 0.20448237657546997, acc: 0.9467455744743347)
[2025-02-13 20:09:11,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:11,728][root][INFO] - Training Epoch: 1/2, step 6078/7134 completed (loss: 0.4331166744232178, acc: 0.9152542352676392)
[2025-02-13 20:09:11,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:12,125][root][INFO] - Training Epoch: 1/2, step 6079/7134 completed (loss: 0.18390265107154846, acc: 0.9545454382896423)
[2025-02-13 20:09:12,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:12,501][root][INFO] - Training Epoch: 1/2, step 6080/7134 completed (loss: 0.15569370985031128, acc: 0.9482758641242981)
[2025-02-13 20:09:12,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:12,874][root][INFO] - Training Epoch: 1/2, step 6081/7134 completed (loss: 0.2696501910686493, acc: 0.9459459185600281)
[2025-02-13 20:09:13,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:13,231][root][INFO] - Training Epoch: 1/2, step 6082/7134 completed (loss: 0.14506796002388, acc: 0.9597315192222595)
[2025-02-13 20:09:13,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:13,619][root][INFO] - Training Epoch: 1/2, step 6083/7134 completed (loss: 0.11474777013063431, acc: 0.9689119458198547)
[2025-02-13 20:09:13,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:14,013][root][INFO] - Training Epoch: 1/2, step 6084/7134 completed (loss: 0.1364728808403015, acc: 0.9666666388511658)
[2025-02-13 20:09:14,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:14,392][root][INFO] - Training Epoch: 1/2, step 6085/7134 completed (loss: 0.21834152936935425, acc: 0.9513888955116272)
[2025-02-13 20:09:14,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:14,793][root][INFO] - Training Epoch: 1/2, step 6086/7134 completed (loss: 0.06578826904296875, acc: 0.9781420826911926)
[2025-02-13 20:09:14,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:15,151][root][INFO] - Training Epoch: 1/2, step 6087/7134 completed (loss: 0.16981449723243713, acc: 0.9656862616539001)
[2025-02-13 20:09:15,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:15,513][root][INFO] - Training Epoch: 1/2, step 6088/7134 completed (loss: 0.25362011790275574, acc: 0.929729700088501)
[2025-02-13 20:09:15,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:15,905][root][INFO] - Training Epoch: 1/2, step 6089/7134 completed (loss: 0.13769973814487457, acc: 0.9485714435577393)
[2025-02-13 20:09:16,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:16,305][root][INFO] - Training Epoch: 1/2, step 6090/7134 completed (loss: 0.11684439331293106, acc: 0.9786096215248108)
[2025-02-13 20:09:16,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:16,730][root][INFO] - Training Epoch: 1/2, step 6091/7134 completed (loss: 0.09915915876626968, acc: 0.987261176109314)
[2025-02-13 20:09:16,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:17,101][root][INFO] - Training Epoch: 1/2, step 6092/7134 completed (loss: 0.23171782493591309, acc: 0.9454545378684998)
[2025-02-13 20:09:17,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:17,514][root][INFO] - Training Epoch: 1/2, step 6093/7134 completed (loss: 0.20455756783485413, acc: 0.9402984976768494)
[2025-02-13 20:09:17,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:17,912][root][INFO] - Training Epoch: 1/2, step 6094/7134 completed (loss: 0.380403071641922, acc: 0.9034090638160706)
[2025-02-13 20:09:18,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:18,280][root][INFO] - Training Epoch: 1/2, step 6095/7134 completed (loss: 0.16617651283740997, acc: 0.9570552110671997)
[2025-02-13 20:09:18,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:18,675][root][INFO] - Training Epoch: 1/2, step 6096/7134 completed (loss: 0.31560254096984863, acc: 0.903553307056427)
[2025-02-13 20:09:18,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:19,075][root][INFO] - Training Epoch: 1/2, step 6097/7134 completed (loss: 0.244008868932724, acc: 0.9503105878829956)
[2025-02-13 20:09:19,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:19,509][root][INFO] - Training Epoch: 1/2, step 6098/7134 completed (loss: 0.20469965040683746, acc: 0.9356725215911865)
[2025-02-13 20:09:19,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:19,928][root][INFO] - Training Epoch: 1/2, step 6099/7134 completed (loss: 0.185115247964859, acc: 0.9591836929321289)
[2025-02-13 20:09:20,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:20,314][root][INFO] - Training Epoch: 1/2, step 6100/7134 completed (loss: 0.24898558855056763, acc: 0.9615384340286255)
[2025-02-13 20:09:20,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:20,670][root][INFO] - Training Epoch: 1/2, step 6101/7134 completed (loss: 0.12392988801002502, acc: 0.9583333134651184)
[2025-02-13 20:09:20,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:21,011][root][INFO] - Training Epoch: 1/2, step 6102/7134 completed (loss: 0.2627103328704834, acc: 0.9296875)
[2025-02-13 20:09:21,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:21,426][root][INFO] - Training Epoch: 1/2, step 6103/7134 completed (loss: 0.13934597373008728, acc: 0.9727272987365723)
[2025-02-13 20:09:21,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:21,825][root][INFO] - Training Epoch: 1/2, step 6104/7134 completed (loss: 0.4552594721317291, acc: 0.8783783912658691)
[2025-02-13 20:09:21,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:22,204][root][INFO] - Training Epoch: 1/2, step 6105/7134 completed (loss: 0.613179624080658, acc: 0.8666666746139526)
[2025-02-13 20:09:22,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:22,590][root][INFO] - Training Epoch: 1/2, step 6106/7134 completed (loss: 0.2382243275642395, acc: 0.9370078444480896)
[2025-02-13 20:09:22,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:22,953][root][INFO] - Training Epoch: 1/2, step 6107/7134 completed (loss: 0.05242801085114479, acc: 0.9914529919624329)
[2025-02-13 20:09:23,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:23,336][root][INFO] - Training Epoch: 1/2, step 6108/7134 completed (loss: 0.27406957745552063, acc: 0.93388432264328)
[2025-02-13 20:09:23,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:23,699][root][INFO] - Training Epoch: 1/2, step 6109/7134 completed (loss: 0.14956168830394745, acc: 0.95652174949646)
[2025-02-13 20:09:23,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:24,098][root][INFO] - Training Epoch: 1/2, step 6110/7134 completed (loss: 0.5517566204071045, acc: 0.875)
[2025-02-13 20:09:24,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:24,481][root][INFO] - Training Epoch: 1/2, step 6111/7134 completed (loss: 0.3746437132358551, acc: 0.9239130616188049)
[2025-02-13 20:09:24,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:24,864][root][INFO] - Training Epoch: 1/2, step 6112/7134 completed (loss: 0.33596792817115784, acc: 0.9281045794487)
[2025-02-13 20:09:25,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:25,247][root][INFO] - Training Epoch: 1/2, step 6113/7134 completed (loss: 0.24852317571640015, acc: 0.957317054271698)
[2025-02-13 20:09:25,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:25,633][root][INFO] - Training Epoch: 1/2, step 6114/7134 completed (loss: 0.1738167554140091, acc: 0.9650349617004395)
[2025-02-13 20:09:25,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:25,991][root][INFO] - Training Epoch: 1/2, step 6115/7134 completed (loss: 0.1388862133026123, acc: 0.9679999947547913)
[2025-02-13 20:09:26,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:26,341][root][INFO] - Training Epoch: 1/2, step 6116/7134 completed (loss: 0.39931046962738037, acc: 0.9354838728904724)
[2025-02-13 20:09:26,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:26,732][root][INFO] - Training Epoch: 1/2, step 6117/7134 completed (loss: 0.2437938004732132, acc: 0.9341317415237427)
[2025-02-13 20:09:26,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:27,138][root][INFO] - Training Epoch: 1/2, step 6118/7134 completed (loss: 0.09487316757440567, acc: 0.9682539701461792)
[2025-02-13 20:09:27,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:27,529][root][INFO] - Training Epoch: 1/2, step 6119/7134 completed (loss: 0.12122157216072083, acc: 0.9621621370315552)
[2025-02-13 20:09:27,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:27,937][root][INFO] - Training Epoch: 1/2, step 6120/7134 completed (loss: 0.08951529115438461, acc: 0.9772727489471436)
[2025-02-13 20:09:28,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:28,354][root][INFO] - Training Epoch: 1/2, step 6121/7134 completed (loss: 0.202142596244812, acc: 0.9527559280395508)
[2025-02-13 20:09:28,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:28,724][root][INFO] - Training Epoch: 1/2, step 6122/7134 completed (loss: 0.09502233564853668, acc: 0.9838709831237793)
[2025-02-13 20:09:28,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:29,089][root][INFO] - Training Epoch: 1/2, step 6123/7134 completed (loss: 0.21363230049610138, acc: 0.9658119678497314)
[2025-02-13 20:09:29,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:29,477][root][INFO] - Training Epoch: 1/2, step 6124/7134 completed (loss: 0.10902418941259384, acc: 0.9663865566253662)
[2025-02-13 20:09:29,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:29,853][root][INFO] - Training Epoch: 1/2, step 6125/7134 completed (loss: 0.3710745573043823, acc: 0.9532710313796997)
[2025-02-13 20:09:29,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:30,214][root][INFO] - Training Epoch: 1/2, step 6126/7134 completed (loss: 0.18898701667785645, acc: 0.9735099077224731)
[2025-02-13 20:09:30,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:30,570][root][INFO] - Training Epoch: 1/2, step 6127/7134 completed (loss: 0.051318567246198654, acc: 1.0)
[2025-02-13 20:09:30,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:30,900][root][INFO] - Training Epoch: 1/2, step 6128/7134 completed (loss: 0.18853236734867096, acc: 0.9495798349380493)
[2025-02-13 20:09:31,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:31,265][root][INFO] - Training Epoch: 1/2, step 6129/7134 completed (loss: 0.23089782893657684, acc: 0.9467455744743347)
[2025-02-13 20:09:31,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:31,616][root][INFO] - Training Epoch: 1/2, step 6130/7134 completed (loss: 0.38036635518074036, acc: 0.9179104566574097)
[2025-02-13 20:09:31,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:31,987][root][INFO] - Training Epoch: 1/2, step 6131/7134 completed (loss: 0.3328631520271301, acc: 0.9223300814628601)
[2025-02-13 20:09:32,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:32,336][root][INFO] - Training Epoch: 1/2, step 6132/7134 completed (loss: 0.2041672170162201, acc: 0.9358974099159241)
[2025-02-13 20:09:32,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:32,721][root][INFO] - Training Epoch: 1/2, step 6133/7134 completed (loss: 0.18730305135250092, acc: 0.9663461446762085)
[2025-02-13 20:09:32,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:33,078][root][INFO] - Training Epoch: 1/2, step 6134/7134 completed (loss: 0.33727219700813293, acc: 0.9075144529342651)
[2025-02-13 20:09:33,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:33,437][root][INFO] - Training Epoch: 1/2, step 6135/7134 completed (loss: 0.12045493721961975, acc: 0.9767441749572754)
[2025-02-13 20:09:33,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:33,795][root][INFO] - Training Epoch: 1/2, step 6136/7134 completed (loss: 0.1799454540014267, acc: 0.9580838084220886)
[2025-02-13 20:09:33,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:34,163][root][INFO] - Training Epoch: 1/2, step 6137/7134 completed (loss: 0.19747784733772278, acc: 0.9740259647369385)
[2025-02-13 20:09:34,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:34,539][root][INFO] - Training Epoch: 1/2, step 6138/7134 completed (loss: 0.2682894766330719, acc: 0.9509202241897583)
[2025-02-13 20:09:34,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:34,927][root][INFO] - Training Epoch: 1/2, step 6139/7134 completed (loss: 0.296496719121933, acc: 0.9047619104385376)
[2025-02-13 20:09:35,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:35,315][root][INFO] - Training Epoch: 1/2, step 6140/7134 completed (loss: 0.17890848219394684, acc: 0.9615384340286255)
[2025-02-13 20:09:35,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:35,685][root][INFO] - Training Epoch: 1/2, step 6141/7134 completed (loss: 0.357915997505188, acc: 0.9130434989929199)
[2025-02-13 20:09:35,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:36,059][root][INFO] - Training Epoch: 1/2, step 6142/7134 completed (loss: 0.1669652909040451, acc: 0.9599999785423279)
[2025-02-13 20:09:36,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:36,419][root][INFO] - Training Epoch: 1/2, step 6143/7134 completed (loss: 0.18499290943145752, acc: 0.9696969985961914)
[2025-02-13 20:09:36,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:36,794][root][INFO] - Training Epoch: 1/2, step 6144/7134 completed (loss: 0.1365503966808319, acc: 0.9589040875434875)
[2025-02-13 20:09:36,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:37,177][root][INFO] - Training Epoch: 1/2, step 6145/7134 completed (loss: 0.2841542065143585, acc: 0.9398906826972961)
[2025-02-13 20:09:37,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:37,558][root][INFO] - Training Epoch: 1/2, step 6146/7134 completed (loss: 0.28631392121315, acc: 0.9397590160369873)
[2025-02-13 20:09:37,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:37,928][root][INFO] - Training Epoch: 1/2, step 6147/7134 completed (loss: 0.18659569323062897, acc: 0.9527559280395508)
[2025-02-13 20:09:38,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:38,311][root][INFO] - Training Epoch: 1/2, step 6148/7134 completed (loss: 0.1632700264453888, acc: 0.9448275566101074)
[2025-02-13 20:09:38,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:38,684][root][INFO] - Training Epoch: 1/2, step 6149/7134 completed (loss: 0.20032069087028503, acc: 0.9602649211883545)
[2025-02-13 20:09:38,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39,049][root][INFO] - Training Epoch: 1/2, step 6150/7134 completed (loss: 0.13258123397827148, acc: 0.9510489702224731)
[2025-02-13 20:09:39,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39,361][root][INFO] - Training Epoch: 1/2, step 6151/7134 completed (loss: 0.13015583157539368, acc: 0.9681528806686401)
[2025-02-13 20:09:39,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39,741][root][INFO] - Training Epoch: 1/2, step 6152/7134 completed (loss: 0.2140999436378479, acc: 0.977142870426178)
[2025-02-13 20:09:39,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:40,127][root][INFO] - Training Epoch: 1/2, step 6153/7134 completed (loss: 0.0850580632686615, acc: 0.9876543283462524)
[2025-02-13 20:09:40,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:40,506][root][INFO] - Training Epoch: 1/2, step 6154/7134 completed (loss: 0.17468388378620148, acc: 0.9613259434700012)
[2025-02-13 20:09:40,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:40,882][root][INFO] - Training Epoch: 1/2, step 6155/7134 completed (loss: 0.19193249940872192, acc: 0.9644970297813416)
[2025-02-13 20:09:41,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:41,229][root][INFO] - Training Epoch: 1/2, step 6156/7134 completed (loss: 0.1589285135269165, acc: 0.9556962251663208)
[2025-02-13 20:09:41,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:41,608][root][INFO] - Training Epoch: 1/2, step 6157/7134 completed (loss: 0.1537143439054489, acc: 0.9683544039726257)
[2025-02-13 20:09:41,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:41,983][root][INFO] - Training Epoch: 1/2, step 6158/7134 completed (loss: 0.2012028992176056, acc: 0.9473684430122375)
[2025-02-13 20:09:42,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:42,358][root][INFO] - Training Epoch: 1/2, step 6159/7134 completed (loss: 0.15616285800933838, acc: 0.9702380895614624)
[2025-02-13 20:09:42,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:42,771][root][INFO] - Training Epoch: 1/2, step 6160/7134 completed (loss: 0.318733274936676, acc: 0.9235668778419495)
[2025-02-13 20:09:42,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:43,163][root][INFO] - Training Epoch: 1/2, step 6161/7134 completed (loss: 0.2796298563480377, acc: 0.9527559280395508)
[2025-02-13 20:09:43,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:43,591][root][INFO] - Training Epoch: 1/2, step 6162/7134 completed (loss: 0.48644304275512695, acc: 0.9038461446762085)
[2025-02-13 20:09:43,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:43,979][root][INFO] - Training Epoch: 1/2, step 6163/7134 completed (loss: 0.4364906847476959, acc: 0.9224806427955627)
[2025-02-13 20:09:44,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:44,375][root][INFO] - Training Epoch: 1/2, step 6164/7134 completed (loss: 0.24972064793109894, acc: 0.9451219439506531)
[2025-02-13 20:09:44,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:44,750][root][INFO] - Training Epoch: 1/2, step 6165/7134 completed (loss: 0.29610973596572876, acc: 0.9312977194786072)
[2025-02-13 20:09:44,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:45,110][root][INFO] - Training Epoch: 1/2, step 6166/7134 completed (loss: 0.14674295485019684, acc: 0.970802903175354)
[2025-02-13 20:09:45,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:45,517][root][INFO] - Training Epoch: 1/2, step 6167/7134 completed (loss: 0.06653118878602982, acc: 0.9709302186965942)
[2025-02-13 20:09:45,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:45,887][root][INFO] - Training Epoch: 1/2, step 6168/7134 completed (loss: 0.10007624328136444, acc: 0.9838709831237793)
[2025-02-13 20:09:46,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:46,234][root][INFO] - Training Epoch: 1/2, step 6169/7134 completed (loss: 0.14839617908000946, acc: 0.9879518151283264)
[2025-02-13 20:09:46,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:46,609][root][INFO] - Training Epoch: 1/2, step 6170/7134 completed (loss: 0.13432389497756958, acc: 0.9496402740478516)
[2025-02-13 20:09:46,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:46,982][root][INFO] - Training Epoch: 1/2, step 6171/7134 completed (loss: 0.04687366262078285, acc: 0.9945054650306702)
[2025-02-13 20:09:47,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:47,326][root][INFO] - Training Epoch: 1/2, step 6172/7134 completed (loss: 0.09815017879009247, acc: 0.96875)
[2025-02-13 20:09:47,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:47,694][root][INFO] - Training Epoch: 1/2, step 6173/7134 completed (loss: 0.11385553330183029, acc: 0.9551281929016113)
[2025-02-13 20:09:47,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48,071][root][INFO] - Training Epoch: 1/2, step 6174/7134 completed (loss: 0.06331341713666916, acc: 0.9818181991577148)
[2025-02-13 20:09:48,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48,446][root][INFO] - Training Epoch: 1/2, step 6175/7134 completed (loss: 0.13288578391075134, acc: 0.9629629850387573)
[2025-02-13 20:09:48,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48,807][root][INFO] - Training Epoch: 1/2, step 6176/7134 completed (loss: 0.10497380793094635, acc: 0.9878048896789551)
[2025-02-13 20:09:48,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:49,176][root][INFO] - Training Epoch: 1/2, step 6177/7134 completed (loss: 0.06587774306535721, acc: 0.9890109896659851)
[2025-02-13 20:09:49,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:49,535][root][INFO] - Training Epoch: 1/2, step 6178/7134 completed (loss: 0.13401249051094055, acc: 0.9738562107086182)
[2025-02-13 20:09:49,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:49,877][root][INFO] - Training Epoch: 1/2, step 6179/7134 completed (loss: 0.26416948437690735, acc: 0.9333333373069763)
[2025-02-13 20:09:50,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:50,272][root][INFO] - Training Epoch: 1/2, step 6180/7134 completed (loss: 0.8801756501197815, acc: 0.8085106611251831)
[2025-02-13 20:09:50,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:50,647][root][INFO] - Training Epoch: 1/2, step 6181/7134 completed (loss: 1.0090773105621338, acc: 0.7777777910232544)
[2025-02-13 20:09:50,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:51,019][root][INFO] - Training Epoch: 1/2, step 6182/7134 completed (loss: 0.4746568202972412, acc: 0.8837209343910217)
[2025-02-13 20:09:51,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:51,377][root][INFO] - Training Epoch: 1/2, step 6183/7134 completed (loss: 0.6561386585235596, acc: 0.8395061492919922)
[2025-02-13 20:09:51,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:51,765][root][INFO] - Training Epoch: 1/2, step 6184/7134 completed (loss: 0.7801652550697327, acc: 0.843478262424469)
[2025-02-13 20:09:51,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:52,139][root][INFO] - Training Epoch: 1/2, step 6185/7134 completed (loss: 0.2635704576969147, acc: 0.9274193644523621)
[2025-02-13 20:09:52,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:52,524][root][INFO] - Training Epoch: 1/2, step 6186/7134 completed (loss: 0.1807437390089035, acc: 0.9694656729698181)
[2025-02-13 20:09:52,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:52,885][root][INFO] - Training Epoch: 1/2, step 6187/7134 completed (loss: 0.23301884531974792, acc: 0.9415204524993896)
[2025-02-13 20:09:53,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:53,303][root][INFO] - Training Epoch: 1/2, step 6188/7134 completed (loss: 0.26446956396102905, acc: 0.9411764740943909)
[2025-02-13 20:09:53,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:53,713][root][INFO] - Training Epoch: 1/2, step 6189/7134 completed (loss: 0.3960663974285126, acc: 0.9230769276618958)
[2025-02-13 20:09:53,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:54,076][root][INFO] - Training Epoch: 1/2, step 6190/7134 completed (loss: 0.4816876947879791, acc: 0.9071428775787354)
[2025-02-13 20:09:54,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:54,464][root][INFO] - Training Epoch: 1/2, step 6191/7134 completed (loss: 0.22131645679473877, acc: 0.934959352016449)
[2025-02-13 20:09:54,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:54,860][root][INFO] - Training Epoch: 1/2, step 6192/7134 completed (loss: 0.2117232233285904, acc: 0.9463414549827576)
[2025-02-13 20:09:55,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:55,236][root][INFO] - Training Epoch: 1/2, step 6193/7134 completed (loss: 0.23114927113056183, acc: 0.9292929172515869)
[2025-02-13 20:09:55,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:55,635][root][INFO] - Training Epoch: 1/2, step 6194/7134 completed (loss: 0.23307885229587555, acc: 0.9262295365333557)
[2025-02-13 20:09:55,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:56,046][root][INFO] - Training Epoch: 1/2, step 6195/7134 completed (loss: 0.22510161995887756, acc: 0.9150943160057068)
[2025-02-13 20:09:56,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:56,481][root][INFO] - Training Epoch: 1/2, step 6196/7134 completed (loss: 0.11022945493459702, acc: 0.9567901492118835)
[2025-02-13 20:09:56,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:56,896][root][INFO] - Training Epoch: 1/2, step 6197/7134 completed (loss: 0.26183098554611206, acc: 0.9488636255264282)
[2025-02-13 20:09:57,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:57,283][root][INFO] - Training Epoch: 1/2, step 6198/7134 completed (loss: 0.11024429649114609, acc: 0.9734513163566589)
[2025-02-13 20:09:57,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:57,672][root][INFO] - Training Epoch: 1/2, step 6199/7134 completed (loss: 0.20730805397033691, acc: 0.9682539701461792)
[2025-02-13 20:09:57,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:58,039][root][INFO] - Training Epoch: 1/2, step 6200/7134 completed (loss: 0.0646514967083931, acc: 0.9897435903549194)
[2025-02-13 20:09:58,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:58,426][root][INFO] - Training Epoch: 1/2, step 6201/7134 completed (loss: 0.09149499982595444, acc: 0.9760765433311462)
[2025-02-13 20:09:58,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:58,770][root][INFO] - Training Epoch: 1/2, step 6202/7134 completed (loss: 0.19271452724933624, acc: 0.961904764175415)
[2025-02-13 20:09:58,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:59,167][root][INFO] - Training Epoch: 1/2, step 6203/7134 completed (loss: 0.09199804812669754, acc: 0.9593023061752319)
[2025-02-13 20:09:59,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:59,611][root][INFO] - Training Epoch: 1/2, step 6204/7134 completed (loss: 0.03542345017194748, acc: 1.0)
[2025-02-13 20:09:59,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:59,999][root][INFO] - Training Epoch: 1/2, step 6205/7134 completed (loss: 0.08463849127292633, acc: 0.9928057789802551)
[2025-02-13 20:10:00,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:00,397][root][INFO] - Training Epoch: 1/2, step 6206/7134 completed (loss: 0.14862050116062164, acc: 0.9627329111099243)
[2025-02-13 20:10:00,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:00,794][root][INFO] - Training Epoch: 1/2, step 6207/7134 completed (loss: 0.4841483235359192, acc: 0.8909090757369995)
[2025-02-13 20:10:00,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:01,204][root][INFO] - Training Epoch: 1/2, step 6208/7134 completed (loss: 0.08755552768707275, acc: 0.9635416865348816)
[2025-02-13 20:10:01,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:01,606][root][INFO] - Training Epoch: 1/2, step 6209/7134 completed (loss: 0.12193859368562698, acc: 0.9702380895614624)
[2025-02-13 20:10:01,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:02,089][root][INFO] - Training Epoch: 1/2, step 6210/7134 completed (loss: 0.1783885508775711, acc: 0.9411764740943909)
[2025-02-13 20:10:02,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:02,465][root][INFO] - Training Epoch: 1/2, step 6211/7134 completed (loss: 0.17798584699630737, acc: 0.9515151381492615)
[2025-02-13 20:10:02,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:02,816][root][INFO] - Training Epoch: 1/2, step 6212/7134 completed (loss: 0.14163227379322052, acc: 0.9572649598121643)
[2025-02-13 20:10:02,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:03,160][root][INFO] - Training Epoch: 1/2, step 6213/7134 completed (loss: 0.07093685120344162, acc: 0.9909090995788574)
[2025-02-13 20:10:03,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:03,550][root][INFO] - Training Epoch: 1/2, step 6214/7134 completed (loss: 0.09238924086093903, acc: 0.9830508232116699)
[2025-02-13 20:10:03,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:03,936][root][INFO] - Training Epoch: 1/2, step 6215/7134 completed (loss: 0.20832647383213043, acc: 0.9468598961830139)
[2025-02-13 20:10:04,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:04,310][root][INFO] - Training Epoch: 1/2, step 6216/7134 completed (loss: 0.06028808280825615, acc: 1.0)
[2025-02-13 20:10:04,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:04,692][root][INFO] - Training Epoch: 1/2, step 6217/7134 completed (loss: 0.1010090634226799, acc: 0.9746835231781006)
[2025-02-13 20:10:04,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:05,150][root][INFO] - Training Epoch: 1/2, step 6218/7134 completed (loss: 0.11074372380971909, acc: 0.9741935729980469)
[2025-02-13 20:10:05,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:05,563][root][INFO] - Training Epoch: 1/2, step 6219/7134 completed (loss: 0.16945059597492218, acc: 0.9477124214172363)
[2025-02-13 20:10:05,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:05,941][root][INFO] - Training Epoch: 1/2, step 6220/7134 completed (loss: 0.21310187876224518, acc: 0.9655172228813171)
[2025-02-13 20:10:06,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:06,329][root][INFO] - Training Epoch: 1/2, step 6221/7134 completed (loss: 0.03519885614514351, acc: 0.9925373196601868)
[2025-02-13 20:10:06,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:06,757][root][INFO] - Training Epoch: 1/2, step 6222/7134 completed (loss: 0.3352525234222412, acc: 0.9036144614219666)
[2025-02-13 20:10:06,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:07,183][root][INFO] - Training Epoch: 1/2, step 6223/7134 completed (loss: 0.3252774477005005, acc: 0.9270833134651184)
[2025-02-13 20:10:07,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:07,590][root][INFO] - Training Epoch: 1/2, step 6224/7134 completed (loss: 0.08301857113838196, acc: 0.9729729890823364)
[2025-02-13 20:10:07,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:07,966][root][INFO] - Training Epoch: 1/2, step 6225/7134 completed (loss: 0.1812080293893814, acc: 0.9553072452545166)
[2025-02-13 20:10:08,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:08,380][root][INFO] - Training Epoch: 1/2, step 6226/7134 completed (loss: 0.3919679522514343, acc: 0.9021739363670349)
[2025-02-13 20:10:08,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:08,820][root][INFO] - Training Epoch: 1/2, step 6227/7134 completed (loss: 0.1080569177865982, acc: 0.956250011920929)
[2025-02-13 20:10:08,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:09,228][root][INFO] - Training Epoch: 1/2, step 6228/7134 completed (loss: 0.2026301473379135, acc: 0.936170220375061)
[2025-02-13 20:10:09,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:09,617][root][INFO] - Training Epoch: 1/2, step 6229/7134 completed (loss: 0.33913150429725647, acc: 0.9073171019554138)
[2025-02-13 20:10:09,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:09,994][root][INFO] - Training Epoch: 1/2, step 6230/7134 completed (loss: 0.24939526617527008, acc: 0.9308510422706604)
[2025-02-13 20:10:10,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:10,334][root][INFO] - Training Epoch: 1/2, step 6231/7134 completed (loss: 0.1018882542848587, acc: 0.9649122953414917)
[2025-02-13 20:10:10,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:10,724][root][INFO] - Training Epoch: 1/2, step 6232/7134 completed (loss: 0.18018190562725067, acc: 0.9450549483299255)
[2025-02-13 20:10:10,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:11,156][root][INFO] - Training Epoch: 1/2, step 6233/7134 completed (loss: 0.12016282975673676, acc: 0.977011501789093)
[2025-02-13 20:10:11,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:11,543][root][INFO] - Training Epoch: 1/2, step 6234/7134 completed (loss: 0.27881842851638794, acc: 0.9388889074325562)
[2025-02-13 20:10:11,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:11,990][root][INFO] - Training Epoch: 1/2, step 6235/7134 completed (loss: 0.0645918995141983, acc: 0.9821428656578064)
[2025-02-13 20:10:12,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:12,412][root][INFO] - Training Epoch: 1/2, step 6236/7134 completed (loss: 0.1798442155122757, acc: 0.9497206807136536)
[2025-02-13 20:10:12,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:12,815][root][INFO] - Training Epoch: 1/2, step 6237/7134 completed (loss: 0.3073683977127075, acc: 0.9265536665916443)
[2025-02-13 20:10:12,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:13,179][root][INFO] - Training Epoch: 1/2, step 6238/7134 completed (loss: 0.1155935600399971, acc: 0.9735099077224731)
[2025-02-13 20:10:13,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:13,573][root][INFO] - Training Epoch: 1/2, step 6239/7134 completed (loss: 0.1406128704547882, acc: 0.9533678889274597)
[2025-02-13 20:10:13,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:14,007][root][INFO] - Training Epoch: 1/2, step 6240/7134 completed (loss: 0.04756767302751541, acc: 0.9819276928901672)
[2025-02-13 20:10:14,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:14,419][root][INFO] - Training Epoch: 1/2, step 6241/7134 completed (loss: 0.09482333809137344, acc: 0.9800000190734863)
[2025-02-13 20:10:14,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:14,786][root][INFO] - Training Epoch: 1/2, step 6242/7134 completed (loss: 0.11592443287372589, acc: 0.9591836929321289)
[2025-02-13 20:10:14,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:15,189][root][INFO] - Training Epoch: 1/2, step 6243/7134 completed (loss: 0.17330749332904816, acc: 0.9463087320327759)
[2025-02-13 20:10:15,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:15,559][root][INFO] - Training Epoch: 1/2, step 6244/7134 completed (loss: 0.1363924741744995, acc: 0.9580838084220886)
[2025-02-13 20:10:15,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:15,940][root][INFO] - Training Epoch: 1/2, step 6245/7134 completed (loss: 0.1953369677066803, acc: 0.9585798978805542)
[2025-02-13 20:10:16,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:16,390][root][INFO] - Training Epoch: 1/2, step 6246/7134 completed (loss: 0.06296698749065399, acc: 0.9786096215248108)
[2025-02-13 20:10:16,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:16,822][root][INFO] - Training Epoch: 1/2, step 6247/7134 completed (loss: 0.1817173808813095, acc: 0.95652174949646)
[2025-02-13 20:10:16,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:17,234][root][INFO] - Training Epoch: 1/2, step 6248/7134 completed (loss: 0.3890683352947235, acc: 0.9221556782722473)
[2025-02-13 20:10:17,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:17,618][root][INFO] - Training Epoch: 1/2, step 6249/7134 completed (loss: 0.03141973912715912, acc: 0.9933775067329407)
[2025-02-13 20:10:17,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:17,993][root][INFO] - Training Epoch: 1/2, step 6250/7134 completed (loss: 0.02167261764407158, acc: 1.0)
[2025-02-13 20:10:18,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:18,355][root][INFO] - Training Epoch: 1/2, step 6251/7134 completed (loss: 0.13990892469882965, acc: 0.9602649211883545)
[2025-02-13 20:10:18,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:18,744][root][INFO] - Training Epoch: 1/2, step 6252/7134 completed (loss: 0.060499560087919235, acc: 0.9857142567634583)
[2025-02-13 20:10:18,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:19,123][root][INFO] - Training Epoch: 1/2, step 6253/7134 completed (loss: 0.12650494277477264, acc: 0.9720279574394226)
[2025-02-13 20:10:19,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:19,556][root][INFO] - Training Epoch: 1/2, step 6254/7134 completed (loss: 0.23537081480026245, acc: 0.9552238583564758)
[2025-02-13 20:10:19,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:19,944][root][INFO] - Training Epoch: 1/2, step 6255/7134 completed (loss: 0.15844736993312836, acc: 0.9715909361839294)
[2025-02-13 20:10:20,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:20,305][root][INFO] - Training Epoch: 1/2, step 6256/7134 completed (loss: 0.14693352580070496, acc: 0.9717513918876648)
[2025-02-13 20:10:20,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:20,688][root][INFO] - Training Epoch: 1/2, step 6257/7134 completed (loss: 0.12512660026550293, acc: 0.976331353187561)
[2025-02-13 20:10:20,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:21,052][root][INFO] - Training Epoch: 1/2, step 6258/7134 completed (loss: 0.09533185511827469, acc: 0.9666666388511658)
[2025-02-13 20:10:21,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:21,429][root][INFO] - Training Epoch: 1/2, step 6259/7134 completed (loss: 0.04613586887717247, acc: 0.9937888383865356)
[2025-02-13 20:10:21,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:21,792][root][INFO] - Training Epoch: 1/2, step 6260/7134 completed (loss: 0.10554248094558716, acc: 0.9745222926139832)
[2025-02-13 20:10:21,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:22,185][root][INFO] - Training Epoch: 1/2, step 6261/7134 completed (loss: 0.12659607827663422, acc: 0.9627329111099243)
[2025-02-13 20:10:22,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:22,548][root][INFO] - Training Epoch: 1/2, step 6262/7134 completed (loss: 0.0363435298204422, acc: 0.9876543283462524)
[2025-02-13 20:10:22,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:22,923][root][INFO] - Training Epoch: 1/2, step 6263/7134 completed (loss: 0.04447174072265625, acc: 0.9871794581413269)
[2025-02-13 20:10:23,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:23,302][root][INFO] - Training Epoch: 1/2, step 6264/7134 completed (loss: 0.029141690582036972, acc: 0.9931507110595703)
[2025-02-13 20:10:23,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:23,701][root][INFO] - Training Epoch: 1/2, step 6265/7134 completed (loss: 0.03612560033798218, acc: 0.9938650131225586)
[2025-02-13 20:10:23,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:24,070][root][INFO] - Training Epoch: 1/2, step 6266/7134 completed (loss: 0.03184933960437775, acc: 0.9923076629638672)
[2025-02-13 20:10:24,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:24,454][root][INFO] - Training Epoch: 1/2, step 6267/7134 completed (loss: 0.059503354132175446, acc: 0.9802631735801697)
[2025-02-13 20:10:24,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:24,841][root][INFO] - Training Epoch: 1/2, step 6268/7134 completed (loss: 0.0644739419221878, acc: 0.9875776171684265)
[2025-02-13 20:10:24,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:25,243][root][INFO] - Training Epoch: 1/2, step 6269/7134 completed (loss: 0.023222725838422775, acc: 0.9910714030265808)
[2025-02-13 20:10:25,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:25,623][root][INFO] - Training Epoch: 1/2, step 6270/7134 completed (loss: 0.033589091151952744, acc: 0.9868420958518982)
[2025-02-13 20:10:25,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:26,022][root][INFO] - Training Epoch: 1/2, step 6271/7134 completed (loss: 0.07313168793916702, acc: 0.9886363744735718)
[2025-02-13 20:10:26,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:26,408][root][INFO] - Training Epoch: 1/2, step 6272/7134 completed (loss: 0.012630018405616283, acc: 0.9938650131225586)
[2025-02-13 20:10:26,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:26,791][root][INFO] - Training Epoch: 1/2, step 6273/7134 completed (loss: 0.03086109831929207, acc: 0.9881656765937805)
[2025-02-13 20:10:26,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:27,167][root][INFO] - Training Epoch: 1/2, step 6274/7134 completed (loss: 0.03426153212785721, acc: 0.9940828680992126)
[2025-02-13 20:10:27,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:27,544][root][INFO] - Training Epoch: 1/2, step 6275/7134 completed (loss: 0.08267868310213089, acc: 0.9865771532058716)
[2025-02-13 20:10:27,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:27,900][root][INFO] - Training Epoch: 1/2, step 6276/7134 completed (loss: 0.12726721167564392, acc: 0.9593495726585388)
[2025-02-13 20:10:28,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:28,278][root][INFO] - Training Epoch: 1/2, step 6277/7134 completed (loss: 0.20557689666748047, acc: 0.9245283007621765)
[2025-02-13 20:10:28,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:28,661][root][INFO] - Training Epoch: 1/2, step 6278/7134 completed (loss: 0.04676234722137451, acc: 0.9926470518112183)
[2025-02-13 20:10:28,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:29,040][root][INFO] - Training Epoch: 1/2, step 6279/7134 completed (loss: 0.09510084241628647, acc: 0.9923076629638672)
[2025-02-13 20:10:29,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:29,406][root][INFO] - Training Epoch: 1/2, step 6280/7134 completed (loss: 0.0885872021317482, acc: 0.9776119589805603)
[2025-02-13 20:10:29,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:29,769][root][INFO] - Training Epoch: 1/2, step 6281/7134 completed (loss: 0.04664716497063637, acc: 0.9926470518112183)
[2025-02-13 20:10:29,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:30,144][root][INFO] - Training Epoch: 1/2, step 6282/7134 completed (loss: 0.13461613655090332, acc: 0.9652777910232544)
[2025-02-13 20:10:30,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:30,524][root][INFO] - Training Epoch: 1/2, step 6283/7134 completed (loss: 0.10268806666135788, acc: 0.9739130139350891)
[2025-02-13 20:10:30,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:30,857][root][INFO] - Training Epoch: 1/2, step 6284/7134 completed (loss: 0.23802612721920013, acc: 0.9454545378684998)
[2025-02-13 20:10:30,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:31,215][root][INFO] - Training Epoch: 1/2, step 6285/7134 completed (loss: 0.29212886095046997, acc: 0.9052631855010986)
[2025-02-13 20:10:31,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:31,577][root][INFO] - Training Epoch: 1/2, step 6286/7134 completed (loss: 0.09021176397800446, acc: 0.9722222089767456)
[2025-02-13 20:10:31,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:31,936][root][INFO] - Training Epoch: 1/2, step 6287/7134 completed (loss: 0.1508013904094696, acc: 0.9545454382896423)
[2025-02-13 20:10:32,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:32,295][root][INFO] - Training Epoch: 1/2, step 6288/7134 completed (loss: 0.03880701959133148, acc: 0.9929078221321106)
[2025-02-13 20:10:32,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:32,662][root][INFO] - Training Epoch: 1/2, step 6289/7134 completed (loss: 0.10812993347644806, acc: 0.9621211886405945)
[2025-02-13 20:10:32,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:33,030][root][INFO] - Training Epoch: 1/2, step 6290/7134 completed (loss: 0.12650537490844727, acc: 0.9701492786407471)
[2025-02-13 20:10:33,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:33,386][root][INFO] - Training Epoch: 1/2, step 6291/7134 completed (loss: 0.04044385626912117, acc: 0.9848484992980957)
[2025-02-13 20:10:33,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:33,732][root][INFO] - Training Epoch: 1/2, step 6292/7134 completed (loss: 0.17902064323425293, acc: 0.9375)
[2025-02-13 20:10:33,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:34,106][root][INFO] - Training Epoch: 1/2, step 6293/7134 completed (loss: 0.22311775386333466, acc: 0.939393937587738)
[2025-02-13 20:10:34,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:34,482][root][INFO] - Training Epoch: 1/2, step 6294/7134 completed (loss: 0.2626853287220001, acc: 0.9407894611358643)
[2025-02-13 20:10:34,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:34,846][root][INFO] - Training Epoch: 1/2, step 6295/7134 completed (loss: 0.06723085790872574, acc: 0.9848484992980957)
[2025-02-13 20:10:34,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:35,201][root][INFO] - Training Epoch: 1/2, step 6296/7134 completed (loss: 0.24187837541103363, acc: 0.9424460530281067)
[2025-02-13 20:10:35,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:35,561][root][INFO] - Training Epoch: 1/2, step 6297/7134 completed (loss: 0.13488854467868805, acc: 0.9774436354637146)
[2025-02-13 20:10:35,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:35,921][root][INFO] - Training Epoch: 1/2, step 6298/7134 completed (loss: 0.22469796240329742, acc: 0.9375)
[2025-02-13 20:10:36,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:36,299][root][INFO] - Training Epoch: 1/2, step 6299/7134 completed (loss: 0.15234623849391937, acc: 0.9736841917037964)
[2025-02-13 20:10:36,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:36,664][root][INFO] - Training Epoch: 1/2, step 6300/7134 completed (loss: 0.178641676902771, acc: 0.9605262875556946)
[2025-02-13 20:10:36,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:37,023][root][INFO] - Training Epoch: 1/2, step 6301/7134 completed (loss: 0.03431433066725731, acc: 0.9953703880310059)
[2025-02-13 20:10:37,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:37,376][root][INFO] - Training Epoch: 1/2, step 6302/7134 completed (loss: 0.0750010684132576, acc: 0.9720670580863953)
[2025-02-13 20:10:37,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:37,751][root][INFO] - Training Epoch: 1/2, step 6303/7134 completed (loss: 0.04182806238532066, acc: 0.9820627570152283)
[2025-02-13 20:10:37,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:38,104][root][INFO] - Training Epoch: 1/2, step 6304/7134 completed (loss: 0.07852960377931595, acc: 0.9829545617103577)
[2025-02-13 20:10:38,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:38,458][root][INFO] - Training Epoch: 1/2, step 6305/7134 completed (loss: 0.14483045041561127, acc: 0.9589040875434875)
[2025-02-13 20:10:38,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:38,815][root][INFO] - Training Epoch: 1/2, step 6306/7134 completed (loss: 0.21809335052967072, acc: 0.9620853066444397)
[2025-02-13 20:10:38,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:39,191][root][INFO] - Training Epoch: 1/2, step 6307/7134 completed (loss: 0.28556686639785767, acc: 0.9618320465087891)
[2025-02-13 20:10:39,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:39,566][root][INFO] - Training Epoch: 1/2, step 6308/7134 completed (loss: 0.1471736580133438, acc: 0.9637305736541748)
[2025-02-13 20:10:39,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:39,920][root][INFO] - Training Epoch: 1/2, step 6309/7134 completed (loss: 0.14376267790794373, acc: 0.9585798978805542)
[2025-02-13 20:10:40,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:40,328][root][INFO] - Training Epoch: 1/2, step 6310/7134 completed (loss: 0.24890506267547607, acc: 0.9385964870452881)
[2025-02-13 20:10:40,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:40,702][root][INFO] - Training Epoch: 1/2, step 6311/7134 completed (loss: 0.1436062604188919, acc: 0.9479166865348816)
[2025-02-13 20:10:40,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:41,071][root][INFO] - Training Epoch: 1/2, step 6312/7134 completed (loss: 0.07285244017839432, acc: 0.993630588054657)
[2025-02-13 20:10:41,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:41,428][root][INFO] - Training Epoch: 1/2, step 6313/7134 completed (loss: 0.038494523614645004, acc: 0.9891892075538635)
[2025-02-13 20:10:41,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:41,773][root][INFO] - Training Epoch: 1/2, step 6314/7134 completed (loss: 0.08913757652044296, acc: 0.9677419066429138)
[2025-02-13 20:10:41,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:42,127][root][INFO] - Training Epoch: 1/2, step 6315/7134 completed (loss: 0.2118149697780609, acc: 0.9798657894134521)
[2025-02-13 20:10:42,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:42,488][root][INFO] - Training Epoch: 1/2, step 6316/7134 completed (loss: 0.052870046347379684, acc: 0.9862068891525269)
[2025-02-13 20:10:42,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:42,836][root][INFO] - Training Epoch: 1/2, step 6317/7134 completed (loss: 0.054319433867931366, acc: 0.9797297120094299)
[2025-02-13 20:10:42,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:43,191][root][INFO] - Training Epoch: 1/2, step 6318/7134 completed (loss: 0.05052504688501358, acc: 0.9736841917037964)
[2025-02-13 20:10:43,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:43,556][root][INFO] - Training Epoch: 1/2, step 6319/7134 completed (loss: 0.06788279861211777, acc: 0.9733333587646484)
[2025-02-13 20:10:43,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:43,978][root][INFO] - Training Epoch: 1/2, step 6320/7134 completed (loss: 0.08196204155683517, acc: 0.9797297120094299)
[2025-02-13 20:10:44,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:44,362][root][INFO] - Training Epoch: 1/2, step 6321/7134 completed (loss: 0.04018436744809151, acc: 0.9904761910438538)
[2025-02-13 20:10:44,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:44,747][root][INFO] - Training Epoch: 1/2, step 6322/7134 completed (loss: 0.13269928097724915, acc: 0.9554139971733093)
[2025-02-13 20:10:44,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:45,212][root][INFO] - Training Epoch: 1/2, step 6323/7134 completed (loss: 0.13412721455097198, acc: 0.949999988079071)
[2025-02-13 20:10:45,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:45,631][root][INFO] - Training Epoch: 1/2, step 6324/7134 completed (loss: 0.20887453854084015, acc: 0.9562841653823853)
[2025-02-13 20:10:45,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:46,022][root][INFO] - Training Epoch: 1/2, step 6325/7134 completed (loss: 0.15128998458385468, acc: 0.9596773982048035)
[2025-02-13 20:10:46,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:46,466][root][INFO] - Training Epoch: 1/2, step 6326/7134 completed (loss: 0.21387933194637299, acc: 0.9308176040649414)
[2025-02-13 20:10:46,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:46,831][root][INFO] - Training Epoch: 1/2, step 6327/7134 completed (loss: 0.025072619318962097, acc: 1.0)
[2025-02-13 20:10:46,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:47,201][root][INFO] - Training Epoch: 1/2, step 6328/7134 completed (loss: 0.24115034937858582, acc: 0.9254658222198486)
[2025-02-13 20:10:47,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:47,567][root][INFO] - Training Epoch: 1/2, step 6329/7134 completed (loss: 0.12066090106964111, acc: 0.9615384340286255)
[2025-02-13 20:10:47,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:47,934][root][INFO] - Training Epoch: 1/2, step 6330/7134 completed (loss: 0.07883434742689133, acc: 0.9811320900917053)
[2025-02-13 20:10:48,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:48,298][root][INFO] - Training Epoch: 1/2, step 6331/7134 completed (loss: 0.03894360363483429, acc: 0.9925373196601868)
[2025-02-13 20:10:48,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:48,682][root][INFO] - Training Epoch: 1/2, step 6332/7134 completed (loss: 0.21996404230594635, acc: 0.9503546357154846)
[2025-02-13 20:10:48,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:49,049][root][INFO] - Training Epoch: 1/2, step 6333/7134 completed (loss: 0.13563072681427002, acc: 0.9655172228813171)
[2025-02-13 20:10:49,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:49,393][root][INFO] - Training Epoch: 1/2, step 6334/7134 completed (loss: 0.06920206546783447, acc: 0.9779411554336548)
[2025-02-13 20:10:49,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:49,760][root][INFO] - Training Epoch: 1/2, step 6335/7134 completed (loss: 0.03498537093400955, acc: 0.993630588054657)
[2025-02-13 20:10:49,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:50,119][root][INFO] - Training Epoch: 1/2, step 6336/7134 completed (loss: 0.14944149553775787, acc: 0.9736841917037964)
[2025-02-13 20:10:50,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:50,490][root][INFO] - Training Epoch: 1/2, step 6337/7134 completed (loss: 0.0370379276573658, acc: 0.9930070042610168)
[2025-02-13 20:10:50,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:50,855][root][INFO] - Training Epoch: 1/2, step 6338/7134 completed (loss: 0.12964387238025665, acc: 0.9685534834861755)
[2025-02-13 20:10:50,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:51,227][root][INFO] - Training Epoch: 1/2, step 6339/7134 completed (loss: 0.040962591767311096, acc: 0.9869281053543091)
[2025-02-13 20:10:51,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:51,592][root][INFO] - Training Epoch: 1/2, step 6340/7134 completed (loss: 0.055828072130680084, acc: 0.9754601120948792)
[2025-02-13 20:10:51,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:51,946][root][INFO] - Training Epoch: 1/2, step 6341/7134 completed (loss: 0.10227545350790024, acc: 0.960629940032959)
[2025-02-13 20:10:52,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:52,324][root][INFO] - Training Epoch: 1/2, step 6342/7134 completed (loss: 0.12208051234483719, acc: 0.9710982441902161)
[2025-02-13 20:10:52,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:52,713][root][INFO] - Training Epoch: 1/2, step 6343/7134 completed (loss: 0.13244076073169708, acc: 0.9726027250289917)
[2025-02-13 20:10:52,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:53,097][root][INFO] - Training Epoch: 1/2, step 6344/7134 completed (loss: 0.16396230459213257, acc: 0.950276255607605)
[2025-02-13 20:10:53,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:53,473][root][INFO] - Training Epoch: 1/2, step 6345/7134 completed (loss: 0.10193481296300888, acc: 0.9836956262588501)
[2025-02-13 20:10:53,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:53,833][root][INFO] - Training Epoch: 1/2, step 6346/7134 completed (loss: 0.2468319982290268, acc: 0.9418604373931885)
[2025-02-13 20:10:53,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:54,180][root][INFO] - Training Epoch: 1/2, step 6347/7134 completed (loss: 0.19171154499053955, acc: 0.9663865566253662)
[2025-02-13 20:10:54,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:54,539][root][INFO] - Training Epoch: 1/2, step 6348/7134 completed (loss: 0.11558420211076736, acc: 0.9652777910232544)
[2025-02-13 20:10:54,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:54,893][root][INFO] - Training Epoch: 1/2, step 6349/7134 completed (loss: 0.1450827419757843, acc: 0.9621211886405945)
[2025-02-13 20:10:55,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:55,264][root][INFO] - Training Epoch: 1/2, step 6350/7134 completed (loss: 0.2340613454580307, acc: 0.9365079402923584)
[2025-02-13 20:10:55,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:55,646][root][INFO] - Training Epoch: 1/2, step 6351/7134 completed (loss: 0.26659563183784485, acc: 0.9341317415237427)
[2025-02-13 20:10:55,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:56,034][root][INFO] - Training Epoch: 1/2, step 6352/7134 completed (loss: 0.3067793548107147, acc: 0.9017341136932373)
[2025-02-13 20:10:56,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:56,400][root][INFO] - Training Epoch: 1/2, step 6353/7134 completed (loss: 0.15536943078041077, acc: 0.9386503100395203)
[2025-02-13 20:10:56,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:56,755][root][INFO] - Training Epoch: 1/2, step 6354/7134 completed (loss: 0.09035276621580124, acc: 0.977011501789093)
[2025-02-13 20:10:56,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:57,115][root][INFO] - Training Epoch: 1/2, step 6355/7134 completed (loss: 0.05749480053782463, acc: 0.9800000190734863)
[2025-02-13 20:10:57,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:57,481][root][INFO] - Training Epoch: 1/2, step 6356/7134 completed (loss: 0.09595917910337448, acc: 0.9745222926139832)
[2025-02-13 20:10:57,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:57,849][root][INFO] - Training Epoch: 1/2, step 6357/7134 completed (loss: 0.10124868899583817, acc: 0.9757575988769531)
[2025-02-13 20:10:58,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:58,229][root][INFO] - Training Epoch: 1/2, step 6358/7134 completed (loss: 0.1275709867477417, acc: 0.9649122953414917)
[2025-02-13 20:10:58,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:58,584][root][INFO] - Training Epoch: 1/2, step 6359/7134 completed (loss: 0.17603906989097595, acc: 0.9608938694000244)
[2025-02-13 20:10:58,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:58,954][root][INFO] - Training Epoch: 1/2, step 6360/7134 completed (loss: 0.13793468475341797, acc: 0.9727891087532043)
[2025-02-13 20:10:59,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:59,356][root][INFO] - Training Epoch: 1/2, step 6361/7134 completed (loss: 0.2135322540998459, acc: 0.935251772403717)
[2025-02-13 20:10:59,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:59,743][root][INFO] - Training Epoch: 1/2, step 6362/7134 completed (loss: 0.09344343841075897, acc: 0.9741935729980469)
[2025-02-13 20:10:59,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:00,144][root][INFO] - Training Epoch: 1/2, step 6363/7134 completed (loss: 0.18643492460250854, acc: 0.9704142212867737)
[2025-02-13 20:11:00,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:00,568][root][INFO] - Training Epoch: 1/2, step 6364/7134 completed (loss: 0.10297852754592896, acc: 0.9681528806686401)
[2025-02-13 20:11:00,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:00,941][root][INFO] - Training Epoch: 1/2, step 6365/7134 completed (loss: 0.10885989665985107, acc: 0.9745762944221497)
[2025-02-13 20:11:01,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:01,361][root][INFO] - Training Epoch: 1/2, step 6366/7134 completed (loss: 0.1737024486064911, acc: 0.9704142212867737)
[2025-02-13 20:11:01,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:01,762][root][INFO] - Training Epoch: 1/2, step 6367/7134 completed (loss: 0.09896767884492874, acc: 0.9767441749572754)
[2025-02-13 20:11:01,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:02,126][root][INFO] - Training Epoch: 1/2, step 6368/7134 completed (loss: 0.1426495462656021, acc: 0.9638554453849792)
[2025-02-13 20:11:02,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:02,498][root][INFO] - Training Epoch: 1/2, step 6369/7134 completed (loss: 0.18573683500289917, acc: 0.9464285969734192)
[2025-02-13 20:11:02,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:02,874][root][INFO] - Training Epoch: 1/2, step 6370/7134 completed (loss: 0.46649080514907837, acc: 0.9069767594337463)
[2025-02-13 20:11:03,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:03,233][root][INFO] - Training Epoch: 1/2, step 6371/7134 completed (loss: 0.23877353966236115, acc: 0.9281045794487)
[2025-02-13 20:11:03,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:03,605][root][INFO] - Training Epoch: 1/2, step 6372/7134 completed (loss: 0.30669254064559937, acc: 0.9224137663841248)
[2025-02-13 20:11:03,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:03,955][root][INFO] - Training Epoch: 1/2, step 6373/7134 completed (loss: 0.30371856689453125, acc: 0.9274193644523621)
[2025-02-13 20:11:04,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:04,353][root][INFO] - Training Epoch: 1/2, step 6374/7134 completed (loss: 0.10411915183067322, acc: 0.9831932783126831)
[2025-02-13 20:11:04,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:04,734][root][INFO] - Training Epoch: 1/2, step 6375/7134 completed (loss: 0.19689764082431793, acc: 0.9447513818740845)
[2025-02-13 20:11:04,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:05,093][root][INFO] - Training Epoch: 1/2, step 6376/7134 completed (loss: 0.12160512059926987, acc: 0.9512194991111755)
[2025-02-13 20:11:05,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:05,423][root][INFO] - Training Epoch: 1/2, step 6377/7134 completed (loss: 0.1556200236082077, acc: 0.9440000057220459)
[2025-02-13 20:11:05,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:05,815][root][INFO] - Training Epoch: 1/2, step 6378/7134 completed (loss: 0.31019097566604614, acc: 0.8930232524871826)
[2025-02-13 20:11:05,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:06,180][root][INFO] - Training Epoch: 1/2, step 6379/7134 completed (loss: 0.2541496455669403, acc: 0.9210526347160339)
[2025-02-13 20:11:06,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:06,568][root][INFO] - Training Epoch: 1/2, step 6380/7134 completed (loss: 0.23282046616077423, acc: 0.956250011920929)
[2025-02-13 20:11:06,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:06,926][root][INFO] - Training Epoch: 1/2, step 6381/7134 completed (loss: 0.050477370619773865, acc: 0.9864864945411682)
[2025-02-13 20:11:07,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:07,299][root][INFO] - Training Epoch: 1/2, step 6382/7134 completed (loss: 0.07598178833723068, acc: 0.9835164546966553)
[2025-02-13 20:11:07,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:07,659][root][INFO] - Training Epoch: 1/2, step 6383/7134 completed (loss: 0.2825981378555298, acc: 0.9479768872261047)
[2025-02-13 20:11:07,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:08,037][root][INFO] - Training Epoch: 1/2, step 6384/7134 completed (loss: 0.11746202409267426, acc: 0.9556962251663208)
[2025-02-13 20:11:08,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:08,406][root][INFO] - Training Epoch: 1/2, step 6385/7134 completed (loss: 0.2526046335697174, acc: 0.9414634108543396)
[2025-02-13 20:11:08,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:08,770][root][INFO] - Training Epoch: 1/2, step 6386/7134 completed (loss: 0.1423846185207367, acc: 0.9462365508079529)
[2025-02-13 20:11:08,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:09,129][root][INFO] - Training Epoch: 1/2, step 6387/7134 completed (loss: 0.23891684412956238, acc: 0.9696969985961914)
[2025-02-13 20:11:09,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:09,548][root][INFO] - Training Epoch: 1/2, step 6388/7134 completed (loss: 0.17415830492973328, acc: 0.9556962251663208)
[2025-02-13 20:11:09,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:09,923][root][INFO] - Training Epoch: 1/2, step 6389/7134 completed (loss: 0.15816131234169006, acc: 0.9519230723381042)
[2025-02-13 20:11:10,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:10,314][root][INFO] - Training Epoch: 1/2, step 6390/7134 completed (loss: 0.1677127629518509, acc: 0.9434782862663269)
[2025-02-13 20:11:10,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:10,652][root][INFO] - Training Epoch: 1/2, step 6391/7134 completed (loss: 0.20557913184165955, acc: 0.9593023061752319)
[2025-02-13 20:11:10,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:11,004][root][INFO] - Training Epoch: 1/2, step 6392/7134 completed (loss: 0.2674761414527893, acc: 0.9347826242446899)
[2025-02-13 20:11:11,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:11,369][root][INFO] - Training Epoch: 1/2, step 6393/7134 completed (loss: 0.09957187622785568, acc: 0.9861111044883728)
[2025-02-13 20:11:11,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:11,734][root][INFO] - Training Epoch: 1/2, step 6394/7134 completed (loss: 0.26581358909606934, acc: 0.9611111283302307)
[2025-02-13 20:11:11,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:12,109][root][INFO] - Training Epoch: 1/2, step 6395/7134 completed (loss: 0.0987231656908989, acc: 0.9801980257034302)
[2025-02-13 20:11:12,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:12,470][root][INFO] - Training Epoch: 1/2, step 6396/7134 completed (loss: 0.16017791628837585, acc: 0.9617224931716919)
[2025-02-13 20:11:12,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:12,877][root][INFO] - Training Epoch: 1/2, step 6397/7134 completed (loss: 0.07380978763103485, acc: 0.9751243591308594)
[2025-02-13 20:11:13,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:13,250][root][INFO] - Training Epoch: 1/2, step 6398/7134 completed (loss: 0.12392684817314148, acc: 0.9655172228813171)
[2025-02-13 20:11:13,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:13,615][root][INFO] - Training Epoch: 1/2, step 6399/7134 completed (loss: 0.16464969515800476, acc: 0.9719101190567017)
[2025-02-13 20:11:13,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:13,969][root][INFO] - Training Epoch: 1/2, step 6400/7134 completed (loss: 0.059399232268333435, acc: 0.9819276928901672)
[2025-02-13 20:11:14,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:14,344][root][INFO] - Training Epoch: 1/2, step 6401/7134 completed (loss: 0.08940310776233673, acc: 0.976190447807312)
[2025-02-13 20:11:14,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:14,730][root][INFO] - Training Epoch: 1/2, step 6402/7134 completed (loss: 0.18046189844608307, acc: 0.9696969985961914)
[2025-02-13 20:11:14,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:15,072][root][INFO] - Training Epoch: 1/2, step 6403/7134 completed (loss: 0.04535533860325813, acc: 0.9896373152732849)
[2025-02-13 20:11:15,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:15,421][root][INFO] - Training Epoch: 1/2, step 6404/7134 completed (loss: 0.06312768906354904, acc: 0.9822485446929932)
[2025-02-13 20:11:15,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:15,842][root][INFO] - Training Epoch: 1/2, step 6405/7134 completed (loss: 0.08095476031303406, acc: 0.9855769276618958)
[2025-02-13 20:11:15,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:16,213][root][INFO] - Training Epoch: 1/2, step 6406/7134 completed (loss: 0.21997974812984467, acc: 0.9449999928474426)
[2025-02-13 20:11:16,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:16,591][root][INFO] - Training Epoch: 1/2, step 6407/7134 completed (loss: 0.07827789336442947, acc: 0.9722222089767456)
[2025-02-13 20:11:16,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:16,946][root][INFO] - Training Epoch: 1/2, step 6408/7134 completed (loss: 0.2931567430496216, acc: 0.9166666865348816)
[2025-02-13 20:11:17,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:17,292][root][INFO] - Training Epoch: 1/2, step 6409/7134 completed (loss: 0.1729796826839447, acc: 0.9756097793579102)
[2025-02-13 20:11:17,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:17,655][root][INFO] - Training Epoch: 1/2, step 6410/7134 completed (loss: 0.25702518224716187, acc: 0.9166666865348816)
[2025-02-13 20:11:17,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:18,028][root][INFO] - Training Epoch: 1/2, step 6411/7134 completed (loss: 0.23024390637874603, acc: 0.9457364082336426)
[2025-02-13 20:11:18,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:18,449][root][INFO] - Training Epoch: 1/2, step 6412/7134 completed (loss: 0.05612222105264664, acc: 0.9806451797485352)
[2025-02-13 20:11:18,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:18,833][root][INFO] - Training Epoch: 1/2, step 6413/7134 completed (loss: 0.23630401492118835, acc: 0.9597315192222595)
[2025-02-13 20:11:18,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:19,216][root][INFO] - Training Epoch: 1/2, step 6414/7134 completed (loss: 0.08642511814832687, acc: 0.9736841917037964)
[2025-02-13 20:11:19,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:19,594][root][INFO] - Training Epoch: 1/2, step 6415/7134 completed (loss: 0.10301756858825684, acc: 0.9715909361839294)
[2025-02-13 20:11:19,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:19,999][root][INFO] - Training Epoch: 1/2, step 6416/7134 completed (loss: 0.16991040110588074, acc: 0.9710144996643066)
[2025-02-13 20:11:20,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:20,365][root][INFO] - Training Epoch: 1/2, step 6417/7134 completed (loss: 0.32558760046958923, acc: 0.9224806427955627)
[2025-02-13 20:11:20,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:20,744][root][INFO] - Training Epoch: 1/2, step 6418/7134 completed (loss: 0.6077844500541687, acc: 0.8456375598907471)
[2025-02-13 20:11:20,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:21,124][root][INFO] - Training Epoch: 1/2, step 6419/7134 completed (loss: 0.3412172794342041, acc: 0.9285714030265808)
[2025-02-13 20:11:21,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:21,480][root][INFO] - Training Epoch: 1/2, step 6420/7134 completed (loss: 0.28298458456993103, acc: 0.9240506291389465)
[2025-02-13 20:11:21,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:21,857][root][INFO] - Training Epoch: 1/2, step 6421/7134 completed (loss: 0.2250019907951355, acc: 0.9457831382751465)
[2025-02-13 20:11:21,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:22,226][root][INFO] - Training Epoch: 1/2, step 6422/7134 completed (loss: 0.19181402027606964, acc: 0.9608938694000244)
[2025-02-13 20:11:22,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:22,599][root][INFO] - Training Epoch: 1/2, step 6423/7134 completed (loss: 0.2233327180147171, acc: 0.942307710647583)
[2025-02-13 20:11:22,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:22,978][root][INFO] - Training Epoch: 1/2, step 6424/7134 completed (loss: 0.13034480810165405, acc: 0.9734042286872864)
[2025-02-13 20:11:23,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:23,348][root][INFO] - Training Epoch: 1/2, step 6425/7134 completed (loss: 0.12300684303045273, acc: 0.9553072452545166)
[2025-02-13 20:11:23,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:23,731][root][INFO] - Training Epoch: 1/2, step 6426/7134 completed (loss: 0.2256108969449997, acc: 0.9553072452545166)
[2025-02-13 20:11:23,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:24,095][root][INFO] - Training Epoch: 1/2, step 6427/7134 completed (loss: 0.16796272993087769, acc: 0.9426751732826233)
[2025-02-13 20:11:24,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:24,484][root][INFO] - Training Epoch: 1/2, step 6428/7134 completed (loss: 0.1057085394859314, acc: 0.9808917045593262)
[2025-02-13 20:11:24,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:24,848][root][INFO] - Training Epoch: 1/2, step 6429/7134 completed (loss: 0.11275167018175125, acc: 0.9677419066429138)
[2025-02-13 20:11:24,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:25,267][root][INFO] - Training Epoch: 1/2, step 6430/7134 completed (loss: 0.3265465199947357, acc: 0.9312499761581421)
[2025-02-13 20:11:25,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:25,698][root][INFO] - Training Epoch: 1/2, step 6431/7134 completed (loss: 0.37754544615745544, acc: 0.8918918967247009)
[2025-02-13 20:11:25,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:26,098][root][INFO] - Training Epoch: 1/2, step 6432/7134 completed (loss: 0.8529845476150513, acc: 0.8322580456733704)
[2025-02-13 20:11:26,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:26,532][root][INFO] - Training Epoch: 1/2, step 6433/7134 completed (loss: 0.35520699620246887, acc: 0.9261363744735718)
[2025-02-13 20:11:26,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:26,945][root][INFO] - Training Epoch: 1/2, step 6434/7134 completed (loss: 0.08413416892290115, acc: 0.9653179049491882)
[2025-02-13 20:11:27,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:27,340][root][INFO] - Training Epoch: 1/2, step 6435/7134 completed (loss: 0.12525776028633118, acc: 0.9637681245803833)
[2025-02-13 20:11:27,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:27,715][root][INFO] - Training Epoch: 1/2, step 6436/7134 completed (loss: 0.18638651072978973, acc: 0.9530201554298401)
[2025-02-13 20:11:27,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:28,095][root][INFO] - Training Epoch: 1/2, step 6437/7134 completed (loss: 0.35362935066223145, acc: 0.9407894611358643)
[2025-02-13 20:11:28,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:28,470][root][INFO] - Training Epoch: 1/2, step 6438/7134 completed (loss: 0.3059524893760681, acc: 0.9368932247161865)
[2025-02-13 20:11:28,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:28,827][root][INFO] - Training Epoch: 1/2, step 6439/7134 completed (loss: 0.21542872488498688, acc: 0.9503105878829956)
[2025-02-13 20:11:28,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:29,216][root][INFO] - Training Epoch: 1/2, step 6440/7134 completed (loss: 0.21278709173202515, acc: 0.9519230723381042)
[2025-02-13 20:11:29,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:29,570][root][INFO] - Training Epoch: 1/2, step 6441/7134 completed (loss: 0.11233725398778915, acc: 0.9811320900917053)
[2025-02-13 20:11:29,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:29,950][root][INFO] - Training Epoch: 1/2, step 6442/7134 completed (loss: 0.1627333015203476, acc: 0.9660193920135498)
[2025-02-13 20:11:30,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:30,354][root][INFO] - Training Epoch: 1/2, step 6443/7134 completed (loss: 0.07581332325935364, acc: 0.9852941036224365)
[2025-02-13 20:11:30,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:30,741][root][INFO] - Training Epoch: 1/2, step 6444/7134 completed (loss: 0.17969724535942078, acc: 0.9444444179534912)
[2025-02-13 20:11:30,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:31,122][root][INFO] - Training Epoch: 1/2, step 6445/7134 completed (loss: 0.3001975119113922, acc: 0.9458128213882446)
[2025-02-13 20:11:31,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:31,490][root][INFO] - Training Epoch: 1/2, step 6446/7134 completed (loss: 0.0822221040725708, acc: 0.9803921580314636)
[2025-02-13 20:11:31,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:31,866][root][INFO] - Training Epoch: 1/2, step 6447/7134 completed (loss: 0.17205096781253815, acc: 0.9583333134651184)
[2025-02-13 20:11:32,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:32,259][root][INFO] - Training Epoch: 1/2, step 6448/7134 completed (loss: 0.4535680413246155, acc: 0.9096774458885193)
[2025-02-13 20:11:32,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:32,637][root][INFO] - Training Epoch: 1/2, step 6449/7134 completed (loss: 0.17725147306919098, acc: 0.9637305736541748)
[2025-02-13 20:11:32,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:33,007][root][INFO] - Training Epoch: 1/2, step 6450/7134 completed (loss: 0.15807418525218964, acc: 0.9653179049491882)
[2025-02-13 20:11:33,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:33,411][root][INFO] - Training Epoch: 1/2, step 6451/7134 completed (loss: 0.20815427601337433, acc: 0.9579831957817078)
[2025-02-13 20:11:33,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:33,806][root][INFO] - Training Epoch: 1/2, step 6452/7134 completed (loss: 0.10173813253641129, acc: 0.9795918464660645)
[2025-02-13 20:11:33,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:34,176][root][INFO] - Training Epoch: 1/2, step 6453/7134 completed (loss: 0.1477060616016388, acc: 0.9754601120948792)
[2025-02-13 20:11:34,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:34,575][root][INFO] - Training Epoch: 1/2, step 6454/7134 completed (loss: 0.2669714391231537, acc: 0.9398906826972961)
[2025-02-13 20:11:34,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:34,942][root][INFO] - Training Epoch: 1/2, step 6455/7134 completed (loss: 0.32805630564689636, acc: 0.9266666769981384)
[2025-02-13 20:11:35,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:35,304][root][INFO] - Training Epoch: 1/2, step 6456/7134 completed (loss: 0.5214253664016724, acc: 0.8922155499458313)
[2025-02-13 20:11:35,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:35,678][root][INFO] - Training Epoch: 1/2, step 6457/7134 completed (loss: 0.20286411046981812, acc: 0.9541284441947937)
[2025-02-13 20:11:35,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:36,023][root][INFO] - Training Epoch: 1/2, step 6458/7134 completed (loss: 0.1834743618965149, acc: 0.9527027010917664)
[2025-02-13 20:11:36,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:36,371][root][INFO] - Training Epoch: 1/2, step 6459/7134 completed (loss: 0.023286966606974602, acc: 1.0)
[2025-02-13 20:11:36,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:36,721][root][INFO] - Training Epoch: 1/2, step 6460/7134 completed (loss: 0.07328029721975327, acc: 0.9795918464660645)
[2025-02-13 20:11:36,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:37,091][root][INFO] - Training Epoch: 1/2, step 6461/7134 completed (loss: 0.06411528587341309, acc: 0.9710982441902161)
[2025-02-13 20:11:37,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:37,481][root][INFO] - Training Epoch: 1/2, step 6462/7134 completed (loss: 0.11337078362703323, acc: 0.978723406791687)
[2025-02-13 20:11:37,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:37,866][root][INFO] - Training Epoch: 1/2, step 6463/7134 completed (loss: 0.15198180079460144, acc: 0.9611650705337524)
[2025-02-13 20:11:38,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:38,265][root][INFO] - Training Epoch: 1/2, step 6464/7134 completed (loss: 0.1362592577934265, acc: 0.9664804339408875)
[2025-02-13 20:11:38,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:38,623][root][INFO] - Training Epoch: 1/2, step 6465/7134 completed (loss: 0.03980163484811783, acc: 0.9890710115432739)
[2025-02-13 20:11:38,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:38,985][root][INFO] - Training Epoch: 1/2, step 6466/7134 completed (loss: 0.16492284834384918, acc: 0.9629629850387573)
[2025-02-13 20:11:39,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:39,414][root][INFO] - Training Epoch: 1/2, step 6467/7134 completed (loss: 0.24512693285942078, acc: 0.9277108311653137)
[2025-02-13 20:11:39,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:39,805][root][INFO] - Training Epoch: 1/2, step 6468/7134 completed (loss: 0.11963273584842682, acc: 0.9632353186607361)
[2025-02-13 20:11:39,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:40,226][root][INFO] - Training Epoch: 1/2, step 6469/7134 completed (loss: 0.22768127918243408, acc: 0.9579439163208008)
[2025-02-13 20:11:40,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:40,663][root][INFO] - Training Epoch: 1/2, step 6470/7134 completed (loss: 0.15264272689819336, acc: 0.9509803652763367)
[2025-02-13 20:11:40,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:41,084][root][INFO] - Training Epoch: 1/2, step 6471/7134 completed (loss: 0.07841651886701584, acc: 0.9784172773361206)
[2025-02-13 20:11:41,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:41,466][root][INFO] - Training Epoch: 1/2, step 6472/7134 completed (loss: 0.1568184345960617, acc: 0.9599999785423279)
[2025-02-13 20:11:41,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:41,856][root][INFO] - Training Epoch: 1/2, step 6473/7134 completed (loss: 0.066752128303051, acc: 0.9777777791023254)
[2025-02-13 20:11:41,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:42,222][root][INFO] - Training Epoch: 1/2, step 6474/7134 completed (loss: 0.079250268638134, acc: 0.9892473220825195)
[2025-02-13 20:11:42,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:42,586][root][INFO] - Training Epoch: 1/2, step 6475/7134 completed (loss: 0.06958334892988205, acc: 0.9785714149475098)
[2025-02-13 20:11:42,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:42,939][root][INFO] - Training Epoch: 1/2, step 6476/7134 completed (loss: 0.1150045245885849, acc: 0.9666666388511658)
[2025-02-13 20:11:43,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:43,301][root][INFO] - Training Epoch: 1/2, step 6477/7134 completed (loss: 0.0794224664568901, acc: 0.9826086759567261)
[2025-02-13 20:11:43,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:43,679][root][INFO] - Training Epoch: 1/2, step 6478/7134 completed (loss: 0.1194915920495987, acc: 0.9710144996643066)
[2025-02-13 20:11:43,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:44,077][root][INFO] - Training Epoch: 1/2, step 6479/7134 completed (loss: 0.09185582399368286, acc: 0.9798657894134521)
[2025-02-13 20:11:44,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:44,496][root][INFO] - Training Epoch: 1/2, step 6480/7134 completed (loss: 0.11068523675203323, acc: 0.9857142567634583)
[2025-02-13 20:11:44,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:44,897][root][INFO] - Training Epoch: 1/2, step 6481/7134 completed (loss: 0.040918394923210144, acc: 0.9857142567634583)
[2025-02-13 20:11:45,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:45,316][root][INFO] - Training Epoch: 1/2, step 6482/7134 completed (loss: 0.030303971841931343, acc: 0.9900000095367432)
[2025-02-13 20:11:45,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:45,707][root][INFO] - Training Epoch: 1/2, step 6483/7134 completed (loss: 0.20678098499774933, acc: 0.9652777910232544)
[2025-02-13 20:11:45,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:46,121][root][INFO] - Training Epoch: 1/2, step 6484/7134 completed (loss: 0.11187601089477539, acc: 0.9707602262496948)
[2025-02-13 20:11:46,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:46,531][root][INFO] - Training Epoch: 1/2, step 6485/7134 completed (loss: 0.05846310034394264, acc: 0.9831932783126831)
[2025-02-13 20:11:46,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:46,897][root][INFO] - Training Epoch: 1/2, step 6486/7134 completed (loss: 0.3771530091762543, acc: 0.9202898740768433)
[2025-02-13 20:11:47,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:47,287][root][INFO] - Training Epoch: 1/2, step 6487/7134 completed (loss: 0.7612811923027039, acc: 0.8529411554336548)
[2025-02-13 20:11:47,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:47,679][root][INFO] - Training Epoch: 1/2, step 6488/7134 completed (loss: 0.2754533886909485, acc: 0.9090909361839294)
[2025-02-13 20:11:47,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:48,107][root][INFO] - Training Epoch: 1/2, step 6489/7134 completed (loss: 0.3219994902610779, acc: 0.9316239356994629)
[2025-02-13 20:11:48,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:48,489][root][INFO] - Training Epoch: 1/2, step 6490/7134 completed (loss: 0.5714382529258728, acc: 0.8656716346740723)
[2025-02-13 20:11:48,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:48,906][root][INFO] - Training Epoch: 1/2, step 6491/7134 completed (loss: 0.25326502323150635, acc: 0.948051929473877)
[2025-02-13 20:11:49,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:49,277][root][INFO] - Training Epoch: 1/2, step 6492/7134 completed (loss: 0.17630743980407715, acc: 0.9677419066429138)
[2025-02-13 20:11:49,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:49,657][root][INFO] - Training Epoch: 1/2, step 6493/7134 completed (loss: 0.2217264473438263, acc: 0.9523809552192688)
[2025-02-13 20:11:49,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:49,998][root][INFO] - Training Epoch: 1/2, step 6494/7134 completed (loss: 0.34132370352745056, acc: 0.9380530714988708)
[2025-02-13 20:11:50,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:50,357][root][INFO] - Training Epoch: 1/2, step 6495/7134 completed (loss: 0.3364231586456299, acc: 0.9074074029922485)
[2025-02-13 20:11:50,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:50,771][root][INFO] - Training Epoch: 1/2, step 6496/7134 completed (loss: 0.2688113749027252, acc: 0.9248554706573486)
[2025-02-13 20:11:50,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:51,178][root][INFO] - Training Epoch: 1/2, step 6497/7134 completed (loss: 0.31505441665649414, acc: 0.9305555820465088)
[2025-02-13 20:11:51,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:51,544][root][INFO] - Training Epoch: 1/2, step 6498/7134 completed (loss: 0.24281425774097443, acc: 0.9352940917015076)
[2025-02-13 20:11:51,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:51,970][root][INFO] - Training Epoch: 1/2, step 6499/7134 completed (loss: 0.3348434865474701, acc: 0.9095744490623474)
[2025-02-13 20:11:52,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:52,376][root][INFO] - Training Epoch: 1/2, step 6500/7134 completed (loss: 0.48296549916267395, acc: 0.9137930870056152)
[2025-02-13 20:11:52,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:52,748][root][INFO] - Training Epoch: 1/2, step 6501/7134 completed (loss: 0.6410004496574402, acc: 0.8556700944900513)
[2025-02-13 20:11:52,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:53,149][root][INFO] - Training Epoch: 1/2, step 6502/7134 completed (loss: 0.17508822679519653, acc: 0.939393937587738)
[2025-02-13 20:11:53,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:53,520][root][INFO] - Training Epoch: 1/2, step 6503/7134 completed (loss: 0.2874385416507721, acc: 0.9130434989929199)
[2025-02-13 20:11:53,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:53,909][root][INFO] - Training Epoch: 1/2, step 6504/7134 completed (loss: 0.2172514945268631, acc: 0.9520547986030579)
[2025-02-13 20:11:54,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:54,281][root][INFO] - Training Epoch: 1/2, step 6505/7134 completed (loss: 0.2963923513889313, acc: 0.9266666769981384)
[2025-02-13 20:11:54,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:54,676][root][INFO] - Training Epoch: 1/2, step 6506/7134 completed (loss: 0.2818722426891327, acc: 0.938144326210022)
[2025-02-13 20:11:54,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:55,050][root][INFO] - Training Epoch: 1/2, step 6507/7134 completed (loss: 0.3769679069519043, acc: 0.907975435256958)
[2025-02-13 20:11:55,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:55,426][root][INFO] - Training Epoch: 1/2, step 6508/7134 completed (loss: 0.1316632479429245, acc: 0.9611111283302307)
[2025-02-13 20:11:55,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:55,840][root][INFO] - Training Epoch: 1/2, step 6509/7134 completed (loss: 0.14829270541667938, acc: 0.9587628841400146)
[2025-02-13 20:11:55,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:56,236][root][INFO] - Training Epoch: 1/2, step 6510/7134 completed (loss: 0.1942967176437378, acc: 0.9563106894493103)
[2025-02-13 20:11:56,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:56,640][root][INFO] - Training Epoch: 1/2, step 6511/7134 completed (loss: 0.18765227496623993, acc: 0.9477611780166626)
[2025-02-13 20:11:56,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:57,072][root][INFO] - Training Epoch: 1/2, step 6512/7134 completed (loss: 0.20734931528568268, acc: 0.9636363387107849)
[2025-02-13 20:11:57,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:57,442][root][INFO] - Training Epoch: 1/2, step 6513/7134 completed (loss: 0.13040335476398468, acc: 0.9777777791023254)
[2025-02-13 20:11:57,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:57,832][root][INFO] - Training Epoch: 1/2, step 6514/7134 completed (loss: 0.3262965977191925, acc: 0.9506173133850098)
[2025-02-13 20:11:57,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:58,260][root][INFO] - Training Epoch: 1/2, step 6515/7134 completed (loss: 0.12897755205631256, acc: 0.9670329689979553)
[2025-02-13 20:11:58,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:58,661][root][INFO] - Training Epoch: 1/2, step 6516/7134 completed (loss: 0.26868489384651184, acc: 0.9411764740943909)
[2025-02-13 20:11:58,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:59,056][root][INFO] - Training Epoch: 1/2, step 6517/7134 completed (loss: 0.08391787111759186, acc: 0.9937888383865356)
[2025-02-13 20:11:59,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:59,443][root][INFO] - Training Epoch: 1/2, step 6518/7134 completed (loss: 0.08536186814308167, acc: 0.976047933101654)
[2025-02-13 20:11:59,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:59,843][root][INFO] - Training Epoch: 1/2, step 6519/7134 completed (loss: 0.11724766343832016, acc: 0.9763779640197754)
[2025-02-13 20:11:59,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:00,234][root][INFO] - Training Epoch: 1/2, step 6520/7134 completed (loss: 0.1431063413619995, acc: 0.9675324559211731)
[2025-02-13 20:12:00,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:00,637][root][INFO] - Training Epoch: 1/2, step 6521/7134 completed (loss: 0.14153940975666046, acc: 0.9675675630569458)
[2025-02-13 20:12:00,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:01,009][root][INFO] - Training Epoch: 1/2, step 6522/7134 completed (loss: 0.21020101010799408, acc: 0.9411764740943909)
[2025-02-13 20:12:01,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:01,391][root][INFO] - Training Epoch: 1/2, step 6523/7134 completed (loss: 0.47525250911712646, acc: 0.9027777910232544)
[2025-02-13 20:12:01,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:01,802][root][INFO] - Training Epoch: 1/2, step 6524/7134 completed (loss: 0.21670226752758026, acc: 0.9613259434700012)
[2025-02-13 20:12:01,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:02,226][root][INFO] - Training Epoch: 1/2, step 6525/7134 completed (loss: 0.09817097336053848, acc: 0.9752066135406494)
[2025-02-13 20:12:02,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:02,646][root][INFO] - Training Epoch: 1/2, step 6526/7134 completed (loss: 0.21663081645965576, acc: 0.9593908786773682)
[2025-02-13 20:12:02,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:03,023][root][INFO] - Training Epoch: 1/2, step 6527/7134 completed (loss: 0.19015812873840332, acc: 0.9620253443717957)
[2025-02-13 20:12:03,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:03,413][root][INFO] - Training Epoch: 1/2, step 6528/7134 completed (loss: 0.2998424768447876, acc: 0.9275362491607666)
[2025-02-13 20:12:03,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:03,818][root][INFO] - Training Epoch: 1/2, step 6529/7134 completed (loss: 0.4616408050060272, acc: 0.9236640930175781)
[2025-02-13 20:12:03,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:04,216][root][INFO] - Training Epoch: 1/2, step 6530/7134 completed (loss: 0.1558360904455185, acc: 0.9668508172035217)
[2025-02-13 20:12:04,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:04,602][root][INFO] - Training Epoch: 1/2, step 6531/7134 completed (loss: 0.16093191504478455, acc: 0.957317054271698)
[2025-02-13 20:12:04,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:04,993][root][INFO] - Training Epoch: 1/2, step 6532/7134 completed (loss: 0.27802953124046326, acc: 0.9560439586639404)
[2025-02-13 20:12:05,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:05,378][root][INFO] - Training Epoch: 1/2, step 6533/7134 completed (loss: 0.04786432161927223, acc: 0.9805194735527039)
[2025-02-13 20:12:05,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:05,753][root][INFO] - Training Epoch: 1/2, step 6534/7134 completed (loss: 0.2836083769798279, acc: 0.9537572264671326)
[2025-02-13 20:12:05,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:06,114][root][INFO] - Training Epoch: 1/2, step 6535/7134 completed (loss: 0.1761619746685028, acc: 0.9433962106704712)
[2025-02-13 20:12:06,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:06,497][root][INFO] - Training Epoch: 1/2, step 6536/7134 completed (loss: 0.06396712362766266, acc: 0.9826086759567261)
[2025-02-13 20:12:06,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:06,862][root][INFO] - Training Epoch: 1/2, step 6537/7134 completed (loss: 0.2547438442707062, acc: 0.9107142686843872)
[2025-02-13 20:12:06,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:07,237][root][INFO] - Training Epoch: 1/2, step 6538/7134 completed (loss: 0.2892901599407196, acc: 0.9383561611175537)
[2025-02-13 20:12:07,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:07,624][root][INFO] - Training Epoch: 1/2, step 6539/7134 completed (loss: 0.04772767424583435, acc: 0.9740259647369385)
[2025-02-13 20:12:07,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:08,006][root][INFO] - Training Epoch: 1/2, step 6540/7134 completed (loss: 0.8837946057319641, acc: 0.8409090638160706)
[2025-02-13 20:12:08,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:08,397][root][INFO] - Training Epoch: 1/2, step 6541/7134 completed (loss: 0.13111120462417603, acc: 0.9558011293411255)
[2025-02-13 20:12:08,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:08,801][root][INFO] - Training Epoch: 1/2, step 6542/7134 completed (loss: 0.20740287005901337, acc: 0.9571428298950195)
[2025-02-13 20:12:08,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:09,249][root][INFO] - Training Epoch: 1/2, step 6543/7134 completed (loss: 0.10454785823822021, acc: 0.9736841917037964)
[2025-02-13 20:12:09,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:09,654][root][INFO] - Training Epoch: 1/2, step 6544/7134 completed (loss: 0.24827036261558533, acc: 0.9369369149208069)
[2025-02-13 20:12:09,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:10,043][root][INFO] - Training Epoch: 1/2, step 6545/7134 completed (loss: 0.06147807091474533, acc: 0.9917355179786682)
[2025-02-13 20:12:10,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:10,421][root][INFO] - Training Epoch: 1/2, step 6546/7134 completed (loss: 0.13909856975078583, acc: 0.97826087474823)
[2025-02-13 20:12:10,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:10,803][root][INFO] - Training Epoch: 1/2, step 6547/7134 completed (loss: 0.2883313298225403, acc: 0.9492753744125366)
[2025-02-13 20:12:10,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:11,169][root][INFO] - Training Epoch: 1/2, step 6548/7134 completed (loss: 0.10932092368602753, acc: 0.9714285731315613)
[2025-02-13 20:12:11,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:11,580][root][INFO] - Training Epoch: 1/2, step 6549/7134 completed (loss: 0.448246031999588, acc: 0.9236640930175781)
[2025-02-13 20:12:11,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:11,973][root][INFO] - Training Epoch: 1/2, step 6550/7134 completed (loss: 0.22593311965465546, acc: 0.9426751732826233)
[2025-02-13 20:12:12,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:12,358][root][INFO] - Training Epoch: 1/2, step 6551/7134 completed (loss: 0.16205795109272003, acc: 0.9644669890403748)
[2025-02-13 20:12:12,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:12,747][root][INFO] - Training Epoch: 1/2, step 6552/7134 completed (loss: 0.28191033005714417, acc: 0.933920681476593)
[2025-02-13 20:12:12,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:13,130][root][INFO] - Training Epoch: 1/2, step 6553/7134 completed (loss: 0.19465981423854828, acc: 0.9494949579238892)
[2025-02-13 20:12:13,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:13,501][root][INFO] - Training Epoch: 1/2, step 6554/7134 completed (loss: 0.16912424564361572, acc: 0.9672897458076477)
[2025-02-13 20:12:13,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:13,889][root][INFO] - Training Epoch: 1/2, step 6555/7134 completed (loss: 0.2702941298484802, acc: 0.9353448152542114)
[2025-02-13 20:12:14,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:14,240][root][INFO] - Training Epoch: 1/2, step 6556/7134 completed (loss: 0.2631676197052002, acc: 0.9638009071350098)
[2025-02-13 20:12:14,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:14,618][root][INFO] - Training Epoch: 1/2, step 6557/7134 completed (loss: 0.2045729011297226, acc: 0.9504504799842834)
[2025-02-13 20:12:14,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:15,052][root][INFO] - Training Epoch: 1/2, step 6558/7134 completed (loss: 0.19798226654529572, acc: 0.9672897458076477)
[2025-02-13 20:12:15,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:15,437][root][INFO] - Training Epoch: 1/2, step 6559/7134 completed (loss: 0.2381199300289154, acc: 0.9638554453849792)
[2025-02-13 20:12:15,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:15,872][root][INFO] - Training Epoch: 1/2, step 6560/7134 completed (loss: 0.10930280387401581, acc: 0.9790576100349426)
[2025-02-13 20:12:16,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:16,289][root][INFO] - Training Epoch: 1/2, step 6561/7134 completed (loss: 0.12200578302145004, acc: 0.9581151604652405)
[2025-02-13 20:12:16,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:16,647][root][INFO] - Training Epoch: 1/2, step 6562/7134 completed (loss: 0.1863543838262558, acc: 0.9371069073677063)
[2025-02-13 20:12:16,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:17,027][root][INFO] - Training Epoch: 1/2, step 6563/7134 completed (loss: 0.17012879252433777, acc: 0.9444444179534912)
[2025-02-13 20:12:17,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:17,436][root][INFO] - Training Epoch: 1/2, step 6564/7134 completed (loss: 0.15224725008010864, acc: 0.9698795080184937)
[2025-02-13 20:12:17,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:17,819][root][INFO] - Training Epoch: 1/2, step 6565/7134 completed (loss: 0.3067956566810608, acc: 0.9279279112815857)
[2025-02-13 20:12:17,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:18,214][root][INFO] - Training Epoch: 1/2, step 6566/7134 completed (loss: 0.14537861943244934, acc: 0.9596773982048035)
[2025-02-13 20:12:18,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:18,628][root][INFO] - Training Epoch: 1/2, step 6567/7134 completed (loss: 0.17256461083889008, acc: 0.9466666579246521)
[2025-02-13 20:12:18,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:19,030][root][INFO] - Training Epoch: 1/2, step 6568/7134 completed (loss: 0.14021804928779602, acc: 0.9725274443626404)
[2025-02-13 20:12:19,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:19,390][root][INFO] - Training Epoch: 1/2, step 6569/7134 completed (loss: 0.11609704047441483, acc: 0.9543147087097168)
[2025-02-13 20:12:19,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:19,772][root][INFO] - Training Epoch: 1/2, step 6570/7134 completed (loss: 0.15924718976020813, acc: 0.9507389068603516)
[2025-02-13 20:12:19,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:20,150][root][INFO] - Training Epoch: 1/2, step 6571/7134 completed (loss: 0.08483950793743134, acc: 0.9818181991577148)
[2025-02-13 20:12:20,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:20,572][root][INFO] - Training Epoch: 1/2, step 6572/7134 completed (loss: 0.13122007250785828, acc: 0.9756097793579102)
[2025-02-13 20:12:20,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:20,986][root][INFO] - Training Epoch: 1/2, step 6573/7134 completed (loss: 0.12437152862548828, acc: 0.9611650705337524)
[2025-02-13 20:12:21,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:21,406][root][INFO] - Training Epoch: 1/2, step 6574/7134 completed (loss: 0.08194677531719208, acc: 0.982758641242981)
[2025-02-13 20:12:21,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:21,808][root][INFO] - Training Epoch: 1/2, step 6575/7134 completed (loss: 0.1543085128068924, acc: 0.9530516266822815)
[2025-02-13 20:12:21,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:22,175][root][INFO] - Training Epoch: 1/2, step 6576/7134 completed (loss: 0.12440498918294907, acc: 0.9678899049758911)
[2025-02-13 20:12:22,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:22,549][root][INFO] - Training Epoch: 1/2, step 6577/7134 completed (loss: 0.22660702466964722, acc: 0.9272727370262146)
[2025-02-13 20:12:22,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:23,013][root][INFO] - Training Epoch: 1/2, step 6578/7134 completed (loss: 0.10508758574724197, acc: 0.9713114500045776)
[2025-02-13 20:12:23,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:23,369][root][INFO] - Training Epoch: 1/2, step 6579/7134 completed (loss: 0.4412241280078888, acc: 0.9097744226455688)
[2025-02-13 20:12:23,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:23,756][root][INFO] - Training Epoch: 1/2, step 6580/7134 completed (loss: 0.1681511551141739, acc: 0.9526315927505493)
[2025-02-13 20:12:23,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:24,139][root][INFO] - Training Epoch: 1/2, step 6581/7134 completed (loss: 0.2373044192790985, acc: 0.9459459185600281)
[2025-02-13 20:12:24,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:24,507][root][INFO] - Training Epoch: 1/2, step 6582/7134 completed (loss: 0.26108208298683167, acc: 0.9473684430122375)
[2025-02-13 20:12:24,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:24,883][root][INFO] - Training Epoch: 1/2, step 6583/7134 completed (loss: 0.1310446709394455, acc: 0.9748743772506714)
[2025-02-13 20:12:25,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:25,339][root][INFO] - Training Epoch: 1/2, step 6584/7134 completed (loss: 0.17710046470165253, acc: 0.9677419066429138)
[2025-02-13 20:12:25,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:25,737][root][INFO] - Training Epoch: 1/2, step 6585/7134 completed (loss: 0.16798633337020874, acc: 0.9459459185600281)
[2025-02-13 20:12:25,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:26,133][root][INFO] - Training Epoch: 1/2, step 6586/7134 completed (loss: 0.19303859770298004, acc: 0.9631901979446411)
[2025-02-13 20:12:26,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:26,564][root][INFO] - Training Epoch: 1/2, step 6587/7134 completed (loss: 0.24563272297382355, acc: 0.969072163105011)
[2025-02-13 20:12:26,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:26,988][root][INFO] - Training Epoch: 1/2, step 6588/7134 completed (loss: 0.16964000463485718, acc: 0.9593023061752319)
[2025-02-13 20:12:27,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:27,361][root][INFO] - Training Epoch: 1/2, step 6589/7134 completed (loss: 0.2173255980014801, acc: 0.9440993666648865)
[2025-02-13 20:12:27,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:27,761][root][INFO] - Training Epoch: 1/2, step 6590/7134 completed (loss: 0.20052240788936615, acc: 0.9521276354789734)
[2025-02-13 20:12:27,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:28,180][root][INFO] - Training Epoch: 1/2, step 6591/7134 completed (loss: 0.15796397626399994, acc: 0.9590643048286438)
[2025-02-13 20:12:28,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:28,596][root][INFO] - Training Epoch: 1/2, step 6592/7134 completed (loss: 0.19004298746585846, acc: 0.9458128213882446)
[2025-02-13 20:12:28,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:28,965][root][INFO] - Training Epoch: 1/2, step 6593/7134 completed (loss: 0.15731216967105865, acc: 0.9666666388511658)
[2025-02-13 20:12:29,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:29,333][root][INFO] - Training Epoch: 1/2, step 6594/7134 completed (loss: 0.1679200381040573, acc: 0.9605911374092102)
[2025-02-13 20:12:29,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:29,749][root][INFO] - Training Epoch: 1/2, step 6595/7134 completed (loss: 0.13543541729450226, acc: 0.9691358208656311)
[2025-02-13 20:12:29,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:30,111][root][INFO] - Training Epoch: 1/2, step 6596/7134 completed (loss: 0.16887259483337402, acc: 0.9578313231468201)
[2025-02-13 20:12:30,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:30,479][root][INFO] - Training Epoch: 1/2, step 6597/7134 completed (loss: 0.1276543140411377, acc: 0.9476439952850342)
[2025-02-13 20:12:30,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:30,922][root][INFO] - Training Epoch: 1/2, step 6598/7134 completed (loss: 0.1812397688627243, acc: 0.9617486596107483)
[2025-02-13 20:12:31,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:31,325][root][INFO] - Training Epoch: 1/2, step 6599/7134 completed (loss: 0.16836804151535034, acc: 0.9463414549827576)
[2025-02-13 20:12:31,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:31,728][root][INFO] - Training Epoch: 1/2, step 6600/7134 completed (loss: 0.32034197449684143, acc: 0.9101123809814453)
[2025-02-13 20:12:31,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:32,113][root][INFO] - Training Epoch: 1/2, step 6601/7134 completed (loss: 0.2612632215023041, acc: 0.9313725233078003)
[2025-02-13 20:12:32,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:32,534][root][INFO] - Training Epoch: 1/2, step 6602/7134 completed (loss: 0.19129334390163422, acc: 0.9504950642585754)
[2025-02-13 20:12:32,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:32,975][root][INFO] - Training Epoch: 1/2, step 6603/7134 completed (loss: 0.14959369599819183, acc: 0.955974817276001)
[2025-02-13 20:12:33,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:33,355][root][INFO] - Training Epoch: 1/2, step 6604/7134 completed (loss: 0.328751802444458, acc: 0.9139785170555115)
[2025-02-13 20:12:33,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:33,748][root][INFO] - Training Epoch: 1/2, step 6605/7134 completed (loss: 0.3342815041542053, acc: 0.9032257795333862)
[2025-02-13 20:12:33,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:34,181][root][INFO] - Training Epoch: 1/2, step 6606/7134 completed (loss: 0.1619012951850891, acc: 0.9553072452545166)
[2025-02-13 20:12:34,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:34,609][root][INFO] - Training Epoch: 1/2, step 6607/7134 completed (loss: 0.3015470504760742, acc: 0.9419354796409607)
[2025-02-13 20:12:34,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:35,000][root][INFO] - Training Epoch: 1/2, step 6608/7134 completed (loss: 0.12585890293121338, acc: 0.965753436088562)
[2025-02-13 20:12:35,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:35,385][root][INFO] - Training Epoch: 1/2, step 6609/7134 completed (loss: 0.19036197662353516, acc: 0.9496855139732361)
[2025-02-13 20:12:35,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:35,738][root][INFO] - Training Epoch: 1/2, step 6610/7134 completed (loss: 0.27002665400505066, acc: 0.9333333373069763)
[2025-02-13 20:12:35,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:36,141][root][INFO] - Training Epoch: 1/2, step 6611/7134 completed (loss: 0.21043729782104492, acc: 0.9246575236320496)
[2025-02-13 20:12:36,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:36,565][root][INFO] - Training Epoch: 1/2, step 6612/7134 completed (loss: 0.31949925422668457, acc: 0.9384615421295166)
[2025-02-13 20:12:36,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:36,955][root][INFO] - Training Epoch: 1/2, step 6613/7134 completed (loss: 0.172223761677742, acc: 0.9664429426193237)
[2025-02-13 20:12:37,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:37,309][root][INFO] - Training Epoch: 1/2, step 6614/7134 completed (loss: 0.15097209811210632, acc: 0.9670329689979553)
[2025-02-13 20:12:37,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:37,719][root][INFO] - Training Epoch: 1/2, step 6615/7134 completed (loss: 0.2088935673236847, acc: 0.9527027010917664)
[2025-02-13 20:12:37,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:38,134][root][INFO] - Training Epoch: 1/2, step 6616/7134 completed (loss: 0.14610427618026733, acc: 0.9757575988769531)
[2025-02-13 20:12:38,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:38,505][root][INFO] - Training Epoch: 1/2, step 6617/7134 completed (loss: 0.14905036985874176, acc: 0.970370352268219)
[2025-02-13 20:12:38,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:38,853][root][INFO] - Training Epoch: 1/2, step 6618/7134 completed (loss: 0.04520281404256821, acc: 0.9927536249160767)
[2025-02-13 20:12:39,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:39,236][root][INFO] - Training Epoch: 1/2, step 6619/7134 completed (loss: 0.07838578522205353, acc: 0.9887005686759949)
[2025-02-13 20:12:39,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:39,662][root][INFO] - Training Epoch: 1/2, step 6620/7134 completed (loss: 0.23901303112506866, acc: 0.9305555820465088)
[2025-02-13 20:12:39,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:40,107][root][INFO] - Training Epoch: 1/2, step 6621/7134 completed (loss: 0.16800403594970703, acc: 0.9599999785423279)
[2025-02-13 20:12:40,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:40,467][root][INFO] - Training Epoch: 1/2, step 6622/7134 completed (loss: 0.12215261906385422, acc: 0.9615384340286255)
[2025-02-13 20:12:40,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:40,886][root][INFO] - Training Epoch: 1/2, step 6623/7134 completed (loss: 0.12825435400009155, acc: 0.9640718698501587)
[2025-02-13 20:12:41,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:41,331][root][INFO] - Training Epoch: 1/2, step 6624/7134 completed (loss: 0.08988448232412338, acc: 0.987261176109314)
[2025-02-13 20:12:41,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:41,730][root][INFO] - Training Epoch: 1/2, step 6625/7134 completed (loss: 0.10896240919828415, acc: 0.9668874144554138)
[2025-02-13 20:12:41,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:42,163][root][INFO] - Training Epoch: 1/2, step 6626/7134 completed (loss: 0.12595734000205994, acc: 0.954285740852356)
[2025-02-13 20:12:42,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:42,557][root][INFO] - Training Epoch: 1/2, step 6627/7134 completed (loss: 0.1448289006948471, acc: 0.9623655676841736)
[2025-02-13 20:12:42,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:42,964][root][INFO] - Training Epoch: 1/2, step 6628/7134 completed (loss: 0.13251745700836182, acc: 0.9642857313156128)
[2025-02-13 20:12:43,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:43,342][root][INFO] - Training Epoch: 1/2, step 6629/7134 completed (loss: 0.2265925407409668, acc: 0.9624999761581421)
[2025-02-13 20:12:43,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:43,791][root][INFO] - Training Epoch: 1/2, step 6630/7134 completed (loss: 0.12694968283176422, acc: 0.9739583134651184)
[2025-02-13 20:12:43,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:44,185][root][INFO] - Training Epoch: 1/2, step 6631/7134 completed (loss: 0.09867363423109055, acc: 0.9813664555549622)
[2025-02-13 20:12:44,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:44,619][root][INFO] - Training Epoch: 1/2, step 6632/7134 completed (loss: 0.10008414089679718, acc: 0.9695431590080261)
[2025-02-13 20:12:44,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:44,989][root][INFO] - Training Epoch: 1/2, step 6633/7134 completed (loss: 0.12327191233634949, acc: 0.9835164546966553)
[2025-02-13 20:12:45,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:45,412][root][INFO] - Training Epoch: 1/2, step 6634/7134 completed (loss: 0.13975095748901367, acc: 0.9548872113227844)
[2025-02-13 20:12:45,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:45,768][root][INFO] - Training Epoch: 1/2, step 6635/7134 completed (loss: 0.33744800090789795, acc: 0.9350000023841858)
[2025-02-13 20:12:45,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:46,163][root][INFO] - Training Epoch: 1/2, step 6636/7134 completed (loss: 0.2087973654270172, acc: 0.9317073225975037)
[2025-02-13 20:12:46,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:46,564][root][INFO] - Training Epoch: 1/2, step 6637/7134 completed (loss: 0.13049757480621338, acc: 0.9624999761581421)
[2025-02-13 20:12:46,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:46,968][root][INFO] - Training Epoch: 1/2, step 6638/7134 completed (loss: 0.15111176669597626, acc: 0.9670329689979553)
[2025-02-13 20:12:47,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:47,362][root][INFO] - Training Epoch: 1/2, step 6639/7134 completed (loss: 0.30158257484436035, acc: 0.9281437397003174)
[2025-02-13 20:12:47,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:47,798][root][INFO] - Training Epoch: 1/2, step 6640/7134 completed (loss: 0.11287851631641388, acc: 0.9808917045593262)
[2025-02-13 20:12:47,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:48,185][root][INFO] - Training Epoch: 1/2, step 6641/7134 completed (loss: 0.3303983509540558, acc: 0.9336283206939697)
[2025-02-13 20:12:48,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:48,604][root][INFO] - Training Epoch: 1/2, step 6642/7134 completed (loss: 0.21562433242797852, acc: 0.9277108311653137)
[2025-02-13 20:12:48,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:49,010][root][INFO] - Training Epoch: 1/2, step 6643/7134 completed (loss: 0.1573558896780014, acc: 0.9623655676841736)
[2025-02-13 20:12:49,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:49,388][root][INFO] - Training Epoch: 1/2, step 6644/7134 completed (loss: 0.24124890565872192, acc: 0.9336099624633789)
[2025-02-13 20:12:49,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:49,725][root][INFO] - Training Epoch: 1/2, step 6645/7134 completed (loss: 0.10884464532136917, acc: 0.9682539701461792)
[2025-02-13 20:12:49,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:50,141][root][INFO] - Training Epoch: 1/2, step 6646/7134 completed (loss: 0.19129520654678345, acc: 0.9650654792785645)
[2025-02-13 20:12:50,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:50,553][root][INFO] - Training Epoch: 1/2, step 6647/7134 completed (loss: 0.08615157753229141, acc: 0.9764705896377563)
[2025-02-13 20:12:50,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:50,954][root][INFO] - Training Epoch: 1/2, step 6648/7134 completed (loss: 0.22825519740581512, acc: 0.948387086391449)
[2025-02-13 20:12:51,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:51,354][root][INFO] - Training Epoch: 1/2, step 6649/7134 completed (loss: 0.1970311403274536, acc: 0.9468598961830139)
[2025-02-13 20:12:51,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:51,770][root][INFO] - Training Epoch: 1/2, step 6650/7134 completed (loss: 0.14425556361675262, acc: 0.9545454382896423)
[2025-02-13 20:12:51,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:52,170][root][INFO] - Training Epoch: 1/2, step 6651/7134 completed (loss: 0.1914801150560379, acc: 0.9488636255264282)
[2025-02-13 20:12:52,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:52,566][root][INFO] - Training Epoch: 1/2, step 6652/7134 completed (loss: 0.2789318561553955, acc: 0.9215686321258545)
[2025-02-13 20:12:52,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:52,935][root][INFO] - Training Epoch: 1/2, step 6653/7134 completed (loss: 0.38098540902137756, acc: 0.8909090757369995)
[2025-02-13 20:12:53,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:53,332][root][INFO] - Training Epoch: 1/2, step 6654/7134 completed (loss: 0.39611873030662537, acc: 0.9177215099334717)
[2025-02-13 20:12:53,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:53,734][root][INFO] - Training Epoch: 1/2, step 6655/7134 completed (loss: 0.13568519055843353, acc: 0.9714285731315613)
[2025-02-13 20:12:53,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:54,121][root][INFO] - Training Epoch: 1/2, step 6656/7134 completed (loss: 0.2865672707557678, acc: 0.9210526347160339)
[2025-02-13 20:12:54,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:54,513][root][INFO] - Training Epoch: 1/2, step 6657/7134 completed (loss: 0.2412971556186676, acc: 0.936170220375061)
[2025-02-13 20:12:54,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:54,910][root][INFO] - Training Epoch: 1/2, step 6658/7134 completed (loss: 0.15402869880199432, acc: 0.9543147087097168)
[2025-02-13 20:12:55,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:55,290][root][INFO] - Training Epoch: 1/2, step 6659/7134 completed (loss: 0.14183448255062103, acc: 0.9726775884628296)
[2025-02-13 20:12:55,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:55,648][root][INFO] - Training Epoch: 1/2, step 6660/7134 completed (loss: 0.10240821540355682, acc: 0.9836065769195557)
[2025-02-13 20:12:55,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:56,030][root][INFO] - Training Epoch: 1/2, step 6661/7134 completed (loss: 0.23435567319393158, acc: 0.9402984976768494)
[2025-02-13 20:12:56,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:56,426][root][INFO] - Training Epoch: 1/2, step 6662/7134 completed (loss: 0.41727060079574585, acc: 0.8633093237876892)
[2025-02-13 20:12:56,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:56,813][root][INFO] - Training Epoch: 1/2, step 6663/7134 completed (loss: 0.17949558794498444, acc: 0.948051929473877)
[2025-02-13 20:12:56,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:57,214][root][INFO] - Training Epoch: 1/2, step 6664/7134 completed (loss: 0.2189904898405075, acc: 0.9357143044471741)
[2025-02-13 20:12:57,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:57,627][root][INFO] - Training Epoch: 1/2, step 6665/7134 completed (loss: 0.34382081031799316, acc: 0.9119496941566467)
[2025-02-13 20:12:57,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:58,017][root][INFO] - Training Epoch: 1/2, step 6666/7134 completed (loss: 0.09684644639492035, acc: 0.9696969985961914)
[2025-02-13 20:12:58,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:58,391][root][INFO] - Training Epoch: 1/2, step 6667/7134 completed (loss: 0.2722523510456085, acc: 0.8979591727256775)
[2025-02-13 20:12:58,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:58,756][root][INFO] - Training Epoch: 1/2, step 6668/7134 completed (loss: 0.06852996349334717, acc: 0.985401451587677)
[2025-02-13 20:12:58,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:59,144][root][INFO] - Training Epoch: 1/2, step 6669/7134 completed (loss: 0.08455568552017212, acc: 0.9791666865348816)
[2025-02-13 20:12:59,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:59,535][root][INFO] - Training Epoch: 1/2, step 6670/7134 completed (loss: 0.12610583007335663, acc: 0.9640287756919861)
[2025-02-13 20:12:59,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:59,902][root][INFO] - Training Epoch: 1/2, step 6671/7134 completed (loss: 0.06900608539581299, acc: 0.9817073345184326)
[2025-02-13 20:13:00,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:00,276][root][INFO] - Training Epoch: 1/2, step 6672/7134 completed (loss: 0.0831698477268219, acc: 0.9760000109672546)
[2025-02-13 20:13:00,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:00,668][root][INFO] - Training Epoch: 1/2, step 6673/7134 completed (loss: 0.08438819646835327, acc: 0.9685534834861755)
[2025-02-13 20:13:00,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:01,061][root][INFO] - Training Epoch: 1/2, step 6674/7134 completed (loss: 0.04343391954898834, acc: 0.991525411605835)
[2025-02-13 20:13:01,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:01,472][root][INFO] - Training Epoch: 1/2, step 6675/7134 completed (loss: 0.2210482656955719, acc: 0.9509803652763367)
[2025-02-13 20:13:01,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:01,874][root][INFO] - Training Epoch: 1/2, step 6676/7134 completed (loss: 0.0344444140791893, acc: 0.9900000095367432)
[2025-02-13 20:13:02,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:02,250][root][INFO] - Training Epoch: 1/2, step 6677/7134 completed (loss: 0.07692841440439224, acc: 0.9923664331436157)
[2025-02-13 20:13:02,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:02,635][root][INFO] - Training Epoch: 1/2, step 6678/7134 completed (loss: 0.08439698070287704, acc: 0.9851852059364319)
[2025-02-13 20:13:02,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:03,022][root][INFO] - Training Epoch: 1/2, step 6679/7134 completed (loss: 0.042435210198163986, acc: 0.991150438785553)
[2025-02-13 20:13:03,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:03,402][root][INFO] - Training Epoch: 1/2, step 6680/7134 completed (loss: 0.03550906479358673, acc: 0.9885057210922241)
[2025-02-13 20:13:03,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:03,770][root][INFO] - Training Epoch: 1/2, step 6681/7134 completed (loss: 0.01153819914907217, acc: 1.0)
[2025-02-13 20:13:03,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:04,146][root][INFO] - Training Epoch: 1/2, step 6682/7134 completed (loss: 0.214344784617424, acc: 0.9611650705337524)
[2025-02-13 20:13:04,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:04,579][root][INFO] - Training Epoch: 1/2, step 6683/7134 completed (loss: 0.2212887704372406, acc: 0.977011501789093)
[2025-02-13 20:13:04,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:04,974][root][INFO] - Training Epoch: 1/2, step 6684/7134 completed (loss: 0.14785218238830566, acc: 0.960629940032959)
[2025-02-13 20:13:05,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:05,358][root][INFO] - Training Epoch: 1/2, step 6685/7134 completed (loss: 0.13928140699863434, acc: 0.9359999895095825)
[2025-02-13 20:13:05,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:05,735][root][INFO] - Training Epoch: 1/2, step 6686/7134 completed (loss: 0.22969241440296173, acc: 0.9568965435028076)
[2025-02-13 20:13:05,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:06,098][root][INFO] - Training Epoch: 1/2, step 6687/7134 completed (loss: 0.2052774429321289, acc: 0.9651162624359131)
[2025-02-13 20:13:06,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:06,517][root][INFO] - Training Epoch: 1/2, step 6688/7134 completed (loss: 0.4503091871738434, acc: 0.8848921060562134)
[2025-02-13 20:13:06,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:06,914][root][INFO] - Training Epoch: 1/2, step 6689/7134 completed (loss: 0.2550901174545288, acc: 0.9259259104728699)
[2025-02-13 20:13:07,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:07,330][root][INFO] - Training Epoch: 1/2, step 6690/7134 completed (loss: 0.3639076054096222, acc: 0.9019607901573181)
[2025-02-13 20:13:07,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:07,719][root][INFO] - Training Epoch: 1/2, step 6691/7134 completed (loss: 0.15427003800868988, acc: 0.9583333134651184)
[2025-02-13 20:13:07,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:08,135][root][INFO] - Training Epoch: 1/2, step 6692/7134 completed (loss: 0.34320345520973206, acc: 0.9158878326416016)
[2025-02-13 20:13:08,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:08,500][root][INFO] - Training Epoch: 1/2, step 6693/7134 completed (loss: 0.20707759261131287, acc: 0.9555555582046509)
[2025-02-13 20:13:08,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:08,910][root][INFO] - Training Epoch: 1/2, step 6694/7134 completed (loss: 0.086354561150074, acc: 0.9668874144554138)
[2025-02-13 20:13:09,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:09,300][root][INFO] - Training Epoch: 1/2, step 6695/7134 completed (loss: 0.12058553844690323, acc: 0.9715909361839294)
[2025-02-13 20:13:09,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:09,691][root][INFO] - Training Epoch: 1/2, step 6696/7134 completed (loss: 0.21001702547073364, acc: 0.9583333134651184)
[2025-02-13 20:13:09,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:10,086][root][INFO] - Training Epoch: 1/2, step 6697/7134 completed (loss: 0.3497186005115509, acc: 0.9285714030265808)
[2025-02-13 20:13:10,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:10,462][root][INFO] - Training Epoch: 1/2, step 6698/7134 completed (loss: 0.1403219848871231, acc: 0.9597315192222595)
[2025-02-13 20:13:10,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:10,824][root][INFO] - Training Epoch: 1/2, step 6699/7134 completed (loss: 0.18604165315628052, acc: 0.961904764175415)
[2025-02-13 20:13:10,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:11,196][root][INFO] - Training Epoch: 1/2, step 6700/7134 completed (loss: 0.39966604113578796, acc: 0.8888888955116272)
[2025-02-13 20:13:11,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:11,586][root][INFO] - Training Epoch: 1/2, step 6701/7134 completed (loss: 0.6901951432228088, acc: 0.8134328126907349)
[2025-02-13 20:13:11,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:11,999][root][INFO] - Training Epoch: 1/2, step 6702/7134 completed (loss: 0.2508508861064911, acc: 0.9304347634315491)
[2025-02-13 20:13:12,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:12,330][root][INFO] - Training Epoch: 1/2, step 6703/7134 completed (loss: 0.23007574677467346, acc: 0.9492753744125366)
[2025-02-13 20:13:12,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:12,717][root][INFO] - Training Epoch: 1/2, step 6704/7134 completed (loss: 0.3427896499633789, acc: 0.8850574493408203)
[2025-02-13 20:13:12,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:13,080][root][INFO] - Training Epoch: 1/2, step 6705/7134 completed (loss: 0.15290199220180511, acc: 0.9421965479850769)
[2025-02-13 20:13:13,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:13,436][root][INFO] - Training Epoch: 1/2, step 6706/7134 completed (loss: 0.17703816294670105, acc: 0.9636363387107849)
[2025-02-13 20:13:13,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:13,771][root][INFO] - Training Epoch: 1/2, step 6707/7134 completed (loss: 0.17929479479789734, acc: 0.9487179517745972)
[2025-02-13 20:13:13,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:14,131][root][INFO] - Training Epoch: 1/2, step 6708/7134 completed (loss: 0.19873318076133728, acc: 0.9389312863349915)
[2025-02-13 20:13:14,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:14,495][root][INFO] - Training Epoch: 1/2, step 6709/7134 completed (loss: 0.22859403491020203, acc: 0.9634146094322205)
[2025-02-13 20:13:14,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:14,897][root][INFO] - Training Epoch: 1/2, step 6710/7134 completed (loss: 0.2098073810338974, acc: 0.9473684430122375)
[2025-02-13 20:13:15,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:15,322][root][INFO] - Training Epoch: 1/2, step 6711/7134 completed (loss: 0.21362581849098206, acc: 0.9746835231781006)
[2025-02-13 20:13:15,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:15,706][root][INFO] - Training Epoch: 1/2, step 6712/7134 completed (loss: 0.31361123919487, acc: 0.8965517282485962)
[2025-02-13 20:13:15,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:16,157][root][INFO] - Training Epoch: 1/2, step 6713/7134 completed (loss: 0.522320568561554, acc: 0.907975435256958)
[2025-02-13 20:13:16,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:16,562][root][INFO] - Training Epoch: 1/2, step 6714/7134 completed (loss: 0.3075938820838928, acc: 0.9364162087440491)
[2025-02-13 20:13:16,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:16,954][root][INFO] - Training Epoch: 1/2, step 6715/7134 completed (loss: 0.49102601408958435, acc: 0.8986486196517944)
[2025-02-13 20:13:17,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:17,359][root][INFO] - Training Epoch: 1/2, step 6716/7134 completed (loss: 0.16282221674919128, acc: 0.9452054500579834)
[2025-02-13 20:13:17,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:17,729][root][INFO] - Training Epoch: 1/2, step 6717/7134 completed (loss: 0.26753440499305725, acc: 0.9459459185600281)
[2025-02-13 20:13:17,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:18,112][root][INFO] - Training Epoch: 1/2, step 6718/7134 completed (loss: 0.17217570543289185, acc: 0.9586206674575806)
[2025-02-13 20:13:18,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:18,496][root][INFO] - Training Epoch: 1/2, step 6719/7134 completed (loss: 0.20811748504638672, acc: 0.9463087320327759)
[2025-02-13 20:13:18,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:18,909][root][INFO] - Training Epoch: 1/2, step 6720/7134 completed (loss: 0.27734896540641785, acc: 0.931034505367279)
[2025-02-13 20:13:19,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:19,257][root][INFO] - Training Epoch: 1/2, step 6721/7134 completed (loss: 0.29666632413864136, acc: 0.9363057613372803)
[2025-02-13 20:13:19,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:19,622][root][INFO] - Training Epoch: 1/2, step 6722/7134 completed (loss: 0.37606188654899597, acc: 0.9300699234008789)
[2025-02-13 20:13:19,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:20,002][root][INFO] - Training Epoch: 1/2, step 6723/7134 completed (loss: 0.2190527617931366, acc: 0.9234972596168518)
[2025-02-13 20:13:20,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:20,403][root][INFO] - Training Epoch: 1/2, step 6724/7134 completed (loss: 0.2794899046421051, acc: 0.9496855139732361)
[2025-02-13 20:13:20,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:20,813][root][INFO] - Training Epoch: 1/2, step 6725/7134 completed (loss: 0.18023979663848877, acc: 0.9539473652839661)
[2025-02-13 20:13:20,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:21,175][root][INFO] - Training Epoch: 1/2, step 6726/7134 completed (loss: 0.39018186926841736, acc: 0.942148745059967)
[2025-02-13 20:13:21,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:21,592][root][INFO] - Training Epoch: 1/2, step 6727/7134 completed (loss: 0.23116381466388702, acc: 0.9280575513839722)
[2025-02-13 20:13:21,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:21,983][root][INFO] - Training Epoch: 1/2, step 6728/7134 completed (loss: 0.3607781231403351, acc: 0.8903225660324097)
[2025-02-13 20:13:22,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:22,372][root][INFO] - Training Epoch: 1/2, step 6729/7134 completed (loss: 0.08633396774530411, acc: 0.9873417615890503)
[2025-02-13 20:13:22,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:22,771][root][INFO] - Training Epoch: 1/2, step 6730/7134 completed (loss: 0.31061214208602905, acc: 0.9195402264595032)
[2025-02-13 20:13:22,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:23,181][root][INFO] - Training Epoch: 1/2, step 6731/7134 completed (loss: 0.16179999709129333, acc: 0.9698795080184937)
[2025-02-13 20:13:23,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:23,557][root][INFO] - Training Epoch: 1/2, step 6732/7134 completed (loss: 0.14972077310085297, acc: 0.9551281929016113)
[2025-02-13 20:13:23,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:23,944][root][INFO] - Training Epoch: 1/2, step 6733/7134 completed (loss: 0.16161909699440002, acc: 0.9568345546722412)
[2025-02-13 20:13:24,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:24,316][root][INFO] - Training Epoch: 1/2, step 6734/7134 completed (loss: 0.26208731532096863, acc: 0.9440559148788452)
[2025-02-13 20:13:24,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:24,672][root][INFO] - Training Epoch: 1/2, step 6735/7134 completed (loss: 0.2563275992870331, acc: 0.9358974099159241)
[2025-02-13 20:13:24,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:25,079][root][INFO] - Training Epoch: 1/2, step 6736/7134 completed (loss: 0.24979858100414276, acc: 0.9333333373069763)
[2025-02-13 20:13:25,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:25,460][root][INFO] - Training Epoch: 1/2, step 6737/7134 completed (loss: 0.25627565383911133, acc: 0.9470198750495911)
[2025-02-13 20:13:25,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:25,839][root][INFO] - Training Epoch: 1/2, step 6738/7134 completed (loss: 0.17735472321510315, acc: 0.9485294222831726)
[2025-02-13 20:13:25,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:26,244][root][INFO] - Training Epoch: 1/2, step 6739/7134 completed (loss: 0.12612974643707275, acc: 0.9642857313156128)
[2025-02-13 20:13:26,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:26,635][root][INFO] - Training Epoch: 1/2, step 6740/7134 completed (loss: 0.31622743606567383, acc: 0.9391891956329346)
[2025-02-13 20:13:26,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:27,030][root][INFO] - Training Epoch: 1/2, step 6741/7134 completed (loss: 0.10043454170227051, acc: 0.9759036302566528)
[2025-02-13 20:13:27,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:27,465][root][INFO] - Training Epoch: 1/2, step 6742/7134 completed (loss: 0.07224605977535248, acc: 0.9764705896377563)
[2025-02-13 20:13:27,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:27,885][root][INFO] - Training Epoch: 1/2, step 6743/7134 completed (loss: 0.06082713603973389, acc: 0.9887005686759949)
[2025-02-13 20:13:28,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:28,296][root][INFO] - Training Epoch: 1/2, step 6744/7134 completed (loss: 0.10300514101982117, acc: 0.9662162065505981)
[2025-02-13 20:13:28,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:28,678][root][INFO] - Training Epoch: 1/2, step 6745/7134 completed (loss: 0.03840257227420807, acc: 0.9941860437393188)
[2025-02-13 20:13:28,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:29,055][root][INFO] - Training Epoch: 1/2, step 6746/7134 completed (loss: 0.05853276699781418, acc: 0.9925373196601868)
[2025-02-13 20:13:29,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:29,405][root][INFO] - Training Epoch: 1/2, step 6747/7134 completed (loss: 0.09005124121904373, acc: 0.9685863852500916)
[2025-02-13 20:13:29,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:29,779][root][INFO] - Training Epoch: 1/2, step 6748/7134 completed (loss: 0.11945396661758423, acc: 0.9852941036224365)
[2025-02-13 20:13:29,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:30,173][root][INFO] - Training Epoch: 1/2, step 6749/7134 completed (loss: 0.1609116643667221, acc: 0.9511111378669739)
[2025-02-13 20:13:30,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:30,566][root][INFO] - Training Epoch: 1/2, step 6750/7134 completed (loss: 0.14070548117160797, acc: 0.9653465151786804)
[2025-02-13 20:13:30,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:31,000][root][INFO] - Training Epoch: 1/2, step 6751/7134 completed (loss: 0.10629774630069733, acc: 0.9800994992256165)
[2025-02-13 20:13:31,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:31,403][root][INFO] - Training Epoch: 1/2, step 6752/7134 completed (loss: 0.17414703965187073, acc: 0.9693251252174377)
[2025-02-13 20:13:31,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:31,804][root][INFO] - Training Epoch: 1/2, step 6753/7134 completed (loss: 0.05630648881196976, acc: 0.9797979593276978)
[2025-02-13 20:13:31,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:32,254][root][INFO] - Training Epoch: 1/2, step 6754/7134 completed (loss: 0.13460755348205566, acc: 0.9738219976425171)
[2025-02-13 20:13:32,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:32,710][root][INFO] - Training Epoch: 1/2, step 6755/7134 completed (loss: 0.16203556954860687, acc: 0.963350772857666)
[2025-02-13 20:13:32,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:33,112][root][INFO] - Training Epoch: 1/2, step 6756/7134 completed (loss: 0.041221946477890015, acc: 0.983146071434021)
[2025-02-13 20:13:33,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:33,480][root][INFO] - Training Epoch: 1/2, step 6757/7134 completed (loss: 0.047685541212558746, acc: 0.9935483932495117)
[2025-02-13 20:13:33,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:33,873][root][INFO] - Training Epoch: 1/2, step 6758/7134 completed (loss: 0.0609988272190094, acc: 0.9896373152732849)
[2025-02-13 20:13:34,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:34,285][root][INFO] - Training Epoch: 1/2, step 6759/7134 completed (loss: 0.07168907672166824, acc: 0.9776785969734192)
[2025-02-13 20:13:34,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:34,741][root][INFO] - Training Epoch: 1/2, step 6760/7134 completed (loss: 0.2642480134963989, acc: 0.9680851101875305)
[2025-02-13 20:13:34,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:35,171][root][INFO] - Training Epoch: 1/2, step 6761/7134 completed (loss: 0.2076931744813919, acc: 0.9714285731315613)
[2025-02-13 20:13:35,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:35,609][root][INFO] - Training Epoch: 1/2, step 6762/7134 completed (loss: 0.048367906361818314, acc: 0.9850000143051147)
[2025-02-13 20:13:35,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:35,996][root][INFO] - Training Epoch: 1/2, step 6763/7134 completed (loss: 0.06030064821243286, acc: 0.9946523904800415)
[2025-02-13 20:13:36,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:36,402][root][INFO] - Training Epoch: 1/2, step 6764/7134 completed (loss: 0.08844148367643356, acc: 0.9823529124259949)
[2025-02-13 20:13:36,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:36,803][root][INFO] - Training Epoch: 1/2, step 6765/7134 completed (loss: 0.052408117800951004, acc: 0.9890109896659851)
[2025-02-13 20:13:36,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:37,163][root][INFO] - Training Epoch: 1/2, step 6766/7134 completed (loss: 0.1041344478726387, acc: 0.9828571677207947)
[2025-02-13 20:13:37,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:37,510][root][INFO] - Training Epoch: 1/2, step 6767/7134 completed (loss: 0.18795092403888702, acc: 0.9553571343421936)
[2025-02-13 20:13:37,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:37,892][root][INFO] - Training Epoch: 1/2, step 6768/7134 completed (loss: 0.05655105412006378, acc: 0.976190447807312)
[2025-02-13 20:13:38,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:38,261][root][INFO] - Training Epoch: 1/2, step 6769/7134 completed (loss: 0.20664231479167938, acc: 0.9551281929016113)
[2025-02-13 20:13:38,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:38,664][root][INFO] - Training Epoch: 1/2, step 6770/7134 completed (loss: 0.16104035079479218, acc: 0.9684210419654846)
[2025-02-13 20:13:38,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:39,061][root][INFO] - Training Epoch: 1/2, step 6771/7134 completed (loss: 0.06117497384548187, acc: 0.9941520690917969)
[2025-02-13 20:13:39,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:39,453][root][INFO] - Training Epoch: 1/2, step 6772/7134 completed (loss: 0.2984682619571686, acc: 0.9230769276618958)
[2025-02-13 20:13:39,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:39,799][root][INFO] - Training Epoch: 1/2, step 6773/7134 completed (loss: 0.13020873069763184, acc: 0.9691358208656311)
[2025-02-13 20:13:39,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:40,178][root][INFO] - Training Epoch: 1/2, step 6774/7134 completed (loss: 0.18887558579444885, acc: 0.9526627063751221)
[2025-02-13 20:13:40,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:40,554][root][INFO] - Training Epoch: 1/2, step 6775/7134 completed (loss: 0.14958657324314117, acc: 0.9655172228813171)
[2025-02-13 20:13:40,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:40,952][root][INFO] - Training Epoch: 1/2, step 6776/7134 completed (loss: 0.10677629709243774, acc: 0.984000027179718)
[2025-02-13 20:13:41,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:41,339][root][INFO] - Training Epoch: 1/2, step 6777/7134 completed (loss: 0.19180160760879517, acc: 0.977142870426178)
[2025-02-13 20:13:41,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:41,757][root][INFO] - Training Epoch: 1/2, step 6778/7134 completed (loss: 0.06960117071866989, acc: 0.9886363744735718)
[2025-02-13 20:13:41,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:42,120][root][INFO] - Training Epoch: 1/2, step 6779/7134 completed (loss: 0.09298785775899887, acc: 0.976190447807312)
[2025-02-13 20:13:42,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:42,488][root][INFO] - Training Epoch: 1/2, step 6780/7134 completed (loss: 0.051328230649232864, acc: 0.9940119981765747)
[2025-02-13 20:13:42,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:42,858][root][INFO] - Training Epoch: 1/2, step 6781/7134 completed (loss: 0.040383003652095795, acc: 0.9941520690917969)
[2025-02-13 20:13:42,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:43,230][root][INFO] - Training Epoch: 1/2, step 6782/7134 completed (loss: 0.04440658539533615, acc: 0.9935483932495117)
[2025-02-13 20:13:43,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:43,585][root][INFO] - Training Epoch: 1/2, step 6783/7134 completed (loss: 0.12370628863573074, acc: 0.9655172228813171)
[2025-02-13 20:13:43,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:43,954][root][INFO] - Training Epoch: 1/2, step 6784/7134 completed (loss: 0.04443508759140968, acc: 0.9873417615890503)
[2025-02-13 20:13:44,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:44,326][root][INFO] - Training Epoch: 1/2, step 6785/7134 completed (loss: 0.05460642650723457, acc: 0.9803921580314636)
[2025-02-13 20:13:44,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:44,742][root][INFO] - Training Epoch: 1/2, step 6786/7134 completed (loss: 0.05646616593003273, acc: 0.9887005686759949)
[2025-02-13 20:13:44,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:45,134][root][INFO] - Training Epoch: 1/2, step 6787/7134 completed (loss: 0.05648575350642204, acc: 0.9836065769195557)
[2025-02-13 20:13:45,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:45,552][root][INFO] - Training Epoch: 1/2, step 6788/7134 completed (loss: 0.09851237386465073, acc: 0.971222996711731)
[2025-02-13 20:13:45,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:45,934][root][INFO] - Training Epoch: 1/2, step 6789/7134 completed (loss: 0.050495900213718414, acc: 0.978723406791687)
[2025-02-13 20:13:46,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:46,322][root][INFO] - Training Epoch: 1/2, step 6790/7134 completed (loss: 0.1484643816947937, acc: 0.9659090638160706)
[2025-02-13 20:13:46,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:46,691][root][INFO] - Training Epoch: 1/2, step 6791/7134 completed (loss: 0.10234414041042328, acc: 0.9551281929016113)
[2025-02-13 20:13:46,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:47,102][root][INFO] - Training Epoch: 1/2, step 6792/7134 completed (loss: 0.032939981669187546, acc: 1.0)
[2025-02-13 20:13:47,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:47,492][root][INFO] - Training Epoch: 1/2, step 6793/7134 completed (loss: 0.05097740888595581, acc: 0.9824561476707458)
[2025-02-13 20:13:47,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:47,884][root][INFO] - Training Epoch: 1/2, step 6794/7134 completed (loss: 0.0840175524353981, acc: 0.9863945841789246)
[2025-02-13 20:13:48,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:48,262][root][INFO] - Training Epoch: 1/2, step 6795/7134 completed (loss: 0.05447862297296524, acc: 0.9942857027053833)
[2025-02-13 20:13:48,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:48,640][root][INFO] - Training Epoch: 1/2, step 6796/7134 completed (loss: 0.027117162942886353, acc: 0.9883720874786377)
[2025-02-13 20:13:48,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:49,019][root][INFO] - Training Epoch: 1/2, step 6797/7134 completed (loss: 0.18722674250602722, acc: 0.9634146094322205)
[2025-02-13 20:13:49,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:49,383][root][INFO] - Training Epoch: 1/2, step 6798/7134 completed (loss: 0.0884893462061882, acc: 0.9879518151283264)
[2025-02-13 20:13:49,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:49,746][root][INFO] - Training Epoch: 1/2, step 6799/7134 completed (loss: 0.06009684503078461, acc: 0.9820359349250793)
[2025-02-13 20:13:49,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:50,103][root][INFO] - Training Epoch: 1/2, step 6800/7134 completed (loss: 0.11152045428752899, acc: 0.9620253443717957)
[2025-02-13 20:13:50,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:50,476][root][INFO] - Training Epoch: 1/2, step 6801/7134 completed (loss: 0.08855731040239334, acc: 0.9784172773361206)
[2025-02-13 20:13:50,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:50,857][root][INFO] - Training Epoch: 1/2, step 6802/7134 completed (loss: 0.06673014909029007, acc: 0.9860140085220337)
[2025-02-13 20:13:50,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:51,243][root][INFO] - Training Epoch: 1/2, step 6803/7134 completed (loss: 0.1157887876033783, acc: 0.9666666388511658)
[2025-02-13 20:13:51,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:51,623][root][INFO] - Training Epoch: 1/2, step 6804/7134 completed (loss: 0.17337258160114288, acc: 0.9597315192222595)
[2025-02-13 20:13:51,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:52,000][root][INFO] - Training Epoch: 1/2, step 6805/7134 completed (loss: 0.19879576563835144, acc: 0.9407407641410828)
[2025-02-13 20:13:52,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:52,387][root][INFO] - Training Epoch: 1/2, step 6806/7134 completed (loss: 0.11616550385951996, acc: 0.9719101190567017)
[2025-02-13 20:13:52,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:52,777][root][INFO] - Training Epoch: 1/2, step 6807/7134 completed (loss: 0.0639999657869339, acc: 0.981249988079071)
[2025-02-13 20:13:52,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:53,191][root][INFO] - Training Epoch: 1/2, step 6808/7134 completed (loss: 0.12371952831745148, acc: 0.9664429426193237)
[2025-02-13 20:13:53,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:53,605][root][INFO] - Training Epoch: 1/2, step 6809/7134 completed (loss: 0.17669068276882172, acc: 0.9615384340286255)
[2025-02-13 20:13:53,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:53,982][root][INFO] - Training Epoch: 1/2, step 6810/7134 completed (loss: 0.09229018539190292, acc: 0.9545454382896423)
[2025-02-13 20:13:54,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:54,432][root][INFO] - Training Epoch: 1/2, step 6811/7134 completed (loss: 0.08963677287101746, acc: 0.9767441749572754)
[2025-02-13 20:13:54,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:54,833][root][INFO] - Training Epoch: 1/2, step 6812/7134 completed (loss: 0.0635271817445755, acc: 1.0)
[2025-02-13 20:13:54,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:55,221][root][INFO] - Training Epoch: 1/2, step 6813/7134 completed (loss: 0.07540526986122131, acc: 0.987500011920929)
[2025-02-13 20:13:55,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:55,606][root][INFO] - Training Epoch: 1/2, step 6814/7134 completed (loss: 0.052224528044462204, acc: 0.9942857027053833)
[2025-02-13 20:13:55,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:56,015][root][INFO] - Training Epoch: 1/2, step 6815/7134 completed (loss: 0.08736351132392883, acc: 0.9764705896377563)
[2025-02-13 20:13:56,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:56,404][root][INFO] - Training Epoch: 1/2, step 6816/7134 completed (loss: 0.13168659806251526, acc: 0.9652777910232544)
[2025-02-13 20:13:56,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:56,767][root][INFO] - Training Epoch: 1/2, step 6817/7134 completed (loss: 0.19341520965099335, acc: 0.9577465057373047)
[2025-02-13 20:13:56,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:57,140][root][INFO] - Training Epoch: 1/2, step 6818/7134 completed (loss: 0.056044138967990875, acc: 0.9931972622871399)
[2025-02-13 20:13:57,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:57,527][root][INFO] - Training Epoch: 1/2, step 6819/7134 completed (loss: 0.10744896531105042, acc: 0.9781022071838379)
[2025-02-13 20:13:57,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:57,896][root][INFO] - Training Epoch: 1/2, step 6820/7134 completed (loss: 0.06280112266540527, acc: 0.9781022071838379)
[2025-02-13 20:13:58,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:58,265][root][INFO] - Training Epoch: 1/2, step 6821/7134 completed (loss: 0.01624401845037937, acc: 1.0)
[2025-02-13 20:13:58,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:58,631][root][INFO] - Training Epoch: 1/2, step 6822/7134 completed (loss: 0.3105320930480957, acc: 0.930232584476471)
[2025-02-13 20:13:58,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:59,012][root][INFO] - Training Epoch: 1/2, step 6823/7134 completed (loss: 0.15030701458454132, acc: 0.9457831382751465)
[2025-02-13 20:13:59,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:59,416][root][INFO] - Training Epoch: 1/2, step 6824/7134 completed (loss: 0.30724072456359863, acc: 0.8780487775802612)
[2025-02-13 20:13:59,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:59,808][root][INFO] - Training Epoch: 1/2, step 6825/7134 completed (loss: 0.14958442747592926, acc: 0.9569892287254333)
[2025-02-13 20:13:59,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:00,173][root][INFO] - Training Epoch: 1/2, step 6826/7134 completed (loss: 0.26362723112106323, acc: 0.9057971239089966)
[2025-02-13 20:14:00,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:00,553][root][INFO] - Training Epoch: 1/2, step 6827/7134 completed (loss: 0.5047502517700195, acc: 0.90625)
[2025-02-13 20:14:00,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:00,929][root][INFO] - Training Epoch: 1/2, step 6828/7134 completed (loss: 0.4180763363838196, acc: 0.8943662047386169)
[2025-02-13 20:14:01,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:01,302][root][INFO] - Training Epoch: 1/2, step 6829/7134 completed (loss: 0.35119229555130005, acc: 0.9171597361564636)
[2025-02-13 20:14:01,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:01,714][root][INFO] - Training Epoch: 1/2, step 6830/7134 completed (loss: 0.214164137840271, acc: 0.9459459185600281)
[2025-02-13 20:14:01,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:02,086][root][INFO] - Training Epoch: 1/2, step 6831/7134 completed (loss: 0.26062148809432983, acc: 0.9385474920272827)
[2025-02-13 20:14:02,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:02,454][root][INFO] - Training Epoch: 1/2, step 6832/7134 completed (loss: 0.2460840493440628, acc: 0.9305555820465088)
[2025-02-13 20:14:02,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:02,818][root][INFO] - Training Epoch: 1/2, step 6833/7134 completed (loss: 0.22990849614143372, acc: 0.9363057613372803)
[2025-02-13 20:14:02,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:03,161][root][INFO] - Training Epoch: 1/2, step 6834/7134 completed (loss: 0.2998310923576355, acc: 0.9328858852386475)
[2025-02-13 20:14:03,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:03,523][root][INFO] - Training Epoch: 1/2, step 6835/7134 completed (loss: 0.2515541911125183, acc: 0.9477124214172363)
[2025-02-13 20:14:03,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:03,874][root][INFO] - Training Epoch: 1/2, step 6836/7134 completed (loss: 0.18074296414852142, acc: 0.9431279897689819)
[2025-02-13 20:14:04,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:04,254][root][INFO] - Training Epoch: 1/2, step 6837/7134 completed (loss: 0.1913311630487442, acc: 0.9646464586257935)
[2025-02-13 20:14:04,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:04,612][root][INFO] - Training Epoch: 1/2, step 6838/7134 completed (loss: 0.3274846374988556, acc: 0.9096385836601257)
[2025-02-13 20:14:04,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:04,956][root][INFO] - Training Epoch: 1/2, step 6839/7134 completed (loss: 0.5170289874076843, acc: 0.8933333158493042)
[2025-02-13 20:14:05,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:05,311][root][INFO] - Training Epoch: 1/2, step 6840/7134 completed (loss: 0.33230307698249817, acc: 0.9299362897872925)
[2025-02-13 20:14:05,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:05,673][root][INFO] - Training Epoch: 1/2, step 6841/7134 completed (loss: 0.1080833151936531, acc: 0.9756097793579102)
[2025-02-13 20:14:05,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:06,050][root][INFO] - Training Epoch: 1/2, step 6842/7134 completed (loss: 0.19480514526367188, acc: 0.9395604133605957)
[2025-02-13 20:14:06,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:06,446][root][INFO] - Training Epoch: 1/2, step 6843/7134 completed (loss: 0.18406569957733154, acc: 0.9481865167617798)
[2025-02-13 20:14:06,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:06,822][root][INFO] - Training Epoch: 1/2, step 6844/7134 completed (loss: 0.21630585193634033, acc: 0.9328858852386475)
[2025-02-13 20:14:06,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:07,206][root][INFO] - Training Epoch: 1/2, step 6845/7134 completed (loss: 0.1679336130619049, acc: 0.9461538195610046)
[2025-02-13 20:14:07,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:07,570][root][INFO] - Training Epoch: 1/2, step 6846/7134 completed (loss: 0.14157195389270782, acc: 0.9629629850387573)
[2025-02-13 20:14:07,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:07,974][root][INFO] - Training Epoch: 1/2, step 6847/7134 completed (loss: 0.17764458060264587, acc: 0.9437500238418579)
[2025-02-13 20:14:08,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:08,336][root][INFO] - Training Epoch: 1/2, step 6848/7134 completed (loss: 0.17175737023353577, acc: 0.966292142868042)
[2025-02-13 20:14:08,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:08,719][root][INFO] - Training Epoch: 1/2, step 6849/7134 completed (loss: 0.0961720272898674, acc: 0.9785714149475098)
[2025-02-13 20:14:08,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:09,090][root][INFO] - Training Epoch: 1/2, step 6850/7134 completed (loss: 0.1338805854320526, acc: 0.9523809552192688)
[2025-02-13 20:14:09,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:09,471][root][INFO] - Training Epoch: 1/2, step 6851/7134 completed (loss: 0.16800400614738464, acc: 0.950276255607605)
[2025-02-13 20:14:09,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:09,839][root][INFO] - Training Epoch: 1/2, step 6852/7134 completed (loss: 0.1738201379776001, acc: 0.9571428298950195)
[2025-02-13 20:14:09,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:10,227][root][INFO] - Training Epoch: 1/2, step 6853/7134 completed (loss: 0.1733105331659317, acc: 0.9479768872261047)
[2025-02-13 20:14:10,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:10,575][root][INFO] - Training Epoch: 1/2, step 6854/7134 completed (loss: 0.1104448139667511, acc: 0.9745762944221497)
[2025-02-13 20:14:10,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:10,940][root][INFO] - Training Epoch: 1/2, step 6855/7134 completed (loss: 0.044679511338472366, acc: 0.9906542301177979)
[2025-02-13 20:14:11,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:11,343][root][INFO] - Training Epoch: 1/2, step 6856/7134 completed (loss: 0.03072257712483406, acc: 0.9939758777618408)
[2025-02-13 20:14:11,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:11,744][root][INFO] - Training Epoch: 1/2, step 6857/7134 completed (loss: 0.031242744997143745, acc: 0.9940476417541504)
[2025-02-13 20:14:11,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:12,149][root][INFO] - Training Epoch: 1/2, step 6858/7134 completed (loss: 0.03882096707820892, acc: 0.9848484992980957)
[2025-02-13 20:14:12,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:12,560][root][INFO] - Training Epoch: 1/2, step 6859/7134 completed (loss: 0.03931907191872597, acc: 0.9887640476226807)
[2025-02-13 20:14:12,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:12,938][root][INFO] - Training Epoch: 1/2, step 6860/7134 completed (loss: 0.05935472249984741, acc: 0.9862068891525269)
[2025-02-13 20:14:13,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:13,300][root][INFO] - Training Epoch: 1/2, step 6861/7134 completed (loss: 0.0829387903213501, acc: 0.9790209531784058)
[2025-02-13 20:14:13,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:13,690][root][INFO] - Training Epoch: 1/2, step 6862/7134 completed (loss: 0.1365058273077011, acc: 0.9696969985961914)
[2025-02-13 20:14:13,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:14,114][root][INFO] - Training Epoch: 1/2, step 6863/7134 completed (loss: 0.06804364919662476, acc: 0.9882352948188782)
[2025-02-13 20:14:14,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:14,498][root][INFO] - Training Epoch: 1/2, step 6864/7134 completed (loss: 0.06528908014297485, acc: 0.978723406791687)
[2025-02-13 20:14:14,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:14,868][root][INFO] - Training Epoch: 1/2, step 6865/7134 completed (loss: 0.13755464553833008, acc: 0.9674796462059021)
[2025-02-13 20:14:14,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:15,226][root][INFO] - Training Epoch: 1/2, step 6866/7134 completed (loss: 0.03256634622812271, acc: 0.9934640526771545)
[2025-02-13 20:14:15,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:15,595][root][INFO] - Training Epoch: 1/2, step 6867/7134 completed (loss: 0.01113909762352705, acc: 1.0)
[2025-02-13 20:14:15,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:15,927][root][INFO] - Training Epoch: 1/2, step 6868/7134 completed (loss: 0.04346991330385208, acc: 0.987500011920929)
[2025-02-13 20:14:16,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:16,269][root][INFO] - Training Epoch: 1/2, step 6869/7134 completed (loss: 0.020263681188225746, acc: 1.0)
[2025-02-13 20:14:16,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:16,638][root][INFO] - Training Epoch: 1/2, step 6870/7134 completed (loss: 0.09885169565677643, acc: 0.9873417615890503)
[2025-02-13 20:14:16,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:17,008][root][INFO] - Training Epoch: 1/2, step 6871/7134 completed (loss: 0.03443550691008568, acc: 0.9920634627342224)
[2025-02-13 20:14:17,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:17,364][root][INFO] - Training Epoch: 1/2, step 6872/7134 completed (loss: 0.08606012910604477, acc: 0.9943181872367859)
[2025-02-13 20:14:17,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:17,736][root][INFO] - Training Epoch: 1/2, step 6873/7134 completed (loss: 0.10718023777008057, acc: 0.9823529124259949)
[2025-02-13 20:14:17,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:18,111][root][INFO] - Training Epoch: 1/2, step 6874/7134 completed (loss: 0.04427697882056236, acc: 0.984375)
[2025-02-13 20:14:18,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:18,510][root][INFO] - Training Epoch: 1/2, step 6875/7134 completed (loss: 0.15053638815879822, acc: 0.9597315192222595)
[2025-02-13 20:14:18,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:18,875][root][INFO] - Training Epoch: 1/2, step 6876/7134 completed (loss: 0.05893857032060623, acc: 0.9770992398262024)
[2025-02-13 20:14:19,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:19,255][root][INFO] - Training Epoch: 1/2, step 6877/7134 completed (loss: 0.0645647868514061, acc: 0.9823529124259949)
[2025-02-13 20:14:19,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:19,627][root][INFO] - Training Epoch: 1/2, step 6878/7134 completed (loss: 0.16436666250228882, acc: 0.9542483687400818)
[2025-02-13 20:14:19,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:20,022][root][INFO] - Training Epoch: 1/2, step 6879/7134 completed (loss: 0.1592807173728943, acc: 0.9741379022598267)
[2025-02-13 20:14:20,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:20,427][root][INFO] - Training Epoch: 1/2, step 6880/7134 completed (loss: 0.04862276092171669, acc: 0.989847719669342)
[2025-02-13 20:14:20,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:20,806][root][INFO] - Training Epoch: 1/2, step 6881/7134 completed (loss: 0.1705673635005951, acc: 0.9680851101875305)
[2025-02-13 20:14:20,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:21,137][root][INFO] - Training Epoch: 1/2, step 6882/7134 completed (loss: 0.3086310923099518, acc: 0.8943662047386169)
[2025-02-13 20:14:21,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:21,493][root][INFO] - Training Epoch: 1/2, step 6883/7134 completed (loss: 0.09212426096200943, acc: 0.9632353186607361)
[2025-02-13 20:14:21,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:21,857][root][INFO] - Training Epoch: 1/2, step 6884/7134 completed (loss: 0.14014945924282074, acc: 0.9659090638160706)
[2025-02-13 20:14:21,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:22,201][root][INFO] - Training Epoch: 1/2, step 6885/7134 completed (loss: 0.06781511753797531, acc: 0.9719626307487488)
[2025-02-13 20:14:22,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:22,615][root][INFO] - Training Epoch: 1/2, step 6886/7134 completed (loss: 0.10996894538402557, acc: 0.9594594836235046)
[2025-02-13 20:14:22,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:22,975][root][INFO] - Training Epoch: 1/2, step 6887/7134 completed (loss: 0.19034183025360107, acc: 0.9304347634315491)
[2025-02-13 20:14:23,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:23,379][root][INFO] - Training Epoch: 1/2, step 6888/7134 completed (loss: 0.16441041231155396, acc: 0.9466666579246521)
[2025-02-13 20:14:23,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:23,777][root][INFO] - Training Epoch: 1/2, step 6889/7134 completed (loss: 0.10341436415910721, acc: 0.9803921580314636)
[2025-02-13 20:14:23,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:24,197][root][INFO] - Training Epoch: 1/2, step 6890/7134 completed (loss: 0.23179560899734497, acc: 0.9408602118492126)
[2025-02-13 20:14:24,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:24,601][root][INFO] - Training Epoch: 1/2, step 6891/7134 completed (loss: 0.12290425598621368, acc: 0.9629629850387573)
[2025-02-13 20:14:24,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:24,979][root][INFO] - Training Epoch: 1/2, step 6892/7134 completed (loss: 0.13512741029262543, acc: 0.9677419066429138)
[2025-02-13 20:14:25,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:25,345][root][INFO] - Training Epoch: 1/2, step 6893/7134 completed (loss: 0.17361965775489807, acc: 0.942307710647583)
[2025-02-13 20:14:25,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:25,719][root][INFO] - Training Epoch: 1/2, step 6894/7134 completed (loss: 0.1206262931227684, acc: 0.9791666865348816)
[2025-02-13 20:14:25,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:26,102][root][INFO] - Training Epoch: 1/2, step 6895/7134 completed (loss: 0.12571211159229279, acc: 0.9704433679580688)
[2025-02-13 20:14:26,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:26,445][root][INFO] - Training Epoch: 1/2, step 6896/7134 completed (loss: 0.20327381789684296, acc: 0.9479768872261047)
[2025-02-13 20:14:26,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:26,814][root][INFO] - Training Epoch: 1/2, step 6897/7134 completed (loss: 0.15963177382946014, acc: 0.9702970385551453)
[2025-02-13 20:14:26,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:27,184][root][INFO] - Training Epoch: 1/2, step 6898/7134 completed (loss: 0.134402796626091, acc: 0.9768785834312439)
[2025-02-13 20:14:27,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:27,614][root][INFO] - Training Epoch: 1/2, step 6899/7134 completed (loss: 0.13860084116458893, acc: 0.9627659320831299)
[2025-02-13 20:14:27,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:28,044][root][INFO] - Training Epoch: 1/2, step 6900/7134 completed (loss: 0.3930590748786926, acc: 0.9248554706573486)
[2025-02-13 20:14:28,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:28,432][root][INFO] - Training Epoch: 1/2, step 6901/7134 completed (loss: 0.1971861571073532, acc: 0.9567567706108093)
[2025-02-13 20:14:28,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:28,861][root][INFO] - Training Epoch: 1/2, step 6902/7134 completed (loss: 0.03280647099018097, acc: 0.9943820238113403)
[2025-02-13 20:14:29,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:29,282][root][INFO] - Training Epoch: 1/2, step 6903/7134 completed (loss: 0.08663684874773026, acc: 0.9756097793579102)
[2025-02-13 20:14:29,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:29,664][root][INFO] - Training Epoch: 1/2, step 6904/7134 completed (loss: 0.098810113966465, acc: 0.9740932583808899)
[2025-02-13 20:14:29,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:30,042][root][INFO] - Training Epoch: 1/2, step 6905/7134 completed (loss: 0.15477992594242096, acc: 0.9552238583564758)
[2025-02-13 20:14:30,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:30,422][root][INFO] - Training Epoch: 1/2, step 6906/7134 completed (loss: 0.18985776603221893, acc: 0.970588207244873)
[2025-02-13 20:14:30,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:30,799][root][INFO] - Training Epoch: 1/2, step 6907/7134 completed (loss: 0.12715768814086914, acc: 0.9740932583808899)
[2025-02-13 20:14:30,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:31,166][root][INFO] - Training Epoch: 1/2, step 6908/7134 completed (loss: 0.16390997171401978, acc: 0.9714285731315613)
[2025-02-13 20:14:31,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:31,545][root][INFO] - Training Epoch: 1/2, step 6909/7134 completed (loss: 0.14755721390247345, acc: 0.9672130942344666)
[2025-02-13 20:14:31,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:31,920][root][INFO] - Training Epoch: 1/2, step 6910/7134 completed (loss: 0.07364213466644287, acc: 0.9791666865348816)
[2025-02-13 20:14:32,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:32,301][root][INFO] - Training Epoch: 1/2, step 6911/7134 completed (loss: 0.3543236553668976, acc: 0.9337016344070435)
[2025-02-13 20:14:32,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:32,731][root][INFO] - Training Epoch: 1/2, step 6912/7134 completed (loss: 0.15422190725803375, acc: 0.9707317352294922)
[2025-02-13 20:14:32,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:33,160][root][INFO] - Training Epoch: 1/2, step 6913/7134 completed (loss: 0.16207218170166016, acc: 0.9710144996643066)
[2025-02-13 20:14:33,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:33,512][root][INFO] - Training Epoch: 1/2, step 6914/7134 completed (loss: 0.32621756196022034, acc: 0.9281045794487)
[2025-02-13 20:14:33,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:33,924][root][INFO] - Training Epoch: 1/2, step 6915/7134 completed (loss: 0.1790170818567276, acc: 0.9567567706108093)
[2025-02-13 20:14:34,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:34,296][root][INFO] - Training Epoch: 1/2, step 6916/7134 completed (loss: 0.19048915803432465, acc: 0.9512194991111755)
[2025-02-13 20:14:34,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:34,673][root][INFO] - Training Epoch: 1/2, step 6917/7134 completed (loss: 0.22633102536201477, acc: 0.9484536051750183)
[2025-02-13 20:14:34,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:35,018][root][INFO] - Training Epoch: 1/2, step 6918/7134 completed (loss: 0.12741345167160034, acc: 0.9742268323898315)
[2025-02-13 20:14:35,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:35,450][root][INFO] - Training Epoch: 1/2, step 6919/7134 completed (loss: 0.2099463939666748, acc: 0.9593908786773682)
[2025-02-13 20:14:35,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:35,870][root][INFO] - Training Epoch: 1/2, step 6920/7134 completed (loss: 0.06307796388864517, acc: 0.976047933101654)
[2025-02-13 20:14:36,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:36,252][root][INFO] - Training Epoch: 1/2, step 6921/7134 completed (loss: 0.15783622860908508, acc: 0.9558823704719543)
[2025-02-13 20:14:36,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:36,596][root][INFO] - Training Epoch: 1/2, step 6922/7134 completed (loss: 0.20788100361824036, acc: 0.9551281929016113)
[2025-02-13 20:14:36,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:36,954][root][INFO] - Training Epoch: 1/2, step 6923/7134 completed (loss: 0.1511783003807068, acc: 0.9561403393745422)
[2025-02-13 20:14:37,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:37,318][root][INFO] - Training Epoch: 1/2, step 6924/7134 completed (loss: 0.11646687239408493, acc: 0.9696969985961914)
[2025-02-13 20:14:37,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:37,718][root][INFO] - Training Epoch: 1/2, step 6925/7134 completed (loss: 0.18095941841602325, acc: 0.9529411792755127)
[2025-02-13 20:14:37,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:38,136][root][INFO] - Training Epoch: 1/2, step 6926/7134 completed (loss: 0.23530781269073486, acc: 0.9365079402923584)
[2025-02-13 20:14:38,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:38,531][root][INFO] - Training Epoch: 1/2, step 6927/7134 completed (loss: 0.15181830525398254, acc: 0.9702970385551453)
[2025-02-13 20:14:38,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:38,910][root][INFO] - Training Epoch: 1/2, step 6928/7134 completed (loss: 0.21014606952667236, acc: 0.9609375)
[2025-02-13 20:14:39,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:39,354][root][INFO] - Training Epoch: 1/2, step 6929/7134 completed (loss: 0.09657415747642517, acc: 0.9659090638160706)
[2025-02-13 20:14:39,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:39,731][root][INFO] - Training Epoch: 1/2, step 6930/7134 completed (loss: 0.1621045172214508, acc: 0.96875)
[2025-02-13 20:14:39,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:40,125][root][INFO] - Training Epoch: 1/2, step 6931/7134 completed (loss: 0.15980730950832367, acc: 0.9470899701118469)
[2025-02-13 20:14:40,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:40,470][root][INFO] - Training Epoch: 1/2, step 6932/7134 completed (loss: 0.20708906650543213, acc: 0.9520958065986633)
[2025-02-13 20:14:40,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:40,917][root][INFO] - Training Epoch: 1/2, step 6933/7134 completed (loss: 0.23703449964523315, acc: 0.946107804775238)
[2025-02-13 20:14:41,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:41,332][root][INFO] - Training Epoch: 1/2, step 6934/7134 completed (loss: 0.19852541387081146, acc: 0.9679487347602844)
[2025-02-13 20:14:41,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:41,730][root][INFO] - Training Epoch: 1/2, step 6935/7134 completed (loss: 0.13924385607242584, acc: 0.978723406791687)
[2025-02-13 20:14:41,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:42,130][root][INFO] - Training Epoch: 1/2, step 6936/7134 completed (loss: 0.10229835659265518, acc: 0.9784946441650391)
[2025-02-13 20:14:42,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:42,534][root][INFO] - Training Epoch: 1/2, step 6937/7134 completed (loss: 0.24219176173210144, acc: 0.9520547986030579)
[2025-02-13 20:14:42,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:42,960][root][INFO] - Training Epoch: 1/2, step 6938/7134 completed (loss: 0.547214925289154, acc: 0.8678160905838013)
[2025-02-13 20:14:43,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:43,370][root][INFO] - Training Epoch: 1/2, step 6939/7134 completed (loss: 0.2607412338256836, acc: 0.9345238208770752)
[2025-02-13 20:14:43,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:43,744][root][INFO] - Training Epoch: 1/2, step 6940/7134 completed (loss: 0.3659331500530243, acc: 0.9034482836723328)
[2025-02-13 20:14:43,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:44,129][root][INFO] - Training Epoch: 1/2, step 6941/7134 completed (loss: 0.07748708128929138, acc: 0.9852941036224365)
[2025-02-13 20:14:44,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:44,558][root][INFO] - Training Epoch: 1/2, step 6942/7134 completed (loss: 0.22553007304668427, acc: 0.9554139971733093)
[2025-02-13 20:14:44,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:44,941][root][INFO] - Training Epoch: 1/2, step 6943/7134 completed (loss: 0.15349435806274414, acc: 0.9292035102844238)
[2025-02-13 20:14:45,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:45,344][root][INFO] - Training Epoch: 1/2, step 6944/7134 completed (loss: 0.16239523887634277, acc: 0.9586206674575806)
[2025-02-13 20:14:45,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:45,675][root][INFO] - Training Epoch: 1/2, step 6945/7134 completed (loss: 0.2568158805370331, acc: 0.96875)
[2025-02-13 20:14:45,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:46,109][root][INFO] - Training Epoch: 1/2, step 6946/7134 completed (loss: 0.3537663221359253, acc: 0.9032257795333862)
[2025-02-13 20:14:46,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:46,469][root][INFO] - Training Epoch: 1/2, step 6947/7134 completed (loss: 0.23143377900123596, acc: 0.9557521939277649)
[2025-02-13 20:14:46,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:46,883][root][INFO] - Training Epoch: 1/2, step 6948/7134 completed (loss: 0.30702799558639526, acc: 0.9142857193946838)
[2025-02-13 20:14:47,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:47,253][root][INFO] - Training Epoch: 1/2, step 6949/7134 completed (loss: 0.22589850425720215, acc: 0.9217391014099121)
[2025-02-13 20:14:47,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:47,564][root][INFO] - Training Epoch: 1/2, step 6950/7134 completed (loss: 0.16290514171123505, acc: 0.9620253443717957)
[2025-02-13 20:14:47,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:48,005][root][INFO] - Training Epoch: 1/2, step 6951/7134 completed (loss: 0.2761479616165161, acc: 0.9097222089767456)
[2025-02-13 20:14:48,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:48,362][root][INFO] - Training Epoch: 1/2, step 6952/7134 completed (loss: 0.22058755159378052, acc: 0.9238095283508301)
[2025-02-13 20:14:48,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:48,722][root][INFO] - Training Epoch: 1/2, step 6953/7134 completed (loss: 0.1968141794204712, acc: 0.9473684430122375)
[2025-02-13 20:14:48,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:49,071][root][INFO] - Training Epoch: 1/2, step 6954/7134 completed (loss: 0.3695699870586395, acc: 0.9396551847457886)
[2025-02-13 20:14:49,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:49,432][root][INFO] - Training Epoch: 1/2, step 6955/7134 completed (loss: 0.13887837529182434, acc: 0.9375)
[2025-02-13 20:14:49,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:49,803][root][INFO] - Training Epoch: 1/2, step 6956/7134 completed (loss: 0.14155270159244537, acc: 0.9635036587715149)
[2025-02-13 20:14:49,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:50,164][root][INFO] - Training Epoch: 1/2, step 6957/7134 completed (loss: 0.1581997573375702, acc: 0.9599999785423279)
[2025-02-13 20:14:50,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:50,532][root][INFO] - Training Epoch: 1/2, step 6958/7134 completed (loss: 0.3618256449699402, acc: 0.9145299196243286)
[2025-02-13 20:14:50,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:50,895][root][INFO] - Training Epoch: 1/2, step 6959/7134 completed (loss: 0.39658859372138977, acc: 0.8990825414657593)
[2025-02-13 20:14:51,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:51,302][root][INFO] - Training Epoch: 1/2, step 6960/7134 completed (loss: 0.20998093485832214, acc: 0.9444444179534912)
[2025-02-13 20:14:51,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:51,712][root][INFO] - Training Epoch: 1/2, step 6961/7134 completed (loss: 0.13807718455791473, acc: 0.9658119678497314)
[2025-02-13 20:14:51,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:52,117][root][INFO] - Training Epoch: 1/2, step 6962/7134 completed (loss: 0.17236174643039703, acc: 0.9492753744125366)
[2025-02-13 20:14:52,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:52,521][root][INFO] - Training Epoch: 1/2, step 6963/7134 completed (loss: 0.14445513486862183, acc: 0.9668874144554138)
[2025-02-13 20:14:52,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:52,929][root][INFO] - Training Epoch: 1/2, step 6964/7134 completed (loss: 0.05541643872857094, acc: 0.9848484992980957)
[2025-02-13 20:14:53,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:53,368][root][INFO] - Training Epoch: 1/2, step 6965/7134 completed (loss: 0.036121778190135956, acc: 1.0)
[2025-02-13 20:14:53,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:53,783][root][INFO] - Training Epoch: 1/2, step 6966/7134 completed (loss: 0.13008421659469604, acc: 0.9523809552192688)
[2025-02-13 20:14:53,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:54,159][root][INFO] - Training Epoch: 1/2, step 6967/7134 completed (loss: 0.05772556737065315, acc: 0.9863945841789246)
[2025-02-13 20:14:54,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:54,550][root][INFO] - Training Epoch: 1/2, step 6968/7134 completed (loss: 0.11628981679677963, acc: 0.9798657894134521)
[2025-02-13 20:14:54,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:54,936][root][INFO] - Training Epoch: 1/2, step 6969/7134 completed (loss: 0.19637219607830048, acc: 0.9516128897666931)
[2025-02-13 20:14:55,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:55,315][root][INFO] - Training Epoch: 1/2, step 6970/7134 completed (loss: 0.31363552808761597, acc: 0.9396551847457886)
[2025-02-13 20:14:55,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:55,723][root][INFO] - Training Epoch: 1/2, step 6971/7134 completed (loss: 0.24065852165222168, acc: 0.9444444179534912)
[2025-02-13 20:14:55,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:56,159][root][INFO] - Training Epoch: 1/2, step 6972/7134 completed (loss: 0.2089245468378067, acc: 0.961240291595459)
[2025-02-13 20:14:56,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:56,531][root][INFO] - Training Epoch: 1/2, step 6973/7134 completed (loss: 0.25727203488349915, acc: 0.9741379022598267)
[2025-02-13 20:14:56,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:56,902][root][INFO] - Training Epoch: 1/2, step 6974/7134 completed (loss: 0.23702044785022736, acc: 0.9411764740943909)
[2025-02-13 20:14:57,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:57,284][root][INFO] - Training Epoch: 1/2, step 6975/7134 completed (loss: 0.23371803760528564, acc: 0.9658119678497314)
[2025-02-13 20:14:57,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:57,664][root][INFO] - Training Epoch: 1/2, step 6976/7134 completed (loss: 0.18381650745868683, acc: 0.9571428298950195)
[2025-02-13 20:14:57,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:58,034][root][INFO] - Training Epoch: 1/2, step 6977/7134 completed (loss: 0.15539854764938354, acc: 0.9389312863349915)
[2025-02-13 20:14:58,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:58,410][root][INFO] - Training Epoch: 1/2, step 6978/7134 completed (loss: 0.12632080912590027, acc: 0.9683544039726257)
[2025-02-13 20:14:58,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:58,813][root][INFO] - Training Epoch: 1/2, step 6979/7134 completed (loss: 0.23625223338603973, acc: 0.9271523356437683)
[2025-02-13 20:14:58,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:59,198][root][INFO] - Training Epoch: 1/2, step 6980/7134 completed (loss: 0.4634791314601898, acc: 0.8757061958312988)
[2025-02-13 20:14:59,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:59,586][root][INFO] - Training Epoch: 1/2, step 6981/7134 completed (loss: 0.3049238622188568, acc: 0.907216489315033)
[2025-02-13 20:14:59,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:59,982][root][INFO] - Training Epoch: 1/2, step 6982/7134 completed (loss: 0.32545095682144165, acc: 0.926701545715332)
[2025-02-13 20:15:00,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:00,387][root][INFO] - Training Epoch: 1/2, step 6983/7134 completed (loss: 0.40884697437286377, acc: 0.8846153616905212)
[2025-02-13 20:15:00,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:00,792][root][INFO] - Training Epoch: 1/2, step 6984/7134 completed (loss: 0.2072332501411438, acc: 0.9488372206687927)
[2025-02-13 20:15:00,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:01,190][root][INFO] - Training Epoch: 1/2, step 6985/7134 completed (loss: 0.4122457206249237, acc: 0.9141631126403809)
[2025-02-13 20:15:01,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:01,571][root][INFO] - Training Epoch: 1/2, step 6986/7134 completed (loss: 0.23269179463386536, acc: 0.9197530746459961)
[2025-02-13 20:15:01,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:02,008][root][INFO] - Training Epoch: 1/2, step 6987/7134 completed (loss: 0.20546916127204895, acc: 0.9462810158729553)
[2025-02-13 20:15:02,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:02,413][root][INFO] - Training Epoch: 1/2, step 6988/7134 completed (loss: 0.10694966465234756, acc: 0.9842519760131836)
[2025-02-13 20:15:02,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:02,843][root][INFO] - Training Epoch: 1/2, step 6989/7134 completed (loss: 0.3354046940803528, acc: 0.9189189076423645)
[2025-02-13 20:15:02,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:03,207][root][INFO] - Training Epoch: 1/2, step 6990/7134 completed (loss: 0.05059901252388954, acc: 0.9907407164573669)
[2025-02-13 20:15:03,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:03,578][root][INFO] - Training Epoch: 1/2, step 6991/7134 completed (loss: 0.4295251965522766, acc: 0.9207921028137207)
[2025-02-13 20:15:03,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:03,961][root][INFO] - Training Epoch: 1/2, step 6992/7134 completed (loss: 0.18683414161205292, acc: 0.9369369149208069)
[2025-02-13 20:15:04,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:04,332][root][INFO] - Training Epoch: 1/2, step 6993/7134 completed (loss: 0.26681992411613464, acc: 0.9426751732826233)
[2025-02-13 20:15:04,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:04,715][root][INFO] - Training Epoch: 1/2, step 6994/7134 completed (loss: 0.484354704618454, acc: 0.9066666960716248)
[2025-02-13 20:15:04,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:05,088][root][INFO] - Training Epoch: 1/2, step 6995/7134 completed (loss: 0.1451362818479538, acc: 0.9418604373931885)
[2025-02-13 20:15:05,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:05,447][root][INFO] - Training Epoch: 1/2, step 6996/7134 completed (loss: 0.16931550204753876, acc: 0.931506872177124)
[2025-02-13 20:15:05,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:05,808][root][INFO] - Training Epoch: 1/2, step 6997/7134 completed (loss: 0.1454371064901352, acc: 0.9612902998924255)
[2025-02-13 20:15:05,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:06,135][root][INFO] - Training Epoch: 1/2, step 6998/7134 completed (loss: 0.3339456617832184, acc: 0.8947368264198303)
[2025-02-13 20:15:06,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:06,493][root][INFO] - Training Epoch: 1/2, step 6999/7134 completed (loss: 0.1022295132279396, acc: 0.9491525292396545)
[2025-02-13 20:15:06,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:06,828][root][INFO] - Training Epoch: 1/2, step 7000/7134 completed (loss: 0.10930456966161728, acc: 0.9636363387107849)
[2025-02-13 20:15:06,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:07,191][root][INFO] - Training Epoch: 1/2, step 7001/7134 completed (loss: 0.3109344244003296, acc: 0.9740259647369385)
[2025-02-13 20:15:07,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:07,625][root][INFO] - Training Epoch: 1/2, step 7002/7134 completed (loss: 0.12661141157150269, acc: 0.9663865566253662)
[2025-02-13 20:15:07,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:07,990][root][INFO] - Training Epoch: 1/2, step 7003/7134 completed (loss: 0.048305802047252655, acc: 0.9792746305465698)
[2025-02-13 20:15:08,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:08,366][root][INFO] - Training Epoch: 1/2, step 7004/7134 completed (loss: 0.15383775532245636, acc: 0.9634146094322205)
[2025-02-13 20:15:08,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:08,760][root][INFO] - Training Epoch: 1/2, step 7005/7134 completed (loss: 0.1453232318162918, acc: 0.9698795080184937)
[2025-02-13 20:15:08,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:09,192][root][INFO] - Training Epoch: 1/2, step 7006/7134 completed (loss: 0.08623483777046204, acc: 0.9813664555549622)
[2025-02-13 20:15:09,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:09,571][root][INFO] - Training Epoch: 1/2, step 7007/7134 completed (loss: 0.16738393902778625, acc: 0.9548386931419373)
[2025-02-13 20:15:09,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:09,959][root][INFO] - Training Epoch: 1/2, step 7008/7134 completed (loss: 0.07790037244558334, acc: 0.9834254384040833)
[2025-02-13 20:15:10,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:10,341][root][INFO] - Training Epoch: 1/2, step 7009/7134 completed (loss: 0.2672492265701294, acc: 0.9375)
[2025-02-13 20:15:10,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:10,743][root][INFO] - Training Epoch: 1/2, step 7010/7134 completed (loss: 0.07033095508813858, acc: 0.9870967864990234)
[2025-02-13 20:15:10,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:11,140][root][INFO] - Training Epoch: 1/2, step 7011/7134 completed (loss: 0.20941750705242157, acc: 0.9476439952850342)
[2025-02-13 20:15:11,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:11,492][root][INFO] - Training Epoch: 1/2, step 7012/7134 completed (loss: 0.07751496136188507, acc: 0.9879518151283264)
[2025-02-13 20:15:11,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:11,872][root][INFO] - Training Epoch: 1/2, step 7013/7134 completed (loss: 0.0731060728430748, acc: 0.987261176109314)
[2025-02-13 20:15:12,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:12,307][root][INFO] - Training Epoch: 1/2, step 7014/7134 completed (loss: 0.16580718755722046, acc: 0.9594594836235046)
[2025-02-13 20:15:12,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:12,716][root][INFO] - Training Epoch: 1/2, step 7015/7134 completed (loss: 0.15295861661434174, acc: 0.9776119589805603)
[2025-02-13 20:15:12,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:13,183][root][INFO] - Training Epoch: 1/2, step 7016/7134 completed (loss: 0.22409889101982117, acc: 0.9487179517745972)
[2025-02-13 20:15:13,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:13,602][root][INFO] - Training Epoch: 1/2, step 7017/7134 completed (loss: 0.2741575241088867, acc: 0.9380530714988708)
[2025-02-13 20:15:13,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:13,985][root][INFO] - Training Epoch: 1/2, step 7018/7134 completed (loss: 0.29305294156074524, acc: 0.9567901492118835)
[2025-02-13 20:15:14,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:14,342][root][INFO] - Training Epoch: 1/2, step 7019/7134 completed (loss: 0.35590025782585144, acc: 0.9430379867553711)
[2025-02-13 20:15:14,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:14,711][root][INFO] - Training Epoch: 1/2, step 7020/7134 completed (loss: 0.05357876792550087, acc: 0.9879518151283264)
[2025-02-13 20:15:14,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:15,153][root][INFO] - Training Epoch: 1/2, step 7021/7134 completed (loss: 0.14298410713672638, acc: 0.9555555582046509)
[2025-02-13 20:15:15,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:15,532][root][INFO] - Training Epoch: 1/2, step 7022/7134 completed (loss: 0.3746229112148285, acc: 0.9027777910232544)
[2025-02-13 20:15:15,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:15,915][root][INFO] - Training Epoch: 1/2, step 7023/7134 completed (loss: 0.3610285818576813, acc: 0.891566276550293)
[2025-02-13 20:15:16,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:16,272][root][INFO] - Training Epoch: 1/2, step 7024/7134 completed (loss: 0.21222475171089172, acc: 0.95652174949646)
[2025-02-13 20:15:16,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:16,705][root][INFO] - Training Epoch: 1/2, step 7025/7134 completed (loss: 0.31959283351898193, acc: 0.9047619104385376)
[2025-02-13 20:15:16,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:17,147][root][INFO] - Training Epoch: 1/2, step 7026/7134 completed (loss: 0.3959607183933258, acc: 0.90625)
[2025-02-13 20:15:17,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:17,565][root][INFO] - Training Epoch: 1/2, step 7027/7134 completed (loss: 0.3245582580566406, acc: 0.9154929518699646)
[2025-02-13 20:15:17,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:17,964][root][INFO] - Training Epoch: 1/2, step 7028/7134 completed (loss: 0.13399869203567505, acc: 0.9670329689979553)
[2025-02-13 20:15:18,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:18,407][root][INFO] - Training Epoch: 1/2, step 7029/7134 completed (loss: 0.12502175569534302, acc: 0.966292142868042)
[2025-02-13 20:15:18,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:18,826][root][INFO] - Training Epoch: 1/2, step 7030/7134 completed (loss: 0.2992580533027649, acc: 0.9130434989929199)
[2025-02-13 20:15:18,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:19,244][root][INFO] - Training Epoch: 1/2, step 7031/7134 completed (loss: 0.28010621666908264, acc: 0.9382715821266174)
[2025-02-13 20:15:19,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:19,662][root][INFO] - Training Epoch: 1/2, step 7032/7134 completed (loss: 0.33420124650001526, acc: 0.9117646813392639)
[2025-02-13 20:15:19,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:20,041][root][INFO] - Training Epoch: 1/2, step 7033/7134 completed (loss: 0.4510413706302643, acc: 0.8765432238578796)
[2025-02-13 20:15:20,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:20,400][root][INFO] - Training Epoch: 1/2, step 7034/7134 completed (loss: 0.364717572927475, acc: 0.9375)
[2025-02-13 20:15:20,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:20,811][root][INFO] - Training Epoch: 1/2, step 7035/7134 completed (loss: 0.35336828231811523, acc: 0.920634925365448)
[2025-02-13 20:15:20,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:21,193][root][INFO] - Training Epoch: 1/2, step 7036/7134 completed (loss: 0.34941717982292175, acc: 0.8888888955116272)
[2025-02-13 20:15:21,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:21,567][root][INFO] - Training Epoch: 1/2, step 7037/7134 completed (loss: 0.15025436878204346, acc: 0.9701492786407471)
[2025-02-13 20:15:21,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:21,962][root][INFO] - Training Epoch: 1/2, step 7038/7134 completed (loss: 0.7512034177780151, acc: 0.8734177350997925)
[2025-02-13 20:15:22,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:22,322][root][INFO] - Training Epoch: 1/2, step 7039/7134 completed (loss: 0.19397051632404327, acc: 0.9555555582046509)
[2025-02-13 20:15:22,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:22,733][root][INFO] - Training Epoch: 1/2, step 7040/7134 completed (loss: 0.37579962611198425, acc: 0.9200000166893005)
[2025-02-13 20:15:22,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:23,146][root][INFO] - Training Epoch: 1/2, step 7041/7134 completed (loss: 0.3973511755466461, acc: 0.887499988079071)
[2025-02-13 20:15:23,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:23,521][root][INFO] - Training Epoch: 1/2, step 7042/7134 completed (loss: 0.30695345997810364, acc: 0.9436619877815247)
[2025-02-13 20:15:23,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:23,881][root][INFO] - Training Epoch: 1/2, step 7043/7134 completed (loss: 0.46718811988830566, acc: 0.8533333539962769)
[2025-02-13 20:15:24,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:24,277][root][INFO] - Training Epoch: 1/2, step 7044/7134 completed (loss: 0.30323636531829834, acc: 0.9154929518699646)
[2025-02-13 20:15:24,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:24,707][root][INFO] - Training Epoch: 1/2, step 7045/7134 completed (loss: 0.23808766901493073, acc: 0.9448275566101074)
[2025-02-13 20:15:24,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:25,113][root][INFO] - Training Epoch: 1/2, step 7046/7134 completed (loss: 0.40480470657348633, acc: 0.916167676448822)
[2025-02-13 20:15:25,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:25,552][root][INFO] - Training Epoch: 1/2, step 7047/7134 completed (loss: 0.32737547159194946, acc: 0.9248826503753662)
[2025-02-13 20:15:25,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:25,935][root][INFO] - Training Epoch: 1/2, step 7048/7134 completed (loss: 0.04067762568593025, acc: 0.9948453903198242)
[2025-02-13 20:15:26,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:26,287][root][INFO] - Training Epoch: 1/2, step 7049/7134 completed (loss: 0.2327069789171219, acc: 0.9595375657081604)
[2025-02-13 20:15:26,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:26,645][root][INFO] - Training Epoch: 1/2, step 7050/7134 completed (loss: 0.07987676560878754, acc: 0.9828571677207947)
[2025-02-13 20:15:26,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:27,013][root][INFO] - Training Epoch: 1/2, step 7051/7134 completed (loss: 0.08896379172801971, acc: 0.9728260636329651)
[2025-02-13 20:15:27,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:27,383][root][INFO] - Training Epoch: 1/2, step 7052/7134 completed (loss: 0.09697645157575607, acc: 0.9768785834312439)
[2025-02-13 20:15:27,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:27,746][root][INFO] - Training Epoch: 1/2, step 7053/7134 completed (loss: 0.13305017352104187, acc: 0.9672130942344666)
[2025-02-13 20:15:27,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:28,139][root][INFO] - Training Epoch: 1/2, step 7054/7134 completed (loss: 0.2286531776189804, acc: 0.9548386931419373)
[2025-02-13 20:15:28,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:28,532][root][INFO] - Training Epoch: 1/2, step 7055/7134 completed (loss: 0.19510500133037567, acc: 0.9560439586639404)
[2025-02-13 20:15:28,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:28,916][root][INFO] - Training Epoch: 1/2, step 7056/7134 completed (loss: 0.18383005261421204, acc: 0.9611111283302307)
[2025-02-13 20:15:29,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:29,279][root][INFO] - Training Epoch: 1/2, step 7057/7134 completed (loss: 0.5876258015632629, acc: 0.9171270728111267)
[2025-02-13 20:15:29,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:29,658][root][INFO] - Training Epoch: 1/2, step 7058/7134 completed (loss: 0.26071324944496155, acc: 0.930232584476471)
[2025-02-13 20:15:29,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:30,041][root][INFO] - Training Epoch: 1/2, step 7059/7134 completed (loss: 0.1389109492301941, acc: 0.9615384340286255)
[2025-02-13 20:15:30,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:30,398][root][INFO] - Training Epoch: 1/2, step 7060/7134 completed (loss: 0.2653666138648987, acc: 0.9436619877815247)
[2025-02-13 20:15:30,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:30,777][root][INFO] - Training Epoch: 1/2, step 7061/7134 completed (loss: 0.21295678615570068, acc: 0.9354838728904724)
[2025-02-13 20:15:30,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:31,132][root][INFO] - Training Epoch: 1/2, step 7062/7134 completed (loss: 0.17999637126922607, acc: 0.9491525292396545)
[2025-02-13 20:15:31,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:31,553][root][INFO] - Training Epoch: 1/2, step 7063/7134 completed (loss: 0.3767143189907074, acc: 0.929729700088501)
[2025-02-13 20:15:31,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:31,938][root][INFO] - Training Epoch: 1/2, step 7064/7134 completed (loss: 0.21656575798988342, acc: 0.9578313231468201)
[2025-02-13 20:15:32,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:32,352][root][INFO] - Training Epoch: 1/2, step 7065/7134 completed (loss: 0.11945406347513199, acc: 0.9848484992980957)
[2025-02-13 20:15:32,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:32,727][root][INFO] - Training Epoch: 1/2, step 7066/7134 completed (loss: 0.17548593878746033, acc: 0.9528796076774597)
[2025-02-13 20:15:32,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:33,117][root][INFO] - Training Epoch: 1/2, step 7067/7134 completed (loss: 0.18190410733222961, acc: 0.9576719403266907)
[2025-02-13 20:15:33,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:33,487][root][INFO] - Training Epoch: 1/2, step 7068/7134 completed (loss: 0.0433591790497303, acc: 0.995121955871582)
[2025-02-13 20:15:33,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:33,864][root][INFO] - Training Epoch: 1/2, step 7069/7134 completed (loss: 0.11255253851413727, acc: 0.9870129823684692)
[2025-02-13 20:15:34,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:34,209][root][INFO] - Training Epoch: 1/2, step 7070/7134 completed (loss: 0.10194813460111618, acc: 0.9710982441902161)
[2025-02-13 20:15:34,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:34,575][root][INFO] - Training Epoch: 1/2, step 7071/7134 completed (loss: 0.1317615807056427, acc: 0.9747899174690247)
[2025-02-13 20:15:34,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:34,978][root][INFO] - Training Epoch: 1/2, step 7072/7134 completed (loss: 0.1042301207780838, acc: 0.9595959782600403)
[2025-02-13 20:15:35,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:35,367][root][INFO] - Training Epoch: 1/2, step 7073/7134 completed (loss: 0.13965262472629547, acc: 0.9597989916801453)
[2025-02-13 20:15:35,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:35,737][root][INFO] - Training Epoch: 1/2, step 7074/7134 completed (loss: 0.43011683225631714, acc: 0.8943089246749878)
[2025-02-13 20:15:35,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:36,132][root][INFO] - Training Epoch: 1/2, step 7075/7134 completed (loss: 0.26933568716049194, acc: 0.9363057613372803)
[2025-02-13 20:15:36,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:36,483][root][INFO] - Training Epoch: 1/2, step 7076/7134 completed (loss: 0.33750659227371216, acc: 0.9213483333587646)
[2025-02-13 20:15:36,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:36,832][root][INFO] - Training Epoch: 1/2, step 7077/7134 completed (loss: 0.3687364161014557, acc: 0.9109588861465454)
[2025-02-13 20:15:36,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:37,189][root][INFO] - Training Epoch: 1/2, step 7078/7134 completed (loss: 0.1882932335138321, acc: 0.9724137783050537)
[2025-02-13 20:15:37,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:37,611][root][INFO] - Training Epoch: 1/2, step 7079/7134 completed (loss: 0.12635751068592072, acc: 0.9605262875556946)
[2025-02-13 20:15:37,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:37,974][root][INFO] - Training Epoch: 1/2, step 7080/7134 completed (loss: 0.24069911241531372, acc: 0.9319728016853333)
[2025-02-13 20:15:38,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:38,353][root][INFO] - Training Epoch: 1/2, step 7081/7134 completed (loss: 0.1313670426607132, acc: 0.9595375657081604)
[2025-02-13 20:15:38,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:38,724][root][INFO] - Training Epoch: 1/2, step 7082/7134 completed (loss: 0.3218110501766205, acc: 0.9276315569877625)
[2025-02-13 20:15:38,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:39,095][root][INFO] - Training Epoch: 1/2, step 7083/7134 completed (loss: 0.09031042456626892, acc: 0.9716312289237976)
[2025-02-13 20:15:39,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:39,451][root][INFO] - Training Epoch: 1/2, step 7084/7134 completed (loss: 0.14825038611888885, acc: 0.954023003578186)
[2025-02-13 20:15:39,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:39,810][root][INFO] - Training Epoch: 1/2, step 7085/7134 completed (loss: 0.2287401258945465, acc: 0.9466666579246521)
[2025-02-13 20:15:39,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:40,195][root][INFO] - Training Epoch: 1/2, step 7086/7134 completed (loss: 0.12732180953025818, acc: 0.9741935729980469)
[2025-02-13 20:15:40,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:40,613][root][INFO] - Training Epoch: 1/2, step 7087/7134 completed (loss: 0.19883906841278076, acc: 0.9534883499145508)
[2025-02-13 20:15:40,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:40,983][root][INFO] - Training Epoch: 1/2, step 7088/7134 completed (loss: 0.21013565361499786, acc: 0.9503546357154846)
[2025-02-13 20:15:41,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:41,387][root][INFO] - Training Epoch: 1/2, step 7089/7134 completed (loss: 0.2486906796693802, acc: 0.9419354796409607)
[2025-02-13 20:15:41,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:41,778][root][INFO] - Training Epoch: 1/2, step 7090/7134 completed (loss: 0.15504728257656097, acc: 0.9702380895614624)
[2025-02-13 20:15:41,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:42,189][root][INFO] - Training Epoch: 1/2, step 7091/7134 completed (loss: 0.1142946183681488, acc: 0.9575757384300232)
[2025-02-13 20:15:42,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:42,568][root][INFO] - Training Epoch: 1/2, step 7092/7134 completed (loss: 0.17055000364780426, acc: 0.9548872113227844)
[2025-02-13 20:15:42,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:42,922][root][INFO] - Training Epoch: 1/2, step 7093/7134 completed (loss: 0.26846811175346375, acc: 0.8888888955116272)
[2025-02-13 20:15:43,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:43,270][root][INFO] - Training Epoch: 1/2, step 7094/7134 completed (loss: 0.12332862615585327, acc: 0.9791666865348816)
[2025-02-13 20:15:43,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:43,611][root][INFO] - Training Epoch: 1/2, step 7095/7134 completed (loss: 0.18938478827476501, acc: 0.9406779408454895)
[2025-02-13 20:15:43,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:43,963][root][INFO] - Training Epoch: 1/2, step 7096/7134 completed (loss: 0.15372344851493835, acc: 0.9647887349128723)
[2025-02-13 20:15:44,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:44,324][root][INFO] - Training Epoch: 1/2, step 7097/7134 completed (loss: 0.24764786660671234, acc: 0.9166666865348816)
[2025-02-13 20:15:44,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:44,693][root][INFO] - Training Epoch: 1/2, step 7098/7134 completed (loss: 0.09038130193948746, acc: 0.9790209531784058)
[2025-02-13 20:15:44,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:45,063][root][INFO] - Training Epoch: 1/2, step 7099/7134 completed (loss: 0.2245844006538391, acc: 0.9347826242446899)
[2025-02-13 20:15:45,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:45,434][root][INFO] - Training Epoch: 1/2, step 7100/7134 completed (loss: 0.186005637049675, acc: 0.9520958065986633)
[2025-02-13 20:15:45,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:45,789][root][INFO] - Training Epoch: 1/2, step 7101/7134 completed (loss: 0.08961964398622513, acc: 0.9856114983558655)
[2025-02-13 20:15:45,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:46,161][root][INFO] - Training Epoch: 1/2, step 7102/7134 completed (loss: 0.16599304974079132, acc: 0.9576271176338196)
[2025-02-13 20:15:46,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:46,516][root][INFO] - Training Epoch: 1/2, step 7103/7134 completed (loss: 0.13297554850578308, acc: 0.96875)
[2025-02-13 20:15:46,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:46,879][root][INFO] - Training Epoch: 1/2, step 7104/7134 completed (loss: 0.10843916237354279, acc: 0.9649122953414917)
[2025-02-13 20:15:47,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:47,246][root][INFO] - Training Epoch: 1/2, step 7105/7134 completed (loss: 0.10548266768455505, acc: 0.9791666865348816)
[2025-02-13 20:15:47,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:47,623][root][INFO] - Training Epoch: 1/2, step 7106/7134 completed (loss: 0.05004303902387619, acc: 0.9851852059364319)
[2025-02-13 20:15:47,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:48,036][root][INFO] - Training Epoch: 1/2, step 7107/7134 completed (loss: 0.10854891687631607, acc: 0.9689440727233887)
[2025-02-13 20:15:48,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:48,412][root][INFO] - Training Epoch: 1/2, step 7108/7134 completed (loss: 0.13667058944702148, acc: 0.9635036587715149)
[2025-02-13 20:15:48,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:48,816][root][INFO] - Training Epoch: 1/2, step 7109/7134 completed (loss: 0.27505314350128174, acc: 0.9037036895751953)
[2025-02-13 20:15:48,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:49,229][root][INFO] - Training Epoch: 1/2, step 7110/7134 completed (loss: 0.05604492127895355, acc: 0.9924812316894531)
[2025-02-13 20:15:49,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:49,578][root][INFO] - Training Epoch: 1/2, step 7111/7134 completed (loss: 0.21579761803150177, acc: 0.9351851940155029)
[2025-02-13 20:15:49,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:49,978][root][INFO] - Training Epoch: 1/2, step 7112/7134 completed (loss: 0.16258199512958527, acc: 0.954954981803894)
[2025-02-13 20:15:50,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:50,362][root][INFO] - Training Epoch: 1/2, step 7113/7134 completed (loss: 0.3016678988933563, acc: 0.9299362897872925)
[2025-02-13 20:15:50,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:50,735][root][INFO] - Training Epoch: 1/2, step 7114/7134 completed (loss: 0.1667231321334839, acc: 0.9541284441947937)
[2025-02-13 20:15:50,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:51,103][root][INFO] - Training Epoch: 1/2, step 7115/7134 completed (loss: 0.23390521109104156, acc: 0.9160305261611938)
[2025-02-13 20:15:51,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:51,469][root][INFO] - Training Epoch: 1/2, step 7116/7134 completed (loss: 0.34358254075050354, acc: 0.9117646813392639)
[2025-02-13 20:15:51,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:51,825][root][INFO] - Training Epoch: 1/2, step 7117/7134 completed (loss: 0.2553677260875702, acc: 0.9448819160461426)
[2025-02-13 20:15:51,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:52,169][root][INFO] - Training Epoch: 1/2, step 7118/7134 completed (loss: 0.16748853027820587, acc: 0.9642857313156128)
[2025-02-13 20:15:52,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:52,549][root][INFO] - Training Epoch: 1/2, step 7119/7134 completed (loss: 0.1332518756389618, acc: 0.9586206674575806)
[2025-02-13 20:15:52,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:52,888][root][INFO] - Training Epoch: 1/2, step 7120/7134 completed (loss: 0.18437954783439636, acc: 0.9514563083648682)
[2025-02-13 20:15:53,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:53,255][root][INFO] - Training Epoch: 1/2, step 7121/7134 completed (loss: 0.0345289520919323, acc: 1.0)
[2025-02-13 20:15:53,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:53,634][root][INFO] - Training Epoch: 1/2, step 7122/7134 completed (loss: 0.17728954553604126, acc: 0.9569892287254333)
[2025-02-13 20:15:53,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:54,027][root][INFO] - Training Epoch: 1/2, step 7123/7134 completed (loss: 0.11641890555620193, acc: 0.9595959782600403)
[2025-02-13 20:15:54,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:54,438][root][INFO] - Training Epoch: 1/2, step 7124/7134 completed (loss: 0.16556371748447418, acc: 0.9672130942344666)
[2025-02-13 20:15:54,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:54,824][root][INFO] - Training Epoch: 1/2, step 7125/7134 completed (loss: 0.08133949339389801, acc: 0.9694656729698181)
[2025-02-13 20:15:54,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:55,209][root][INFO] - Training Epoch: 1/2, step 7126/7134 completed (loss: 0.31359240412712097, acc: 0.9051094651222229)
[2025-02-13 20:15:55,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:55,628][root][INFO] - Training Epoch: 1/2, step 7127/7134 completed (loss: 0.1431387960910797, acc: 0.9506173133850098)
[2025-02-13 20:15:55,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:56,048][root][INFO] - Training Epoch: 1/2, step 7128/7134 completed (loss: 0.1460019201040268, acc: 0.9589040875434875)
[2025-02-13 20:15:56,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:56,421][root][INFO] - Training Epoch: 1/2, step 7129/7134 completed (loss: 0.49487948417663574, acc: 0.8682170510292053)
[2025-02-13 20:15:56,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:56,801][root][INFO] - Training Epoch: 1/2, step 7130/7134 completed (loss: 0.8376147150993347, acc: 0.8199999928474426)
[2025-02-13 20:15:56,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:57,160][root][INFO] - Training Epoch: 1/2, step 7131/7134 completed (loss: 0.22695739567279816, acc: 0.9433962106704712)
[2025-02-13 20:15:58,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:58,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:58,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:00,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:00,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:02,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:02,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:02,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:03,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:03,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:03,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:04,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:04,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:04,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:05,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:05,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:05,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:06,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:06,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:06,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:07,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:07,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:08,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:08,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:08,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:09,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:09,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:09,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:09,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:10,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:10,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:10,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:11,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:11,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:11,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:11,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:12,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:12,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:13,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:13,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:13,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:14,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:14,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:14,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:15,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:15,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:15,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:17,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:17,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:17,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:18,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:18,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:19,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:19,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:19,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:19,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:20,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:20,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:20,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:21,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:21,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:21,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:22,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:22,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:23,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:23,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:23,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:24,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:24,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:24,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:25,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:25,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:25,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:26,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:26,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:26,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:28,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:28,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:28,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:29,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:29,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:29,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:31,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:31,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:31,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:32,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:32,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:32,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:33,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:33,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:33,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:34,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:34,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:34,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:35,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:35,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:35,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:36,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:36,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:36,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:37,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:37,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:38,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:38,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:38,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:39,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:39,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:39,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:40,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:40,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:40,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:41,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:41,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:41,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:42,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:42,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:42,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:43,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:43,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:43,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:43,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:44,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:44,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:44,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:45,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:45,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:45,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:46,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:46,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:46,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:47,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:47,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:47,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:48,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:48,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:48,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:49,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:49,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:49,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:51,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:51,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:51,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:52,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:52,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:53,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:53,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:53,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:54,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:54,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:54,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:57,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:57,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:58,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:58,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:58,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:59,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:59,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:59,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:00,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:00,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:00,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:01,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:01,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:01,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:02,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:02,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:02,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:03,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:03,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:03,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:04,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:04,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:04,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:05,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:05,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:05,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:06,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:06,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:06,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:07,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:07,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:07,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:08,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:08,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:08,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:08,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:09,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:09,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:09,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:10,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:10,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:10,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:11,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:11,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:11,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:12,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:12,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:12,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:13,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:13,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:13,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:14,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:14,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:14,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:15,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:15,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:15,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:16,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:16,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:16,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:17,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:17,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:17,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:18,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:18,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:20,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:20,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:20,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:21,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:21,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:21,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:22,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:22,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:23,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:23,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:23,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:24,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:24,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:24,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:25,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:25,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:25,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:26,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:26,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:26,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:27,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:27,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:27,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:28,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:28,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:28,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:29,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:29,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:30,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:30,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:30,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:31,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:31,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:31,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:32,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:32,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:32,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:33,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:33,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:34,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:34,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:34,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:35,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:35,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:35,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:36,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:36,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:36,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:37,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:37,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:38,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:38,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:38,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:39,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:39,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:39,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:39,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:40,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:40,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:40,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:41,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:41,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:41,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:42,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:42,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:43,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:43,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:44,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:44,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:44,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:45,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:45,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:45,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:46,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:46,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:47,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:47,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:47,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:48,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:48,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:49,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:49,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:49,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:50,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:50,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:51,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:51,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:52,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:52,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:52,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:53,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:53,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:53,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:54,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:54,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:55,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:55,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:55,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:56,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:56,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:57,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:57,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:57,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:58,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:58,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:58,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:59,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:59,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:59,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:00,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:00,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:01,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:02,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:02,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:02,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:03,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:03,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:03,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:04,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:04,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:04,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:05,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:05,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:06,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:06,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:06,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:07,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:07,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:07,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:08,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:08,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:08,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:09,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:09,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:09,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:10,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:10,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:10,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:11,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:11,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:11,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:12,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:12,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:12,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:13,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:13,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:13,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:14,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:14,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:14,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:15,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:15,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:15,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:17,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:17,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:17,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:18,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:18,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:18,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:19,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:19,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:19,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:20,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:20,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:20,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:21,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:21,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:21,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:23,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:23,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:23,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:24,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:24,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:24,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:25,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:25,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:25,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:26,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:26,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:26,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:27,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:27,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:27,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:28,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:28,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:28,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:29,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:29,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:29,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:30,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:30,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:30,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:32,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:32,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:32,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:34,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:34,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:34,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:35,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:35,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:36,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:36,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:36,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:37,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:37,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:37,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:38,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:38,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:38,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:39,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:39,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:39,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:39,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:40,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:40,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:41,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:41,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:42,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:42,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:42,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:44,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:44,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:44,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:45,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:45,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:46,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:46,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:46,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:47,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:47,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:47,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:48,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:48,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:49,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:49,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:49,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:49,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:50,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:50,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:51,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:51,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:51,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:51,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:52,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:52,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:52,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:53,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:53,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:53,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:53,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:54,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:54,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:54,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:55,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:55,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:55,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:56,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:56,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:57,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:57,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:57,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:58,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:58,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:58,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:59,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:59,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:59,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:00,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:00,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:00,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:01,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:01,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:01,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:02,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:02,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:02,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:03,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:03,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:03,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:04,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:04,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:05,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:05,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:05,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:06,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:06,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:07,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:07,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:07,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:08,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:08,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:08,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:09,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:09,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:10,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:10,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:10,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:11,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:11,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:11,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:11,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:12,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:12,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:12,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:13,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:13,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:13,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:14,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:14,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:15,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:15,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:16,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:16,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:16,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:17,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:17,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:17,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:18,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:18,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:18,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:19,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:19,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:19,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:21,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:21,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:21,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:22,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:22,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:22,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:23,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:23,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:23,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:24,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:24,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:24,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:25,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:25,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:27,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:27,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:27,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:28,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:28,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:28,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:29,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:29,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:29,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:30,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:31,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:31,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:31,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:32,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:32,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:32,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:33,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:33,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:33,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:34,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:34,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:35,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:35,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:35,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:36,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:36,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:36,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:37,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:37,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:37,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:38,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:38,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:38,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:39,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:39,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:39,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:40,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:40,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:40,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:41,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:41,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:41,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:42,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:42,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:42,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:43,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:43,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:44,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:44,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:44,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:46,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:46,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:47,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:47,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:47,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:48,511][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2487, device='cuda:0') eval_epoch_loss=tensor(0.2221, device='cuda:0') eval_epoch_acc=tensor(0.9471, device='cuda:0')
[2025-02-13 20:19:48,513][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:19:48,513][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:19:48,766][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_7132_loss_0.22211353480815887/model.pt
[2025-02-13 20:19:48,772][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:19:48,773][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.22211353480815887
[2025-02-13 20:19:48,773][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9471306204795837
[2025-02-13 20:19:48,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:49,204][root][INFO] - Training Epoch: 1/2, step 7132/7134 completed (loss: 0.07953780889511108, acc: 0.9852941036224365)
[2025-02-13 20:19:49,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:49,572][root][INFO] - Training Epoch: 1/2, step 7133/7134 completed (loss: 0.16313418745994568, acc: 0.9624060392379761)
[2025-02-13 20:19:49,955][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=1.3058, train_epoch_loss=0.2668, epoch time 3655.8783842939883s
[2025-02-13 20:19:49,955][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-02-13 20:19:49,955][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 21 GB
[2025-02-13 20:19:49,956][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-02-13 20:19:49,956][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2025-02-13 20:19:49,956][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-13 20:19:50,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:51,084][root][INFO] - Training Epoch: 2/2, step 0/7134 completed (loss: 0.20229710638523102, acc: 0.9551281929016113)
[2025-02-13 20:19:51,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:51,498][root][INFO] - Training Epoch: 2/2, step 1/7134 completed (loss: 0.16660600900650024, acc: 0.9617834687232971)
[2025-02-13 20:19:51,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:51,883][root][INFO] - Training Epoch: 2/2, step 2/7134 completed (loss: 0.07281479984521866, acc: 0.9772727489471436)
[2025-02-13 20:19:52,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:52,258][root][INFO] - Training Epoch: 2/2, step 3/7134 completed (loss: 0.1317954808473587, acc: 0.9593023061752319)
[2025-02-13 20:19:52,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:52,657][root][INFO] - Training Epoch: 2/2, step 4/7134 completed (loss: 0.0946822464466095, acc: 0.9811320900917053)
[2025-02-13 20:19:52,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:53,050][root][INFO] - Training Epoch: 2/2, step 5/7134 completed (loss: 0.09116395562887192, acc: 0.9664804339408875)
[2025-02-13 20:19:53,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:53,417][root][INFO] - Training Epoch: 2/2, step 6/7134 completed (loss: 0.03811586648225784, acc: 0.9929577708244324)
[2025-02-13 20:19:53,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:53,798][root][INFO] - Training Epoch: 2/2, step 7/7134 completed (loss: 0.08625024557113647, acc: 0.9838709831237793)
[2025-02-13 20:19:53,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:54,223][root][INFO] - Training Epoch: 2/2, step 8/7134 completed (loss: 0.188359797000885, acc: 0.9512194991111755)
[2025-02-13 20:19:54,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:54,619][root][INFO] - Training Epoch: 2/2, step 9/7134 completed (loss: 0.061296142637729645, acc: 0.9867549538612366)
[2025-02-13 20:19:54,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:55,108][root][INFO] - Training Epoch: 2/2, step 10/7134 completed (loss: 0.15667468309402466, acc: 0.9467455744743347)
[2025-02-13 20:19:55,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:55,530][root][INFO] - Training Epoch: 2/2, step 11/7134 completed (loss: 0.1071561798453331, acc: 0.9583333134651184)
[2025-02-13 20:19:55,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:55,924][root][INFO] - Training Epoch: 2/2, step 12/7134 completed (loss: 0.0667104423046112, acc: 0.9942196607589722)
[2025-02-13 20:19:56,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:56,305][root][INFO] - Training Epoch: 2/2, step 13/7134 completed (loss: 0.08947067707777023, acc: 0.9719101190567017)
[2025-02-13 20:19:56,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:56,718][root][INFO] - Training Epoch: 2/2, step 14/7134 completed (loss: 0.021239086985588074, acc: 1.0)
[2025-02-13 20:19:56,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:57,143][root][INFO] - Training Epoch: 2/2, step 15/7134 completed (loss: 0.032532576471567154, acc: 0.9927536249160767)
[2025-02-13 20:19:57,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:57,553][root][INFO] - Training Epoch: 2/2, step 16/7134 completed (loss: 0.007996313273906708, acc: 1.0)
[2025-02-13 20:19:57,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:57,959][root][INFO] - Training Epoch: 2/2, step 17/7134 completed (loss: 0.15283049643039703, acc: 0.9680851101875305)
[2025-02-13 20:19:58,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:58,350][root][INFO] - Training Epoch: 2/2, step 18/7134 completed (loss: 0.017766354605555534, acc: 1.0)
[2025-02-13 20:19:58,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:58,730][root][INFO] - Training Epoch: 2/2, step 19/7134 completed (loss: 0.08292397111654282, acc: 0.9710982441902161)
[2025-02-13 20:19:58,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:59,137][root][INFO] - Training Epoch: 2/2, step 20/7134 completed (loss: 0.02908077836036682, acc: 0.9886363744735718)
[2025-02-13 20:19:59,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:59,545][root][INFO] - Training Epoch: 2/2, step 21/7134 completed (loss: 0.10734985023736954, acc: 0.9772727489471436)
[2025-02-13 20:19:59,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:59,935][root][INFO] - Training Epoch: 2/2, step 22/7134 completed (loss: 0.007793718948960304, acc: 1.0)
[2025-02-13 20:20:00,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:00,339][root][INFO] - Training Epoch: 2/2, step 23/7134 completed (loss: 0.03504405543208122, acc: 0.994350254535675)
[2025-02-13 20:20:00,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:00,740][root][INFO] - Training Epoch: 2/2, step 24/7134 completed (loss: 0.02595541812479496, acc: 0.9944444298744202)
[2025-02-13 20:20:00,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:01,124][root][INFO] - Training Epoch: 2/2, step 25/7134 completed (loss: 0.10125169157981873, acc: 0.9704142212867737)
[2025-02-13 20:20:01,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:01,517][root][INFO] - Training Epoch: 2/2, step 26/7134 completed (loss: 0.12907518446445465, acc: 0.9693251252174377)
[2025-02-13 20:20:01,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:01,909][root][INFO] - Training Epoch: 2/2, step 27/7134 completed (loss: 0.020123908296227455, acc: 0.9935897588729858)
[2025-02-13 20:20:02,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:02,316][root][INFO] - Training Epoch: 2/2, step 28/7134 completed (loss: 0.21963593363761902, acc: 0.9543147087097168)
[2025-02-13 20:20:02,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:02,741][root][INFO] - Training Epoch: 2/2, step 29/7134 completed (loss: 0.15388809144496918, acc: 0.976331353187561)
[2025-02-13 20:20:02,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:03,140][root][INFO] - Training Epoch: 2/2, step 30/7134 completed (loss: 0.11439217627048492, acc: 0.9758453965187073)
[2025-02-13 20:20:03,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:03,524][root][INFO] - Training Epoch: 2/2, step 31/7134 completed (loss: 0.18922898173332214, acc: 0.9627906680107117)
[2025-02-13 20:20:03,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:03,928][root][INFO] - Training Epoch: 2/2, step 32/7134 completed (loss: 0.2827909290790558, acc: 0.9402173757553101)
[2025-02-13 20:20:04,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:04,291][root][INFO] - Training Epoch: 2/2, step 33/7134 completed (loss: 0.23664623498916626, acc: 0.9538461565971375)
[2025-02-13 20:20:04,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:04,666][root][INFO] - Training Epoch: 2/2, step 34/7134 completed (loss: 0.11896858364343643, acc: 0.9714285731315613)
[2025-02-13 20:20:04,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:05,094][root][INFO] - Training Epoch: 2/2, step 35/7134 completed (loss: 0.14877000451087952, acc: 0.9686274528503418)
[2025-02-13 20:20:05,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:05,563][root][INFO] - Training Epoch: 2/2, step 36/7134 completed (loss: 0.18825267255306244, acc: 0.9677419066429138)
[2025-02-13 20:20:05,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:05,964][root][INFO] - Training Epoch: 2/2, step 37/7134 completed (loss: 0.16478772461414337, acc: 0.9629629850387573)
[2025-02-13 20:20:06,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:06,359][root][INFO] - Training Epoch: 2/2, step 38/7134 completed (loss: 0.12380462884902954, acc: 0.9685534834861755)
[2025-02-13 20:20:06,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:06,777][root][INFO] - Training Epoch: 2/2, step 39/7134 completed (loss: 0.07964816689491272, acc: 0.9766082167625427)
[2025-02-13 20:20:06,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:07,154][root][INFO] - Training Epoch: 2/2, step 40/7134 completed (loss: 0.13247466087341309, acc: 0.9910714030265808)
[2025-02-13 20:20:07,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:07,520][root][INFO] - Training Epoch: 2/2, step 41/7134 completed (loss: 0.14796443283557892, acc: 0.9734042286872864)
[2025-02-13 20:20:07,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:07,909][root][INFO] - Training Epoch: 2/2, step 42/7134 completed (loss: 0.1030639037489891, acc: 0.9876543283462524)
[2025-02-13 20:20:08,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:08,363][root][INFO] - Training Epoch: 2/2, step 43/7134 completed (loss: 0.03267728537321091, acc: 0.994413435459137)
[2025-02-13 20:20:08,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:08,775][root][INFO] - Training Epoch: 2/2, step 44/7134 completed (loss: 0.11893368512392044, acc: 0.9679144620895386)
[2025-02-13 20:20:08,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:09,151][root][INFO] - Training Epoch: 2/2, step 45/7134 completed (loss: 0.12160108983516693, acc: 0.9811320900917053)
[2025-02-13 20:20:09,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:09,590][root][INFO] - Training Epoch: 2/2, step 46/7134 completed (loss: 0.11609154939651489, acc: 0.9651162624359131)
[2025-02-13 20:20:09,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:09,998][root][INFO] - Training Epoch: 2/2, step 47/7134 completed (loss: 0.12408925592899323, acc: 0.9664804339408875)
[2025-02-13 20:20:10,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:10,431][root][INFO] - Training Epoch: 2/2, step 48/7134 completed (loss: 0.10483172535896301, acc: 0.9758453965187073)
[2025-02-13 20:20:10,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:10,863][root][INFO] - Training Epoch: 2/2, step 49/7134 completed (loss: 0.14522621035575867, acc: 0.9823529124259949)
[2025-02-13 20:20:11,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:11,301][root][INFO] - Training Epoch: 2/2, step 50/7134 completed (loss: 0.07842987775802612, acc: 0.9805825352668762)
[2025-02-13 20:20:11,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:11,714][root][INFO] - Training Epoch: 2/2, step 51/7134 completed (loss: 0.09639421105384827, acc: 0.9744898080825806)
[2025-02-13 20:20:11,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:12,078][root][INFO] - Training Epoch: 2/2, step 52/7134 completed (loss: 0.05655250325798988, acc: 0.9801324605941772)
[2025-02-13 20:20:12,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:12,436][root][INFO] - Training Epoch: 2/2, step 53/7134 completed (loss: 0.05358165502548218, acc: 0.9783783555030823)
[2025-02-13 20:20:12,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:12,823][root][INFO] - Training Epoch: 2/2, step 54/7134 completed (loss: 0.08659154921770096, acc: 0.976190447807312)
[2025-02-13 20:20:12,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:13,196][root][INFO] - Training Epoch: 2/2, step 55/7134 completed (loss: 0.18999522924423218, acc: 0.9510869383811951)
[2025-02-13 20:20:13,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:13,579][root][INFO] - Training Epoch: 2/2, step 56/7134 completed (loss: 0.03951753303408623, acc: 0.9846153855323792)
[2025-02-13 20:20:13,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:13,992][root][INFO] - Training Epoch: 2/2, step 57/7134 completed (loss: 0.231705904006958, acc: 0.9505494236946106)
[2025-02-13 20:20:14,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:14,388][root][INFO] - Training Epoch: 2/2, step 58/7134 completed (loss: 0.17533542215824127, acc: 0.9497206807136536)
[2025-02-13 20:20:14,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:14,775][root][INFO] - Training Epoch: 2/2, step 59/7134 completed (loss: 0.07076536118984222, acc: 0.9839572310447693)
[2025-02-13 20:20:14,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:15,167][root][INFO] - Training Epoch: 2/2, step 60/7134 completed (loss: 0.14509348571300507, acc: 0.9620253443717957)
[2025-02-13 20:20:15,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:15,598][root][INFO] - Training Epoch: 2/2, step 61/7134 completed (loss: 0.13836698234081268, acc: 0.9664804339408875)
[2025-02-13 20:20:15,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:16,011][root][INFO] - Training Epoch: 2/2, step 62/7134 completed (loss: 0.24209706485271454, acc: 0.9550561904907227)
[2025-02-13 20:20:16,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:16,419][root][INFO] - Training Epoch: 2/2, step 63/7134 completed (loss: 0.23532569408416748, acc: 0.9450549483299255)
[2025-02-13 20:20:16,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:16,816][root][INFO] - Training Epoch: 2/2, step 64/7134 completed (loss: 0.1805747151374817, acc: 0.9529411792755127)
[2025-02-13 20:20:16,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:17,204][root][INFO] - Training Epoch: 2/2, step 65/7134 completed (loss: 0.27228718996047974, acc: 0.914893627166748)
[2025-02-13 20:20:17,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:17,619][root][INFO] - Training Epoch: 2/2, step 66/7134 completed (loss: 0.18304301798343658, acc: 0.9242424368858337)
[2025-02-13 20:20:17,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:18,024][root][INFO] - Training Epoch: 2/2, step 67/7134 completed (loss: 0.24954254925251007, acc: 0.9252873659133911)
[2025-02-13 20:20:18,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:18,389][root][INFO] - Training Epoch: 2/2, step 68/7134 completed (loss: 0.24231117963790894, acc: 0.9382022619247437)
[2025-02-13 20:20:18,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:18,851][root][INFO] - Training Epoch: 2/2, step 69/7134 completed (loss: 0.19889602065086365, acc: 0.9629629850387573)
[2025-02-13 20:20:18,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:19,237][root][INFO] - Training Epoch: 2/2, step 70/7134 completed (loss: 0.2041080892086029, acc: 0.953125)
[2025-02-13 20:20:19,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:19,602][root][INFO] - Training Epoch: 2/2, step 71/7134 completed (loss: 0.11758624017238617, acc: 0.9757575988769531)
[2025-02-13 20:20:19,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:19,979][root][INFO] - Training Epoch: 2/2, step 72/7134 completed (loss: 0.10744491964578629, acc: 0.9644970297813416)
[2025-02-13 20:20:20,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:20,355][root][INFO] - Training Epoch: 2/2, step 73/7134 completed (loss: 0.09002699702978134, acc: 0.9826589822769165)
[2025-02-13 20:20:20,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:20,717][root][INFO] - Training Epoch: 2/2, step 74/7134 completed (loss: 0.05570494756102562, acc: 0.9825581312179565)
[2025-02-13 20:20:20,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:21,155][root][INFO] - Training Epoch: 2/2, step 75/7134 completed (loss: 0.2846885919570923, acc: 0.9345238208770752)
[2025-02-13 20:20:21,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:21,553][root][INFO] - Training Epoch: 2/2, step 76/7134 completed (loss: 0.1591901332139969, acc: 0.9619565010070801)
[2025-02-13 20:20:21,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:21,930][root][INFO] - Training Epoch: 2/2, step 77/7134 completed (loss: 0.16351261734962463, acc: 0.9613259434700012)
[2025-02-13 20:20:22,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:22,321][root][INFO] - Training Epoch: 2/2, step 78/7134 completed (loss: 0.29105669260025024, acc: 0.8940397500991821)
[2025-02-13 20:20:22,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:22,703][root][INFO] - Training Epoch: 2/2, step 79/7134 completed (loss: 0.30412840843200684, acc: 0.9266666769981384)
[2025-02-13 20:20:22,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:23,088][root][INFO] - Training Epoch: 2/2, step 80/7134 completed (loss: 0.09651513397693634, acc: 0.983146071434021)
[2025-02-13 20:20:23,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:23,464][root][INFO] - Training Epoch: 2/2, step 81/7134 completed (loss: 0.15319068729877472, acc: 0.9576719403266907)
[2025-02-13 20:20:23,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:23,867][root][INFO] - Training Epoch: 2/2, step 82/7134 completed (loss: 0.1824571043252945, acc: 0.9518072009086609)
[2025-02-13 20:20:24,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:24,274][root][INFO] - Training Epoch: 2/2, step 83/7134 completed (loss: 0.1437956988811493, acc: 0.9734042286872864)
[2025-02-13 20:20:24,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:24,711][root][INFO] - Training Epoch: 2/2, step 84/7134 completed (loss: 0.19759786128997803, acc: 0.9640287756919861)
[2025-02-13 20:20:24,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:25,134][root][INFO] - Training Epoch: 2/2, step 85/7134 completed (loss: 0.19264942407608032, acc: 0.949999988079071)
[2025-02-13 20:20:25,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:25,539][root][INFO] - Training Epoch: 2/2, step 86/7134 completed (loss: 0.29561641812324524, acc: 0.9285714030265808)
[2025-02-13 20:20:25,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:26,002][root][INFO] - Training Epoch: 2/2, step 87/7134 completed (loss: 0.31957516074180603, acc: 0.9220778942108154)
[2025-02-13 20:20:26,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:26,457][root][INFO] - Training Epoch: 2/2, step 88/7134 completed (loss: 0.24388492107391357, acc: 0.9397590160369873)
[2025-02-13 20:20:26,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:26,910][root][INFO] - Training Epoch: 2/2, step 89/7134 completed (loss: 0.20891651511192322, acc: 0.9485714435577393)
[2025-02-13 20:20:27,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:27,320][root][INFO] - Training Epoch: 2/2, step 90/7134 completed (loss: 0.13181182742118835, acc: 0.9579831957817078)
[2025-02-13 20:20:27,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:27,659][root][INFO] - Training Epoch: 2/2, step 91/7134 completed (loss: 0.2438657283782959, acc: 0.9256198406219482)
[2025-02-13 20:20:27,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:28,038][root][INFO] - Training Epoch: 2/2, step 92/7134 completed (loss: 0.16669484972953796, acc: 0.9610389471054077)
[2025-02-13 20:20:28,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:28,446][root][INFO] - Training Epoch: 2/2, step 93/7134 completed (loss: 0.17565907537937164, acc: 0.9657142758369446)
[2025-02-13 20:20:28,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:28,825][root][INFO] - Training Epoch: 2/2, step 94/7134 completed (loss: 0.2608449459075928, acc: 0.9433962106704712)
[2025-02-13 20:20:28,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:29,193][root][INFO] - Training Epoch: 2/2, step 95/7134 completed (loss: 0.09774753451347351, acc: 0.9695122241973877)
[2025-02-13 20:20:29,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:29,572][root][INFO] - Training Epoch: 2/2, step 96/7134 completed (loss: 0.0705566480755806, acc: 0.9906542301177979)
[2025-02-13 20:20:29,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:29,949][root][INFO] - Training Epoch: 2/2, step 97/7134 completed (loss: 0.9729387164115906, acc: 0.8333333134651184)
[2025-02-13 20:20:30,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:30,324][root][INFO] - Training Epoch: 2/2, step 98/7134 completed (loss: 0.28706082701683044, acc: 0.908450722694397)
[2025-02-13 20:20:30,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:30,719][root][INFO] - Training Epoch: 2/2, step 99/7134 completed (loss: 0.19056664407253265, acc: 0.9461538195610046)
[2025-02-13 20:20:30,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:31,115][root][INFO] - Training Epoch: 2/2, step 100/7134 completed (loss: 0.2598534822463989, acc: 0.950276255607605)
[2025-02-13 20:20:31,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:31,504][root][INFO] - Training Epoch: 2/2, step 101/7134 completed (loss: 0.2806164026260376, acc: 0.9296875)
[2025-02-13 20:20:31,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:31,920][root][INFO] - Training Epoch: 2/2, step 102/7134 completed (loss: 0.061524733901023865, acc: 0.9836065769195557)
[2025-02-13 20:20:32,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:32,310][root][INFO] - Training Epoch: 2/2, step 103/7134 completed (loss: 0.21833616495132446, acc: 0.9589040875434875)
[2025-02-13 20:20:32,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:32,713][root][INFO] - Training Epoch: 2/2, step 104/7134 completed (loss: 0.1021207943558693, acc: 0.9866666793823242)
[2025-02-13 20:20:32,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:33,064][root][INFO] - Training Epoch: 2/2, step 105/7134 completed (loss: 0.05799730867147446, acc: 0.9776119589805603)
[2025-02-13 20:20:33,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:33,430][root][INFO] - Training Epoch: 2/2, step 106/7134 completed (loss: 0.09441617131233215, acc: 0.9751552939414978)
[2025-02-13 20:20:33,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:33,830][root][INFO] - Training Epoch: 2/2, step 107/7134 completed (loss: 0.4598120450973511, acc: 0.8662790656089783)
[2025-02-13 20:20:33,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:34,209][root][INFO] - Training Epoch: 2/2, step 108/7134 completed (loss: 0.19524258375167847, acc: 0.9513513445854187)
[2025-02-13 20:20:34,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:34,542][root][INFO] - Training Epoch: 2/2, step 109/7134 completed (loss: 0.19152767956256866, acc: 0.9603174328804016)
[2025-02-13 20:20:34,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:34,932][root][INFO] - Training Epoch: 2/2, step 110/7134 completed (loss: 0.1456291228532791, acc: 0.9553072452545166)
[2025-02-13 20:20:35,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:35,310][root][INFO] - Training Epoch: 2/2, step 111/7134 completed (loss: 0.2719128429889679, acc: 0.939393937587738)
[2025-02-13 20:20:35,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:35,713][root][INFO] - Training Epoch: 2/2, step 112/7134 completed (loss: 0.19039510190486908, acc: 0.9375)
[2025-02-13 20:20:35,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:36,090][root][INFO] - Training Epoch: 2/2, step 113/7134 completed (loss: 0.0890955850481987, acc: 0.9664804339408875)
[2025-02-13 20:20:36,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:36,463][root][INFO] - Training Epoch: 2/2, step 114/7134 completed (loss: 0.16754688322544098, acc: 0.9428571462631226)
[2025-02-13 20:20:36,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:36,876][root][INFO] - Training Epoch: 2/2, step 115/7134 completed (loss: 0.3650292456150055, acc: 0.9009901285171509)
[2025-02-13 20:20:37,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:37,292][root][INFO] - Training Epoch: 2/2, step 116/7134 completed (loss: 0.14974169433116913, acc: 0.959770143032074)
[2025-02-13 20:20:37,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:37,700][root][INFO] - Training Epoch: 2/2, step 117/7134 completed (loss: 0.21104063093662262, acc: 0.9454545378684998)
[2025-02-13 20:20:37,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:38,127][root][INFO] - Training Epoch: 2/2, step 118/7134 completed (loss: 0.23987196385860443, acc: 0.9388889074325562)
[2025-02-13 20:20:38,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:38,501][root][INFO] - Training Epoch: 2/2, step 119/7134 completed (loss: 0.17315280437469482, acc: 0.9414893388748169)
[2025-02-13 20:20:38,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:38,882][root][INFO] - Training Epoch: 2/2, step 120/7134 completed (loss: 0.0953308492898941, acc: 0.9702380895614624)
[2025-02-13 20:20:39,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:39,244][root][INFO] - Training Epoch: 2/2, step 121/7134 completed (loss: 0.12422901391983032, acc: 0.9608938694000244)
[2025-02-13 20:20:39,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:39,637][root][INFO] - Training Epoch: 2/2, step 122/7134 completed (loss: 0.1073177382349968, acc: 0.9780219793319702)
[2025-02-13 20:20:39,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:40,021][root][INFO] - Training Epoch: 2/2, step 123/7134 completed (loss: 0.211453378200531, acc: 0.9622641801834106)
[2025-02-13 20:20:40,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:40,386][root][INFO] - Training Epoch: 2/2, step 124/7134 completed (loss: 0.19111473858356476, acc: 0.9583333134651184)
[2025-02-13 20:20:40,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:40,770][root][INFO] - Training Epoch: 2/2, step 125/7134 completed (loss: 0.20142967998981476, acc: 0.9607843160629272)
[2025-02-13 20:20:40,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:41,147][root][INFO] - Training Epoch: 2/2, step 126/7134 completed (loss: 0.2055382877588272, acc: 0.9468085169792175)
[2025-02-13 20:20:41,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:41,509][root][INFO] - Training Epoch: 2/2, step 127/7134 completed (loss: 0.24319975078105927, acc: 0.9375)
[2025-02-13 20:20:41,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:41,913][root][INFO] - Training Epoch: 2/2, step 128/7134 completed (loss: 0.22372914850711823, acc: 0.9665071964263916)
[2025-02-13 20:20:42,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:42,303][root][INFO] - Training Epoch: 2/2, step 129/7134 completed (loss: 0.19594436883926392, acc: 0.9441340565681458)
[2025-02-13 20:20:42,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:42,699][root][INFO] - Training Epoch: 2/2, step 130/7134 completed (loss: 0.10410982370376587, acc: 0.9588235020637512)
[2025-02-13 20:20:42,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:43,076][root][INFO] - Training Epoch: 2/2, step 131/7134 completed (loss: 0.0489288754761219, acc: 0.9824561476707458)
[2025-02-13 20:20:43,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:43,479][root][INFO] - Training Epoch: 2/2, step 132/7134 completed (loss: 0.20522409677505493, acc: 0.955974817276001)
[2025-02-13 20:20:43,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:43,886][root][INFO] - Training Epoch: 2/2, step 133/7134 completed (loss: 0.10887807607650757, acc: 0.970588207244873)
[2025-02-13 20:20:44,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:44,280][root][INFO] - Training Epoch: 2/2, step 134/7134 completed (loss: 0.06924021244049072, acc: 0.9774436354637146)
[2025-02-13 20:20:44,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:44,650][root][INFO] - Training Epoch: 2/2, step 135/7134 completed (loss: 0.1920718103647232, acc: 0.9470899701118469)
[2025-02-13 20:20:44,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:45,019][root][INFO] - Training Epoch: 2/2, step 136/7134 completed (loss: 0.11207006126642227, acc: 0.97826087474823)
[2025-02-13 20:20:45,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:45,424][root][INFO] - Training Epoch: 2/2, step 137/7134 completed (loss: 0.1465246081352234, acc: 0.9597989916801453)
[2025-02-13 20:20:45,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:45,822][root][INFO] - Training Epoch: 2/2, step 138/7134 completed (loss: 0.1189977377653122, acc: 0.9623655676841736)
[2025-02-13 20:20:45,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:46,283][root][INFO] - Training Epoch: 2/2, step 139/7134 completed (loss: 0.1490178406238556, acc: 0.9680851101875305)
[2025-02-13 20:20:46,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:46,691][root][INFO] - Training Epoch: 2/2, step 140/7134 completed (loss: 0.11276969313621521, acc: 0.9815950989723206)
[2025-02-13 20:20:46,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:47,045][root][INFO] - Training Epoch: 2/2, step 141/7134 completed (loss: 0.09821094572544098, acc: 0.9800000190734863)
[2025-02-13 20:20:47,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:47,440][root][INFO] - Training Epoch: 2/2, step 142/7134 completed (loss: 0.16993799805641174, acc: 0.9604519605636597)
[2025-02-13 20:20:47,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:47,825][root][INFO] - Training Epoch: 2/2, step 143/7134 completed (loss: 0.2509186863899231, acc: 0.9171270728111267)
[2025-02-13 20:20:47,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:48,214][root][INFO] - Training Epoch: 2/2, step 144/7134 completed (loss: 0.2819156050682068, acc: 0.9527027010917664)
[2025-02-13 20:20:48,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:48,599][root][INFO] - Training Epoch: 2/2, step 145/7134 completed (loss: 0.599429726600647, acc: 0.8497409224510193)
[2025-02-13 20:20:48,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:48,975][root][INFO] - Training Epoch: 2/2, step 146/7134 completed (loss: 0.32383251190185547, acc: 0.903954803943634)
[2025-02-13 20:20:49,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:49,359][root][INFO] - Training Epoch: 2/2, step 147/7134 completed (loss: 0.45296168327331543, acc: 0.9103448390960693)
[2025-02-13 20:20:49,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:49,791][root][INFO] - Training Epoch: 2/2, step 148/7134 completed (loss: 0.581105649471283, acc: 0.8306451439857483)
[2025-02-13 20:20:49,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:50,176][root][INFO] - Training Epoch: 2/2, step 149/7134 completed (loss: 0.3741338551044464, acc: 0.8883248567581177)
[2025-02-13 20:20:50,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:50,587][root][INFO] - Training Epoch: 2/2, step 150/7134 completed (loss: 0.3693749010562897, acc: 0.9057591557502747)
[2025-02-13 20:20:50,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:51,021][root][INFO] - Training Epoch: 2/2, step 151/7134 completed (loss: 0.21543878316879272, acc: 0.9408602118492126)
[2025-02-13 20:20:51,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:51,414][root][INFO] - Training Epoch: 2/2, step 152/7134 completed (loss: 0.15834081172943115, acc: 0.9712918400764465)
[2025-02-13 20:20:51,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:51,793][root][INFO] - Training Epoch: 2/2, step 153/7134 completed (loss: 0.5241596102714539, acc: 0.9060773253440857)
[2025-02-13 20:20:51,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:52,219][root][INFO] - Training Epoch: 2/2, step 154/7134 completed (loss: 0.42599251866340637, acc: 0.9281045794487)
[2025-02-13 20:20:52,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:52,613][root][INFO] - Training Epoch: 2/2, step 155/7134 completed (loss: 0.3560062646865845, acc: 0.9508196711540222)
[2025-02-13 20:20:52,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:53,014][root][INFO] - Training Epoch: 2/2, step 156/7134 completed (loss: 0.1391267329454422, acc: 0.9704433679580688)
[2025-02-13 20:20:53,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:53,403][root][INFO] - Training Epoch: 2/2, step 157/7134 completed (loss: 0.38095539808273315, acc: 0.9153439402580261)
[2025-02-13 20:20:53,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:53,842][root][INFO] - Training Epoch: 2/2, step 158/7134 completed (loss: 0.15367558598518372, acc: 0.9649122953414917)
[2025-02-13 20:20:53,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:54,251][root][INFO] - Training Epoch: 2/2, step 159/7134 completed (loss: 0.27584680914878845, acc: 0.9479768872261047)
[2025-02-13 20:20:54,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:54,704][root][INFO] - Training Epoch: 2/2, step 160/7134 completed (loss: 0.1354062408208847, acc: 0.9661017060279846)
[2025-02-13 20:20:54,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:55,127][root][INFO] - Training Epoch: 2/2, step 161/7134 completed (loss: 0.035611823201179504, acc: 0.9864864945411682)
[2025-02-13 20:20:55,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:55,584][root][INFO] - Training Epoch: 2/2, step 162/7134 completed (loss: 0.09731264412403107, acc: 0.9506173133850098)
[2025-02-13 20:20:55,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:56,000][root][INFO] - Training Epoch: 2/2, step 163/7134 completed (loss: 0.05839167907834053, acc: 0.9856114983558655)
[2025-02-13 20:20:56,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:56,375][root][INFO] - Training Epoch: 2/2, step 164/7134 completed (loss: 0.2594635784626007, acc: 0.9595959782600403)
[2025-02-13 20:20:56,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:56,754][root][INFO] - Training Epoch: 2/2, step 165/7134 completed (loss: 0.20491579174995422, acc: 0.9700000286102295)
[2025-02-13 20:20:56,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:57,111][root][INFO] - Training Epoch: 2/2, step 166/7134 completed (loss: 0.09665396809577942, acc: 0.9736841917037964)
[2025-02-13 20:20:57,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:57,478][root][INFO] - Training Epoch: 2/2, step 167/7134 completed (loss: 1.3696544170379639, acc: 0.7289156913757324)
[2025-02-13 20:20:57,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:57,917][root][INFO] - Training Epoch: 2/2, step 168/7134 completed (loss: 2.61856746673584, acc: 0.4727272689342499)
[2025-02-13 20:20:58,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:58,261][root][INFO] - Training Epoch: 2/2, step 169/7134 completed (loss: 2.3591151237487793, acc: 0.5859375)
[2025-02-13 20:20:58,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:58,629][root][INFO] - Training Epoch: 2/2, step 170/7134 completed (loss: 1.3763761520385742, acc: 0.726190447807312)
[2025-02-13 20:20:58,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:59,024][root][INFO] - Training Epoch: 2/2, step 171/7134 completed (loss: 1.366296648979187, acc: 0.7425742745399475)
[2025-02-13 20:20:59,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:59,405][root][INFO] - Training Epoch: 2/2, step 172/7134 completed (loss: 0.5844526886940002, acc: 0.8759689927101135)
[2025-02-13 20:20:59,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:59,780][root][INFO] - Training Epoch: 2/2, step 173/7134 completed (loss: 0.6247660517692566, acc: 0.8479999899864197)
[2025-02-13 20:20:59,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:00,168][root][INFO] - Training Epoch: 2/2, step 174/7134 completed (loss: 0.9749202728271484, acc: 0.7631579041481018)
[2025-02-13 20:21:00,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:00,537][root][INFO] - Training Epoch: 2/2, step 175/7134 completed (loss: 0.6596230864524841, acc: 0.8344370722770691)
[2025-02-13 20:21:00,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:00,950][root][INFO] - Training Epoch: 2/2, step 176/7134 completed (loss: 0.5965004563331604, acc: 0.8251366019248962)
[2025-02-13 20:21:01,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:01,335][root][INFO] - Training Epoch: 2/2, step 177/7134 completed (loss: 0.7264479398727417, acc: 0.8203592896461487)
[2025-02-13 20:21:01,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:01,719][root][INFO] - Training Epoch: 2/2, step 178/7134 completed (loss: 0.5321686267852783, acc: 0.8441558480262756)
[2025-02-13 20:21:01,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:02,184][root][INFO] - Training Epoch: 2/2, step 179/7134 completed (loss: 0.22745400667190552, acc: 0.9124087691307068)
[2025-02-13 20:21:02,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:02,601][root][INFO] - Training Epoch: 2/2, step 180/7134 completed (loss: 0.21941319108009338, acc: 0.9597315192222595)
[2025-02-13 20:21:02,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:03,010][root][INFO] - Training Epoch: 2/2, step 181/7134 completed (loss: 0.042559217661619186, acc: 1.0)
[2025-02-13 20:21:03,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:03,408][root][INFO] - Training Epoch: 2/2, step 182/7134 completed (loss: 0.07847930490970612, acc: 0.9875776171684265)
[2025-02-13 20:21:03,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:03,835][root][INFO] - Training Epoch: 2/2, step 183/7134 completed (loss: 0.18289951980113983, acc: 0.9452054500579834)
[2025-02-13 20:21:03,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:04,225][root][INFO] - Training Epoch: 2/2, step 184/7134 completed (loss: 0.17389602959156036, acc: 0.9453125)
[2025-02-13 20:21:04,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:04,623][root][INFO] - Training Epoch: 2/2, step 185/7134 completed (loss: 0.0751333013176918, acc: 0.9795918464660645)
[2025-02-13 20:21:04,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:05,010][root][INFO] - Training Epoch: 2/2, step 186/7134 completed (loss: 0.1875281035900116, acc: 0.9701492786407471)
[2025-02-13 20:21:05,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:05,390][root][INFO] - Training Epoch: 2/2, step 187/7134 completed (loss: 0.26059162616729736, acc: 0.9281045794487)
[2025-02-13 20:21:05,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:05,785][root][INFO] - Training Epoch: 2/2, step 188/7134 completed (loss: 0.28332430124282837, acc: 0.9146341681480408)
[2025-02-13 20:21:05,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:06,209][root][INFO] - Training Epoch: 2/2, step 189/7134 completed (loss: 0.2920302748680115, acc: 0.9180327653884888)
[2025-02-13 20:21:06,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:06,604][root][INFO] - Training Epoch: 2/2, step 190/7134 completed (loss: 0.5608032941818237, acc: 0.8805969953536987)
[2025-02-13 20:21:06,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:06,988][root][INFO] - Training Epoch: 2/2, step 191/7134 completed (loss: 0.4006336033344269, acc: 0.942307710647583)
[2025-02-13 20:21:07,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:07,444][root][INFO] - Training Epoch: 2/2, step 192/7134 completed (loss: 0.13186921179294586, acc: 0.9651162624359131)
[2025-02-13 20:21:07,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:07,840][root][INFO] - Training Epoch: 2/2, step 193/7134 completed (loss: 0.26978012919425964, acc: 0.910179615020752)
[2025-02-13 20:21:07,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:08,254][root][INFO] - Training Epoch: 2/2, step 194/7134 completed (loss: 0.3669496476650238, acc: 0.893081784248352)
[2025-02-13 20:21:08,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:08,667][root][INFO] - Training Epoch: 2/2, step 195/7134 completed (loss: 0.22961382567882538, acc: 0.9375)
[2025-02-13 20:21:08,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:09,114][root][INFO] - Training Epoch: 2/2, step 196/7134 completed (loss: 0.33295705914497375, acc: 0.8938547372817993)
[2025-02-13 20:21:09,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:09,505][root][INFO] - Training Epoch: 2/2, step 197/7134 completed (loss: 0.1309303492307663, acc: 0.9729729890823364)
[2025-02-13 20:21:09,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:09,927][root][INFO] - Training Epoch: 2/2, step 198/7134 completed (loss: 0.1592983454465866, acc: 0.9554139971733093)
[2025-02-13 20:21:10,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:10,359][root][INFO] - Training Epoch: 2/2, step 199/7134 completed (loss: 0.3474261164665222, acc: 0.9264705777168274)
[2025-02-13 20:21:10,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:10,767][root][INFO] - Training Epoch: 2/2, step 200/7134 completed (loss: 0.06514602899551392, acc: 0.9931972622871399)
[2025-02-13 20:21:10,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:11,158][root][INFO] - Training Epoch: 2/2, step 201/7134 completed (loss: 0.062379006296396255, acc: 0.9929078221321106)
[2025-02-13 20:21:11,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:11,527][root][INFO] - Training Epoch: 2/2, step 202/7134 completed (loss: 0.23613241314888, acc: 0.9419354796409607)
[2025-02-13 20:21:11,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:11,929][root][INFO] - Training Epoch: 2/2, step 203/7134 completed (loss: 0.29953911900520325, acc: 0.9289940595626831)
[2025-02-13 20:21:12,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:12,283][root][INFO] - Training Epoch: 2/2, step 204/7134 completed (loss: 0.3731593191623688, acc: 0.893081784248352)
[2025-02-13 20:21:12,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:12,660][root][INFO] - Training Epoch: 2/2, step 205/7134 completed (loss: 0.40452656149864197, acc: 0.9049999713897705)
[2025-02-13 20:21:12,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:13,092][root][INFO] - Training Epoch: 2/2, step 206/7134 completed (loss: 0.1320592761039734, acc: 0.9861111044883728)
[2025-02-13 20:21:13,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:13,499][root][INFO] - Training Epoch: 2/2, step 207/7134 completed (loss: 0.31272971630096436, acc: 0.918367326259613)
[2025-02-13 20:21:13,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:13,857][root][INFO] - Training Epoch: 2/2, step 208/7134 completed (loss: 0.6458314061164856, acc: 0.8928571343421936)
[2025-02-13 20:21:14,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:14,291][root][INFO] - Training Epoch: 2/2, step 209/7134 completed (loss: 0.5262611508369446, acc: 0.8715083599090576)
[2025-02-13 20:21:14,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:14,692][root][INFO] - Training Epoch: 2/2, step 210/7134 completed (loss: 0.35111188888549805, acc: 0.9251101613044739)
[2025-02-13 20:21:14,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:15,079][root][INFO] - Training Epoch: 2/2, step 211/7134 completed (loss: 0.31917694211006165, acc: 0.9256756901741028)
[2025-02-13 20:21:15,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:15,468][root][INFO] - Training Epoch: 2/2, step 212/7134 completed (loss: 0.5469958186149597, acc: 0.8925619721412659)
[2025-02-13 20:21:15,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:15,870][root][INFO] - Training Epoch: 2/2, step 213/7134 completed (loss: 0.5751621127128601, acc: 0.8666666746139526)
[2025-02-13 20:21:15,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:16,249][root][INFO] - Training Epoch: 2/2, step 214/7134 completed (loss: 0.1678144633769989, acc: 0.9603960514068604)
[2025-02-13 20:21:16,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:16,623][root][INFO] - Training Epoch: 2/2, step 215/7134 completed (loss: 0.17251303791999817, acc: 0.9571428298950195)
[2025-02-13 20:21:16,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:17,064][root][INFO] - Training Epoch: 2/2, step 216/7134 completed (loss: 0.14791764318943024, acc: 0.9775280952453613)
[2025-02-13 20:21:17,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:17,481][root][INFO] - Training Epoch: 2/2, step 217/7134 completed (loss: 0.15273118019104004, acc: 0.9791666865348816)
[2025-02-13 20:21:17,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:17,906][root][INFO] - Training Epoch: 2/2, step 218/7134 completed (loss: 0.09832699596881866, acc: 0.9893048405647278)
[2025-02-13 20:21:18,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:18,303][root][INFO] - Training Epoch: 2/2, step 219/7134 completed (loss: 0.07641056925058365, acc: 0.9736841917037964)
[2025-02-13 20:21:18,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:18,685][root][INFO] - Training Epoch: 2/2, step 220/7134 completed (loss: 0.10097906738519669, acc: 0.9739130139350891)
[2025-02-13 20:21:18,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:19,089][root][INFO] - Training Epoch: 2/2, step 221/7134 completed (loss: 0.14955805242061615, acc: 0.96875)
[2025-02-13 20:21:19,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:19,511][root][INFO] - Training Epoch: 2/2, step 222/7134 completed (loss: 0.1330561488866806, acc: 0.9710982441902161)
[2025-02-13 20:21:19,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:19,904][root][INFO] - Training Epoch: 2/2, step 223/7134 completed (loss: 0.2719779312610626, acc: 0.9351351261138916)
[2025-02-13 20:21:20,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:20,314][root][INFO] - Training Epoch: 2/2, step 224/7134 completed (loss: 0.3506017029285431, acc: 0.9329608678817749)
[2025-02-13 20:21:20,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:20,708][root][INFO] - Training Epoch: 2/2, step 225/7134 completed (loss: 0.3245895802974701, acc: 0.9171597361564636)
[2025-02-13 20:21:20,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:21,111][root][INFO] - Training Epoch: 2/2, step 226/7134 completed (loss: 0.170435830950737, acc: 0.9428571462631226)
[2025-02-13 20:21:21,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:21,484][root][INFO] - Training Epoch: 2/2, step 227/7134 completed (loss: 0.3168156147003174, acc: 0.9468085169792175)
[2025-02-13 20:21:21,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:21,873][root][INFO] - Training Epoch: 2/2, step 228/7134 completed (loss: 0.33913081884384155, acc: 0.9281437397003174)
[2025-02-13 20:21:22,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:22,242][root][INFO] - Training Epoch: 2/2, step 229/7134 completed (loss: 0.3059593737125397, acc: 0.9142857193946838)
[2025-02-13 20:21:22,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:22,642][root][INFO] - Training Epoch: 2/2, step 230/7134 completed (loss: 0.19019867479801178, acc: 0.950276255607605)
[2025-02-13 20:21:22,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:23,031][root][INFO] - Training Epoch: 2/2, step 231/7134 completed (loss: 0.18535216152668, acc: 0.9736841917037964)
[2025-02-13 20:21:23,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:23,419][root][INFO] - Training Epoch: 2/2, step 232/7134 completed (loss: 0.29940342903137207, acc: 0.9300699234008789)
[2025-02-13 20:21:23,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:23,849][root][INFO] - Training Epoch: 2/2, step 233/7134 completed (loss: 0.2165079116821289, acc: 0.9489051103591919)
[2025-02-13 20:21:24,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:24,262][root][INFO] - Training Epoch: 2/2, step 234/7134 completed (loss: 0.2200879603624344, acc: 0.9436619877815247)
[2025-02-13 20:21:24,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:24,700][root][INFO] - Training Epoch: 2/2, step 235/7134 completed (loss: 0.2816372513771057, acc: 0.9390243887901306)
[2025-02-13 20:21:24,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:25,077][root][INFO] - Training Epoch: 2/2, step 236/7134 completed (loss: 0.12486068904399872, acc: 0.9602649211883545)
[2025-02-13 20:21:25,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:25,454][root][INFO] - Training Epoch: 2/2, step 237/7134 completed (loss: 0.3087123930454254, acc: 0.9171974658966064)
[2025-02-13 20:21:25,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:25,836][root][INFO] - Training Epoch: 2/2, step 238/7134 completed (loss: 0.34843844175338745, acc: 0.9312977194786072)
[2025-02-13 20:21:25,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:26,205][root][INFO] - Training Epoch: 2/2, step 239/7134 completed (loss: 0.21301807463169098, acc: 0.9528301954269409)
[2025-02-13 20:21:26,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:26,580][root][INFO] - Training Epoch: 2/2, step 240/7134 completed (loss: 0.20481915771961212, acc: 0.9805825352668762)
[2025-02-13 20:21:26,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:26,954][root][INFO] - Training Epoch: 2/2, step 241/7134 completed (loss: 0.4164702296257019, acc: 0.9104477763175964)
[2025-02-13 20:21:27,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:27,310][root][INFO] - Training Epoch: 2/2, step 242/7134 completed (loss: 0.3908917307853699, acc: 0.9060402512550354)
[2025-02-13 20:21:27,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:27,690][root][INFO] - Training Epoch: 2/2, step 243/7134 completed (loss: 0.6003291606903076, acc: 0.8620689511299133)
[2025-02-13 20:21:27,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:28,080][root][INFO] - Training Epoch: 2/2, step 244/7134 completed (loss: 0.1917162537574768, acc: 0.9568345546722412)
[2025-02-13 20:21:28,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:28,448][root][INFO] - Training Epoch: 2/2, step 245/7134 completed (loss: 0.1832663118839264, acc: 0.9508196711540222)
[2025-02-13 20:21:28,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:28,894][root][INFO] - Training Epoch: 2/2, step 246/7134 completed (loss: 0.18484866619110107, acc: 0.9710144996643066)
[2025-02-13 20:21:29,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:29,301][root][INFO] - Training Epoch: 2/2, step 247/7134 completed (loss: 0.19844338297843933, acc: 0.9523809552192688)
[2025-02-13 20:21:29,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:29,695][root][INFO] - Training Epoch: 2/2, step 248/7134 completed (loss: 0.15964898467063904, acc: 0.9756097793579102)
[2025-02-13 20:21:29,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:30,056][root][INFO] - Training Epoch: 2/2, step 249/7134 completed (loss: 0.16071434319019318, acc: 0.95652174949646)
[2025-02-13 20:21:30,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:30,424][root][INFO] - Training Epoch: 2/2, step 250/7134 completed (loss: 0.21423554420471191, acc: 0.940397322177887)
[2025-02-13 20:21:30,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:30,803][root][INFO] - Training Epoch: 2/2, step 251/7134 completed (loss: 0.21812672913074493, acc: 0.971222996711731)
[2025-02-13 20:21:30,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:31,196][root][INFO] - Training Epoch: 2/2, step 252/7134 completed (loss: 0.08785734325647354, acc: 0.971222996711731)
[2025-02-13 20:21:31,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:31,590][root][INFO] - Training Epoch: 2/2, step 253/7134 completed (loss: 0.05727604031562805, acc: 0.9919999837875366)
[2025-02-13 20:21:31,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:31,961][root][INFO] - Training Epoch: 2/2, step 254/7134 completed (loss: 0.08529488742351532, acc: 0.9824561476707458)
[2025-02-13 20:21:32,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:32,371][root][INFO] - Training Epoch: 2/2, step 255/7134 completed (loss: 0.12404777109622955, acc: 0.9679487347602844)
[2025-02-13 20:21:32,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:32,734][root][INFO] - Training Epoch: 2/2, step 256/7134 completed (loss: 0.5035958290100098, acc: 0.8648648858070374)
[2025-02-13 20:21:32,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:33,117][root][INFO] - Training Epoch: 2/2, step 257/7134 completed (loss: 0.21778230369091034, acc: 0.9512194991111755)
[2025-02-13 20:21:33,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:33,444][root][INFO] - Training Epoch: 2/2, step 258/7134 completed (loss: 0.1022171750664711, acc: 0.975806474685669)
[2025-02-13 20:21:33,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:33,821][root][INFO] - Training Epoch: 2/2, step 259/7134 completed (loss: 0.1303766369819641, acc: 0.9696969985961914)
[2025-02-13 20:21:33,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:34,199][root][INFO] - Training Epoch: 2/2, step 260/7134 completed (loss: 0.11399220675230026, acc: 0.9774436354637146)
[2025-02-13 20:21:34,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:34,599][root][INFO] - Training Epoch: 2/2, step 261/7134 completed (loss: 0.19929568469524384, acc: 0.9351851940155029)
[2025-02-13 20:21:34,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:35,002][root][INFO] - Training Epoch: 2/2, step 262/7134 completed (loss: 0.03487817198038101, acc: 1.0)
[2025-02-13 20:21:35,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:35,418][root][INFO] - Training Epoch: 2/2, step 263/7134 completed (loss: 0.16781429946422577, acc: 0.9320987462997437)
[2025-02-13 20:21:35,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:35,862][root][INFO] - Training Epoch: 2/2, step 264/7134 completed (loss: 0.10829532891511917, acc: 0.9764705896377563)
[2025-02-13 20:21:36,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:36,265][root][INFO] - Training Epoch: 2/2, step 265/7134 completed (loss: 0.17216947674751282, acc: 0.9438202381134033)
[2025-02-13 20:21:36,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:36,650][root][INFO] - Training Epoch: 2/2, step 266/7134 completed (loss: 0.1729031205177307, acc: 0.9583333134651184)
[2025-02-13 20:21:36,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:37,052][root][INFO] - Training Epoch: 2/2, step 267/7134 completed (loss: 0.15939012169837952, acc: 0.9521276354789734)
[2025-02-13 20:21:37,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:37,436][root][INFO] - Training Epoch: 2/2, step 268/7134 completed (loss: 0.16462324559688568, acc: 0.9575757384300232)
[2025-02-13 20:21:37,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:37,816][root][INFO] - Training Epoch: 2/2, step 269/7134 completed (loss: 0.2577424943447113, acc: 0.9345238208770752)
[2025-02-13 20:21:37,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:38,179][root][INFO] - Training Epoch: 2/2, step 270/7134 completed (loss: 0.24652227759361267, acc: 0.9624999761581421)
[2025-02-13 20:21:38,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:38,565][root][INFO] - Training Epoch: 2/2, step 271/7134 completed (loss: 0.15423215925693512, acc: 0.9666666388511658)
[2025-02-13 20:21:38,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:38,937][root][INFO] - Training Epoch: 2/2, step 272/7134 completed (loss: 0.23752468824386597, acc: 0.9441340565681458)
[2025-02-13 20:21:39,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:39,301][root][INFO] - Training Epoch: 2/2, step 273/7134 completed (loss: 0.14095765352249146, acc: 0.9680851101875305)
[2025-02-13 20:21:39,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:39,698][root][INFO] - Training Epoch: 2/2, step 274/7134 completed (loss: 0.24328775703907013, acc: 0.9473684430122375)
[2025-02-13 20:21:39,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:40,111][root][INFO] - Training Epoch: 2/2, step 275/7134 completed (loss: 0.07339493185281754, acc: 0.9644669890403748)
[2025-02-13 20:21:40,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:40,508][root][INFO] - Training Epoch: 2/2, step 276/7134 completed (loss: 0.09026411175727844, acc: 0.9742268323898315)
[2025-02-13 20:21:40,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:40,904][root][INFO] - Training Epoch: 2/2, step 277/7134 completed (loss: 0.20589378476142883, acc: 0.9567567706108093)
[2025-02-13 20:21:41,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:41,283][root][INFO] - Training Epoch: 2/2, step 278/7134 completed (loss: 0.15794318914413452, acc: 0.9635416865348816)
[2025-02-13 20:21:41,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:41,656][root][INFO] - Training Epoch: 2/2, step 279/7134 completed (loss: 0.20415550470352173, acc: 0.9567567706108093)
[2025-02-13 20:21:41,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:42,017][root][INFO] - Training Epoch: 2/2, step 280/7134 completed (loss: 0.20048782229423523, acc: 0.9359999895095825)
[2025-02-13 20:21:42,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:42,412][root][INFO] - Training Epoch: 2/2, step 281/7134 completed (loss: 0.11057968437671661, acc: 0.9745762944221497)
[2025-02-13 20:21:42,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:42,843][root][INFO] - Training Epoch: 2/2, step 282/7134 completed (loss: 0.19245290756225586, acc: 0.9367088675498962)
[2025-02-13 20:21:42,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:43,232][root][INFO] - Training Epoch: 2/2, step 283/7134 completed (loss: 0.05325641855597496, acc: 0.9923664331436157)
[2025-02-13 20:21:43,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:43,619][root][INFO] - Training Epoch: 2/2, step 284/7134 completed (loss: 0.05760380998253822, acc: 0.9930070042610168)
[2025-02-13 20:21:43,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:44,001][root][INFO] - Training Epoch: 2/2, step 285/7134 completed (loss: 0.21118228137493134, acc: 0.9481481313705444)
[2025-02-13 20:21:44,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:44,406][root][INFO] - Training Epoch: 2/2, step 286/7134 completed (loss: 0.15984588861465454, acc: 0.9583333134651184)
[2025-02-13 20:21:44,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:44,798][root][INFO] - Training Epoch: 2/2, step 287/7134 completed (loss: 0.19357304275035858, acc: 0.9798657894134521)
[2025-02-13 20:21:44,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:45,197][root][INFO] - Training Epoch: 2/2, step 288/7134 completed (loss: 0.10725419223308563, acc: 0.960629940032959)
[2025-02-13 20:21:45,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:45,580][root][INFO] - Training Epoch: 2/2, step 289/7134 completed (loss: 0.0829186886548996, acc: 0.9767441749572754)
[2025-02-13 20:21:45,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:45,950][root][INFO] - Training Epoch: 2/2, step 290/7134 completed (loss: 0.31260234117507935, acc: 0.9285714030265808)
[2025-02-13 20:21:46,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:46,347][root][INFO] - Training Epoch: 2/2, step 291/7134 completed (loss: 0.3353383541107178, acc: 0.9251700639724731)
[2025-02-13 20:21:46,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:46,733][root][INFO] - Training Epoch: 2/2, step 292/7134 completed (loss: 0.4542402923107147, acc: 0.9012345671653748)
[2025-02-13 20:21:46,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:47,091][root][INFO] - Training Epoch: 2/2, step 293/7134 completed (loss: 0.30164802074432373, acc: 0.9292035102844238)
[2025-02-13 20:21:47,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:47,480][root][INFO] - Training Epoch: 2/2, step 294/7134 completed (loss: 0.21818530559539795, acc: 0.9237288236618042)
[2025-02-13 20:21:47,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:47,932][root][INFO] - Training Epoch: 2/2, step 295/7134 completed (loss: 0.44005388021469116, acc: 0.8974359035491943)
[2025-02-13 20:21:48,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:48,325][root][INFO] - Training Epoch: 2/2, step 296/7134 completed (loss: 0.06787879019975662, acc: 0.9924242496490479)
[2025-02-13 20:21:48,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:48,760][root][INFO] - Training Epoch: 2/2, step 297/7134 completed (loss: 0.1558821052312851, acc: 0.9586777091026306)
[2025-02-13 20:21:48,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:49,141][root][INFO] - Training Epoch: 2/2, step 298/7134 completed (loss: 0.07965726405382156, acc: 0.9846153855323792)
[2025-02-13 20:21:49,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:49,506][root][INFO] - Training Epoch: 2/2, step 299/7134 completed (loss: 0.06172468885779381, acc: 0.985401451587677)
[2025-02-13 20:21:49,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:49,878][root][INFO] - Training Epoch: 2/2, step 300/7134 completed (loss: 0.059671394526958466, acc: 0.9855072498321533)
[2025-02-13 20:21:50,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:50,259][root][INFO] - Training Epoch: 2/2, step 301/7134 completed (loss: 0.08026177436113358, acc: 0.9720279574394226)
[2025-02-13 20:21:50,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:50,613][root][INFO] - Training Epoch: 2/2, step 302/7134 completed (loss: 0.10592398047447205, acc: 0.9900990128517151)
[2025-02-13 20:21:50,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:50,958][root][INFO] - Training Epoch: 2/2, step 303/7134 completed (loss: 0.11426952481269836, acc: 0.9729729890823364)
[2025-02-13 20:21:51,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:51,308][root][INFO] - Training Epoch: 2/2, step 304/7134 completed (loss: 0.0878000259399414, acc: 0.9900990128517151)
[2025-02-13 20:21:51,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:51,673][root][INFO] - Training Epoch: 2/2, step 305/7134 completed (loss: 0.16364194452762604, acc: 0.9609375)
[2025-02-13 20:21:51,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:52,029][root][INFO] - Training Epoch: 2/2, step 306/7134 completed (loss: 0.06305134296417236, acc: 0.9739130139350891)
[2025-02-13 20:21:52,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:52,416][root][INFO] - Training Epoch: 2/2, step 307/7134 completed (loss: 0.16101601719856262, acc: 0.9345794320106506)
[2025-02-13 20:21:52,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:52,782][root][INFO] - Training Epoch: 2/2, step 308/7134 completed (loss: 0.10110428929328918, acc: 0.9685039520263672)
[2025-02-13 20:21:52,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:53,162][root][INFO] - Training Epoch: 2/2, step 309/7134 completed (loss: 0.10624852776527405, acc: 0.9734513163566589)
[2025-02-13 20:21:53,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:53,546][root][INFO] - Training Epoch: 2/2, step 310/7134 completed (loss: 0.10009045153856277, acc: 0.9844961166381836)
[2025-02-13 20:21:53,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:53,906][root][INFO] - Training Epoch: 2/2, step 311/7134 completed (loss: 0.11014329642057419, acc: 0.9624060392379761)
[2025-02-13 20:21:54,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:54,260][root][INFO] - Training Epoch: 2/2, step 312/7134 completed (loss: 0.022538864985108376, acc: 1.0)
[2025-02-13 20:21:54,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:54,643][root][INFO] - Training Epoch: 2/2, step 313/7134 completed (loss: 0.02681405283510685, acc: 1.0)
[2025-02-13 20:21:54,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:55,008][root][INFO] - Training Epoch: 2/2, step 314/7134 completed (loss: 0.05120488256216049, acc: 0.9925925731658936)
[2025-02-13 20:21:55,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:55,375][root][INFO] - Training Epoch: 2/2, step 315/7134 completed (loss: 0.20598334074020386, acc: 0.949999988079071)
[2025-02-13 20:21:55,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:55,753][root][INFO] - Training Epoch: 2/2, step 316/7134 completed (loss: 0.08834730833768845, acc: 0.9819819927215576)
[2025-02-13 20:21:55,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:56,144][root][INFO] - Training Epoch: 2/2, step 317/7134 completed (loss: 0.023112274706363678, acc: 1.0)
[2025-02-13 20:21:56,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:56,542][root][INFO] - Training Epoch: 2/2, step 318/7134 completed (loss: 0.0962919294834137, acc: 0.9731183052062988)
[2025-02-13 20:21:56,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:56,918][root][INFO] - Training Epoch: 2/2, step 319/7134 completed (loss: 0.14654628932476044, acc: 0.9627329111099243)
[2025-02-13 20:21:57,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:57,294][root][INFO] - Training Epoch: 2/2, step 320/7134 completed (loss: 0.1533568799495697, acc: 0.9642857313156128)
[2025-02-13 20:21:57,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:57,704][root][INFO] - Training Epoch: 2/2, step 321/7134 completed (loss: 0.10607884079217911, acc: 0.9655172228813171)
[2025-02-13 20:21:57,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:58,087][root][INFO] - Training Epoch: 2/2, step 322/7134 completed (loss: 0.07128437608480453, acc: 0.9803921580314636)
[2025-02-13 20:21:58,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:58,460][root][INFO] - Training Epoch: 2/2, step 323/7134 completed (loss: 0.2616930305957794, acc: 0.9285714030265808)
[2025-02-13 20:21:58,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:58,835][root][INFO] - Training Epoch: 2/2, step 324/7134 completed (loss: 0.20097331702709198, acc: 0.9398148059844971)
[2025-02-13 20:21:58,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:59,199][root][INFO] - Training Epoch: 2/2, step 325/7134 completed (loss: 0.13156946003437042, acc: 0.9659090638160706)
[2025-02-13 20:21:59,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:59,561][root][INFO] - Training Epoch: 2/2, step 326/7134 completed (loss: 0.09688068926334381, acc: 0.9775280952453613)
[2025-02-13 20:21:59,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:59,938][root][INFO] - Training Epoch: 2/2, step 327/7134 completed (loss: 0.07320304960012436, acc: 0.9747474789619446)
[2025-02-13 20:22:00,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:00,316][root][INFO] - Training Epoch: 2/2, step 328/7134 completed (loss: 0.1085018664598465, acc: 0.9750000238418579)
[2025-02-13 20:22:00,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:00,698][root][INFO] - Training Epoch: 2/2, step 329/7134 completed (loss: 0.07836935669183731, acc: 0.9800994992256165)
[2025-02-13 20:22:00,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:01,087][root][INFO] - Training Epoch: 2/2, step 330/7134 completed (loss: 0.0392305888235569, acc: 0.9912280440330505)
[2025-02-13 20:22:01,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:01,462][root][INFO] - Training Epoch: 2/2, step 331/7134 completed (loss: 0.06336815655231476, acc: 0.9791666865348816)
[2025-02-13 20:22:01,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:01,828][root][INFO] - Training Epoch: 2/2, step 332/7134 completed (loss: 0.033779989928007126, acc: 0.9950248599052429)
[2025-02-13 20:22:01,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:02,192][root][INFO] - Training Epoch: 2/2, step 333/7134 completed (loss: 0.190729558467865, acc: 0.9408602118492126)
[2025-02-13 20:22:02,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:02,566][root][INFO] - Training Epoch: 2/2, step 334/7134 completed (loss: 0.136475071310997, acc: 0.9560439586639404)
[2025-02-13 20:22:02,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:02,956][root][INFO] - Training Epoch: 2/2, step 335/7134 completed (loss: 0.11937128752470016, acc: 0.9729729890823364)
[2025-02-13 20:22:03,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:03,340][root][INFO] - Training Epoch: 2/2, step 336/7134 completed (loss: 0.272967666387558, acc: 0.9585492014884949)
[2025-02-13 20:22:03,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:03,714][root][INFO] - Training Epoch: 2/2, step 337/7134 completed (loss: 0.1367468386888504, acc: 0.9689440727233887)
[2025-02-13 20:22:03,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:04,079][root][INFO] - Training Epoch: 2/2, step 338/7134 completed (loss: 0.20513246953487396, acc: 0.9581151604652405)
[2025-02-13 20:22:04,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:04,447][root][INFO] - Training Epoch: 2/2, step 339/7134 completed (loss: 0.1646742969751358, acc: 0.9572192430496216)
[2025-02-13 20:22:04,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:04,888][root][INFO] - Training Epoch: 2/2, step 340/7134 completed (loss: 0.1181185394525528, acc: 0.9696969985961914)
[2025-02-13 20:22:05,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:05,286][root][INFO] - Training Epoch: 2/2, step 341/7134 completed (loss: 0.09934872388839722, acc: 0.9846938848495483)
[2025-02-13 20:22:05,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:05,666][root][INFO] - Training Epoch: 2/2, step 342/7134 completed (loss: 0.11535603553056717, acc: 0.9811320900917053)
[2025-02-13 20:22:05,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:06,044][root][INFO] - Training Epoch: 2/2, step 343/7134 completed (loss: 0.2001117467880249, acc: 0.9384615421295166)
[2025-02-13 20:22:06,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:06,432][root][INFO] - Training Epoch: 2/2, step 344/7134 completed (loss: 0.13007774949073792, acc: 0.9534883499145508)
[2025-02-13 20:22:06,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:06,850][root][INFO] - Training Epoch: 2/2, step 345/7134 completed (loss: 0.11983165144920349, acc: 0.9645389914512634)
[2025-02-13 20:22:06,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:07,232][root][INFO] - Training Epoch: 2/2, step 346/7134 completed (loss: 0.07170427590608597, acc: 0.9802631735801697)
[2025-02-13 20:22:07,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:07,628][root][INFO] - Training Epoch: 2/2, step 347/7134 completed (loss: 0.14835289120674133, acc: 0.9624999761581421)
[2025-02-13 20:22:07,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:08,010][root][INFO] - Training Epoch: 2/2, step 348/7134 completed (loss: 0.12405704706907272, acc: 0.9585798978805542)
[2025-02-13 20:22:08,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:08,402][root][INFO] - Training Epoch: 2/2, step 349/7134 completed (loss: 0.18651853501796722, acc: 0.9585798978805542)
[2025-02-13 20:22:08,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:08,814][root][INFO] - Training Epoch: 2/2, step 350/7134 completed (loss: 0.11334935575723648, acc: 0.9631578922271729)
[2025-02-13 20:22:08,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:09,238][root][INFO] - Training Epoch: 2/2, step 351/7134 completed (loss: 0.13410702347755432, acc: 0.97826087474823)
[2025-02-13 20:22:09,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:09,621][root][INFO] - Training Epoch: 2/2, step 352/7134 completed (loss: 0.0892031267285347, acc: 0.9707602262496948)
[2025-02-13 20:22:09,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:10,005][root][INFO] - Training Epoch: 2/2, step 353/7134 completed (loss: 0.1063670888543129, acc: 0.9754601120948792)
[2025-02-13 20:22:10,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:10,383][root][INFO] - Training Epoch: 2/2, step 354/7134 completed (loss: 0.08967996388673782, acc: 0.9597989916801453)
[2025-02-13 20:22:10,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:10,751][root][INFO] - Training Epoch: 2/2, step 355/7134 completed (loss: 0.2140585035085678, acc: 0.9467455744743347)
[2025-02-13 20:22:10,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:11,132][root][INFO] - Training Epoch: 2/2, step 356/7134 completed (loss: 0.09749796241521835, acc: 0.9640718698501587)
[2025-02-13 20:22:11,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:11,555][root][INFO] - Training Epoch: 2/2, step 357/7134 completed (loss: 0.07354845106601715, acc: 0.9767441749572754)
[2025-02-13 20:22:11,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:11,925][root][INFO] - Training Epoch: 2/2, step 358/7134 completed (loss: 0.0916961207985878, acc: 0.9772727489471436)
[2025-02-13 20:22:12,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:12,307][root][INFO] - Training Epoch: 2/2, step 359/7134 completed (loss: 0.10270750522613525, acc: 0.970588207244873)
[2025-02-13 20:22:12,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:12,698][root][INFO] - Training Epoch: 2/2, step 360/7134 completed (loss: 0.0807967558503151, acc: 0.9673202633857727)
[2025-02-13 20:22:12,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:13,069][root][INFO] - Training Epoch: 2/2, step 361/7134 completed (loss: 0.045455362647771835, acc: 0.987730085849762)
[2025-02-13 20:22:13,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:13,424][root][INFO] - Training Epoch: 2/2, step 362/7134 completed (loss: 0.05442372336983681, acc: 0.9873417615890503)
[2025-02-13 20:22:13,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:13,807][root][INFO] - Training Epoch: 2/2, step 363/7134 completed (loss: 0.1414613425731659, acc: 0.9726775884628296)
[2025-02-13 20:22:13,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:14,203][root][INFO] - Training Epoch: 2/2, step 364/7134 completed (loss: 0.08356965333223343, acc: 0.9805825352668762)
[2025-02-13 20:22:14,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:14,585][root][INFO] - Training Epoch: 2/2, step 365/7134 completed (loss: 0.05866507813334465, acc: 0.9873417615890503)
[2025-02-13 20:22:14,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:14,962][root][INFO] - Training Epoch: 2/2, step 366/7134 completed (loss: 0.056493766605854034, acc: 0.9745222926139832)
[2025-02-13 20:22:15,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:15,343][root][INFO] - Training Epoch: 2/2, step 367/7134 completed (loss: 0.021591268479824066, acc: 1.0)
[2025-02-13 20:22:15,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:15,749][root][INFO] - Training Epoch: 2/2, step 368/7134 completed (loss: 0.032957904040813446, acc: 0.9881656765937805)
[2025-02-13 20:22:15,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:16,119][root][INFO] - Training Epoch: 2/2, step 369/7134 completed (loss: 0.09118152409791946, acc: 0.9815950989723206)
[2025-02-13 20:22:16,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:16,497][root][INFO] - Training Epoch: 2/2, step 370/7134 completed (loss: 0.08994059264659882, acc: 0.9746835231781006)
[2025-02-13 20:22:16,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:16,862][root][INFO] - Training Epoch: 2/2, step 371/7134 completed (loss: 0.057462964206933975, acc: 0.9813664555549622)
[2025-02-13 20:22:17,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:17,240][root][INFO] - Training Epoch: 2/2, step 372/7134 completed (loss: 0.060946643352508545, acc: 0.9811320900917053)
[2025-02-13 20:22:17,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:17,619][root][INFO] - Training Epoch: 2/2, step 373/7134 completed (loss: 0.03781931474804878, acc: 0.9934640526771545)
[2025-02-13 20:22:17,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:17,994][root][INFO] - Training Epoch: 2/2, step 374/7134 completed (loss: 0.105082668364048, acc: 0.9729729890823364)
[2025-02-13 20:22:18,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:18,343][root][INFO] - Training Epoch: 2/2, step 375/7134 completed (loss: 0.24452628195285797, acc: 0.9521276354789734)
[2025-02-13 20:22:18,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:18,718][root][INFO] - Training Epoch: 2/2, step 376/7134 completed (loss: 0.15717843174934387, acc: 0.9555555582046509)
[2025-02-13 20:22:18,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:19,090][root][INFO] - Training Epoch: 2/2, step 377/7134 completed (loss: 0.10213208198547363, acc: 0.9608938694000244)
[2025-02-13 20:22:19,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:19,459][root][INFO] - Training Epoch: 2/2, step 378/7134 completed (loss: 0.16493703424930573, acc: 0.9435028433799744)
[2025-02-13 20:22:19,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:19,822][root][INFO] - Training Epoch: 2/2, step 379/7134 completed (loss: 0.04310791194438934, acc: 0.9754902124404907)
[2025-02-13 20:22:19,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:20,193][root][INFO] - Training Epoch: 2/2, step 380/7134 completed (loss: 0.24817237257957458, acc: 0.9484536051750183)
[2025-02-13 20:22:20,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:20,593][root][INFO] - Training Epoch: 2/2, step 381/7134 completed (loss: 0.14911986887454987, acc: 0.9631578922271729)
[2025-02-13 20:22:20,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:20,955][root][INFO] - Training Epoch: 2/2, step 382/7134 completed (loss: 0.08204759657382965, acc: 0.9692307710647583)
[2025-02-13 20:22:21,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:21,332][root][INFO] - Training Epoch: 2/2, step 383/7134 completed (loss: 0.08670864254236221, acc: 0.9689922332763672)
[2025-02-13 20:22:21,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:21,708][root][INFO] - Training Epoch: 2/2, step 384/7134 completed (loss: 0.14060471951961517, acc: 0.9558011293411255)
[2025-02-13 20:22:21,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:22,070][root][INFO] - Training Epoch: 2/2, step 385/7134 completed (loss: 0.04928690940141678, acc: 0.9803921580314636)
[2025-02-13 20:22:22,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:22,406][root][INFO] - Training Epoch: 2/2, step 386/7134 completed (loss: 0.029473254457116127, acc: 0.9909909963607788)
[2025-02-13 20:22:22,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:22,774][root][INFO] - Training Epoch: 2/2, step 387/7134 completed (loss: 0.08975358307361603, acc: 0.9644970297813416)
[2025-02-13 20:22:22,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:23,141][root][INFO] - Training Epoch: 2/2, step 388/7134 completed (loss: 0.06837157160043716, acc: 0.980861246585846)
[2025-02-13 20:22:23,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:23,512][root][INFO] - Training Epoch: 2/2, step 389/7134 completed (loss: 0.1448143571615219, acc: 0.9428571462631226)
[2025-02-13 20:22:23,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:23,860][root][INFO] - Training Epoch: 2/2, step 390/7134 completed (loss: 0.08092739433050156, acc: 0.9797297120094299)
[2025-02-13 20:22:23,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:24,216][root][INFO] - Training Epoch: 2/2, step 391/7134 completed (loss: 0.09244923293590546, acc: 0.9800000190734863)
[2025-02-13 20:22:24,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:24,620][root][INFO] - Training Epoch: 2/2, step 392/7134 completed (loss: 0.08690600842237473, acc: 0.9767441749572754)
[2025-02-13 20:22:24,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:24,982][root][INFO] - Training Epoch: 2/2, step 393/7134 completed (loss: 0.086051806807518, acc: 0.9679144620895386)
[2025-02-13 20:22:25,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:25,329][root][INFO] - Training Epoch: 2/2, step 394/7134 completed (loss: 0.09072891622781754, acc: 0.9770992398262024)
[2025-02-13 20:22:25,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:25,680][root][INFO] - Training Epoch: 2/2, step 395/7134 completed (loss: 0.18957608938217163, acc: 0.9734042286872864)
[2025-02-13 20:22:25,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:26,033][root][INFO] - Training Epoch: 2/2, step 396/7134 completed (loss: 0.05443214252591133, acc: 0.982758641242981)
[2025-02-13 20:22:26,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:26,418][root][INFO] - Training Epoch: 2/2, step 397/7134 completed (loss: 0.2226942926645279, acc: 0.95652174949646)
[2025-02-13 20:22:26,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:26,782][root][INFO] - Training Epoch: 2/2, step 398/7134 completed (loss: 0.06551653146743774, acc: 0.9797979593276978)
[2025-02-13 20:22:26,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:27,122][root][INFO] - Training Epoch: 2/2, step 399/7134 completed (loss: 0.07310522347688675, acc: 0.9810126423835754)
[2025-02-13 20:22:27,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:27,490][root][INFO] - Training Epoch: 2/2, step 400/7134 completed (loss: 0.29710420966148376, acc: 0.9303797483444214)
[2025-02-13 20:22:27,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:27,854][root][INFO] - Training Epoch: 2/2, step 401/7134 completed (loss: 0.112198106944561, acc: 0.9509803652763367)
[2025-02-13 20:22:27,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:28,220][root][INFO] - Training Epoch: 2/2, step 402/7134 completed (loss: 0.1375369131565094, acc: 0.9648241400718689)
[2025-02-13 20:22:28,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:28,606][root][INFO] - Training Epoch: 2/2, step 403/7134 completed (loss: 0.08200757205486298, acc: 0.9781659245491028)
[2025-02-13 20:22:28,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:28,961][root][INFO] - Training Epoch: 2/2, step 404/7134 completed (loss: 0.08350706100463867, acc: 0.9731183052062988)
[2025-02-13 20:22:29,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:29,324][root][INFO] - Training Epoch: 2/2, step 405/7134 completed (loss: 0.3118084967136383, acc: 0.895348846912384)
[2025-02-13 20:22:29,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:29,697][root][INFO] - Training Epoch: 2/2, step 406/7134 completed (loss: 0.42271724343299866, acc: 0.8972973227500916)
[2025-02-13 20:22:29,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:30,071][root][INFO] - Training Epoch: 2/2, step 407/7134 completed (loss: 0.4131337106227875, acc: 0.9171974658966064)
[2025-02-13 20:22:30,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:30,436][root][INFO] - Training Epoch: 2/2, step 408/7134 completed (loss: 0.20652751624584198, acc: 0.9385474920272827)
[2025-02-13 20:22:30,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:30,792][root][INFO] - Training Epoch: 2/2, step 409/7134 completed (loss: 0.13917198777198792, acc: 0.9631578922271729)
[2025-02-13 20:22:30,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:31,172][root][INFO] - Training Epoch: 2/2, step 410/7134 completed (loss: 0.18224963545799255, acc: 0.9636363387107849)
[2025-02-13 20:22:31,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:31,551][root][INFO] - Training Epoch: 2/2, step 411/7134 completed (loss: 0.11994216591119766, acc: 0.9684684872627258)
[2025-02-13 20:22:31,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:31,918][root][INFO] - Training Epoch: 2/2, step 412/7134 completed (loss: 0.08924488723278046, acc: 0.9730941653251648)
[2025-02-13 20:22:32,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:32,290][root][INFO] - Training Epoch: 2/2, step 413/7134 completed (loss: 0.13253049552440643, acc: 0.9685863852500916)
[2025-02-13 20:22:32,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:32,668][root][INFO] - Training Epoch: 2/2, step 414/7134 completed (loss: 0.14072436094284058, acc: 0.9595375657081604)
[2025-02-13 20:22:32,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:33,058][root][INFO] - Training Epoch: 2/2, step 415/7134 completed (loss: 0.10557717829942703, acc: 0.976190447807312)
[2025-02-13 20:22:33,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:33,451][root][INFO] - Training Epoch: 2/2, step 416/7134 completed (loss: 0.0776049792766571, acc: 0.9807692170143127)
[2025-02-13 20:22:33,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:33,816][root][INFO] - Training Epoch: 2/2, step 417/7134 completed (loss: 0.14465054869651794, acc: 0.9505494236946106)
[2025-02-13 20:22:33,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:34,213][root][INFO] - Training Epoch: 2/2, step 418/7134 completed (loss: 0.1002921536564827, acc: 0.9710144996643066)
[2025-02-13 20:22:34,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:34,599][root][INFO] - Training Epoch: 2/2, step 419/7134 completed (loss: 0.07041622698307037, acc: 0.9900990128517151)
[2025-02-13 20:22:34,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:34,981][root][INFO] - Training Epoch: 2/2, step 420/7134 completed (loss: 0.12712106108665466, acc: 0.949999988079071)
[2025-02-13 20:22:35,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:35,359][root][INFO] - Training Epoch: 2/2, step 421/7134 completed (loss: 0.07327357679605484, acc: 0.9874213933944702)
[2025-02-13 20:22:35,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:35,745][root][INFO] - Training Epoch: 2/2, step 422/7134 completed (loss: 0.08259277790784836, acc: 0.9526627063751221)
[2025-02-13 20:22:35,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:36,133][root][INFO] - Training Epoch: 2/2, step 423/7134 completed (loss: 0.023045381531119347, acc: 1.0)
[2025-02-13 20:22:36,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:36,511][root][INFO] - Training Epoch: 2/2, step 424/7134 completed (loss: 0.051300447434186935, acc: 0.9938271641731262)
[2025-02-13 20:22:36,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:36,891][root][INFO] - Training Epoch: 2/2, step 425/7134 completed (loss: 0.017496369779109955, acc: 1.0)
[2025-02-13 20:22:37,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:37,260][root][INFO] - Training Epoch: 2/2, step 426/7134 completed (loss: 0.023335224017500877, acc: 0.9941176176071167)
[2025-02-13 20:22:37,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:37,623][root][INFO] - Training Epoch: 2/2, step 427/7134 completed (loss: 0.04130847379565239, acc: 1.0)
[2025-02-13 20:22:37,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:38,012][root][INFO] - Training Epoch: 2/2, step 428/7134 completed (loss: 0.038934942334890366, acc: 0.9940119981765747)
[2025-02-13 20:22:38,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:38,396][root][INFO] - Training Epoch: 2/2, step 429/7134 completed (loss: 0.17736367881298065, acc: 0.9407894611358643)
[2025-02-13 20:22:38,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:38,764][root][INFO] - Training Epoch: 2/2, step 430/7134 completed (loss: 0.06554052978754044, acc: 0.9882352948188782)
[2025-02-13 20:22:38,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:39,149][root][INFO] - Training Epoch: 2/2, step 431/7134 completed (loss: 0.09259849786758423, acc: 0.9940828680992126)
[2025-02-13 20:22:39,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:39,513][root][INFO] - Training Epoch: 2/2, step 432/7134 completed (loss: 0.10398252308368683, acc: 0.9772727489471436)
[2025-02-13 20:22:39,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:39,892][root][INFO] - Training Epoch: 2/2, step 433/7134 completed (loss: 0.10563378781080246, acc: 0.978723406791687)
[2025-02-13 20:22:40,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:40,347][root][INFO] - Training Epoch: 2/2, step 434/7134 completed (loss: 0.06815174967050552, acc: 0.9938271641731262)
[2025-02-13 20:22:40,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:40,747][root][INFO] - Training Epoch: 2/2, step 435/7134 completed (loss: 0.06745489686727524, acc: 0.9722222089767456)
[2025-02-13 20:22:40,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:41,150][root][INFO] - Training Epoch: 2/2, step 436/7134 completed (loss: 0.13073840737342834, acc: 0.9791666865348816)
[2025-02-13 20:22:41,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:41,545][root][INFO] - Training Epoch: 2/2, step 437/7134 completed (loss: 0.16767188906669617, acc: 0.9718309640884399)
[2025-02-13 20:22:41,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:41,911][root][INFO] - Training Epoch: 2/2, step 438/7134 completed (loss: 0.04323667660355568, acc: 0.9921259880065918)
[2025-02-13 20:22:42,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:42,323][root][INFO] - Training Epoch: 2/2, step 439/7134 completed (loss: 0.05664704740047455, acc: 0.9885057210922241)
[2025-02-13 20:22:42,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:42,694][root][INFO] - Training Epoch: 2/2, step 440/7134 completed (loss: 0.13281847536563873, acc: 0.9538461565971375)
[2025-02-13 20:22:42,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:43,092][root][INFO] - Training Epoch: 2/2, step 441/7134 completed (loss: 0.05221012607216835, acc: 0.9931507110595703)
[2025-02-13 20:22:43,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:43,516][root][INFO] - Training Epoch: 2/2, step 442/7134 completed (loss: 0.01634170114994049, acc: 1.0)
[2025-02-13 20:22:43,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:43,904][root][INFO] - Training Epoch: 2/2, step 443/7134 completed (loss: 0.021639840677380562, acc: 0.9931972622871399)
[2025-02-13 20:22:44,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:44,272][root][INFO] - Training Epoch: 2/2, step 444/7134 completed (loss: 0.02522789128124714, acc: 0.9922480583190918)
[2025-02-13 20:22:44,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:44,671][root][INFO] - Training Epoch: 2/2, step 445/7134 completed (loss: 0.14579494297504425, acc: 0.9622641801834106)
[2025-02-13 20:22:44,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:45,059][root][INFO] - Training Epoch: 2/2, step 446/7134 completed (loss: 0.054387759417295456, acc: 0.9881656765937805)
[2025-02-13 20:22:45,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:45,431][root][INFO] - Training Epoch: 2/2, step 447/7134 completed (loss: 0.04450463876128197, acc: 0.987730085849762)
[2025-02-13 20:22:45,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:45,811][root][INFO] - Training Epoch: 2/2, step 448/7134 completed (loss: 0.15437237918376923, acc: 0.9682539701461792)
[2025-02-13 20:22:45,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:46,189][root][INFO] - Training Epoch: 2/2, step 449/7134 completed (loss: 0.17203393578529358, acc: 0.9468085169792175)
[2025-02-13 20:22:46,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:46,567][root][INFO] - Training Epoch: 2/2, step 450/7134 completed (loss: 0.0999617874622345, acc: 0.9764150977134705)
[2025-02-13 20:22:46,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:46,947][root][INFO] - Training Epoch: 2/2, step 451/7134 completed (loss: 0.2648794949054718, acc: 0.9364407062530518)
[2025-02-13 20:22:47,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:47,319][root][INFO] - Training Epoch: 2/2, step 452/7134 completed (loss: 0.2581627368927002, acc: 0.9213483333587646)
[2025-02-13 20:22:47,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:47,656][root][INFO] - Training Epoch: 2/2, step 453/7134 completed (loss: 0.117888905107975, acc: 0.9716312289237976)
[2025-02-13 20:22:47,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:48,031][root][INFO] - Training Epoch: 2/2, step 454/7134 completed (loss: 0.23681002855300903, acc: 0.949367105960846)
[2025-02-13 20:22:48,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:48,477][root][INFO] - Training Epoch: 2/2, step 455/7134 completed (loss: 0.20656555891036987, acc: 0.9487179517745972)
[2025-02-13 20:22:48,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:48,861][root][INFO] - Training Epoch: 2/2, step 456/7134 completed (loss: 0.082065649330616, acc: 0.9693251252174377)
[2025-02-13 20:22:49,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:49,264][root][INFO] - Training Epoch: 2/2, step 457/7134 completed (loss: 0.21181148290634155, acc: 0.9655172228813171)
[2025-02-13 20:22:49,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:49,631][root][INFO] - Training Epoch: 2/2, step 458/7134 completed (loss: 0.1350676715373993, acc: 0.9596773982048035)
[2025-02-13 20:22:49,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:49,985][root][INFO] - Training Epoch: 2/2, step 459/7134 completed (loss: 0.1805286705493927, acc: 0.9397590160369873)
[2025-02-13 20:22:50,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:50,371][root][INFO] - Training Epoch: 2/2, step 460/7134 completed (loss: 0.15238739550113678, acc: 0.9466666579246521)
[2025-02-13 20:22:50,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:50,798][root][INFO] - Training Epoch: 2/2, step 461/7134 completed (loss: 0.17870208621025085, acc: 0.9647058844566345)
[2025-02-13 20:22:50,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:51,201][root][INFO] - Training Epoch: 2/2, step 462/7134 completed (loss: 0.12071940302848816, acc: 0.9814814925193787)
[2025-02-13 20:22:51,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:51,588][root][INFO] - Training Epoch: 2/2, step 463/7134 completed (loss: 0.25289636850357056, acc: 0.9402984976768494)
[2025-02-13 20:22:51,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:51,966][root][INFO] - Training Epoch: 2/2, step 464/7134 completed (loss: 0.14251889288425446, acc: 0.9715909361839294)
[2025-02-13 20:22:52,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:52,337][root][INFO] - Training Epoch: 2/2, step 465/7134 completed (loss: 0.24284891784191132, acc: 0.9608938694000244)
[2025-02-13 20:22:52,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:52,713][root][INFO] - Training Epoch: 2/2, step 466/7134 completed (loss: 0.21850813925266266, acc: 0.9532163739204407)
[2025-02-13 20:22:52,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:53,075][root][INFO] - Training Epoch: 2/2, step 467/7134 completed (loss: 0.19235700368881226, acc: 0.9523809552192688)
[2025-02-13 20:22:53,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:53,447][root][INFO] - Training Epoch: 2/2, step 468/7134 completed (loss: 0.21285782754421234, acc: 0.9466666579246521)
[2025-02-13 20:22:53,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:53,815][root][INFO] - Training Epoch: 2/2, step 469/7134 completed (loss: 0.2519063949584961, acc: 0.9444444179534912)
[2025-02-13 20:22:53,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54,172][root][INFO] - Training Epoch: 2/2, step 470/7134 completed (loss: 0.18657450377941132, acc: 0.9594594836235046)
[2025-02-13 20:22:54,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54,548][root][INFO] - Training Epoch: 2/2, step 471/7134 completed (loss: 0.2019256204366684, acc: 0.9670329689979553)
[2025-02-13 20:22:54,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54,916][root][INFO] - Training Epoch: 2/2, step 472/7134 completed (loss: 0.1496819257736206, acc: 0.9599999785423279)
[2025-02-13 20:22:55,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:55,284][root][INFO] - Training Epoch: 2/2, step 473/7134 completed (loss: 0.15715622901916504, acc: 0.9772727489471436)
[2025-02-13 20:22:55,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:55,670][root][INFO] - Training Epoch: 2/2, step 474/7134 completed (loss: 0.19612117111682892, acc: 0.9515151381492615)
[2025-02-13 20:22:55,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56,035][root][INFO] - Training Epoch: 2/2, step 475/7134 completed (loss: 0.2583417594432831, acc: 0.9440559148788452)
[2025-02-13 20:22:56,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56,428][root][INFO] - Training Epoch: 2/2, step 476/7134 completed (loss: 0.22076353430747986, acc: 0.956250011920929)
[2025-02-13 20:22:56,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56,822][root][INFO] - Training Epoch: 2/2, step 477/7134 completed (loss: 0.20938600599765778, acc: 0.9465240836143494)
[2025-02-13 20:22:56,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:57,188][root][INFO] - Training Epoch: 2/2, step 478/7134 completed (loss: 0.3230658173561096, acc: 0.9304347634315491)
[2025-02-13 20:22:57,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:57,561][root][INFO] - Training Epoch: 2/2, step 479/7134 completed (loss: 0.2009543925523758, acc: 0.9470198750495911)
[2025-02-13 20:22:57,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:57,978][root][INFO] - Training Epoch: 2/2, step 480/7134 completed (loss: 0.12646590173244476, acc: 0.9624999761581421)
[2025-02-13 20:22:58,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:58,358][root][INFO] - Training Epoch: 2/2, step 481/7134 completed (loss: 0.09982086718082428, acc: 0.9813664555549622)
[2025-02-13 20:22:58,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:58,727][root][INFO] - Training Epoch: 2/2, step 482/7134 completed (loss: 0.11624782532453537, acc: 0.9622641801834106)
[2025-02-13 20:22:58,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:59,110][root][INFO] - Training Epoch: 2/2, step 483/7134 completed (loss: 0.10190049558877945, acc: 0.9615384340286255)
[2025-02-13 20:22:59,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:59,484][root][INFO] - Training Epoch: 2/2, step 484/7134 completed (loss: 0.22759774327278137, acc: 0.9387755393981934)
[2025-02-13 20:22:59,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:59,857][root][INFO] - Training Epoch: 2/2, step 485/7134 completed (loss: 0.3134891390800476, acc: 0.9130434989929199)
[2025-02-13 20:22:59,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:00,252][root][INFO] - Training Epoch: 2/2, step 486/7134 completed (loss: 0.16843266785144806, acc: 0.9612902998924255)
[2025-02-13 20:23:00,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:00,623][root][INFO] - Training Epoch: 2/2, step 487/7134 completed (loss: 0.1710902750492096, acc: 0.9753086566925049)
[2025-02-13 20:23:00,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:00,950][root][INFO] - Training Epoch: 2/2, step 488/7134 completed (loss: 0.30241963267326355, acc: 0.9552238583564758)
[2025-02-13 20:23:01,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:01,341][root][INFO] - Training Epoch: 2/2, step 489/7134 completed (loss: 0.4555690884590149, acc: 0.935251772403717)
[2025-02-13 20:23:01,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:01,696][root][INFO] - Training Epoch: 2/2, step 490/7134 completed (loss: 0.13606634736061096, acc: 0.9810126423835754)
[2025-02-13 20:23:01,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:02,074][root][INFO] - Training Epoch: 2/2, step 491/7134 completed (loss: 0.11356372386217117, acc: 0.9722222089767456)
[2025-02-13 20:23:02,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:02,481][root][INFO] - Training Epoch: 2/2, step 492/7134 completed (loss: 0.05359647795557976, acc: 0.9935483932495117)
[2025-02-13 20:23:02,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:02,901][root][INFO] - Training Epoch: 2/2, step 493/7134 completed (loss: 0.08234728872776031, acc: 0.9881656765937805)
[2025-02-13 20:23:03,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:03,301][root][INFO] - Training Epoch: 2/2, step 494/7134 completed (loss: 0.04141125828027725, acc: 0.9866666793823242)
[2025-02-13 20:23:03,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:03,690][root][INFO] - Training Epoch: 2/2, step 495/7134 completed (loss: 0.1132110059261322, acc: 0.9834710955619812)
[2025-02-13 20:23:03,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:04,085][root][INFO] - Training Epoch: 2/2, step 496/7134 completed (loss: 0.11493553221225739, acc: 0.9684210419654846)
[2025-02-13 20:23:04,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:04,514][root][INFO] - Training Epoch: 2/2, step 497/7134 completed (loss: 0.1169842854142189, acc: 0.9615384340286255)
[2025-02-13 20:23:04,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:04,895][root][INFO] - Training Epoch: 2/2, step 498/7134 completed (loss: 0.07548098266124725, acc: 0.9932885766029358)
[2025-02-13 20:23:05,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:05,290][root][INFO] - Training Epoch: 2/2, step 499/7134 completed (loss: 0.0735817551612854, acc: 0.9764705896377563)
[2025-02-13 20:23:05,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:05,671][root][INFO] - Training Epoch: 2/2, step 500/7134 completed (loss: 0.09230447560548782, acc: 0.9651162624359131)
[2025-02-13 20:23:05,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:06,058][root][INFO] - Training Epoch: 2/2, step 501/7134 completed (loss: 0.07949007302522659, acc: 0.9826589822769165)
[2025-02-13 20:23:06,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:06,446][root][INFO] - Training Epoch: 2/2, step 502/7134 completed (loss: 0.05904625728726387, acc: 0.9935064911842346)
[2025-02-13 20:23:06,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:06,826][root][INFO] - Training Epoch: 2/2, step 503/7134 completed (loss: 0.12637081742286682, acc: 0.9772727489471436)
[2025-02-13 20:23:06,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:07,210][root][INFO] - Training Epoch: 2/2, step 504/7134 completed (loss: 0.08517204970121384, acc: 0.9875776171684265)
[2025-02-13 20:23:07,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:07,603][root][INFO] - Training Epoch: 2/2, step 505/7134 completed (loss: 0.058012086898088455, acc: 0.988095223903656)
[2025-02-13 20:23:07,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:08,001][root][INFO] - Training Epoch: 2/2, step 506/7134 completed (loss: 0.07370208203792572, acc: 0.9829545617103577)
[2025-02-13 20:23:08,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:08,396][root][INFO] - Training Epoch: 2/2, step 507/7134 completed (loss: 0.08456495404243469, acc: 0.9836065769195557)
[2025-02-13 20:23:08,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:08,814][root][INFO] - Training Epoch: 2/2, step 508/7134 completed (loss: 0.13234208524227142, acc: 0.9647058844566345)
[2025-02-13 20:23:08,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:09,213][root][INFO] - Training Epoch: 2/2, step 509/7134 completed (loss: 0.12679821252822876, acc: 0.9594594836235046)
[2025-02-13 20:23:09,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:09,596][root][INFO] - Training Epoch: 2/2, step 510/7134 completed (loss: 0.3671860694885254, acc: 0.934959352016449)
[2025-02-13 20:23:09,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:09,958][root][INFO] - Training Epoch: 2/2, step 511/7134 completed (loss: 0.17806456983089447, acc: 0.9428571462631226)
[2025-02-13 20:23:10,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:10,326][root][INFO] - Training Epoch: 2/2, step 512/7134 completed (loss: 0.2790123522281647, acc: 0.9389312863349915)
[2025-02-13 20:23:10,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:10,707][root][INFO] - Training Epoch: 2/2, step 513/7134 completed (loss: 0.19720973074436188, acc: 0.9731543660163879)
[2025-02-13 20:23:10,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11,069][root][INFO] - Training Epoch: 2/2, step 514/7134 completed (loss: 0.31479403376579285, acc: 0.9166666865348816)
[2025-02-13 20:23:11,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11,467][root][INFO] - Training Epoch: 2/2, step 515/7134 completed (loss: 0.11540400236845016, acc: 0.9583333134651184)
[2025-02-13 20:23:11,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11,829][root][INFO] - Training Epoch: 2/2, step 516/7134 completed (loss: 0.1657598614692688, acc: 0.9491525292396545)
[2025-02-13 20:23:11,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:12,213][root][INFO] - Training Epoch: 2/2, step 517/7134 completed (loss: 0.22639811038970947, acc: 0.9637681245803833)
[2025-02-13 20:23:12,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:12,639][root][INFO] - Training Epoch: 2/2, step 518/7134 completed (loss: 0.19444240629673004, acc: 0.9801324605941772)
[2025-02-13 20:23:12,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:13,022][root][INFO] - Training Epoch: 2/2, step 519/7134 completed (loss: 0.07828164845705032, acc: 0.9739130139350891)
[2025-02-13 20:23:13,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:13,397][root][INFO] - Training Epoch: 2/2, step 520/7134 completed (loss: 0.06431257724761963, acc: 0.9736841917037964)
[2025-02-13 20:23:13,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:13,753][root][INFO] - Training Epoch: 2/2, step 521/7134 completed (loss: 0.11816512048244476, acc: 0.9824561476707458)
[2025-02-13 20:23:13,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:14,123][root][INFO] - Training Epoch: 2/2, step 522/7134 completed (loss: 0.08034798502922058, acc: 0.983146071434021)
[2025-02-13 20:23:14,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:14,509][root][INFO] - Training Epoch: 2/2, step 523/7134 completed (loss: 0.06546653807163239, acc: 0.9825581312179565)
[2025-02-13 20:23:14,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:14,885][root][INFO] - Training Epoch: 2/2, step 524/7134 completed (loss: 0.1277257800102234, acc: 0.9717513918876648)
[2025-02-13 20:23:15,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:15,247][root][INFO] - Training Epoch: 2/2, step 525/7134 completed (loss: 0.12355247139930725, acc: 0.9815950989723206)
[2025-02-13 20:23:15,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:15,625][root][INFO] - Training Epoch: 2/2, step 526/7134 completed (loss: 0.1254693865776062, acc: 0.9659090638160706)
[2025-02-13 20:23:15,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:15,990][root][INFO] - Training Epoch: 2/2, step 527/7134 completed (loss: 0.15634271502494812, acc: 0.984375)
[2025-02-13 20:23:16,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:16,390][root][INFO] - Training Epoch: 2/2, step 528/7134 completed (loss: 0.05026436969637871, acc: 0.982758641242981)
[2025-02-13 20:23:16,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:16,789][root][INFO] - Training Epoch: 2/2, step 529/7134 completed (loss: 0.1707002967596054, acc: 0.9426751732826233)
[2025-02-13 20:23:16,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:17,208][root][INFO] - Training Epoch: 2/2, step 530/7134 completed (loss: 0.1639164239168167, acc: 0.9538461565971375)
[2025-02-13 20:23:17,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:17,592][root][INFO] - Training Epoch: 2/2, step 531/7134 completed (loss: 0.264803946018219, acc: 0.9452054500579834)
[2025-02-13 20:23:17,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:17,971][root][INFO] - Training Epoch: 2/2, step 532/7134 completed (loss: 0.02137967385351658, acc: 1.0)
[2025-02-13 20:23:18,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:18,343][root][INFO] - Training Epoch: 2/2, step 533/7134 completed (loss: 0.3471096456050873, acc: 0.9047619104385376)
[2025-02-13 20:23:18,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:18,745][root][INFO] - Training Epoch: 2/2, step 534/7134 completed (loss: 0.07536540180444717, acc: 0.9880239367485046)
[2025-02-13 20:23:18,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19,178][root][INFO] - Training Epoch: 2/2, step 535/7134 completed (loss: 0.08445774018764496, acc: 0.9781022071838379)
[2025-02-13 20:23:19,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19,565][root][INFO] - Training Epoch: 2/2, step 536/7134 completed (loss: 0.06208360567688942, acc: 0.9939024448394775)
[2025-02-13 20:23:19,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19,949][root][INFO] - Training Epoch: 2/2, step 537/7134 completed (loss: 0.09610621631145477, acc: 0.9768785834312439)
[2025-02-13 20:23:20,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:20,368][root][INFO] - Training Epoch: 2/2, step 538/7134 completed (loss: 0.08700495958328247, acc: 0.9759036302566528)
[2025-02-13 20:23:20,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:20,792][root][INFO] - Training Epoch: 2/2, step 539/7134 completed (loss: 0.12018635869026184, acc: 0.9694656729698181)
[2025-02-13 20:23:20,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:21,161][root][INFO] - Training Epoch: 2/2, step 540/7134 completed (loss: 0.1340097337961197, acc: 0.9622641801834106)
[2025-02-13 20:23:21,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:21,542][root][INFO] - Training Epoch: 2/2, step 541/7134 completed (loss: 0.2297222763299942, acc: 0.9404761791229248)
[2025-02-13 20:23:21,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:21,904][root][INFO] - Training Epoch: 2/2, step 542/7134 completed (loss: 0.29253172874450684, acc: 0.9599999785423279)
[2025-02-13 20:23:22,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:22,310][root][INFO] - Training Epoch: 2/2, step 543/7134 completed (loss: 0.3035285770893097, acc: 0.9276315569877625)
[2025-02-13 20:23:22,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:22,717][root][INFO] - Training Epoch: 2/2, step 544/7134 completed (loss: 0.17234008014202118, acc: 0.9642857313156128)
[2025-02-13 20:23:22,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:23,130][root][INFO] - Training Epoch: 2/2, step 545/7134 completed (loss: 0.3190200924873352, acc: 0.9398496150970459)
[2025-02-13 20:23:23,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:23,558][root][INFO] - Training Epoch: 2/2, step 546/7134 completed (loss: 0.3261515200138092, acc: 0.9074074029922485)
[2025-02-13 20:23:23,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:23,960][root][INFO] - Training Epoch: 2/2, step 547/7134 completed (loss: 0.13689184188842773, acc: 0.9801324605941772)
[2025-02-13 20:23:24,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:24,360][root][INFO] - Training Epoch: 2/2, step 548/7134 completed (loss: 0.1038074642419815, acc: 0.9802631735801697)
[2025-02-13 20:23:24,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:24,746][root][INFO] - Training Epoch: 2/2, step 549/7134 completed (loss: 0.3277418315410614, acc: 0.9142857193946838)
[2025-02-13 20:23:24,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:25,138][root][INFO] - Training Epoch: 2/2, step 550/7134 completed (loss: 0.14983409643173218, acc: 0.960629940032959)
[2025-02-13 20:23:25,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:25,503][root][INFO] - Training Epoch: 2/2, step 551/7134 completed (loss: 0.1960369348526001, acc: 0.96875)
[2025-02-13 20:23:25,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:25,862][root][INFO] - Training Epoch: 2/2, step 552/7134 completed (loss: 0.10257631540298462, acc: 0.9727891087532043)
[2025-02-13 20:23:25,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:26,215][root][INFO] - Training Epoch: 2/2, step 553/7134 completed (loss: 0.15882080793380737, acc: 0.9578947424888611)
[2025-02-13 20:23:26,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:26,602][root][INFO] - Training Epoch: 2/2, step 554/7134 completed (loss: 0.19756169617176056, acc: 0.9617834687232971)
[2025-02-13 20:23:26,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:26,978][root][INFO] - Training Epoch: 2/2, step 555/7134 completed (loss: 0.3006752133369446, acc: 0.9220778942108154)
[2025-02-13 20:23:27,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:27,336][root][INFO] - Training Epoch: 2/2, step 556/7134 completed (loss: 0.12595604360103607, acc: 0.96875)
[2025-02-13 20:23:27,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:27,728][root][INFO] - Training Epoch: 2/2, step 557/7134 completed (loss: 0.18206821382045746, acc: 0.9503546357154846)
[2025-02-13 20:23:27,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28,105][root][INFO] - Training Epoch: 2/2, step 558/7134 completed (loss: 0.2556053698062897, acc: 0.9679487347602844)
[2025-02-13 20:23:28,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28,498][root][INFO] - Training Epoch: 2/2, step 559/7134 completed (loss: 0.24230916798114777, acc: 0.926174521446228)
[2025-02-13 20:23:28,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28,877][root][INFO] - Training Epoch: 2/2, step 560/7134 completed (loss: 0.05551542714238167, acc: 0.9837398529052734)
[2025-02-13 20:23:29,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:29,267][root][INFO] - Training Epoch: 2/2, step 561/7134 completed (loss: 0.2029167115688324, acc: 0.9437500238418579)
[2025-02-13 20:23:29,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:29,652][root][INFO] - Training Epoch: 2/2, step 562/7134 completed (loss: 0.10764380544424057, acc: 0.9740259647369385)
[2025-02-13 20:23:29,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:30,025][root][INFO] - Training Epoch: 2/2, step 563/7134 completed (loss: 0.37863776087760925, acc: 0.9020978808403015)
[2025-02-13 20:23:30,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:30,406][root][INFO] - Training Epoch: 2/2, step 564/7134 completed (loss: 0.2238633930683136, acc: 0.9473684430122375)
[2025-02-13 20:23:30,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:30,841][root][INFO] - Training Epoch: 2/2, step 565/7134 completed (loss: 0.12263017147779465, acc: 0.960629940032959)
[2025-02-13 20:23:30,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:31,240][root][INFO] - Training Epoch: 2/2, step 566/7134 completed (loss: 0.1647178828716278, acc: 0.9729729890823364)
[2025-02-13 20:23:31,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:31,669][root][INFO] - Training Epoch: 2/2, step 567/7134 completed (loss: 0.19493107497692108, acc: 0.940119743347168)
[2025-02-13 20:23:31,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:32,045][root][INFO] - Training Epoch: 2/2, step 568/7134 completed (loss: 0.1847875714302063, acc: 0.9578947424888611)
[2025-02-13 20:23:32,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:32,405][root][INFO] - Training Epoch: 2/2, step 569/7134 completed (loss: 0.12049629539251328, acc: 0.9776536226272583)
[2025-02-13 20:23:32,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:32,856][root][INFO] - Training Epoch: 2/2, step 570/7134 completed (loss: 0.19735415279865265, acc: 0.9402984976768494)
[2025-02-13 20:23:33,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:33,291][root][INFO] - Training Epoch: 2/2, step 571/7134 completed (loss: 0.07709775120019913, acc: 0.9835164546966553)
[2025-02-13 20:23:33,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:33,692][root][INFO] - Training Epoch: 2/2, step 572/7134 completed (loss: 0.08054255694150925, acc: 0.9828571677207947)
[2025-02-13 20:23:33,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:34,113][root][INFO] - Training Epoch: 2/2, step 573/7134 completed (loss: 0.12594519555568695, acc: 0.9757575988769531)
[2025-02-13 20:23:34,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:34,501][root][INFO] - Training Epoch: 2/2, step 574/7134 completed (loss: 0.07361564040184021, acc: 0.9881656765937805)
[2025-02-13 20:23:34,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:34,942][root][INFO] - Training Epoch: 2/2, step 575/7134 completed (loss: 0.09706389904022217, acc: 0.9707602262496948)
[2025-02-13 20:23:35,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:35,323][root][INFO] - Training Epoch: 2/2, step 576/7134 completed (loss: 0.08080818504095078, acc: 0.9838709831237793)
[2025-02-13 20:23:35,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:35,695][root][INFO] - Training Epoch: 2/2, step 577/7134 completed (loss: 0.13960987329483032, acc: 0.9454545378684998)
[2025-02-13 20:23:35,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:36,090][root][INFO] - Training Epoch: 2/2, step 578/7134 completed (loss: 0.11092791706323624, acc: 0.9751243591308594)
[2025-02-13 20:23:36,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:36,503][root][INFO] - Training Epoch: 2/2, step 579/7134 completed (loss: 0.15588682889938354, acc: 0.9731183052062988)
[2025-02-13 20:23:36,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:36,923][root][INFO] - Training Epoch: 2/2, step 580/7134 completed (loss: 0.3603293001651764, acc: 0.9388889074325562)
[2025-02-13 20:23:37,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:37,312][root][INFO] - Training Epoch: 2/2, step 581/7134 completed (loss: 0.06590677797794342, acc: 0.9934210777282715)
[2025-02-13 20:23:37,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:37,732][root][INFO] - Training Epoch: 2/2, step 582/7134 completed (loss: 0.10860180854797363, acc: 0.97826087474823)
[2025-02-13 20:23:37,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:38,169][root][INFO] - Training Epoch: 2/2, step 583/7134 completed (loss: 0.08392449468374252, acc: 0.9820359349250793)
[2025-02-13 20:23:38,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:38,589][root][INFO] - Training Epoch: 2/2, step 584/7134 completed (loss: 0.05671695992350578, acc: 0.9836065769195557)
[2025-02-13 20:23:38,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:38,957][root][INFO] - Training Epoch: 2/2, step 585/7134 completed (loss: 0.08066824078559875, acc: 0.9729729890823364)
[2025-02-13 20:23:39,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:39,359][root][INFO] - Training Epoch: 2/2, step 586/7134 completed (loss: 0.0746978297829628, acc: 0.9734042286872864)
[2025-02-13 20:23:39,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:39,749][root][INFO] - Training Epoch: 2/2, step 587/7134 completed (loss: 0.0739167258143425, acc: 0.9779005646705627)
[2025-02-13 20:23:39,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40,142][root][INFO] - Training Epoch: 2/2, step 588/7134 completed (loss: 0.06416556984186172, acc: 0.9797297120094299)
[2025-02-13 20:23:40,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40,538][root][INFO] - Training Epoch: 2/2, step 589/7134 completed (loss: 0.12323024123907089, acc: 0.9693251252174377)
[2025-02-13 20:23:40,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40,921][root][INFO] - Training Epoch: 2/2, step 590/7134 completed (loss: 0.0967748835682869, acc: 0.9735099077224731)
[2025-02-13 20:23:41,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:41,367][root][INFO] - Training Epoch: 2/2, step 591/7134 completed (loss: 0.08960124105215073, acc: 0.994350254535675)
[2025-02-13 20:23:41,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:41,772][root][INFO] - Training Epoch: 2/2, step 592/7134 completed (loss: 0.0620562806725502, acc: 0.9886363744735718)
[2025-02-13 20:23:41,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:42,177][root][INFO] - Training Epoch: 2/2, step 593/7134 completed (loss: 0.09806248545646667, acc: 0.9649122953414917)
[2025-02-13 20:23:42,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:42,578][root][INFO] - Training Epoch: 2/2, step 594/7134 completed (loss: 0.18586833775043488, acc: 0.9640718698501587)
[2025-02-13 20:23:42,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:42,999][root][INFO] - Training Epoch: 2/2, step 595/7134 completed (loss: 0.1212722435593605, acc: 0.9784946441650391)
[2025-02-13 20:23:43,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:43,378][root][INFO] - Training Epoch: 2/2, step 596/7134 completed (loss: 0.08432134985923767, acc: 0.9833333492279053)
[2025-02-13 20:23:43,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:43,772][root][INFO] - Training Epoch: 2/2, step 597/7134 completed (loss: 0.1175713986158371, acc: 0.9664804339408875)
[2025-02-13 20:23:43,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:44,160][root][INFO] - Training Epoch: 2/2, step 598/7134 completed (loss: 0.21617630124092102, acc: 0.9510869383811951)
[2025-02-13 20:23:44,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:44,569][root][INFO] - Training Epoch: 2/2, step 599/7134 completed (loss: 0.11497162282466888, acc: 0.9838709831237793)
[2025-02-13 20:23:44,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:44,994][root][INFO] - Training Epoch: 2/2, step 600/7134 completed (loss: 0.0942508801817894, acc: 0.9672130942344666)
[2025-02-13 20:23:45,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:45,406][root][INFO] - Training Epoch: 2/2, step 601/7134 completed (loss: 0.13063067197799683, acc: 0.9659090638160706)
[2025-02-13 20:23:45,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:45,773][root][INFO] - Training Epoch: 2/2, step 602/7134 completed (loss: 0.11931660771369934, acc: 0.9567567706108093)
[2025-02-13 20:23:45,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:46,180][root][INFO] - Training Epoch: 2/2, step 603/7134 completed (loss: 0.18746596574783325, acc: 0.9470899701118469)
[2025-02-13 20:23:46,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:46,543][root][INFO] - Training Epoch: 2/2, step 604/7134 completed (loss: 0.1405591517686844, acc: 0.9696969985961914)
[2025-02-13 20:23:46,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:46,961][root][INFO] - Training Epoch: 2/2, step 605/7134 completed (loss: 0.08105319738388062, acc: 0.9839572310447693)
[2025-02-13 20:23:47,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:47,371][root][INFO] - Training Epoch: 2/2, step 606/7134 completed (loss: 0.13388952612876892, acc: 0.9649999737739563)
[2025-02-13 20:23:47,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:47,760][root][INFO] - Training Epoch: 2/2, step 607/7134 completed (loss: 0.07258236408233643, acc: 0.9712643623352051)
[2025-02-13 20:23:47,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:48,171][root][INFO] - Training Epoch: 2/2, step 608/7134 completed (loss: 0.06363168358802795, acc: 0.9783783555030823)
[2025-02-13 20:23:48,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:48,573][root][INFO] - Training Epoch: 2/2, step 609/7134 completed (loss: 0.10330548137426376, acc: 0.9846153855323792)
[2025-02-13 20:23:48,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:48,969][root][INFO] - Training Epoch: 2/2, step 610/7134 completed (loss: 0.057333528995513916, acc: 0.9852216839790344)
[2025-02-13 20:23:49,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:49,349][root][INFO] - Training Epoch: 2/2, step 611/7134 completed (loss: 0.12829537689685822, acc: 0.9661017060279846)
[2025-02-13 20:23:49,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:49,746][root][INFO] - Training Epoch: 2/2, step 612/7134 completed (loss: 0.16448037326335907, acc: 0.9619565010070801)
[2025-02-13 20:23:49,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:50,136][root][INFO] - Training Epoch: 2/2, step 613/7134 completed (loss: 0.1089840829372406, acc: 0.9646464586257935)
[2025-02-13 20:23:50,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:50,508][root][INFO] - Training Epoch: 2/2, step 614/7134 completed (loss: 0.02537282183766365, acc: 1.0)
[2025-02-13 20:23:50,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:50,908][root][INFO] - Training Epoch: 2/2, step 615/7134 completed (loss: 0.044801078736782074, acc: 0.9894737005233765)
[2025-02-13 20:23:51,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:51,297][root][INFO] - Training Epoch: 2/2, step 616/7134 completed (loss: 0.06570760905742645, acc: 0.9723502397537231)
[2025-02-13 20:23:51,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:51,724][root][INFO] - Training Epoch: 2/2, step 617/7134 completed (loss: 0.07983456552028656, acc: 0.980861246585846)
[2025-02-13 20:23:51,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:52,120][root][INFO] - Training Epoch: 2/2, step 618/7134 completed (loss: 0.046896591782569885, acc: 0.9946523904800415)
[2025-02-13 20:23:52,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:52,517][root][INFO] - Training Epoch: 2/2, step 619/7134 completed (loss: 0.11985576897859573, acc: 0.9754098653793335)
[2025-02-13 20:23:52,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:52,907][root][INFO] - Training Epoch: 2/2, step 620/7134 completed (loss: 0.10803860425949097, acc: 0.9794520735740662)
[2025-02-13 20:23:53,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:53,267][root][INFO] - Training Epoch: 2/2, step 621/7134 completed (loss: 0.1739863008260727, acc: 0.95333331823349)
[2025-02-13 20:23:53,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:53,636][root][INFO] - Training Epoch: 2/2, step 622/7134 completed (loss: 0.10032077878713608, acc: 0.9736841917037964)
[2025-02-13 20:23:53,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:53,986][root][INFO] - Training Epoch: 2/2, step 623/7134 completed (loss: 0.35144880414009094, acc: 0.935251772403717)
[2025-02-13 20:23:54,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:54,406][root][INFO] - Training Epoch: 2/2, step 624/7134 completed (loss: 0.06850520521402359, acc: 0.9927007555961609)
[2025-02-13 20:23:54,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:54,807][root][INFO] - Training Epoch: 2/2, step 625/7134 completed (loss: 0.17553693056106567, acc: 0.949999988079071)
[2025-02-13 20:23:54,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:55,171][root][INFO] - Training Epoch: 2/2, step 626/7134 completed (loss: 0.1280466914176941, acc: 0.9760000109672546)
[2025-02-13 20:23:55,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:55,534][root][INFO] - Training Epoch: 2/2, step 627/7134 completed (loss: 0.14333680272102356, acc: 0.9863945841789246)
[2025-02-13 20:23:55,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:55,908][root][INFO] - Training Epoch: 2/2, step 628/7134 completed (loss: 0.04674888774752617, acc: 1.0)
[2025-02-13 20:23:56,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:56,295][root][INFO] - Training Epoch: 2/2, step 629/7134 completed (loss: 0.23533755540847778, acc: 0.957446813583374)
[2025-02-13 20:23:56,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:56,676][root][INFO] - Training Epoch: 2/2, step 630/7134 completed (loss: 0.226899191737175, acc: 0.9299362897872925)
[2025-02-13 20:23:56,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:57,038][root][INFO] - Training Epoch: 2/2, step 631/7134 completed (loss: 0.11197792738676071, acc: 0.9610389471054077)
[2025-02-13 20:23:57,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:57,422][root][INFO] - Training Epoch: 2/2, step 632/7134 completed (loss: 0.06811212748289108, acc: 0.98591548204422)
[2025-02-13 20:23:57,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:57,786][root][INFO] - Training Epoch: 2/2, step 633/7134 completed (loss: 0.056782376021146774, acc: 0.987500011920929)
[2025-02-13 20:23:57,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58,166][root][INFO] - Training Epoch: 2/2, step 634/7134 completed (loss: 0.09978292882442474, acc: 0.9740259647369385)
[2025-02-13 20:23:58,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58,584][root][INFO] - Training Epoch: 2/2, step 635/7134 completed (loss: 0.11893671005964279, acc: 0.9545454382896423)
[2025-02-13 20:23:58,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58,982][root][INFO] - Training Epoch: 2/2, step 636/7134 completed (loss: 0.11320306360721588, acc: 0.9820359349250793)
[2025-02-13 20:23:59,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:59,353][root][INFO] - Training Epoch: 2/2, step 637/7134 completed (loss: 0.09573729336261749, acc: 0.971222996711731)
[2025-02-13 20:23:59,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:59,734][root][INFO] - Training Epoch: 2/2, step 638/7134 completed (loss: 0.06318072229623795, acc: 0.9865771532058716)
[2025-02-13 20:23:59,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:00,121][root][INFO] - Training Epoch: 2/2, step 639/7134 completed (loss: 0.06022753193974495, acc: 0.9857142567634583)
[2025-02-13 20:24:00,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:00,543][root][INFO] - Training Epoch: 2/2, step 640/7134 completed (loss: 0.1833924949169159, acc: 0.9710144996643066)
[2025-02-13 20:24:00,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:00,954][root][INFO] - Training Epoch: 2/2, step 641/7134 completed (loss: 0.12204564362764359, acc: 0.9637681245803833)
[2025-02-13 20:24:01,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:01,383][root][INFO] - Training Epoch: 2/2, step 642/7134 completed (loss: 0.08092659711837769, acc: 0.9779411554336548)
[2025-02-13 20:24:01,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:01,771][root][INFO] - Training Epoch: 2/2, step 643/7134 completed (loss: 0.06743736565113068, acc: 0.983146071434021)
[2025-02-13 20:24:01,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:02,180][root][INFO] - Training Epoch: 2/2, step 644/7134 completed (loss: 0.04912359267473221, acc: 0.984375)
[2025-02-13 20:24:02,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:02,561][root][INFO] - Training Epoch: 2/2, step 645/7134 completed (loss: 0.04749387502670288, acc: 0.9793103337287903)
[2025-02-13 20:24:02,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:02,960][root][INFO] - Training Epoch: 2/2, step 646/7134 completed (loss: 0.10510629415512085, acc: 0.9810126423835754)
[2025-02-13 20:24:03,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:03,335][root][INFO] - Training Epoch: 2/2, step 647/7134 completed (loss: 0.01992672123014927, acc: 0.9925925731658936)
[2025-02-13 20:24:03,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:03,682][root][INFO] - Training Epoch: 2/2, step 648/7134 completed (loss: 0.2153073102235794, acc: 0.9461538195610046)
[2025-02-13 20:24:03,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04,071][root][INFO] - Training Epoch: 2/2, step 649/7134 completed (loss: 0.21034042537212372, acc: 0.9351351261138916)
[2025-02-13 20:24:04,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04,487][root][INFO] - Training Epoch: 2/2, step 650/7134 completed (loss: 0.2043028473854065, acc: 0.9452054500579834)
[2025-02-13 20:24:04,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04,866][root][INFO] - Training Epoch: 2/2, step 651/7134 completed (loss: 0.16962580382823944, acc: 0.9675324559211731)
[2025-02-13 20:24:05,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:05,239][root][INFO] - Training Epoch: 2/2, step 652/7134 completed (loss: 0.12122812122106552, acc: 0.9647887349128723)
[2025-02-13 20:24:05,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:05,606][root][INFO] - Training Epoch: 2/2, step 653/7134 completed (loss: 0.042552463710308075, acc: 1.0)
[2025-02-13 20:24:05,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:05,969][root][INFO] - Training Epoch: 2/2, step 654/7134 completed (loss: 0.23735634982585907, acc: 0.9774436354637146)
[2025-02-13 20:24:06,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:06,364][root][INFO] - Training Epoch: 2/2, step 655/7134 completed (loss: 0.05815839394927025, acc: 0.982758641242981)
[2025-02-13 20:24:06,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:06,760][root][INFO] - Training Epoch: 2/2, step 656/7134 completed (loss: 0.15192951261997223, acc: 0.939393937587738)
[2025-02-13 20:24:06,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:07,158][root][INFO] - Training Epoch: 2/2, step 657/7134 completed (loss: 0.18066389858722687, acc: 0.9647887349128723)
[2025-02-13 20:24:07,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:07,533][root][INFO] - Training Epoch: 2/2, step 658/7134 completed (loss: 0.09934577345848083, acc: 0.9830508232116699)
[2025-02-13 20:24:07,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:07,915][root][INFO] - Training Epoch: 2/2, step 659/7134 completed (loss: 0.07536694407463074, acc: 0.976047933101654)
[2025-02-13 20:24:08,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:08,294][root][INFO] - Training Epoch: 2/2, step 660/7134 completed (loss: 0.11605732887983322, acc: 0.970588207244873)
[2025-02-13 20:24:08,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:08,687][root][INFO] - Training Epoch: 2/2, step 661/7134 completed (loss: 0.14325422048568726, acc: 0.9576271176338196)
[2025-02-13 20:24:08,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09,069][root][INFO] - Training Epoch: 2/2, step 662/7134 completed (loss: 0.07104989886283875, acc: 0.9833333492279053)
[2025-02-13 20:24:09,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09,454][root][INFO] - Training Epoch: 2/2, step 663/7134 completed (loss: 0.10557230561971664, acc: 0.9854369163513184)
[2025-02-13 20:24:09,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09,806][root][INFO] - Training Epoch: 2/2, step 664/7134 completed (loss: 0.15519596636295319, acc: 0.9580419659614563)
[2025-02-13 20:24:09,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:10,166][root][INFO] - Training Epoch: 2/2, step 665/7134 completed (loss: 0.253960520029068, acc: 0.9453551769256592)
[2025-02-13 20:24:10,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:10,543][root][INFO] - Training Epoch: 2/2, step 666/7134 completed (loss: 0.1317766159772873, acc: 0.9657142758369446)
[2025-02-13 20:24:10,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:10,910][root][INFO] - Training Epoch: 2/2, step 667/7134 completed (loss: 0.15871767699718475, acc: 0.9640718698501587)
[2025-02-13 20:24:11,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:11,275][root][INFO] - Training Epoch: 2/2, step 668/7134 completed (loss: 0.11688391864299774, acc: 0.9781420826911926)
[2025-02-13 20:24:11,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:11,653][root][INFO] - Training Epoch: 2/2, step 669/7134 completed (loss: 0.1720409244298935, acc: 0.9720670580863953)
[2025-02-13 20:24:11,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:12,046][root][INFO] - Training Epoch: 2/2, step 670/7134 completed (loss: 0.1762700378894806, acc: 0.9497717022895813)
[2025-02-13 20:24:12,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:12,432][root][INFO] - Training Epoch: 2/2, step 671/7134 completed (loss: 0.09005893766880035, acc: 0.9773755669593811)
[2025-02-13 20:24:12,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:12,758][root][INFO] - Training Epoch: 2/2, step 672/7134 completed (loss: 0.14808252453804016, acc: 0.9624999761581421)
[2025-02-13 20:24:12,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:13,137][root][INFO] - Training Epoch: 2/2, step 673/7134 completed (loss: 0.1747339367866516, acc: 0.9552238583564758)
[2025-02-13 20:24:13,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:13,515][root][INFO] - Training Epoch: 2/2, step 674/7134 completed (loss: 0.1324552744626999, acc: 0.9634146094322205)
[2025-02-13 20:24:13,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:13,894][root][INFO] - Training Epoch: 2/2, step 675/7134 completed (loss: 0.10989260673522949, acc: 0.9723756909370422)
[2025-02-13 20:24:14,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:14,284][root][INFO] - Training Epoch: 2/2, step 676/7134 completed (loss: 0.07034170627593994, acc: 0.9836956262588501)
[2025-02-13 20:24:14,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:14,658][root][INFO] - Training Epoch: 2/2, step 677/7134 completed (loss: 0.056158993393182755, acc: 0.9894179701805115)
[2025-02-13 20:24:14,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15,032][root][INFO] - Training Epoch: 2/2, step 678/7134 completed (loss: 0.13967204093933105, acc: 0.9390243887901306)
[2025-02-13 20:24:15,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15,406][root][INFO] - Training Epoch: 2/2, step 679/7134 completed (loss: 0.04070878028869629, acc: 0.9924812316894531)
[2025-02-13 20:24:15,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15,805][root][INFO] - Training Epoch: 2/2, step 680/7134 completed (loss: 0.13159672915935516, acc: 0.9698492288589478)
[2025-02-13 20:24:15,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:16,193][root][INFO] - Training Epoch: 2/2, step 681/7134 completed (loss: 0.1198643296957016, acc: 0.9736841917037964)
[2025-02-13 20:24:16,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:16,558][root][INFO] - Training Epoch: 2/2, step 682/7134 completed (loss: 0.12541745603084564, acc: 0.9830508232116699)
[2025-02-13 20:24:16,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:16,950][root][INFO] - Training Epoch: 2/2, step 683/7134 completed (loss: 0.09183481335639954, acc: 0.9783783555030823)
[2025-02-13 20:24:17,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:17,335][root][INFO] - Training Epoch: 2/2, step 684/7134 completed (loss: 0.07598406821489334, acc: 0.9777777791023254)
[2025-02-13 20:24:17,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:17,725][root][INFO] - Training Epoch: 2/2, step 685/7134 completed (loss: 0.08290491253137589, acc: 0.9748743772506714)
[2025-02-13 20:24:17,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:18,124][root][INFO] - Training Epoch: 2/2, step 686/7134 completed (loss: 0.03122427687048912, acc: 0.994535505771637)
[2025-02-13 20:24:18,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:18,532][root][INFO] - Training Epoch: 2/2, step 687/7134 completed (loss: 0.053284548223018646, acc: 0.9921259880065918)
[2025-02-13 20:24:18,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:18,910][root][INFO] - Training Epoch: 2/2, step 688/7134 completed (loss: 0.021187392994761467, acc: 0.9936708807945251)
[2025-02-13 20:24:19,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:19,263][root][INFO] - Training Epoch: 2/2, step 689/7134 completed (loss: 0.04419687017798424, acc: 0.9925925731658936)
[2025-02-13 20:24:19,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:19,638][root][INFO] - Training Epoch: 2/2, step 690/7134 completed (loss: 0.10070536285638809, acc: 0.9677419066429138)
[2025-02-13 20:24:19,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:20,016][root][INFO] - Training Epoch: 2/2, step 691/7134 completed (loss: 0.14746172726154327, acc: 0.9675324559211731)
[2025-02-13 20:24:20,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:20,390][root][INFO] - Training Epoch: 2/2, step 692/7134 completed (loss: 0.1394374817609787, acc: 0.9644669890403748)
[2025-02-13 20:24:20,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:20,756][root][INFO] - Training Epoch: 2/2, step 693/7134 completed (loss: 0.12335062026977539, acc: 0.9716312289237976)
[2025-02-13 20:24:20,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:21,132][root][INFO] - Training Epoch: 2/2, step 694/7134 completed (loss: 0.18669018149375916, acc: 0.9757575988769531)
[2025-02-13 20:24:21,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:21,516][root][INFO] - Training Epoch: 2/2, step 695/7134 completed (loss: 0.18161366879940033, acc: 0.9402984976768494)
[2025-02-13 20:24:21,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:21,940][root][INFO] - Training Epoch: 2/2, step 696/7134 completed (loss: 0.13726045191287994, acc: 0.9622641801834106)
[2025-02-13 20:24:22,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:22,316][root][INFO] - Training Epoch: 2/2, step 697/7134 completed (loss: 0.11389194428920746, acc: 0.9619565010070801)
[2025-02-13 20:24:22,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:22,678][root][INFO] - Training Epoch: 2/2, step 698/7134 completed (loss: 0.05122552439570427, acc: 0.9848484992980957)
[2025-02-13 20:24:22,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:23,073][root][INFO] - Training Epoch: 2/2, step 699/7134 completed (loss: 0.0882822647690773, acc: 0.9863013625144958)
[2025-02-13 20:24:23,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:23,450][root][INFO] - Training Epoch: 2/2, step 700/7134 completed (loss: 0.13588349521160126, acc: 0.9520000219345093)
[2025-02-13 20:24:23,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:23,810][root][INFO] - Training Epoch: 2/2, step 701/7134 completed (loss: 0.2515506148338318, acc: 0.9304347634315491)
[2025-02-13 20:24:23,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:24,191][root][INFO] - Training Epoch: 2/2, step 702/7134 completed (loss: 0.15665054321289062, acc: 0.9536423683166504)
[2025-02-13 20:24:24,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:24,553][root][INFO] - Training Epoch: 2/2, step 703/7134 completed (loss: 0.1834137737751007, acc: 0.9527027010917664)
[2025-02-13 20:24:24,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:24,927][root][INFO] - Training Epoch: 2/2, step 704/7134 completed (loss: 0.1525341421365738, acc: 0.9675324559211731)
[2025-02-13 20:24:25,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:25,310][root][INFO] - Training Epoch: 2/2, step 705/7134 completed (loss: 0.18390344083309174, acc: 0.9587156176567078)
[2025-02-13 20:24:25,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:25,722][root][INFO] - Training Epoch: 2/2, step 706/7134 completed (loss: 0.043317802250385284, acc: 0.9952830076217651)
[2025-02-13 20:24:25,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:26,104][root][INFO] - Training Epoch: 2/2, step 707/7134 completed (loss: 0.1320100575685501, acc: 0.9702970385551453)
[2025-02-13 20:24:26,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:26,484][root][INFO] - Training Epoch: 2/2, step 708/7134 completed (loss: 0.09115514159202576, acc: 0.9719101190567017)
[2025-02-13 20:24:26,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:26,850][root][INFO] - Training Epoch: 2/2, step 709/7134 completed (loss: 0.11651364713907242, acc: 0.9599999785423279)
[2025-02-13 20:24:26,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:27,234][root][INFO] - Training Epoch: 2/2, step 710/7134 completed (loss: 0.07470214366912842, acc: 0.9791666865348816)
[2025-02-13 20:24:27,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:27,614][root][INFO] - Training Epoch: 2/2, step 711/7134 completed (loss: 0.09241129457950592, acc: 0.9656862616539001)
[2025-02-13 20:24:27,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:27,988][root][INFO] - Training Epoch: 2/2, step 712/7134 completed (loss: 0.041561782360076904, acc: 0.9895833134651184)
[2025-02-13 20:24:28,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:28,364][root][INFO] - Training Epoch: 2/2, step 713/7134 completed (loss: 0.06703199446201324, acc: 0.9849246144294739)
[2025-02-13 20:24:28,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:28,784][root][INFO] - Training Epoch: 2/2, step 714/7134 completed (loss: 0.08943802118301392, acc: 0.9700000286102295)
[2025-02-13 20:24:28,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:29,165][root][INFO] - Training Epoch: 2/2, step 715/7134 completed (loss: 0.07544007897377014, acc: 0.989130437374115)
[2025-02-13 20:24:29,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:29,537][root][INFO] - Training Epoch: 2/2, step 716/7134 completed (loss: 0.0342901386320591, acc: 0.9950980544090271)
[2025-02-13 20:24:29,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:29,951][root][INFO] - Training Epoch: 2/2, step 717/7134 completed (loss: 0.11792685091495514, acc: 0.9786096215248108)
[2025-02-13 20:24:30,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:30,349][root][INFO] - Training Epoch: 2/2, step 718/7134 completed (loss: 0.05540235713124275, acc: 0.9894179701805115)
[2025-02-13 20:24:30,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:30,757][root][INFO] - Training Epoch: 2/2, step 719/7134 completed (loss: 0.08662386238574982, acc: 0.9929078221321106)
[2025-02-13 20:24:30,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:31,131][root][INFO] - Training Epoch: 2/2, step 720/7134 completed (loss: 0.06807779520750046, acc: 0.9837837815284729)
[2025-02-13 20:24:31,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:31,506][root][INFO] - Training Epoch: 2/2, step 721/7134 completed (loss: 0.04065524786710739, acc: 0.9842105507850647)
[2025-02-13 20:24:31,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:31,896][root][INFO] - Training Epoch: 2/2, step 722/7134 completed (loss: 0.05717865005135536, acc: 0.9949495196342468)
[2025-02-13 20:24:32,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:32,271][root][INFO] - Training Epoch: 2/2, step 723/7134 completed (loss: 0.028154751285910606, acc: 0.9945651888847351)
[2025-02-13 20:24:32,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:32,634][root][INFO] - Training Epoch: 2/2, step 724/7134 completed (loss: 0.07075055688619614, acc: 0.9748427867889404)
[2025-02-13 20:24:32,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:33,006][root][INFO] - Training Epoch: 2/2, step 725/7134 completed (loss: 0.014672631397843361, acc: 1.0)
[2025-02-13 20:24:33,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:33,368][root][INFO] - Training Epoch: 2/2, step 726/7134 completed (loss: 0.012522787787020206, acc: 1.0)
[2025-02-13 20:24:33,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:33,754][root][INFO] - Training Epoch: 2/2, step 727/7134 completed (loss: 0.16027484834194183, acc: 0.9653179049491882)
[2025-02-13 20:24:33,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:34,143][root][INFO] - Training Epoch: 2/2, step 728/7134 completed (loss: 0.022651104256510735, acc: 0.9942857027053833)
[2025-02-13 20:24:34,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:34,506][root][INFO] - Training Epoch: 2/2, step 729/7134 completed (loss: 0.03312808275222778, acc: 0.9936708807945251)
[2025-02-13 20:24:34,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:34,881][root][INFO] - Training Epoch: 2/2, step 730/7134 completed (loss: 0.04441694915294647, acc: 0.9893617033958435)
[2025-02-13 20:24:35,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:35,258][root][INFO] - Training Epoch: 2/2, step 731/7134 completed (loss: 0.014255963265895844, acc: 1.0)
[2025-02-13 20:24:35,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:35,648][root][INFO] - Training Epoch: 2/2, step 732/7134 completed (loss: 0.145094633102417, acc: 0.9754098653793335)
[2025-02-13 20:24:35,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36,008][root][INFO] - Training Epoch: 2/2, step 733/7134 completed (loss: 0.09956327080726624, acc: 0.9769230484962463)
[2025-02-13 20:24:36,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36,368][root][INFO] - Training Epoch: 2/2, step 734/7134 completed (loss: 0.05191556364297867, acc: 0.9919354915618896)
[2025-02-13 20:24:36,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36,739][root][INFO] - Training Epoch: 2/2, step 735/7134 completed (loss: 0.07092408835887909, acc: 0.9647887349128723)
[2025-02-13 20:24:36,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:37,136][root][INFO] - Training Epoch: 2/2, step 736/7134 completed (loss: 0.17954592406749725, acc: 0.9504950642585754)
[2025-02-13 20:24:37,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:37,501][root][INFO] - Training Epoch: 2/2, step 737/7134 completed (loss: 0.16129790246486664, acc: 0.9322034120559692)
[2025-02-13 20:24:37,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:37,868][root][INFO] - Training Epoch: 2/2, step 738/7134 completed (loss: 0.03559854254126549, acc: 0.9905660152435303)
[2025-02-13 20:24:38,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:38,242][root][INFO] - Training Epoch: 2/2, step 739/7134 completed (loss: 0.09980832040309906, acc: 0.9652174115180969)
[2025-02-13 20:24:38,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:38,631][root][INFO] - Training Epoch: 2/2, step 740/7134 completed (loss: 0.06579954922199249, acc: 0.9837398529052734)
[2025-02-13 20:24:38,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:39,007][root][INFO] - Training Epoch: 2/2, step 741/7134 completed (loss: 0.10577474534511566, acc: 0.9629629850387573)
[2025-02-13 20:24:39,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:39,405][root][INFO] - Training Epoch: 2/2, step 742/7134 completed (loss: 0.06851574033498764, acc: 0.9894737005233765)
[2025-02-13 20:24:39,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:39,821][root][INFO] - Training Epoch: 2/2, step 743/7134 completed (loss: 0.18977373838424683, acc: 0.9387755393981934)
[2025-02-13 20:24:39,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:40,195][root][INFO] - Training Epoch: 2/2, step 744/7134 completed (loss: 0.04078911244869232, acc: 1.0)
[2025-02-13 20:24:40,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:40,598][root][INFO] - Training Epoch: 2/2, step 745/7134 completed (loss: 0.11125431954860687, acc: 0.9779411554336548)
[2025-02-13 20:24:40,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:40,977][root][INFO] - Training Epoch: 2/2, step 746/7134 completed (loss: 0.12104520946741104, acc: 0.9618320465087891)
[2025-02-13 20:24:41,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:41,337][root][INFO] - Training Epoch: 2/2, step 747/7134 completed (loss: 0.1584090143442154, acc: 0.9370078444480896)
[2025-02-13 20:24:41,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:41,732][root][INFO] - Training Epoch: 2/2, step 748/7134 completed (loss: 0.0910838320851326, acc: 0.9701492786407471)
[2025-02-13 20:24:41,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:42,115][root][INFO] - Training Epoch: 2/2, step 749/7134 completed (loss: 0.06274557113647461, acc: 0.9857142567634583)
[2025-02-13 20:24:42,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:42,529][root][INFO] - Training Epoch: 2/2, step 750/7134 completed (loss: 0.0871465876698494, acc: 0.9917355179786682)
[2025-02-13 20:24:42,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:42,936][root][INFO] - Training Epoch: 2/2, step 751/7134 completed (loss: 0.12978991866111755, acc: 0.9599999785423279)
[2025-02-13 20:24:43,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:43,303][root][INFO] - Training Epoch: 2/2, step 752/7134 completed (loss: 0.2783021032810211, acc: 0.9459459185600281)
[2025-02-13 20:24:43,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:43,679][root][INFO] - Training Epoch: 2/2, step 753/7134 completed (loss: 0.07668997347354889, acc: 0.984375)
[2025-02-13 20:24:43,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:44,059][root][INFO] - Training Epoch: 2/2, step 754/7134 completed (loss: 0.1009424552321434, acc: 0.97826087474823)
[2025-02-13 20:24:44,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:44,435][root][INFO] - Training Epoch: 2/2, step 755/7134 completed (loss: 0.17753298580646515, acc: 0.9760000109672546)
[2025-02-13 20:24:44,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:44,847][root][INFO] - Training Epoch: 2/2, step 756/7134 completed (loss: 0.036372870206832886, acc: 0.9910714030265808)
[2025-02-13 20:24:44,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:45,219][root][INFO] - Training Epoch: 2/2, step 757/7134 completed (loss: 0.038180120289325714, acc: 0.9903846383094788)
[2025-02-13 20:24:45,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:45,583][root][INFO] - Training Epoch: 2/2, step 758/7134 completed (loss: 0.041118260473012924, acc: 1.0)
[2025-02-13 20:24:45,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:45,949][root][INFO] - Training Epoch: 2/2, step 759/7134 completed (loss: 0.09813112765550613, acc: 0.954954981803894)
[2025-02-13 20:24:46,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:46,316][root][INFO] - Training Epoch: 2/2, step 760/7134 completed (loss: 0.04150819033384323, acc: 0.9924242496490479)
[2025-02-13 20:24:46,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:46,675][root][INFO] - Training Epoch: 2/2, step 761/7134 completed (loss: 0.10912769287824631, acc: 0.9640718698501587)
[2025-02-13 20:24:46,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:47,033][root][INFO] - Training Epoch: 2/2, step 762/7134 completed (loss: 0.09631886333227158, acc: 0.9710144996643066)
[2025-02-13 20:24:47,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:47,435][root][INFO] - Training Epoch: 2/2, step 763/7134 completed (loss: 0.15320304036140442, acc: 0.9736841917037964)
[2025-02-13 20:24:47,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:47,815][root][INFO] - Training Epoch: 2/2, step 764/7134 completed (loss: 0.0428459458053112, acc: 0.9881656765937805)
[2025-02-13 20:24:47,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:48,204][root][INFO] - Training Epoch: 2/2, step 765/7134 completed (loss: 0.026722105219960213, acc: 0.9937106966972351)
[2025-02-13 20:24:48,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:48,588][root][INFO] - Training Epoch: 2/2, step 766/7134 completed (loss: 0.09090624749660492, acc: 0.96875)
[2025-02-13 20:24:48,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:48,950][root][INFO] - Training Epoch: 2/2, step 767/7134 completed (loss: 0.08425155282020569, acc: 0.976190447807312)
[2025-02-13 20:24:49,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:49,326][root][INFO] - Training Epoch: 2/2, step 768/7134 completed (loss: 0.1866942197084427, acc: 0.9672130942344666)
[2025-02-13 20:24:49,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:49,718][root][INFO] - Training Epoch: 2/2, step 769/7134 completed (loss: 0.07387005537748337, acc: 0.9875776171684265)
[2025-02-13 20:24:49,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50,084][root][INFO] - Training Epoch: 2/2, step 770/7134 completed (loss: 0.06329518556594849, acc: 0.9814814925193787)
[2025-02-13 20:24:50,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50,450][root][INFO] - Training Epoch: 2/2, step 771/7134 completed (loss: 0.03885125368833542, acc: 0.9927007555961609)
[2025-02-13 20:24:50,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50,849][root][INFO] - Training Epoch: 2/2, step 772/7134 completed (loss: 0.08305760473012924, acc: 0.9727891087532043)
[2025-02-13 20:24:51,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:51,235][root][INFO] - Training Epoch: 2/2, step 773/7134 completed (loss: 0.13403573632240295, acc: 0.9631578922271729)
[2025-02-13 20:24:51,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:51,605][root][INFO] - Training Epoch: 2/2, step 774/7134 completed (loss: 0.12864188849925995, acc: 0.9577465057373047)
[2025-02-13 20:24:51,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:51,998][root][INFO] - Training Epoch: 2/2, step 775/7134 completed (loss: 0.024316158145666122, acc: 0.9946808218955994)
[2025-02-13 20:24:52,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:52,369][root][INFO] - Training Epoch: 2/2, step 776/7134 completed (loss: 0.022250818088650703, acc: 0.993630588054657)
[2025-02-13 20:24:52,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:52,721][root][INFO] - Training Epoch: 2/2, step 777/7134 completed (loss: 0.05751204863190651, acc: 0.9733333587646484)
[2025-02-13 20:24:52,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:53,103][root][INFO] - Training Epoch: 2/2, step 778/7134 completed (loss: 0.07062534987926483, acc: 0.9937106966972351)
[2025-02-13 20:24:53,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:53,474][root][INFO] - Training Epoch: 2/2, step 779/7134 completed (loss: 0.06397230923175812, acc: 0.988304078578949)
[2025-02-13 20:24:53,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:53,875][root][INFO] - Training Epoch: 2/2, step 780/7134 completed (loss: 0.1436944454908371, acc: 0.9459459185600281)
[2025-02-13 20:24:54,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:54,263][root][INFO] - Training Epoch: 2/2, step 781/7134 completed (loss: 0.12893353402614594, acc: 0.9754902124404907)
[2025-02-13 20:24:54,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:54,621][root][INFO] - Training Epoch: 2/2, step 782/7134 completed (loss: 0.1172887459397316, acc: 0.9683544039726257)
[2025-02-13 20:24:54,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:55,006][root][INFO] - Training Epoch: 2/2, step 783/7134 completed (loss: 0.10977989435195923, acc: 0.9719626307487488)
[2025-02-13 20:24:55,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:55,380][root][INFO] - Training Epoch: 2/2, step 784/7134 completed (loss: 0.05219579115509987, acc: 0.9900497794151306)
[2025-02-13 20:24:55,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:55,765][root][INFO] - Training Epoch: 2/2, step 785/7134 completed (loss: 0.18519212305545807, acc: 0.9466666579246521)
[2025-02-13 20:24:55,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:56,138][root][INFO] - Training Epoch: 2/2, step 786/7134 completed (loss: 0.12196612358093262, acc: 0.9701492786407471)
[2025-02-13 20:24:56,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:56,530][root][INFO] - Training Epoch: 2/2, step 787/7134 completed (loss: 0.02070002816617489, acc: 1.0)
[2025-02-13 20:24:56,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:56,941][root][INFO] - Training Epoch: 2/2, step 788/7134 completed (loss: 0.18276026844978333, acc: 0.9441340565681458)
[2025-02-13 20:24:57,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:57,292][root][INFO] - Training Epoch: 2/2, step 789/7134 completed (loss: 0.07494615763425827, acc: 0.9743589758872986)
[2025-02-13 20:24:57,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:57,697][root][INFO] - Training Epoch: 2/2, step 790/7134 completed (loss: 0.12941202521324158, acc: 0.9729729890823364)
[2025-02-13 20:24:57,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:58,108][root][INFO] - Training Epoch: 2/2, step 791/7134 completed (loss: 0.23257826268672943, acc: 0.934959352016449)
[2025-02-13 20:24:58,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:58,488][root][INFO] - Training Epoch: 2/2, step 792/7134 completed (loss: 0.11476520448923111, acc: 0.9620253443717957)
[2025-02-13 20:24:58,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:58,867][root][INFO] - Training Epoch: 2/2, step 793/7134 completed (loss: 0.0553240105509758, acc: 0.9937499761581421)
[2025-02-13 20:24:59,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:59,280][root][INFO] - Training Epoch: 2/2, step 794/7134 completed (loss: 0.12135285139083862, acc: 0.9568965435028076)
[2025-02-13 20:24:59,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:59,664][root][INFO] - Training Epoch: 2/2, step 795/7134 completed (loss: 0.06782186031341553, acc: 0.9873417615890503)
[2025-02-13 20:24:59,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:00,037][root][INFO] - Training Epoch: 2/2, step 796/7134 completed (loss: 0.11038754880428314, acc: 0.9809523820877075)
[2025-02-13 20:25:00,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:00,423][root][INFO] - Training Epoch: 2/2, step 797/7134 completed (loss: 0.15313921868801117, acc: 0.960629940032959)
[2025-02-13 20:25:00,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:00,786][root][INFO] - Training Epoch: 2/2, step 798/7134 completed (loss: 0.058681920170784, acc: 0.9898989796638489)
[2025-02-13 20:25:00,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:01,147][root][INFO] - Training Epoch: 2/2, step 799/7134 completed (loss: 0.0754241943359375, acc: 0.9752066135406494)
[2025-02-13 20:25:01,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:01,523][root][INFO] - Training Epoch: 2/2, step 800/7134 completed (loss: 0.12002193182706833, acc: 0.970370352268219)
[2025-02-13 20:25:01,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:01,910][root][INFO] - Training Epoch: 2/2, step 801/7134 completed (loss: 0.25876855850219727, acc: 0.9548386931419373)
[2025-02-13 20:25:02,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:02,287][root][INFO] - Training Epoch: 2/2, step 802/7134 completed (loss: 0.10872849076986313, acc: 0.9716312289237976)
[2025-02-13 20:25:02,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:02,696][root][INFO] - Training Epoch: 2/2, step 803/7134 completed (loss: 0.07253793627023697, acc: 0.97826087474823)
[2025-02-13 20:25:02,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:03,067][root][INFO] - Training Epoch: 2/2, step 804/7134 completed (loss: 0.2380560040473938, acc: 0.9534883499145508)
[2025-02-13 20:25:03,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:03,458][root][INFO] - Training Epoch: 2/2, step 805/7134 completed (loss: 0.208060622215271, acc: 0.9370629191398621)
[2025-02-13 20:25:03,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:03,819][root][INFO] - Training Epoch: 2/2, step 806/7134 completed (loss: 0.06672538816928864, acc: 0.9814814925193787)
[2025-02-13 20:25:03,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:04,199][root][INFO] - Training Epoch: 2/2, step 807/7134 completed (loss: 0.13411462306976318, acc: 0.9599999785423279)
[2025-02-13 20:25:04,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:04,581][root][INFO] - Training Epoch: 2/2, step 808/7134 completed (loss: 0.05812570080161095, acc: 0.9796954393386841)
[2025-02-13 20:25:04,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:04,968][root][INFO] - Training Epoch: 2/2, step 809/7134 completed (loss: 0.04822955280542374, acc: 0.9866666793823242)
[2025-02-13 20:25:05,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:05,368][root][INFO] - Training Epoch: 2/2, step 810/7134 completed (loss: 0.14017724990844727, acc: 0.9682539701461792)
[2025-02-13 20:25:05,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:05,726][root][INFO] - Training Epoch: 2/2, step 811/7134 completed (loss: 0.23684774339199066, acc: 0.9448275566101074)
[2025-02-13 20:25:05,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:06,134][root][INFO] - Training Epoch: 2/2, step 812/7134 completed (loss: 0.0777573361992836, acc: 0.9789473414421082)
[2025-02-13 20:25:06,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:06,513][root][INFO] - Training Epoch: 2/2, step 813/7134 completed (loss: 0.15206278860569, acc: 0.9588235020637512)
[2025-02-13 20:25:06,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:06,901][root][INFO] - Training Epoch: 2/2, step 814/7134 completed (loss: 0.10561054199934006, acc: 0.9774011373519897)
[2025-02-13 20:25:07,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:07,275][root][INFO] - Training Epoch: 2/2, step 815/7134 completed (loss: 0.0669998750090599, acc: 0.9871794581413269)
[2025-02-13 20:25:07,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:07,657][root][INFO] - Training Epoch: 2/2, step 816/7134 completed (loss: 0.1622539609670639, acc: 0.9645389914512634)
[2025-02-13 20:25:07,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:08,039][root][INFO] - Training Epoch: 2/2, step 817/7134 completed (loss: 0.15517298877239227, acc: 0.9568345546722412)
[2025-02-13 20:25:08,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:08,404][root][INFO] - Training Epoch: 2/2, step 818/7134 completed (loss: 0.2984262704849243, acc: 0.9281045794487)
[2025-02-13 20:25:08,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:08,777][root][INFO] - Training Epoch: 2/2, step 819/7134 completed (loss: 0.24877069890499115, acc: 0.931506872177124)
[2025-02-13 20:25:08,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:09,177][root][INFO] - Training Epoch: 2/2, step 820/7134 completed (loss: 0.1729714721441269, acc: 0.9415584206581116)
[2025-02-13 20:25:09,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:09,578][root][INFO] - Training Epoch: 2/2, step 821/7134 completed (loss: 0.11319240927696228, acc: 0.9710982441902161)
[2025-02-13 20:25:09,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:09,980][root][INFO] - Training Epoch: 2/2, step 822/7134 completed (loss: 0.0802338719367981, acc: 0.9810126423835754)
[2025-02-13 20:25:10,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:10,358][root][INFO] - Training Epoch: 2/2, step 823/7134 completed (loss: 0.08839186280965805, acc: 0.9756097793579102)
[2025-02-13 20:25:10,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:10,733][root][INFO] - Training Epoch: 2/2, step 824/7134 completed (loss: 0.14008334279060364, acc: 0.9621211886405945)
[2025-02-13 20:25:10,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:11,121][root][INFO] - Training Epoch: 2/2, step 825/7134 completed (loss: 0.10050084441900253, acc: 0.9772727489471436)
[2025-02-13 20:25:11,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:11,504][root][INFO] - Training Epoch: 2/2, step 826/7134 completed (loss: 0.06508450955152512, acc: 0.9824561476707458)
[2025-02-13 20:25:11,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:11,858][root][INFO] - Training Epoch: 2/2, step 827/7134 completed (loss: 0.08371028304100037, acc: 0.9766082167625427)
[2025-02-13 20:25:11,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:12,213][root][INFO] - Training Epoch: 2/2, step 828/7134 completed (loss: 0.09356318414211273, acc: 0.9805194735527039)
[2025-02-13 20:25:12,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:12,588][root][INFO] - Training Epoch: 2/2, step 829/7134 completed (loss: 0.13372060656547546, acc: 0.9711538553237915)
[2025-02-13 20:25:12,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:12,972][root][INFO] - Training Epoch: 2/2, step 830/7134 completed (loss: 0.06841816008090973, acc: 0.9784172773361206)
[2025-02-13 20:25:13,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:13,349][root][INFO] - Training Epoch: 2/2, step 831/7134 completed (loss: 0.0806489884853363, acc: 0.9819276928901672)
[2025-02-13 20:25:13,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:13,759][root][INFO] - Training Epoch: 2/2, step 832/7134 completed (loss: 0.19877870380878448, acc: 0.9591836929321289)
[2025-02-13 20:25:13,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:14,140][root][INFO] - Training Epoch: 2/2, step 833/7134 completed (loss: 0.03069429099559784, acc: 0.9940476417541504)
[2025-02-13 20:25:14,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:14,521][root][INFO] - Training Epoch: 2/2, step 834/7134 completed (loss: 0.08811035007238388, acc: 0.970588207244873)
[2025-02-13 20:25:14,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:14,932][root][INFO] - Training Epoch: 2/2, step 835/7134 completed (loss: 0.14993008971214294, acc: 0.9649122953414917)
[2025-02-13 20:25:15,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:15,331][root][INFO] - Training Epoch: 2/2, step 836/7134 completed (loss: 0.1081685870885849, acc: 0.9746192693710327)
[2025-02-13 20:25:15,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:15,710][root][INFO] - Training Epoch: 2/2, step 837/7134 completed (loss: 0.13447345793247223, acc: 0.9774011373519897)
[2025-02-13 20:25:15,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:16,083][root][INFO] - Training Epoch: 2/2, step 838/7134 completed (loss: 0.08043433725833893, acc: 0.9738562107086182)
[2025-02-13 20:25:16,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:16,459][root][INFO] - Training Epoch: 2/2, step 839/7134 completed (loss: 0.08012969046831131, acc: 0.9504950642585754)
[2025-02-13 20:25:16,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:16,846][root][INFO] - Training Epoch: 2/2, step 840/7134 completed (loss: 0.06264091283082962, acc: 0.9820359349250793)
[2025-02-13 20:25:16,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:17,238][root][INFO] - Training Epoch: 2/2, step 841/7134 completed (loss: 0.0403706394135952, acc: 0.9941520690917969)
[2025-02-13 20:25:17,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:17,640][root][INFO] - Training Epoch: 2/2, step 842/7134 completed (loss: 0.03953992947936058, acc: 0.9823529124259949)
[2025-02-13 20:25:17,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:18,020][root][INFO] - Training Epoch: 2/2, step 843/7134 completed (loss: 0.03946191817522049, acc: 0.988304078578949)
[2025-02-13 20:25:18,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:18,382][root][INFO] - Training Epoch: 2/2, step 844/7134 completed (loss: 0.07951198518276215, acc: 0.9863013625144958)
[2025-02-13 20:25:18,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:18,751][root][INFO] - Training Epoch: 2/2, step 845/7134 completed (loss: 0.046349212527275085, acc: 0.9777777791023254)
[2025-02-13 20:25:18,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:19,118][root][INFO] - Training Epoch: 2/2, step 846/7134 completed (loss: 0.028492676094174385, acc: 1.0)
[2025-02-13 20:25:19,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:19,492][root][INFO] - Training Epoch: 2/2, step 847/7134 completed (loss: 0.15653598308563232, acc: 0.970059871673584)
[2025-02-13 20:25:19,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:19,870][root][INFO] - Training Epoch: 2/2, step 848/7134 completed (loss: 0.1342361569404602, acc: 0.9707602262496948)
[2025-02-13 20:25:20,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:20,252][root][INFO] - Training Epoch: 2/2, step 849/7134 completed (loss: 0.05986601486802101, acc: 0.9817073345184326)
[2025-02-13 20:25:20,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:20,631][root][INFO] - Training Epoch: 2/2, step 850/7134 completed (loss: 0.03528032451868057, acc: 0.9932885766029358)
[2025-02-13 20:25:20,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21,016][root][INFO] - Training Epoch: 2/2, step 851/7134 completed (loss: 0.11949639022350311, acc: 0.9709302186965942)
[2025-02-13 20:25:21,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21,418][root][INFO] - Training Epoch: 2/2, step 852/7134 completed (loss: 0.06960295140743256, acc: 0.9947643876075745)
[2025-02-13 20:25:21,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21,801][root][INFO] - Training Epoch: 2/2, step 853/7134 completed (loss: 0.0984286442399025, acc: 0.9738219976425171)
[2025-02-13 20:25:21,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:22,175][root][INFO] - Training Epoch: 2/2, step 854/7134 completed (loss: 0.07494422048330307, acc: 0.9878048896789551)
[2025-02-13 20:25:22,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:22,555][root][INFO] - Training Epoch: 2/2, step 855/7134 completed (loss: 0.06870923936367035, acc: 0.9768785834312439)
[2025-02-13 20:25:22,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:22,921][root][INFO] - Training Epoch: 2/2, step 856/7134 completed (loss: 0.034524064511060715, acc: 1.0)
[2025-02-13 20:25:23,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:23,282][root][INFO] - Training Epoch: 2/2, step 857/7134 completed (loss: 0.08528240025043488, acc: 0.9714285731315613)
[2025-02-13 20:25:23,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:23,648][root][INFO] - Training Epoch: 2/2, step 858/7134 completed (loss: 0.12971261143684387, acc: 0.9702380895614624)
[2025-02-13 20:25:23,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:24,027][root][INFO] - Training Epoch: 2/2, step 859/7134 completed (loss: 0.10482438653707504, acc: 0.9830508232116699)
[2025-02-13 20:25:24,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:24,397][root][INFO] - Training Epoch: 2/2, step 860/7134 completed (loss: 0.14669091999530792, acc: 0.9735099077224731)
[2025-02-13 20:25:24,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:24,791][root][INFO] - Training Epoch: 2/2, step 861/7134 completed (loss: 0.10431158542633057, acc: 0.9784946441650391)
[2025-02-13 20:25:24,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:25,180][root][INFO] - Training Epoch: 2/2, step 862/7134 completed (loss: 0.1380055844783783, acc: 0.9657142758369446)
[2025-02-13 20:25:25,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:25,564][root][INFO] - Training Epoch: 2/2, step 863/7134 completed (loss: 0.14197750389575958, acc: 0.9617486596107483)
[2025-02-13 20:25:25,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:25,930][root][INFO] - Training Epoch: 2/2, step 864/7134 completed (loss: 0.14362485706806183, acc: 0.9682539701461792)
[2025-02-13 20:25:26,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:26,305][root][INFO] - Training Epoch: 2/2, step 865/7134 completed (loss: 0.18273603916168213, acc: 0.9651162624359131)
[2025-02-13 20:25:26,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:26,679][root][INFO] - Training Epoch: 2/2, step 866/7134 completed (loss: 0.20996496081352234, acc: 0.949999988079071)
[2025-02-13 20:25:26,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:27,035][root][INFO] - Training Epoch: 2/2, step 867/7134 completed (loss: 0.17178860306739807, acc: 0.96875)
[2025-02-13 20:25:27,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:27,452][root][INFO] - Training Epoch: 2/2, step 868/7134 completed (loss: 0.3459755778312683, acc: 0.930232584476471)
[2025-02-13 20:25:27,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:27,842][root][INFO] - Training Epoch: 2/2, step 869/7134 completed (loss: 0.17282646894454956, acc: 0.9397590160369873)
[2025-02-13 20:25:27,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:28,214][root][INFO] - Training Epoch: 2/2, step 870/7134 completed (loss: 0.0833704024553299, acc: 0.9766082167625427)
[2025-02-13 20:25:28,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:28,598][root][INFO] - Training Epoch: 2/2, step 871/7134 completed (loss: 0.1505650281906128, acc: 0.9589040875434875)
[2025-02-13 20:25:28,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:28,971][root][INFO] - Training Epoch: 2/2, step 872/7134 completed (loss: 0.04425148665904999, acc: 0.9930555820465088)
[2025-02-13 20:25:29,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:29,339][root][INFO] - Training Epoch: 2/2, step 873/7134 completed (loss: 0.16861487925052643, acc: 0.9454545378684998)
[2025-02-13 20:25:29,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:29,716][root][INFO] - Training Epoch: 2/2, step 874/7134 completed (loss: 0.17222356796264648, acc: 0.9583333134651184)
[2025-02-13 20:25:29,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:30,077][root][INFO] - Training Epoch: 2/2, step 875/7134 completed (loss: 0.030498819425702095, acc: 0.9928057789802551)
[2025-02-13 20:25:30,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:30,423][root][INFO] - Training Epoch: 2/2, step 876/7134 completed (loss: 0.08077367395162582, acc: 0.9802631735801697)
[2025-02-13 20:25:30,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:30,800][root][INFO] - Training Epoch: 2/2, step 877/7134 completed (loss: 0.2182818055152893, acc: 0.9447852969169617)
[2025-02-13 20:25:30,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:31,175][root][INFO] - Training Epoch: 2/2, step 878/7134 completed (loss: 0.12001631408929825, acc: 0.96875)
[2025-02-13 20:25:31,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:31,583][root][INFO] - Training Epoch: 2/2, step 879/7134 completed (loss: 0.13651028275489807, acc: 0.9681528806686401)
[2025-02-13 20:25:31,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:31,957][root][INFO] - Training Epoch: 2/2, step 880/7134 completed (loss: 0.11071129888296127, acc: 0.9693251252174377)
[2025-02-13 20:25:32,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:32,317][root][INFO] - Training Epoch: 2/2, step 881/7134 completed (loss: 0.07735268026590347, acc: 0.9837398529052734)
[2025-02-13 20:25:32,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:32,695][root][INFO] - Training Epoch: 2/2, step 882/7134 completed (loss: 0.09489016234874725, acc: 0.9838709831237793)
[2025-02-13 20:25:32,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:33,024][root][INFO] - Training Epoch: 2/2, step 883/7134 completed (loss: 0.13282066583633423, acc: 0.9736841917037964)
[2025-02-13 20:25:33,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:33,380][root][INFO] - Training Epoch: 2/2, step 884/7134 completed (loss: 0.043997034430503845, acc: 0.9940119981765747)
[2025-02-13 20:25:33,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:33,745][root][INFO] - Training Epoch: 2/2, step 885/7134 completed (loss: 0.022663664072752, acc: 0.9939024448394775)
[2025-02-13 20:25:33,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:34,120][root][INFO] - Training Epoch: 2/2, step 886/7134 completed (loss: 0.1074366495013237, acc: 0.9915966391563416)
[2025-02-13 20:25:34,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:34,487][root][INFO] - Training Epoch: 2/2, step 887/7134 completed (loss: 0.08360664546489716, acc: 0.9850746393203735)
[2025-02-13 20:25:34,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:34,876][root][INFO] - Training Epoch: 2/2, step 888/7134 completed (loss: 0.21387545764446259, acc: 0.9865771532058716)
[2025-02-13 20:25:35,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:35,265][root][INFO] - Training Epoch: 2/2, step 889/7134 completed (loss: 0.029368853196501732, acc: 1.0)
[2025-02-13 20:25:35,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:35,651][root][INFO] - Training Epoch: 2/2, step 890/7134 completed (loss: 0.14232981204986572, acc: 0.9463087320327759)
[2025-02-13 20:25:35,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:36,030][root][INFO] - Training Epoch: 2/2, step 891/7134 completed (loss: 0.1819738745689392, acc: 0.9664804339408875)
[2025-02-13 20:25:36,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:36,407][root][INFO] - Training Epoch: 2/2, step 892/7134 completed (loss: 0.09923696517944336, acc: 0.976190447807312)
[2025-02-13 20:25:36,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:36,794][root][INFO] - Training Epoch: 2/2, step 893/7134 completed (loss: 0.1653391271829605, acc: 0.9673202633857727)
[2025-02-13 20:25:36,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:37,178][root][INFO] - Training Epoch: 2/2, step 894/7134 completed (loss: 0.035052429884672165, acc: 0.994350254535675)
[2025-02-13 20:25:37,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:37,526][root][INFO] - Training Epoch: 2/2, step 895/7134 completed (loss: 0.04027695581316948, acc: 0.9863013625144958)
[2025-02-13 20:25:37,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:37,886][root][INFO] - Training Epoch: 2/2, step 896/7134 completed (loss: 0.028937775641679764, acc: 1.0)
[2025-02-13 20:25:38,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:38,265][root][INFO] - Training Epoch: 2/2, step 897/7134 completed (loss: 0.09570403397083282, acc: 0.9832402467727661)
[2025-02-13 20:25:38,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:38,658][root][INFO] - Training Epoch: 2/2, step 898/7134 completed (loss: 0.1669411063194275, acc: 0.9602272510528564)
[2025-02-13 20:25:38,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:39,030][root][INFO] - Training Epoch: 2/2, step 899/7134 completed (loss: 0.09453658014535904, acc: 0.9625668525695801)
[2025-02-13 20:25:39,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:39,399][root][INFO] - Training Epoch: 2/2, step 900/7134 completed (loss: 0.05290168151259422, acc: 0.9939024448394775)
[2025-02-13 20:25:39,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:39,801][root][INFO] - Training Epoch: 2/2, step 901/7134 completed (loss: 0.12715719640254974, acc: 0.9642857313156128)
[2025-02-13 20:25:39,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:40,182][root][INFO] - Training Epoch: 2/2, step 902/7134 completed (loss: 0.0725819319486618, acc: 0.9805194735527039)
[2025-02-13 20:25:40,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:40,559][root][INFO] - Training Epoch: 2/2, step 903/7134 completed (loss: 0.028273548930883408, acc: 0.9942196607589722)
[2025-02-13 20:25:40,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:40,940][root][INFO] - Training Epoch: 2/2, step 904/7134 completed (loss: 0.10810113698244095, acc: 0.9830508232116699)
[2025-02-13 20:25:41,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:41,318][root][INFO] - Training Epoch: 2/2, step 905/7134 completed (loss: 0.03986860439181328, acc: 0.9942528605461121)
[2025-02-13 20:25:41,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:41,698][root][INFO] - Training Epoch: 2/2, step 906/7134 completed (loss: 0.07265444844961166, acc: 0.9779005646705627)
[2025-02-13 20:25:41,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:42,079][root][INFO] - Training Epoch: 2/2, step 907/7134 completed (loss: 0.07079494744539261, acc: 0.9784946441650391)
[2025-02-13 20:25:42,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:42,485][root][INFO] - Training Epoch: 2/2, step 908/7134 completed (loss: 0.04111652448773384, acc: 0.9888888597488403)
[2025-02-13 20:25:42,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:42,850][root][INFO] - Training Epoch: 2/2, step 909/7134 completed (loss: 0.05915386602282524, acc: 0.9802631735801697)
[2025-02-13 20:25:43,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:43,236][root][INFO] - Training Epoch: 2/2, step 910/7134 completed (loss: 0.07242672145366669, acc: 0.981249988079071)
[2025-02-13 20:25:43,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:43,607][root][INFO] - Training Epoch: 2/2, step 911/7134 completed (loss: 0.07255567610263824, acc: 0.9870967864990234)
[2025-02-13 20:25:43,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:43,993][root][INFO] - Training Epoch: 2/2, step 912/7134 completed (loss: 0.11432112753391266, acc: 0.9736841917037964)
[2025-02-13 20:25:44,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:44,388][root][INFO] - Training Epoch: 2/2, step 913/7134 completed (loss: 0.042611267417669296, acc: 0.9890109896659851)
[2025-02-13 20:25:44,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:44,772][root][INFO] - Training Epoch: 2/2, step 914/7134 completed (loss: 0.0965077206492424, acc: 0.9757575988769531)
[2025-02-13 20:25:44,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:45,154][root][INFO] - Training Epoch: 2/2, step 915/7134 completed (loss: 0.08440003544092178, acc: 0.9857142567634583)
[2025-02-13 20:25:45,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:45,508][root][INFO] - Training Epoch: 2/2, step 916/7134 completed (loss: 0.05861185863614082, acc: 0.9844961166381836)
[2025-02-13 20:25:45,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:45,859][root][INFO] - Training Epoch: 2/2, step 917/7134 completed (loss: 0.1499166488647461, acc: 0.9545454382896423)
[2025-02-13 20:25:45,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:46,241][root][INFO] - Training Epoch: 2/2, step 918/7134 completed (loss: 0.09005466848611832, acc: 0.9735099077224731)
[2025-02-13 20:25:46,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:46,639][root][INFO] - Training Epoch: 2/2, step 919/7134 completed (loss: 0.05332866311073303, acc: 0.9870129823684692)
[2025-02-13 20:25:46,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:47,023][root][INFO] - Training Epoch: 2/2, step 920/7134 completed (loss: 0.08721690624952316, acc: 0.9767441749572754)
[2025-02-13 20:25:47,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:47,405][root][INFO] - Training Epoch: 2/2, step 921/7134 completed (loss: 0.058857399970293045, acc: 0.988304078578949)
[2025-02-13 20:25:47,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:47,780][root][INFO] - Training Epoch: 2/2, step 922/7134 completed (loss: 0.029800020158290863, acc: 0.9925373196601868)
[2025-02-13 20:25:47,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:48,197][root][INFO] - Training Epoch: 2/2, step 923/7134 completed (loss: 0.07792629301548004, acc: 0.9766082167625427)
[2025-02-13 20:25:48,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:48,599][root][INFO] - Training Epoch: 2/2, step 924/7134 completed (loss: 0.0561080239713192, acc: 0.9830508232116699)
[2025-02-13 20:25:48,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:48,975][root][INFO] - Training Epoch: 2/2, step 925/7134 completed (loss: 0.29844123125076294, acc: 0.9419354796409607)
[2025-02-13 20:25:49,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:49,357][root][INFO] - Training Epoch: 2/2, step 926/7134 completed (loss: 0.11434691399335861, acc: 0.9722222089767456)
[2025-02-13 20:25:49,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:49,747][root][INFO] - Training Epoch: 2/2, step 927/7134 completed (loss: 0.19155049324035645, acc: 0.9496402740478516)
[2025-02-13 20:25:49,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:50,129][root][INFO] - Training Epoch: 2/2, step 928/7134 completed (loss: 0.01408995222300291, acc: 1.0)
[2025-02-13 20:25:50,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:50,503][root][INFO] - Training Epoch: 2/2, step 929/7134 completed (loss: 0.05936617776751518, acc: 0.9868420958518982)
[2025-02-13 20:25:50,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:50,920][root][INFO] - Training Epoch: 2/2, step 930/7134 completed (loss: 0.16011150181293488, acc: 0.9596773982048035)
[2025-02-13 20:25:51,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:51,290][root][INFO] - Training Epoch: 2/2, step 931/7134 completed (loss: 0.09258828312158585, acc: 0.9852941036224365)
[2025-02-13 20:25:51,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:51,698][root][INFO] - Training Epoch: 2/2, step 932/7134 completed (loss: 0.0632498487830162, acc: 0.988095223903656)
[2025-02-13 20:25:51,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:52,114][root][INFO] - Training Epoch: 2/2, step 933/7134 completed (loss: 0.13291531801223755, acc: 0.9640718698501587)
[2025-02-13 20:25:52,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:52,486][root][INFO] - Training Epoch: 2/2, step 934/7134 completed (loss: 0.05341552570462227, acc: 0.9825581312179565)
[2025-02-13 20:25:52,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:52,850][root][INFO] - Training Epoch: 2/2, step 935/7134 completed (loss: 0.019435426220297813, acc: 1.0)
[2025-02-13 20:25:52,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:53,219][root][INFO] - Training Epoch: 2/2, step 936/7134 completed (loss: 0.08927302807569504, acc: 0.9921259880065918)
[2025-02-13 20:25:53,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:53,595][root][INFO] - Training Epoch: 2/2, step 937/7134 completed (loss: 0.05805601179599762, acc: 0.9918699264526367)
[2025-02-13 20:25:53,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:53,967][root][INFO] - Training Epoch: 2/2, step 938/7134 completed (loss: 0.07137658447027206, acc: 0.9807692170143127)
[2025-02-13 20:25:54,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:54,336][root][INFO] - Training Epoch: 2/2, step 939/7134 completed (loss: 0.12704400718212128, acc: 0.9695122241973877)
[2025-02-13 20:25:54,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:54,731][root][INFO] - Training Epoch: 2/2, step 940/7134 completed (loss: 0.04852156713604927, acc: 1.0)
[2025-02-13 20:25:54,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:55,147][root][INFO] - Training Epoch: 2/2, step 941/7134 completed (loss: 0.11854308843612671, acc: 0.9775280952453613)
[2025-02-13 20:25:55,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:55,549][root][INFO] - Training Epoch: 2/2, step 942/7134 completed (loss: 0.20242589712142944, acc: 0.9651162624359131)
[2025-02-13 20:25:55,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:55,937][root][INFO] - Training Epoch: 2/2, step 943/7134 completed (loss: 0.07971303164958954, acc: 0.9938650131225586)
[2025-02-13 20:25:56,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:56,309][root][INFO] - Training Epoch: 2/2, step 944/7134 completed (loss: 0.07348725944757462, acc: 0.9808917045593262)
[2025-02-13 20:25:56,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:56,677][root][INFO] - Training Epoch: 2/2, step 945/7134 completed (loss: 0.0893375501036644, acc: 0.9735099077224731)
[2025-02-13 20:25:56,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:57,084][root][INFO] - Training Epoch: 2/2, step 946/7134 completed (loss: 0.03747578710317612, acc: 0.9902912378311157)
[2025-02-13 20:25:57,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:57,438][root][INFO] - Training Epoch: 2/2, step 947/7134 completed (loss: 0.16987299919128418, acc: 0.9558823704719543)
[2025-02-13 20:25:57,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:57,849][root][INFO] - Training Epoch: 2/2, step 948/7134 completed (loss: 0.039790909737348557, acc: 0.9939758777618408)
[2025-02-13 20:25:58,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:58,240][root][INFO] - Training Epoch: 2/2, step 949/7134 completed (loss: 0.0540798082947731, acc: 0.981249988079071)
[2025-02-13 20:25:58,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:58,618][root][INFO] - Training Epoch: 2/2, step 950/7134 completed (loss: 0.07800506800413132, acc: 0.9740259647369385)
[2025-02-13 20:25:58,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:58,960][root][INFO] - Training Epoch: 2/2, step 951/7134 completed (loss: 0.0920695960521698, acc: 0.9770992398262024)
[2025-02-13 20:25:59,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:59,320][root][INFO] - Training Epoch: 2/2, step 952/7134 completed (loss: 0.13111650943756104, acc: 0.9558823704719543)
[2025-02-13 20:25:59,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:59,712][root][INFO] - Training Epoch: 2/2, step 953/7134 completed (loss: 0.1140645295381546, acc: 0.9756097793579102)
[2025-02-13 20:25:59,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:00,086][root][INFO] - Training Epoch: 2/2, step 954/7134 completed (loss: 0.13336504995822906, acc: 0.9586206674575806)
[2025-02-13 20:26:00,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:00,465][root][INFO] - Training Epoch: 2/2, step 955/7134 completed (loss: 0.14320151507854462, acc: 0.9604519605636597)
[2025-02-13 20:26:00,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:00,837][root][INFO] - Training Epoch: 2/2, step 956/7134 completed (loss: 0.12157119065523148, acc: 0.9728260636329651)
[2025-02-13 20:26:00,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:01,218][root][INFO] - Training Epoch: 2/2, step 957/7134 completed (loss: 0.12811283767223358, acc: 0.9754902124404907)
[2025-02-13 20:26:01,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:01,581][root][INFO] - Training Epoch: 2/2, step 958/7134 completed (loss: 0.11174841970205307, acc: 0.9552238583564758)
[2025-02-13 20:26:01,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:01,966][root][INFO] - Training Epoch: 2/2, step 959/7134 completed (loss: 0.1818573772907257, acc: 0.9484536051750183)
[2025-02-13 20:26:02,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:02,377][root][INFO] - Training Epoch: 2/2, step 960/7134 completed (loss: 0.24079598486423492, acc: 0.9477611780166626)
[2025-02-13 20:26:02,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:02,768][root][INFO] - Training Epoch: 2/2, step 961/7134 completed (loss: 0.1492651253938675, acc: 0.9450549483299255)
[2025-02-13 20:26:02,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:03,148][root][INFO] - Training Epoch: 2/2, step 962/7134 completed (loss: 0.2240048050880432, acc: 0.9239766001701355)
[2025-02-13 20:26:03,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:03,524][root][INFO] - Training Epoch: 2/2, step 963/7134 completed (loss: 0.15410052239894867, acc: 0.9534883499145508)
[2025-02-13 20:26:03,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:03,897][root][INFO] - Training Epoch: 2/2, step 964/7134 completed (loss: 0.3197857439517975, acc: 0.9512194991111755)
[2025-02-13 20:26:04,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:04,306][root][INFO] - Training Epoch: 2/2, step 965/7134 completed (loss: 0.20094475150108337, acc: 0.9572192430496216)
[2025-02-13 20:26:04,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:04,677][root][INFO] - Training Epoch: 2/2, step 966/7134 completed (loss: 0.07995288074016571, acc: 0.9754601120948792)
[2025-02-13 20:26:04,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:05,042][root][INFO] - Training Epoch: 2/2, step 967/7134 completed (loss: 0.0766868069767952, acc: 0.978723406791687)
[2025-02-13 20:26:05,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:05,396][root][INFO] - Training Epoch: 2/2, step 968/7134 completed (loss: 0.18181516230106354, acc: 0.9398496150970459)
[2025-02-13 20:26:05,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:05,772][root][INFO] - Training Epoch: 2/2, step 969/7134 completed (loss: 0.18394945561885834, acc: 0.9485714435577393)
[2025-02-13 20:26:05,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:06,142][root][INFO] - Training Epoch: 2/2, step 970/7134 completed (loss: 0.2632947564125061, acc: 0.9411764740943909)
[2025-02-13 20:26:06,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:06,510][root][INFO] - Training Epoch: 2/2, step 971/7134 completed (loss: 0.15367750823497772, acc: 0.9573459625244141)
[2025-02-13 20:26:06,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:06,914][root][INFO] - Training Epoch: 2/2, step 972/7134 completed (loss: 0.19724491238594055, acc: 0.9384615421295166)
[2025-02-13 20:26:07,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:07,284][root][INFO] - Training Epoch: 2/2, step 973/7134 completed (loss: 0.1777358055114746, acc: 0.9560439586639404)
[2025-02-13 20:26:07,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:07,714][root][INFO] - Training Epoch: 2/2, step 974/7134 completed (loss: 0.12082936614751816, acc: 0.9752475023269653)
[2025-02-13 20:26:07,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:08,082][root][INFO] - Training Epoch: 2/2, step 975/7134 completed (loss: 0.17962545156478882, acc: 0.978723406791687)
[2025-02-13 20:26:08,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:08,450][root][INFO] - Training Epoch: 2/2, step 976/7134 completed (loss: 0.08606996387243271, acc: 0.9668874144554138)
[2025-02-13 20:26:08,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:08,900][root][INFO] - Training Epoch: 2/2, step 977/7134 completed (loss: 0.25831952691078186, acc: 0.915730357170105)
[2025-02-13 20:26:09,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:09,273][root][INFO] - Training Epoch: 2/2, step 978/7134 completed (loss: 0.07328880578279495, acc: 0.9639175534248352)
[2025-02-13 20:26:09,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:09,646][root][INFO] - Training Epoch: 2/2, step 979/7134 completed (loss: 0.10385588556528091, acc: 0.9621621370315552)
[2025-02-13 20:26:09,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:10,034][root][INFO] - Training Epoch: 2/2, step 980/7134 completed (loss: 0.09501683712005615, acc: 0.9718309640884399)
[2025-02-13 20:26:10,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:10,389][root][INFO] - Training Epoch: 2/2, step 981/7134 completed (loss: 0.2496379166841507, acc: 0.9748427867889404)
[2025-02-13 20:26:10,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:10,754][root][INFO] - Training Epoch: 2/2, step 982/7134 completed (loss: 0.2758321166038513, acc: 0.954285740852356)
[2025-02-13 20:26:10,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:11,119][root][INFO] - Training Epoch: 2/2, step 983/7134 completed (loss: 0.1279595047235489, acc: 0.9774436354637146)
[2025-02-13 20:26:11,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:11,489][root][INFO] - Training Epoch: 2/2, step 984/7134 completed (loss: 0.02658066712319851, acc: 0.9929078221321106)
[2025-02-13 20:26:11,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:11,855][root][INFO] - Training Epoch: 2/2, step 985/7134 completed (loss: 0.10751888155937195, acc: 0.9777777791023254)
[2025-02-13 20:26:11,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:12,228][root][INFO] - Training Epoch: 2/2, step 986/7134 completed (loss: 0.07584114372730255, acc: 0.9776536226272583)
[2025-02-13 20:26:12,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:12,579][root][INFO] - Training Epoch: 2/2, step 987/7134 completed (loss: 0.031101493164896965, acc: 1.0)
[2025-02-13 20:26:12,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:12,944][root][INFO] - Training Epoch: 2/2, step 988/7134 completed (loss: 0.27226054668426514, acc: 0.9491525292396545)
[2025-02-13 20:26:13,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:13,312][root][INFO] - Training Epoch: 2/2, step 989/7134 completed (loss: 0.20023350417613983, acc: 0.9386503100395203)
[2025-02-13 20:26:13,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:13,693][root][INFO] - Training Epoch: 2/2, step 990/7134 completed (loss: 0.13345885276794434, acc: 0.9725274443626404)
[2025-02-13 20:26:13,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:14,064][root][INFO] - Training Epoch: 2/2, step 991/7134 completed (loss: 0.36447903513908386, acc: 0.9308176040649414)
[2025-02-13 20:26:14,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:14,441][root][INFO] - Training Epoch: 2/2, step 992/7134 completed (loss: 0.08555734902620316, acc: 0.9826589822769165)
[2025-02-13 20:26:14,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:14,809][root][INFO] - Training Epoch: 2/2, step 993/7134 completed (loss: 0.12371090799570084, acc: 0.971222996711731)
[2025-02-13 20:26:14,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:15,199][root][INFO] - Training Epoch: 2/2, step 994/7134 completed (loss: 0.14177659153938293, acc: 0.9660193920135498)
[2025-02-13 20:26:15,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:15,591][root][INFO] - Training Epoch: 2/2, step 995/7134 completed (loss: 0.10037779808044434, acc: 0.9727891087532043)
[2025-02-13 20:26:15,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:15,954][root][INFO] - Training Epoch: 2/2, step 996/7134 completed (loss: 0.18810708820819855, acc: 0.9593023061752319)
[2025-02-13 20:26:16,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:16,327][root][INFO] - Training Epoch: 2/2, step 997/7134 completed (loss: 0.09397739171981812, acc: 0.9764705896377563)
[2025-02-13 20:26:16,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:16,698][root][INFO] - Training Epoch: 2/2, step 998/7134 completed (loss: 0.12558795511722565, acc: 0.976047933101654)
[2025-02-13 20:26:16,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:17,068][root][INFO] - Training Epoch: 2/2, step 999/7134 completed (loss: 0.07393474131822586, acc: 0.9769230484962463)
[2025-02-13 20:26:17,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:17,439][root][INFO] - Training Epoch: 2/2, step 1000/7134 completed (loss: 0.07383746653795242, acc: 0.9852941036224365)
[2025-02-13 20:26:17,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:17,805][root][INFO] - Training Epoch: 2/2, step 1001/7134 completed (loss: 0.050027795135974884, acc: 0.9818181991577148)
[2025-02-13 20:26:17,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:18,173][root][INFO] - Training Epoch: 2/2, step 1002/7134 completed (loss: 0.05088819935917854, acc: 0.9811320900917053)
[2025-02-13 20:26:18,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:18,543][root][INFO] - Training Epoch: 2/2, step 1003/7134 completed (loss: 0.03502087667584419, acc: 0.9930555820465088)
[2025-02-13 20:26:18,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:18,939][root][INFO] - Training Epoch: 2/2, step 1004/7134 completed (loss: 0.1622694432735443, acc: 0.9557521939277649)
[2025-02-13 20:26:19,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:19,351][root][INFO] - Training Epoch: 2/2, step 1005/7134 completed (loss: 0.11160984635353088, acc: 0.9645389914512634)
[2025-02-13 20:26:19,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:19,727][root][INFO] - Training Epoch: 2/2, step 1006/7134 completed (loss: 0.13223588466644287, acc: 0.9490445852279663)
[2025-02-13 20:26:19,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:20,085][root][INFO] - Training Epoch: 2/2, step 1007/7134 completed (loss: 0.3658304810523987, acc: 0.922535240650177)
[2025-02-13 20:26:20,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:20,446][root][INFO] - Training Epoch: 2/2, step 1008/7134 completed (loss: 0.0937342569231987, acc: 0.9753086566925049)
[2025-02-13 20:26:20,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:20,821][root][INFO] - Training Epoch: 2/2, step 1009/7134 completed (loss: 0.08800125122070312, acc: 0.9922480583190918)
[2025-02-13 20:26:20,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:21,215][root][INFO] - Training Epoch: 2/2, step 1010/7134 completed (loss: 0.11678696423768997, acc: 0.9572649598121643)
[2025-02-13 20:26:21,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:21,600][root][INFO] - Training Epoch: 2/2, step 1011/7134 completed (loss: 0.05502290651202202, acc: 0.9937106966972351)
[2025-02-13 20:26:21,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:21,988][root][INFO] - Training Epoch: 2/2, step 1012/7134 completed (loss: 0.25358399748802185, acc: 0.9437500238418579)
[2025-02-13 20:26:22,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:22,394][root][INFO] - Training Epoch: 2/2, step 1013/7134 completed (loss: 0.21671795845031738, acc: 0.9578313231468201)
[2025-02-13 20:26:22,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:22,770][root][INFO] - Training Epoch: 2/2, step 1014/7134 completed (loss: 0.11654291301965714, acc: 0.9716312289237976)
[2025-02-13 20:26:22,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:23,144][root][INFO] - Training Epoch: 2/2, step 1015/7134 completed (loss: 0.06272577494382858, acc: 0.9933775067329407)
[2025-02-13 20:26:23,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:23,512][root][INFO] - Training Epoch: 2/2, step 1016/7134 completed (loss: 0.04524178430438042, acc: 0.9831932783126831)
[2025-02-13 20:26:23,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:23,903][root][INFO] - Training Epoch: 2/2, step 1017/7134 completed (loss: 0.0245501808822155, acc: 1.0)
[2025-02-13 20:26:24,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:24,282][root][INFO] - Training Epoch: 2/2, step 1018/7134 completed (loss: 0.10385187715291977, acc: 0.9751552939414978)
[2025-02-13 20:26:24,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:24,656][root][INFO] - Training Epoch: 2/2, step 1019/7134 completed (loss: 0.08109406381845474, acc: 0.9805194735527039)
[2025-02-13 20:26:24,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:25,009][root][INFO] - Training Epoch: 2/2, step 1020/7134 completed (loss: 0.029234355315566063, acc: 1.0)
[2025-02-13 20:26:25,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:25,385][root][INFO] - Training Epoch: 2/2, step 1021/7134 completed (loss: 0.05508754402399063, acc: 0.9869281053543091)
[2025-02-13 20:26:25,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:25,780][root][INFO] - Training Epoch: 2/2, step 1022/7134 completed (loss: 0.05296926945447922, acc: 0.9768785834312439)
[2025-02-13 20:26:25,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:26,153][root][INFO] - Training Epoch: 2/2, step 1023/7134 completed (loss: 0.07456326484680176, acc: 0.994413435459137)
[2025-02-13 20:26:26,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:26,505][root][INFO] - Training Epoch: 2/2, step 1024/7134 completed (loss: 0.12033791095018387, acc: 0.9701492786407471)
[2025-02-13 20:26:26,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:26,864][root][INFO] - Training Epoch: 2/2, step 1025/7134 completed (loss: 0.05602632090449333, acc: 0.978723406791687)
[2025-02-13 20:26:27,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:27,234][root][INFO] - Training Epoch: 2/2, step 1026/7134 completed (loss: 0.16326904296875, acc: 0.9646017551422119)
[2025-02-13 20:26:27,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:27,646][root][INFO] - Training Epoch: 2/2, step 1027/7134 completed (loss: 0.04212191700935364, acc: 0.9882352948188782)
[2025-02-13 20:26:27,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:28,026][root][INFO] - Training Epoch: 2/2, step 1028/7134 completed (loss: 0.027362719178199768, acc: 1.0)
[2025-02-13 20:26:28,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:28,406][root][INFO] - Training Epoch: 2/2, step 1029/7134 completed (loss: 0.019144881516695023, acc: 1.0)
[2025-02-13 20:26:28,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:28,798][root][INFO] - Training Epoch: 2/2, step 1030/7134 completed (loss: 0.054339781403541565, acc: 0.977011501789093)
[2025-02-13 20:26:28,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:29,159][root][INFO] - Training Epoch: 2/2, step 1031/7134 completed (loss: 0.16175620257854462, acc: 0.95652174949646)
[2025-02-13 20:26:29,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:29,553][root][INFO] - Training Epoch: 2/2, step 1032/7134 completed (loss: 0.060176096856594086, acc: 0.9840425252914429)
[2025-02-13 20:26:29,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:29,936][root][INFO] - Training Epoch: 2/2, step 1033/7134 completed (loss: 0.027291318401694298, acc: 1.0)
[2025-02-13 20:26:30,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:30,304][root][INFO] - Training Epoch: 2/2, step 1034/7134 completed (loss: 0.22402629256248474, acc: 0.9285714030265808)
[2025-02-13 20:26:30,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:30,670][root][INFO] - Training Epoch: 2/2, step 1035/7134 completed (loss: 0.3356735110282898, acc: 0.9189189076423645)
[2025-02-13 20:26:30,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:31,026][root][INFO] - Training Epoch: 2/2, step 1036/7134 completed (loss: 0.4119979739189148, acc: 0.8813559412956238)
[2025-02-13 20:26:31,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:31,400][root][INFO] - Training Epoch: 2/2, step 1037/7134 completed (loss: 0.045732948929071426, acc: 0.9840425252914429)
[2025-02-13 20:26:31,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:31,775][root][INFO] - Training Epoch: 2/2, step 1038/7134 completed (loss: 0.06681106239557266, acc: 0.984000027179718)
[2025-02-13 20:26:31,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:32,152][root][INFO] - Training Epoch: 2/2, step 1039/7134 completed (loss: 0.14260134100914001, acc: 0.963302731513977)
[2025-02-13 20:26:32,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:32,568][root][INFO] - Training Epoch: 2/2, step 1040/7134 completed (loss: 0.03693602234125137, acc: 0.9927536249160767)
[2025-02-13 20:26:32,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:32,979][root][INFO] - Training Epoch: 2/2, step 1041/7134 completed (loss: 0.0310499370098114, acc: 1.0)
[2025-02-13 20:26:33,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:33,355][root][INFO] - Training Epoch: 2/2, step 1042/7134 completed (loss: 0.1142432913184166, acc: 0.9553072452545166)
[2025-02-13 20:26:33,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:33,752][root][INFO] - Training Epoch: 2/2, step 1043/7134 completed (loss: 0.1019287109375, acc: 0.9752475023269653)
[2025-02-13 20:26:33,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:34,114][root][INFO] - Training Epoch: 2/2, step 1044/7134 completed (loss: 0.032248303294181824, acc: 0.9937888383865356)
[2025-02-13 20:26:34,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:34,467][root][INFO] - Training Epoch: 2/2, step 1045/7134 completed (loss: 0.2715124189853668, acc: 0.9192546606063843)
[2025-02-13 20:26:34,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:34,846][root][INFO] - Training Epoch: 2/2, step 1046/7134 completed (loss: 0.2122807800769806, acc: 0.9548872113227844)
[2025-02-13 20:26:34,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:35,247][root][INFO] - Training Epoch: 2/2, step 1047/7134 completed (loss: 0.2538892924785614, acc: 0.9314285516738892)
[2025-02-13 20:26:35,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:35,678][root][INFO] - Training Epoch: 2/2, step 1048/7134 completed (loss: 0.1442350447177887, acc: 0.9745222926139832)
[2025-02-13 20:26:35,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:36,077][root][INFO] - Training Epoch: 2/2, step 1049/7134 completed (loss: 0.11404332518577576, acc: 0.9802631735801697)
[2025-02-13 20:26:36,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:36,458][root][INFO] - Training Epoch: 2/2, step 1050/7134 completed (loss: 0.23476870357990265, acc: 0.9375)
[2025-02-13 20:26:36,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:36,844][root][INFO] - Training Epoch: 2/2, step 1051/7134 completed (loss: 0.06910481303930283, acc: 0.9828571677207947)
[2025-02-13 20:26:36,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:37,213][root][INFO] - Training Epoch: 2/2, step 1052/7134 completed (loss: 0.10185062140226364, acc: 0.9578313231468201)
[2025-02-13 20:26:37,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:37,592][root][INFO] - Training Epoch: 2/2, step 1053/7134 completed (loss: 0.15378475189208984, acc: 0.970059871673584)
[2025-02-13 20:26:37,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:37,967][root][INFO] - Training Epoch: 2/2, step 1054/7134 completed (loss: 0.06571807712316513, acc: 0.984375)
[2025-02-13 20:26:38,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:38,330][root][INFO] - Training Epoch: 2/2, step 1055/7134 completed (loss: 0.04580620676279068, acc: 0.9847328066825867)
[2025-02-13 20:26:38,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:38,708][root][INFO] - Training Epoch: 2/2, step 1056/7134 completed (loss: 0.14526699483394623, acc: 0.9709302186965942)
[2025-02-13 20:26:38,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:39,093][root][INFO] - Training Epoch: 2/2, step 1057/7134 completed (loss: 0.10813300311565399, acc: 0.9792746305465698)
[2025-02-13 20:26:39,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:39,511][root][INFO] - Training Epoch: 2/2, step 1058/7134 completed (loss: 0.09314439445734024, acc: 0.9736841917037964)
[2025-02-13 20:26:39,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:39,902][root][INFO] - Training Epoch: 2/2, step 1059/7134 completed (loss: 0.08126664161682129, acc: 0.9810426831245422)
[2025-02-13 20:26:40,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:40,276][root][INFO] - Training Epoch: 2/2, step 1060/7134 completed (loss: 0.11671854555606842, acc: 0.9743589758872986)
[2025-02-13 20:26:40,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:40,657][root][INFO] - Training Epoch: 2/2, step 1061/7134 completed (loss: 0.09984895586967468, acc: 0.9742268323898315)
[2025-02-13 20:26:40,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:41,012][root][INFO] - Training Epoch: 2/2, step 1062/7134 completed (loss: 0.09124541282653809, acc: 0.9759036302566528)
[2025-02-13 20:26:41,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:41,405][root][INFO] - Training Epoch: 2/2, step 1063/7134 completed (loss: 0.03218221664428711, acc: 1.0)
[2025-02-13 20:26:41,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:41,814][root][INFO] - Training Epoch: 2/2, step 1064/7134 completed (loss: 0.13751435279846191, acc: 0.9567567706108093)
[2025-02-13 20:26:41,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:42,198][root][INFO] - Training Epoch: 2/2, step 1065/7134 completed (loss: 0.10067509114742279, acc: 0.9779005646705627)
[2025-02-13 20:26:42,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:42,582][root][INFO] - Training Epoch: 2/2, step 1066/7134 completed (loss: 0.1447698324918747, acc: 0.9618320465087891)
[2025-02-13 20:26:42,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:42,930][root][INFO] - Training Epoch: 2/2, step 1067/7134 completed (loss: 0.1032501831650734, acc: 0.9673202633857727)
[2025-02-13 20:26:43,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:43,303][root][INFO] - Training Epoch: 2/2, step 1068/7134 completed (loss: 0.1636057198047638, acc: 0.9453125)
[2025-02-13 20:26:43,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:43,698][root][INFO] - Training Epoch: 2/2, step 1069/7134 completed (loss: 0.11375585943460464, acc: 0.9583333134651184)
[2025-02-13 20:26:43,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:44,059][root][INFO] - Training Epoch: 2/2, step 1070/7134 completed (loss: 0.10706629604101181, acc: 0.9726027250289917)
[2025-02-13 20:26:44,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:44,437][root][INFO] - Training Epoch: 2/2, step 1071/7134 completed (loss: 0.05611153319478035, acc: 0.981249988079071)
[2025-02-13 20:26:44,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:44,804][root][INFO] - Training Epoch: 2/2, step 1072/7134 completed (loss: 0.01783393882215023, acc: 0.9939024448394775)
[2025-02-13 20:26:44,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:45,173][root][INFO] - Training Epoch: 2/2, step 1073/7134 completed (loss: 0.04453875869512558, acc: 0.9813664555549622)
[2025-02-13 20:26:45,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:45,537][root][INFO] - Training Epoch: 2/2, step 1074/7134 completed (loss: 0.11104162037372589, acc: 0.9716312289237976)
[2025-02-13 20:26:45,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:45,908][root][INFO] - Training Epoch: 2/2, step 1075/7134 completed (loss: 0.27335676550865173, acc: 0.9635036587715149)
[2025-02-13 20:26:46,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:46,287][root][INFO] - Training Epoch: 2/2, step 1076/7134 completed (loss: 0.1991080492734909, acc: 0.9468085169792175)
[2025-02-13 20:26:46,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:46,688][root][INFO] - Training Epoch: 2/2, step 1077/7134 completed (loss: 0.15700788795948029, acc: 0.9646017551422119)
[2025-02-13 20:26:46,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:47,066][root][INFO] - Training Epoch: 2/2, step 1078/7134 completed (loss: 0.2836835980415344, acc: 0.9308176040649414)
[2025-02-13 20:26:47,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:47,434][root][INFO] - Training Epoch: 2/2, step 1079/7134 completed (loss: 0.13481946289539337, acc: 0.9723502397537231)
[2025-02-13 20:26:47,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:47,834][root][INFO] - Training Epoch: 2/2, step 1080/7134 completed (loss: 0.17798155546188354, acc: 0.9767441749572754)
[2025-02-13 20:26:47,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:48,218][root][INFO] - Training Epoch: 2/2, step 1081/7134 completed (loss: 0.13247151672840118, acc: 0.9578313231468201)
[2025-02-13 20:26:48,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:48,623][root][INFO] - Training Epoch: 2/2, step 1082/7134 completed (loss: 0.030788714066147804, acc: 0.9910314083099365)
[2025-02-13 20:26:48,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:49,032][root][INFO] - Training Epoch: 2/2, step 1083/7134 completed (loss: 0.10753501206636429, acc: 0.9746192693710327)
[2025-02-13 20:26:49,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:49,435][root][INFO] - Training Epoch: 2/2, step 1084/7134 completed (loss: 0.07567690312862396, acc: 0.9734042286872864)
[2025-02-13 20:26:49,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:49,832][root][INFO] - Training Epoch: 2/2, step 1085/7134 completed (loss: 0.06129191815853119, acc: 0.9748427867889404)
[2025-02-13 20:26:49,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:50,219][root][INFO] - Training Epoch: 2/2, step 1086/7134 completed (loss: 0.16007117927074432, acc: 0.9596773982048035)
[2025-02-13 20:26:50,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:50,638][root][INFO] - Training Epoch: 2/2, step 1087/7134 completed (loss: 0.10119558870792389, acc: 0.9784946441650391)
[2025-02-13 20:26:50,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:51,008][root][INFO] - Training Epoch: 2/2, step 1088/7134 completed (loss: 0.06835276633501053, acc: 0.9886363744735718)
[2025-02-13 20:26:51,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:51,380][root][INFO] - Training Epoch: 2/2, step 1089/7134 completed (loss: 0.1384691745042801, acc: 0.9693251252174377)
[2025-02-13 20:26:51,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:51,764][root][INFO] - Training Epoch: 2/2, step 1090/7134 completed (loss: 0.1160271167755127, acc: 0.9583333134651184)
[2025-02-13 20:26:51,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:52,168][root][INFO] - Training Epoch: 2/2, step 1091/7134 completed (loss: 0.08568558096885681, acc: 0.9689440727233887)
[2025-02-13 20:26:52,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:52,580][root][INFO] - Training Epoch: 2/2, step 1092/7134 completed (loss: 0.10554641485214233, acc: 0.970059871673584)
[2025-02-13 20:26:52,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:52,966][root][INFO] - Training Epoch: 2/2, step 1093/7134 completed (loss: 0.09673650562763214, acc: 0.9863945841789246)
[2025-02-13 20:26:53,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:53,398][root][INFO] - Training Epoch: 2/2, step 1094/7134 completed (loss: 0.258099228143692, acc: 0.9473684430122375)
[2025-02-13 20:26:53,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:53,797][root][INFO] - Training Epoch: 2/2, step 1095/7134 completed (loss: 0.1163221001625061, acc: 0.9894179701805115)
[2025-02-13 20:26:53,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:54,200][root][INFO] - Training Epoch: 2/2, step 1096/7134 completed (loss: 0.22941796481609344, acc: 0.9247311949729919)
[2025-02-13 20:26:54,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:54,591][root][INFO] - Training Epoch: 2/2, step 1097/7134 completed (loss: 0.18459220230579376, acc: 0.9453551769256592)
[2025-02-13 20:26:54,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:54,975][root][INFO] - Training Epoch: 2/2, step 1098/7134 completed (loss: 0.04823562875390053, acc: 0.9888888597488403)
[2025-02-13 20:26:55,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:55,364][root][INFO] - Training Epoch: 2/2, step 1099/7134 completed (loss: 0.12475500255823135, acc: 0.9791666865348816)
[2025-02-13 20:26:55,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:55,750][root][INFO] - Training Epoch: 2/2, step 1100/7134 completed (loss: 0.13471494615077972, acc: 0.9649122953414917)
[2025-02-13 20:26:55,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:56,138][root][INFO] - Training Epoch: 2/2, step 1101/7134 completed (loss: 0.20320385694503784, acc: 0.9487179517745972)
[2025-02-13 20:26:56,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:56,532][root][INFO] - Training Epoch: 2/2, step 1102/7134 completed (loss: 0.32898104190826416, acc: 0.9097222089767456)
[2025-02-13 20:26:56,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:56,905][root][INFO] - Training Epoch: 2/2, step 1103/7134 completed (loss: 0.31964364647865295, acc: 0.9523809552192688)
[2025-02-13 20:26:57,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:57,272][root][INFO] - Training Epoch: 2/2, step 1104/7134 completed (loss: 0.09708519279956818, acc: 0.9800000190734863)
[2025-02-13 20:26:57,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:57,646][root][INFO] - Training Epoch: 2/2, step 1105/7134 completed (loss: 0.10144657641649246, acc: 0.9753086566925049)
[2025-02-13 20:26:57,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:58,018][root][INFO] - Training Epoch: 2/2, step 1106/7134 completed (loss: 0.05059479549527168, acc: 0.9933775067329407)
[2025-02-13 20:26:58,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:58,405][root][INFO] - Training Epoch: 2/2, step 1107/7134 completed (loss: 0.024502823129296303, acc: 1.0)
[2025-02-13 20:26:58,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:58,770][root][INFO] - Training Epoch: 2/2, step 1108/7134 completed (loss: 0.044038139283657074, acc: 0.9940828680992126)
[2025-02-13 20:26:58,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:59,148][root][INFO] - Training Epoch: 2/2, step 1109/7134 completed (loss: 0.10798102617263794, acc: 0.9768785834312439)
[2025-02-13 20:26:59,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:59,514][root][INFO] - Training Epoch: 2/2, step 1110/7134 completed (loss: 0.1288733333349228, acc: 0.9731183052062988)
[2025-02-13 20:26:59,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:59,878][root][INFO] - Training Epoch: 2/2, step 1111/7134 completed (loss: 0.07096656411886215, acc: 0.981249988079071)
[2025-02-13 20:26:59,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:00,225][root][INFO] - Training Epoch: 2/2, step 1112/7134 completed (loss: 0.06017354130744934, acc: 0.9860140085220337)
[2025-02-13 20:27:00,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:00,621][root][INFO] - Training Epoch: 2/2, step 1113/7134 completed (loss: 0.09461943060159683, acc: 0.9748427867889404)
[2025-02-13 20:27:00,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:00,997][root][INFO] - Training Epoch: 2/2, step 1114/7134 completed (loss: 0.1891608089208603, acc: 0.9523809552192688)
[2025-02-13 20:27:01,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:01,374][root][INFO] - Training Epoch: 2/2, step 1115/7134 completed (loss: 0.07017870992422104, acc: 0.9806451797485352)
[2025-02-13 20:27:01,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:01,749][root][INFO] - Training Epoch: 2/2, step 1116/7134 completed (loss: 0.06161191314458847, acc: 0.9674796462059021)
[2025-02-13 20:27:01,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:02,121][root][INFO] - Training Epoch: 2/2, step 1117/7134 completed (loss: 0.06890762597322464, acc: 0.9802631735801697)
[2025-02-13 20:27:02,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:02,491][root][INFO] - Training Epoch: 2/2, step 1118/7134 completed (loss: 0.05143670365214348, acc: 0.9847328066825867)
[2025-02-13 20:27:02,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:02,850][root][INFO] - Training Epoch: 2/2, step 1119/7134 completed (loss: 0.08537063747644424, acc: 0.9724137783050537)
[2025-02-13 20:27:02,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:03,222][root][INFO] - Training Epoch: 2/2, step 1120/7134 completed (loss: 0.12237991392612457, acc: 0.9770992398262024)
[2025-02-13 20:27:03,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:03,619][root][INFO] - Training Epoch: 2/2, step 1121/7134 completed (loss: 0.11754824966192245, acc: 0.9677419066429138)
[2025-02-13 20:27:03,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:04,003][root][INFO] - Training Epoch: 2/2, step 1122/7134 completed (loss: 0.01752694696187973, acc: 1.0)
[2025-02-13 20:27:04,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:04,373][root][INFO] - Training Epoch: 2/2, step 1123/7134 completed (loss: 0.07453791052103043, acc: 0.9724137783050537)
[2025-02-13 20:27:04,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:04,756][root][INFO] - Training Epoch: 2/2, step 1124/7134 completed (loss: 0.09333401918411255, acc: 0.9729729890823364)
[2025-02-13 20:27:04,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:05,128][root][INFO] - Training Epoch: 2/2, step 1125/7134 completed (loss: 0.076431043446064, acc: 0.9821428656578064)
[2025-02-13 20:27:05,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:05,530][root][INFO] - Training Epoch: 2/2, step 1126/7134 completed (loss: 0.10360224545001984, acc: 0.966292142868042)
[2025-02-13 20:27:05,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:05,920][root][INFO] - Training Epoch: 2/2, step 1127/7134 completed (loss: 0.03919381648302078, acc: 0.9852941036224365)
[2025-02-13 20:27:06,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:06,288][root][INFO] - Training Epoch: 2/2, step 1128/7134 completed (loss: 0.08461873978376389, acc: 0.9847328066825867)
[2025-02-13 20:27:06,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:06,664][root][INFO] - Training Epoch: 2/2, step 1129/7134 completed (loss: 0.08475100249052048, acc: 0.9750000238418579)
[2025-02-13 20:27:06,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:07,042][root][INFO] - Training Epoch: 2/2, step 1130/7134 completed (loss: 0.0334639735519886, acc: 0.9856114983558655)
[2025-02-13 20:27:07,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:07,424][root][INFO] - Training Epoch: 2/2, step 1131/7134 completed (loss: 0.10520678013563156, acc: 0.9848484992980957)
[2025-02-13 20:27:07,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:07,778][root][INFO] - Training Epoch: 2/2, step 1132/7134 completed (loss: 0.28165993094444275, acc: 0.9395973086357117)
[2025-02-13 20:27:07,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:08,133][root][INFO] - Training Epoch: 2/2, step 1133/7134 completed (loss: 0.2054135799407959, acc: 0.9595375657081604)
[2025-02-13 20:27:08,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:08,480][root][INFO] - Training Epoch: 2/2, step 1134/7134 completed (loss: 0.10573463886976242, acc: 0.9725274443626404)
[2025-02-13 20:27:08,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:08,829][root][INFO] - Training Epoch: 2/2, step 1135/7134 completed (loss: 0.07104494422674179, acc: 0.9798657894134521)
[2025-02-13 20:27:08,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:09,209][root][INFO] - Training Epoch: 2/2, step 1136/7134 completed (loss: 0.22299696505069733, acc: 0.9387755393981934)
[2025-02-13 20:27:09,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:09,631][root][INFO] - Training Epoch: 2/2, step 1137/7134 completed (loss: 0.16151712834835052, acc: 0.9528301954269409)
[2025-02-13 20:27:09,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:10,028][root][INFO] - Training Epoch: 2/2, step 1138/7134 completed (loss: 0.11363052576780319, acc: 0.9700000286102295)
[2025-02-13 20:27:10,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:10,395][root][INFO] - Training Epoch: 2/2, step 1139/7134 completed (loss: 0.10725103318691254, acc: 0.9725274443626404)
[2025-02-13 20:27:10,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:10,772][root][INFO] - Training Epoch: 2/2, step 1140/7134 completed (loss: 0.05215020477771759, acc: 0.9894737005233765)
[2025-02-13 20:27:10,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:11,144][root][INFO] - Training Epoch: 2/2, step 1141/7134 completed (loss: 0.056801557540893555, acc: 0.9909090995788574)
[2025-02-13 20:27:11,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:11,510][root][INFO] - Training Epoch: 2/2, step 1142/7134 completed (loss: 0.11825644969940186, acc: 0.9777777791023254)
[2025-02-13 20:27:11,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:11,879][root][INFO] - Training Epoch: 2/2, step 1143/7134 completed (loss: 0.23295500874519348, acc: 0.9578313231468201)
[2025-02-13 20:27:12,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:12,245][root][INFO] - Training Epoch: 2/2, step 1144/7134 completed (loss: 0.16415084898471832, acc: 0.9702380895614624)
[2025-02-13 20:27:12,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:12,616][root][INFO] - Training Epoch: 2/2, step 1145/7134 completed (loss: 0.07812807708978653, acc: 0.9888268113136292)
[2025-02-13 20:27:12,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:12,972][root][INFO] - Training Epoch: 2/2, step 1146/7134 completed (loss: 0.03346193954348564, acc: 0.9924242496490479)
[2025-02-13 20:27:13,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:13,328][root][INFO] - Training Epoch: 2/2, step 1147/7134 completed (loss: 0.08884959667921066, acc: 0.9837837815284729)
[2025-02-13 20:27:13,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:13,703][root][INFO] - Training Epoch: 2/2, step 1148/7134 completed (loss: 0.04310857877135277, acc: 0.9928057789802551)
[2025-02-13 20:27:13,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:14,069][root][INFO] - Training Epoch: 2/2, step 1149/7134 completed (loss: 0.1539493054151535, acc: 0.9583333134651184)
[2025-02-13 20:27:14,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:14,441][root][INFO] - Training Epoch: 2/2, step 1150/7134 completed (loss: 0.06818384677171707, acc: 0.9803921580314636)
[2025-02-13 20:27:14,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:14,816][root][INFO] - Training Epoch: 2/2, step 1151/7134 completed (loss: 0.07306346297264099, acc: 0.9747899174690247)
[2025-02-13 20:27:14,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:15,200][root][INFO] - Training Epoch: 2/2, step 1152/7134 completed (loss: 0.04961993917822838, acc: 0.9919354915618896)
[2025-02-13 20:27:15,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:15,603][root][INFO] - Training Epoch: 2/2, step 1153/7134 completed (loss: 0.10297638177871704, acc: 0.9803921580314636)
[2025-02-13 20:27:15,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:15,984][root][INFO] - Training Epoch: 2/2, step 1154/7134 completed (loss: 0.08225101977586746, acc: 0.9765625)
[2025-02-13 20:27:16,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:16,366][root][INFO] - Training Epoch: 2/2, step 1155/7134 completed (loss: 0.09709611535072327, acc: 0.9914529919624329)
[2025-02-13 20:27:16,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:16,723][root][INFO] - Training Epoch: 2/2, step 1156/7134 completed (loss: 0.02491830475628376, acc: 1.0)
[2025-02-13 20:27:16,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:17,093][root][INFO] - Training Epoch: 2/2, step 1157/7134 completed (loss: 0.29361459612846375, acc: 0.931034505367279)
[2025-02-13 20:27:17,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:17,484][root][INFO] - Training Epoch: 2/2, step 1158/7134 completed (loss: 0.10370543599128723, acc: 0.9739130139350891)
[2025-02-13 20:27:17,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:17,856][root][INFO] - Training Epoch: 2/2, step 1159/7134 completed (loss: 0.08387216925621033, acc: 0.9820359349250793)
[2025-02-13 20:27:17,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:18,273][root][INFO] - Training Epoch: 2/2, step 1160/7134 completed (loss: 0.02960813418030739, acc: 0.9938271641731262)
[2025-02-13 20:27:18,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:18,651][root][INFO] - Training Epoch: 2/2, step 1161/7134 completed (loss: 0.059412699192762375, acc: 0.9710144996643066)
[2025-02-13 20:27:18,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:19,034][root][INFO] - Training Epoch: 2/2, step 1162/7134 completed (loss: 0.1445392221212387, acc: 0.9620253443717957)
[2025-02-13 20:27:19,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:19,427][root][INFO] - Training Epoch: 2/2, step 1163/7134 completed (loss: 0.06263168156147003, acc: 0.9793103337287903)
[2025-02-13 20:27:19,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:19,807][root][INFO] - Training Epoch: 2/2, step 1164/7134 completed (loss: 0.11975473165512085, acc: 0.9723756909370422)
[2025-02-13 20:27:19,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:20,186][root][INFO] - Training Epoch: 2/2, step 1165/7134 completed (loss: 0.05170240253210068, acc: 0.9794871807098389)
[2025-02-13 20:27:20,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:20,556][root][INFO] - Training Epoch: 2/2, step 1166/7134 completed (loss: 0.029701458290219307, acc: 0.9946808218955994)
[2025-02-13 20:27:20,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:20,930][root][INFO] - Training Epoch: 2/2, step 1167/7134 completed (loss: 0.027666615322232246, acc: 0.9919999837875366)
[2025-02-13 20:27:21,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:21,342][root][INFO] - Training Epoch: 2/2, step 1168/7134 completed (loss: 0.06621987372636795, acc: 0.9841269850730896)
[2025-02-13 20:27:21,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:21,734][root][INFO] - Training Epoch: 2/2, step 1169/7134 completed (loss: 0.10906101763248444, acc: 0.957446813583374)
[2025-02-13 20:27:21,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:22,176][root][INFO] - Training Epoch: 2/2, step 1170/7134 completed (loss: 0.19186100363731384, acc: 0.9604519605636597)
[2025-02-13 20:27:22,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:22,542][root][INFO] - Training Epoch: 2/2, step 1171/7134 completed (loss: 0.04833782836794853, acc: 0.9933333396911621)
[2025-02-13 20:27:22,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:22,891][root][INFO] - Training Epoch: 2/2, step 1172/7134 completed (loss: 0.06926809251308441, acc: 0.9821428656578064)
[2025-02-13 20:27:23,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:23,242][root][INFO] - Training Epoch: 2/2, step 1173/7134 completed (loss: 0.06904907524585724, acc: 0.9841269850730896)
[2025-02-13 20:27:23,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:23,602][root][INFO] - Training Epoch: 2/2, step 1174/7134 completed (loss: 0.04184075817465782, acc: 0.987730085849762)
[2025-02-13 20:27:23,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:24,024][root][INFO] - Training Epoch: 2/2, step 1175/7134 completed (loss: 0.056978948414325714, acc: 0.9882352948188782)
[2025-02-13 20:27:24,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:24,402][root][INFO] - Training Epoch: 2/2, step 1176/7134 completed (loss: 0.0348123162984848, acc: 0.9930555820465088)
[2025-02-13 20:27:24,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:24,796][root][INFO] - Training Epoch: 2/2, step 1177/7134 completed (loss: 0.012641878798604012, acc: 1.0)
[2025-02-13 20:27:24,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:25,164][root][INFO] - Training Epoch: 2/2, step 1178/7134 completed (loss: 0.038538046181201935, acc: 0.9903846383094788)
[2025-02-13 20:27:25,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:25,548][root][INFO] - Training Epoch: 2/2, step 1179/7134 completed (loss: 0.038573045283555984, acc: 0.9838709831237793)
[2025-02-13 20:27:25,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:25,931][root][INFO] - Training Epoch: 2/2, step 1180/7134 completed (loss: 0.05803874507546425, acc: 0.9729729890823364)
[2025-02-13 20:27:26,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:26,336][root][INFO] - Training Epoch: 2/2, step 1181/7134 completed (loss: 0.03959598019719124, acc: 0.9945054650306702)
[2025-02-13 20:27:26,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:26,715][root][INFO] - Training Epoch: 2/2, step 1182/7134 completed (loss: 0.023060064762830734, acc: 1.0)
[2025-02-13 20:27:26,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:27,096][root][INFO] - Training Epoch: 2/2, step 1183/7134 completed (loss: 0.014162275940179825, acc: 1.0)
[2025-02-13 20:27:27,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:27,478][root][INFO] - Training Epoch: 2/2, step 1184/7134 completed (loss: 0.11640918999910355, acc: 0.9768785834312439)
[2025-02-13 20:27:27,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:27,847][root][INFO] - Training Epoch: 2/2, step 1185/7134 completed (loss: 0.01630137860774994, acc: 1.0)
[2025-02-13 20:27:27,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:28,227][root][INFO] - Training Epoch: 2/2, step 1186/7134 completed (loss: 0.03182819113135338, acc: 0.9945651888847351)
[2025-02-13 20:27:28,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:28,606][root][INFO] - Training Epoch: 2/2, step 1187/7134 completed (loss: 0.06907875835895538, acc: 0.976190447807312)
[2025-02-13 20:27:28,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:28,976][root][INFO] - Training Epoch: 2/2, step 1188/7134 completed (loss: 0.028931202366948128, acc: 0.9892473220825195)
[2025-02-13 20:27:29,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:29,340][root][INFO] - Training Epoch: 2/2, step 1189/7134 completed (loss: 0.05936240032315254, acc: 0.9822485446929932)
[2025-02-13 20:27:29,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:29,719][root][INFO] - Training Epoch: 2/2, step 1190/7134 completed (loss: 0.035432055592536926, acc: 0.9888888597488403)
[2025-02-13 20:27:29,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:30,139][root][INFO] - Training Epoch: 2/2, step 1191/7134 completed (loss: 0.1470487415790558, acc: 0.9659863710403442)
[2025-02-13 20:27:30,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:30,520][root][INFO] - Training Epoch: 2/2, step 1192/7134 completed (loss: 0.04082576185464859, acc: 0.9924812316894531)
[2025-02-13 20:27:30,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:30,903][root][INFO] - Training Epoch: 2/2, step 1193/7134 completed (loss: 0.08920572698116302, acc: 0.9763779640197754)
[2025-02-13 20:27:31,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:31,291][root][INFO] - Training Epoch: 2/2, step 1194/7134 completed (loss: 0.03073306381702423, acc: 0.9932885766029358)
[2025-02-13 20:27:31,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:31,662][root][INFO] - Training Epoch: 2/2, step 1195/7134 completed (loss: 0.04405442997813225, acc: 0.9846153855323792)
[2025-02-13 20:27:31,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:32,046][root][INFO] - Training Epoch: 2/2, step 1196/7134 completed (loss: 0.14451588690280914, acc: 0.9645389914512634)
[2025-02-13 20:27:32,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:32,408][root][INFO] - Training Epoch: 2/2, step 1197/7134 completed (loss: 0.07263879477977753, acc: 0.9863013625144958)
[2025-02-13 20:27:32,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:32,784][root][INFO] - Training Epoch: 2/2, step 1198/7134 completed (loss: 0.12019693851470947, acc: 0.9750000238418579)
[2025-02-13 20:27:32,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:33,159][root][INFO] - Training Epoch: 2/2, step 1199/7134 completed (loss: 0.09429571032524109, acc: 0.9805194735527039)
[2025-02-13 20:27:33,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:33,529][root][INFO] - Training Epoch: 2/2, step 1200/7134 completed (loss: 0.023683013394474983, acc: 1.0)
[2025-02-13 20:27:33,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:33,906][root][INFO] - Training Epoch: 2/2, step 1201/7134 completed (loss: 0.0678485557436943, acc: 0.9874213933944702)
[2025-02-13 20:27:34,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:34,292][root][INFO] - Training Epoch: 2/2, step 1202/7134 completed (loss: 0.08872858434915543, acc: 0.9836956262588501)
[2025-02-13 20:27:34,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:34,657][root][INFO] - Training Epoch: 2/2, step 1203/7134 completed (loss: 0.04574466124176979, acc: 0.9933333396911621)
[2025-02-13 20:27:34,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:35,023][root][INFO] - Training Epoch: 2/2, step 1204/7134 completed (loss: 0.1499740183353424, acc: 0.9779411554336548)
[2025-02-13 20:27:35,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:35,408][root][INFO] - Training Epoch: 2/2, step 1205/7134 completed (loss: 0.29604607820510864, acc: 0.931034505367279)
[2025-02-13 20:27:35,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:35,794][root][INFO] - Training Epoch: 2/2, step 1206/7134 completed (loss: 0.1586369127035141, acc: 0.9698492288589478)
[2025-02-13 20:27:35,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:36,185][root][INFO] - Training Epoch: 2/2, step 1207/7134 completed (loss: 0.07638459652662277, acc: 0.9838709831237793)
[2025-02-13 20:27:36,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:36,570][root][INFO] - Training Epoch: 2/2, step 1208/7134 completed (loss: 0.18297487497329712, acc: 0.9694656729698181)
[2025-02-13 20:27:36,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:36,944][root][INFO] - Training Epoch: 2/2, step 1209/7134 completed (loss: 0.14129169285297394, acc: 0.9672130942344666)
[2025-02-13 20:27:37,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:37,317][root][INFO] - Training Epoch: 2/2, step 1210/7134 completed (loss: 0.1896895319223404, acc: 0.9466666579246521)
[2025-02-13 20:27:37,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:37,699][root][INFO] - Training Epoch: 2/2, step 1211/7134 completed (loss: 0.4086362421512604, acc: 0.8920863270759583)
[2025-02-13 20:27:37,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:38,071][root][INFO] - Training Epoch: 2/2, step 1212/7134 completed (loss: 0.2749442756175995, acc: 0.9489051103591919)
[2025-02-13 20:27:38,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:38,443][root][INFO] - Training Epoch: 2/2, step 1213/7134 completed (loss: 0.2175751030445099, acc: 0.9467455744743347)
[2025-02-13 20:27:38,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:38,816][root][INFO] - Training Epoch: 2/2, step 1214/7134 completed (loss: 0.2295813262462616, acc: 0.9200000166893005)
[2025-02-13 20:27:38,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:39,198][root][INFO] - Training Epoch: 2/2, step 1215/7134 completed (loss: 0.21571053564548492, acc: 0.9411764740943909)
[2025-02-13 20:27:39,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:39,611][root][INFO] - Training Epoch: 2/2, step 1216/7134 completed (loss: 0.07820035517215729, acc: 0.9867549538612366)
[2025-02-13 20:27:39,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:39,995][root][INFO] - Training Epoch: 2/2, step 1217/7134 completed (loss: 0.129612997174263, acc: 0.9671052694320679)
[2025-02-13 20:27:40,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:40,371][root][INFO] - Training Epoch: 2/2, step 1218/7134 completed (loss: 0.19246232509613037, acc: 0.949999988079071)
[2025-02-13 20:27:40,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:40,742][root][INFO] - Training Epoch: 2/2, step 1219/7134 completed (loss: 0.19316312670707703, acc: 0.9466666579246521)
[2025-02-13 20:27:40,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:41,113][root][INFO] - Training Epoch: 2/2, step 1220/7134 completed (loss: 0.16577492654323578, acc: 0.9639175534248352)
[2025-02-13 20:27:41,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:41,494][root][INFO] - Training Epoch: 2/2, step 1221/7134 completed (loss: 0.17494605481624603, acc: 0.9547511339187622)
[2025-02-13 20:27:41,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:41,877][root][INFO] - Training Epoch: 2/2, step 1222/7134 completed (loss: 0.22945046424865723, acc: 0.9611650705337524)
[2025-02-13 20:27:42,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:42,254][root][INFO] - Training Epoch: 2/2, step 1223/7134 completed (loss: 0.3526664972305298, acc: 0.8920454382896423)
[2025-02-13 20:27:42,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:42,629][root][INFO] - Training Epoch: 2/2, step 1224/7134 completed (loss: 0.14625637233257294, acc: 0.9622641801834106)
[2025-02-13 20:27:42,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:43,006][root][INFO] - Training Epoch: 2/2, step 1225/7134 completed (loss: 0.2938470244407654, acc: 0.9452054500579834)
[2025-02-13 20:27:43,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:43,371][root][INFO] - Training Epoch: 2/2, step 1226/7134 completed (loss: 0.3613623380661011, acc: 0.89682537317276)
[2025-02-13 20:27:43,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:43,753][root][INFO] - Training Epoch: 2/2, step 1227/7134 completed (loss: 0.3766740560531616, acc: 0.9175257682800293)
[2025-02-13 20:27:43,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:44,121][root][INFO] - Training Epoch: 2/2, step 1228/7134 completed (loss: 0.23403695225715637, acc: 0.9593023061752319)
[2025-02-13 20:27:44,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:44,487][root][INFO] - Training Epoch: 2/2, step 1229/7134 completed (loss: 0.10392776131629944, acc: 0.9851484894752502)
[2025-02-13 20:27:44,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:44,876][root][INFO] - Training Epoch: 2/2, step 1230/7134 completed (loss: 0.23136192560195923, acc: 0.9433962106704712)
[2025-02-13 20:27:45,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:45,286][root][INFO] - Training Epoch: 2/2, step 1231/7134 completed (loss: 0.20706689357757568, acc: 0.9482758641242981)
[2025-02-13 20:27:45,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:45,670][root][INFO] - Training Epoch: 2/2, step 1232/7134 completed (loss: 0.20968802273273468, acc: 0.9417040348052979)
[2025-02-13 20:27:45,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:46,064][root][INFO] - Training Epoch: 2/2, step 1233/7134 completed (loss: 0.2908422350883484, acc: 0.9140271544456482)
[2025-02-13 20:27:46,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:46,459][root][INFO] - Training Epoch: 2/2, step 1234/7134 completed (loss: 0.10489076375961304, acc: 0.9743589758872986)
[2025-02-13 20:27:46,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:46,864][root][INFO] - Training Epoch: 2/2, step 1235/7134 completed (loss: 0.16524410247802734, acc: 0.9744898080825806)
[2025-02-13 20:27:47,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:47,242][root][INFO] - Training Epoch: 2/2, step 1236/7134 completed (loss: 0.2346438467502594, acc: 0.9352940917015076)
[2025-02-13 20:27:47,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:47,619][root][INFO] - Training Epoch: 2/2, step 1237/7134 completed (loss: 0.17468580603599548, acc: 0.9735099077224731)
[2025-02-13 20:27:47,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:47,996][root][INFO] - Training Epoch: 2/2, step 1238/7134 completed (loss: 0.6008497476577759, acc: 0.8651162981987)
[2025-02-13 20:27:48,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:48,392][root][INFO] - Training Epoch: 2/2, step 1239/7134 completed (loss: 0.2374669313430786, acc: 0.9512194991111755)
[2025-02-13 20:27:48,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:48,784][root][INFO] - Training Epoch: 2/2, step 1240/7134 completed (loss: 0.20226219296455383, acc: 0.9476743936538696)
[2025-02-13 20:27:48,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:49,121][root][INFO] - Training Epoch: 2/2, step 1241/7134 completed (loss: 0.06636994332075119, acc: 0.9836065769195557)
[2025-02-13 20:27:49,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:49,517][root][INFO] - Training Epoch: 2/2, step 1242/7134 completed (loss: 0.2788337767124176, acc: 0.9162303805351257)
[2025-02-13 20:27:49,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:49,904][root][INFO] - Training Epoch: 2/2, step 1243/7134 completed (loss: 0.226931631565094, acc: 0.9429824352264404)
[2025-02-13 20:27:50,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:50,281][root][INFO] - Training Epoch: 2/2, step 1244/7134 completed (loss: 0.18848562240600586, acc: 0.9647887349128723)
[2025-02-13 20:27:50,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:50,659][root][INFO] - Training Epoch: 2/2, step 1245/7134 completed (loss: 0.16587600111961365, acc: 0.9494949579238892)
[2025-02-13 20:27:50,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:51,031][root][INFO] - Training Epoch: 2/2, step 1246/7134 completed (loss: 0.32052716612815857, acc: 0.9105263352394104)
[2025-02-13 20:27:51,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:51,407][root][INFO] - Training Epoch: 2/2, step 1247/7134 completed (loss: 0.12646472454071045, acc: 0.9638554453849792)
[2025-02-13 20:27:51,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:51,800][root][INFO] - Training Epoch: 2/2, step 1248/7134 completed (loss: 0.10717736929655075, acc: 0.9800000190734863)
[2025-02-13 20:27:51,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:52,172][root][INFO] - Training Epoch: 2/2, step 1249/7134 completed (loss: 0.07454658299684525, acc: 0.9913793206214905)
[2025-02-13 20:27:52,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:52,539][root][INFO] - Training Epoch: 2/2, step 1250/7134 completed (loss: 0.04600705951452255, acc: 0.9933333396911621)
[2025-02-13 20:27:52,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:52,934][root][INFO] - Training Epoch: 2/2, step 1251/7134 completed (loss: 0.12121739238500595, acc: 0.9757575988769531)
[2025-02-13 20:27:53,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:53,349][root][INFO] - Training Epoch: 2/2, step 1252/7134 completed (loss: 0.08820764720439911, acc: 0.9842519760131836)
[2025-02-13 20:27:53,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:53,747][root][INFO] - Training Epoch: 2/2, step 1253/7134 completed (loss: 0.17455950379371643, acc: 0.9572649598121643)
[2025-02-13 20:27:53,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:54,136][root][INFO] - Training Epoch: 2/2, step 1254/7134 completed (loss: 0.14556190371513367, acc: 0.9599999785423279)
[2025-02-13 20:27:54,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:54,533][root][INFO] - Training Epoch: 2/2, step 1255/7134 completed (loss: 0.08132865279912949, acc: 0.9793103337287903)
[2025-02-13 20:27:54,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:54,916][root][INFO] - Training Epoch: 2/2, step 1256/7134 completed (loss: 0.09807956218719482, acc: 0.9770992398262024)
[2025-02-13 20:27:55,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:55,310][root][INFO] - Training Epoch: 2/2, step 1257/7134 completed (loss: 0.0829630196094513, acc: 0.9770992398262024)
[2025-02-13 20:27:55,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:55,704][root][INFO] - Training Epoch: 2/2, step 1258/7134 completed (loss: 0.15161113440990448, acc: 0.9583333134651184)
[2025-02-13 20:27:55,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:56,068][root][INFO] - Training Epoch: 2/2, step 1259/7134 completed (loss: 0.21181713044643402, acc: 0.9615384340286255)
[2025-02-13 20:27:56,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:56,443][root][INFO] - Training Epoch: 2/2, step 1260/7134 completed (loss: 0.09817666560411453, acc: 0.9798657894134521)
[2025-02-13 20:27:56,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:56,843][root][INFO] - Training Epoch: 2/2, step 1261/7134 completed (loss: 0.1004989966750145, acc: 0.9729729890823364)
[2025-02-13 20:27:56,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:57,210][root][INFO] - Training Epoch: 2/2, step 1262/7134 completed (loss: 0.020587865263223648, acc: 0.9922480583190918)
[2025-02-13 20:27:57,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:57,573][root][INFO] - Training Epoch: 2/2, step 1263/7134 completed (loss: 0.04828009009361267, acc: 0.9870967864990234)
[2025-02-13 20:27:57,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:57,946][root][INFO] - Training Epoch: 2/2, step 1264/7134 completed (loss: 0.054848987609148026, acc: 0.9851852059364319)
[2025-02-13 20:27:58,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:58,329][root][INFO] - Training Epoch: 2/2, step 1265/7134 completed (loss: 0.051037225872278214, acc: 0.9942857027053833)
[2025-02-13 20:27:58,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:58,702][root][INFO] - Training Epoch: 2/2, step 1266/7134 completed (loss: 0.09324329346418381, acc: 0.9770992398262024)
[2025-02-13 20:27:58,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:59,067][root][INFO] - Training Epoch: 2/2, step 1267/7134 completed (loss: 0.04670456424355507, acc: 1.0)
[2025-02-13 20:27:59,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:59,446][root][INFO] - Training Epoch: 2/2, step 1268/7134 completed (loss: 0.08187773078680038, acc: 0.970370352268219)
[2025-02-13 20:27:59,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:59,830][root][INFO] - Training Epoch: 2/2, step 1269/7134 completed (loss: 0.22411957383155823, acc: 0.939393937587738)
[2025-02-13 20:27:59,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:00,205][root][INFO] - Training Epoch: 2/2, step 1270/7134 completed (loss: 0.11022954434156418, acc: 0.965753436088562)
[2025-02-13 20:28:00,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:00,555][root][INFO] - Training Epoch: 2/2, step 1271/7134 completed (loss: 0.12085293978452682, acc: 0.9916666746139526)
[2025-02-13 20:28:00,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:00,927][root][INFO] - Training Epoch: 2/2, step 1272/7134 completed (loss: 0.0330059789121151, acc: 1.0)
[2025-02-13 20:28:01,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:01,310][root][INFO] - Training Epoch: 2/2, step 1273/7134 completed (loss: 0.11592166870832443, acc: 0.9777777791023254)
[2025-02-13 20:28:01,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:01,688][root][INFO] - Training Epoch: 2/2, step 1274/7134 completed (loss: 0.028959738090634346, acc: 0.9917355179786682)
[2025-02-13 20:28:01,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:02,060][root][INFO] - Training Epoch: 2/2, step 1275/7134 completed (loss: 0.0719725638628006, acc: 0.9826086759567261)
[2025-02-13 20:28:02,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:02,414][root][INFO] - Training Epoch: 2/2, step 1276/7134 completed (loss: 0.08798719197511673, acc: 0.9696969985961914)
[2025-02-13 20:28:02,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:02,773][root][INFO] - Training Epoch: 2/2, step 1277/7134 completed (loss: 0.029906587675213814, acc: 1.0)
[2025-02-13 20:28:02,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:03,152][root][INFO] - Training Epoch: 2/2, step 1278/7134 completed (loss: 0.15835878252983093, acc: 0.96875)
[2025-02-13 20:28:03,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:03,531][root][INFO] - Training Epoch: 2/2, step 1279/7134 completed (loss: 0.12365195900201797, acc: 0.9619565010070801)
[2025-02-13 20:28:03,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:03,923][root][INFO] - Training Epoch: 2/2, step 1280/7134 completed (loss: 0.07357899844646454, acc: 0.9786096215248108)
[2025-02-13 20:28:04,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:04,259][root][INFO] - Training Epoch: 2/2, step 1281/7134 completed (loss: 0.08717814087867737, acc: 0.9772727489471436)
[2025-02-13 20:28:04,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:04,658][root][INFO] - Training Epoch: 2/2, step 1282/7134 completed (loss: 0.3293967843055725, acc: 0.9289617538452148)
[2025-02-13 20:28:04,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:05,060][root][INFO] - Training Epoch: 2/2, step 1283/7134 completed (loss: 0.0591326579451561, acc: 0.9922480583190918)
[2025-02-13 20:28:05,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:05,440][root][INFO] - Training Epoch: 2/2, step 1284/7134 completed (loss: 0.14921975135803223, acc: 0.9580838084220886)
[2025-02-13 20:28:05,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:05,790][root][INFO] - Training Epoch: 2/2, step 1285/7134 completed (loss: 0.172406405210495, acc: 0.9750000238418579)
[2025-02-13 20:28:05,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:06,170][root][INFO] - Training Epoch: 2/2, step 1286/7134 completed (loss: 0.087883859872818, acc: 0.9756097793579102)
[2025-02-13 20:28:06,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:06,521][root][INFO] - Training Epoch: 2/2, step 1287/7134 completed (loss: 0.06742307543754578, acc: 0.9777777791023254)
[2025-02-13 20:28:06,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:06,904][root][INFO] - Training Epoch: 2/2, step 1288/7134 completed (loss: 0.05901763215661049, acc: 0.984455943107605)
[2025-02-13 20:28:07,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:07,310][root][INFO] - Training Epoch: 2/2, step 1289/7134 completed (loss: 0.19655005633831024, acc: 0.9448275566101074)
[2025-02-13 20:28:07,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:07,678][root][INFO] - Training Epoch: 2/2, step 1290/7134 completed (loss: 0.05375940352678299, acc: 0.9880239367485046)
[2025-02-13 20:28:07,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:08,072][root][INFO] - Training Epoch: 2/2, step 1291/7134 completed (loss: 0.15037879347801208, acc: 0.9647058844566345)
[2025-02-13 20:28:08,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:08,450][root][INFO] - Training Epoch: 2/2, step 1292/7134 completed (loss: 0.09145943075418472, acc: 0.9670329689979553)
[2025-02-13 20:28:08,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:08,840][root][INFO] - Training Epoch: 2/2, step 1293/7134 completed (loss: 0.20477426052093506, acc: 0.964102566242218)
[2025-02-13 20:28:08,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:09,221][root][INFO] - Training Epoch: 2/2, step 1294/7134 completed (loss: 0.1283247470855713, acc: 0.9562841653823853)
[2025-02-13 20:28:09,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:09,631][root][INFO] - Training Epoch: 2/2, step 1295/7134 completed (loss: 0.0744854211807251, acc: 0.9701492786407471)
[2025-02-13 20:28:09,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:10,042][root][INFO] - Training Epoch: 2/2, step 1296/7134 completed (loss: 0.03167757764458656, acc: 1.0)
[2025-02-13 20:28:10,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:10,395][root][INFO] - Training Epoch: 2/2, step 1297/7134 completed (loss: 0.13182322680950165, acc: 0.9732142686843872)
[2025-02-13 20:28:10,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:10,779][root][INFO] - Training Epoch: 2/2, step 1298/7134 completed (loss: 0.060393597930669785, acc: 0.9917355179786682)
[2025-02-13 20:28:10,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:11,133][root][INFO] - Training Epoch: 2/2, step 1299/7134 completed (loss: 0.038000404834747314, acc: 0.9879518151283264)
[2025-02-13 20:28:11,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:11,517][root][INFO] - Training Epoch: 2/2, step 1300/7134 completed (loss: 0.04861210659146309, acc: 0.9950494766235352)
[2025-02-13 20:28:11,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:11,920][root][INFO] - Training Epoch: 2/2, step 1301/7134 completed (loss: 0.0927330031991005, acc: 0.9736841917037964)
[2025-02-13 20:28:12,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:12,307][root][INFO] - Training Epoch: 2/2, step 1302/7134 completed (loss: 0.12739257514476776, acc: 0.9603960514068604)
[2025-02-13 20:28:12,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:12,696][root][INFO] - Training Epoch: 2/2, step 1303/7134 completed (loss: 0.06174503639340401, acc: 0.9900000095367432)
[2025-02-13 20:28:12,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:13,086][root][INFO] - Training Epoch: 2/2, step 1304/7134 completed (loss: 0.13125145435333252, acc: 0.9830508232116699)
[2025-02-13 20:28:13,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:13,483][root][INFO] - Training Epoch: 2/2, step 1305/7134 completed (loss: 0.15637701749801636, acc: 0.9536082744598389)
[2025-02-13 20:28:13,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:13,889][root][INFO] - Training Epoch: 2/2, step 1306/7134 completed (loss: 0.14381437003612518, acc: 0.9605262875556946)
[2025-02-13 20:28:14,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:14,301][root][INFO] - Training Epoch: 2/2, step 1307/7134 completed (loss: 0.03592894598841667, acc: 0.9928057789802551)
[2025-02-13 20:28:14,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:14,674][root][INFO] - Training Epoch: 2/2, step 1308/7134 completed (loss: 0.061502888798713684, acc: 0.9935483932495117)
[2025-02-13 20:28:14,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:15,052][root][INFO] - Training Epoch: 2/2, step 1309/7134 completed (loss: 0.1176246702671051, acc: 0.9710144996643066)
[2025-02-13 20:28:15,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:15,420][root][INFO] - Training Epoch: 2/2, step 1310/7134 completed (loss: 0.14301243424415588, acc: 0.9723756909370422)
[2025-02-13 20:28:15,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:15,782][root][INFO] - Training Epoch: 2/2, step 1311/7134 completed (loss: 0.07459883391857147, acc: 0.9807692170143127)
[2025-02-13 20:28:15,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:16,162][root][INFO] - Training Epoch: 2/2, step 1312/7134 completed (loss: 0.255074679851532, acc: 0.9503105878829956)
[2025-02-13 20:28:16,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:16,579][root][INFO] - Training Epoch: 2/2, step 1313/7134 completed (loss: 0.18834476172924042, acc: 0.9567901492118835)
[2025-02-13 20:28:16,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:16,980][root][INFO] - Training Epoch: 2/2, step 1314/7134 completed (loss: 0.08693987131118774, acc: 0.9710144996643066)
[2025-02-13 20:28:17,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:17,354][root][INFO] - Training Epoch: 2/2, step 1315/7134 completed (loss: 0.26276895403862, acc: 0.9243243336677551)
[2025-02-13 20:28:17,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:17,737][root][INFO] - Training Epoch: 2/2, step 1316/7134 completed (loss: 0.17680728435516357, acc: 0.9512194991111755)
[2025-02-13 20:28:17,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:18,168][root][INFO] - Training Epoch: 2/2, step 1317/7134 completed (loss: 0.14277003705501556, acc: 0.9552238583564758)
[2025-02-13 20:28:18,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:18,556][root][INFO] - Training Epoch: 2/2, step 1318/7134 completed (loss: 0.0718713328242302, acc: 0.9780219793319702)
[2025-02-13 20:28:18,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:18,935][root][INFO] - Training Epoch: 2/2, step 1319/7134 completed (loss: 0.13513386249542236, acc: 0.9818181991577148)
[2025-02-13 20:28:19,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:19,301][root][INFO] - Training Epoch: 2/2, step 1320/7134 completed (loss: 0.2289707362651825, acc: 0.940397322177887)
[2025-02-13 20:28:19,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:19,694][root][INFO] - Training Epoch: 2/2, step 1321/7134 completed (loss: 0.10920830070972443, acc: 0.9585492014884949)
[2025-02-13 20:28:19,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:20,068][root][INFO] - Training Epoch: 2/2, step 1322/7134 completed (loss: 0.13246959447860718, acc: 0.9591836929321289)
[2025-02-13 20:28:20,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:20,482][root][INFO] - Training Epoch: 2/2, step 1323/7134 completed (loss: 0.2210996448993683, acc: 0.9507042169570923)
[2025-02-13 20:28:20,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:20,856][root][INFO] - Training Epoch: 2/2, step 1324/7134 completed (loss: 0.06931041926145554, acc: 0.9869281053543091)
[2025-02-13 20:28:20,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:21,222][root][INFO] - Training Epoch: 2/2, step 1325/7134 completed (loss: 0.1006205677986145, acc: 0.9805825352668762)
[2025-02-13 20:28:21,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:21,599][root][INFO] - Training Epoch: 2/2, step 1326/7134 completed (loss: 0.25593966245651245, acc: 0.9452054500579834)
[2025-02-13 20:28:21,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:21,970][root][INFO] - Training Epoch: 2/2, step 1327/7134 completed (loss: 0.08753282576799393, acc: 0.9822485446929932)
[2025-02-13 20:28:22,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:22,338][root][INFO] - Training Epoch: 2/2, step 1328/7134 completed (loss: 0.09775067120790482, acc: 0.9603960514068604)
[2025-02-13 20:28:22,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:22,712][root][INFO] - Training Epoch: 2/2, step 1329/7134 completed (loss: 0.15130051970481873, acc: 0.9599999785423279)
[2025-02-13 20:28:22,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:23,053][root][INFO] - Training Epoch: 2/2, step 1330/7134 completed (loss: 0.07457740604877472, acc: 0.9784172773361206)
[2025-02-13 20:28:23,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:23,429][root][INFO] - Training Epoch: 2/2, step 1331/7134 completed (loss: 0.04937959834933281, acc: 0.98591548204422)
[2025-02-13 20:28:23,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:23,761][root][INFO] - Training Epoch: 2/2, step 1332/7134 completed (loss: 0.06852135807275772, acc: 0.9736841917037964)
[2025-02-13 20:28:23,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:24,153][root][INFO] - Training Epoch: 2/2, step 1333/7134 completed (loss: 0.12646915018558502, acc: 0.9736841917037964)
[2025-02-13 20:28:24,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:24,540][root][INFO] - Training Epoch: 2/2, step 1334/7134 completed (loss: 0.27477505803108215, acc: 0.9382715821266174)
[2025-02-13 20:28:24,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:24,894][root][INFO] - Training Epoch: 2/2, step 1335/7134 completed (loss: 0.23892457783222198, acc: 0.9528796076774597)
[2025-02-13 20:28:25,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:25,263][root][INFO] - Training Epoch: 2/2, step 1336/7134 completed (loss: 0.14090470969676971, acc: 0.9518716335296631)
[2025-02-13 20:28:25,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:25,650][root][INFO] - Training Epoch: 2/2, step 1337/7134 completed (loss: 0.11657131463289261, acc: 0.9612902998924255)
[2025-02-13 20:28:25,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:25,998][root][INFO] - Training Epoch: 2/2, step 1338/7134 completed (loss: 0.290290892124176, acc: 0.953125)
[2025-02-13 20:28:26,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:26,364][root][INFO] - Training Epoch: 2/2, step 1339/7134 completed (loss: 0.24686264991760254, acc: 0.9387755393981934)
[2025-02-13 20:28:26,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:26,741][root][INFO] - Training Epoch: 2/2, step 1340/7134 completed (loss: 0.16591013967990875, acc: 0.9805194735527039)
[2025-02-13 20:28:26,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:27,093][root][INFO] - Training Epoch: 2/2, step 1341/7134 completed (loss: 0.04850006103515625, acc: 0.9890109896659851)
[2025-02-13 20:28:27,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:27,442][root][INFO] - Training Epoch: 2/2, step 1342/7134 completed (loss: 0.05960587039589882, acc: 0.9801324605941772)
[2025-02-13 20:28:27,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:27,820][root][INFO] - Training Epoch: 2/2, step 1343/7134 completed (loss: 0.16873715817928314, acc: 0.9411764740943909)
[2025-02-13 20:28:27,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:28,177][root][INFO] - Training Epoch: 2/2, step 1344/7134 completed (loss: 0.0514950156211853, acc: 0.9908257126808167)
[2025-02-13 20:28:28,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:28,597][root][INFO] - Training Epoch: 2/2, step 1345/7134 completed (loss: 0.056085024029016495, acc: 1.0)
[2025-02-13 20:28:28,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:29,008][root][INFO] - Training Epoch: 2/2, step 1346/7134 completed (loss: 0.10021361708641052, acc: 0.9836065769195557)
[2025-02-13 20:28:29,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:29,383][root][INFO] - Training Epoch: 2/2, step 1347/7134 completed (loss: 0.10301279276609421, acc: 0.9722222089767456)
[2025-02-13 20:28:29,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:29,761][root][INFO] - Training Epoch: 2/2, step 1348/7134 completed (loss: 0.05394122004508972, acc: 0.988950252532959)
[2025-02-13 20:28:29,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:30,157][root][INFO] - Training Epoch: 2/2, step 1349/7134 completed (loss: 0.02496408298611641, acc: 1.0)
[2025-02-13 20:28:30,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:30,519][root][INFO] - Training Epoch: 2/2, step 1350/7134 completed (loss: 0.05707661807537079, acc: 0.9820359349250793)
[2025-02-13 20:28:30,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:30,882][root][INFO] - Training Epoch: 2/2, step 1351/7134 completed (loss: 0.021669037640094757, acc: 1.0)
[2025-02-13 20:28:31,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:31,263][root][INFO] - Training Epoch: 2/2, step 1352/7134 completed (loss: 0.08108599483966827, acc: 0.9817073345184326)
[2025-02-13 20:28:31,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:31,623][root][INFO] - Training Epoch: 2/2, step 1353/7134 completed (loss: 0.06028358265757561, acc: 0.9916666746139526)
[2025-02-13 20:28:31,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:32,000][root][INFO] - Training Epoch: 2/2, step 1354/7134 completed (loss: 0.012620688416063786, acc: 1.0)
[2025-02-13 20:28:32,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:32,365][root][INFO] - Training Epoch: 2/2, step 1355/7134 completed (loss: 0.02112802304327488, acc: 0.9870967864990234)
[2025-02-13 20:28:32,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:32,746][root][INFO] - Training Epoch: 2/2, step 1356/7134 completed (loss: 0.054474566131830215, acc: 0.9883720874786377)
[2025-02-13 20:28:32,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:33,125][root][INFO] - Training Epoch: 2/2, step 1357/7134 completed (loss: 0.021472960710525513, acc: 1.0)
[2025-02-13 20:28:33,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:33,485][root][INFO] - Training Epoch: 2/2, step 1358/7134 completed (loss: 0.051200103014707565, acc: 0.9886363744735718)
[2025-02-13 20:28:33,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:33,849][root][INFO] - Training Epoch: 2/2, step 1359/7134 completed (loss: 0.015251608565449715, acc: 1.0)
[2025-02-13 20:28:33,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:34,219][root][INFO] - Training Epoch: 2/2, step 1360/7134 completed (loss: 0.014252032153308392, acc: 1.0)
[2025-02-13 20:28:34,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:34,621][root][INFO] - Training Epoch: 2/2, step 1361/7134 completed (loss: 0.03986736014485359, acc: 0.9940828680992126)
[2025-02-13 20:28:34,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:34,997][root][INFO] - Training Epoch: 2/2, step 1362/7134 completed (loss: 0.0310048945248127, acc: 0.994535505771637)
[2025-02-13 20:28:35,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:35,387][root][INFO] - Training Epoch: 2/2, step 1363/7134 completed (loss: 0.03392241522669792, acc: 0.982758641242981)
[2025-02-13 20:28:35,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:35,774][root][INFO] - Training Epoch: 2/2, step 1364/7134 completed (loss: 0.032799892127513885, acc: 0.9882352948188782)
[2025-02-13 20:28:35,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:36,183][root][INFO] - Training Epoch: 2/2, step 1365/7134 completed (loss: 0.012400038540363312, acc: 1.0)
[2025-02-13 20:28:36,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:36,573][root][INFO] - Training Epoch: 2/2, step 1366/7134 completed (loss: 0.009223476983606815, acc: 1.0)
[2025-02-13 20:28:36,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:36,986][root][INFO] - Training Epoch: 2/2, step 1367/7134 completed (loss: 0.021845225244760513, acc: 0.9926470518112183)
[2025-02-13 20:28:37,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:37,374][root][INFO] - Training Epoch: 2/2, step 1368/7134 completed (loss: 0.037643224000930786, acc: 0.9942528605461121)
[2025-02-13 20:28:37,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:37,749][root][INFO] - Training Epoch: 2/2, step 1369/7134 completed (loss: 0.050407953560352325, acc: 0.9927007555961609)
[2025-02-13 20:28:37,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:38,111][root][INFO] - Training Epoch: 2/2, step 1370/7134 completed (loss: 0.0886976346373558, acc: 0.9897959232330322)
[2025-02-13 20:28:38,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:38,526][root][INFO] - Training Epoch: 2/2, step 1371/7134 completed (loss: 0.06231861189007759, acc: 0.9814814925193787)
[2025-02-13 20:28:38,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:38,896][root][INFO] - Training Epoch: 2/2, step 1372/7134 completed (loss: 0.08001788705587387, acc: 0.9696969985961914)
[2025-02-13 20:28:39,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:39,276][root][INFO] - Training Epoch: 2/2, step 1373/7134 completed (loss: 0.06487379968166351, acc: 0.9844961166381836)
[2025-02-13 20:28:39,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:39,669][root][INFO] - Training Epoch: 2/2, step 1374/7134 completed (loss: 0.11373031139373779, acc: 0.9666666388511658)
[2025-02-13 20:28:39,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:40,037][root][INFO] - Training Epoch: 2/2, step 1375/7134 completed (loss: 0.10706739127635956, acc: 0.970588207244873)
[2025-02-13 20:28:40,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:40,411][root][INFO] - Training Epoch: 2/2, step 1376/7134 completed (loss: 0.17066612839698792, acc: 0.9642857313156128)
[2025-02-13 20:28:40,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:40,796][root][INFO] - Training Epoch: 2/2, step 1377/7134 completed (loss: 0.1490670144557953, acc: 0.9411764740943909)
[2025-02-13 20:28:40,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:41,182][root][INFO] - Training Epoch: 2/2, step 1378/7134 completed (loss: 0.12598411738872528, acc: 0.9903846383094788)
[2025-02-13 20:28:41,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:41,539][root][INFO] - Training Epoch: 2/2, step 1379/7134 completed (loss: 0.14516682922840118, acc: 0.9504950642585754)
[2025-02-13 20:28:41,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:41,922][root][INFO] - Training Epoch: 2/2, step 1380/7134 completed (loss: 0.07884277403354645, acc: 0.9629629850387573)
[2025-02-13 20:28:42,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:42,334][root][INFO] - Training Epoch: 2/2, step 1381/7134 completed (loss: 0.07065735757350922, acc: 0.9855072498321533)
[2025-02-13 20:28:42,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:42,746][root][INFO] - Training Epoch: 2/2, step 1382/7134 completed (loss: 0.10984120517969131, acc: 0.9769230484962463)
[2025-02-13 20:28:42,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:43,143][root][INFO] - Training Epoch: 2/2, step 1383/7134 completed (loss: 0.2257215529680252, acc: 0.9558823704719543)
[2025-02-13 20:28:43,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:43,533][root][INFO] - Training Epoch: 2/2, step 1384/7134 completed (loss: 0.1766931265592575, acc: 0.9645389914512634)
[2025-02-13 20:28:43,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:43,896][root][INFO] - Training Epoch: 2/2, step 1385/7134 completed (loss: 0.25652143359184265, acc: 0.9440000057220459)
[2025-02-13 20:28:44,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:44,269][root][INFO] - Training Epoch: 2/2, step 1386/7134 completed (loss: 0.07143310457468033, acc: 0.98591548204422)
[2025-02-13 20:28:44,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:44,647][root][INFO] - Training Epoch: 2/2, step 1387/7134 completed (loss: 0.065921351313591, acc: 0.9930070042610168)
[2025-02-13 20:28:44,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:45,026][root][INFO] - Training Epoch: 2/2, step 1388/7134 completed (loss: 0.14140678942203522, acc: 0.9599999785423279)
[2025-02-13 20:28:45,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:45,426][root][INFO] - Training Epoch: 2/2, step 1389/7134 completed (loss: 0.10528314858675003, acc: 0.9850746393203735)
[2025-02-13 20:28:45,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:45,822][root][INFO] - Training Epoch: 2/2, step 1390/7134 completed (loss: 0.11644694954156876, acc: 0.970802903175354)
[2025-02-13 20:28:45,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:46,196][root][INFO] - Training Epoch: 2/2, step 1391/7134 completed (loss: 0.2342909276485443, acc: 0.9264705777168274)
[2025-02-13 20:28:46,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:46,587][root][INFO] - Training Epoch: 2/2, step 1392/7134 completed (loss: 0.11581696569919586, acc: 0.9756097793579102)
[2025-02-13 20:28:46,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:46,978][root][INFO] - Training Epoch: 2/2, step 1393/7134 completed (loss: 0.10303095728158951, acc: 0.96875)
[2025-02-13 20:28:47,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:47,340][root][INFO] - Training Epoch: 2/2, step 1394/7134 completed (loss: 0.08674443513154984, acc: 0.9905660152435303)
[2025-02-13 20:28:47,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:47,725][root][INFO] - Training Epoch: 2/2, step 1395/7134 completed (loss: 0.1735733300447464, acc: 0.9765625)
[2025-02-13 20:28:47,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:48,107][root][INFO] - Training Epoch: 2/2, step 1396/7134 completed (loss: 0.062156595289707184, acc: 0.9760000109672546)
[2025-02-13 20:28:48,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:48,482][root][INFO] - Training Epoch: 2/2, step 1397/7134 completed (loss: 0.38179656863212585, acc: 0.912162184715271)
[2025-02-13 20:28:48,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:48,864][root][INFO] - Training Epoch: 2/2, step 1398/7134 completed (loss: 0.059159837663173676, acc: 0.9801980257034302)
[2025-02-13 20:28:49,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:49,278][root][INFO] - Training Epoch: 2/2, step 1399/7134 completed (loss: 0.1774023473262787, acc: 0.9424460530281067)
[2025-02-13 20:28:49,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:49,644][root][INFO] - Training Epoch: 2/2, step 1400/7134 completed (loss: 0.04831541329622269, acc: 0.9927536249160767)
[2025-02-13 20:28:49,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:50,021][root][INFO] - Training Epoch: 2/2, step 1401/7134 completed (loss: 0.06874649226665497, acc: 0.9851852059364319)
[2025-02-13 20:28:50,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:50,400][root][INFO] - Training Epoch: 2/2, step 1402/7134 completed (loss: 0.05609792843461037, acc: 0.9776119589805603)
[2025-02-13 20:28:50,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:50,815][root][INFO] - Training Epoch: 2/2, step 1403/7134 completed (loss: 0.10079829394817352, acc: 0.9704142212867737)
[2025-02-13 20:28:50,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:51,193][root][INFO] - Training Epoch: 2/2, step 1404/7134 completed (loss: 0.059581268578767776, acc: 0.988095223903656)
[2025-02-13 20:28:51,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:51,548][root][INFO] - Training Epoch: 2/2, step 1405/7134 completed (loss: 0.01855507120490074, acc: 1.0)
[2025-02-13 20:28:51,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:51,921][root][INFO] - Training Epoch: 2/2, step 1406/7134 completed (loss: 0.02654310129582882, acc: 0.9937888383865356)
[2025-02-13 20:28:52,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:52,320][root][INFO] - Training Epoch: 2/2, step 1407/7134 completed (loss: 0.013069704174995422, acc: 1.0)
[2025-02-13 20:28:52,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:52,721][root][INFO] - Training Epoch: 2/2, step 1408/7134 completed (loss: 0.020667361095547676, acc: 1.0)
[2025-02-13 20:28:52,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:53,123][root][INFO] - Training Epoch: 2/2, step 1409/7134 completed (loss: 0.010761226527392864, acc: 1.0)
[2025-02-13 20:28:53,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:53,496][root][INFO] - Training Epoch: 2/2, step 1410/7134 completed (loss: 0.13453812897205353, acc: 0.9834254384040833)
[2025-02-13 20:28:53,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:53,858][root][INFO] - Training Epoch: 2/2, step 1411/7134 completed (loss: 0.042694613337516785, acc: 0.9885714054107666)
[2025-02-13 20:28:53,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:54,231][root][INFO] - Training Epoch: 2/2, step 1412/7134 completed (loss: 0.08388730138540268, acc: 0.9736841917037964)
[2025-02-13 20:28:54,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:54,610][root][INFO] - Training Epoch: 2/2, step 1413/7134 completed (loss: 0.033736780285835266, acc: 0.9940119981765747)
[2025-02-13 20:28:54,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:54,988][root][INFO] - Training Epoch: 2/2, step 1414/7134 completed (loss: 0.009006530977785587, acc: 1.0)
[2025-02-13 20:28:55,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:55,363][root][INFO] - Training Epoch: 2/2, step 1415/7134 completed (loss: 0.09305188059806824, acc: 0.970802903175354)
[2025-02-13 20:28:55,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:55,751][root][INFO] - Training Epoch: 2/2, step 1416/7134 completed (loss: 0.009318333119153976, acc: 1.0)
[2025-02-13 20:28:55,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:56,139][root][INFO] - Training Epoch: 2/2, step 1417/7134 completed (loss: 0.02336398884654045, acc: 0.9930555820465088)
[2025-02-13 20:28:56,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:56,581][root][INFO] - Training Epoch: 2/2, step 1418/7134 completed (loss: 0.011571762152016163, acc: 1.0)
[2025-02-13 20:28:56,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:56,991][root][INFO] - Training Epoch: 2/2, step 1419/7134 completed (loss: 0.05166909843683243, acc: 0.9886363744735718)
[2025-02-13 20:28:57,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:57,371][root][INFO] - Training Epoch: 2/2, step 1420/7134 completed (loss: 0.05480831488966942, acc: 0.9789473414421082)
[2025-02-13 20:28:57,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:57,777][root][INFO] - Training Epoch: 2/2, step 1421/7134 completed (loss: 0.014263044111430645, acc: 1.0)
[2025-02-13 20:28:57,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:58,145][root][INFO] - Training Epoch: 2/2, step 1422/7134 completed (loss: 0.09567821025848389, acc: 0.9698795080184937)
[2025-02-13 20:28:58,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:58,500][root][INFO] - Training Epoch: 2/2, step 1423/7134 completed (loss: 0.29109668731689453, acc: 0.9312977194786072)
[2025-02-13 20:28:58,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:58,882][root][INFO] - Training Epoch: 2/2, step 1424/7134 completed (loss: 0.14597639441490173, acc: 0.9593495726585388)
[2025-02-13 20:28:59,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:59,267][root][INFO] - Training Epoch: 2/2, step 1425/7134 completed (loss: 0.1459614336490631, acc: 0.9479768872261047)
[2025-02-13 20:28:59,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:59,654][root][INFO] - Training Epoch: 2/2, step 1426/7134 completed (loss: 0.06302957981824875, acc: 0.9896907210350037)
[2025-02-13 20:28:59,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:00,038][root][INFO] - Training Epoch: 2/2, step 1427/7134 completed (loss: 0.18330027163028717, acc: 0.9322034120559692)
[2025-02-13 20:29:00,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:00,404][root][INFO] - Training Epoch: 2/2, step 1428/7134 completed (loss: 0.5151417255401611, acc: 0.8548387289047241)
[2025-02-13 20:29:00,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:00,786][root][INFO] - Training Epoch: 2/2, step 1429/7134 completed (loss: 0.1187279149889946, acc: 0.9722222089767456)
[2025-02-13 20:29:00,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:01,180][root][INFO] - Training Epoch: 2/2, step 1430/7134 completed (loss: 0.14778371155261993, acc: 0.9640287756919861)
[2025-02-13 20:29:01,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:01,557][root][INFO] - Training Epoch: 2/2, step 1431/7134 completed (loss: 0.05713821202516556, acc: 0.9784946441650391)
[2025-02-13 20:29:01,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:01,944][root][INFO] - Training Epoch: 2/2, step 1432/7134 completed (loss: 0.043786998838186264, acc: 0.9899497628211975)
[2025-02-13 20:29:02,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:02,320][root][INFO] - Training Epoch: 2/2, step 1433/7134 completed (loss: 0.059723541140556335, acc: 0.9852941036224365)
[2025-02-13 20:29:02,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:02,731][root][INFO] - Training Epoch: 2/2, step 1434/7134 completed (loss: 0.057309895753860474, acc: 0.9950248599052429)
[2025-02-13 20:29:02,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:03,128][root][INFO] - Training Epoch: 2/2, step 1435/7134 completed (loss: 0.0728677436709404, acc: 0.9838709831237793)
[2025-02-13 20:29:03,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:03,514][root][INFO] - Training Epoch: 2/2, step 1436/7134 completed (loss: 0.07669796794652939, acc: 0.9861111044883728)
[2025-02-13 20:29:03,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:03,890][root][INFO] - Training Epoch: 2/2, step 1437/7134 completed (loss: 0.061160795390605927, acc: 0.9892473220825195)
[2025-02-13 20:29:04,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:04,273][root][INFO] - Training Epoch: 2/2, step 1438/7134 completed (loss: 0.07343599945306778, acc: 0.9878787994384766)
[2025-02-13 20:29:04,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:04,696][root][INFO] - Training Epoch: 2/2, step 1439/7134 completed (loss: 0.03923368453979492, acc: 0.9925925731658936)
[2025-02-13 20:29:04,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:05,057][root][INFO] - Training Epoch: 2/2, step 1440/7134 completed (loss: 0.059839170426130295, acc: 0.9813664555549622)
[2025-02-13 20:29:05,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:05,422][root][INFO] - Training Epoch: 2/2, step 1441/7134 completed (loss: 0.018048657104372978, acc: 0.9942528605461121)
[2025-02-13 20:29:05,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:05,769][root][INFO] - Training Epoch: 2/2, step 1442/7134 completed (loss: 0.02976338379085064, acc: 0.9821428656578064)
[2025-02-13 20:29:05,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:06,148][root][INFO] - Training Epoch: 2/2, step 1443/7134 completed (loss: 0.13825587928295135, acc: 0.9814814925193787)
[2025-02-13 20:29:06,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:06,518][root][INFO] - Training Epoch: 2/2, step 1444/7134 completed (loss: 0.09462360292673111, acc: 0.9791666865348816)
[2025-02-13 20:29:06,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:06,900][root][INFO] - Training Epoch: 2/2, step 1445/7134 completed (loss: 0.07561475038528442, acc: 0.987261176109314)
[2025-02-13 20:29:07,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:07,302][root][INFO] - Training Epoch: 2/2, step 1446/7134 completed (loss: 0.05515080317854881, acc: 0.9898989796638489)
[2025-02-13 20:29:07,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:07,705][root][INFO] - Training Epoch: 2/2, step 1447/7134 completed (loss: 0.045934487134218216, acc: 0.9892473220825195)
[2025-02-13 20:29:07,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:08,137][root][INFO] - Training Epoch: 2/2, step 1448/7134 completed (loss: 0.15090049803256989, acc: 0.9402984976768494)
[2025-02-13 20:29:08,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:08,575][root][INFO] - Training Epoch: 2/2, step 1449/7134 completed (loss: 0.16958646476268768, acc: 0.9562841653823853)
[2025-02-13 20:29:08,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:08,944][root][INFO] - Training Epoch: 2/2, step 1450/7134 completed (loss: 0.08446977287530899, acc: 0.9839572310447693)
[2025-02-13 20:29:09,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:09,302][root][INFO] - Training Epoch: 2/2, step 1451/7134 completed (loss: 0.05610313639044762, acc: 0.9842932224273682)
[2025-02-13 20:29:09,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:09,735][root][INFO] - Training Epoch: 2/2, step 1452/7134 completed (loss: 0.11127502471208572, acc: 0.9791666865348816)
[2025-02-13 20:29:09,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:10,074][root][INFO] - Training Epoch: 2/2, step 1453/7134 completed (loss: 0.31910789012908936, acc: 0.9011628031730652)
[2025-02-13 20:29:10,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:10,500][root][INFO] - Training Epoch: 2/2, step 1454/7134 completed (loss: 0.10966943949460983, acc: 0.96875)
[2025-02-13 20:29:10,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:10,897][root][INFO] - Training Epoch: 2/2, step 1455/7134 completed (loss: 0.23301878571510315, acc: 0.931034505367279)
[2025-02-13 20:29:11,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:11,251][root][INFO] - Training Epoch: 2/2, step 1456/7134 completed (loss: 0.15339446067810059, acc: 0.9586777091026306)
[2025-02-13 20:29:11,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:11,610][root][INFO] - Training Epoch: 2/2, step 1457/7134 completed (loss: 0.06113176792860031, acc: 0.9813664555549622)
[2025-02-13 20:29:11,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:11,982][root][INFO] - Training Epoch: 2/2, step 1458/7134 completed (loss: 0.04334333911538124, acc: 0.9940828680992126)
[2025-02-13 20:29:12,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:12,366][root][INFO] - Training Epoch: 2/2, step 1459/7134 completed (loss: 0.03629367798566818, acc: 0.9925925731658936)
[2025-02-13 20:29:12,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:12,759][root][INFO] - Training Epoch: 2/2, step 1460/7134 completed (loss: 0.1300448328256607, acc: 0.9704142212867737)
[2025-02-13 20:29:12,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:13,130][root][INFO] - Training Epoch: 2/2, step 1461/7134 completed (loss: 0.07878542691469193, acc: 0.9602649211883545)
[2025-02-13 20:29:13,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:13,506][root][INFO] - Training Epoch: 2/2, step 1462/7134 completed (loss: 0.06901717185974121, acc: 0.9850746393203735)
[2025-02-13 20:29:13,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:13,880][root][INFO] - Training Epoch: 2/2, step 1463/7134 completed (loss: 0.09130100160837173, acc: 0.9803921580314636)
[2025-02-13 20:29:14,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:14,255][root][INFO] - Training Epoch: 2/2, step 1464/7134 completed (loss: 0.17943882942199707, acc: 0.9578313231468201)
[2025-02-13 20:29:14,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:14,651][root][INFO] - Training Epoch: 2/2, step 1465/7134 completed (loss: 0.13130298256874084, acc: 0.9743589758872986)
[2025-02-13 20:29:14,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:15,042][root][INFO] - Training Epoch: 2/2, step 1466/7134 completed (loss: 0.02708544209599495, acc: 0.994413435459137)
[2025-02-13 20:29:15,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:15,394][root][INFO] - Training Epoch: 2/2, step 1467/7134 completed (loss: 0.03576516732573509, acc: 0.9878787994384766)
[2025-02-13 20:29:15,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:15,806][root][INFO] - Training Epoch: 2/2, step 1468/7134 completed (loss: 0.04730316251516342, acc: 0.9887640476226807)
[2025-02-13 20:29:15,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:16,198][root][INFO] - Training Epoch: 2/2, step 1469/7134 completed (loss: 0.021539000794291496, acc: 1.0)
[2025-02-13 20:29:16,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:16,557][root][INFO] - Training Epoch: 2/2, step 1470/7134 completed (loss: 0.044674504548311234, acc: 0.987261176109314)
[2025-02-13 20:29:16,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:16,924][root][INFO] - Training Epoch: 2/2, step 1471/7134 completed (loss: 0.1802314817905426, acc: 0.9461538195610046)
[2025-02-13 20:29:17,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:17,299][root][INFO] - Training Epoch: 2/2, step 1472/7134 completed (loss: 0.03342225030064583, acc: 0.9941520690917969)
[2025-02-13 20:29:17,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:17,670][root][INFO] - Training Epoch: 2/2, step 1473/7134 completed (loss: 0.06629602611064911, acc: 0.9818181991577148)
[2025-02-13 20:29:17,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:18,026][root][INFO] - Training Epoch: 2/2, step 1474/7134 completed (loss: 0.0765254944562912, acc: 0.9655172228813171)
[2025-02-13 20:29:18,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:18,394][root][INFO] - Training Epoch: 2/2, step 1475/7134 completed (loss: 0.18312616646289825, acc: 0.9757575988769531)
[2025-02-13 20:29:18,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:18,781][root][INFO] - Training Epoch: 2/2, step 1476/7134 completed (loss: 0.08706555515527725, acc: 0.9866666793823242)
[2025-02-13 20:29:18,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:19,141][root][INFO] - Training Epoch: 2/2, step 1477/7134 completed (loss: 0.10102470964193344, acc: 0.9655172228813171)
[2025-02-13 20:29:19,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:19,545][root][INFO] - Training Epoch: 2/2, step 1478/7134 completed (loss: 0.11425045877695084, acc: 0.9802631735801697)
[2025-02-13 20:29:19,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:19,934][root][INFO] - Training Epoch: 2/2, step 1479/7134 completed (loss: 0.03299294784665108, acc: 0.9942528605461121)
[2025-02-13 20:29:20,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:20,317][root][INFO] - Training Epoch: 2/2, step 1480/7134 completed (loss: 0.023342909291386604, acc: 0.9944751262664795)
[2025-02-13 20:29:20,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:20,718][root][INFO] - Training Epoch: 2/2, step 1481/7134 completed (loss: 0.09103990346193314, acc: 0.9833333492279053)
[2025-02-13 20:29:20,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:21,095][root][INFO] - Training Epoch: 2/2, step 1482/7134 completed (loss: 0.08951220661401749, acc: 0.976331353187561)
[2025-02-13 20:29:21,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:21,468][root][INFO] - Training Epoch: 2/2, step 1483/7134 completed (loss: 0.02754245698451996, acc: 0.9923664331436157)
[2025-02-13 20:29:21,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:21,864][root][INFO] - Training Epoch: 2/2, step 1484/7134 completed (loss: 0.17271369695663452, acc: 0.9759036302566528)
[2025-02-13 20:29:22,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:22,241][root][INFO] - Training Epoch: 2/2, step 1485/7134 completed (loss: 0.07390830665826797, acc: 0.9824561476707458)
[2025-02-13 20:29:22,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:22,612][root][INFO] - Training Epoch: 2/2, step 1486/7134 completed (loss: 0.036125730723142624, acc: 0.9937106966972351)
[2025-02-13 20:29:22,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:22,993][root][INFO] - Training Epoch: 2/2, step 1487/7134 completed (loss: 0.03978005051612854, acc: 0.9783783555030823)
[2025-02-13 20:29:23,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:23,396][root][INFO] - Training Epoch: 2/2, step 1488/7134 completed (loss: 0.11127964407205582, acc: 0.9767441749572754)
[2025-02-13 20:29:23,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:23,762][root][INFO] - Training Epoch: 2/2, step 1489/7134 completed (loss: 0.09547750651836395, acc: 0.9795918464660645)
[2025-02-13 20:29:23,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:24,143][root][INFO] - Training Epoch: 2/2, step 1490/7134 completed (loss: 0.07401800900697708, acc: 0.9646017551422119)
[2025-02-13 20:29:24,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:24,521][root][INFO] - Training Epoch: 2/2, step 1491/7134 completed (loss: 0.031915463507175446, acc: 0.9952380657196045)
[2025-02-13 20:29:24,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:24,913][root][INFO] - Training Epoch: 2/2, step 1492/7134 completed (loss: 0.09495415538549423, acc: 0.971563994884491)
[2025-02-13 20:29:25,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:25,282][root][INFO] - Training Epoch: 2/2, step 1493/7134 completed (loss: 0.06327608227729797, acc: 0.9906542301177979)
[2025-02-13 20:29:25,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:25,667][root][INFO] - Training Epoch: 2/2, step 1494/7134 completed (loss: 0.03594985604286194, acc: 0.9896373152732849)
[2025-02-13 20:29:25,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:26,031][root][INFO] - Training Epoch: 2/2, step 1495/7134 completed (loss: 0.025576459243893623, acc: 1.0)
[2025-02-13 20:29:26,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:26,456][root][INFO] - Training Epoch: 2/2, step 1496/7134 completed (loss: 0.07359844446182251, acc: 0.987500011920929)
[2025-02-13 20:29:26,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:26,835][root][INFO] - Training Epoch: 2/2, step 1497/7134 completed (loss: 0.03045094758272171, acc: 1.0)
[2025-02-13 20:29:26,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:27,196][root][INFO] - Training Epoch: 2/2, step 1498/7134 completed (loss: 0.0367966964840889, acc: 0.9891892075538635)
[2025-02-13 20:29:27,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:27,587][root][INFO] - Training Epoch: 2/2, step 1499/7134 completed (loss: 0.06665224581956863, acc: 0.9720930457115173)
[2025-02-13 20:29:27,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:27,944][root][INFO] - Training Epoch: 2/2, step 1500/7134 completed (loss: 0.12758015096187592, acc: 0.9520547986030579)
[2025-02-13 20:29:28,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:28,323][root][INFO] - Training Epoch: 2/2, step 1501/7134 completed (loss: 0.14385086297988892, acc: 0.9644970297813416)
[2025-02-13 20:29:28,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:28,705][root][INFO] - Training Epoch: 2/2, step 1502/7134 completed (loss: 0.12306376546621323, acc: 0.9727272987365723)
[2025-02-13 20:29:28,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:29,080][root][INFO] - Training Epoch: 2/2, step 1503/7134 completed (loss: 0.10493694990873337, acc: 0.9828571677207947)
[2025-02-13 20:29:29,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:29,441][root][INFO] - Training Epoch: 2/2, step 1504/7134 completed (loss: 0.048403266817331314, acc: 0.9894179701805115)
[2025-02-13 20:29:29,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:29,826][root][INFO] - Training Epoch: 2/2, step 1505/7134 completed (loss: 0.07155560702085495, acc: 0.9746835231781006)
[2025-02-13 20:29:29,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:30,196][root][INFO] - Training Epoch: 2/2, step 1506/7134 completed (loss: 0.09218529611825943, acc: 0.9685534834861755)
[2025-02-13 20:29:30,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:30,563][root][INFO] - Training Epoch: 2/2, step 1507/7134 completed (loss: 0.10865798592567444, acc: 0.96875)
[2025-02-13 20:29:30,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:30,972][root][INFO] - Training Epoch: 2/2, step 1508/7134 completed (loss: 0.05183256417512894, acc: 0.985981285572052)
[2025-02-13 20:29:31,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:31,344][root][INFO] - Training Epoch: 2/2, step 1509/7134 completed (loss: 0.057344477623701096, acc: 0.9833333492279053)
[2025-02-13 20:29:31,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:31,731][root][INFO] - Training Epoch: 2/2, step 1510/7134 completed (loss: 0.10000666230916977, acc: 0.9548386931419373)
[2025-02-13 20:29:31,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:32,110][root][INFO] - Training Epoch: 2/2, step 1511/7134 completed (loss: 0.09121681749820709, acc: 0.9664804339408875)
[2025-02-13 20:29:32,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:32,516][root][INFO] - Training Epoch: 2/2, step 1512/7134 completed (loss: 0.02313700132071972, acc: 1.0)
[2025-02-13 20:29:32,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:32,898][root][INFO] - Training Epoch: 2/2, step 1513/7134 completed (loss: 0.03529628738760948, acc: 0.9944751262664795)
[2025-02-13 20:29:33,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:33,303][root][INFO] - Training Epoch: 2/2, step 1514/7134 completed (loss: 0.08940227329730988, acc: 0.9637305736541748)
[2025-02-13 20:29:33,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:33,729][root][INFO] - Training Epoch: 2/2, step 1515/7134 completed (loss: 0.10323118418455124, acc: 0.9819819927215576)
[2025-02-13 20:29:33,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:34,082][root][INFO] - Training Epoch: 2/2, step 1516/7134 completed (loss: 0.1471766084432602, acc: 0.949999988079071)
[2025-02-13 20:29:34,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:34,485][root][INFO] - Training Epoch: 2/2, step 1517/7134 completed (loss: 0.19633318483829498, acc: 0.9464285969734192)
[2025-02-13 20:29:34,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:34,864][root][INFO] - Training Epoch: 2/2, step 1518/7134 completed (loss: 0.04103144630789757, acc: 0.9896907210350037)
[2025-02-13 20:29:35,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:35,254][root][INFO] - Training Epoch: 2/2, step 1519/7134 completed (loss: 0.06820200383663177, acc: 0.9805825352668762)
[2025-02-13 20:29:35,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:35,614][root][INFO] - Training Epoch: 2/2, step 1520/7134 completed (loss: 0.04817989096045494, acc: 0.985401451587677)
[2025-02-13 20:29:35,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:35,994][root][INFO] - Training Epoch: 2/2, step 1521/7134 completed (loss: 0.38830074667930603, acc: 0.8823529481887817)
[2025-02-13 20:29:36,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:36,400][root][INFO] - Training Epoch: 2/2, step 1522/7134 completed (loss: 0.18733668327331543, acc: 0.9523809552192688)
[2025-02-13 20:29:36,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:36,766][root][INFO] - Training Epoch: 2/2, step 1523/7134 completed (loss: 0.3208089768886566, acc: 0.9144737124443054)
[2025-02-13 20:29:36,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:37,093][root][INFO] - Training Epoch: 2/2, step 1524/7134 completed (loss: 0.05994215980172157, acc: 0.9726027250289917)
[2025-02-13 20:29:37,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:37,459][root][INFO] - Training Epoch: 2/2, step 1525/7134 completed (loss: 0.1512427181005478, acc: 0.9700000286102295)
[2025-02-13 20:29:37,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:37,855][root][INFO] - Training Epoch: 2/2, step 1526/7134 completed (loss: 0.17273424565792084, acc: 0.9387755393981934)
[2025-02-13 20:29:38,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:38,261][root][INFO] - Training Epoch: 2/2, step 1527/7134 completed (loss: 0.29129353165626526, acc: 0.9292035102844238)
[2025-02-13 20:29:38,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:38,668][root][INFO] - Training Epoch: 2/2, step 1528/7134 completed (loss: 0.1566501259803772, acc: 0.9420289993286133)
[2025-02-13 20:29:38,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:39,048][root][INFO] - Training Epoch: 2/2, step 1529/7134 completed (loss: 0.08255542814731598, acc: 0.9734042286872864)
[2025-02-13 20:29:39,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:39,410][root][INFO] - Training Epoch: 2/2, step 1530/7134 completed (loss: 0.11722897738218307, acc: 0.9642857313156128)
[2025-02-13 20:29:39,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:39,766][root][INFO] - Training Epoch: 2/2, step 1531/7134 completed (loss: 0.3552708029747009, acc: 0.9166666865348816)
[2025-02-13 20:29:39,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:40,157][root][INFO] - Training Epoch: 2/2, step 1532/7134 completed (loss: 0.04362818971276283, acc: 0.9897435903549194)
[2025-02-13 20:29:40,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:40,522][root][INFO] - Training Epoch: 2/2, step 1533/7134 completed (loss: 0.17872576415538788, acc: 0.9518072009086609)
[2025-02-13 20:29:40,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:40,896][root][INFO] - Training Epoch: 2/2, step 1534/7134 completed (loss: 0.14220915734767914, acc: 0.971222996711731)
[2025-02-13 20:29:41,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:41,293][root][INFO] - Training Epoch: 2/2, step 1535/7134 completed (loss: 0.12988530099391937, acc: 0.9599999785423279)
[2025-02-13 20:29:41,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:41,666][root][INFO] - Training Epoch: 2/2, step 1536/7134 completed (loss: 0.07537822425365448, acc: 0.9814814925193787)
[2025-02-13 20:29:41,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:42,058][root][INFO] - Training Epoch: 2/2, step 1537/7134 completed (loss: 0.10670487582683563, acc: 0.9824561476707458)
[2025-02-13 20:29:42,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:42,451][root][INFO] - Training Epoch: 2/2, step 1538/7134 completed (loss: 0.25076040625572205, acc: 0.9583333134651184)
[2025-02-13 20:29:42,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:42,816][root][INFO] - Training Epoch: 2/2, step 1539/7134 completed (loss: 0.11795494705438614, acc: 0.9757575988769531)
[2025-02-13 20:29:42,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:43,185][root][INFO] - Training Epoch: 2/2, step 1540/7134 completed (loss: 0.12106595933437347, acc: 0.9683544039726257)
[2025-02-13 20:29:43,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:43,560][root][INFO] - Training Epoch: 2/2, step 1541/7134 completed (loss: 0.13405020534992218, acc: 0.9578947424888611)
[2025-02-13 20:29:43,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:43,937][root][INFO] - Training Epoch: 2/2, step 1542/7134 completed (loss: 0.07812181860208511, acc: 0.983146071434021)
[2025-02-13 20:29:44,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:44,313][root][INFO] - Training Epoch: 2/2, step 1543/7134 completed (loss: 0.10124827921390533, acc: 0.9666666388511658)
[2025-02-13 20:29:44,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:44,696][root][INFO] - Training Epoch: 2/2, step 1544/7134 completed (loss: 0.05804426968097687, acc: 0.9750000238418579)
[2025-02-13 20:29:44,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:45,090][root][INFO] - Training Epoch: 2/2, step 1545/7134 completed (loss: 0.11917007714509964, acc: 0.9523809552192688)
[2025-02-13 20:29:45,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:45,487][root][INFO] - Training Epoch: 2/2, step 1546/7134 completed (loss: 0.30815842747688293, acc: 0.9411764740943909)
[2025-02-13 20:29:45,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:45,878][root][INFO] - Training Epoch: 2/2, step 1547/7134 completed (loss: 0.16893643140792847, acc: 0.9444444179534912)
[2025-02-13 20:29:46,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:46,250][root][INFO] - Training Epoch: 2/2, step 1548/7134 completed (loss: 0.09875789284706116, acc: 0.9736841917037964)
[2025-02-13 20:29:46,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:46,631][root][INFO] - Training Epoch: 2/2, step 1549/7134 completed (loss: 0.02157234027981758, acc: 1.0)
[2025-02-13 20:29:46,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:47,016][root][INFO] - Training Epoch: 2/2, step 1550/7134 completed (loss: 0.05248354747891426, acc: 0.9882352948188782)
[2025-02-13 20:29:47,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:47,367][root][INFO] - Training Epoch: 2/2, step 1551/7134 completed (loss: 0.04139704629778862, acc: 0.9867549538612366)
[2025-02-13 20:29:47,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:47,775][root][INFO] - Training Epoch: 2/2, step 1552/7134 completed (loss: 0.04039063677191734, acc: 0.9948979616165161)
[2025-02-13 20:29:47,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:48,165][root][INFO] - Training Epoch: 2/2, step 1553/7134 completed (loss: 0.0685853436589241, acc: 0.9805194735527039)
[2025-02-13 20:29:48,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:48,561][root][INFO] - Training Epoch: 2/2, step 1554/7134 completed (loss: 0.09248726814985275, acc: 0.9710982441902161)
[2025-02-13 20:29:48,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:48,934][root][INFO] - Training Epoch: 2/2, step 1555/7134 completed (loss: 0.05095862224698067, acc: 0.987500011920929)
[2025-02-13 20:29:49,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:49,313][root][INFO] - Training Epoch: 2/2, step 1556/7134 completed (loss: 0.04897641763091087, acc: 0.9887005686759949)
[2025-02-13 20:29:49,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:49,690][root][INFO] - Training Epoch: 2/2, step 1557/7134 completed (loss: 0.053361717611551285, acc: 0.9888888597488403)
[2025-02-13 20:29:49,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:50,033][root][INFO] - Training Epoch: 2/2, step 1558/7134 completed (loss: 0.11916928738355637, acc: 0.965753436088562)
[2025-02-13 20:29:50,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:50,408][root][INFO] - Training Epoch: 2/2, step 1559/7134 completed (loss: 0.05777589976787567, acc: 0.977142870426178)
[2025-02-13 20:29:50,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:50,795][root][INFO] - Training Epoch: 2/2, step 1560/7134 completed (loss: 0.07001496106386185, acc: 0.9847715497016907)
[2025-02-13 20:29:50,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:51,186][root][INFO] - Training Epoch: 2/2, step 1561/7134 completed (loss: 0.16441667079925537, acc: 0.9525862336158752)
[2025-02-13 20:29:51,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:51,564][root][INFO] - Training Epoch: 2/2, step 1562/7134 completed (loss: 0.08817814290523529, acc: 0.9822485446929932)
[2025-02-13 20:29:51,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:51,927][root][INFO] - Training Epoch: 2/2, step 1563/7134 completed (loss: 0.09412997215986252, acc: 0.9656862616539001)
[2025-02-13 20:29:52,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:52,347][root][INFO] - Training Epoch: 2/2, step 1564/7134 completed (loss: 0.18098683655261993, acc: 0.9405940771102905)
[2025-02-13 20:29:52,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:52,746][root][INFO] - Training Epoch: 2/2, step 1565/7134 completed (loss: 0.0936143770813942, acc: 0.9874213933944702)
[2025-02-13 20:29:52,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:53,115][root][INFO] - Training Epoch: 2/2, step 1566/7134 completed (loss: 0.0914001539349556, acc: 0.988950252532959)
[2025-02-13 20:29:53,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:53,501][root][INFO] - Training Epoch: 2/2, step 1567/7134 completed (loss: 0.23978212475776672, acc: 0.9560975432395935)
[2025-02-13 20:29:53,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:53,903][root][INFO] - Training Epoch: 2/2, step 1568/7134 completed (loss: 0.1602088063955307, acc: 0.9471153616905212)
[2025-02-13 20:29:54,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:54,274][root][INFO] - Training Epoch: 2/2, step 1569/7134 completed (loss: 0.14637236297130585, acc: 0.9515151381492615)
[2025-02-13 20:29:54,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:54,666][root][INFO] - Training Epoch: 2/2, step 1570/7134 completed (loss: 0.36271482706069946, acc: 0.9483568072319031)
[2025-02-13 20:29:54,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:55,077][root][INFO] - Training Epoch: 2/2, step 1571/7134 completed (loss: 0.21939389407634735, acc: 0.9631336331367493)
[2025-02-13 20:29:55,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:55,462][root][INFO] - Training Epoch: 2/2, step 1572/7134 completed (loss: 0.26187342405319214, acc: 0.9624413251876831)
[2025-02-13 20:29:55,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:55,820][root][INFO] - Training Epoch: 2/2, step 1573/7134 completed (loss: 0.08826320618391037, acc: 0.9820359349250793)
[2025-02-13 20:29:55,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:56,189][root][INFO] - Training Epoch: 2/2, step 1574/7134 completed (loss: 0.1977323591709137, acc: 0.9551569223403931)
[2025-02-13 20:29:56,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:56,549][root][INFO] - Training Epoch: 2/2, step 1575/7134 completed (loss: 0.10711441189050674, acc: 0.9772727489471436)
[2025-02-13 20:29:56,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:56,942][root][INFO] - Training Epoch: 2/2, step 1576/7134 completed (loss: 0.11803964525461197, acc: 0.9765258431434631)
[2025-02-13 20:29:57,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:57,350][root][INFO] - Training Epoch: 2/2, step 1577/7134 completed (loss: 0.12359154969453812, acc: 0.966183602809906)
[2025-02-13 20:29:57,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:57,753][root][INFO] - Training Epoch: 2/2, step 1578/7134 completed (loss: 0.11511614918708801, acc: 0.9659090638160706)
[2025-02-13 20:29:57,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:58,160][root][INFO] - Training Epoch: 2/2, step 1579/7134 completed (loss: 0.04651212319731712, acc: 0.995192289352417)
[2025-02-13 20:29:58,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:58,545][root][INFO] - Training Epoch: 2/2, step 1580/7134 completed (loss: 0.1783173531293869, acc: 0.9420289993286133)
[2025-02-13 20:29:58,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:58,900][root][INFO] - Training Epoch: 2/2, step 1581/7134 completed (loss: 0.19632384181022644, acc: 0.9345238208770752)
[2025-02-13 20:29:59,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:59,267][root][INFO] - Training Epoch: 2/2, step 1582/7134 completed (loss: 0.1494670808315277, acc: 0.9666666388511658)
[2025-02-13 20:29:59,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:59,617][root][INFO] - Training Epoch: 2/2, step 1583/7134 completed (loss: 0.08507753908634186, acc: 0.9714285731315613)
[2025-02-13 20:29:59,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:00,024][root][INFO] - Training Epoch: 2/2, step 1584/7134 completed (loss: 0.10762955248355865, acc: 0.9729729890823364)
[2025-02-13 20:30:00,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:00,413][root][INFO] - Training Epoch: 2/2, step 1585/7134 completed (loss: 0.05792892351746559, acc: 0.9854369163513184)
[2025-02-13 20:30:00,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:00,795][root][INFO] - Training Epoch: 2/2, step 1586/7134 completed (loss: 0.13531950116157532, acc: 0.95652174949646)
[2025-02-13 20:30:00,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:01,146][root][INFO] - Training Epoch: 2/2, step 1587/7134 completed (loss: 0.05032862350344658, acc: 0.987261176109314)
[2025-02-13 20:30:01,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:01,517][root][INFO] - Training Epoch: 2/2, step 1588/7134 completed (loss: 0.0874922052025795, acc: 0.9808917045593262)
[2025-02-13 20:30:01,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:01,902][root][INFO] - Training Epoch: 2/2, step 1589/7134 completed (loss: 0.06539232283830643, acc: 0.9878048896789551)
[2025-02-13 20:30:02,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:02,298][root][INFO] - Training Epoch: 2/2, step 1590/7134 completed (loss: 0.07518003135919571, acc: 0.9857142567634583)
[2025-02-13 20:30:02,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:02,673][root][INFO] - Training Epoch: 2/2, step 1591/7134 completed (loss: 0.04607083275914192, acc: 0.9844961166381836)
[2025-02-13 20:30:02,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:03,066][root][INFO] - Training Epoch: 2/2, step 1592/7134 completed (loss: 0.11081906408071518, acc: 0.9740259647369385)
[2025-02-13 20:30:03,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:03,445][root][INFO] - Training Epoch: 2/2, step 1593/7134 completed (loss: 0.051571428775787354, acc: 0.9946523904800415)
[2025-02-13 20:30:03,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:03,827][root][INFO] - Training Epoch: 2/2, step 1594/7134 completed (loss: 0.06053537130355835, acc: 0.9765625)
[2025-02-13 20:30:03,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:04,203][root][INFO] - Training Epoch: 2/2, step 1595/7134 completed (loss: 0.019284849986433983, acc: 1.0)
[2025-02-13 20:30:04,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:04,580][root][INFO] - Training Epoch: 2/2, step 1596/7134 completed (loss: 0.15755830705165863, acc: 0.9861111044883728)
[2025-02-13 20:30:04,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:04,950][root][INFO] - Training Epoch: 2/2, step 1597/7134 completed (loss: 0.021423552185297012, acc: 1.0)
[2025-02-13 20:30:05,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:05,310][root][INFO] - Training Epoch: 2/2, step 1598/7134 completed (loss: 0.10252130776643753, acc: 0.9583333134651184)
[2025-02-13 20:30:05,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:05,698][root][INFO] - Training Epoch: 2/2, step 1599/7134 completed (loss: 0.11314129084348679, acc: 0.9793103337287903)
[2025-02-13 20:30:05,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:06,065][root][INFO] - Training Epoch: 2/2, step 1600/7134 completed (loss: 0.11725005507469177, acc: 0.9744898080825806)
[2025-02-13 20:30:06,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:06,432][root][INFO] - Training Epoch: 2/2, step 1601/7134 completed (loss: 0.11353439837694168, acc: 0.9818181991577148)
[2025-02-13 20:30:06,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:06,822][root][INFO] - Training Epoch: 2/2, step 1602/7134 completed (loss: 0.10888515412807465, acc: 0.9807692170143127)
[2025-02-13 20:30:06,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:07,209][root][INFO] - Training Epoch: 2/2, step 1603/7134 completed (loss: 0.09953484684228897, acc: 0.9879518151283264)
[2025-02-13 20:30:07,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:07,586][root][INFO] - Training Epoch: 2/2, step 1604/7134 completed (loss: 0.06484193354845047, acc: 0.9757575988769531)
[2025-02-13 20:30:07,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:07,962][root][INFO] - Training Epoch: 2/2, step 1605/7134 completed (loss: 0.07036146521568298, acc: 0.9823529124259949)
[2025-02-13 20:30:08,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:08,355][root][INFO] - Training Epoch: 2/2, step 1606/7134 completed (loss: 0.07564102858304977, acc: 0.9863945841789246)
[2025-02-13 20:30:08,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:08,708][root][INFO] - Training Epoch: 2/2, step 1607/7134 completed (loss: 0.017453210428357124, acc: 1.0)
[2025-02-13 20:30:08,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:09,071][root][INFO] - Training Epoch: 2/2, step 1608/7134 completed (loss: 0.026843257248401642, acc: 0.9921259880065918)
[2025-02-13 20:30:09,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:09,455][root][INFO] - Training Epoch: 2/2, step 1609/7134 completed (loss: 0.10650506615638733, acc: 0.9583333134651184)
[2025-02-13 20:30:09,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:09,816][root][INFO] - Training Epoch: 2/2, step 1610/7134 completed (loss: 0.057098206132650375, acc: 0.9923664331436157)
[2025-02-13 20:30:09,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:10,172][root][INFO] - Training Epoch: 2/2, step 1611/7134 completed (loss: 0.06551007926464081, acc: 0.9876543283462524)
[2025-02-13 20:30:10,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:10,566][root][INFO] - Training Epoch: 2/2, step 1612/7134 completed (loss: 0.12617504596710205, acc: 0.9638554453849792)
[2025-02-13 20:30:10,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:10,966][root][INFO] - Training Epoch: 2/2, step 1613/7134 completed (loss: 0.13550680875778198, acc: 0.9751552939414978)
[2025-02-13 20:30:11,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:11,386][root][INFO] - Training Epoch: 2/2, step 1614/7134 completed (loss: 0.09793303161859512, acc: 0.9671052694320679)
[2025-02-13 20:30:11,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:11,805][root][INFO] - Training Epoch: 2/2, step 1615/7134 completed (loss: 0.07242497056722641, acc: 0.9698795080184937)
[2025-02-13 20:30:11,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:12,177][root][INFO] - Training Epoch: 2/2, step 1616/7134 completed (loss: 0.08174069970846176, acc: 0.9935483932495117)
[2025-02-13 20:30:12,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:12,524][root][INFO] - Training Epoch: 2/2, step 1617/7134 completed (loss: 0.1053449884057045, acc: 0.9784172773361206)
[2025-02-13 20:30:12,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:12,891][root][INFO] - Training Epoch: 2/2, step 1618/7134 completed (loss: 0.27195242047309875, acc: 0.9465649127960205)
[2025-02-13 20:30:13,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:13,262][root][INFO] - Training Epoch: 2/2, step 1619/7134 completed (loss: 0.1231820285320282, acc: 0.9756097793579102)
[2025-02-13 20:30:13,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:13,663][root][INFO] - Training Epoch: 2/2, step 1620/7134 completed (loss: 0.15414729714393616, acc: 0.9545454382896423)
[2025-02-13 20:30:13,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:14,094][root][INFO] - Training Epoch: 2/2, step 1621/7134 completed (loss: 0.12007544934749603, acc: 0.9607843160629272)
[2025-02-13 20:30:14,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:14,466][root][INFO] - Training Epoch: 2/2, step 1622/7134 completed (loss: 0.07705622911453247, acc: 0.9803921580314636)
[2025-02-13 20:30:14,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:14,859][root][INFO] - Training Epoch: 2/2, step 1623/7134 completed (loss: 0.05889261141419411, acc: 0.9935897588729858)
[2025-02-13 20:30:15,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:15,238][root][INFO] - Training Epoch: 2/2, step 1624/7134 completed (loss: 0.1691141128540039, acc: 0.9675324559211731)
[2025-02-13 20:30:15,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:15,611][root][INFO] - Training Epoch: 2/2, step 1625/7134 completed (loss: 0.12016265094280243, acc: 0.956250011920929)
[2025-02-13 20:30:15,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:15,963][root][INFO] - Training Epoch: 2/2, step 1626/7134 completed (loss: 0.12756486237049103, acc: 0.969072163105011)
[2025-02-13 20:30:16,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:16,324][root][INFO] - Training Epoch: 2/2, step 1627/7134 completed (loss: 0.08170318603515625, acc: 0.9593495726585388)
[2025-02-13 20:30:16,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:16,704][root][INFO] - Training Epoch: 2/2, step 1628/7134 completed (loss: 0.050310760736465454, acc: 0.9814814925193787)
[2025-02-13 20:30:16,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:17,071][root][INFO] - Training Epoch: 2/2, step 1629/7134 completed (loss: 0.07490044087171555, acc: 0.9924242496490479)
[2025-02-13 20:30:17,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:17,477][root][INFO] - Training Epoch: 2/2, step 1630/7134 completed (loss: 0.044465746730566025, acc: 1.0)
[2025-02-13 20:30:17,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:17,887][root][INFO] - Training Epoch: 2/2, step 1631/7134 completed (loss: 0.15840116143226624, acc: 0.9640287756919861)
[2025-02-13 20:30:18,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:18,288][root][INFO] - Training Epoch: 2/2, step 1632/7134 completed (loss: 0.08812259137630463, acc: 0.9925373196601868)
[2025-02-13 20:30:18,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:18,694][root][INFO] - Training Epoch: 2/2, step 1633/7134 completed (loss: 0.061501454561948776, acc: 0.9785714149475098)
[2025-02-13 20:30:18,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:19,051][root][INFO] - Training Epoch: 2/2, step 1634/7134 completed (loss: 0.08729232102632523, acc: 0.9748427867889404)
[2025-02-13 20:30:19,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:19,401][root][INFO] - Training Epoch: 2/2, step 1635/7134 completed (loss: 0.11240953207015991, acc: 0.9722222089767456)
[2025-02-13 20:30:19,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:19,766][root][INFO] - Training Epoch: 2/2, step 1636/7134 completed (loss: 0.1134125217795372, acc: 0.963302731513977)
[2025-02-13 20:30:19,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:20,147][root][INFO] - Training Epoch: 2/2, step 1637/7134 completed (loss: 0.07768606394529343, acc: 0.9875776171684265)
[2025-02-13 20:30:20,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:20,522][root][INFO] - Training Epoch: 2/2, step 1638/7134 completed (loss: 0.20390696823596954, acc: 0.9212598204612732)
[2025-02-13 20:30:20,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:20,912][root][INFO] - Training Epoch: 2/2, step 1639/7134 completed (loss: 0.39009130001068115, acc: 0.8985507488250732)
[2025-02-13 20:30:21,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:21,321][root][INFO] - Training Epoch: 2/2, step 1640/7134 completed (loss: 0.1326839029788971, acc: 0.9492753744125366)
[2025-02-13 20:30:21,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:21,697][root][INFO] - Training Epoch: 2/2, step 1641/7134 completed (loss: 0.07802878320217133, acc: 0.9802631735801697)
[2025-02-13 20:30:21,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:22,073][root][INFO] - Training Epoch: 2/2, step 1642/7134 completed (loss: 0.11047384887933731, acc: 0.955974817276001)
[2025-02-13 20:30:22,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:22,431][root][INFO] - Training Epoch: 2/2, step 1643/7134 completed (loss: 0.22527670860290527, acc: 0.918181836605072)
[2025-02-13 20:30:22,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:22,810][root][INFO] - Training Epoch: 2/2, step 1644/7134 completed (loss: 0.14056234061717987, acc: 0.9622641801834106)
[2025-02-13 20:30:22,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:23,183][root][INFO] - Training Epoch: 2/2, step 1645/7134 completed (loss: 0.20085211098194122, acc: 0.9379310607910156)
[2025-02-13 20:30:23,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:23,568][root][INFO] - Training Epoch: 2/2, step 1646/7134 completed (loss: 0.13254041969776154, acc: 0.9741935729980469)
[2025-02-13 20:30:23,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:23,935][root][INFO] - Training Epoch: 2/2, step 1647/7134 completed (loss: 0.08663932979106903, acc: 0.9718309640884399)
[2025-02-13 20:30:24,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:24,306][root][INFO] - Training Epoch: 2/2, step 1648/7134 completed (loss: 0.11338817328214645, acc: 0.9701492786407471)
[2025-02-13 20:30:24,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:24,676][root][INFO] - Training Epoch: 2/2, step 1649/7134 completed (loss: 0.03847215697169304, acc: 0.991150438785553)
[2025-02-13 20:30:24,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:25,052][root][INFO] - Training Epoch: 2/2, step 1650/7134 completed (loss: 0.053302474319934845, acc: 0.9949748516082764)
[2025-02-13 20:30:25,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:25,430][root][INFO] - Training Epoch: 2/2, step 1651/7134 completed (loss: 0.12376397848129272, acc: 0.9636363387107849)
[2025-02-13 20:30:25,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:25,836][root][INFO] - Training Epoch: 2/2, step 1652/7134 completed (loss: 0.16171583533287048, acc: 0.9529914259910583)
[2025-02-13 20:30:25,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:26,210][root][INFO] - Training Epoch: 2/2, step 1653/7134 completed (loss: 0.07493841648101807, acc: 0.9781420826911926)
[2025-02-13 20:30:26,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:26,603][root][INFO] - Training Epoch: 2/2, step 1654/7134 completed (loss: 0.0944431722164154, acc: 0.9742489457130432)
[2025-02-13 20:30:26,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:26,983][root][INFO] - Training Epoch: 2/2, step 1655/7134 completed (loss: 0.061572443693876266, acc: 0.9897959232330322)
[2025-02-13 20:30:27,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:27,390][root][INFO] - Training Epoch: 2/2, step 1656/7134 completed (loss: 0.16745169460773468, acc: 0.9730941653251648)
[2025-02-13 20:30:27,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:27,775][root][INFO] - Training Epoch: 2/2, step 1657/7134 completed (loss: 0.11087857186794281, acc: 0.9689922332763672)
[2025-02-13 20:30:27,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:28,152][root][INFO] - Training Epoch: 2/2, step 1658/7134 completed (loss: 0.09123087674379349, acc: 0.9751037359237671)
[2025-02-13 20:30:28,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:28,526][root][INFO] - Training Epoch: 2/2, step 1659/7134 completed (loss: 0.08029531687498093, acc: 0.9800000190734863)
[2025-02-13 20:30:28,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:28,907][root][INFO] - Training Epoch: 2/2, step 1660/7134 completed (loss: 0.08860365301370621, acc: 0.9855769276618958)
[2025-02-13 20:30:29,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:29,286][root][INFO] - Training Epoch: 2/2, step 1661/7134 completed (loss: 0.08295512199401855, acc: 0.9740259647369385)
[2025-02-13 20:30:29,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:29,690][root][INFO] - Training Epoch: 2/2, step 1662/7134 completed (loss: 0.0479443185031414, acc: 0.9905213117599487)
[2025-02-13 20:30:29,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:30,071][root][INFO] - Training Epoch: 2/2, step 1663/7134 completed (loss: 0.05579139292240143, acc: 0.9872340559959412)
[2025-02-13 20:30:30,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:30,438][root][INFO] - Training Epoch: 2/2, step 1664/7134 completed (loss: 0.05377854034304619, acc: 0.9800000190734863)
[2025-02-13 20:30:30,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:30,828][root][INFO] - Training Epoch: 2/2, step 1665/7134 completed (loss: 0.0634964108467102, acc: 0.987730085849762)
[2025-02-13 20:30:30,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:31,235][root][INFO] - Training Epoch: 2/2, step 1666/7134 completed (loss: 0.03756099194288254, acc: 0.9915966391563416)
[2025-02-13 20:30:31,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:31,616][root][INFO] - Training Epoch: 2/2, step 1667/7134 completed (loss: 0.05211058631539345, acc: 0.9898989796638489)
[2025-02-13 20:30:31,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:32,019][root][INFO] - Training Epoch: 2/2, step 1668/7134 completed (loss: 0.058447666466236115, acc: 0.9820627570152283)
[2025-02-13 20:30:32,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:32,425][root][INFO] - Training Epoch: 2/2, step 1669/7134 completed (loss: 0.059462275356054306, acc: 0.9847328066825867)
[2025-02-13 20:30:32,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:32,815][root][INFO] - Training Epoch: 2/2, step 1670/7134 completed (loss: 0.11437103152275085, acc: 0.9733840227127075)
[2025-02-13 20:30:32,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:33,228][root][INFO] - Training Epoch: 2/2, step 1671/7134 completed (loss: 0.07834234833717346, acc: 0.9744898080825806)
[2025-02-13 20:30:33,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:33,603][root][INFO] - Training Epoch: 2/2, step 1672/7134 completed (loss: 0.08848346769809723, acc: 0.9649122953414917)
[2025-02-13 20:30:33,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:33,980][root][INFO] - Training Epoch: 2/2, step 1673/7134 completed (loss: 0.10864255577325821, acc: 0.9813664555549622)
[2025-02-13 20:30:34,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:34,335][root][INFO] - Training Epoch: 2/2, step 1674/7134 completed (loss: 0.09634125232696533, acc: 0.9802631735801697)
[2025-02-13 20:30:34,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:34,710][root][INFO] - Training Epoch: 2/2, step 1675/7134 completed (loss: 0.12154927849769592, acc: 0.965753436088562)
[2025-02-13 20:30:34,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:35,083][root][INFO] - Training Epoch: 2/2, step 1676/7134 completed (loss: 0.11005362123250961, acc: 0.9807692170143127)
[2025-02-13 20:30:35,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:35,439][root][INFO] - Training Epoch: 2/2, step 1677/7134 completed (loss: 0.2335422933101654, acc: 0.9503546357154846)
[2025-02-13 20:30:35,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:35,871][root][INFO] - Training Epoch: 2/2, step 1678/7134 completed (loss: 0.17245498299598694, acc: 0.970059871673584)
[2025-02-13 20:30:36,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:36,250][root][INFO] - Training Epoch: 2/2, step 1679/7134 completed (loss: 0.10182588547468185, acc: 0.9817073345184326)
[2025-02-13 20:30:36,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:36,634][root][INFO] - Training Epoch: 2/2, step 1680/7134 completed (loss: 0.08597222715616226, acc: 0.9823529124259949)
[2025-02-13 20:30:36,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:36,998][root][INFO] - Training Epoch: 2/2, step 1681/7134 completed (loss: 0.0880013108253479, acc: 0.9811320900917053)
[2025-02-13 20:30:37,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:37,366][root][INFO] - Training Epoch: 2/2, step 1682/7134 completed (loss: 0.13695019483566284, acc: 0.965753436088562)
[2025-02-13 20:30:37,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:37,741][root][INFO] - Training Epoch: 2/2, step 1683/7134 completed (loss: 0.07353787869215012, acc: 0.9793103337287903)
[2025-02-13 20:30:37,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:38,121][root][INFO] - Training Epoch: 2/2, step 1684/7134 completed (loss: 0.05041491240262985, acc: 0.9858155846595764)
[2025-02-13 20:30:38,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:38,496][root][INFO] - Training Epoch: 2/2, step 1685/7134 completed (loss: 0.11324264109134674, acc: 0.9575757384300232)
[2025-02-13 20:30:38,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:38,898][root][INFO] - Training Epoch: 2/2, step 1686/7134 completed (loss: 0.026159213855862617, acc: 0.9849624037742615)
[2025-02-13 20:30:39,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:39,283][root][INFO] - Training Epoch: 2/2, step 1687/7134 completed (loss: 0.032685671001672745, acc: 0.991150438785553)
[2025-02-13 20:30:39,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:39,634][root][INFO] - Training Epoch: 2/2, step 1688/7134 completed (loss: 0.051798731088638306, acc: 0.9863013625144958)
[2025-02-13 20:30:39,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:39,998][root][INFO] - Training Epoch: 2/2, step 1689/7134 completed (loss: 0.05621945858001709, acc: 0.9935483932495117)
[2025-02-13 20:30:40,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:40,364][root][INFO] - Training Epoch: 2/2, step 1690/7134 completed (loss: 0.06379597634077072, acc: 0.982300877571106)
[2025-02-13 20:30:40,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:40,737][root][INFO] - Training Epoch: 2/2, step 1691/7134 completed (loss: 0.03211129084229469, acc: 0.9929078221321106)
[2025-02-13 20:30:40,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:41,113][root][INFO] - Training Epoch: 2/2, step 1692/7134 completed (loss: 0.062015973031520844, acc: 0.9674796462059021)
[2025-02-13 20:30:41,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:41,516][root][INFO] - Training Epoch: 2/2, step 1693/7134 completed (loss: 0.03379041701555252, acc: 0.9931972622871399)
[2025-02-13 20:30:41,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:41,947][root][INFO] - Training Epoch: 2/2, step 1694/7134 completed (loss: 0.10732776671648026, acc: 0.9793103337287903)
[2025-02-13 20:30:42,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:42,335][root][INFO] - Training Epoch: 2/2, step 1695/7134 completed (loss: 0.09843698143959045, acc: 0.9663865566253662)
[2025-02-13 20:30:42,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:42,698][root][INFO] - Training Epoch: 2/2, step 1696/7134 completed (loss: 0.131830096244812, acc: 0.9624060392379761)
[2025-02-13 20:30:42,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:43,055][root][INFO] - Training Epoch: 2/2, step 1697/7134 completed (loss: 0.04002612084150314, acc: 1.0)
[2025-02-13 20:30:43,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:43,431][root][INFO] - Training Epoch: 2/2, step 1698/7134 completed (loss: 0.01683684252202511, acc: 1.0)
[2025-02-13 20:30:43,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:43,829][root][INFO] - Training Epoch: 2/2, step 1699/7134 completed (loss: 0.04383523389697075, acc: 0.9925373196601868)
[2025-02-13 20:30:43,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:44,211][root][INFO] - Training Epoch: 2/2, step 1700/7134 completed (loss: 0.0827488824725151, acc: 0.9701492786407471)
[2025-02-13 20:30:44,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:44,610][root][INFO] - Training Epoch: 2/2, step 1701/7134 completed (loss: 0.05814477428793907, acc: 1.0)
[2025-02-13 20:30:44,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:44,997][root][INFO] - Training Epoch: 2/2, step 1702/7134 completed (loss: 0.10191837698221207, acc: 0.970059871673584)
[2025-02-13 20:30:45,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:45,411][root][INFO] - Training Epoch: 2/2, step 1703/7134 completed (loss: 0.2041560709476471, acc: 0.9470198750495911)
[2025-02-13 20:30:45,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:45,804][root][INFO] - Training Epoch: 2/2, step 1704/7134 completed (loss: 0.22054311633110046, acc: 0.9451219439506531)
[2025-02-13 20:30:45,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:46,201][root][INFO] - Training Epoch: 2/2, step 1705/7134 completed (loss: 0.1472734808921814, acc: 0.9760000109672546)
[2025-02-13 20:30:46,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:46,575][root][INFO] - Training Epoch: 2/2, step 1706/7134 completed (loss: 0.07637554407119751, acc: 0.9714285731315613)
[2025-02-13 20:30:46,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:46,982][root][INFO] - Training Epoch: 2/2, step 1707/7134 completed (loss: 0.1760099232196808, acc: 0.9655172228813171)
[2025-02-13 20:30:47,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:47,359][root][INFO] - Training Epoch: 2/2, step 1708/7134 completed (loss: 0.055883195251226425, acc: 0.9925925731658936)
[2025-02-13 20:30:47,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:47,762][root][INFO] - Training Epoch: 2/2, step 1709/7134 completed (loss: 0.08704815804958344, acc: 0.9866666793823242)
[2025-02-13 20:30:47,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:48,158][root][INFO] - Training Epoch: 2/2, step 1710/7134 completed (loss: 0.12022225558757782, acc: 0.9864864945411682)
[2025-02-13 20:30:48,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:48,516][root][INFO] - Training Epoch: 2/2, step 1711/7134 completed (loss: 0.11062947660684586, acc: 0.9919354915618896)
[2025-02-13 20:30:48,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:48,889][root][INFO] - Training Epoch: 2/2, step 1712/7134 completed (loss: 0.04964108765125275, acc: 0.9920634627342224)
[2025-02-13 20:30:49,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:49,244][root][INFO] - Training Epoch: 2/2, step 1713/7134 completed (loss: 0.04447115212678909, acc: 1.0)
[2025-02-13 20:30:49,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:49,635][root][INFO] - Training Epoch: 2/2, step 1714/7134 completed (loss: 0.08628429472446442, acc: 0.9716312289237976)
[2025-02-13 20:30:49,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:50,005][root][INFO] - Training Epoch: 2/2, step 1715/7134 completed (loss: 0.04898032173514366, acc: 0.9905660152435303)
[2025-02-13 20:30:50,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:50,381][root][INFO] - Training Epoch: 2/2, step 1716/7134 completed (loss: 0.15021105110645294, acc: 0.9671052694320679)
[2025-02-13 20:30:50,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:50,785][root][INFO] - Training Epoch: 2/2, step 1717/7134 completed (loss: 0.08598224818706512, acc: 0.9922480583190918)
[2025-02-13 20:30:50,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:51,167][root][INFO] - Training Epoch: 2/2, step 1718/7134 completed (loss: 0.08047878742218018, acc: 0.9914529919624329)
[2025-02-13 20:30:51,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:51,582][root][INFO] - Training Epoch: 2/2, step 1719/7134 completed (loss: 0.1512233167886734, acc: 0.9704142212867737)
[2025-02-13 20:30:51,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:51,984][root][INFO] - Training Epoch: 2/2, step 1720/7134 completed (loss: 0.14314071834087372, acc: 0.9692307710647583)
[2025-02-13 20:30:52,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:52,339][root][INFO] - Training Epoch: 2/2, step 1721/7134 completed (loss: 0.07497603446245193, acc: 0.96875)
[2025-02-13 20:30:52,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:52,673][root][INFO] - Training Epoch: 2/2, step 1722/7134 completed (loss: 0.07669419050216675, acc: 0.9696969985961914)
[2025-02-13 20:30:52,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:53,033][root][INFO] - Training Epoch: 2/2, step 1723/7134 completed (loss: 0.18247562646865845, acc: 0.9567901492118835)
[2025-02-13 20:30:53,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:53,403][root][INFO] - Training Epoch: 2/2, step 1724/7134 completed (loss: 0.10746873915195465, acc: 0.984375)
[2025-02-13 20:30:53,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:53,736][root][INFO] - Training Epoch: 2/2, step 1725/7134 completed (loss: 0.037347570061683655, acc: 1.0)
[2025-02-13 20:30:53,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:54,096][root][INFO] - Training Epoch: 2/2, step 1726/7134 completed (loss: 0.04129481315612793, acc: 0.9784946441650391)
[2025-02-13 20:30:54,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:54,460][root][INFO] - Training Epoch: 2/2, step 1727/7134 completed (loss: 0.04704825207591057, acc: 0.991150438785553)
[2025-02-13 20:30:54,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:54,849][root][INFO] - Training Epoch: 2/2, step 1728/7134 completed (loss: 0.060801513493061066, acc: 0.9846153855323792)
[2025-02-13 20:30:54,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:55,244][root][INFO] - Training Epoch: 2/2, step 1729/7134 completed (loss: 0.08825979381799698, acc: 0.9819819927215576)
[2025-02-13 20:30:55,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:55,623][root][INFO] - Training Epoch: 2/2, step 1730/7134 completed (loss: 0.060437221080064774, acc: 0.9813084006309509)
[2025-02-13 20:30:55,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:56,002][root][INFO] - Training Epoch: 2/2, step 1731/7134 completed (loss: 0.22216638922691345, acc: 0.9518072009086609)
[2025-02-13 20:30:56,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:56,404][root][INFO] - Training Epoch: 2/2, step 1732/7134 completed (loss: 0.04123555123806, acc: 1.0)
[2025-02-13 20:30:56,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:56,778][root][INFO] - Training Epoch: 2/2, step 1733/7134 completed (loss: 0.13624435663223267, acc: 0.9541284441947937)
[2025-02-13 20:30:56,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:57,159][root][INFO] - Training Epoch: 2/2, step 1734/7134 completed (loss: 0.155544251203537, acc: 0.9518072009086609)
[2025-02-13 20:30:57,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:57,523][root][INFO] - Training Epoch: 2/2, step 1735/7134 completed (loss: 0.07772823423147202, acc: 0.9867549538612366)
[2025-02-13 20:30:57,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:57,903][root][INFO] - Training Epoch: 2/2, step 1736/7134 completed (loss: 0.09497205913066864, acc: 0.9810126423835754)
[2025-02-13 20:30:58,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:58,284][root][INFO] - Training Epoch: 2/2, step 1737/7134 completed (loss: 0.0752921774983406, acc: 0.9933333396911621)
[2025-02-13 20:30:58,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:58,655][root][INFO] - Training Epoch: 2/2, step 1738/7134 completed (loss: 0.022860711440443993, acc: 0.994413435459137)
[2025-02-13 20:30:58,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:59,031][root][INFO] - Training Epoch: 2/2, step 1739/7134 completed (loss: 0.09000111371278763, acc: 0.9624060392379761)
[2025-02-13 20:30:59,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:59,400][root][INFO] - Training Epoch: 2/2, step 1740/7134 completed (loss: 0.03772512450814247, acc: 0.9864864945411682)
[2025-02-13 20:30:59,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:59,771][root][INFO] - Training Epoch: 2/2, step 1741/7134 completed (loss: 0.07644634693861008, acc: 0.9751552939414978)
[2025-02-13 20:30:59,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:00,166][root][INFO] - Training Epoch: 2/2, step 1742/7134 completed (loss: 0.07018385827541351, acc: 0.9801324605941772)
[2025-02-13 20:31:00,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:00,537][root][INFO] - Training Epoch: 2/2, step 1743/7134 completed (loss: 0.08897239714860916, acc: 0.9863945841789246)
[2025-02-13 20:31:00,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:00,947][root][INFO] - Training Epoch: 2/2, step 1744/7134 completed (loss: 0.03275449946522713, acc: 0.9924812316894531)
[2025-02-13 20:31:01,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:01,324][root][INFO] - Training Epoch: 2/2, step 1745/7134 completed (loss: 0.07101590186357498, acc: 0.9764705896377563)
[2025-02-13 20:31:01,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:01,710][root][INFO] - Training Epoch: 2/2, step 1746/7134 completed (loss: 0.033178407698869705, acc: 0.9858155846595764)
[2025-02-13 20:31:01,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:02,103][root][INFO] - Training Epoch: 2/2, step 1747/7134 completed (loss: 0.11594164371490479, acc: 0.9634146094322205)
[2025-02-13 20:31:02,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:02,494][root][INFO] - Training Epoch: 2/2, step 1748/7134 completed (loss: 0.07993406802415848, acc: 0.9882352948188782)
[2025-02-13 20:31:02,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:02,896][root][INFO] - Training Epoch: 2/2, step 1749/7134 completed (loss: 0.12321748584508896, acc: 0.9685863852500916)
[2025-02-13 20:31:03,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:03,286][root][INFO] - Training Epoch: 2/2, step 1750/7134 completed (loss: 0.17344962060451508, acc: 0.9704142212867737)
[2025-02-13 20:31:03,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:03,656][root][INFO] - Training Epoch: 2/2, step 1751/7134 completed (loss: 0.18116477131843567, acc: 0.9702380895614624)
[2025-02-13 20:31:03,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:04,033][root][INFO] - Training Epoch: 2/2, step 1752/7134 completed (loss: 0.2220337837934494, acc: 0.9696969985961914)
[2025-02-13 20:31:04,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:04,390][root][INFO] - Training Epoch: 2/2, step 1753/7134 completed (loss: 0.16535431146621704, acc: 0.9735099077224731)
[2025-02-13 20:31:04,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:04,759][root][INFO] - Training Epoch: 2/2, step 1754/7134 completed (loss: 0.12126903980970383, acc: 0.9803921580314636)
[2025-02-13 20:31:04,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:05,126][root][INFO] - Training Epoch: 2/2, step 1755/7134 completed (loss: 0.08226751536130905, acc: 0.9738562107086182)
[2025-02-13 20:31:05,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:05,493][root][INFO] - Training Epoch: 2/2, step 1756/7134 completed (loss: 0.1469520628452301, acc: 0.9666666388511658)
[2025-02-13 20:31:05,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:05,848][root][INFO] - Training Epoch: 2/2, step 1757/7134 completed (loss: 0.1251254379749298, acc: 0.9752066135406494)
[2025-02-13 20:31:05,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:06,227][root][INFO] - Training Epoch: 2/2, step 1758/7134 completed (loss: 0.05271497741341591, acc: 0.9882352948188782)
[2025-02-13 20:31:06,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:06,604][root][INFO] - Training Epoch: 2/2, step 1759/7134 completed (loss: 0.047779228538274765, acc: 1.0)
[2025-02-13 20:31:06,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:06,955][root][INFO] - Training Epoch: 2/2, step 1760/7134 completed (loss: 0.03942961245775223, acc: 1.0)
[2025-02-13 20:31:07,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:07,320][root][INFO] - Training Epoch: 2/2, step 1761/7134 completed (loss: 0.03889339789748192, acc: 1.0)
[2025-02-13 20:31:07,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:07,703][root][INFO] - Training Epoch: 2/2, step 1762/7134 completed (loss: 0.07083196192979813, acc: 0.9902912378311157)
[2025-02-13 20:31:07,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:08,070][root][INFO] - Training Epoch: 2/2, step 1763/7134 completed (loss: 0.113422691822052, acc: 0.9726027250289917)
[2025-02-13 20:31:08,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:08,431][root][INFO] - Training Epoch: 2/2, step 1764/7134 completed (loss: 0.1479131430387497, acc: 0.975806474685669)
[2025-02-13 20:31:08,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:08,810][root][INFO] - Training Epoch: 2/2, step 1765/7134 completed (loss: 0.106679767370224, acc: 0.9673202633857727)
[2025-02-13 20:31:08,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:09,176][root][INFO] - Training Epoch: 2/2, step 1766/7134 completed (loss: 0.10467637330293655, acc: 0.9740259647369385)
[2025-02-13 20:31:09,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:09,566][root][INFO] - Training Epoch: 2/2, step 1767/7134 completed (loss: 0.2540464699268341, acc: 0.9435028433799744)
[2025-02-13 20:31:09,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:09,908][root][INFO] - Training Epoch: 2/2, step 1768/7134 completed (loss: 0.026601780205965042, acc: 0.9910714030265808)
[2025-02-13 20:31:10,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:10,286][root][INFO] - Training Epoch: 2/2, step 1769/7134 completed (loss: 0.12467709928750992, acc: 0.9666666388511658)
[2025-02-13 20:31:10,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:10,643][root][INFO] - Training Epoch: 2/2, step 1770/7134 completed (loss: 0.08838587254285812, acc: 0.9794520735740662)
[2025-02-13 20:31:10,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:11,015][root][INFO] - Training Epoch: 2/2, step 1771/7134 completed (loss: 0.10736642777919769, acc: 0.9740259647369385)
[2025-02-13 20:31:11,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:11,370][root][INFO] - Training Epoch: 2/2, step 1772/7134 completed (loss: 0.12350154668092728, acc: 0.9785714149475098)
[2025-02-13 20:31:11,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:11,740][root][INFO] - Training Epoch: 2/2, step 1773/7134 completed (loss: 0.05709484964609146, acc: 0.9858155846595764)
[2025-02-13 20:31:11,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:12,102][root][INFO] - Training Epoch: 2/2, step 1774/7134 completed (loss: 0.02110971510410309, acc: 1.0)
[2025-02-13 20:31:12,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:12,454][root][INFO] - Training Epoch: 2/2, step 1775/7134 completed (loss: 0.026882939040660858, acc: 0.9931507110595703)
[2025-02-13 20:31:12,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:12,816][root][INFO] - Training Epoch: 2/2, step 1776/7134 completed (loss: 0.04188547283411026, acc: 1.0)
[2025-02-13 20:31:12,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:13,175][root][INFO] - Training Epoch: 2/2, step 1777/7134 completed (loss: 0.023736368864774704, acc: 0.993630588054657)
[2025-02-13 20:31:13,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:13,561][root][INFO] - Training Epoch: 2/2, step 1778/7134 completed (loss: 0.14313168823719025, acc: 0.955974817276001)
[2025-02-13 20:31:13,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:13,928][root][INFO] - Training Epoch: 2/2, step 1779/7134 completed (loss: 0.06163373962044716, acc: 0.982300877571106)
[2025-02-13 20:31:14,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:14,340][root][INFO] - Training Epoch: 2/2, step 1780/7134 completed (loss: 0.03948263078927994, acc: 0.9940119981765747)
[2025-02-13 20:31:15,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:15,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:16,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:16,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:16,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:18,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:18,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:18,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:19,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:19,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:19,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:20,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:20,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:20,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:21,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:21,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:21,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:22,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:22,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:22,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:23,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:23,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:23,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:24,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:24,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:24,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:25,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:25,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:25,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:26,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:26,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:26,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:27,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:27,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:27,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:28,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:28,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:28,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:29,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:29,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:29,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:30,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:30,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:30,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:31,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:31,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:31,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:32,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:32,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:32,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:33,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:33,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:33,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:34,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:34,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:34,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:35,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:35,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:35,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:36,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:36,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:36,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:37,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:37,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:37,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:38,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:38,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:38,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:38,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:39,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:39,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:39,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:40,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:40,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:40,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:41,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:41,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:41,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:42,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:42,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:42,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:43,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:43,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:43,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:44,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:44,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:44,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:45,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:45,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:45,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:46,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:46,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:46,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:47,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:47,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:47,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:48,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:48,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:48,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:49,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:49,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:49,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:50,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:50,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:50,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:50,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:51,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:51,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:52,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:52,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:52,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:52,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:53,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:53,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:53,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:54,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:54,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:54,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:54,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:55,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:55,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:55,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:56,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:56,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:57,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:57,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:57,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:58,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:58,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:58,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:59,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:59,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:59,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:00,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:00,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:00,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:01,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:01,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:01,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:02,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:02,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:02,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:03,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:03,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:03,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:04,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:04,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:04,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:05,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:05,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:05,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:06,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:06,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:06,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:07,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:07,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:07,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:07,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:08,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:08,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:09,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:09,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:09,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:10,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:10,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:10,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:11,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:11,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:11,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:12,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:12,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:12,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:13,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:13,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:13,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:14,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:14,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:14,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:14,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:15,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:15,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:15,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:16,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:16,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:16,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:17,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:17,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:17,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:18,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:18,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:18,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:19,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:19,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:19,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:21,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:21,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:21,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:22,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:22,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:22,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:23,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:23,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:23,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:24,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:24,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:24,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:25,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:25,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:25,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:26,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:26,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:26,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:27,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:27,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:27,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:28,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:28,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:28,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:28,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:29,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:29,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:29,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:30,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:30,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:30,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:31,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:31,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:31,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:32,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:32,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:32,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:32,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:33,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:33,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:33,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:34,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:34,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:35,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:35,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:36,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:36,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:36,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:37,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:37,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:37,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:38,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:38,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:38,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:39,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:39,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:40,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:40,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:40,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:40,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:41,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:41,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:41,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:42,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:42,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:42,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:43,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:43,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:43,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:44,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:44,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:44,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:45,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:45,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:45,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:46,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:46,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:47,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:47,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:47,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:48,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:48,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:48,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:49,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:49,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:50,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:50,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:50,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:51,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:51,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:51,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:52,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:52,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:53,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:53,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:53,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:54,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:54,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:54,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:56,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:56,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:56,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:57,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:57,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:57,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:58,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:58,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:58,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:59,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:59,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:00,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:00,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:00,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:01,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:01,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:02,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:02,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:02,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:03,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:03,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:03,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:04,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:04,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:05,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:05,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:05,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:07,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:07,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:07,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:08,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:08,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:08,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:09,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:09,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:09,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:10,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:10,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:11,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:11,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:11,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:12,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:12,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:12,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:12,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:13,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:13,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:13,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:14,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:14,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:15,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:15,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:15,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:16,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:17,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:17,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:17,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:18,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:18,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:18,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:19,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:19,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:19,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:20,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:20,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:20,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:21,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:21,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:22,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:22,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:22,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:22,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:23,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:23,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:23,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:24,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:24,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:24,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:24,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:25,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:25,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:25,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:26,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:26,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:26,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:27,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:27,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:28,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:28,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:28,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:29,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:29,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:29,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:29,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:30,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:30,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:32,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:32,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:32,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:32,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:33,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:33,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:34,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:34,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:34,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:34,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:35,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:35,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:35,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:36,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:36,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:36,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:37,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:37,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:37,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:38,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:38,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:38,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:38,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:39,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:39,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:39,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:40,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:40,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:40,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:41,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:41,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:42,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:42,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:42,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:42,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:43,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:43,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:43,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:44,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:44,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:44,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:45,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:45,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:45,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:46,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:46,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:46,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:47,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:47,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:47,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:48,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:48,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:48,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:49,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:49,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:49,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:50,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:50,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:50,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:51,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:51,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:51,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:52,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:52,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:52,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:53,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:53,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:53,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:54,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:54,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:54,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:55,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:55,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:55,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:56,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:56,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:56,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:57,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:57,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:57,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:58,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:58,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:59,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:59,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:59,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:00,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:00,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:00,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:01,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:01,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:01,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:02,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:02,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:03,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:03,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:03,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:03,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:04,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:04,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:05,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:05,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:05,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:06,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:06,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:06,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:07,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:07,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:07,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:07,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:08,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:08,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:08,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:09,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:09,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:09,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:10,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:10,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:10,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:10,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:11,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:11,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:13,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:13,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:14,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:14,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:14,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:14,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:15,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:15,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:15,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:16,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:16,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:16,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:17,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:17,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:17,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:18,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:18,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:18,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:19,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:19,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:20,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:20,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:20,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:20,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:21,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:21,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:21,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:22,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:22,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:22,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:23,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:23,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:24,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:24,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:24,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:25,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:25,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:25,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:26,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:26,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:26,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:27,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:27,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:27,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:28,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:28,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:29,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:29,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:29,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:30,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:30,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:30,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:31,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:31,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:32,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:32,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:32,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:32,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:33,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:33,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:33,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:34,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:34,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:34,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:35,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:35,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:35,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:36,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:36,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:36,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:37,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:37,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:37,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:38,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:38,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:38,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:39,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:39,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:39,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:40,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:40,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:40,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:41,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:41,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:41,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:42,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:42,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:42,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:43,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:43,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:43,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:44,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:45,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:45,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:46,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:46,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:46,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:47,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:47,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:48,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:48,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:48,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:49,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:49,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:49,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:50,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:50,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:50,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:51,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:51,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:51,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:52,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:52,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:52,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:54,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:54,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:54,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:55,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:55,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:55,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:56,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:56,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:56,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:57,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:57,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:57,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:57,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:58,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:58,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:58,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:59,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:59,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:59,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:00,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:00,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:01,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:01,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:02,142][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2413, device='cuda:0') eval_epoch_loss=tensor(0.2162, device='cuda:0') eval_epoch_acc=tensor(0.9491, device='cuda:0')
[2025-02-13 20:35:02,144][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:35:02,144][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:35:02,475][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_1781_loss_0.21616549789905548/model.pt
[2025-02-13 20:35:02,480][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:35:02,481][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.21616549789905548
[2025-02-13 20:35:02,482][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9491442441940308
[2025-02-13 20:35:02,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:02,888][root][INFO] - Training Epoch: 2/2, step 1781/7134 completed (loss: 0.07574019581079483, acc: 0.9731543660163879)
[2025-02-13 20:35:03,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:03,241][root][INFO] - Training Epoch: 2/2, step 1782/7134 completed (loss: 0.022250577807426453, acc: 0.9935897588729858)
[2025-02-13 20:35:03,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:03,612][root][INFO] - Training Epoch: 2/2, step 1783/7134 completed (loss: 0.09535353630781174, acc: 0.9768785834312439)
[2025-02-13 20:35:03,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:03,978][root][INFO] - Training Epoch: 2/2, step 1784/7134 completed (loss: 0.07002651691436768, acc: 0.9935897588729858)
[2025-02-13 20:35:04,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:04,327][root][INFO] - Training Epoch: 2/2, step 1785/7134 completed (loss: 0.12143029272556305, acc: 0.9638554453849792)
[2025-02-13 20:35:04,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:04,694][root][INFO] - Training Epoch: 2/2, step 1786/7134 completed (loss: 0.028621722012758255, acc: 1.0)
[2025-02-13 20:35:04,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:05,066][root][INFO] - Training Epoch: 2/2, step 1787/7134 completed (loss: 0.11848493665456772, acc: 0.9477611780166626)
[2025-02-13 20:35:05,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:05,413][root][INFO] - Training Epoch: 2/2, step 1788/7134 completed (loss: 0.11206266283988953, acc: 0.9793103337287903)
[2025-02-13 20:35:05,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:05,779][root][INFO] - Training Epoch: 2/2, step 1789/7134 completed (loss: 0.08173343539237976, acc: 0.9802631735801697)
[2025-02-13 20:35:05,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:06,151][root][INFO] - Training Epoch: 2/2, step 1790/7134 completed (loss: 0.39166903495788574, acc: 0.9161290526390076)
[2025-02-13 20:35:06,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:06,521][root][INFO] - Training Epoch: 2/2, step 1791/7134 completed (loss: 0.34279462695121765, acc: 0.9386503100395203)
[2025-02-13 20:35:06,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:06,893][root][INFO] - Training Epoch: 2/2, step 1792/7134 completed (loss: 0.6441323757171631, acc: 0.910614550113678)
[2025-02-13 20:35:07,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:07,260][root][INFO] - Training Epoch: 2/2, step 1793/7134 completed (loss: 0.3056476414203644, acc: 0.9585798978805542)
[2025-02-13 20:35:07,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:07,639][root][INFO] - Training Epoch: 2/2, step 1794/7134 completed (loss: 0.26083487272262573, acc: 0.9276315569877625)
[2025-02-13 20:35:07,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:08,005][root][INFO] - Training Epoch: 2/2, step 1795/7134 completed (loss: 0.41824230551719666, acc: 0.887499988079071)
[2025-02-13 20:35:08,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:08,368][root][INFO] - Training Epoch: 2/2, step 1796/7134 completed (loss: 0.3172526955604553, acc: 0.909604549407959)
[2025-02-13 20:35:08,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:08,739][root][INFO] - Training Epoch: 2/2, step 1797/7134 completed (loss: 0.2647654116153717, acc: 0.9257143139839172)
[2025-02-13 20:35:08,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:09,109][root][INFO] - Training Epoch: 2/2, step 1798/7134 completed (loss: 0.3105056583881378, acc: 0.9069767594337463)
[2025-02-13 20:35:09,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:09,482][root][INFO] - Training Epoch: 2/2, step 1799/7134 completed (loss: 0.3023350238800049, acc: 0.9292929172515869)
[2025-02-13 20:35:09,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:09,830][root][INFO] - Training Epoch: 2/2, step 1800/7134 completed (loss: 0.18818169832229614, acc: 0.9476190209388733)
[2025-02-13 20:35:09,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:10,197][root][INFO] - Training Epoch: 2/2, step 1801/7134 completed (loss: 0.21303848922252655, acc: 0.9369369149208069)
[2025-02-13 20:35:10,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:10,537][root][INFO] - Training Epoch: 2/2, step 1802/7134 completed (loss: 0.17343033850193024, acc: 0.9521276354789734)
[2025-02-13 20:35:10,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:10,902][root][INFO] - Training Epoch: 2/2, step 1803/7134 completed (loss: 0.21578004956245422, acc: 0.9451219439506531)
[2025-02-13 20:35:11,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:11,262][root][INFO] - Training Epoch: 2/2, step 1804/7134 completed (loss: 0.0920172929763794, acc: 0.9788359999656677)
[2025-02-13 20:35:11,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:11,616][root][INFO] - Training Epoch: 2/2, step 1805/7134 completed (loss: 0.16100408136844635, acc: 0.9532163739204407)
[2025-02-13 20:35:11,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:11,962][root][INFO] - Training Epoch: 2/2, step 1806/7134 completed (loss: 0.15845143795013428, acc: 0.9562841653823853)
[2025-02-13 20:35:12,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:12,317][root][INFO] - Training Epoch: 2/2, step 1807/7134 completed (loss: 0.1780662089586258, acc: 0.9457831382751465)
[2025-02-13 20:35:12,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:12,666][root][INFO] - Training Epoch: 2/2, step 1808/7134 completed (loss: 0.08979115635156631, acc: 0.9774011373519897)
[2025-02-13 20:35:12,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:13,038][root][INFO] - Training Epoch: 2/2, step 1809/7134 completed (loss: 0.07883104681968689, acc: 0.9826589822769165)
[2025-02-13 20:35:13,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:13,405][root][INFO] - Training Epoch: 2/2, step 1810/7134 completed (loss: 0.08028978109359741, acc: 0.9890109896659851)
[2025-02-13 20:35:13,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:13,770][root][INFO] - Training Epoch: 2/2, step 1811/7134 completed (loss: 0.13497115671634674, acc: 0.9680851101875305)
[2025-02-13 20:35:13,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:14,131][root][INFO] - Training Epoch: 2/2, step 1812/7134 completed (loss: 0.1018727645277977, acc: 0.9717513918876648)
[2025-02-13 20:35:14,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:14,503][root][INFO] - Training Epoch: 2/2, step 1813/7134 completed (loss: 0.1273394227027893, acc: 0.9779005646705627)
[2025-02-13 20:35:14,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:14,864][root][INFO] - Training Epoch: 2/2, step 1814/7134 completed (loss: 0.09807019680738449, acc: 0.9779005646705627)
[2025-02-13 20:35:14,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:15,231][root][INFO] - Training Epoch: 2/2, step 1815/7134 completed (loss: 0.12204147130250931, acc: 0.9745222926139832)
[2025-02-13 20:35:15,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:15,592][root][INFO] - Training Epoch: 2/2, step 1816/7134 completed (loss: 0.2176761031150818, acc: 0.9236111044883728)
[2025-02-13 20:35:15,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:15,958][root][INFO] - Training Epoch: 2/2, step 1817/7134 completed (loss: 0.09256912022829056, acc: 0.96875)
[2025-02-13 20:35:16,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:16,333][root][INFO] - Training Epoch: 2/2, step 1818/7134 completed (loss: 0.11022277176380157, acc: 0.9790576100349426)
[2025-02-13 20:35:16,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:16,697][root][INFO] - Training Epoch: 2/2, step 1819/7134 completed (loss: 0.06614375114440918, acc: 0.9767441749572754)
[2025-02-13 20:35:16,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:17,142][root][INFO] - Training Epoch: 2/2, step 1820/7134 completed (loss: 0.07510208338499069, acc: 0.9764150977134705)
[2025-02-13 20:35:17,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:17,501][root][INFO] - Training Epoch: 2/2, step 1821/7134 completed (loss: 0.09132738411426544, acc: 0.9813084006309509)
[2025-02-13 20:35:17,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:17,847][root][INFO] - Training Epoch: 2/2, step 1822/7134 completed (loss: 0.06129544600844383, acc: 0.9868420958518982)
[2025-02-13 20:35:17,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:18,220][root][INFO] - Training Epoch: 2/2, step 1823/7134 completed (loss: 0.10454093664884567, acc: 0.9651162624359131)
[2025-02-13 20:35:18,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:18,587][root][INFO] - Training Epoch: 2/2, step 1824/7134 completed (loss: 0.07715509086847305, acc: 0.9781420826911926)
[2025-02-13 20:35:18,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:18,956][root][INFO] - Training Epoch: 2/2, step 1825/7134 completed (loss: 0.07750013470649719, acc: 0.9766355156898499)
[2025-02-13 20:35:19,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:19,315][root][INFO] - Training Epoch: 2/2, step 1826/7134 completed (loss: 0.0545983724296093, acc: 0.988304078578949)
[2025-02-13 20:35:19,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:19,680][root][INFO] - Training Epoch: 2/2, step 1827/7134 completed (loss: 0.10254906862974167, acc: 0.9810426831245422)
[2025-02-13 20:35:19,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:20,035][root][INFO] - Training Epoch: 2/2, step 1828/7134 completed (loss: 0.062178775668144226, acc: 0.9833333492279053)
[2025-02-13 20:35:20,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:20,406][root][INFO] - Training Epoch: 2/2, step 1829/7134 completed (loss: 0.06296277046203613, acc: 0.9838709831237793)
[2025-02-13 20:35:20,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:20,776][root][INFO] - Training Epoch: 2/2, step 1830/7134 completed (loss: 0.07637689262628555, acc: 0.984455943107605)
[2025-02-13 20:35:20,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:21,123][root][INFO] - Training Epoch: 2/2, step 1831/7134 completed (loss: 0.03387909382581711, acc: 1.0)
[2025-02-13 20:35:21,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:21,468][root][INFO] - Training Epoch: 2/2, step 1832/7134 completed (loss: 0.18036748468875885, acc: 0.9534883499145508)
[2025-02-13 20:35:21,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:21,859][root][INFO] - Training Epoch: 2/2, step 1833/7134 completed (loss: 0.4288063645362854, acc: 0.9024389982223511)
[2025-02-13 20:35:22,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:22,238][root][INFO] - Training Epoch: 2/2, step 1834/7134 completed (loss: 0.04205871373414993, acc: 0.9871794581413269)
[2025-02-13 20:35:22,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:22,599][root][INFO] - Training Epoch: 2/2, step 1835/7134 completed (loss: 0.168730691075325, acc: 0.9610389471054077)
[2025-02-13 20:35:22,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:22,961][root][INFO] - Training Epoch: 2/2, step 1836/7134 completed (loss: 0.11219076812267303, acc: 0.9679144620895386)
[2025-02-13 20:35:23,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:23,328][root][INFO] - Training Epoch: 2/2, step 1837/7134 completed (loss: 0.12153021991252899, acc: 0.9780219793319702)
[2025-02-13 20:35:23,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:23,649][root][INFO] - Training Epoch: 2/2, step 1838/7134 completed (loss: 0.07318843901157379, acc: 0.9724770784378052)
[2025-02-13 20:35:23,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:24,016][root][INFO] - Training Epoch: 2/2, step 1839/7134 completed (loss: 0.07463668286800385, acc: 0.9890109896659851)
[2025-02-13 20:35:24,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:24,388][root][INFO] - Training Epoch: 2/2, step 1840/7134 completed (loss: 0.1460610330104828, acc: 0.9620253443717957)
[2025-02-13 20:35:24,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:24,754][root][INFO] - Training Epoch: 2/2, step 1841/7134 completed (loss: 0.06993923336267471, acc: 0.9818181991577148)
[2025-02-13 20:35:24,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:25,117][root][INFO] - Training Epoch: 2/2, step 1842/7134 completed (loss: 0.14139443635940552, acc: 0.9634146094322205)
[2025-02-13 20:35:25,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:25,472][root][INFO] - Training Epoch: 2/2, step 1843/7134 completed (loss: 0.17071644961833954, acc: 0.9702380895614624)
[2025-02-13 20:35:25,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:25,840][root][INFO] - Training Epoch: 2/2, step 1844/7134 completed (loss: 0.06406334042549133, acc: 0.978723406791687)
[2025-02-13 20:35:25,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:26,212][root][INFO] - Training Epoch: 2/2, step 1845/7134 completed (loss: 0.049414120614528656, acc: 0.989130437374115)
[2025-02-13 20:35:26,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:26,579][root][INFO] - Training Epoch: 2/2, step 1846/7134 completed (loss: 0.1313873529434204, acc: 0.9743589758872986)
[2025-02-13 20:35:26,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:26,940][root][INFO] - Training Epoch: 2/2, step 1847/7134 completed (loss: 0.1335427314043045, acc: 0.9545454382896423)
[2025-02-13 20:35:27,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:27,310][root][INFO] - Training Epoch: 2/2, step 1848/7134 completed (loss: 0.2217937409877777, acc: 0.9414893388748169)
[2025-02-13 20:35:27,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:27,666][root][INFO] - Training Epoch: 2/2, step 1849/7134 completed (loss: 0.09136262536048889, acc: 0.9647887349128723)
[2025-02-13 20:35:27,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:28,028][root][INFO] - Training Epoch: 2/2, step 1850/7134 completed (loss: 0.20976904034614563, acc: 0.9432989954948425)
[2025-02-13 20:35:28,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:28,391][root][INFO] - Training Epoch: 2/2, step 1851/7134 completed (loss: 0.07392095774412155, acc: 0.9846153855323792)
[2025-02-13 20:35:28,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:28,741][root][INFO] - Training Epoch: 2/2, step 1852/7134 completed (loss: 0.17831771075725555, acc: 0.925000011920929)
[2025-02-13 20:35:28,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:29,106][root][INFO] - Training Epoch: 2/2, step 1853/7134 completed (loss: 0.16307571530342102, acc: 0.9547738432884216)
[2025-02-13 20:35:29,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:29,466][root][INFO] - Training Epoch: 2/2, step 1854/7134 completed (loss: 0.15003487467765808, acc: 0.9638009071350098)
[2025-02-13 20:35:29,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:29,831][root][INFO] - Training Epoch: 2/2, step 1855/7134 completed (loss: 0.10327684134244919, acc: 0.9810426831245422)
[2025-02-13 20:35:29,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:30,205][root][INFO] - Training Epoch: 2/2, step 1856/7134 completed (loss: 0.1294029951095581, acc: 0.9591836929321289)
[2025-02-13 20:35:30,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:30,585][root][INFO] - Training Epoch: 2/2, step 1857/7134 completed (loss: 0.29497942328453064, acc: 0.9264705777168274)
[2025-02-13 20:35:30,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:30,948][root][INFO] - Training Epoch: 2/2, step 1858/7134 completed (loss: 0.12535898387432098, acc: 0.9642857313156128)
[2025-02-13 20:35:31,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:31,320][root][INFO] - Training Epoch: 2/2, step 1859/7134 completed (loss: 0.07988261431455612, acc: 0.980861246585846)
[2025-02-13 20:35:31,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:31,685][root][INFO] - Training Epoch: 2/2, step 1860/7134 completed (loss: 0.12338920682668686, acc: 0.9726775884628296)
[2025-02-13 20:35:31,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:32,053][root][INFO] - Training Epoch: 2/2, step 1861/7134 completed (loss: 0.13311795890331268, acc: 0.9790576100349426)
[2025-02-13 20:35:32,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:32,409][root][INFO] - Training Epoch: 2/2, step 1862/7134 completed (loss: 0.04994070902466774, acc: 0.9841269850730896)
[2025-02-13 20:35:32,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:32,769][root][INFO] - Training Epoch: 2/2, step 1863/7134 completed (loss: 0.10013818740844727, acc: 0.9753694534301758)
[2025-02-13 20:35:32,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:33,134][root][INFO] - Training Epoch: 2/2, step 1864/7134 completed (loss: 0.33876514434814453, acc: 0.903553307056427)
[2025-02-13 20:35:33,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:33,481][root][INFO] - Training Epoch: 2/2, step 1865/7134 completed (loss: 0.15999779105186462, acc: 0.9731543660163879)
[2025-02-13 20:35:33,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:33,835][root][INFO] - Training Epoch: 2/2, step 1866/7134 completed (loss: 0.05066157504916191, acc: 0.9882352948188782)
[2025-02-13 20:35:33,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:34,242][root][INFO] - Training Epoch: 2/2, step 1867/7134 completed (loss: 0.052462127059698105, acc: 0.9886363744735718)
[2025-02-13 20:35:34,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:34,632][root][INFO] - Training Epoch: 2/2, step 1868/7134 completed (loss: 0.24182501435279846, acc: 0.9447852969169617)
[2025-02-13 20:35:34,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:34,990][root][INFO] - Training Epoch: 2/2, step 1869/7134 completed (loss: 0.08932619541883469, acc: 0.9794520735740662)
[2025-02-13 20:35:35,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:35,340][root][INFO] - Training Epoch: 2/2, step 1870/7134 completed (loss: 0.0704478994011879, acc: 0.9675675630569458)
[2025-02-13 20:35:35,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:35,736][root][INFO] - Training Epoch: 2/2, step 1871/7134 completed (loss: 0.26002034544944763, acc: 0.9612902998924255)
[2025-02-13 20:35:35,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:36,138][root][INFO] - Training Epoch: 2/2, step 1872/7134 completed (loss: 1.225756287574768, acc: 0.7246376872062683)
[2025-02-13 20:35:36,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:36,534][root][INFO] - Training Epoch: 2/2, step 1873/7134 completed (loss: 0.5129888653755188, acc: 0.8677685856819153)
[2025-02-13 20:35:36,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:36,915][root][INFO] - Training Epoch: 2/2, step 1874/7134 completed (loss: 0.10456806421279907, acc: 0.977011501789093)
[2025-02-13 20:35:37,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:37,307][root][INFO] - Training Epoch: 2/2, step 1875/7134 completed (loss: 0.08825763314962387, acc: 0.9732142686843872)
[2025-02-13 20:35:37,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:37,699][root][INFO] - Training Epoch: 2/2, step 1876/7134 completed (loss: 0.3054509460926056, acc: 0.9257143139839172)
[2025-02-13 20:35:37,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:38,104][root][INFO] - Training Epoch: 2/2, step 1877/7134 completed (loss: 0.09225017577409744, acc: 0.9830508232116699)
[2025-02-13 20:35:38,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:38,463][root][INFO] - Training Epoch: 2/2, step 1878/7134 completed (loss: 0.10324069857597351, acc: 0.9673202633857727)
[2025-02-13 20:35:38,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:38,837][root][INFO] - Training Epoch: 2/2, step 1879/7134 completed (loss: 0.47040069103240967, acc: 0.8888888955116272)
[2025-02-13 20:35:38,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:39,184][root][INFO] - Training Epoch: 2/2, step 1880/7134 completed (loss: 0.22413787245750427, acc: 0.9593495726585388)
[2025-02-13 20:35:39,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:39,554][root][INFO] - Training Epoch: 2/2, step 1881/7134 completed (loss: 0.15645889937877655, acc: 0.9577465057373047)
[2025-02-13 20:35:39,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:39,953][root][INFO] - Training Epoch: 2/2, step 1882/7134 completed (loss: 0.15131238102912903, acc: 0.967391312122345)
[2025-02-13 20:35:40,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:40,322][root][INFO] - Training Epoch: 2/2, step 1883/7134 completed (loss: 0.13163937628269196, acc: 0.961240291595459)
[2025-02-13 20:35:40,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:40,688][root][INFO] - Training Epoch: 2/2, step 1884/7134 completed (loss: 0.16795554757118225, acc: 0.9594594836235046)
[2025-02-13 20:35:40,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:41,074][root][INFO] - Training Epoch: 2/2, step 1885/7134 completed (loss: 0.29678821563720703, acc: 0.9014084339141846)
[2025-02-13 20:35:41,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:41,485][root][INFO] - Training Epoch: 2/2, step 1886/7134 completed (loss: 0.3019702136516571, acc: 0.9444444179534912)
[2025-02-13 20:35:41,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:41,869][root][INFO] - Training Epoch: 2/2, step 1887/7134 completed (loss: 0.17395754158496857, acc: 0.9536423683166504)
[2025-02-13 20:35:42,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:42,253][root][INFO] - Training Epoch: 2/2, step 1888/7134 completed (loss: 0.10071544349193573, acc: 0.9829059839248657)
[2025-02-13 20:35:42,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:42,657][root][INFO] - Training Epoch: 2/2, step 1889/7134 completed (loss: 0.0980871245265007, acc: 0.9590643048286438)
[2025-02-13 20:35:42,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:43,064][root][INFO] - Training Epoch: 2/2, step 1890/7134 completed (loss: 0.05259091407060623, acc: 0.9801324605941772)
[2025-02-13 20:35:43,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:43,447][root][INFO] - Training Epoch: 2/2, step 1891/7134 completed (loss: 0.19319970905780792, acc: 0.9709302186965942)
[2025-02-13 20:35:43,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:43,830][root][INFO] - Training Epoch: 2/2, step 1892/7134 completed (loss: 0.13128581643104553, acc: 0.9820359349250793)
[2025-02-13 20:35:43,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:44,181][root][INFO] - Training Epoch: 2/2, step 1893/7134 completed (loss: 0.14064404368400574, acc: 0.9444444179534912)
[2025-02-13 20:35:44,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:44,567][root][INFO] - Training Epoch: 2/2, step 1894/7134 completed (loss: 0.17043127119541168, acc: 0.9662162065505981)
[2025-02-13 20:35:44,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:44,943][root][INFO] - Training Epoch: 2/2, step 1895/7134 completed (loss: 0.5947397947311401, acc: 0.8314606547355652)
[2025-02-13 20:35:45,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:45,333][root][INFO] - Training Epoch: 2/2, step 1896/7134 completed (loss: 0.27976876497268677, acc: 0.9090909361839294)
[2025-02-13 20:35:45,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:45,719][root][INFO] - Training Epoch: 2/2, step 1897/7134 completed (loss: 0.12450242042541504, acc: 0.9647887349128723)
[2025-02-13 20:35:45,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:46,116][root][INFO] - Training Epoch: 2/2, step 1898/7134 completed (loss: 0.0833907499909401, acc: 0.9586206674575806)
[2025-02-13 20:35:46,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:46,474][root][INFO] - Training Epoch: 2/2, step 1899/7134 completed (loss: 0.07381630688905716, acc: 0.9848484992980957)
[2025-02-13 20:35:46,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:46,833][root][INFO] - Training Epoch: 2/2, step 1900/7134 completed (loss: 0.06155337393283844, acc: 1.0)
[2025-02-13 20:35:46,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:47,184][root][INFO] - Training Epoch: 2/2, step 1901/7134 completed (loss: 0.16244164109230042, acc: 0.9784172773361206)
[2025-02-13 20:35:47,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:47,584][root][INFO] - Training Epoch: 2/2, step 1902/7134 completed (loss: 0.07422647625207901, acc: 0.9784172773361206)
[2025-02-13 20:35:47,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:47,975][root][INFO] - Training Epoch: 2/2, step 1903/7134 completed (loss: 0.06734132766723633, acc: 0.9803921580314636)
[2025-02-13 20:35:48,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:48,360][root][INFO] - Training Epoch: 2/2, step 1904/7134 completed (loss: 0.08161178976297379, acc: 0.9904761910438538)
[2025-02-13 20:35:48,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:48,774][root][INFO] - Training Epoch: 2/2, step 1905/7134 completed (loss: 0.12130596488714218, acc: 0.978723406791687)
[2025-02-13 20:35:48,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:49,136][root][INFO] - Training Epoch: 2/2, step 1906/7134 completed (loss: 0.10795316100120544, acc: 0.985401451587677)
[2025-02-13 20:35:49,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:49,495][root][INFO] - Training Epoch: 2/2, step 1907/7134 completed (loss: 0.16812366247177124, acc: 0.9523809552192688)
[2025-02-13 20:35:49,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:49,869][root][INFO] - Training Epoch: 2/2, step 1908/7134 completed (loss: 0.107975535094738, acc: 0.9719626307487488)
[2025-02-13 20:35:50,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:50,225][root][INFO] - Training Epoch: 2/2, step 1909/7134 completed (loss: 0.1463344693183899, acc: 0.9387755393981934)
[2025-02-13 20:35:50,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:50,620][root][INFO] - Training Epoch: 2/2, step 1910/7134 completed (loss: 0.10452061146497726, acc: 0.9627906680107117)
[2025-02-13 20:35:50,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:50,997][root][INFO] - Training Epoch: 2/2, step 1911/7134 completed (loss: 0.07891132682561874, acc: 0.9847715497016907)
[2025-02-13 20:35:51,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:51,365][root][INFO] - Training Epoch: 2/2, step 1912/7134 completed (loss: 0.033680833876132965, acc: 0.9818181991577148)
[2025-02-13 20:35:51,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:51,739][root][INFO] - Training Epoch: 2/2, step 1913/7134 completed (loss: 0.11672613769769669, acc: 0.9763779640197754)
[2025-02-13 20:35:51,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:52,110][root][INFO] - Training Epoch: 2/2, step 1914/7134 completed (loss: 0.15890781581401825, acc: 0.9508196711540222)
[2025-02-13 20:35:52,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:52,481][root][INFO] - Training Epoch: 2/2, step 1915/7134 completed (loss: 0.07916862517595291, acc: 0.9860140085220337)
[2025-02-13 20:35:52,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:52,876][root][INFO] - Training Epoch: 2/2, step 1916/7134 completed (loss: 0.061209216713905334, acc: 0.9835164546966553)
[2025-02-13 20:35:53,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:53,269][root][INFO] - Training Epoch: 2/2, step 1917/7134 completed (loss: 0.11668724566698074, acc: 0.9818181991577148)
[2025-02-13 20:35:53,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:53,636][root][INFO] - Training Epoch: 2/2, step 1918/7134 completed (loss: 0.11637633293867111, acc: 0.9801324605941772)
[2025-02-13 20:35:53,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:53,995][root][INFO] - Training Epoch: 2/2, step 1919/7134 completed (loss: 0.0735335722565651, acc: 0.9878048896789551)
[2025-02-13 20:35:54,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:54,352][root][INFO] - Training Epoch: 2/2, step 1920/7134 completed (loss: 0.10132439434528351, acc: 0.9863945841789246)
[2025-02-13 20:35:54,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:54,705][root][INFO] - Training Epoch: 2/2, step 1921/7134 completed (loss: 0.09842915087938309, acc: 0.9766082167625427)
[2025-02-13 20:35:54,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:55,102][root][INFO] - Training Epoch: 2/2, step 1922/7134 completed (loss: 0.026686063036322594, acc: 0.9959016442298889)
[2025-02-13 20:35:55,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:55,504][root][INFO] - Training Epoch: 2/2, step 1923/7134 completed (loss: 0.05779210478067398, acc: 0.9790209531784058)
[2025-02-13 20:35:55,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:55,866][root][INFO] - Training Epoch: 2/2, step 1924/7134 completed (loss: 0.14935840666294098, acc: 0.953125)
[2025-02-13 20:35:56,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:56,264][root][INFO] - Training Epoch: 2/2, step 1925/7134 completed (loss: 0.16897229850292206, acc: 0.9372385144233704)
[2025-02-13 20:35:56,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:56,644][root][INFO] - Training Epoch: 2/2, step 1926/7134 completed (loss: 0.049689341336488724, acc: 0.9866071343421936)
[2025-02-13 20:35:56,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:57,012][root][INFO] - Training Epoch: 2/2, step 1927/7134 completed (loss: 0.12370247393846512, acc: 0.9716981053352356)
[2025-02-13 20:35:57,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:57,384][root][INFO] - Training Epoch: 2/2, step 1928/7134 completed (loss: 0.08858736604452133, acc: 0.9702380895614624)
[2025-02-13 20:35:57,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:57,737][root][INFO] - Training Epoch: 2/2, step 1929/7134 completed (loss: 0.03956922888755798, acc: 0.994413435459137)
[2025-02-13 20:35:57,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:58,148][root][INFO] - Training Epoch: 2/2, step 1930/7134 completed (loss: 0.29160526394844055, acc: 0.9130434989929199)
[2025-02-13 20:35:58,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:58,545][root][INFO] - Training Epoch: 2/2, step 1931/7134 completed (loss: 0.23748953640460968, acc: 0.9490740895271301)
[2025-02-13 20:35:58,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:58,909][root][INFO] - Training Epoch: 2/2, step 1932/7134 completed (loss: 0.22194920480251312, acc: 0.9497206807136536)
[2025-02-13 20:35:59,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:59,285][root][INFO] - Training Epoch: 2/2, step 1933/7134 completed (loss: 0.2908783257007599, acc: 0.9346733689308167)
[2025-02-13 20:35:59,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:59,697][root][INFO] - Training Epoch: 2/2, step 1934/7134 completed (loss: 0.08596143126487732, acc: 0.9657142758369446)
[2025-02-13 20:35:59,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:00,095][root][INFO] - Training Epoch: 2/2, step 1935/7134 completed (loss: 0.30071479082107544, acc: 0.9390243887901306)
[2025-02-13 20:36:00,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:00,516][root][INFO] - Training Epoch: 2/2, step 1936/7134 completed (loss: 0.06533374637365341, acc: 0.9738562107086182)
[2025-02-13 20:36:00,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:00,873][root][INFO] - Training Epoch: 2/2, step 1937/7134 completed (loss: 0.1386520117521286, acc: 0.9695122241973877)
[2025-02-13 20:36:01,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:01,281][root][INFO] - Training Epoch: 2/2, step 1938/7134 completed (loss: 0.3505672514438629, acc: 0.929347813129425)
[2025-02-13 20:36:01,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:01,680][root][INFO] - Training Epoch: 2/2, step 1939/7134 completed (loss: 0.06233195215463638, acc: 0.9913793206214905)
[2025-02-13 20:36:01,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:02,042][root][INFO] - Training Epoch: 2/2, step 1940/7134 completed (loss: 0.2038959413766861, acc: 0.940119743347168)
[2025-02-13 20:36:02,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:02,401][root][INFO] - Training Epoch: 2/2, step 1941/7134 completed (loss: 0.06805303692817688, acc: 0.976331353187561)
[2025-02-13 20:36:02,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:02,716][root][INFO] - Training Epoch: 2/2, step 1942/7134 completed (loss: 0.059500839561223984, acc: 0.9814814925193787)
[2025-02-13 20:36:02,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:03,079][root][INFO] - Training Epoch: 2/2, step 1943/7134 completed (loss: 0.17468400299549103, acc: 0.9684210419654846)
[2025-02-13 20:36:03,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:03,467][root][INFO] - Training Epoch: 2/2, step 1944/7134 completed (loss: 0.06067603826522827, acc: 0.9836065769195557)
[2025-02-13 20:36:03,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:03,826][root][INFO] - Training Epoch: 2/2, step 1945/7134 completed (loss: 0.057267479598522186, acc: 0.9919354915618896)
[2025-02-13 20:36:03,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:04,178][root][INFO] - Training Epoch: 2/2, step 1946/7134 completed (loss: 0.08536549657583237, acc: 0.9785714149475098)
[2025-02-13 20:36:04,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:04,560][root][INFO] - Training Epoch: 2/2, step 1947/7134 completed (loss: 0.1148313656449318, acc: 0.9537572264671326)
[2025-02-13 20:36:04,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:04,931][root][INFO] - Training Epoch: 2/2, step 1948/7134 completed (loss: 0.09297840297222137, acc: 0.9769230484962463)
[2025-02-13 20:36:05,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:05,302][root][INFO] - Training Epoch: 2/2, step 1949/7134 completed (loss: 0.027090156450867653, acc: 1.0)
[2025-02-13 20:36:05,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:05,690][root][INFO] - Training Epoch: 2/2, step 1950/7134 completed (loss: 0.060405027121305466, acc: 0.9855072498321533)
[2025-02-13 20:36:05,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:06,107][root][INFO] - Training Epoch: 2/2, step 1951/7134 completed (loss: 0.11439275741577148, acc: 0.969924807548523)
[2025-02-13 20:36:06,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:06,494][root][INFO] - Training Epoch: 2/2, step 1952/7134 completed (loss: 0.08966665714979172, acc: 0.9759036302566528)
[2025-02-13 20:36:06,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:06,870][root][INFO] - Training Epoch: 2/2, step 1953/7134 completed (loss: 0.04554009437561035, acc: 0.9873417615890503)
[2025-02-13 20:36:07,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:07,250][root][INFO] - Training Epoch: 2/2, step 1954/7134 completed (loss: 0.06857583671808243, acc: 0.9803921580314636)
[2025-02-13 20:36:07,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:07,630][root][INFO] - Training Epoch: 2/2, step 1955/7134 completed (loss: 0.021027380600571632, acc: 1.0)
[2025-02-13 20:36:07,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:07,995][root][INFO] - Training Epoch: 2/2, step 1956/7134 completed (loss: 0.025827426463365555, acc: 0.989130437374115)
[2025-02-13 20:36:08,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:08,354][root][INFO] - Training Epoch: 2/2, step 1957/7134 completed (loss: 0.08501958101987839, acc: 0.9849624037742615)
[2025-02-13 20:36:08,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:08,681][root][INFO] - Training Epoch: 2/2, step 1958/7134 completed (loss: 0.0630260705947876, acc: 0.9888888597488403)
[2025-02-13 20:36:08,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:09,038][root][INFO] - Training Epoch: 2/2, step 1959/7134 completed (loss: 0.1441996544599533, acc: 0.9642857313156128)
[2025-02-13 20:36:09,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:09,401][root][INFO] - Training Epoch: 2/2, step 1960/7134 completed (loss: 0.08019410818815231, acc: 0.9833333492279053)
[2025-02-13 20:36:09,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:09,759][root][INFO] - Training Epoch: 2/2, step 1961/7134 completed (loss: 0.08405578881502151, acc: 0.9746835231781006)
[2025-02-13 20:36:09,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:10,128][root][INFO] - Training Epoch: 2/2, step 1962/7134 completed (loss: 0.03909534588456154, acc: 1.0)
[2025-02-13 20:36:10,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:10,534][root][INFO] - Training Epoch: 2/2, step 1963/7134 completed (loss: 0.1593240201473236, acc: 0.9668874144554138)
[2025-02-13 20:36:10,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:10,909][root][INFO] - Training Epoch: 2/2, step 1964/7134 completed (loss: 0.15481628477573395, acc: 0.9747474789619446)
[2025-02-13 20:36:11,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:11,278][root][INFO] - Training Epoch: 2/2, step 1965/7134 completed (loss: 0.15299241244792938, acc: 0.9550561904907227)
[2025-02-13 20:36:11,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:11,676][root][INFO] - Training Epoch: 2/2, step 1966/7134 completed (loss: 0.15787608921527863, acc: 0.9617834687232971)
[2025-02-13 20:36:11,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:12,048][root][INFO] - Training Epoch: 2/2, step 1967/7134 completed (loss: 0.12201680988073349, acc: 0.9631578922271729)
[2025-02-13 20:36:12,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:12,444][root][INFO] - Training Epoch: 2/2, step 1968/7134 completed (loss: 0.08790004998445511, acc: 0.9680851101875305)
[2025-02-13 20:36:12,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:12,786][root][INFO] - Training Epoch: 2/2, step 1969/7134 completed (loss: 0.11085343360900879, acc: 0.9661017060279846)
[2025-02-13 20:36:12,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:13,160][root][INFO] - Training Epoch: 2/2, step 1970/7134 completed (loss: 0.09445621073246002, acc: 0.9644970297813416)
[2025-02-13 20:36:13,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:13,526][root][INFO] - Training Epoch: 2/2, step 1971/7134 completed (loss: 0.22706584632396698, acc: 0.9424083828926086)
[2025-02-13 20:36:13,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:13,949][root][INFO] - Training Epoch: 2/2, step 1972/7134 completed (loss: 0.4053366482257843, acc: 0.8771929740905762)
[2025-02-13 20:36:14,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:14,333][root][INFO] - Training Epoch: 2/2, step 1973/7134 completed (loss: 0.5826848149299622, acc: 0.875)
[2025-02-13 20:36:14,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:14,719][root][INFO] - Training Epoch: 2/2, step 1974/7134 completed (loss: 0.07806592434644699, acc: 0.9845361113548279)
[2025-02-13 20:36:14,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:15,102][root][INFO] - Training Epoch: 2/2, step 1975/7134 completed (loss: 0.04697706922888756, acc: 0.9937499761581421)
[2025-02-13 20:36:15,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:15,533][root][INFO] - Training Epoch: 2/2, step 1976/7134 completed (loss: 0.2609497308731079, acc: 0.942307710647583)
[2025-02-13 20:36:15,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:15,922][root][INFO] - Training Epoch: 2/2, step 1977/7134 completed (loss: 0.15764769911766052, acc: 0.970059871673584)
[2025-02-13 20:36:16,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:16,304][root][INFO] - Training Epoch: 2/2, step 1978/7134 completed (loss: 0.0846719816327095, acc: 0.9883720874786377)
[2025-02-13 20:36:16,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:16,694][root][INFO] - Training Epoch: 2/2, step 1979/7134 completed (loss: 0.058846328407526016, acc: 0.9835164546966553)
[2025-02-13 20:36:16,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:17,073][root][INFO] - Training Epoch: 2/2, step 1980/7134 completed (loss: 0.10983496904373169, acc: 0.9842932224273682)
[2025-02-13 20:36:17,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:17,473][root][INFO] - Training Epoch: 2/2, step 1981/7134 completed (loss: 0.08332031220197678, acc: 0.9890109896659851)
[2025-02-13 20:36:17,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:17,837][root][INFO] - Training Epoch: 2/2, step 1982/7134 completed (loss: 0.09119099378585815, acc: 0.9743589758872986)
[2025-02-13 20:36:17,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:18,202][root][INFO] - Training Epoch: 2/2, step 1983/7134 completed (loss: 0.09132164716720581, acc: 0.9838709831237793)
[2025-02-13 20:36:18,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:18,600][root][INFO] - Training Epoch: 2/2, step 1984/7134 completed (loss: 0.0621686689555645, acc: 0.9767441749572754)
[2025-02-13 20:36:18,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:18,972][root][INFO] - Training Epoch: 2/2, step 1985/7134 completed (loss: 0.10311441868543625, acc: 0.9751552939414978)
[2025-02-13 20:36:19,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:19,349][root][INFO] - Training Epoch: 2/2, step 1986/7134 completed (loss: 0.03555221110582352, acc: 0.994413435459137)
[2025-02-13 20:36:19,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:19,764][root][INFO] - Training Epoch: 2/2, step 1987/7134 completed (loss: 0.09098271280527115, acc: 0.966292142868042)
[2025-02-13 20:36:19,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:20,155][root][INFO] - Training Epoch: 2/2, step 1988/7134 completed (loss: 0.07381458580493927, acc: 0.9939393997192383)
[2025-02-13 20:36:20,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:20,547][root][INFO] - Training Epoch: 2/2, step 1989/7134 completed (loss: 0.0664009153842926, acc: 0.9726027250289917)
[2025-02-13 20:36:20,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:20,942][root][INFO] - Training Epoch: 2/2, step 1990/7134 completed (loss: 0.08147589862346649, acc: 0.9779005646705627)
[2025-02-13 20:36:21,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:21,290][root][INFO] - Training Epoch: 2/2, step 1991/7134 completed (loss: 0.07334043085575104, acc: 0.9677419066429138)
[2025-02-13 20:36:21,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:21,705][root][INFO] - Training Epoch: 2/2, step 1992/7134 completed (loss: 0.21366479992866516, acc: 0.9347826242446899)
[2025-02-13 20:36:21,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:22,079][root][INFO] - Training Epoch: 2/2, step 1993/7134 completed (loss: 0.0959801971912384, acc: 0.9739130139350891)
[2025-02-13 20:36:22,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:22,499][root][INFO] - Training Epoch: 2/2, step 1994/7134 completed (loss: 0.041415877640247345, acc: 1.0)
[2025-02-13 20:36:22,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:22,879][root][INFO] - Training Epoch: 2/2, step 1995/7134 completed (loss: 0.05902710556983948, acc: 0.9724137783050537)
[2025-02-13 20:36:23,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:23,261][root][INFO] - Training Epoch: 2/2, step 1996/7134 completed (loss: 0.07577797025442123, acc: 0.9781022071838379)
[2025-02-13 20:36:23,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:23,648][root][INFO] - Training Epoch: 2/2, step 1997/7134 completed (loss: 0.11194363981485367, acc: 0.9571428298950195)
[2025-02-13 20:36:23,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:24,032][root][INFO] - Training Epoch: 2/2, step 1998/7134 completed (loss: 0.04626701772212982, acc: 0.9929577708244324)
[2025-02-13 20:36:24,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:24,440][root][INFO] - Training Epoch: 2/2, step 1999/7134 completed (loss: 0.09775339812040329, acc: 0.9580419659614563)
[2025-02-13 20:36:24,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:24,819][root][INFO] - Training Epoch: 2/2, step 2000/7134 completed (loss: 0.08186879754066467, acc: 0.9791666865348816)
[2025-02-13 20:36:24,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:25,209][root][INFO] - Training Epoch: 2/2, step 2001/7134 completed (loss: 0.03717294707894325, acc: 0.9931034445762634)
[2025-02-13 20:36:25,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:25,605][root][INFO] - Training Epoch: 2/2, step 2002/7134 completed (loss: 0.08164975792169571, acc: 0.9849624037742615)
[2025-02-13 20:36:25,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:25,964][root][INFO] - Training Epoch: 2/2, step 2003/7134 completed (loss: 0.022978805005550385, acc: 0.9912280440330505)
[2025-02-13 20:36:26,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:26,316][root][INFO] - Training Epoch: 2/2, step 2004/7134 completed (loss: 0.29309341311454773, acc: 0.9344262480735779)
[2025-02-13 20:36:26,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:26,745][root][INFO] - Training Epoch: 2/2, step 2005/7134 completed (loss: 0.10743610560894012, acc: 0.9591836929321289)
[2025-02-13 20:36:26,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:27,136][root][INFO] - Training Epoch: 2/2, step 2006/7134 completed (loss: 0.03224432095885277, acc: 0.9883720874786377)
[2025-02-13 20:36:27,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:27,514][root][INFO] - Training Epoch: 2/2, step 2007/7134 completed (loss: 0.03573278337717056, acc: 0.9831932783126831)
[2025-02-13 20:36:27,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:27,905][root][INFO] - Training Epoch: 2/2, step 2008/7134 completed (loss: 0.0737404152750969, acc: 0.9794520735740662)
[2025-02-13 20:36:28,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:28,274][root][INFO] - Training Epoch: 2/2, step 2009/7134 completed (loss: 0.0531836561858654, acc: 0.9777777791023254)
[2025-02-13 20:36:28,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:28,659][root][INFO] - Training Epoch: 2/2, step 2010/7134 completed (loss: 0.07698100805282593, acc: 0.970802903175354)
[2025-02-13 20:36:28,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:29,019][root][INFO] - Training Epoch: 2/2, step 2011/7134 completed (loss: 0.05004284530878067, acc: 0.9841269850730896)
[2025-02-13 20:36:29,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:29,403][root][INFO] - Training Epoch: 2/2, step 2012/7134 completed (loss: 0.09533721208572388, acc: 0.9745762944221497)
[2025-02-13 20:36:29,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:29,800][root][INFO] - Training Epoch: 2/2, step 2013/7134 completed (loss: 0.07995302230119705, acc: 0.9836065769195557)
[2025-02-13 20:36:29,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:30,191][root][INFO] - Training Epoch: 2/2, step 2014/7134 completed (loss: 0.06765351444482803, acc: 0.9876543283462524)
[2025-02-13 20:36:30,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:30,584][root][INFO] - Training Epoch: 2/2, step 2015/7134 completed (loss: 0.04921690374612808, acc: 0.987500011920929)
[2025-02-13 20:36:30,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:30,967][root][INFO] - Training Epoch: 2/2, step 2016/7134 completed (loss: 0.2291799783706665, acc: 0.9455782175064087)
[2025-02-13 20:36:31,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:31,351][root][INFO] - Training Epoch: 2/2, step 2017/7134 completed (loss: 0.12544068694114685, acc: 0.9536423683166504)
[2025-02-13 20:36:31,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:31,735][root][INFO] - Training Epoch: 2/2, step 2018/7134 completed (loss: 0.03189832717180252, acc: 1.0)
[2025-02-13 20:36:31,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:32,105][root][INFO] - Training Epoch: 2/2, step 2019/7134 completed (loss: 0.0649770051240921, acc: 0.9735099077224731)
[2025-02-13 20:36:32,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:32,460][root][INFO] - Training Epoch: 2/2, step 2020/7134 completed (loss: 0.06062600761651993, acc: 0.9834710955619812)
[2025-02-13 20:36:32,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:32,853][root][INFO] - Training Epoch: 2/2, step 2021/7134 completed (loss: 0.10155188292264938, acc: 0.9621621370315552)
[2025-02-13 20:36:32,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:33,209][root][INFO] - Training Epoch: 2/2, step 2022/7134 completed (loss: 0.16746918857097626, acc: 0.9552238583564758)
[2025-02-13 20:36:33,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:33,587][root][INFO] - Training Epoch: 2/2, step 2023/7134 completed (loss: 0.12407917529344559, acc: 0.9754601120948792)
[2025-02-13 20:36:33,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:33,963][root][INFO] - Training Epoch: 2/2, step 2024/7134 completed (loss: 0.11965454369783401, acc: 0.9767441749572754)
[2025-02-13 20:36:34,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:34,338][root][INFO] - Training Epoch: 2/2, step 2025/7134 completed (loss: 0.12369746714830399, acc: 0.9824561476707458)
[2025-02-13 20:36:34,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:34,692][root][INFO] - Training Epoch: 2/2, step 2026/7134 completed (loss: 0.05412048101425171, acc: 0.9941520690917969)
[2025-02-13 20:36:34,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:35,068][root][INFO] - Training Epoch: 2/2, step 2027/7134 completed (loss: 0.05452492833137512, acc: 0.9924812316894531)
[2025-02-13 20:36:35,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:35,450][root][INFO] - Training Epoch: 2/2, step 2028/7134 completed (loss: 0.08754585683345795, acc: 0.9693251252174377)
[2025-02-13 20:36:35,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:35,852][root][INFO] - Training Epoch: 2/2, step 2029/7134 completed (loss: 0.1123122051358223, acc: 0.970588207244873)
[2025-02-13 20:36:35,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:36,243][root][INFO] - Training Epoch: 2/2, step 2030/7134 completed (loss: 0.21231432259082794, acc: 0.9277108311653137)
[2025-02-13 20:36:36,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:36,612][root][INFO] - Training Epoch: 2/2, step 2031/7134 completed (loss: 0.043931178748607635, acc: 0.9944751262664795)
[2025-02-13 20:36:36,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:36,982][root][INFO] - Training Epoch: 2/2, step 2032/7134 completed (loss: 0.19195270538330078, acc: 0.9473684430122375)
[2025-02-13 20:36:37,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:37,375][root][INFO] - Training Epoch: 2/2, step 2033/7134 completed (loss: 0.014815577305853367, acc: 1.0)
[2025-02-13 20:36:37,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:37,712][root][INFO] - Training Epoch: 2/2, step 2034/7134 completed (loss: 0.19436179101467133, acc: 0.9328858852386475)
[2025-02-13 20:36:37,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:38,102][root][INFO] - Training Epoch: 2/2, step 2035/7134 completed (loss: 0.041955817490816116, acc: 0.9838709831237793)
[2025-02-13 20:36:38,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:38,461][root][INFO] - Training Epoch: 2/2, step 2036/7134 completed (loss: 0.07786212116479874, acc: 0.9770992398262024)
[2025-02-13 20:36:38,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:38,826][root][INFO] - Training Epoch: 2/2, step 2037/7134 completed (loss: 0.0533176064491272, acc: 0.9754601120948792)
[2025-02-13 20:36:38,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:39,187][root][INFO] - Training Epoch: 2/2, step 2038/7134 completed (loss: 0.03998776152729988, acc: 0.9945054650306702)
[2025-02-13 20:36:39,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:39,554][root][INFO] - Training Epoch: 2/2, step 2039/7134 completed (loss: 0.02241263911128044, acc: 1.0)
[2025-02-13 20:36:39,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:39,943][root][INFO] - Training Epoch: 2/2, step 2040/7134 completed (loss: 0.12938085198402405, acc: 0.9741379022598267)
[2025-02-13 20:36:40,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:40,331][root][INFO] - Training Epoch: 2/2, step 2041/7134 completed (loss: 0.04163240268826485, acc: 0.9878787994384766)
[2025-02-13 20:36:40,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:40,734][root][INFO] - Training Epoch: 2/2, step 2042/7134 completed (loss: 0.08458641171455383, acc: 0.9745762944221497)
[2025-02-13 20:36:40,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:41,090][root][INFO] - Training Epoch: 2/2, step 2043/7134 completed (loss: 0.027961529791355133, acc: 0.9932885766029358)
[2025-02-13 20:36:41,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:41,466][root][INFO] - Training Epoch: 2/2, step 2044/7134 completed (loss: 0.03994157910346985, acc: 0.9919999837875366)
[2025-02-13 20:36:41,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:41,839][root][INFO] - Training Epoch: 2/2, step 2045/7134 completed (loss: 0.08215738087892532, acc: 0.9916666746139526)
[2025-02-13 20:36:41,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:42,211][root][INFO] - Training Epoch: 2/2, step 2046/7134 completed (loss: 0.13595077395439148, acc: 0.9663865566253662)
[2025-02-13 20:36:42,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:42,612][root][INFO] - Training Epoch: 2/2, step 2047/7134 completed (loss: 0.19185984134674072, acc: 0.9448275566101074)
[2025-02-13 20:36:42,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:43,014][root][INFO] - Training Epoch: 2/2, step 2048/7134 completed (loss: 0.1950305551290512, acc: 0.9642857313156128)
[2025-02-13 20:36:43,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:43,393][root][INFO] - Training Epoch: 2/2, step 2049/7134 completed (loss: 0.19219569861888885, acc: 0.9363636374473572)
[2025-02-13 20:36:43,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:43,717][root][INFO] - Training Epoch: 2/2, step 2050/7134 completed (loss: 0.14431166648864746, acc: 0.965753436088562)
[2025-02-13 20:36:43,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:44,084][root][INFO] - Training Epoch: 2/2, step 2051/7134 completed (loss: 0.11021436005830765, acc: 0.9646017551422119)
[2025-02-13 20:36:44,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:44,466][root][INFO] - Training Epoch: 2/2, step 2052/7134 completed (loss: 0.19232651591300964, acc: 0.9583333134651184)
[2025-02-13 20:36:44,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:44,860][root][INFO] - Training Epoch: 2/2, step 2053/7134 completed (loss: 0.2895621657371521, acc: 0.9465649127960205)
[2025-02-13 20:36:45,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:45,267][root][INFO] - Training Epoch: 2/2, step 2054/7134 completed (loss: 0.2430313378572464, acc: 0.942307710647583)
[2025-02-13 20:36:45,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:45,627][root][INFO] - Training Epoch: 2/2, step 2055/7134 completed (loss: 0.17687098681926727, acc: 0.9520547986030579)
[2025-02-13 20:36:45,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:46,005][root][INFO] - Training Epoch: 2/2, step 2056/7134 completed (loss: 0.1943623125553131, acc: 0.9661017060279846)
[2025-02-13 20:36:46,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:46,391][root][INFO] - Training Epoch: 2/2, step 2057/7134 completed (loss: 0.1338062286376953, acc: 0.9719101190567017)
[2025-02-13 20:36:46,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:46,784][root][INFO] - Training Epoch: 2/2, step 2058/7134 completed (loss: 0.21718262135982513, acc: 0.9595375657081604)
[2025-02-13 20:36:46,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:47,190][root][INFO] - Training Epoch: 2/2, step 2059/7134 completed (loss: 0.35782548785209656, acc: 0.936170220375061)
[2025-02-13 20:36:47,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:47,539][root][INFO] - Training Epoch: 2/2, step 2060/7134 completed (loss: 0.13025717437267303, acc: 0.9745222926139832)
[2025-02-13 20:36:47,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:47,910][root][INFO] - Training Epoch: 2/2, step 2061/7134 completed (loss: 0.167150616645813, acc: 0.9464285969734192)
[2025-02-13 20:36:48,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:48,313][root][INFO] - Training Epoch: 2/2, step 2062/7134 completed (loss: 0.17352734506130219, acc: 0.953125)
[2025-02-13 20:36:48,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:48,703][root][INFO] - Training Epoch: 2/2, step 2063/7134 completed (loss: 0.0952862948179245, acc: 0.9716312289237976)
[2025-02-13 20:36:48,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:49,075][root][INFO] - Training Epoch: 2/2, step 2064/7134 completed (loss: 0.11317531019449234, acc: 0.970588207244873)
[2025-02-13 20:36:49,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:49,447][root][INFO] - Training Epoch: 2/2, step 2065/7134 completed (loss: 0.12399609386920929, acc: 0.9714285731315613)
[2025-02-13 20:36:49,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:49,841][root][INFO] - Training Epoch: 2/2, step 2066/7134 completed (loss: 0.08908108621835709, acc: 0.9756097793579102)
[2025-02-13 20:36:49,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:50,225][root][INFO] - Training Epoch: 2/2, step 2067/7134 completed (loss: 0.12122034281492233, acc: 0.9583333134651184)
[2025-02-13 20:36:50,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:50,611][root][INFO] - Training Epoch: 2/2, step 2068/7134 completed (loss: 0.08879449218511581, acc: 0.970588207244873)
[2025-02-13 20:36:50,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:51,008][root][INFO] - Training Epoch: 2/2, step 2069/7134 completed (loss: 0.06768150627613068, acc: 0.9731543660163879)
[2025-02-13 20:36:51,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:51,392][root][INFO] - Training Epoch: 2/2, step 2070/7134 completed (loss: 0.03451148048043251, acc: 1.0)
[2025-02-13 20:36:51,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:51,789][root][INFO] - Training Epoch: 2/2, step 2071/7134 completed (loss: 0.01727174036204815, acc: 1.0)
[2025-02-13 20:36:51,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:52,156][root][INFO] - Training Epoch: 2/2, step 2072/7134 completed (loss: 0.06573647260665894, acc: 0.9803921580314636)
[2025-02-13 20:36:52,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:52,549][root][INFO] - Training Epoch: 2/2, step 2073/7134 completed (loss: 0.044456373900175095, acc: 0.9917355179786682)
[2025-02-13 20:36:52,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:52,946][root][INFO] - Training Epoch: 2/2, step 2074/7134 completed (loss: 0.11980472505092621, acc: 0.9738219976425171)
[2025-02-13 20:36:53,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:53,322][root][INFO] - Training Epoch: 2/2, step 2075/7134 completed (loss: 0.12026539444923401, acc: 0.955974817276001)
[2025-02-13 20:36:53,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:53,695][root][INFO] - Training Epoch: 2/2, step 2076/7134 completed (loss: 0.08042513579130173, acc: 0.9863945841789246)
[2025-02-13 20:36:53,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:54,080][root][INFO] - Training Epoch: 2/2, step 2077/7134 completed (loss: 0.07432994991540909, acc: 0.9830508232116699)
[2025-02-13 20:36:54,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:54,476][root][INFO] - Training Epoch: 2/2, step 2078/7134 completed (loss: 0.06476149708032608, acc: 0.989847719669342)
[2025-02-13 20:36:54,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:54,854][root][INFO] - Training Epoch: 2/2, step 2079/7134 completed (loss: 0.0571429468691349, acc: 0.9879518151283264)
[2025-02-13 20:36:54,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:55,232][root][INFO] - Training Epoch: 2/2, step 2080/7134 completed (loss: 0.09622947871685028, acc: 0.9803921580314636)
[2025-02-13 20:36:55,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:55,603][root][INFO] - Training Epoch: 2/2, step 2081/7134 completed (loss: 0.13658718764781952, acc: 0.987261176109314)
[2025-02-13 20:36:55,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:55,986][root][INFO] - Training Epoch: 2/2, step 2082/7134 completed (loss: 0.14152681827545166, acc: 0.9491525292396545)
[2025-02-13 20:36:56,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:56,327][root][INFO] - Training Epoch: 2/2, step 2083/7134 completed (loss: 0.11118229478597641, acc: 0.9844961166381836)
[2025-02-13 20:36:56,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:56,710][root][INFO] - Training Epoch: 2/2, step 2084/7134 completed (loss: 0.08680947124958038, acc: 0.9800000190734863)
[2025-02-13 20:36:56,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:57,129][root][INFO] - Training Epoch: 2/2, step 2085/7134 completed (loss: 0.021604018285870552, acc: 0.9918699264526367)
[2025-02-13 20:36:57,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:57,474][root][INFO] - Training Epoch: 2/2, step 2086/7134 completed (loss: 0.04340027645230293, acc: 0.9901960492134094)
[2025-02-13 20:36:57,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:57,849][root][INFO] - Training Epoch: 2/2, step 2087/7134 completed (loss: 0.02714763954281807, acc: 0.9940476417541504)
[2025-02-13 20:36:57,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:58,233][root][INFO] - Training Epoch: 2/2, step 2088/7134 completed (loss: 0.1816859394311905, acc: 0.9644970297813416)
[2025-02-13 20:36:58,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:58,633][root][INFO] - Training Epoch: 2/2, step 2089/7134 completed (loss: 0.12380111962556839, acc: 0.976190447807312)
[2025-02-13 20:36:58,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:58,992][root][INFO] - Training Epoch: 2/2, step 2090/7134 completed (loss: 0.04106791317462921, acc: 0.9869281053543091)
[2025-02-13 20:36:59,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:59,374][root][INFO] - Training Epoch: 2/2, step 2091/7134 completed (loss: 0.03692009299993515, acc: 0.9862068891525269)
[2025-02-13 20:36:59,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:59,744][root][INFO] - Training Epoch: 2/2, step 2092/7134 completed (loss: 0.028587501496076584, acc: 0.9933775067329407)
[2025-02-13 20:36:59,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:00,118][root][INFO] - Training Epoch: 2/2, step 2093/7134 completed (loss: 0.06940431147813797, acc: 0.9789473414421082)
[2025-02-13 20:37:00,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:00,488][root][INFO] - Training Epoch: 2/2, step 2094/7134 completed (loss: 0.09965475648641586, acc: 0.9701492786407471)
[2025-02-13 20:37:00,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:00,869][root][INFO] - Training Epoch: 2/2, step 2095/7134 completed (loss: 0.09641778469085693, acc: 0.9701492786407471)
[2025-02-13 20:37:00,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:01,225][root][INFO] - Training Epoch: 2/2, step 2096/7134 completed (loss: 0.04836583882570267, acc: 0.9860140085220337)
[2025-02-13 20:37:01,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:01,578][root][INFO] - Training Epoch: 2/2, step 2097/7134 completed (loss: 0.07989255338907242, acc: 0.9819276928901672)
[2025-02-13 20:37:01,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:01,952][root][INFO] - Training Epoch: 2/2, step 2098/7134 completed (loss: 0.04177040234208107, acc: 0.9868420958518982)
[2025-02-13 20:37:02,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:02,302][root][INFO] - Training Epoch: 2/2, step 2099/7134 completed (loss: 0.04688216745853424, acc: 0.9765625)
[2025-02-13 20:37:02,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:02,695][root][INFO] - Training Epoch: 2/2, step 2100/7134 completed (loss: 0.060569360852241516, acc: 0.977142870426178)
[2025-02-13 20:37:02,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:03,053][root][INFO] - Training Epoch: 2/2, step 2101/7134 completed (loss: 0.05485326796770096, acc: 0.9940476417541504)
[2025-02-13 20:37:03,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:03,425][root][INFO] - Training Epoch: 2/2, step 2102/7134 completed (loss: 0.0650172010064125, acc: 0.9837837815284729)
[2025-02-13 20:37:03,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:03,817][root][INFO] - Training Epoch: 2/2, step 2103/7134 completed (loss: 0.10125476121902466, acc: 0.987500011920929)
[2025-02-13 20:37:03,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:04,204][root][INFO] - Training Epoch: 2/2, step 2104/7134 completed (loss: 0.024861155077815056, acc: 1.0)
[2025-02-13 20:37:04,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:04,558][root][INFO] - Training Epoch: 2/2, step 2105/7134 completed (loss: 0.06361650675535202, acc: 0.9858155846595764)
[2025-02-13 20:37:04,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:04,937][root][INFO] - Training Epoch: 2/2, step 2106/7134 completed (loss: 0.06520902365446091, acc: 0.9802631735801697)
[2025-02-13 20:37:05,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:05,313][root][INFO] - Training Epoch: 2/2, step 2107/7134 completed (loss: 0.06810179352760315, acc: 0.9779005646705627)
[2025-02-13 20:37:05,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:05,680][root][INFO] - Training Epoch: 2/2, step 2108/7134 completed (loss: 0.062388189136981964, acc: 0.9751552939414978)
[2025-02-13 20:37:05,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:06,078][root][INFO] - Training Epoch: 2/2, step 2109/7134 completed (loss: 0.09610016644001007, acc: 0.97826087474823)
[2025-02-13 20:37:06,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:06,458][root][INFO] - Training Epoch: 2/2, step 2110/7134 completed (loss: 0.08996519446372986, acc: 0.970059871673584)
[2025-02-13 20:37:06,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:06,858][root][INFO] - Training Epoch: 2/2, step 2111/7134 completed (loss: 0.18828783929347992, acc: 0.9637305736541748)
[2025-02-13 20:37:06,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:07,242][root][INFO] - Training Epoch: 2/2, step 2112/7134 completed (loss: 0.06604819744825363, acc: 0.9733333587646484)
[2025-02-13 20:37:07,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:07,632][root][INFO] - Training Epoch: 2/2, step 2113/7134 completed (loss: 0.06727339327335358, acc: 0.9866666793823242)
[2025-02-13 20:37:07,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:08,005][root][INFO] - Training Epoch: 2/2, step 2114/7134 completed (loss: 0.10301313549280167, acc: 0.976331353187561)
[2025-02-13 20:37:08,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:08,355][root][INFO] - Training Epoch: 2/2, step 2115/7134 completed (loss: 0.09489147365093231, acc: 0.9712643623352051)
[2025-02-13 20:37:08,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:08,730][root][INFO] - Training Epoch: 2/2, step 2116/7134 completed (loss: 0.08202628046274185, acc: 0.9727891087532043)
[2025-02-13 20:37:08,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:09,113][root][INFO] - Training Epoch: 2/2, step 2117/7134 completed (loss: 0.10203515738248825, acc: 0.9717513918876648)
[2025-02-13 20:37:09,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:09,499][root][INFO] - Training Epoch: 2/2, step 2118/7134 completed (loss: 0.1057540625333786, acc: 0.9585798978805542)
[2025-02-13 20:37:09,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:09,879][root][INFO] - Training Epoch: 2/2, step 2119/7134 completed (loss: 0.06443415582180023, acc: 0.9826589822769165)
[2025-02-13 20:37:10,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:10,272][root][INFO] - Training Epoch: 2/2, step 2120/7134 completed (loss: 0.07517867535352707, acc: 0.9753086566925049)
[2025-02-13 20:37:10,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:10,640][root][INFO] - Training Epoch: 2/2, step 2121/7134 completed (loss: 0.16725392639636993, acc: 0.9731543660163879)
[2025-02-13 20:37:10,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:11,017][root][INFO] - Training Epoch: 2/2, step 2122/7134 completed (loss: 0.07618676871061325, acc: 0.9774011373519897)
[2025-02-13 20:37:11,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:11,397][root][INFO] - Training Epoch: 2/2, step 2123/7134 completed (loss: 0.16187232732772827, acc: 0.9802631735801697)
[2025-02-13 20:37:11,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:11,734][root][INFO] - Training Epoch: 2/2, step 2124/7134 completed (loss: 0.05034255608916283, acc: 0.9864864945411682)
[2025-02-13 20:37:11,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:12,141][root][INFO] - Training Epoch: 2/2, step 2125/7134 completed (loss: 0.07133429497480392, acc: 0.9820359349250793)
[2025-02-13 20:37:12,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:12,521][root][INFO] - Training Epoch: 2/2, step 2126/7134 completed (loss: 0.08352271467447281, acc: 0.9858155846595764)
[2025-02-13 20:37:12,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:12,871][root][INFO] - Training Epoch: 2/2, step 2127/7134 completed (loss: 0.03957681357860565, acc: 0.9879518151283264)
[2025-02-13 20:37:13,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:13,240][root][INFO] - Training Epoch: 2/2, step 2128/7134 completed (loss: 0.031593576073646545, acc: 1.0)
[2025-02-13 20:37:13,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:13,648][root][INFO] - Training Epoch: 2/2, step 2129/7134 completed (loss: 0.08696573227643967, acc: 0.976331353187561)
[2025-02-13 20:37:13,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:14,004][root][INFO] - Training Epoch: 2/2, step 2130/7134 completed (loss: 0.08946561813354492, acc: 0.9729729890823364)
[2025-02-13 20:37:14,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:14,376][root][INFO] - Training Epoch: 2/2, step 2131/7134 completed (loss: 0.04149569198489189, acc: 0.987730085849762)
[2025-02-13 20:37:14,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:14,753][root][INFO] - Training Epoch: 2/2, step 2132/7134 completed (loss: 0.017716359347105026, acc: 0.9939024448394775)
[2025-02-13 20:37:14,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:15,113][root][INFO] - Training Epoch: 2/2, step 2133/7134 completed (loss: 0.08983418345451355, acc: 0.9707602262496948)
[2025-02-13 20:37:15,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:15,511][root][INFO] - Training Epoch: 2/2, step 2134/7134 completed (loss: 0.07383966445922852, acc: 0.977011501789093)
[2025-02-13 20:37:15,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:15,905][root][INFO] - Training Epoch: 2/2, step 2135/7134 completed (loss: 0.03648552671074867, acc: 0.9867549538612366)
[2025-02-13 20:37:16,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:16,277][root][INFO] - Training Epoch: 2/2, step 2136/7134 completed (loss: 0.05426521971821785, acc: 0.9945054650306702)
[2025-02-13 20:37:16,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:16,670][root][INFO] - Training Epoch: 2/2, step 2137/7134 completed (loss: 0.07712094485759735, acc: 0.9893617033958435)
[2025-02-13 20:37:16,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:17,069][root][INFO] - Training Epoch: 2/2, step 2138/7134 completed (loss: 0.06402166187763214, acc: 0.9820359349250793)
[2025-02-13 20:37:17,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:17,453][root][INFO] - Training Epoch: 2/2, step 2139/7134 completed (loss: 0.04464925825595856, acc: 0.9883720874786377)
[2025-02-13 20:37:17,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:17,826][root][INFO] - Training Epoch: 2/2, step 2140/7134 completed (loss: 0.11668284982442856, acc: 0.9548022747039795)
[2025-02-13 20:37:17,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:18,133][root][INFO] - Training Epoch: 2/2, step 2141/7134 completed (loss: 0.06737704575061798, acc: 0.9893617033958435)
[2025-02-13 20:37:18,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:18,509][root][INFO] - Training Epoch: 2/2, step 2142/7134 completed (loss: 0.12922130525112152, acc: 0.9774011373519897)
[2025-02-13 20:37:18,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:18,905][root][INFO] - Training Epoch: 2/2, step 2143/7134 completed (loss: 0.02600196562707424, acc: 0.9931034445762634)
[2025-02-13 20:37:19,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:19,295][root][INFO] - Training Epoch: 2/2, step 2144/7134 completed (loss: 0.05768568813800812, acc: 0.9886363744735718)
[2025-02-13 20:37:19,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:19,664][root][INFO] - Training Epoch: 2/2, step 2145/7134 completed (loss: 0.08843820542097092, acc: 0.9941860437393188)
[2025-02-13 20:37:19,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:20,007][root][INFO] - Training Epoch: 2/2, step 2146/7134 completed (loss: 0.038648780435323715, acc: 0.9932432174682617)
[2025-02-13 20:37:20,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:20,387][root][INFO] - Training Epoch: 2/2, step 2147/7134 completed (loss: 0.11675727367401123, acc: 0.9545454382896423)
[2025-02-13 20:37:20,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:20,769][root][INFO] - Training Epoch: 2/2, step 2148/7134 completed (loss: 0.06731554120779037, acc: 0.9772727489471436)
[2025-02-13 20:37:20,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:21,156][root][INFO] - Training Epoch: 2/2, step 2149/7134 completed (loss: 0.02844255603849888, acc: 0.9941520690917969)
[2025-02-13 20:37:21,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:21,542][root][INFO] - Training Epoch: 2/2, step 2150/7134 completed (loss: 0.06788899004459381, acc: 0.9774011373519897)
[2025-02-13 20:37:21,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:21,916][root][INFO] - Training Epoch: 2/2, step 2151/7134 completed (loss: 0.07377512753009796, acc: 0.9819276928901672)
[2025-02-13 20:37:22,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:22,271][root][INFO] - Training Epoch: 2/2, step 2152/7134 completed (loss: 0.08397446572780609, acc: 0.9806451797485352)
[2025-02-13 20:37:22,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:22,653][root][INFO] - Training Epoch: 2/2, step 2153/7134 completed (loss: 0.07167372107505798, acc: 0.9901960492134094)
[2025-02-13 20:37:22,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:23,049][root][INFO] - Training Epoch: 2/2, step 2154/7134 completed (loss: 0.10804656147956848, acc: 0.982758641242981)
[2025-02-13 20:37:23,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:23,420][root][INFO] - Training Epoch: 2/2, step 2155/7134 completed (loss: 0.07903604954481125, acc: 0.9871794581413269)
[2025-02-13 20:37:23,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:23,796][root][INFO] - Training Epoch: 2/2, step 2156/7134 completed (loss: 0.05339657887816429, acc: 0.9852941036224365)
[2025-02-13 20:37:23,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:24,184][root][INFO] - Training Epoch: 2/2, step 2157/7134 completed (loss: 0.1260741651058197, acc: 0.9646464586257935)
[2025-02-13 20:37:24,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:24,552][root][INFO] - Training Epoch: 2/2, step 2158/7134 completed (loss: 0.10442465543746948, acc: 0.9845361113548279)
[2025-02-13 20:37:24,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:24,943][root][INFO] - Training Epoch: 2/2, step 2159/7134 completed (loss: 0.050853751599788666, acc: 0.9821428656578064)
[2025-02-13 20:37:25,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:25,316][root][INFO] - Training Epoch: 2/2, step 2160/7134 completed (loss: 0.07646549493074417, acc: 0.9695122241973877)
[2025-02-13 20:37:25,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:25,696][root][INFO] - Training Epoch: 2/2, step 2161/7134 completed (loss: 0.13508526980876923, acc: 0.9555555582046509)
[2025-02-13 20:37:25,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:26,066][root][INFO] - Training Epoch: 2/2, step 2162/7134 completed (loss: 0.05558890849351883, acc: 0.9870967864990234)
[2025-02-13 20:37:26,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:26,446][root][INFO] - Training Epoch: 2/2, step 2163/7134 completed (loss: 0.07578238099813461, acc: 0.9866666793823242)
[2025-02-13 20:37:26,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:26,808][root][INFO] - Training Epoch: 2/2, step 2164/7134 completed (loss: 0.017081378027796745, acc: 1.0)
[2025-02-13 20:37:26,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:27,206][root][INFO] - Training Epoch: 2/2, step 2165/7134 completed (loss: 0.08138323575258255, acc: 0.9878048896789551)
[2025-02-13 20:37:27,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:27,585][root][INFO] - Training Epoch: 2/2, step 2166/7134 completed (loss: 0.024819042533636093, acc: 0.994350254535675)
[2025-02-13 20:37:27,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:27,969][root][INFO] - Training Epoch: 2/2, step 2167/7134 completed (loss: 0.13412991166114807, acc: 0.9708737730979919)
[2025-02-13 20:37:28,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:28,311][root][INFO] - Training Epoch: 2/2, step 2168/7134 completed (loss: 0.038991738110780716, acc: 0.9928057789802551)
[2025-02-13 20:37:28,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:28,696][root][INFO] - Training Epoch: 2/2, step 2169/7134 completed (loss: 0.10867350548505783, acc: 0.9748743772506714)
[2025-02-13 20:37:28,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:29,066][root][INFO] - Training Epoch: 2/2, step 2170/7134 completed (loss: 0.04830757528543472, acc: 0.9803921580314636)
[2025-02-13 20:37:29,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:29,454][root][INFO] - Training Epoch: 2/2, step 2171/7134 completed (loss: 0.19414059817790985, acc: 0.9621621370315552)
[2025-02-13 20:37:29,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:29,850][root][INFO] - Training Epoch: 2/2, step 2172/7134 completed (loss: 0.04035266488790512, acc: 0.9952606558799744)
[2025-02-13 20:37:29,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:30,220][root][INFO] - Training Epoch: 2/2, step 2173/7134 completed (loss: 0.09142985194921494, acc: 0.9863013625144958)
[2025-02-13 20:37:30,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:30,634][root][INFO] - Training Epoch: 2/2, step 2174/7134 completed (loss: 0.0568355917930603, acc: 0.9880239367485046)
[2025-02-13 20:37:30,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:31,007][root][INFO] - Training Epoch: 2/2, step 2175/7134 completed (loss: 0.06542415171861649, acc: 0.9818181991577148)
[2025-02-13 20:37:31,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:31,406][root][INFO] - Training Epoch: 2/2, step 2176/7134 completed (loss: 0.05020929500460625, acc: 0.9836956262588501)
[2025-02-13 20:37:31,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:31,823][root][INFO] - Training Epoch: 2/2, step 2177/7134 completed (loss: 0.08833152800798416, acc: 0.9781420826911926)
[2025-02-13 20:37:31,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:32,192][root][INFO] - Training Epoch: 2/2, step 2178/7134 completed (loss: 0.047468822449445724, acc: 0.984375)
[2025-02-13 20:37:32,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:32,607][root][INFO] - Training Epoch: 2/2, step 2179/7134 completed (loss: 0.06420358270406723, acc: 0.9885057210922241)
[2025-02-13 20:37:32,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:33,011][root][INFO] - Training Epoch: 2/2, step 2180/7134 completed (loss: 0.11278069764375687, acc: 0.9780219793319702)
[2025-02-13 20:37:33,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:33,412][root][INFO] - Training Epoch: 2/2, step 2181/7134 completed (loss: 0.050702936947345734, acc: 0.9875776171684265)
[2025-02-13 20:37:33,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:33,807][root][INFO] - Training Epoch: 2/2, step 2182/7134 completed (loss: 0.11993368715047836, acc: 0.9743589758872986)
[2025-02-13 20:37:33,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:34,195][root][INFO] - Training Epoch: 2/2, step 2183/7134 completed (loss: 0.07736450433731079, acc: 0.988950252532959)
[2025-02-13 20:37:34,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:34,594][root][INFO] - Training Epoch: 2/2, step 2184/7134 completed (loss: 0.04669904336333275, acc: 0.98591548204422)
[2025-02-13 20:37:34,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:34,993][root][INFO] - Training Epoch: 2/2, step 2185/7134 completed (loss: 0.13672447204589844, acc: 0.955974817276001)
[2025-02-13 20:37:35,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:35,364][root][INFO] - Training Epoch: 2/2, step 2186/7134 completed (loss: 0.20192456245422363, acc: 0.9599999785423279)
[2025-02-13 20:37:35,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:35,752][root][INFO] - Training Epoch: 2/2, step 2187/7134 completed (loss: 0.14665718376636505, acc: 0.9496402740478516)
[2025-02-13 20:37:35,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:36,156][root][INFO] - Training Epoch: 2/2, step 2188/7134 completed (loss: 0.03752652555704117, acc: 0.9939393997192383)
[2025-02-13 20:37:36,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:36,546][root][INFO] - Training Epoch: 2/2, step 2189/7134 completed (loss: 0.08688965439796448, acc: 0.9870129823684692)
[2025-02-13 20:37:36,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:36,942][root][INFO] - Training Epoch: 2/2, step 2190/7134 completed (loss: 0.1351102739572525, acc: 0.9664804339408875)
[2025-02-13 20:37:37,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:37,289][root][INFO] - Training Epoch: 2/2, step 2191/7134 completed (loss: 0.08268409222364426, acc: 0.9578947424888611)
[2025-02-13 20:37:37,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:37,661][root][INFO] - Training Epoch: 2/2, step 2192/7134 completed (loss: 0.03143773227930069, acc: 0.9925925731658936)
[2025-02-13 20:37:37,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:38,035][root][INFO] - Training Epoch: 2/2, step 2193/7134 completed (loss: 0.1055726557970047, acc: 0.9781420826911926)
[2025-02-13 20:37:38,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:38,432][root][INFO] - Training Epoch: 2/2, step 2194/7134 completed (loss: 0.08305700123310089, acc: 0.9671052694320679)
[2025-02-13 20:37:38,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:38,832][root][INFO] - Training Epoch: 2/2, step 2195/7134 completed (loss: 0.0182347409427166, acc: 1.0)
[2025-02-13 20:37:38,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:39,202][root][INFO] - Training Epoch: 2/2, step 2196/7134 completed (loss: 0.06188216060400009, acc: 0.9780219793319702)
[2025-02-13 20:37:39,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:39,551][root][INFO] - Training Epoch: 2/2, step 2197/7134 completed (loss: 0.10397985577583313, acc: 0.960629940032959)
[2025-02-13 20:37:39,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:39,937][root][INFO] - Training Epoch: 2/2, step 2198/7134 completed (loss: 0.039322059601545334, acc: 1.0)
[2025-02-13 20:37:40,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:40,323][root][INFO] - Training Epoch: 2/2, step 2199/7134 completed (loss: 0.012952371500432491, acc: 1.0)
[2025-02-13 20:37:40,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:40,718][root][INFO] - Training Epoch: 2/2, step 2200/7134 completed (loss: 0.029678313061594963, acc: 0.9921259880065918)
[2025-02-13 20:37:40,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:41,110][root][INFO] - Training Epoch: 2/2, step 2201/7134 completed (loss: 0.027753887698054314, acc: 0.9932432174682617)
[2025-02-13 20:37:41,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:41,477][root][INFO] - Training Epoch: 2/2, step 2202/7134 completed (loss: 0.01851562410593033, acc: 1.0)
[2025-02-13 20:37:41,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:41,861][root][INFO] - Training Epoch: 2/2, step 2203/7134 completed (loss: 0.024124875664711, acc: 0.9940119981765747)
[2025-02-13 20:37:42,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:42,251][root][INFO] - Training Epoch: 2/2, step 2204/7134 completed (loss: 0.06656447798013687, acc: 0.982758641242981)
[2025-02-13 20:37:42,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:42,634][root][INFO] - Training Epoch: 2/2, step 2205/7134 completed (loss: 0.016145838424563408, acc: 1.0)
[2025-02-13 20:37:42,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:42,999][root][INFO] - Training Epoch: 2/2, step 2206/7134 completed (loss: 0.08862751722335815, acc: 0.9865771532058716)
[2025-02-13 20:37:43,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:43,364][root][INFO] - Training Epoch: 2/2, step 2207/7134 completed (loss: 0.022899437695741653, acc: 0.9931972622871399)
[2025-02-13 20:37:43,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:43,721][root][INFO] - Training Epoch: 2/2, step 2208/7134 completed (loss: 0.0510161817073822, acc: 0.9935483932495117)
[2025-02-13 20:37:43,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:44,081][root][INFO] - Training Epoch: 2/2, step 2209/7134 completed (loss: 0.03290652856230736, acc: 0.9939024448394775)
[2025-02-13 20:37:44,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:44,444][root][INFO] - Training Epoch: 2/2, step 2210/7134 completed (loss: 0.04650656506419182, acc: 0.9817073345184326)
[2025-02-13 20:37:44,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:44,818][root][INFO] - Training Epoch: 2/2, step 2211/7134 completed (loss: 0.03810344636440277, acc: 0.9939758777618408)
[2025-02-13 20:37:44,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:45,168][root][INFO] - Training Epoch: 2/2, step 2212/7134 completed (loss: 0.02011147513985634, acc: 0.9942857027053833)
[2025-02-13 20:37:45,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:45,534][root][INFO] - Training Epoch: 2/2, step 2213/7134 completed (loss: 0.13087016344070435, acc: 0.9647058844566345)
[2025-02-13 20:37:45,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:45,889][root][INFO] - Training Epoch: 2/2, step 2214/7134 completed (loss: 0.0646766647696495, acc: 0.9864864945411682)
[2025-02-13 20:37:46,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:46,248][root][INFO] - Training Epoch: 2/2, step 2215/7134 completed (loss: 0.022238338366150856, acc: 1.0)
[2025-02-13 20:37:46,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:46,609][root][INFO] - Training Epoch: 2/2, step 2216/7134 completed (loss: 0.01768137887120247, acc: 1.0)
[2025-02-13 20:37:46,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:46,973][root][INFO] - Training Epoch: 2/2, step 2217/7134 completed (loss: 0.044924862682819366, acc: 0.9880239367485046)
[2025-02-13 20:37:47,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:47,330][root][INFO] - Training Epoch: 2/2, step 2218/7134 completed (loss: 0.07259902358055115, acc: 0.9753086566925049)
[2025-02-13 20:37:47,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:47,695][root][INFO] - Training Epoch: 2/2, step 2219/7134 completed (loss: 0.04061251878738403, acc: 0.9928571581840515)
[2025-02-13 20:37:47,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:48,059][root][INFO] - Training Epoch: 2/2, step 2220/7134 completed (loss: 0.03633951395750046, acc: 0.9851852059364319)
[2025-02-13 20:37:48,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:48,428][root][INFO] - Training Epoch: 2/2, step 2221/7134 completed (loss: 0.06063321605324745, acc: 0.9813664555549622)
[2025-02-13 20:37:48,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:48,777][root][INFO] - Training Epoch: 2/2, step 2222/7134 completed (loss: 0.06772693991661072, acc: 0.9726027250289917)
[2025-02-13 20:37:48,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:49,136][root][INFO] - Training Epoch: 2/2, step 2223/7134 completed (loss: 0.06175609678030014, acc: 0.9865771532058716)
[2025-02-13 20:37:49,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:49,499][root][INFO] - Training Epoch: 2/2, step 2224/7134 completed (loss: 0.14130869507789612, acc: 0.9746835231781006)
[2025-02-13 20:37:49,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:49,868][root][INFO] - Training Epoch: 2/2, step 2225/7134 completed (loss: 0.04917251318693161, acc: 0.9931972622871399)
[2025-02-13 20:37:50,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:50,234][root][INFO] - Training Epoch: 2/2, step 2226/7134 completed (loss: 0.034537170082330704, acc: 0.9932885766029358)
[2025-02-13 20:37:50,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:50,592][root][INFO] - Training Epoch: 2/2, step 2227/7134 completed (loss: 0.02805495820939541, acc: 0.9935064911842346)
[2025-02-13 20:37:50,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:50,935][root][INFO] - Training Epoch: 2/2, step 2228/7134 completed (loss: 0.046489227563142776, acc: 0.9922480583190918)
[2025-02-13 20:37:51,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:51,284][root][INFO] - Training Epoch: 2/2, step 2229/7134 completed (loss: 0.09015820920467377, acc: 0.9902912378311157)
[2025-02-13 20:37:51,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:51,650][root][INFO] - Training Epoch: 2/2, step 2230/7134 completed (loss: 0.015382139012217522, acc: 1.0)
[2025-02-13 20:37:51,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:52,023][root][INFO] - Training Epoch: 2/2, step 2231/7134 completed (loss: 0.07826364040374756, acc: 0.9915966391563416)
[2025-02-13 20:37:52,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:52,393][root][INFO] - Training Epoch: 2/2, step 2232/7134 completed (loss: 0.07698355615139008, acc: 0.977011501789093)
[2025-02-13 20:37:52,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:52,756][root][INFO] - Training Epoch: 2/2, step 2233/7134 completed (loss: 0.06989644467830658, acc: 0.9807692170143127)
[2025-02-13 20:37:52,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:53,169][root][INFO] - Training Epoch: 2/2, step 2234/7134 completed (loss: 0.0237300805747509, acc: 0.9935064911842346)
[2025-02-13 20:37:53,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:53,543][root][INFO] - Training Epoch: 2/2, step 2235/7134 completed (loss: 0.12331023812294006, acc: 0.9720279574394226)
[2025-02-13 20:37:53,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:53,904][root][INFO] - Training Epoch: 2/2, step 2236/7134 completed (loss: 0.054563358426094055, acc: 0.9866666793823242)
[2025-02-13 20:37:54,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:54,279][root][INFO] - Training Epoch: 2/2, step 2237/7134 completed (loss: 0.06661107391119003, acc: 0.9800000190734863)
[2025-02-13 20:37:54,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:54,646][root][INFO] - Training Epoch: 2/2, step 2238/7134 completed (loss: 0.09019073843955994, acc: 0.9814814925193787)
[2025-02-13 20:37:54,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:55,027][root][INFO] - Training Epoch: 2/2, step 2239/7134 completed (loss: 0.24229532480239868, acc: 0.9465240836143494)
[2025-02-13 20:37:55,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:55,418][root][INFO] - Training Epoch: 2/2, step 2240/7134 completed (loss: 0.08830254524946213, acc: 0.9772727489471436)
[2025-02-13 20:37:55,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:55,779][root][INFO] - Training Epoch: 2/2, step 2241/7134 completed (loss: 0.09787320345640182, acc: 0.9583333134651184)
[2025-02-13 20:37:55,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:56,137][root][INFO] - Training Epoch: 2/2, step 2242/7134 completed (loss: 0.1797209084033966, acc: 0.9444444179534912)
[2025-02-13 20:37:56,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:56,536][root][INFO] - Training Epoch: 2/2, step 2243/7134 completed (loss: 0.2361164391040802, acc: 0.9354838728904724)
[2025-02-13 20:37:56,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:56,907][root][INFO] - Training Epoch: 2/2, step 2244/7134 completed (loss: 0.14021442830562592, acc: 0.9589040875434875)
[2025-02-13 20:37:57,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:57,261][root][INFO] - Training Epoch: 2/2, step 2245/7134 completed (loss: 0.18920333683490753, acc: 0.9542483687400818)
[2025-02-13 20:37:57,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:57,617][root][INFO] - Training Epoch: 2/2, step 2246/7134 completed (loss: 0.43104541301727295, acc: 0.868852436542511)
[2025-02-13 20:37:57,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:57,982][root][INFO] - Training Epoch: 2/2, step 2247/7134 completed (loss: 0.24385106563568115, acc: 0.9421965479850769)
[2025-02-13 20:37:58,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:58,352][root][INFO] - Training Epoch: 2/2, step 2248/7134 completed (loss: 0.3089987337589264, acc: 0.9248554706573486)
[2025-02-13 20:37:58,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:58,716][root][INFO] - Training Epoch: 2/2, step 2249/7134 completed (loss: 0.17064343392848969, acc: 0.9613259434700012)
[2025-02-13 20:37:58,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:59,059][root][INFO] - Training Epoch: 2/2, step 2250/7134 completed (loss: 0.18294447660446167, acc: 0.9521276354789734)
[2025-02-13 20:37:59,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:59,412][root][INFO] - Training Epoch: 2/2, step 2251/7134 completed (loss: 0.09216949343681335, acc: 0.9754098653793335)
[2025-02-13 20:37:59,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:59,764][root][INFO] - Training Epoch: 2/2, step 2252/7134 completed (loss: 0.05677160993218422, acc: 0.989130437374115)
[2025-02-13 20:37:59,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:00,075][root][INFO] - Training Epoch: 2/2, step 2253/7134 completed (loss: 0.08197545260190964, acc: 0.9808917045593262)
[2025-02-13 20:38:00,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:00,424][root][INFO] - Training Epoch: 2/2, step 2254/7134 completed (loss: 0.18849502503871918, acc: 0.9596773982048035)
[2025-02-13 20:38:00,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:00,792][root][INFO] - Training Epoch: 2/2, step 2255/7134 completed (loss: 0.07626418024301529, acc: 0.9748427867889404)
[2025-02-13 20:38:00,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:01,101][root][INFO] - Training Epoch: 2/2, step 2256/7134 completed (loss: 0.08226010203361511, acc: 0.9837398529052734)
[2025-02-13 20:38:01,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:01,466][root][INFO] - Training Epoch: 2/2, step 2257/7134 completed (loss: 0.09040437638759613, acc: 0.9814814925193787)
[2025-02-13 20:38:01,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:01,822][root][INFO] - Training Epoch: 2/2, step 2258/7134 completed (loss: 0.07674159854650497, acc: 0.9922480583190918)
[2025-02-13 20:38:01,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:02,171][root][INFO] - Training Epoch: 2/2, step 2259/7134 completed (loss: 0.12038297206163406, acc: 0.9829059839248657)
[2025-02-13 20:38:02,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:02,519][root][INFO] - Training Epoch: 2/2, step 2260/7134 completed (loss: 0.22896058857440948, acc: 0.9155844449996948)
[2025-02-13 20:38:02,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:02,872][root][INFO] - Training Epoch: 2/2, step 2261/7134 completed (loss: 0.21691690385341644, acc: 0.9444444179534912)
[2025-02-13 20:38:02,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:03,225][root][INFO] - Training Epoch: 2/2, step 2262/7134 completed (loss: 0.07155880331993103, acc: 0.9849624037742615)
[2025-02-13 20:38:03,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:03,602][root][INFO] - Training Epoch: 2/2, step 2263/7134 completed (loss: 0.20774264633655548, acc: 0.9548872113227844)
[2025-02-13 20:38:03,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:03,965][root][INFO] - Training Epoch: 2/2, step 2264/7134 completed (loss: 0.2919726073741913, acc: 0.9351351261138916)
[2025-02-13 20:38:04,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:04,316][root][INFO] - Training Epoch: 2/2, step 2265/7134 completed (loss: 0.2284747213125229, acc: 0.9545454382896423)
[2025-02-13 20:38:04,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:04,690][root][INFO] - Training Epoch: 2/2, step 2266/7134 completed (loss: 0.20065562427043915, acc: 0.939226508140564)
[2025-02-13 20:38:04,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:05,057][root][INFO] - Training Epoch: 2/2, step 2267/7134 completed (loss: 0.0753902792930603, acc: 0.9674796462059021)
[2025-02-13 20:38:05,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:05,430][root][INFO] - Training Epoch: 2/2, step 2268/7134 completed (loss: 0.0878402441740036, acc: 0.9863013625144958)
[2025-02-13 20:38:05,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:05,845][root][INFO] - Training Epoch: 2/2, step 2269/7134 completed (loss: 0.04451720416545868, acc: 0.994413435459137)
[2025-02-13 20:38:05,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:06,232][root][INFO] - Training Epoch: 2/2, step 2270/7134 completed (loss: 0.18436388671398163, acc: 0.96875)
[2025-02-13 20:38:06,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:06,610][root][INFO] - Training Epoch: 2/2, step 2271/7134 completed (loss: 0.20586319267749786, acc: 0.9459459185600281)
[2025-02-13 20:38:06,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:06,979][root][INFO] - Training Epoch: 2/2, step 2272/7134 completed (loss: 0.09228174388408661, acc: 0.9623655676841736)
[2025-02-13 20:38:07,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:07,334][root][INFO] - Training Epoch: 2/2, step 2273/7134 completed (loss: 0.05583509802818298, acc: 0.9888888597488403)
[2025-02-13 20:38:07,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:07,728][root][INFO] - Training Epoch: 2/2, step 2274/7134 completed (loss: 0.45723283290863037, acc: 0.9230769276618958)
[2025-02-13 20:38:07,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:08,129][root][INFO] - Training Epoch: 2/2, step 2275/7134 completed (loss: 0.07377313077449799, acc: 0.9950980544090271)
[2025-02-13 20:38:08,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:08,505][root][INFO] - Training Epoch: 2/2, step 2276/7134 completed (loss: 0.15400519967079163, acc: 0.9788359999656677)
[2025-02-13 20:38:08,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:08,873][root][INFO] - Training Epoch: 2/2, step 2277/7134 completed (loss: 0.08314258605241776, acc: 0.9897435903549194)
[2025-02-13 20:38:09,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:09,246][root][INFO] - Training Epoch: 2/2, step 2278/7134 completed (loss: 0.3050205409526825, acc: 0.9354838728904724)
[2025-02-13 20:38:09,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:09,609][root][INFO] - Training Epoch: 2/2, step 2279/7134 completed (loss: 0.08779407292604446, acc: 0.9796954393386841)
[2025-02-13 20:38:09,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:10,002][root][INFO] - Training Epoch: 2/2, step 2280/7134 completed (loss: 0.16669628024101257, acc: 0.9587156176567078)
[2025-02-13 20:38:10,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:10,374][root][INFO] - Training Epoch: 2/2, step 2281/7134 completed (loss: 0.3304857015609741, acc: 0.936170220375061)
[2025-02-13 20:38:10,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:10,747][root][INFO] - Training Epoch: 2/2, step 2282/7134 completed (loss: 0.02691163681447506, acc: 0.9947368502616882)
[2025-02-13 20:38:10,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:11,106][root][INFO] - Training Epoch: 2/2, step 2283/7134 completed (loss: 0.06749889254570007, acc: 0.9807692170143127)
[2025-02-13 20:38:11,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:11,466][root][INFO] - Training Epoch: 2/2, step 2284/7134 completed (loss: 0.046108394861221313, acc: 0.9870967864990234)
[2025-02-13 20:38:11,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:11,847][root][INFO] - Training Epoch: 2/2, step 2285/7134 completed (loss: 0.0514225997030735, acc: 0.9900000095367432)
[2025-02-13 20:38:11,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:12,212][root][INFO] - Training Epoch: 2/2, step 2286/7134 completed (loss: 0.1286020427942276, acc: 0.9655172228813171)
[2025-02-13 20:38:12,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:12,587][root][INFO] - Training Epoch: 2/2, step 2287/7134 completed (loss: 0.10280732810497284, acc: 0.9693251252174377)
[2025-02-13 20:38:12,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:12,951][root][INFO] - Training Epoch: 2/2, step 2288/7134 completed (loss: 0.015590700320899487, acc: 1.0)
[2025-02-13 20:38:13,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:13,308][root][INFO] - Training Epoch: 2/2, step 2289/7134 completed (loss: 0.10101348906755447, acc: 0.9679144620895386)
[2025-02-13 20:38:13,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:13,660][root][INFO] - Training Epoch: 2/2, step 2290/7134 completed (loss: 0.06040160357952118, acc: 0.9838709831237793)
[2025-02-13 20:38:13,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:14,020][root][INFO] - Training Epoch: 2/2, step 2291/7134 completed (loss: 0.039638932794332504, acc: 1.0)
[2025-02-13 20:38:14,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:14,384][root][INFO] - Training Epoch: 2/2, step 2292/7134 completed (loss: 0.10679829120635986, acc: 0.9797979593276978)
[2025-02-13 20:38:14,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:14,757][root][INFO] - Training Epoch: 2/2, step 2293/7134 completed (loss: 0.07796543836593628, acc: 0.9817351698875427)
[2025-02-13 20:38:14,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:15,116][root][INFO] - Training Epoch: 2/2, step 2294/7134 completed (loss: 0.06330154836177826, acc: 0.9856459498405457)
[2025-02-13 20:38:15,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:15,499][root][INFO] - Training Epoch: 2/2, step 2295/7134 completed (loss: 0.12308892607688904, acc: 0.95703125)
[2025-02-13 20:38:15,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:15,879][root][INFO] - Training Epoch: 2/2, step 2296/7134 completed (loss: 0.05238091200590134, acc: 0.9807692170143127)
[2025-02-13 20:38:16,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:16,246][root][INFO] - Training Epoch: 2/2, step 2297/7134 completed (loss: 0.02938300557434559, acc: 1.0)
[2025-02-13 20:38:16,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:16,608][root][INFO] - Training Epoch: 2/2, step 2298/7134 completed (loss: 0.06573515385389328, acc: 0.9809523820877075)
[2025-02-13 20:38:16,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:16,972][root][INFO] - Training Epoch: 2/2, step 2299/7134 completed (loss: 0.10602100938558578, acc: 0.9784946441650391)
[2025-02-13 20:38:17,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:17,333][root][INFO] - Training Epoch: 2/2, step 2300/7134 completed (loss: 0.09346672147512436, acc: 0.9747474789619446)
[2025-02-13 20:38:17,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:17,725][root][INFO] - Training Epoch: 2/2, step 2301/7134 completed (loss: 0.09965589642524719, acc: 0.967391312122345)
[2025-02-13 20:38:17,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:18,094][root][INFO] - Training Epoch: 2/2, step 2302/7134 completed (loss: 0.09911448508501053, acc: 0.9692307710647583)
[2025-02-13 20:38:18,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:18,445][root][INFO] - Training Epoch: 2/2, step 2303/7134 completed (loss: 0.13018272817134857, acc: 0.981566846370697)
[2025-02-13 20:38:18,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:18,813][root][INFO] - Training Epoch: 2/2, step 2304/7134 completed (loss: 0.14145290851593018, acc: 0.9471365809440613)
[2025-02-13 20:38:18,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:19,185][root][INFO] - Training Epoch: 2/2, step 2305/7134 completed (loss: 0.10523355007171631, acc: 0.9716981053352356)
[2025-02-13 20:38:19,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:19,555][root][INFO] - Training Epoch: 2/2, step 2306/7134 completed (loss: 0.05214553326368332, acc: 0.9860464930534363)
[2025-02-13 20:38:19,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:19,920][root][INFO] - Training Epoch: 2/2, step 2307/7134 completed (loss: 0.04802856221795082, acc: 0.991525411605835)
[2025-02-13 20:38:20,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:20,290][root][INFO] - Training Epoch: 2/2, step 2308/7134 completed (loss: 0.06391452252864838, acc: 0.9867841601371765)
[2025-02-13 20:38:20,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:20,643][root][INFO] - Training Epoch: 2/2, step 2309/7134 completed (loss: 0.04711447283625603, acc: 0.9841269850730896)
[2025-02-13 20:38:20,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21,007][root][INFO] - Training Epoch: 2/2, step 2310/7134 completed (loss: 0.07371716946363449, acc: 0.9857142567634583)
[2025-02-13 20:38:21,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21,359][root][INFO] - Training Epoch: 2/2, step 2311/7134 completed (loss: 0.05797090753912926, acc: 0.9802955389022827)
[2025-02-13 20:38:21,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21,739][root][INFO] - Training Epoch: 2/2, step 2312/7134 completed (loss: 0.04870228469371796, acc: 0.9953051805496216)
[2025-02-13 20:38:21,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:22,110][root][INFO] - Training Epoch: 2/2, step 2313/7134 completed (loss: 0.09035709500312805, acc: 0.9807692170143127)
[2025-02-13 20:38:22,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:22,492][root][INFO] - Training Epoch: 2/2, step 2314/7134 completed (loss: 0.058340493589639664, acc: 0.9770641922950745)
[2025-02-13 20:38:22,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:22,871][root][INFO] - Training Epoch: 2/2, step 2315/7134 completed (loss: 0.08958154171705246, acc: 0.9627906680107117)
[2025-02-13 20:38:23,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:23,233][root][INFO] - Training Epoch: 2/2, step 2316/7134 completed (loss: 0.023914648219943047, acc: 1.0)
[2025-02-13 20:38:23,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:23,617][root][INFO] - Training Epoch: 2/2, step 2317/7134 completed (loss: 0.11949370801448822, acc: 0.9621848464012146)
[2025-02-13 20:38:23,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:23,995][root][INFO] - Training Epoch: 2/2, step 2318/7134 completed (loss: 0.029862847179174423, acc: 0.9956331849098206)
[2025-02-13 20:38:24,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:24,362][root][INFO] - Training Epoch: 2/2, step 2319/7134 completed (loss: 0.026462778449058533, acc: 0.9954954981803894)
[2025-02-13 20:38:24,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:24,741][root][INFO] - Training Epoch: 2/2, step 2320/7134 completed (loss: 0.028796490281820297, acc: 0.9948717951774597)
[2025-02-13 20:38:24,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:25,114][root][INFO] - Training Epoch: 2/2, step 2321/7134 completed (loss: 0.06520278006792068, acc: 0.9882352948188782)
[2025-02-13 20:38:25,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:25,493][root][INFO] - Training Epoch: 2/2, step 2322/7134 completed (loss: 0.06832096725702286, acc: 0.9693251252174377)
[2025-02-13 20:38:25,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:25,865][root][INFO] - Training Epoch: 2/2, step 2323/7134 completed (loss: 0.05274941027164459, acc: 0.987261176109314)
[2025-02-13 20:38:26,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:26,238][root][INFO] - Training Epoch: 2/2, step 2324/7134 completed (loss: 0.2337590456008911, acc: 0.9759036302566528)
[2025-02-13 20:38:26,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:26,630][root][INFO] - Training Epoch: 2/2, step 2325/7134 completed (loss: 0.08841542154550552, acc: 0.9714285731315613)
[2025-02-13 20:38:26,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:26,988][root][INFO] - Training Epoch: 2/2, step 2326/7134 completed (loss: 0.043059926480054855, acc: 0.9923076629638672)
[2025-02-13 20:38:27,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:27,349][root][INFO] - Training Epoch: 2/2, step 2327/7134 completed (loss: 0.04877838119864464, acc: 0.9798657894134521)
[2025-02-13 20:38:27,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:27,737][root][INFO] - Training Epoch: 2/2, step 2328/7134 completed (loss: 0.10350597649812698, acc: 0.9865771532058716)
[2025-02-13 20:38:27,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:28,114][root][INFO] - Training Epoch: 2/2, step 2329/7134 completed (loss: 0.08452337235212326, acc: 0.9803921580314636)
[2025-02-13 20:38:28,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:28,472][root][INFO] - Training Epoch: 2/2, step 2330/7134 completed (loss: 0.013184154406189919, acc: 1.0)
[2025-02-13 20:38:28,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:28,839][root][INFO] - Training Epoch: 2/2, step 2331/7134 completed (loss: 0.029598534107208252, acc: 0.9870967864990234)
[2025-02-13 20:38:28,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:29,214][root][INFO] - Training Epoch: 2/2, step 2332/7134 completed (loss: 0.041150614619255066, acc: 0.9939393997192383)
[2025-02-13 20:38:29,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:29,581][root][INFO] - Training Epoch: 2/2, step 2333/7134 completed (loss: 0.08354304730892181, acc: 0.9817073345184326)
[2025-02-13 20:38:29,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:29,950][root][INFO] - Training Epoch: 2/2, step 2334/7134 completed (loss: 0.15881487727165222, acc: 0.9681528806686401)
[2025-02-13 20:38:30,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:30,328][root][INFO] - Training Epoch: 2/2, step 2335/7134 completed (loss: 0.0322946161031723, acc: 0.9873417615890503)
[2025-02-13 20:38:30,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:30,692][root][INFO] - Training Epoch: 2/2, step 2336/7134 completed (loss: 0.020619375631213188, acc: 1.0)
[2025-02-13 20:38:30,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:31,076][root][INFO] - Training Epoch: 2/2, step 2337/7134 completed (loss: 0.023626793175935745, acc: 0.9930555820465088)
[2025-02-13 20:38:31,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:31,453][root][INFO] - Training Epoch: 2/2, step 2338/7134 completed (loss: 0.03287607803940773, acc: 1.0)
[2025-02-13 20:38:31,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:31,819][root][INFO] - Training Epoch: 2/2, step 2339/7134 completed (loss: 0.12290889769792557, acc: 0.9615384340286255)
[2025-02-13 20:38:31,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:32,187][root][INFO] - Training Epoch: 2/2, step 2340/7134 completed (loss: 0.016161417588591576, acc: 1.0)
[2025-02-13 20:38:32,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:32,565][root][INFO] - Training Epoch: 2/2, step 2341/7134 completed (loss: 0.022710751742124557, acc: 1.0)
[2025-02-13 20:38:32,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:32,952][root][INFO] - Training Epoch: 2/2, step 2342/7134 completed (loss: 0.024975482374429703, acc: 0.988095223903656)
[2025-02-13 20:38:33,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:33,321][root][INFO] - Training Epoch: 2/2, step 2343/7134 completed (loss: 0.05146780610084534, acc: 0.987500011920929)
[2025-02-13 20:38:33,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:33,689][root][INFO] - Training Epoch: 2/2, step 2344/7134 completed (loss: 0.050090543925762177, acc: 0.9876543283462524)
[2025-02-13 20:38:33,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:34,030][root][INFO] - Training Epoch: 2/2, step 2345/7134 completed (loss: 0.051089249551296234, acc: 0.982758641242981)
[2025-02-13 20:38:34,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:34,420][root][INFO] - Training Epoch: 2/2, step 2346/7134 completed (loss: 0.05134579539299011, acc: 0.988950252532959)
[2025-02-13 20:38:34,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:34,813][root][INFO] - Training Epoch: 2/2, step 2347/7134 completed (loss: 0.11209019273519516, acc: 0.98591548204422)
[2025-02-13 20:38:34,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:35,222][root][INFO] - Training Epoch: 2/2, step 2348/7134 completed (loss: 0.2469230592250824, acc: 0.938144326210022)
[2025-02-13 20:38:35,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:35,615][root][INFO] - Training Epoch: 2/2, step 2349/7134 completed (loss: 0.14138631522655487, acc: 0.9624999761581421)
[2025-02-13 20:38:35,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:36,025][root][INFO] - Training Epoch: 2/2, step 2350/7134 completed (loss: 0.20235353708267212, acc: 0.9430379867553711)
[2025-02-13 20:38:36,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:36,424][root][INFO] - Training Epoch: 2/2, step 2351/7134 completed (loss: 0.10477498918771744, acc: 0.9726775884628296)
[2025-02-13 20:38:36,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:36,810][root][INFO] - Training Epoch: 2/2, step 2352/7134 completed (loss: 0.16200324892997742, acc: 0.948387086391449)
[2025-02-13 20:38:36,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:37,235][root][INFO] - Training Epoch: 2/2, step 2353/7134 completed (loss: 0.1730400025844574, acc: 0.9612902998924255)
[2025-02-13 20:38:37,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:37,616][root][INFO] - Training Epoch: 2/2, step 2354/7134 completed (loss: 0.061363089829683304, acc: 0.9726775884628296)
[2025-02-13 20:38:37,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:37,993][root][INFO] - Training Epoch: 2/2, step 2355/7134 completed (loss: 0.13507068157196045, acc: 0.9723756909370422)
[2025-02-13 20:38:38,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:38,391][root][INFO] - Training Epoch: 2/2, step 2356/7134 completed (loss: 0.14510558545589447, acc: 0.97826087474823)
[2025-02-13 20:38:38,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:38,756][root][INFO] - Training Epoch: 2/2, step 2357/7134 completed (loss: 0.0944036915898323, acc: 0.976190447807312)
[2025-02-13 20:38:38,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:39,147][root][INFO] - Training Epoch: 2/2, step 2358/7134 completed (loss: 0.15122711658477783, acc: 0.9591836929321289)
[2025-02-13 20:38:39,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:39,554][root][INFO] - Training Epoch: 2/2, step 2359/7134 completed (loss: 0.10645876824855804, acc: 0.987500011920929)
[2025-02-13 20:38:39,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:39,974][root][INFO] - Training Epoch: 2/2, step 2360/7134 completed (loss: 0.11971405148506165, acc: 0.9810126423835754)
[2025-02-13 20:38:40,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:40,353][root][INFO] - Training Epoch: 2/2, step 2361/7134 completed (loss: 0.09953277558088303, acc: 0.976190447807312)
[2025-02-13 20:38:40,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:40,707][root][INFO] - Training Epoch: 2/2, step 2362/7134 completed (loss: 0.14639706909656525, acc: 0.9459459185600281)
[2025-02-13 20:38:40,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:41,101][root][INFO] - Training Epoch: 2/2, step 2363/7134 completed (loss: 0.10552405565977097, acc: 0.9790576100349426)
[2025-02-13 20:38:41,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:41,461][root][INFO] - Training Epoch: 2/2, step 2364/7134 completed (loss: 0.06335719674825668, acc: 0.9869281053543091)
[2025-02-13 20:38:41,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:41,885][root][INFO] - Training Epoch: 2/2, step 2365/7134 completed (loss: 0.06339982151985168, acc: 0.9825581312179565)
[2025-02-13 20:38:42,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:42,276][root][INFO] - Training Epoch: 2/2, step 2366/7134 completed (loss: 0.059637024998664856, acc: 0.9897435903549194)
[2025-02-13 20:38:42,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:42,681][root][INFO] - Training Epoch: 2/2, step 2367/7134 completed (loss: 0.10033439099788666, acc: 0.9848484992980957)
[2025-02-13 20:38:42,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:43,064][root][INFO] - Training Epoch: 2/2, step 2368/7134 completed (loss: 0.11532223969697952, acc: 0.979899525642395)
[2025-02-13 20:38:43,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:43,439][root][INFO] - Training Epoch: 2/2, step 2369/7134 completed (loss: 0.09604640305042267, acc: 0.9858490824699402)
[2025-02-13 20:38:43,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:43,821][root][INFO] - Training Epoch: 2/2, step 2370/7134 completed (loss: 0.019317401573061943, acc: 1.0)
[2025-02-13 20:38:43,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:44,196][root][INFO] - Training Epoch: 2/2, step 2371/7134 completed (loss: 0.06894438713788986, acc: 0.9689440727233887)
[2025-02-13 20:38:44,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:44,568][root][INFO] - Training Epoch: 2/2, step 2372/7134 completed (loss: 0.05432838946580887, acc: 0.9900000095367432)
[2025-02-13 20:38:44,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:44,987][root][INFO] - Training Epoch: 2/2, step 2373/7134 completed (loss: 0.047389473766088486, acc: 0.984455943107605)
[2025-02-13 20:38:45,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:45,374][root][INFO] - Training Epoch: 2/2, step 2374/7134 completed (loss: 0.04626866802573204, acc: 0.9779005646705627)
[2025-02-13 20:38:45,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:45,749][root][INFO] - Training Epoch: 2/2, step 2375/7134 completed (loss: 0.11174428462982178, acc: 0.9696969985961914)
[2025-02-13 20:38:45,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:46,123][root][INFO] - Training Epoch: 2/2, step 2376/7134 completed (loss: 0.179476797580719, acc: 0.9673202633857727)
[2025-02-13 20:38:46,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:46,501][root][INFO] - Training Epoch: 2/2, step 2377/7134 completed (loss: 0.08068221062421799, acc: 0.9766355156898499)
[2025-02-13 20:38:46,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:46,866][root][INFO] - Training Epoch: 2/2, step 2378/7134 completed (loss: 0.13164380192756653, acc: 0.9752475023269653)
[2025-02-13 20:38:47,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:47,239][root][INFO] - Training Epoch: 2/2, step 2379/7134 completed (loss: 0.18859098851680756, acc: 0.9438775777816772)
[2025-02-13 20:38:47,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:47,608][root][INFO] - Training Epoch: 2/2, step 2380/7134 completed (loss: 0.1327069252729416, acc: 0.9836956262588501)
[2025-02-13 20:38:47,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:47,950][root][INFO] - Training Epoch: 2/2, step 2381/7134 completed (loss: 0.3246143162250519, acc: 0.9068322777748108)
[2025-02-13 20:38:48,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:48,317][root][INFO] - Training Epoch: 2/2, step 2382/7134 completed (loss: 0.11368589103221893, acc: 0.9691358208656311)
[2025-02-13 20:38:48,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:48,698][root][INFO] - Training Epoch: 2/2, step 2383/7134 completed (loss: 0.10611751675605774, acc: 0.9672130942344666)
[2025-02-13 20:38:48,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:49,101][root][INFO] - Training Epoch: 2/2, step 2384/7134 completed (loss: 0.21264871954917908, acc: 0.9655172228813171)
[2025-02-13 20:38:49,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:49,524][root][INFO] - Training Epoch: 2/2, step 2385/7134 completed (loss: 0.08875033259391785, acc: 0.9725274443626404)
[2025-02-13 20:38:49,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:49,897][root][INFO] - Training Epoch: 2/2, step 2386/7134 completed (loss: 0.10324030369520187, acc: 0.9735449552536011)
[2025-02-13 20:38:50,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:50,274][root][INFO] - Training Epoch: 2/2, step 2387/7134 completed (loss: 0.08898476511240005, acc: 0.9714285731315613)
[2025-02-13 20:38:50,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:50,644][root][INFO] - Training Epoch: 2/2, step 2388/7134 completed (loss: 0.09297850728034973, acc: 0.9759615659713745)
[2025-02-13 20:38:50,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:51,068][root][INFO] - Training Epoch: 2/2, step 2389/7134 completed (loss: 0.22228789329528809, acc: 0.9478672742843628)
[2025-02-13 20:38:51,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:51,439][root][INFO] - Training Epoch: 2/2, step 2390/7134 completed (loss: 0.2814774215221405, acc: 0.9194312691688538)
[2025-02-13 20:38:51,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:51,804][root][INFO] - Training Epoch: 2/2, step 2391/7134 completed (loss: 0.16585591435432434, acc: 0.9621621370315552)
[2025-02-13 20:38:51,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:52,182][root][INFO] - Training Epoch: 2/2, step 2392/7134 completed (loss: 0.15031222999095917, acc: 0.9617486596107483)
[2025-02-13 20:38:52,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:52,544][root][INFO] - Training Epoch: 2/2, step 2393/7134 completed (loss: 0.10145705193281174, acc: 0.9740932583808899)
[2025-02-13 20:38:52,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:52,928][root][INFO] - Training Epoch: 2/2, step 2394/7134 completed (loss: 0.11829216778278351, acc: 0.9846938848495483)
[2025-02-13 20:38:53,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:53,300][root][INFO] - Training Epoch: 2/2, step 2395/7134 completed (loss: 0.34681758284568787, acc: 0.9314285516738892)
[2025-02-13 20:38:53,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:53,706][root][INFO] - Training Epoch: 2/2, step 2396/7134 completed (loss: 0.3306187391281128, acc: 0.9141414165496826)
[2025-02-13 20:38:53,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:54,072][root][INFO] - Training Epoch: 2/2, step 2397/7134 completed (loss: 0.2154674082994461, acc: 0.9695122241973877)
[2025-02-13 20:38:54,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:54,433][root][INFO] - Training Epoch: 2/2, step 2398/7134 completed (loss: 0.07649923861026764, acc: 0.9783783555030823)
[2025-02-13 20:38:54,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:54,789][root][INFO] - Training Epoch: 2/2, step 2399/7134 completed (loss: 0.2592836618423462, acc: 0.9274611473083496)
[2025-02-13 20:38:54,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:55,173][root][INFO] - Training Epoch: 2/2, step 2400/7134 completed (loss: 0.28156572580337524, acc: 0.9246231317520142)
[2025-02-13 20:38:55,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:55,530][root][INFO] - Training Epoch: 2/2, step 2401/7134 completed (loss: 0.6225901246070862, acc: 0.8221153616905212)
[2025-02-13 20:38:55,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:55,914][root][INFO] - Training Epoch: 2/2, step 2402/7134 completed (loss: 0.25115540623664856, acc: 0.9082125425338745)
[2025-02-13 20:38:56,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:56,289][root][INFO] - Training Epoch: 2/2, step 2403/7134 completed (loss: 0.07565966993570328, acc: 0.9820359349250793)
[2025-02-13 20:38:56,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:56,638][root][INFO] - Training Epoch: 2/2, step 2404/7134 completed (loss: 0.08586685359477997, acc: 0.9802631735801697)
[2025-02-13 20:38:56,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:56,979][root][INFO] - Training Epoch: 2/2, step 2405/7134 completed (loss: 0.20555172860622406, acc: 0.9681528806686401)
[2025-02-13 20:38:57,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:57,341][root][INFO] - Training Epoch: 2/2, step 2406/7134 completed (loss: 0.027241230010986328, acc: 1.0)
[2025-02-13 20:38:57,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:57,710][root][INFO] - Training Epoch: 2/2, step 2407/7134 completed (loss: 0.2188822627067566, acc: 0.9607843160629272)
[2025-02-13 20:38:57,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:58,099][root][INFO] - Training Epoch: 2/2, step 2408/7134 completed (loss: 0.22098581492900848, acc: 0.950276255607605)
[2025-02-13 20:38:58,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:58,476][root][INFO] - Training Epoch: 2/2, step 2409/7134 completed (loss: 0.12886610627174377, acc: 0.9624999761581421)
[2025-02-13 20:38:58,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:58,836][root][INFO] - Training Epoch: 2/2, step 2410/7134 completed (loss: 0.053756795823574066, acc: 0.9929577708244324)
[2025-02-13 20:38:58,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:59,202][root][INFO] - Training Epoch: 2/2, step 2411/7134 completed (loss: 0.03660735860466957, acc: 0.988950252532959)
[2025-02-13 20:38:59,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:59,557][root][INFO] - Training Epoch: 2/2, step 2412/7134 completed (loss: 0.042633768171072006, acc: 0.9943181872367859)
[2025-02-13 20:38:59,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:59,927][root][INFO] - Training Epoch: 2/2, step 2413/7134 completed (loss: 0.034987300634384155, acc: 1.0)
[2025-02-13 20:39:00,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:00,310][root][INFO] - Training Epoch: 2/2, step 2414/7134 completed (loss: 0.05312179774045944, acc: 0.9890710115432739)
[2025-02-13 20:39:00,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:00,711][root][INFO] - Training Epoch: 2/2, step 2415/7134 completed (loss: 0.22559793293476105, acc: 0.9817073345184326)
[2025-02-13 20:39:00,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:01,078][root][INFO] - Training Epoch: 2/2, step 2416/7134 completed (loss: 0.07128933072090149, acc: 0.9811320900917053)
[2025-02-13 20:39:01,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:01,460][root][INFO] - Training Epoch: 2/2, step 2417/7134 completed (loss: 0.11547955125570297, acc: 0.9804878234863281)
[2025-02-13 20:39:01,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:01,831][root][INFO] - Training Epoch: 2/2, step 2418/7134 completed (loss: 0.11279484629631042, acc: 0.9720279574394226)
[2025-02-13 20:39:01,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:02,195][root][INFO] - Training Epoch: 2/2, step 2419/7134 completed (loss: 0.09489937871694565, acc: 0.9815950989723206)
[2025-02-13 20:39:02,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:02,575][root][INFO] - Training Epoch: 2/2, step 2420/7134 completed (loss: 0.10570420324802399, acc: 0.9722222089767456)
[2025-02-13 20:39:02,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:02,954][root][INFO] - Training Epoch: 2/2, step 2421/7134 completed (loss: 0.06669308245182037, acc: 0.9783783555030823)
[2025-02-13 20:39:03,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:03,361][root][INFO] - Training Epoch: 2/2, step 2422/7134 completed (loss: 0.07629949599504471, acc: 0.9828571677207947)
[2025-02-13 20:39:03,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:03,768][root][INFO] - Training Epoch: 2/2, step 2423/7134 completed (loss: 0.0875249058008194, acc: 0.988095223903656)
[2025-02-13 20:39:03,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:04,180][root][INFO] - Training Epoch: 2/2, step 2424/7134 completed (loss: 0.10006535053253174, acc: 0.9772727489471436)
[2025-02-13 20:39:04,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:04,590][root][INFO] - Training Epoch: 2/2, step 2425/7134 completed (loss: 0.09422608464956284, acc: 0.966292142868042)
[2025-02-13 20:39:04,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:04,975][root][INFO] - Training Epoch: 2/2, step 2426/7134 completed (loss: 0.06320357322692871, acc: 0.982758641242981)
[2025-02-13 20:39:05,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:05,340][root][INFO] - Training Epoch: 2/2, step 2427/7134 completed (loss: 0.04638237878680229, acc: 0.9791666865348816)
[2025-02-13 20:39:05,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:05,728][root][INFO] - Training Epoch: 2/2, step 2428/7134 completed (loss: 0.03930094465613365, acc: 0.9947368502616882)
[2025-02-13 20:39:05,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:06,103][root][INFO] - Training Epoch: 2/2, step 2429/7134 completed (loss: 0.0881194919347763, acc: 0.9746835231781006)
[2025-02-13 20:39:06,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:06,459][root][INFO] - Training Epoch: 2/2, step 2430/7134 completed (loss: 0.07511167228221893, acc: 0.9800000190734863)
[2025-02-13 20:39:06,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:06,878][root][INFO] - Training Epoch: 2/2, step 2431/7134 completed (loss: 0.04593789577484131, acc: 0.9860140085220337)
[2025-02-13 20:39:07,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:07,246][root][INFO] - Training Epoch: 2/2, step 2432/7134 completed (loss: 0.19139310717582703, acc: 0.942148745059967)
[2025-02-13 20:39:07,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:07,624][root][INFO] - Training Epoch: 2/2, step 2433/7134 completed (loss: 0.14249302446842194, acc: 0.971222996711731)
[2025-02-13 20:39:07,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:07,979][root][INFO] - Training Epoch: 2/2, step 2434/7134 completed (loss: 0.14876040816307068, acc: 0.9597315192222595)
[2025-02-13 20:39:08,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:08,383][root][INFO] - Training Epoch: 2/2, step 2435/7134 completed (loss: 0.15458045899868011, acc: 0.9602649211883545)
[2025-02-13 20:39:08,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:08,743][root][INFO] - Training Epoch: 2/2, step 2436/7134 completed (loss: 0.05035283789038658, acc: 0.9847715497016907)
[2025-02-13 20:39:08,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:09,098][root][INFO] - Training Epoch: 2/2, step 2437/7134 completed (loss: 0.11353867501020432, acc: 0.9718309640884399)
[2025-02-13 20:39:09,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:09,466][root][INFO] - Training Epoch: 2/2, step 2438/7134 completed (loss: 0.11574223637580872, acc: 0.9814814925193787)
[2025-02-13 20:39:09,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:09,848][root][INFO] - Training Epoch: 2/2, step 2439/7134 completed (loss: 0.09251084923744202, acc: 0.9893048405647278)
[2025-02-13 20:39:09,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:10,224][root][INFO] - Training Epoch: 2/2, step 2440/7134 completed (loss: 0.05669538676738739, acc: 0.9886363744735718)
[2025-02-13 20:39:10,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:10,617][root][INFO] - Training Epoch: 2/2, step 2441/7134 completed (loss: 0.2862533628940582, acc: 0.9520000219345093)
[2025-02-13 20:39:10,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:10,978][root][INFO] - Training Epoch: 2/2, step 2442/7134 completed (loss: 0.13951575756072998, acc: 0.976331353187561)
[2025-02-13 20:39:11,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:11,339][root][INFO] - Training Epoch: 2/2, step 2443/7134 completed (loss: 0.19136889278888702, acc: 0.9631901979446411)
[2025-02-13 20:39:11,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:11,703][root][INFO] - Training Epoch: 2/2, step 2444/7134 completed (loss: 0.1844761222600937, acc: 0.9534883499145508)
[2025-02-13 20:39:11,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:12,076][root][INFO] - Training Epoch: 2/2, step 2445/7134 completed (loss: 0.16766910254955292, acc: 0.9617834687232971)
[2025-02-13 20:39:12,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:12,451][root][INFO] - Training Epoch: 2/2, step 2446/7134 completed (loss: 0.3183051347732544, acc: 0.9285714030265808)
[2025-02-13 20:39:12,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:12,822][root][INFO] - Training Epoch: 2/2, step 2447/7134 completed (loss: 0.07277902960777283, acc: 0.9887640476226807)
[2025-02-13 20:39:12,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:13,179][root][INFO] - Training Epoch: 2/2, step 2448/7134 completed (loss: 0.07138895243406296, acc: 0.9798657894134521)
[2025-02-13 20:39:13,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:13,526][root][INFO] - Training Epoch: 2/2, step 2449/7134 completed (loss: 0.20825077593326569, acc: 0.9418604373931885)
[2025-02-13 20:39:13,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:13,903][root][INFO] - Training Epoch: 2/2, step 2450/7134 completed (loss: 0.046063702553510666, acc: 0.9882352948188782)
[2025-02-13 20:39:14,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14,252][root][INFO] - Training Epoch: 2/2, step 2451/7134 completed (loss: 0.07766169309616089, acc: 0.9839572310447693)
[2025-02-13 20:39:14,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14,615][root][INFO] - Training Epoch: 2/2, step 2452/7134 completed (loss: 0.08636526018381119, acc: 0.9731183052062988)
[2025-02-13 20:39:14,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14,983][root][INFO] - Training Epoch: 2/2, step 2453/7134 completed (loss: 0.0856482982635498, acc: 0.9740932583808899)
[2025-02-13 20:39:15,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:15,329][root][INFO] - Training Epoch: 2/2, step 2454/7134 completed (loss: 0.07339641451835632, acc: 0.9716312289237976)
[2025-02-13 20:39:15,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:15,720][root][INFO] - Training Epoch: 2/2, step 2455/7134 completed (loss: 0.04882936552166939, acc: 0.987730085849762)
[2025-02-13 20:39:15,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:16,075][root][INFO] - Training Epoch: 2/2, step 2456/7134 completed (loss: 0.13483041524887085, acc: 0.9481865167617798)
[2025-02-13 20:39:16,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:16,443][root][INFO] - Training Epoch: 2/2, step 2457/7134 completed (loss: 0.14852869510650635, acc: 0.9738562107086182)
[2025-02-13 20:39:16,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:16,778][root][INFO] - Training Epoch: 2/2, step 2458/7134 completed (loss: 0.281974732875824, acc: 0.9437500238418579)
[2025-02-13 20:39:16,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:17,145][root][INFO] - Training Epoch: 2/2, step 2459/7134 completed (loss: 0.06097635254263878, acc: 0.9824561476707458)
[2025-02-13 20:39:17,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:17,502][root][INFO] - Training Epoch: 2/2, step 2460/7134 completed (loss: 0.0976099893450737, acc: 0.9725274443626404)
[2025-02-13 20:39:17,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:17,850][root][INFO] - Training Epoch: 2/2, step 2461/7134 completed (loss: 0.034329853951931, acc: 0.9937106966972351)
[2025-02-13 20:39:17,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:18,234][root][INFO] - Training Epoch: 2/2, step 2462/7134 completed (loss: 0.16793277859687805, acc: 0.9636363387107849)
[2025-02-13 20:39:18,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:18,625][root][INFO] - Training Epoch: 2/2, step 2463/7134 completed (loss: 0.01910456083714962, acc: 1.0)
[2025-02-13 20:39:18,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:18,984][root][INFO] - Training Epoch: 2/2, step 2464/7134 completed (loss: 0.10387872159481049, acc: 0.9750000238418579)
[2025-02-13 20:39:19,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:19,382][root][INFO] - Training Epoch: 2/2, step 2465/7134 completed (loss: 0.0824272409081459, acc: 0.9664804339408875)
[2025-02-13 20:39:19,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:19,767][root][INFO] - Training Epoch: 2/2, step 2466/7134 completed (loss: 0.04038577899336815, acc: 0.9820359349250793)
[2025-02-13 20:39:19,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:20,148][root][INFO] - Training Epoch: 2/2, step 2467/7134 completed (loss: 0.11899620294570923, acc: 0.966292142868042)
[2025-02-13 20:39:20,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:20,542][root][INFO] - Training Epoch: 2/2, step 2468/7134 completed (loss: 0.06704005599021912, acc: 0.9709302186965942)
[2025-02-13 20:39:20,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:20,912][root][INFO] - Training Epoch: 2/2, step 2469/7134 completed (loss: 0.018636072054505348, acc: 1.0)
[2025-02-13 20:39:21,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:21,273][root][INFO] - Training Epoch: 2/2, step 2470/7134 completed (loss: 0.11132261157035828, acc: 0.9716312289237976)
[2025-02-13 20:39:21,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:21,660][root][INFO] - Training Epoch: 2/2, step 2471/7134 completed (loss: 0.09581959247589111, acc: 0.957446813583374)
[2025-02-13 20:39:21,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:22,023][root][INFO] - Training Epoch: 2/2, step 2472/7134 completed (loss: 0.019265752285718918, acc: 1.0)
[2025-02-13 20:39:22,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:22,415][root][INFO] - Training Epoch: 2/2, step 2473/7134 completed (loss: 0.0420488566160202, acc: 0.9906542301177979)
[2025-02-13 20:39:22,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:22,796][root][INFO] - Training Epoch: 2/2, step 2474/7134 completed (loss: 0.016163058578968048, acc: 1.0)
[2025-02-13 20:39:22,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:23,167][root][INFO] - Training Epoch: 2/2, step 2475/7134 completed (loss: 0.02904493734240532, acc: 0.9918032884597778)
[2025-02-13 20:39:23,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:23,531][root][INFO] - Training Epoch: 2/2, step 2476/7134 completed (loss: 0.03799529746174812, acc: 0.9824561476707458)
[2025-02-13 20:39:23,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:23,914][root][INFO] - Training Epoch: 2/2, step 2477/7134 completed (loss: 0.013157207518815994, acc: 1.0)
[2025-02-13 20:39:24,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:24,298][root][INFO] - Training Epoch: 2/2, step 2478/7134 completed (loss: 0.024406710639595985, acc: 1.0)
[2025-02-13 20:39:24,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:24,684][root][INFO] - Training Epoch: 2/2, step 2479/7134 completed (loss: 0.15858761966228485, acc: 0.9591836929321289)
[2025-02-13 20:39:24,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:25,117][root][INFO] - Training Epoch: 2/2, step 2480/7134 completed (loss: 0.06048274040222168, acc: 0.9807692170143127)
[2025-02-13 20:39:25,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:25,515][root][INFO] - Training Epoch: 2/2, step 2481/7134 completed (loss: 0.026193767786026, acc: 1.0)
[2025-02-13 20:39:25,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:25,891][root][INFO] - Training Epoch: 2/2, step 2482/7134 completed (loss: 0.06438374519348145, acc: 0.9805194735527039)
[2025-02-13 20:39:26,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:26,272][root][INFO] - Training Epoch: 2/2, step 2483/7134 completed (loss: 0.047441598027944565, acc: 0.9940119981765747)
[2025-02-13 20:39:26,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:26,669][root][INFO] - Training Epoch: 2/2, step 2484/7134 completed (loss: 0.10662846267223358, acc: 0.9819276928901672)
[2025-02-13 20:39:26,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:27,020][root][INFO] - Training Epoch: 2/2, step 2485/7134 completed (loss: 0.04962143674492836, acc: 0.9925925731658936)
[2025-02-13 20:39:27,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:27,397][root][INFO] - Training Epoch: 2/2, step 2486/7134 completed (loss: 0.07505276799201965, acc: 0.9931507110595703)
[2025-02-13 20:39:27,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:27,787][root][INFO] - Training Epoch: 2/2, step 2487/7134 completed (loss: 0.20462584495544434, acc: 0.9583333134651184)
[2025-02-13 20:39:27,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:28,174][root][INFO] - Training Epoch: 2/2, step 2488/7134 completed (loss: 0.07286573201417923, acc: 0.9818181991577148)
[2025-02-13 20:39:28,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:28,571][root][INFO] - Training Epoch: 2/2, step 2489/7134 completed (loss: 0.10280707478523254, acc: 0.976331353187561)
[2025-02-13 20:39:28,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:28,947][root][INFO] - Training Epoch: 2/2, step 2490/7134 completed (loss: 0.07266996800899506, acc: 0.9870129823684692)
[2025-02-13 20:39:29,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:29,333][root][INFO] - Training Epoch: 2/2, step 2491/7134 completed (loss: 0.15305280685424805, acc: 0.9701492786407471)
[2025-02-13 20:39:29,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:29,711][root][INFO] - Training Epoch: 2/2, step 2492/7134 completed (loss: 0.0646941065788269, acc: 0.9937106966972351)
[2025-02-13 20:39:29,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:30,080][root][INFO] - Training Epoch: 2/2, step 2493/7134 completed (loss: 0.07618031650781631, acc: 0.988095223903656)
[2025-02-13 20:39:30,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:30,427][root][INFO] - Training Epoch: 2/2, step 2494/7134 completed (loss: 0.05742722377181053, acc: 0.9876543283462524)
[2025-02-13 20:39:30,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:30,841][root][INFO] - Training Epoch: 2/2, step 2495/7134 completed (loss: 0.08286537230014801, acc: 0.9882352948188782)
[2025-02-13 20:39:30,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:31,226][root][INFO] - Training Epoch: 2/2, step 2496/7134 completed (loss: 0.07570842653512955, acc: 0.9810126423835754)
[2025-02-13 20:39:31,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:31,637][root][INFO] - Training Epoch: 2/2, step 2497/7134 completed (loss: 0.0756172388792038, acc: 0.9885057210922241)
[2025-02-13 20:39:31,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:32,010][root][INFO] - Training Epoch: 2/2, step 2498/7134 completed (loss: 0.059497103095054626, acc: 0.9828571677207947)
[2025-02-13 20:39:32,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:32,375][root][INFO] - Training Epoch: 2/2, step 2499/7134 completed (loss: 0.03312436863780022, acc: 0.9939758777618408)
[2025-02-13 20:39:32,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:32,757][root][INFO] - Training Epoch: 2/2, step 2500/7134 completed (loss: 0.059096354991197586, acc: 0.9932432174682617)
[2025-02-13 20:39:32,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:33,161][root][INFO] - Training Epoch: 2/2, step 2501/7134 completed (loss: 0.09205345809459686, acc: 0.9775280952453613)
[2025-02-13 20:39:33,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:33,575][root][INFO] - Training Epoch: 2/2, step 2502/7134 completed (loss: 0.04305935651063919, acc: 0.9914529919624329)
[2025-02-13 20:39:33,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:33,954][root][INFO] - Training Epoch: 2/2, step 2503/7134 completed (loss: 0.03783832862973213, acc: 0.9927536249160767)
[2025-02-13 20:39:34,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:34,331][root][INFO] - Training Epoch: 2/2, step 2504/7134 completed (loss: 0.10804783552885056, acc: 0.9683544039726257)
[2025-02-13 20:39:34,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:34,701][root][INFO] - Training Epoch: 2/2, step 2505/7134 completed (loss: 0.051071859896183014, acc: 0.9825581312179565)
[2025-02-13 20:39:34,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:35,077][root][INFO] - Training Epoch: 2/2, step 2506/7134 completed (loss: 0.021769866347312927, acc: 1.0)
[2025-02-13 20:39:35,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:35,455][root][INFO] - Training Epoch: 2/2, step 2507/7134 completed (loss: 0.021230235695838928, acc: 0.9944444298744202)
[2025-02-13 20:39:35,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:35,853][root][INFO] - Training Epoch: 2/2, step 2508/7134 completed (loss: 0.035526175051927567, acc: 0.9830508232116699)
[2025-02-13 20:39:35,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:36,223][root][INFO] - Training Epoch: 2/2, step 2509/7134 completed (loss: 0.029779156669974327, acc: 0.9882352948188782)
[2025-02-13 20:39:36,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:36,614][root][INFO] - Training Epoch: 2/2, step 2510/7134 completed (loss: 0.027163591235876083, acc: 0.9928571581840515)
[2025-02-13 20:39:36,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:36,981][root][INFO] - Training Epoch: 2/2, step 2511/7134 completed (loss: 0.11696360260248184, acc: 0.9726775884628296)
[2025-02-13 20:39:37,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:37,347][root][INFO] - Training Epoch: 2/2, step 2512/7134 completed (loss: 0.09130667895078659, acc: 0.9764705896377563)
[2025-02-13 20:39:37,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:37,734][root][INFO] - Training Epoch: 2/2, step 2513/7134 completed (loss: 0.03219348564743996, acc: 1.0)
[2025-02-13 20:39:37,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:38,125][root][INFO] - Training Epoch: 2/2, step 2514/7134 completed (loss: 0.07214875519275665, acc: 0.9808917045593262)
[2025-02-13 20:39:38,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:38,498][root][INFO] - Training Epoch: 2/2, step 2515/7134 completed (loss: 0.04519590362906456, acc: 0.9870129823684692)
[2025-02-13 20:39:38,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:38,904][root][INFO] - Training Epoch: 2/2, step 2516/7134 completed (loss: 0.09317527711391449, acc: 0.9888268113136292)
[2025-02-13 20:39:39,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:39,289][root][INFO] - Training Epoch: 2/2, step 2517/7134 completed (loss: 0.08858238905668259, acc: 0.9833333492279053)
[2025-02-13 20:39:39,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:39,690][root][INFO] - Training Epoch: 2/2, step 2518/7134 completed (loss: 0.04221625253558159, acc: 0.9869281053543091)
[2025-02-13 20:39:39,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:40,072][root][INFO] - Training Epoch: 2/2, step 2519/7134 completed (loss: 0.01174253225326538, acc: 0.9935483932495117)
[2025-02-13 20:39:40,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:40,473][root][INFO] - Training Epoch: 2/2, step 2520/7134 completed (loss: 0.0315842404961586, acc: 0.9934210777282715)
[2025-02-13 20:39:40,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:40,857][root][INFO] - Training Epoch: 2/2, step 2521/7134 completed (loss: 0.09677775949239731, acc: 0.9890109896659851)
[2025-02-13 20:39:40,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:41,234][root][INFO] - Training Epoch: 2/2, step 2522/7134 completed (loss: 0.041194476187229156, acc: 0.9931034445762634)
[2025-02-13 20:39:41,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:41,606][root][INFO] - Training Epoch: 2/2, step 2523/7134 completed (loss: 0.097249336540699, acc: 0.9815950989723206)
[2025-02-13 20:39:41,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:41,972][root][INFO] - Training Epoch: 2/2, step 2524/7134 completed (loss: 0.03241449594497681, acc: 0.9878048896789551)
[2025-02-13 20:39:42,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:42,332][root][INFO] - Training Epoch: 2/2, step 2525/7134 completed (loss: 0.06514962762594223, acc: 0.9794520735740662)
[2025-02-13 20:39:42,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:42,706][root][INFO] - Training Epoch: 2/2, step 2526/7134 completed (loss: 0.008621168322861195, acc: 1.0)
[2025-02-13 20:39:42,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:43,072][root][INFO] - Training Epoch: 2/2, step 2527/7134 completed (loss: 0.025494113564491272, acc: 1.0)
[2025-02-13 20:39:43,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:43,436][root][INFO] - Training Epoch: 2/2, step 2528/7134 completed (loss: 0.027557414025068283, acc: 0.9879518151283264)
[2025-02-13 20:39:43,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:43,805][root][INFO] - Training Epoch: 2/2, step 2529/7134 completed (loss: 0.04347870871424675, acc: 0.988095223903656)
[2025-02-13 20:39:43,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:44,182][root][INFO] - Training Epoch: 2/2, step 2530/7134 completed (loss: 0.0481424443423748, acc: 0.983146071434021)
[2025-02-13 20:39:44,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:44,570][root][INFO] - Training Epoch: 2/2, step 2531/7134 completed (loss: 0.01632041297852993, acc: 1.0)
[2025-02-13 20:39:44,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:44,952][root][INFO] - Training Epoch: 2/2, step 2532/7134 completed (loss: 0.03004462830722332, acc: 0.9933333396911621)
[2025-02-13 20:39:45,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:45,299][root][INFO] - Training Epoch: 2/2, step 2533/7134 completed (loss: 0.046726495027542114, acc: 0.9935897588729858)
[2025-02-13 20:39:45,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:45,666][root][INFO] - Training Epoch: 2/2, step 2534/7134 completed (loss: 0.08596759289503098, acc: 0.9832402467727661)
[2025-02-13 20:39:45,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:46,028][root][INFO] - Training Epoch: 2/2, step 2535/7134 completed (loss: 0.011340070515871048, acc: 1.0)
[2025-02-13 20:39:46,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:46,388][root][INFO] - Training Epoch: 2/2, step 2536/7134 completed (loss: 0.04528943821787834, acc: 0.9948186278343201)
[2025-02-13 20:39:46,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:46,784][root][INFO] - Training Epoch: 2/2, step 2537/7134 completed (loss: 0.08497779071331024, acc: 0.965753436088562)
[2025-02-13 20:39:46,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47,181][root][INFO] - Training Epoch: 2/2, step 2538/7134 completed (loss: 0.12285424023866653, acc: 0.977011501789093)
[2025-02-13 20:39:47,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47,541][root][INFO] - Training Epoch: 2/2, step 2539/7134 completed (loss: 0.06382086127996445, acc: 0.9849624037742615)
[2025-02-13 20:39:47,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47,919][root][INFO] - Training Epoch: 2/2, step 2540/7134 completed (loss: 0.06528124958276749, acc: 0.9801324605941772)
[2025-02-13 20:39:48,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:48,277][root][INFO] - Training Epoch: 2/2, step 2541/7134 completed (loss: 0.018031753599643707, acc: 1.0)
[2025-02-13 20:39:48,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:48,665][root][INFO] - Training Epoch: 2/2, step 2542/7134 completed (loss: 0.06959360092878342, acc: 0.987500011920929)
[2025-02-13 20:39:48,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:49,048][root][INFO] - Training Epoch: 2/2, step 2543/7134 completed (loss: 0.11807797849178314, acc: 0.9748427867889404)
[2025-02-13 20:39:49,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:49,437][root][INFO] - Training Epoch: 2/2, step 2544/7134 completed (loss: 0.10717979073524475, acc: 0.9702380895614624)
[2025-02-13 20:39:49,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:49,847][root][INFO] - Training Epoch: 2/2, step 2545/7134 completed (loss: 0.016294533386826515, acc: 1.0)
[2025-02-13 20:39:50,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:50,252][root][INFO] - Training Epoch: 2/2, step 2546/7134 completed (loss: 0.195029616355896, acc: 0.9489796161651611)
[2025-02-13 20:39:50,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:50,650][root][INFO] - Training Epoch: 2/2, step 2547/7134 completed (loss: 0.1354677379131317, acc: 0.9640287756919861)
[2025-02-13 20:39:50,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:51,055][root][INFO] - Training Epoch: 2/2, step 2548/7134 completed (loss: 0.0802827924489975, acc: 0.991150438785553)
[2025-02-13 20:39:51,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:51,439][root][INFO] - Training Epoch: 2/2, step 2549/7134 completed (loss: 0.0327993743121624, acc: 1.0)
[2025-02-13 20:39:51,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:51,814][root][INFO] - Training Epoch: 2/2, step 2550/7134 completed (loss: 0.022681601345539093, acc: 0.9863013625144958)
[2025-02-13 20:39:51,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:52,197][root][INFO] - Training Epoch: 2/2, step 2551/7134 completed (loss: 0.14466094970703125, acc: 0.9637681245803833)
[2025-02-13 20:39:52,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:52,575][root][INFO] - Training Epoch: 2/2, step 2552/7134 completed (loss: 0.0294062327593565, acc: 0.9873417615890503)
[2025-02-13 20:39:52,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:52,946][root][INFO] - Training Epoch: 2/2, step 2553/7134 completed (loss: 0.07473991811275482, acc: 0.9868420958518982)
[2025-02-13 20:39:53,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:53,333][root][INFO] - Training Epoch: 2/2, step 2554/7134 completed (loss: 0.10874611884355545, acc: 0.9763779640197754)
[2025-02-13 20:39:53,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:53,688][root][INFO] - Training Epoch: 2/2, step 2555/7134 completed (loss: 0.03634052351117134, acc: 0.9909909963607788)
[2025-02-13 20:39:53,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:54,051][root][INFO] - Training Epoch: 2/2, step 2556/7134 completed (loss: 0.08945612609386444, acc: 0.9900000095367432)
[2025-02-13 20:39:54,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:54,423][root][INFO] - Training Epoch: 2/2, step 2557/7134 completed (loss: 0.12189683318138123, acc: 0.9795918464660645)
[2025-02-13 20:39:54,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:54,785][root][INFO] - Training Epoch: 2/2, step 2558/7134 completed (loss: 0.020297260954976082, acc: 1.0)
[2025-02-13 20:39:54,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:55,145][root][INFO] - Training Epoch: 2/2, step 2559/7134 completed (loss: 0.03546352684497833, acc: 0.9937499761581421)
[2025-02-13 20:39:55,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:55,551][root][INFO] - Training Epoch: 2/2, step 2560/7134 completed (loss: 0.0546465702354908, acc: 0.9797297120094299)
[2025-02-13 20:39:55,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:55,937][root][INFO] - Training Epoch: 2/2, step 2561/7134 completed (loss: 0.025303438305854797, acc: 1.0)
[2025-02-13 20:39:56,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:56,300][root][INFO] - Training Epoch: 2/2, step 2562/7134 completed (loss: 0.022740615531802177, acc: 0.9919999837875366)
[2025-02-13 20:39:56,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:56,666][root][INFO] - Training Epoch: 2/2, step 2563/7134 completed (loss: 0.04831279069185257, acc: 0.9940828680992126)
[2025-02-13 20:39:56,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:57,030][root][INFO] - Training Epoch: 2/2, step 2564/7134 completed (loss: 0.17789599299430847, acc: 0.9496402740478516)
[2025-02-13 20:39:57,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:57,375][root][INFO] - Training Epoch: 2/2, step 2565/7134 completed (loss: 0.02948959916830063, acc: 0.9928057789802551)
[2025-02-13 20:39:57,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:57,741][root][INFO] - Training Epoch: 2/2, step 2566/7134 completed (loss: 0.05803953483700752, acc: 0.9879518151283264)
[2025-02-13 20:39:57,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:58,135][root][INFO] - Training Epoch: 2/2, step 2567/7134 completed (loss: 0.04436726123094559, acc: 0.9849624037742615)
[2025-02-13 20:39:58,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:58,529][root][INFO] - Training Epoch: 2/2, step 2568/7134 completed (loss: 0.09389396756887436, acc: 0.9807692170143127)
[2025-02-13 20:39:58,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:58,902][root][INFO] - Training Epoch: 2/2, step 2569/7134 completed (loss: 0.02132171392440796, acc: 1.0)
[2025-02-13 20:39:59,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:59,260][root][INFO] - Training Epoch: 2/2, step 2570/7134 completed (loss: 0.011894789524376392, acc: 1.0)
[2025-02-13 20:39:59,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:59,607][root][INFO] - Training Epoch: 2/2, step 2571/7134 completed (loss: 0.0773647129535675, acc: 0.9672130942344666)
[2025-02-13 20:39:59,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:59,982][root][INFO] - Training Epoch: 2/2, step 2572/7134 completed (loss: 0.021519258618354797, acc: 1.0)
[2025-02-13 20:40:00,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:00,362][root][INFO] - Training Epoch: 2/2, step 2573/7134 completed (loss: 0.05598396435379982, acc: 0.9928057789802551)
[2025-02-13 20:40:00,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:00,697][root][INFO] - Training Epoch: 2/2, step 2574/7134 completed (loss: 0.038387615233659744, acc: 0.9898989796638489)
[2025-02-13 20:40:00,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:01,068][root][INFO] - Training Epoch: 2/2, step 2575/7134 completed (loss: 0.10930906981229782, acc: 0.9754098653793335)
[2025-02-13 20:40:01,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:01,418][root][INFO] - Training Epoch: 2/2, step 2576/7134 completed (loss: 0.02077564224600792, acc: 1.0)
[2025-02-13 20:40:01,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:01,781][root][INFO] - Training Epoch: 2/2, step 2577/7134 completed (loss: 0.15344254672527313, acc: 0.9921259880065918)
[2025-02-13 20:40:01,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:02,129][root][INFO] - Training Epoch: 2/2, step 2578/7134 completed (loss: 0.1569080799818039, acc: 0.9673202633857727)
[2025-02-13 20:40:02,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:02,489][root][INFO] - Training Epoch: 2/2, step 2579/7134 completed (loss: 0.14301766455173492, acc: 0.9679487347602844)
[2025-02-13 20:40:02,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:02,875][root][INFO] - Training Epoch: 2/2, step 2580/7134 completed (loss: 0.08004488050937653, acc: 0.9826589822769165)
[2025-02-13 20:40:03,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:03,246][root][INFO] - Training Epoch: 2/2, step 2581/7134 completed (loss: 0.21545018255710602, acc: 0.9604519605636597)
[2025-02-13 20:40:03,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:03,632][root][INFO] - Training Epoch: 2/2, step 2582/7134 completed (loss: 0.12381691485643387, acc: 0.9774011373519897)
[2025-02-13 20:40:03,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:03,969][root][INFO] - Training Epoch: 2/2, step 2583/7134 completed (loss: 0.16661566495895386, acc: 0.931034505367279)
[2025-02-13 20:40:04,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:04,321][root][INFO] - Training Epoch: 2/2, step 2584/7134 completed (loss: 0.173951655626297, acc: 0.9503546357154846)
[2025-02-13 20:40:04,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:04,712][root][INFO] - Training Epoch: 2/2, step 2585/7134 completed (loss: 0.1000160425901413, acc: 0.9801324605941772)
[2025-02-13 20:40:04,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:05,091][root][INFO] - Training Epoch: 2/2, step 2586/7134 completed (loss: 0.16300733387470245, acc: 0.9692307710647583)
[2025-02-13 20:40:05,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:05,473][root][INFO] - Training Epoch: 2/2, step 2587/7134 completed (loss: 0.07197238504886627, acc: 0.9750000238418579)
[2025-02-13 20:40:05,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:05,839][root][INFO] - Training Epoch: 2/2, step 2588/7134 completed (loss: 0.05752824246883392, acc: 0.9863945841789246)
[2025-02-13 20:40:05,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:06,217][root][INFO] - Training Epoch: 2/2, step 2589/7134 completed (loss: 0.13625235855579376, acc: 0.9523809552192688)
[2025-02-13 20:40:06,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:06,560][root][INFO] - Training Epoch: 2/2, step 2590/7134 completed (loss: 0.09522966295480728, acc: 0.9629629850387573)
[2025-02-13 20:40:06,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:06,936][root][INFO] - Training Epoch: 2/2, step 2591/7134 completed (loss: 0.1705358922481537, acc: 0.9532163739204407)
[2025-02-13 20:40:07,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:07,308][root][INFO] - Training Epoch: 2/2, step 2592/7134 completed (loss: 0.223329558968544, acc: 0.9441340565681458)
[2025-02-13 20:40:07,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:07,674][root][INFO] - Training Epoch: 2/2, step 2593/7134 completed (loss: 0.07654263079166412, acc: 0.9939024448394775)
[2025-02-13 20:40:07,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:08,069][root][INFO] - Training Epoch: 2/2, step 2594/7134 completed (loss: 0.11412589251995087, acc: 0.9607843160629272)
[2025-02-13 20:40:08,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:08,432][root][INFO] - Training Epoch: 2/2, step 2595/7134 completed (loss: 0.0785137489438057, acc: 0.9817073345184326)
[2025-02-13 20:40:08,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:08,816][root][INFO] - Training Epoch: 2/2, step 2596/7134 completed (loss: 0.08136207610368729, acc: 0.9823529124259949)
[2025-02-13 20:40:08,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:09,180][root][INFO] - Training Epoch: 2/2, step 2597/7134 completed (loss: 0.10800833255052567, acc: 0.9717513918876648)
[2025-02-13 20:40:09,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:09,539][root][INFO] - Training Epoch: 2/2, step 2598/7134 completed (loss: 0.147967129945755, acc: 0.9602649211883545)
[2025-02-13 20:40:09,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:09,880][root][INFO] - Training Epoch: 2/2, step 2599/7134 completed (loss: 0.021012848243117332, acc: 0.9937106966972351)
[2025-02-13 20:40:10,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:10,253][root][INFO] - Training Epoch: 2/2, step 2600/7134 completed (loss: 0.14038729667663574, acc: 0.9775280952453613)
[2025-02-13 20:40:10,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:10,645][root][INFO] - Training Epoch: 2/2, step 2601/7134 completed (loss: 0.1453876942396164, acc: 0.948387086391449)
[2025-02-13 20:40:10,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:11,020][root][INFO] - Training Epoch: 2/2, step 2602/7134 completed (loss: 0.08337663859128952, acc: 0.988095223903656)
[2025-02-13 20:40:11,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:11,419][root][INFO] - Training Epoch: 2/2, step 2603/7134 completed (loss: 0.11881788074970245, acc: 0.9673202633857727)
[2025-02-13 20:40:11,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:11,795][root][INFO] - Training Epoch: 2/2, step 2604/7134 completed (loss: 0.07991019636392593, acc: 0.9735449552536011)
[2025-02-13 20:40:11,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:12,179][root][INFO] - Training Epoch: 2/2, step 2605/7134 completed (loss: 0.1124536320567131, acc: 0.9469026327133179)
[2025-02-13 20:40:12,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:12,569][root][INFO] - Training Epoch: 2/2, step 2606/7134 completed (loss: 0.09048904478549957, acc: 0.9800000190734863)
[2025-02-13 20:40:12,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:12,962][root][INFO] - Training Epoch: 2/2, step 2607/7134 completed (loss: 0.058313027024269104, acc: 0.983146071434021)
[2025-02-13 20:40:13,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:13,355][root][INFO] - Training Epoch: 2/2, step 2608/7134 completed (loss: 0.1430404633283615, acc: 0.9629629850387573)
[2025-02-13 20:40:13,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:13,745][root][INFO] - Training Epoch: 2/2, step 2609/7134 completed (loss: 0.07937180250883102, acc: 0.9738562107086182)
[2025-02-13 20:40:13,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14,100][root][INFO] - Training Epoch: 2/2, step 2610/7134 completed (loss: 0.13672977685928345, acc: 0.9640287756919861)
[2025-02-13 20:40:14,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14,477][root][INFO] - Training Epoch: 2/2, step 2611/7134 completed (loss: 0.20372989773750305, acc: 0.9441340565681458)
[2025-02-13 20:40:14,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14,865][root][INFO] - Training Epoch: 2/2, step 2612/7134 completed (loss: 0.1552315354347229, acc: 0.949999988079071)
[2025-02-13 20:40:15,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:15,254][root][INFO] - Training Epoch: 2/2, step 2613/7134 completed (loss: 0.10586019605398178, acc: 0.9702380895614624)
[2025-02-13 20:40:15,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:15,640][root][INFO] - Training Epoch: 2/2, step 2614/7134 completed (loss: 0.16750212013721466, acc: 0.9521276354789734)
[2025-02-13 20:40:15,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:16,008][root][INFO] - Training Epoch: 2/2, step 2615/7134 completed (loss: 0.08604680001735687, acc: 0.983146071434021)
[2025-02-13 20:40:16,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:16,392][root][INFO] - Training Epoch: 2/2, step 2616/7134 completed (loss: 0.11337202042341232, acc: 0.9747899174690247)
[2025-02-13 20:40:16,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:16,765][root][INFO] - Training Epoch: 2/2, step 2617/7134 completed (loss: 0.2262839674949646, acc: 0.9441624283790588)
[2025-02-13 20:40:16,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:17,147][root][INFO] - Training Epoch: 2/2, step 2618/7134 completed (loss: 0.052267517894506454, acc: 0.9914529919624329)
[2025-02-13 20:40:17,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:17,535][root][INFO] - Training Epoch: 2/2, step 2619/7134 completed (loss: 0.040915295481681824, acc: 0.9927536249160767)
[2025-02-13 20:40:17,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:17,915][root][INFO] - Training Epoch: 2/2, step 2620/7134 completed (loss: 0.07251483201980591, acc: 0.9894179701805115)
[2025-02-13 20:40:18,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:18,290][root][INFO] - Training Epoch: 2/2, step 2621/7134 completed (loss: 0.15224166214466095, acc: 0.9679144620895386)
[2025-02-13 20:40:18,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:18,644][root][INFO] - Training Epoch: 2/2, step 2622/7134 completed (loss: 0.14395646750926971, acc: 0.9589040875434875)
[2025-02-13 20:40:18,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:18,997][root][INFO] - Training Epoch: 2/2, step 2623/7134 completed (loss: 0.13749435544013977, acc: 0.963302731513977)
[2025-02-13 20:40:19,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:19,351][root][INFO] - Training Epoch: 2/2, step 2624/7134 completed (loss: 0.2045789211988449, acc: 0.9333333373069763)
[2025-02-13 20:40:19,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:19,737][root][INFO] - Training Epoch: 2/2, step 2625/7134 completed (loss: 0.10036717355251312, acc: 0.9702380895614624)
[2025-02-13 20:40:19,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:20,123][root][INFO] - Training Epoch: 2/2, step 2626/7134 completed (loss: 0.05438830330967903, acc: 0.9942528605461121)
[2025-02-13 20:40:20,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:20,509][root][INFO] - Training Epoch: 2/2, step 2627/7134 completed (loss: 0.07417508214712143, acc: 0.9815950989723206)
[2025-02-13 20:40:20,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:20,906][root][INFO] - Training Epoch: 2/2, step 2628/7134 completed (loss: 0.13275977969169617, acc: 0.9666666388511658)
[2025-02-13 20:40:21,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:21,340][root][INFO] - Training Epoch: 2/2, step 2629/7134 completed (loss: 0.048129111528396606, acc: 0.9880239367485046)
[2025-02-13 20:40:21,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:21,724][root][INFO] - Training Epoch: 2/2, step 2630/7134 completed (loss: 0.0927603617310524, acc: 0.9637681245803833)
[2025-02-13 20:40:21,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22,108][root][INFO] - Training Epoch: 2/2, step 2631/7134 completed (loss: 0.10009396821260452, acc: 0.9695431590080261)
[2025-02-13 20:40:22,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22,476][root][INFO] - Training Epoch: 2/2, step 2632/7134 completed (loss: 0.025814613327383995, acc: 0.991304337978363)
[2025-02-13 20:40:22,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22,844][root][INFO] - Training Epoch: 2/2, step 2633/7134 completed (loss: 0.15313299000263214, acc: 0.9615384340286255)
[2025-02-13 20:40:22,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:23,195][root][INFO] - Training Epoch: 2/2, step 2634/7134 completed (loss: 0.027082713320851326, acc: 0.9940828680992126)
[2025-02-13 20:40:23,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:23,576][root][INFO] - Training Epoch: 2/2, step 2635/7134 completed (loss: 0.14458106458187103, acc: 0.9611111283302307)
[2025-02-13 20:40:23,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:23,980][root][INFO] - Training Epoch: 2/2, step 2636/7134 completed (loss: 0.16541284322738647, acc: 0.9608938694000244)
[2025-02-13 20:40:24,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:24,379][root][INFO] - Training Epoch: 2/2, step 2637/7134 completed (loss: 0.29501640796661377, acc: 0.9202454090118408)
[2025-02-13 20:40:24,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:24,769][root][INFO] - Training Epoch: 2/2, step 2638/7134 completed (loss: 0.21581779420375824, acc: 0.9402984976768494)
[2025-02-13 20:40:24,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:25,146][root][INFO] - Training Epoch: 2/2, step 2639/7134 completed (loss: 0.32676783204078674, acc: 0.9248120188713074)
[2025-02-13 20:40:25,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:25,525][root][INFO] - Training Epoch: 2/2, step 2640/7134 completed (loss: 0.35390543937683105, acc: 0.9402984976768494)
[2025-02-13 20:40:25,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:25,913][root][INFO] - Training Epoch: 2/2, step 2641/7134 completed (loss: 0.1501055210828781, acc: 0.9599999785423279)
[2025-02-13 20:40:26,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:26,304][root][INFO] - Training Epoch: 2/2, step 2642/7134 completed (loss: 0.432407021522522, acc: 0.9047619104385376)
[2025-02-13 20:40:26,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:26,702][root][INFO] - Training Epoch: 2/2, step 2643/7134 completed (loss: 0.2732669711112976, acc: 0.9476743936538696)
[2025-02-13 20:40:26,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:27,136][root][INFO] - Training Epoch: 2/2, step 2644/7134 completed (loss: 0.18681471049785614, acc: 0.9629629850387573)
[2025-02-13 20:40:27,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:27,521][root][INFO] - Training Epoch: 2/2, step 2645/7134 completed (loss: 0.14502079784870148, acc: 0.954285740852356)
[2025-02-13 20:40:27,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:27,887][root][INFO] - Training Epoch: 2/2, step 2646/7134 completed (loss: 0.23762966692447662, acc: 0.9378882050514221)
[2025-02-13 20:40:28,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:28,266][root][INFO] - Training Epoch: 2/2, step 2647/7134 completed (loss: 0.17609666287899017, acc: 0.95652174949646)
[2025-02-13 20:40:28,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:28,649][root][INFO] - Training Epoch: 2/2, step 2648/7134 completed (loss: 0.2646183669567108, acc: 0.9306930899620056)
[2025-02-13 20:40:28,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:29,050][root][INFO] - Training Epoch: 2/2, step 2649/7134 completed (loss: 0.2761240303516388, acc: 0.908108115196228)
[2025-02-13 20:40:29,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:29,390][root][INFO] - Training Epoch: 2/2, step 2650/7134 completed (loss: 0.24863873422145844, acc: 0.929411768913269)
[2025-02-13 20:40:29,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:29,759][root][INFO] - Training Epoch: 2/2, step 2651/7134 completed (loss: 0.10736316442489624, acc: 0.9756097793579102)
[2025-02-13 20:40:29,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:30,145][root][INFO] - Training Epoch: 2/2, step 2652/7134 completed (loss: 0.06245328485965729, acc: 0.9887640476226807)
[2025-02-13 20:40:30,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:30,499][root][INFO] - Training Epoch: 2/2, step 2653/7134 completed (loss: 0.05556800961494446, acc: 0.9934640526771545)
[2025-02-13 20:40:30,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:30,865][root][INFO] - Training Epoch: 2/2, step 2654/7134 completed (loss: 0.040451791137456894, acc: 0.9857142567634583)
[2025-02-13 20:40:31,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31,229][root][INFO] - Training Epoch: 2/2, step 2655/7134 completed (loss: 0.08726655691862106, acc: 0.9751552939414978)
[2025-02-13 20:40:31,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31,596][root][INFO] - Training Epoch: 2/2, step 2656/7134 completed (loss: 0.1814921349287033, acc: 0.9651162624359131)
[2025-02-13 20:40:31,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31,983][root][INFO] - Training Epoch: 2/2, step 2657/7134 completed (loss: 0.05923281982541084, acc: 0.9873417615890503)
[2025-02-13 20:40:32,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:32,358][root][INFO] - Training Epoch: 2/2, step 2658/7134 completed (loss: 0.2627980709075928, acc: 0.939393937587738)
[2025-02-13 20:40:32,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:32,710][root][INFO] - Training Epoch: 2/2, step 2659/7134 completed (loss: 0.18360166251659393, acc: 0.9477611780166626)
[2025-02-13 20:40:32,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:33,100][root][INFO] - Training Epoch: 2/2, step 2660/7134 completed (loss: 0.17180779576301575, acc: 0.9658119678497314)
[2025-02-13 20:40:33,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:33,483][root][INFO] - Training Epoch: 2/2, step 2661/7134 completed (loss: 0.03638608381152153, acc: 0.9940828680992126)
[2025-02-13 20:40:33,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:33,847][root][INFO] - Training Epoch: 2/2, step 2662/7134 completed (loss: 0.05652377009391785, acc: 0.9814814925193787)
[2025-02-13 20:40:33,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:34,232][root][INFO] - Training Epoch: 2/2, step 2663/7134 completed (loss: 0.08591345697641373, acc: 0.9740932583808899)
[2025-02-13 20:40:34,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:34,630][root][INFO] - Training Epoch: 2/2, step 2664/7134 completed (loss: 0.0676824301481247, acc: 0.9883720874786377)
[2025-02-13 20:40:34,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:35,012][root][INFO] - Training Epoch: 2/2, step 2665/7134 completed (loss: 0.03844771161675453, acc: 0.9939758777618408)
[2025-02-13 20:40:35,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:35,403][root][INFO] - Training Epoch: 2/2, step 2666/7134 completed (loss: 0.016570627689361572, acc: 0.9941860437393188)
[2025-02-13 20:40:35,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:35,755][root][INFO] - Training Epoch: 2/2, step 2667/7134 completed (loss: 0.03129943832755089, acc: 0.9936708807945251)
[2025-02-13 20:40:35,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:36,135][root][INFO] - Training Epoch: 2/2, step 2668/7134 completed (loss: 0.011122827418148518, acc: 1.0)
[2025-02-13 20:40:36,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:36,515][root][INFO] - Training Epoch: 2/2, step 2669/7134 completed (loss: 0.009109525941312313, acc: 1.0)
[2025-02-13 20:40:36,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:36,906][root][INFO] - Training Epoch: 2/2, step 2670/7134 completed (loss: 0.053248949348926544, acc: 0.9828571677207947)
[2025-02-13 20:40:37,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:37,287][root][INFO] - Training Epoch: 2/2, step 2671/7134 completed (loss: 0.05006275698542595, acc: 0.9828571677207947)
[2025-02-13 20:40:37,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:37,659][root][INFO] - Training Epoch: 2/2, step 2672/7134 completed (loss: 0.030962156131863594, acc: 0.9939758777618408)
[2025-02-13 20:40:37,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:38,040][root][INFO] - Training Epoch: 2/2, step 2673/7134 completed (loss: 0.018810104578733444, acc: 1.0)
[2025-02-13 20:40:38,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:38,388][root][INFO] - Training Epoch: 2/2, step 2674/7134 completed (loss: 0.06810571998357773, acc: 0.9938271641731262)
[2025-02-13 20:40:38,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:38,745][root][INFO] - Training Epoch: 2/2, step 2675/7134 completed (loss: 0.014297768473625183, acc: 1.0)
[2025-02-13 20:40:38,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:39,099][root][INFO] - Training Epoch: 2/2, step 2676/7134 completed (loss: 0.02783256582915783, acc: 0.9886363744735718)
[2025-02-13 20:40:39,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:39,466][root][INFO] - Training Epoch: 2/2, step 2677/7134 completed (loss: 0.024219345301389694, acc: 0.9938650131225586)
[2025-02-13 20:40:39,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:39,835][root][INFO] - Training Epoch: 2/2, step 2678/7134 completed (loss: 0.012575282715260983, acc: 1.0)
[2025-02-13 20:40:39,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:40,205][root][INFO] - Training Epoch: 2/2, step 2679/7134 completed (loss: 0.01347876712679863, acc: 1.0)
[2025-02-13 20:40:40,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:40,568][root][INFO] - Training Epoch: 2/2, step 2680/7134 completed (loss: 0.04666643217206001, acc: 0.9780219793319702)
[2025-02-13 20:40:40,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:40,921][root][INFO] - Training Epoch: 2/2, step 2681/7134 completed (loss: 0.012523040175437927, acc: 1.0)
[2025-02-13 20:40:41,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:41,245][root][INFO] - Training Epoch: 2/2, step 2682/7134 completed (loss: 0.0350622721016407, acc: 0.9865771532058716)
[2025-02-13 20:40:41,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:41,590][root][INFO] - Training Epoch: 2/2, step 2683/7134 completed (loss: 0.011458809487521648, acc: 1.0)
[2025-02-13 20:40:41,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:41,970][root][INFO] - Training Epoch: 2/2, step 2684/7134 completed (loss: 0.09860671311616898, acc: 0.9921875)
[2025-02-13 20:40:42,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:42,335][root][INFO] - Training Epoch: 2/2, step 2685/7134 completed (loss: 0.027461867779493332, acc: 1.0)
[2025-02-13 20:40:42,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:42,728][root][INFO] - Training Epoch: 2/2, step 2686/7134 completed (loss: 0.13726940751075745, acc: 0.9599999785423279)
[2025-02-13 20:40:42,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:43,076][root][INFO] - Training Epoch: 2/2, step 2687/7134 completed (loss: 0.0632500872015953, acc: 0.9831932783126831)
[2025-02-13 20:40:43,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:43,426][root][INFO] - Training Epoch: 2/2, step 2688/7134 completed (loss: 0.1172771155834198, acc: 0.9624060392379761)
[2025-02-13 20:40:43,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:43,724][root][INFO] - Training Epoch: 2/2, step 2689/7134 completed (loss: 0.09614215046167374, acc: 0.9837398529052734)
[2025-02-13 20:40:43,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:44,078][root][INFO] - Training Epoch: 2/2, step 2690/7134 completed (loss: 0.0981319472193718, acc: 0.9784172773361206)
[2025-02-13 20:40:44,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:44,428][root][INFO] - Training Epoch: 2/2, step 2691/7134 completed (loss: 0.1665487140417099, acc: 0.9469026327133179)
[2025-02-13 20:40:44,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:44,781][root][INFO] - Training Epoch: 2/2, step 2692/7134 completed (loss: 0.1869201362133026, acc: 0.942148745059967)
[2025-02-13 20:40:44,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:45,191][root][INFO] - Training Epoch: 2/2, step 2693/7134 completed (loss: 0.1566895693540573, acc: 0.9734513163566589)
[2025-02-13 20:40:45,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:45,550][root][INFO] - Training Epoch: 2/2, step 2694/7134 completed (loss: 0.17759107053279877, acc: 0.9735099077224731)
[2025-02-13 20:40:45,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:45,919][root][INFO] - Training Epoch: 2/2, step 2695/7134 completed (loss: 0.07365907728672028, acc: 0.970370352268219)
[2025-02-13 20:40:46,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:46,278][root][INFO] - Training Epoch: 2/2, step 2696/7134 completed (loss: 0.1320313662290573, acc: 0.9642857313156128)
[2025-02-13 20:40:46,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:46,627][root][INFO] - Training Epoch: 2/2, step 2697/7134 completed (loss: 0.1680176854133606, acc: 0.9610389471054077)
[2025-02-13 20:40:46,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:46,972][root][INFO] - Training Epoch: 2/2, step 2698/7134 completed (loss: 0.10471721738576889, acc: 0.9659863710403442)
[2025-02-13 20:40:47,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:47,343][root][INFO] - Training Epoch: 2/2, step 2699/7134 completed (loss: 0.0770149901509285, acc: 0.9741379022598267)
[2025-02-13 20:40:47,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:47,705][root][INFO] - Training Epoch: 2/2, step 2700/7134 completed (loss: 0.07372809201478958, acc: 0.9857142567634583)
[2025-02-13 20:40:47,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:48,078][root][INFO] - Training Epoch: 2/2, step 2701/7134 completed (loss: 0.11870437115430832, acc: 0.969924807548523)
[2025-02-13 20:40:48,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:48,365][root][INFO] - Training Epoch: 2/2, step 2702/7134 completed (loss: 0.13893458247184753, acc: 0.9814814925193787)
[2025-02-13 20:40:48,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:48,692][root][INFO] - Training Epoch: 2/2, step 2703/7134 completed (loss: 0.03813907131552696, acc: 0.9866666793823242)
[2025-02-13 20:40:48,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49,054][root][INFO] - Training Epoch: 2/2, step 2704/7134 completed (loss: 0.07733836770057678, acc: 0.988095223903656)
[2025-02-13 20:40:49,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49,418][root][INFO] - Training Epoch: 2/2, step 2705/7134 completed (loss: 0.1232042983174324, acc: 0.9718309640884399)
[2025-02-13 20:40:49,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49,795][root][INFO] - Training Epoch: 2/2, step 2706/7134 completed (loss: 0.09603758901357651, acc: 0.9794520735740662)
[2025-02-13 20:40:49,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:50,154][root][INFO] - Training Epoch: 2/2, step 2707/7134 completed (loss: 0.13521911203861237, acc: 0.9583333134651184)
[2025-02-13 20:40:50,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:50,520][root][INFO] - Training Epoch: 2/2, step 2708/7134 completed (loss: 0.09161845594644547, acc: 0.9828571677207947)
[2025-02-13 20:40:50,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:50,890][root][INFO] - Training Epoch: 2/2, step 2709/7134 completed (loss: 0.04044753313064575, acc: 0.9929577708244324)
[2025-02-13 20:40:51,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:51,285][root][INFO] - Training Epoch: 2/2, step 2710/7134 completed (loss: 0.058940671384334564, acc: 0.9900000095367432)
[2025-02-13 20:40:51,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:51,673][root][INFO] - Training Epoch: 2/2, step 2711/7134 completed (loss: 0.022406764328479767, acc: 1.0)
[2025-02-13 20:40:51,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:52,040][root][INFO] - Training Epoch: 2/2, step 2712/7134 completed (loss: 0.07002298533916473, acc: 0.978723406791687)
[2025-02-13 20:40:52,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:52,425][root][INFO] - Training Epoch: 2/2, step 2713/7134 completed (loss: 0.03838133439421654, acc: 0.987500011920929)
[2025-02-13 20:40:52,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:52,799][root][INFO] - Training Epoch: 2/2, step 2714/7134 completed (loss: 0.05886658653616905, acc: 0.9852941036224365)
[2025-02-13 20:40:52,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:53,166][root][INFO] - Training Epoch: 2/2, step 2715/7134 completed (loss: 0.03211972862482071, acc: 0.9931034445762634)
[2025-02-13 20:40:53,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:53,534][root][INFO] - Training Epoch: 2/2, step 2716/7134 completed (loss: 0.05153590440750122, acc: 0.9883720874786377)
[2025-02-13 20:40:53,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:53,903][root][INFO] - Training Epoch: 2/2, step 2717/7134 completed (loss: 0.03602851182222366, acc: 1.0)
[2025-02-13 20:40:54,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:54,266][root][INFO] - Training Epoch: 2/2, step 2718/7134 completed (loss: 0.15740469098091125, acc: 0.9513888955116272)
[2025-02-13 20:40:54,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:54,649][root][INFO] - Training Epoch: 2/2, step 2719/7134 completed (loss: 0.1273951381444931, acc: 0.971222996711731)
[2025-02-13 20:40:54,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:55,011][root][INFO] - Training Epoch: 2/2, step 2720/7134 completed (loss: 0.1687127947807312, acc: 0.956204354763031)
[2025-02-13 20:40:55,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:55,396][root][INFO] - Training Epoch: 2/2, step 2721/7134 completed (loss: 0.13581934571266174, acc: 0.9801324605941772)
[2025-02-13 20:40:55,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:55,750][root][INFO] - Training Epoch: 2/2, step 2722/7134 completed (loss: 0.10643883794546127, acc: 0.978723406791687)
[2025-02-13 20:40:55,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:56,126][root][INFO] - Training Epoch: 2/2, step 2723/7134 completed (loss: 0.26598116755485535, acc: 0.9520958065986633)
[2025-02-13 20:40:56,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:56,488][root][INFO] - Training Epoch: 2/2, step 2724/7134 completed (loss: 0.05258197709918022, acc: 0.9774011373519897)
[2025-02-13 20:40:56,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:56,851][root][INFO] - Training Epoch: 2/2, step 2725/7134 completed (loss: 0.08139721304178238, acc: 0.9626865386962891)
[2025-02-13 20:40:56,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:57,219][root][INFO] - Training Epoch: 2/2, step 2726/7134 completed (loss: 0.12717337906360626, acc: 0.9674796462059021)
[2025-02-13 20:40:57,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:57,604][root][INFO] - Training Epoch: 2/2, step 2727/7134 completed (loss: 0.03309169039130211, acc: 0.9912280440330505)
[2025-02-13 20:40:57,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:57,976][root][INFO] - Training Epoch: 2/2, step 2728/7134 completed (loss: 0.5830280780792236, acc: 0.8740741014480591)
[2025-02-13 20:40:58,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:58,298][root][INFO] - Training Epoch: 2/2, step 2729/7134 completed (loss: 0.2857707738876343, acc: 0.949999988079071)
[2025-02-13 20:40:58,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:58,692][root][INFO] - Training Epoch: 2/2, step 2730/7134 completed (loss: 0.18123915791511536, acc: 0.9469026327133179)
[2025-02-13 20:40:58,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:59,071][root][INFO] - Training Epoch: 2/2, step 2731/7134 completed (loss: 0.15573081374168396, acc: 0.9514563083648682)
[2025-02-13 20:40:59,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:59,452][root][INFO] - Training Epoch: 2/2, step 2732/7134 completed (loss: 0.21997390687465668, acc: 0.936274528503418)
[2025-02-13 20:40:59,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:59,859][root][INFO] - Training Epoch: 2/2, step 2733/7134 completed (loss: 0.13608288764953613, acc: 0.9611111283302307)
[2025-02-13 20:40:59,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:00,212][root][INFO] - Training Epoch: 2/2, step 2734/7134 completed (loss: 0.19707468152046204, acc: 0.9504132270812988)
[2025-02-13 20:41:00,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:00,587][root][INFO] - Training Epoch: 2/2, step 2735/7134 completed (loss: 0.15922243893146515, acc: 0.9756097793579102)
[2025-02-13 20:41:00,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:00,983][root][INFO] - Training Epoch: 2/2, step 2736/7134 completed (loss: 0.20612064003944397, acc: 0.9308176040649414)
[2025-02-13 20:41:01,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:01,372][root][INFO] - Training Epoch: 2/2, step 2737/7134 completed (loss: 0.15462952852249146, acc: 0.9785714149475098)
[2025-02-13 20:41:01,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:01,747][root][INFO] - Training Epoch: 2/2, step 2738/7134 completed (loss: 0.1759873926639557, acc: 0.9580838084220886)
[2025-02-13 20:41:01,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:02,136][root][INFO] - Training Epoch: 2/2, step 2739/7134 completed (loss: 0.181313619017601, acc: 0.9420289993286133)
[2025-02-13 20:41:02,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:02,511][root][INFO] - Training Epoch: 2/2, step 2740/7134 completed (loss: 0.1054740622639656, acc: 0.9727891087532043)
[2025-02-13 20:41:02,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:02,884][root][INFO] - Training Epoch: 2/2, step 2741/7134 completed (loss: 0.07058238238096237, acc: 0.9741379022598267)
[2025-02-13 20:41:03,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:03,251][root][INFO] - Training Epoch: 2/2, step 2742/7134 completed (loss: 0.08640501648187637, acc: 0.9747899174690247)
[2025-02-13 20:41:03,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:03,592][root][INFO] - Training Epoch: 2/2, step 2743/7134 completed (loss: 0.048071496188640594, acc: 0.978723406791687)
[2025-02-13 20:41:03,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:03,947][root][INFO] - Training Epoch: 2/2, step 2744/7134 completed (loss: 0.01953672058880329, acc: 1.0)
[2025-02-13 20:41:04,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:04,300][root][INFO] - Training Epoch: 2/2, step 2745/7134 completed (loss: 0.35000863671302795, acc: 0.9107142686843872)
[2025-02-13 20:41:04,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:04,636][root][INFO] - Training Epoch: 2/2, step 2746/7134 completed (loss: 0.1577812135219574, acc: 0.9772727489471436)
[2025-02-13 20:41:04,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:04,983][root][INFO] - Training Epoch: 2/2, step 2747/7134 completed (loss: 0.17262105643749237, acc: 0.9629629850387573)
[2025-02-13 20:41:05,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:05,303][root][INFO] - Training Epoch: 2/2, step 2748/7134 completed (loss: 0.04323224350810051, acc: 0.9870129823684692)
[2025-02-13 20:41:05,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:05,651][root][INFO] - Training Epoch: 2/2, step 2749/7134 completed (loss: 0.14960090816020966, acc: 0.9793814420700073)
[2025-02-13 20:41:05,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:06,038][root][INFO] - Training Epoch: 2/2, step 2750/7134 completed (loss: 0.13423337042331696, acc: 0.9578313231468201)
[2025-02-13 20:41:06,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:06,389][root][INFO] - Training Epoch: 2/2, step 2751/7134 completed (loss: 0.1533680409193039, acc: 0.9639639854431152)
[2025-02-13 20:41:06,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:06,744][root][INFO] - Training Epoch: 2/2, step 2752/7134 completed (loss: 0.1132204532623291, acc: 0.9781022071838379)
[2025-02-13 20:41:06,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:07,111][root][INFO] - Training Epoch: 2/2, step 2753/7134 completed (loss: 0.2871388792991638, acc: 0.9495798349380493)
[2025-02-13 20:41:07,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:07,511][root][INFO] - Training Epoch: 2/2, step 2754/7134 completed (loss: 0.11956901103258133, acc: 0.9750000238418579)
[2025-02-13 20:41:07,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:07,890][root][INFO] - Training Epoch: 2/2, step 2755/7134 completed (loss: 0.1035676822066307, acc: 0.9921875)
[2025-02-13 20:41:08,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:08,294][root][INFO] - Training Epoch: 2/2, step 2756/7134 completed (loss: 0.07869207113981247, acc: 0.9864864945411682)
[2025-02-13 20:41:08,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:08,624][root][INFO] - Training Epoch: 2/2, step 2757/7134 completed (loss: 0.0805509090423584, acc: 0.97826087474823)
[2025-02-13 20:41:08,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:09,014][root][INFO] - Training Epoch: 2/2, step 2758/7134 completed (loss: 0.14485079050064087, acc: 0.9590163826942444)
[2025-02-13 20:41:09,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:09,398][root][INFO] - Training Epoch: 2/2, step 2759/7134 completed (loss: 0.053586073219776154, acc: 0.9857142567634583)
[2025-02-13 20:41:09,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:09,800][root][INFO] - Training Epoch: 2/2, step 2760/7134 completed (loss: 0.09417364001274109, acc: 0.9777777791023254)
[2025-02-13 20:41:09,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:10,190][root][INFO] - Training Epoch: 2/2, step 2761/7134 completed (loss: 0.18241992592811584, acc: 0.9578947424888611)
[2025-02-13 20:41:10,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:10,610][root][INFO] - Training Epoch: 2/2, step 2762/7134 completed (loss: 0.15598410367965698, acc: 0.9556650519371033)
[2025-02-13 20:41:10,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:10,986][root][INFO] - Training Epoch: 2/2, step 2763/7134 completed (loss: 0.09888183325529099, acc: 0.9781420826911926)
[2025-02-13 20:41:11,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:11,357][root][INFO] - Training Epoch: 2/2, step 2764/7134 completed (loss: 0.11745131760835648, acc: 0.9739583134651184)
[2025-02-13 20:41:11,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:11,741][root][INFO] - Training Epoch: 2/2, step 2765/7134 completed (loss: 0.05655568093061447, acc: 0.9823529124259949)
[2025-02-13 20:41:11,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:12,136][root][INFO] - Training Epoch: 2/2, step 2766/7134 completed (loss: 0.021830623969435692, acc: 1.0)
[2025-02-13 20:41:12,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:12,540][root][INFO] - Training Epoch: 2/2, step 2767/7134 completed (loss: 0.03242618218064308, acc: 0.9950000047683716)
[2025-02-13 20:41:12,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:12,906][root][INFO] - Training Epoch: 2/2, step 2768/7134 completed (loss: 0.059288397431373596, acc: 0.9879518151283264)
[2025-02-13 20:41:13,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:13,279][root][INFO] - Training Epoch: 2/2, step 2769/7134 completed (loss: 0.06818733364343643, acc: 0.9736841917037964)
[2025-02-13 20:41:13,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:13,685][root][INFO] - Training Epoch: 2/2, step 2770/7134 completed (loss: 0.031499505043029785, acc: 0.9938271641731262)
[2025-02-13 20:41:13,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:14,077][root][INFO] - Training Epoch: 2/2, step 2771/7134 completed (loss: 0.14207766950130463, acc: 0.9620853066444397)
[2025-02-13 20:41:14,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:14,456][root][INFO] - Training Epoch: 2/2, step 2772/7134 completed (loss: 0.08777135610580444, acc: 0.9700000286102295)
[2025-02-13 20:41:14,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:14,836][root][INFO] - Training Epoch: 2/2, step 2773/7134 completed (loss: 0.10985102504491806, acc: 0.9698492288589478)
[2025-02-13 20:41:14,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:15,215][root][INFO] - Training Epoch: 2/2, step 2774/7134 completed (loss: 0.15594646334648132, acc: 0.9512194991111755)
[2025-02-13 20:41:15,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:15,587][root][INFO] - Training Epoch: 2/2, step 2775/7134 completed (loss: 0.184650719165802, acc: 0.9398906826972961)
[2025-02-13 20:41:15,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:15,987][root][INFO] - Training Epoch: 2/2, step 2776/7134 completed (loss: 0.09021047502756119, acc: 0.9826839566230774)
[2025-02-13 20:41:16,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:16,347][root][INFO] - Training Epoch: 2/2, step 2777/7134 completed (loss: 0.025735866278409958, acc: 0.9885714054107666)
[2025-02-13 20:41:16,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:16,714][root][INFO] - Training Epoch: 2/2, step 2778/7134 completed (loss: 0.12543794512748718, acc: 0.9595959782600403)
[2025-02-13 20:41:16,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:17,092][root][INFO] - Training Epoch: 2/2, step 2779/7134 completed (loss: 0.13370868563652039, acc: 0.9634703397750854)
[2025-02-13 20:41:17,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:17,467][root][INFO] - Training Epoch: 2/2, step 2780/7134 completed (loss: 0.17671823501586914, acc: 0.9513274431228638)
[2025-02-13 20:41:17,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:17,855][root][INFO] - Training Epoch: 2/2, step 2781/7134 completed (loss: 0.07169492542743683, acc: 0.97826087474823)
[2025-02-13 20:41:17,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:18,218][root][INFO] - Training Epoch: 2/2, step 2782/7134 completed (loss: 0.07318920642137527, acc: 0.9732142686843872)
[2025-02-13 20:41:18,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:18,591][root][INFO] - Training Epoch: 2/2, step 2783/7134 completed (loss: 0.04855727031826973, acc: 0.9795918464660645)
[2025-02-13 20:41:18,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:18,967][root][INFO] - Training Epoch: 2/2, step 2784/7134 completed (loss: 0.047961436212062836, acc: 0.9806763529777527)
[2025-02-13 20:41:19,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:19,358][root][INFO] - Training Epoch: 2/2, step 2785/7134 completed (loss: 0.07499851286411285, acc: 0.9805825352668762)
[2025-02-13 20:41:19,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:19,777][root][INFO] - Training Epoch: 2/2, step 2786/7134 completed (loss: 0.18559399247169495, acc: 0.9333333373069763)
[2025-02-13 20:41:19,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:20,165][root][INFO] - Training Epoch: 2/2, step 2787/7134 completed (loss: 0.04349394515156746, acc: 0.9826589822769165)
[2025-02-13 20:41:20,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:20,532][root][INFO] - Training Epoch: 2/2, step 2788/7134 completed (loss: 0.14125840365886688, acc: 0.9681528806686401)
[2025-02-13 20:41:20,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:20,920][root][INFO] - Training Epoch: 2/2, step 2789/7134 completed (loss: 0.5366693139076233, acc: 0.8666666746139526)
[2025-02-13 20:41:21,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:21,298][root][INFO] - Training Epoch: 2/2, step 2790/7134 completed (loss: 0.476826936006546, acc: 0.875)
[2025-02-13 20:41:21,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:21,667][root][INFO] - Training Epoch: 2/2, step 2791/7134 completed (loss: 0.24267719686031342, acc: 0.940397322177887)
[2025-02-13 20:41:21,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:22,040][root][INFO] - Training Epoch: 2/2, step 2792/7134 completed (loss: 0.13293133676052094, acc: 0.9709302186965942)
[2025-02-13 20:41:22,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:22,408][root][INFO] - Training Epoch: 2/2, step 2793/7134 completed (loss: 0.07539626955986023, acc: 0.9820359349250793)
[2025-02-13 20:41:22,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:22,770][root][INFO] - Training Epoch: 2/2, step 2794/7134 completed (loss: 0.016270413994789124, acc: 1.0)
[2025-02-13 20:41:22,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:23,129][root][INFO] - Training Epoch: 2/2, step 2795/7134 completed (loss: 0.25139784812927246, acc: 0.9259259104728699)
[2025-02-13 20:41:23,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:23,476][root][INFO] - Training Epoch: 2/2, step 2796/7134 completed (loss: 0.12234870344400406, acc: 0.9674796462059021)
[2025-02-13 20:41:23,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:23,822][root][INFO] - Training Epoch: 2/2, step 2797/7134 completed (loss: 0.07291299104690552, acc: 0.9756097793579102)
[2025-02-13 20:41:23,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:24,186][root][INFO] - Training Epoch: 2/2, step 2798/7134 completed (loss: 0.15363077819347382, acc: 0.949999988079071)
[2025-02-13 20:41:24,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:24,525][root][INFO] - Training Epoch: 2/2, step 2799/7134 completed (loss: 0.1748102456331253, acc: 0.9296875)
[2025-02-13 20:41:24,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:24,862][root][INFO] - Training Epoch: 2/2, step 2800/7134 completed (loss: 0.13637235760688782, acc: 0.9781022071838379)
[2025-02-13 20:41:25,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:25,241][root][INFO] - Training Epoch: 2/2, step 2801/7134 completed (loss: 0.17115308344364166, acc: 0.9617486596107483)
[2025-02-13 20:41:25,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:25,612][root][INFO] - Training Epoch: 2/2, step 2802/7134 completed (loss: 0.11802776902914047, acc: 0.9698492288589478)
[2025-02-13 20:41:25,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:26,000][root][INFO] - Training Epoch: 2/2, step 2803/7134 completed (loss: 0.04932468757033348, acc: 0.9814814925193787)
[2025-02-13 20:41:26,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:26,385][root][INFO] - Training Epoch: 2/2, step 2804/7134 completed (loss: 0.09993910044431686, acc: 0.9942196607589722)
[2025-02-13 20:41:26,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:26,755][root][INFO] - Training Epoch: 2/2, step 2805/7134 completed (loss: 0.06999543309211731, acc: 0.9767441749572754)
[2025-02-13 20:41:26,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:27,110][root][INFO] - Training Epoch: 2/2, step 2806/7134 completed (loss: 0.08684175461530685, acc: 0.9779411554336548)
[2025-02-13 20:41:27,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:27,467][root][INFO] - Training Epoch: 2/2, step 2807/7134 completed (loss: 0.1000475287437439, acc: 0.9902912378311157)
[2025-02-13 20:41:27,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:27,849][root][INFO] - Training Epoch: 2/2, step 2808/7134 completed (loss: 0.10829365998506546, acc: 0.9698492288589478)
[2025-02-13 20:41:27,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:28,263][root][INFO] - Training Epoch: 2/2, step 2809/7134 completed (loss: 0.16928282380104065, acc: 0.960629940032959)
[2025-02-13 20:41:28,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:28,655][root][INFO] - Training Epoch: 2/2, step 2810/7134 completed (loss: 0.044027235358953476, acc: 0.9890710115432739)
[2025-02-13 20:41:28,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:29,017][root][INFO] - Training Epoch: 2/2, step 2811/7134 completed (loss: 0.05328042432665825, acc: 0.988950252532959)
[2025-02-13 20:41:29,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:29,383][root][INFO] - Training Epoch: 2/2, step 2812/7134 completed (loss: 0.030561452731490135, acc: 1.0)
[2025-02-13 20:41:29,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:29,759][root][INFO] - Training Epoch: 2/2, step 2813/7134 completed (loss: 0.09819873422384262, acc: 0.9885057210922241)
[2025-02-13 20:41:29,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:30,119][root][INFO] - Training Epoch: 2/2, step 2814/7134 completed (loss: 0.16505950689315796, acc: 0.9709302186965942)
[2025-02-13 20:41:30,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:30,480][root][INFO] - Training Epoch: 2/2, step 2815/7134 completed (loss: 0.07919833809137344, acc: 0.9835164546966553)
[2025-02-13 20:41:30,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:30,855][root][INFO] - Training Epoch: 2/2, step 2816/7134 completed (loss: 0.06457086652517319, acc: 0.9869281053543091)
[2025-02-13 20:41:31,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:31,230][root][INFO] - Training Epoch: 2/2, step 2817/7134 completed (loss: 0.06699034571647644, acc: 0.9847715497016907)
[2025-02-13 20:41:31,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:31,596][root][INFO] - Training Epoch: 2/2, step 2818/7134 completed (loss: 0.053626649081707, acc: 0.9946808218955994)
[2025-02-13 20:41:31,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:31,947][root][INFO] - Training Epoch: 2/2, step 2819/7134 completed (loss: 0.06481441855430603, acc: 0.9863013625144958)
[2025-02-13 20:41:32,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:32,316][root][INFO] - Training Epoch: 2/2, step 2820/7134 completed (loss: 0.2627902925014496, acc: 0.9386503100395203)
[2025-02-13 20:41:32,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:32,683][root][INFO] - Training Epoch: 2/2, step 2821/7134 completed (loss: 0.13646523654460907, acc: 0.9689119458198547)
[2025-02-13 20:41:32,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:33,091][root][INFO] - Training Epoch: 2/2, step 2822/7134 completed (loss: 0.14158247411251068, acc: 0.9675675630569458)
[2025-02-13 20:41:33,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:33,476][root][INFO] - Training Epoch: 2/2, step 2823/7134 completed (loss: 0.10081931948661804, acc: 0.9813664555549622)
[2025-02-13 20:41:33,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:33,852][root][INFO] - Training Epoch: 2/2, step 2824/7134 completed (loss: 0.22409871220588684, acc: 0.9308176040649414)
[2025-02-13 20:41:33,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:34,204][root][INFO] - Training Epoch: 2/2, step 2825/7134 completed (loss: 0.10177439451217651, acc: 0.9777777791023254)
[2025-02-13 20:41:34,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:34,575][root][INFO] - Training Epoch: 2/2, step 2826/7134 completed (loss: 0.10981401056051254, acc: 0.977011501789093)
[2025-02-13 20:41:34,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:34,979][root][INFO] - Training Epoch: 2/2, step 2827/7134 completed (loss: 0.10575374215841293, acc: 0.9904305934906006)
[2025-02-13 20:41:35,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:35,369][root][INFO] - Training Epoch: 2/2, step 2828/7134 completed (loss: 0.1061965748667717, acc: 0.9746835231781006)
[2025-02-13 20:41:35,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:35,751][root][INFO] - Training Epoch: 2/2, step 2829/7134 completed (loss: 0.12283492088317871, acc: 0.9583333134651184)
[2025-02-13 20:41:35,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:36,118][root][INFO] - Training Epoch: 2/2, step 2830/7134 completed (loss: 0.04972356557846069, acc: 0.9701492786407471)
[2025-02-13 20:41:36,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:36,495][root][INFO] - Training Epoch: 2/2, step 2831/7134 completed (loss: 0.060448408126831055, acc: 0.9797297120094299)
[2025-02-13 20:41:36,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:36,868][root][INFO] - Training Epoch: 2/2, step 2832/7134 completed (loss: 0.1426895707845688, acc: 0.9784946441650391)
[2025-02-13 20:41:37,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:37,246][root][INFO] - Training Epoch: 2/2, step 2833/7134 completed (loss: 0.1606634110212326, acc: 0.9841269850730896)
[2025-02-13 20:41:37,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:37,626][root][INFO] - Training Epoch: 2/2, step 2834/7134 completed (loss: 0.1648804396390915, acc: 0.9611111283302307)
[2025-02-13 20:41:37,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:37,997][root][INFO] - Training Epoch: 2/2, step 2835/7134 completed (loss: 0.023373117670416832, acc: 0.9934210777282715)
[2025-02-13 20:41:38,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:38,370][root][INFO] - Training Epoch: 2/2, step 2836/7134 completed (loss: 0.17375335097312927, acc: 0.9700000286102295)
[2025-02-13 20:41:38,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:38,744][root][INFO] - Training Epoch: 2/2, step 2837/7134 completed (loss: 0.04584962874650955, acc: 0.9824561476707458)
[2025-02-13 20:41:38,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:39,117][root][INFO] - Training Epoch: 2/2, step 2838/7134 completed (loss: 0.1359047293663025, acc: 0.9704433679580688)
[2025-02-13 20:41:39,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:39,490][root][INFO] - Training Epoch: 2/2, step 2839/7134 completed (loss: 0.15887562930583954, acc: 0.9850746393203735)
[2025-02-13 20:41:39,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:39,866][root][INFO] - Training Epoch: 2/2, step 2840/7134 completed (loss: 0.05809012055397034, acc: 0.9882352948188782)
[2025-02-13 20:41:40,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:40,221][root][INFO] - Training Epoch: 2/2, step 2841/7134 completed (loss: 0.06278099864721298, acc: 0.9905660152435303)
[2025-02-13 20:41:40,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:40,600][root][INFO] - Training Epoch: 2/2, step 2842/7134 completed (loss: 0.12315502762794495, acc: 0.9791666865348816)
[2025-02-13 20:41:40,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:40,993][root][INFO] - Training Epoch: 2/2, step 2843/7134 completed (loss: 0.037696968764066696, acc: 0.9937106966972351)
[2025-02-13 20:41:41,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:41,372][root][INFO] - Training Epoch: 2/2, step 2844/7134 completed (loss: 0.1612221747636795, acc: 0.9728260636329651)
[2025-02-13 20:41:41,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:41,729][root][INFO] - Training Epoch: 2/2, step 2845/7134 completed (loss: 0.11362165957689285, acc: 0.9797297120094299)
[2025-02-13 20:41:41,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:42,102][root][INFO] - Training Epoch: 2/2, step 2846/7134 completed (loss: 0.10062415897846222, acc: 0.9738562107086182)
[2025-02-13 20:41:42,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:42,487][root][INFO] - Training Epoch: 2/2, step 2847/7134 completed (loss: 0.07664553076028824, acc: 0.993630588054657)
[2025-02-13 20:41:42,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:42,865][root][INFO] - Training Epoch: 2/2, step 2848/7134 completed (loss: 0.11576028168201447, acc: 0.9816513657569885)
[2025-02-13 20:41:42,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:43,227][root][INFO] - Training Epoch: 2/2, step 2849/7134 completed (loss: 0.14180396497249603, acc: 0.9657142758369446)
[2025-02-13 20:41:43,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:43,611][root][INFO] - Training Epoch: 2/2, step 2850/7134 completed (loss: 0.18076002597808838, acc: 0.9572649598121643)
[2025-02-13 20:41:43,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44,008][root][INFO] - Training Epoch: 2/2, step 2851/7134 completed (loss: 0.09414949268102646, acc: 0.970370352268219)
[2025-02-13 20:41:44,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44,379][root][INFO] - Training Epoch: 2/2, step 2852/7134 completed (loss: 0.0877164974808693, acc: 0.9837398529052734)
[2025-02-13 20:41:44,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44,722][root][INFO] - Training Epoch: 2/2, step 2853/7134 completed (loss: 0.06768559664487839, acc: 0.9795918464660645)
[2025-02-13 20:41:44,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:45,095][root][INFO] - Training Epoch: 2/2, step 2854/7134 completed (loss: 0.09037677943706512, acc: 0.9659863710403442)
[2025-02-13 20:41:45,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:45,516][root][INFO] - Training Epoch: 2/2, step 2855/7134 completed (loss: 0.09823737293481827, acc: 0.9662162065505981)
[2025-02-13 20:41:45,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:45,900][root][INFO] - Training Epoch: 2/2, step 2856/7134 completed (loss: 0.03723720833659172, acc: 0.9937888383865356)
[2025-02-13 20:41:46,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:46,279][root][INFO] - Training Epoch: 2/2, step 2857/7134 completed (loss: 0.07865642756223679, acc: 0.9880239367485046)
[2025-02-13 20:41:46,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:46,629][root][INFO] - Training Epoch: 2/2, step 2858/7134 completed (loss: 0.04139072075486183, acc: 0.9848484992980957)
[2025-02-13 20:41:46,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:46,998][root][INFO] - Training Epoch: 2/2, step 2859/7134 completed (loss: 0.1710330694913864, acc: 0.9464285969734192)
[2025-02-13 20:41:47,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:47,357][root][INFO] - Training Epoch: 2/2, step 2860/7134 completed (loss: 0.13254691660404205, acc: 0.9765625)
[2025-02-13 20:41:47,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:47,716][root][INFO] - Training Epoch: 2/2, step 2861/7134 completed (loss: 0.06417020410299301, acc: 0.9821428656578064)
[2025-02-13 20:41:47,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:48,082][root][INFO] - Training Epoch: 2/2, step 2862/7134 completed (loss: 0.07896813750267029, acc: 0.9849624037742615)
[2025-02-13 20:41:48,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:48,467][root][INFO] - Training Epoch: 2/2, step 2863/7134 completed (loss: 0.02602205239236355, acc: 1.0)
[2025-02-13 20:41:48,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:48,844][root][INFO] - Training Epoch: 2/2, step 2864/7134 completed (loss: 0.10013896226882935, acc: 0.9789473414421082)
[2025-02-13 20:41:48,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:49,220][root][INFO] - Training Epoch: 2/2, step 2865/7134 completed (loss: 0.04176894947886467, acc: 0.991304337978363)
[2025-02-13 20:41:49,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:49,586][root][INFO] - Training Epoch: 2/2, step 2866/7134 completed (loss: 0.06410785764455795, acc: 0.9870967864990234)
[2025-02-13 20:41:49,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:49,959][root][INFO] - Training Epoch: 2/2, step 2867/7134 completed (loss: 0.034713014960289, acc: 0.9876543283462524)
[2025-02-13 20:41:50,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:50,338][root][INFO] - Training Epoch: 2/2, step 2868/7134 completed (loss: 0.01982731744647026, acc: 1.0)
[2025-02-13 20:41:50,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:50,699][root][INFO] - Training Epoch: 2/2, step 2869/7134 completed (loss: 0.1495843380689621, acc: 0.9793103337287903)
[2025-02-13 20:41:50,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:51,080][root][INFO] - Training Epoch: 2/2, step 2870/7134 completed (loss: 0.2583758234977722, acc: 0.9636363387107849)
[2025-02-13 20:41:51,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:51,433][root][INFO] - Training Epoch: 2/2, step 2871/7134 completed (loss: 0.09947335720062256, acc: 0.9862068891525269)
[2025-02-13 20:41:51,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:51,856][root][INFO] - Training Epoch: 2/2, step 2872/7134 completed (loss: 0.03411749377846718, acc: 0.991150438785553)
[2025-02-13 20:41:51,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:52,234][root][INFO] - Training Epoch: 2/2, step 2873/7134 completed (loss: 0.02839590236544609, acc: 1.0)
[2025-02-13 20:41:52,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:52,624][root][INFO] - Training Epoch: 2/2, step 2874/7134 completed (loss: 0.07125288248062134, acc: 0.9791666865348816)
[2025-02-13 20:41:52,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:52,997][root][INFO] - Training Epoch: 2/2, step 2875/7134 completed (loss: 0.14812219142913818, acc: 0.9464285969734192)
[2025-02-13 20:41:53,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:53,357][root][INFO] - Training Epoch: 2/2, step 2876/7134 completed (loss: 0.25319328904151917, acc: 0.9339622855186462)
[2025-02-13 20:41:53,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:53,712][root][INFO] - Training Epoch: 2/2, step 2877/7134 completed (loss: 0.04953429102897644, acc: 0.9914529919624329)
[2025-02-13 20:41:53,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:54,071][root][INFO] - Training Epoch: 2/2, step 2878/7134 completed (loss: 0.058548346161842346, acc: 0.9912280440330505)
[2025-02-13 20:41:54,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:54,463][root][INFO] - Training Epoch: 2/2, step 2879/7134 completed (loss: 0.038391903042793274, acc: 0.9931972622871399)
[2025-02-13 20:41:54,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:54,847][root][INFO] - Training Epoch: 2/2, step 2880/7134 completed (loss: 0.014415242709219456, acc: 1.0)
[2025-02-13 20:41:54,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:55,197][root][INFO] - Training Epoch: 2/2, step 2881/7134 completed (loss: 0.058750420808792114, acc: 0.9841269850730896)
[2025-02-13 20:41:55,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:55,584][root][INFO] - Training Epoch: 2/2, step 2882/7134 completed (loss: 0.18314500153064728, acc: 0.9537572264671326)
[2025-02-13 20:41:55,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:55,976][root][INFO] - Training Epoch: 2/2, step 2883/7134 completed (loss: 0.14536640048027039, acc: 0.9580838084220886)
[2025-02-13 20:41:56,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:56,344][root][INFO] - Training Epoch: 2/2, step 2884/7134 completed (loss: 0.17439983785152435, acc: 0.9454545378684998)
[2025-02-13 20:41:56,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:56,717][root][INFO] - Training Epoch: 2/2, step 2885/7134 completed (loss: 0.2661910653114319, acc: 0.9151515364646912)
[2025-02-13 20:41:56,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:57,093][root][INFO] - Training Epoch: 2/2, step 2886/7134 completed (loss: 0.1849725842475891, acc: 0.9529411792755127)
[2025-02-13 20:41:57,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:57,474][root][INFO] - Training Epoch: 2/2, step 2887/7134 completed (loss: 0.18398834764957428, acc: 0.9563318490982056)
[2025-02-13 20:41:57,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:57,861][root][INFO] - Training Epoch: 2/2, step 2888/7134 completed (loss: 0.12355374544858932, acc: 0.9631578922271729)
[2025-02-13 20:41:58,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:58,231][root][INFO] - Training Epoch: 2/2, step 2889/7134 completed (loss: 0.10165154933929443, acc: 0.9720670580863953)
[2025-02-13 20:41:58,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:58,604][root][INFO] - Training Epoch: 2/2, step 2890/7134 completed (loss: 0.1185959130525589, acc: 0.9692307710647583)
[2025-02-13 20:41:58,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:58,945][root][INFO] - Training Epoch: 2/2, step 2891/7134 completed (loss: 0.05780480057001114, acc: 0.970370352268219)
[2025-02-13 20:41:59,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:59,315][root][INFO] - Training Epoch: 2/2, step 2892/7134 completed (loss: 0.12115829437971115, acc: 0.9526627063751221)
[2025-02-13 20:41:59,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:59,709][root][INFO] - Training Epoch: 2/2, step 2893/7134 completed (loss: 0.13383041322231293, acc: 0.9672130942344666)
[2025-02-13 20:41:59,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:00,096][root][INFO] - Training Epoch: 2/2, step 2894/7134 completed (loss: 0.1894632875919342, acc: 0.9481481313705444)
[2025-02-13 20:42:00,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:00,478][root][INFO] - Training Epoch: 2/2, step 2895/7134 completed (loss: 0.04892369359731674, acc: 0.9745762944221497)
[2025-02-13 20:42:00,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:00,852][root][INFO] - Training Epoch: 2/2, step 2896/7134 completed (loss: 0.12700895965099335, acc: 0.9820359349250793)
[2025-02-13 20:42:00,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:01,233][root][INFO] - Training Epoch: 2/2, step 2897/7134 completed (loss: 0.19591893255710602, acc: 0.969924807548523)
[2025-02-13 20:42:01,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:01,614][root][INFO] - Training Epoch: 2/2, step 2898/7134 completed (loss: 0.11141636967658997, acc: 0.97826087474823)
[2025-02-13 20:42:01,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:02,008][root][INFO] - Training Epoch: 2/2, step 2899/7134 completed (loss: 0.17728564143180847, acc: 0.9520958065986633)
[2025-02-13 20:42:02,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:02,402][root][INFO] - Training Epoch: 2/2, step 2900/7134 completed (loss: 0.08493853360414505, acc: 0.9893617033958435)
[2025-02-13 20:42:02,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:02,785][root][INFO] - Training Epoch: 2/2, step 2901/7134 completed (loss: 0.07094654440879822, acc: 0.9864864945411682)
[2025-02-13 20:42:02,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:03,145][root][INFO] - Training Epoch: 2/2, step 2902/7134 completed (loss: 0.07259742170572281, acc: 0.9829059839248657)
[2025-02-13 20:42:03,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:03,527][root][INFO] - Training Epoch: 2/2, step 2903/7134 completed (loss: 0.05102632939815521, acc: 0.9943820238113403)
[2025-02-13 20:42:03,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:03,912][root][INFO] - Training Epoch: 2/2, step 2904/7134 completed (loss: 0.025468377396464348, acc: 0.9945945739746094)
[2025-02-13 20:42:04,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:04,312][root][INFO] - Training Epoch: 2/2, step 2905/7134 completed (loss: 0.08353831619024277, acc: 0.9732620120048523)
[2025-02-13 20:42:04,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:04,682][root][INFO] - Training Epoch: 2/2, step 2906/7134 completed (loss: 0.07413029670715332, acc: 0.9842932224273682)
[2025-02-13 20:42:04,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:05,027][root][INFO] - Training Epoch: 2/2, step 2907/7134 completed (loss: 0.0698765441775322, acc: 0.976047933101654)
[2025-02-13 20:42:05,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:05,396][root][INFO] - Training Epoch: 2/2, step 2908/7134 completed (loss: 0.09085515886545181, acc: 0.977011501789093)
[2025-02-13 20:42:05,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:05,764][root][INFO] - Training Epoch: 2/2, step 2909/7134 completed (loss: 0.05075974389910698, acc: 0.9890710115432739)
[2025-02-13 20:42:05,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:06,146][root][INFO] - Training Epoch: 2/2, step 2910/7134 completed (loss: 0.07410064339637756, acc: 0.9839572310447693)
[2025-02-13 20:42:06,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:06,537][root][INFO] - Training Epoch: 2/2, step 2911/7134 completed (loss: 0.11769698560237885, acc: 0.9650349617004395)
[2025-02-13 20:42:06,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:06,898][root][INFO] - Training Epoch: 2/2, step 2912/7134 completed (loss: 0.030746104195713997, acc: 1.0)
[2025-02-13 20:42:07,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:07,272][root][INFO] - Training Epoch: 2/2, step 2913/7134 completed (loss: 0.11173039674758911, acc: 0.971222996711731)
[2025-02-13 20:42:07,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:07,645][root][INFO] - Training Epoch: 2/2, step 2914/7134 completed (loss: 0.04898791387677193, acc: 1.0)
[2025-02-13 20:42:07,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:08,013][root][INFO] - Training Epoch: 2/2, step 2915/7134 completed (loss: 0.3077809810638428, acc: 0.931034505367279)
[2025-02-13 20:42:08,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:08,389][root][INFO] - Training Epoch: 2/2, step 2916/7134 completed (loss: 0.04319697245955467, acc: 1.0)
[2025-02-13 20:42:08,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:08,767][root][INFO] - Training Epoch: 2/2, step 2917/7134 completed (loss: 0.03607220947742462, acc: 0.9912280440330505)
[2025-02-13 20:42:08,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:09,129][root][INFO] - Training Epoch: 2/2, step 2918/7134 completed (loss: 0.07853047549724579, acc: 0.9824561476707458)
[2025-02-13 20:42:09,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:09,483][root][INFO] - Training Epoch: 2/2, step 2919/7134 completed (loss: 0.08820755779743195, acc: 0.9698795080184937)
[2025-02-13 20:42:09,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:09,869][root][INFO] - Training Epoch: 2/2, step 2920/7134 completed (loss: 0.06676594167947769, acc: 0.9811320900917053)
[2025-02-13 20:42:10,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:10,250][root][INFO] - Training Epoch: 2/2, step 2921/7134 completed (loss: 0.06338878720998764, acc: 0.9857142567634583)
[2025-02-13 20:42:10,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:10,609][root][INFO] - Training Epoch: 2/2, step 2922/7134 completed (loss: 0.21491728723049164, acc: 0.9693251252174377)
[2025-02-13 20:42:10,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:10,973][root][INFO] - Training Epoch: 2/2, step 2923/7134 completed (loss: 0.364265114068985, acc: 0.9436619877815247)
[2025-02-13 20:42:11,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:11,363][root][INFO] - Training Epoch: 2/2, step 2924/7134 completed (loss: 0.10249578952789307, acc: 0.9878048896789551)
[2025-02-13 20:42:11,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:11,743][root][INFO] - Training Epoch: 2/2, step 2925/7134 completed (loss: 0.05148661509156227, acc: 0.9941176176071167)
[2025-02-13 20:42:11,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:12,128][root][INFO] - Training Epoch: 2/2, step 2926/7134 completed (loss: 0.2582254409790039, acc: 0.9698492288589478)
[2025-02-13 20:42:12,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:12,518][root][INFO] - Training Epoch: 2/2, step 2927/7134 completed (loss: 0.0723271518945694, acc: 0.9826589822769165)
[2025-02-13 20:42:12,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:12,906][root][INFO] - Training Epoch: 2/2, step 2928/7134 completed (loss: 0.15457528829574585, acc: 0.9652777910232544)
[2025-02-13 20:42:13,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:13,286][root][INFO] - Training Epoch: 2/2, step 2929/7134 completed (loss: 0.05034996196627617, acc: 0.9940476417541504)
[2025-02-13 20:42:13,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:13,653][root][INFO] - Training Epoch: 2/2, step 2930/7134 completed (loss: 0.04924982786178589, acc: 0.9931972622871399)
[2025-02-13 20:42:13,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:14,042][root][INFO] - Training Epoch: 2/2, step 2931/7134 completed (loss: 0.19644270837306976, acc: 0.9358974099159241)
[2025-02-13 20:42:14,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:14,480][root][INFO] - Training Epoch: 2/2, step 2932/7134 completed (loss: 0.04613376408815384, acc: 0.9946523904800415)
[2025-02-13 20:42:14,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:14,864][root][INFO] - Training Epoch: 2/2, step 2933/7134 completed (loss: 0.07057465612888336, acc: 0.9781022071838379)
[2025-02-13 20:42:15,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:15,240][root][INFO] - Training Epoch: 2/2, step 2934/7134 completed (loss: 0.04039740189909935, acc: 0.9928571581840515)
[2025-02-13 20:42:15,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:15,621][root][INFO] - Training Epoch: 2/2, step 2935/7134 completed (loss: 0.024732772260904312, acc: 1.0)
[2025-02-13 20:42:15,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:16,019][root][INFO] - Training Epoch: 2/2, step 2936/7134 completed (loss: 0.10673502832651138, acc: 0.9778761267662048)
[2025-02-13 20:42:16,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:16,399][root][INFO] - Training Epoch: 2/2, step 2937/7134 completed (loss: 0.10422220826148987, acc: 0.970588207244873)
[2025-02-13 20:42:16,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:16,761][root][INFO] - Training Epoch: 2/2, step 2938/7134 completed (loss: 0.0880543515086174, acc: 0.9672130942344666)
[2025-02-13 20:42:16,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:17,133][root][INFO] - Training Epoch: 2/2, step 2939/7134 completed (loss: 0.06880128383636475, acc: 0.9867549538612366)
[2025-02-13 20:42:17,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:17,499][root][INFO] - Training Epoch: 2/2, step 2940/7134 completed (loss: 0.12132430821657181, acc: 0.9617486596107483)
[2025-02-13 20:42:17,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:17,868][root][INFO] - Training Epoch: 2/2, step 2941/7134 completed (loss: 0.09997604787349701, acc: 0.9719101190567017)
[2025-02-13 20:42:17,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:18,219][root][INFO] - Training Epoch: 2/2, step 2942/7134 completed (loss: 0.07946613430976868, acc: 0.9878787994384766)
[2025-02-13 20:42:18,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:18,615][root][INFO] - Training Epoch: 2/2, step 2943/7134 completed (loss: 0.08828337490558624, acc: 0.9864864945411682)
[2025-02-13 20:42:18,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:18,983][root][INFO] - Training Epoch: 2/2, step 2944/7134 completed (loss: 0.10109400749206543, acc: 0.9698795080184937)
[2025-02-13 20:42:19,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:19,373][root][INFO] - Training Epoch: 2/2, step 2945/7134 completed (loss: 0.11582259088754654, acc: 0.9731543660163879)
[2025-02-13 20:42:19,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:19,713][root][INFO] - Training Epoch: 2/2, step 2946/7134 completed (loss: 0.14207379519939423, acc: 0.9629629850387573)
[2025-02-13 20:42:19,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:20,075][root][INFO] - Training Epoch: 2/2, step 2947/7134 completed (loss: 0.12996654212474823, acc: 0.956250011920929)
[2025-02-13 20:42:20,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:20,451][root][INFO] - Training Epoch: 2/2, step 2948/7134 completed (loss: 0.0875818058848381, acc: 0.9861111044883728)
[2025-02-13 20:42:20,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:20,807][root][INFO] - Training Epoch: 2/2, step 2949/7134 completed (loss: 0.060585394501686096, acc: 0.9869281053543091)
[2025-02-13 20:42:20,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:21,178][root][INFO] - Training Epoch: 2/2, step 2950/7134 completed (loss: 0.08224798738956451, acc: 0.9776536226272583)
[2025-02-13 20:42:21,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:21,586][root][INFO] - Training Epoch: 2/2, step 2951/7134 completed (loss: 0.06547588109970093, acc: 0.9813664555549622)
[2025-02-13 20:42:21,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:21,989][root][INFO] - Training Epoch: 2/2, step 2952/7134 completed (loss: 0.03561496362090111, acc: 0.987261176109314)
[2025-02-13 20:42:22,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:22,382][root][INFO] - Training Epoch: 2/2, step 2953/7134 completed (loss: 0.06536003947257996, acc: 0.9795918464660645)
[2025-02-13 20:42:22,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:22,810][root][INFO] - Training Epoch: 2/2, step 2954/7134 completed (loss: 0.08249630779027939, acc: 0.9742268323898315)
[2025-02-13 20:42:22,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:23,198][root][INFO] - Training Epoch: 2/2, step 2955/7134 completed (loss: 0.057039473205804825, acc: 0.9939393997192383)
[2025-02-13 20:42:23,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:23,583][root][INFO] - Training Epoch: 2/2, step 2956/7134 completed (loss: 0.058415092527866364, acc: 0.9815950989723206)
[2025-02-13 20:42:23,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:23,954][root][INFO] - Training Epoch: 2/2, step 2957/7134 completed (loss: 0.04451214149594307, acc: 0.9942528605461121)
[2025-02-13 20:42:24,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:24,350][root][INFO] - Training Epoch: 2/2, step 2958/7134 completed (loss: 0.031058818101882935, acc: 1.0)
[2025-02-13 20:42:24,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:24,733][root][INFO] - Training Epoch: 2/2, step 2959/7134 completed (loss: 0.018084200099110603, acc: 1.0)
[2025-02-13 20:42:24,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:25,099][root][INFO] - Training Epoch: 2/2, step 2960/7134 completed (loss: 0.061993956565856934, acc: 0.9835164546966553)
[2025-02-13 20:42:25,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:25,471][root][INFO] - Training Epoch: 2/2, step 2961/7134 completed (loss: 0.04622156172990799, acc: 0.9820359349250793)
[2025-02-13 20:42:25,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:25,827][root][INFO] - Training Epoch: 2/2, step 2962/7134 completed (loss: 0.034557051956653595, acc: 0.9940476417541504)
[2025-02-13 20:42:25,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:26,193][root][INFO] - Training Epoch: 2/2, step 2963/7134 completed (loss: 0.05817480385303497, acc: 0.9832402467727661)
[2025-02-13 20:42:26,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:26,547][root][INFO] - Training Epoch: 2/2, step 2964/7134 completed (loss: 0.026705602183938026, acc: 0.9933333396911621)
[2025-02-13 20:42:26,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:26,904][root][INFO] - Training Epoch: 2/2, step 2965/7134 completed (loss: 0.08325305581092834, acc: 0.9943181872367859)
[2025-02-13 20:42:27,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:27,266][root][INFO] - Training Epoch: 2/2, step 2966/7134 completed (loss: 0.007791853044182062, acc: 1.0)
[2025-02-13 20:42:27,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:27,628][root][INFO] - Training Epoch: 2/2, step 2967/7134 completed (loss: 0.18927960097789764, acc: 0.9398906826972961)
[2025-02-13 20:42:27,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:27,997][root][INFO] - Training Epoch: 2/2, step 2968/7134 completed (loss: 0.2560245990753174, acc: 0.918749988079071)
[2025-02-13 20:42:28,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:28,353][root][INFO] - Training Epoch: 2/2, step 2969/7134 completed (loss: 0.8125903010368347, acc: 0.8495145440101624)
[2025-02-13 20:42:28,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:28,708][root][INFO] - Training Epoch: 2/2, step 2970/7134 completed (loss: 0.6488251686096191, acc: 0.858208954334259)
[2025-02-13 20:42:28,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:29,083][root][INFO] - Training Epoch: 2/2, step 2971/7134 completed (loss: 0.12752003967761993, acc: 0.9668508172035217)
[2025-02-13 20:42:29,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:29,445][root][INFO] - Training Epoch: 2/2, step 2972/7134 completed (loss: 0.1716933399438858, acc: 0.9466666579246521)
[2025-02-13 20:42:29,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:29,807][root][INFO] - Training Epoch: 2/2, step 2973/7134 completed (loss: 0.09321867674589157, acc: 0.9757575988769531)
[2025-02-13 20:42:29,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:30,170][root][INFO] - Training Epoch: 2/2, step 2974/7134 completed (loss: 0.13295213878154755, acc: 0.9590643048286438)
[2025-02-13 20:42:30,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:30,570][root][INFO] - Training Epoch: 2/2, step 2975/7134 completed (loss: 0.11858828365802765, acc: 0.9593023061752319)
[2025-02-13 20:42:30,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:30,951][root][INFO] - Training Epoch: 2/2, step 2976/7134 completed (loss: 0.08594230562448502, acc: 0.9811320900917053)
[2025-02-13 20:42:31,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:31,301][root][INFO] - Training Epoch: 2/2, step 2977/7134 completed (loss: 0.07720582187175751, acc: 0.9793103337287903)
[2025-02-13 20:42:31,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:31,685][root][INFO] - Training Epoch: 2/2, step 2978/7134 completed (loss: 0.13096977770328522, acc: 0.9518072009086609)
[2025-02-13 20:42:31,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:32,039][root][INFO] - Training Epoch: 2/2, step 2979/7134 completed (loss: 0.11240192502737045, acc: 0.9655172228813171)
[2025-02-13 20:42:32,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:32,408][root][INFO] - Training Epoch: 2/2, step 2980/7134 completed (loss: 0.1586868017911911, acc: 0.9714285731315613)
[2025-02-13 20:42:32,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:32,787][root][INFO] - Training Epoch: 2/2, step 2981/7134 completed (loss: 0.1309066265821457, acc: 0.9655172228813171)
[2025-02-13 20:42:32,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:33,146][root][INFO] - Training Epoch: 2/2, step 2982/7134 completed (loss: 0.2093685269355774, acc: 0.9395604133605957)
[2025-02-13 20:42:33,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:33,503][root][INFO] - Training Epoch: 2/2, step 2983/7134 completed (loss: 0.11203628033399582, acc: 0.9631901979446411)
[2025-02-13 20:42:33,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:33,878][root][INFO] - Training Epoch: 2/2, step 2984/7134 completed (loss: 0.1887909173965454, acc: 0.9395604133605957)
[2025-02-13 20:42:34,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:34,271][root][INFO] - Training Epoch: 2/2, step 2985/7134 completed (loss: 0.06972409039735794, acc: 0.9804878234863281)
[2025-02-13 20:42:34,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:34,657][root][INFO] - Training Epoch: 2/2, step 2986/7134 completed (loss: 0.11132415384054184, acc: 0.9742268323898315)
[2025-02-13 20:42:34,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:34,997][root][INFO] - Training Epoch: 2/2, step 2987/7134 completed (loss: 0.12072545289993286, acc: 0.97826087474823)
[2025-02-13 20:42:35,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:35,369][root][INFO] - Training Epoch: 2/2, step 2988/7134 completed (loss: 0.11996220797300339, acc: 0.9675675630569458)
[2025-02-13 20:42:35,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:35,784][root][INFO] - Training Epoch: 2/2, step 2989/7134 completed (loss: 0.0910428985953331, acc: 0.9820359349250793)
[2025-02-13 20:42:35,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:36,166][root][INFO] - Training Epoch: 2/2, step 2990/7134 completed (loss: 0.26536205410957336, acc: 0.9527027010917664)
[2025-02-13 20:42:36,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:36,548][root][INFO] - Training Epoch: 2/2, step 2991/7134 completed (loss: 0.36340251564979553, acc: 0.90625)
[2025-02-13 20:42:36,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:36,917][root][INFO] - Training Epoch: 2/2, step 2992/7134 completed (loss: 0.19226230680942535, acc: 0.9496855139732361)
[2025-02-13 20:42:37,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:37,301][root][INFO] - Training Epoch: 2/2, step 2993/7134 completed (loss: 0.08903768658638, acc: 0.9648241400718689)
[2025-02-13 20:42:37,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:37,655][root][INFO] - Training Epoch: 2/2, step 2994/7134 completed (loss: 0.03902081400156021, acc: 0.9920634627342224)
[2025-02-13 20:42:37,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:38,037][root][INFO] - Training Epoch: 2/2, step 2995/7134 completed (loss: 0.07310471683740616, acc: 0.9805825352668762)
[2025-02-13 20:42:38,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:38,416][root][INFO] - Training Epoch: 2/2, step 2996/7134 completed (loss: 0.09424750506877899, acc: 0.9639175534248352)
[2025-02-13 20:42:38,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:38,813][root][INFO] - Training Epoch: 2/2, step 2997/7134 completed (loss: 0.04229193925857544, acc: 0.9858490824699402)
[2025-02-13 20:42:38,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:39,173][root][INFO] - Training Epoch: 2/2, step 2998/7134 completed (loss: 0.0661068931221962, acc: 0.9940476417541504)
[2025-02-13 20:42:39,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:39,572][root][INFO] - Training Epoch: 2/2, step 2999/7134 completed (loss: 0.07162636518478394, acc: 0.987500011920929)
[2025-02-13 20:42:39,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:39,941][root][INFO] - Training Epoch: 2/2, step 3000/7134 completed (loss: 0.1894669383764267, acc: 0.9398148059844971)
[2025-02-13 20:42:40,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:40,306][root][INFO] - Training Epoch: 2/2, step 3001/7134 completed (loss: 0.08392352610826492, acc: 0.9695431590080261)
[2025-02-13 20:42:40,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:40,682][root][INFO] - Training Epoch: 2/2, step 3002/7134 completed (loss: 0.1690673679113388, acc: 0.9723502397537231)
[2025-02-13 20:42:40,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:41,057][root][INFO] - Training Epoch: 2/2, step 3003/7134 completed (loss: 0.01729242503643036, acc: 0.9949238300323486)
[2025-02-13 20:42:41,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:41,421][root][INFO] - Training Epoch: 2/2, step 3004/7134 completed (loss: 0.015128086321055889, acc: 1.0)
[2025-02-13 20:42:41,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:41,799][root][INFO] - Training Epoch: 2/2, step 3005/7134 completed (loss: 0.022989628836512566, acc: 0.9948186278343201)
[2025-02-13 20:42:41,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:42,179][root][INFO] - Training Epoch: 2/2, step 3006/7134 completed (loss: 0.04641829803586006, acc: 0.9885714054107666)
[2025-02-13 20:42:42,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:42,571][root][INFO] - Training Epoch: 2/2, step 3007/7134 completed (loss: 0.06289174407720566, acc: 0.9883720874786377)
[2025-02-13 20:42:42,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:42,954][root][INFO] - Training Epoch: 2/2, step 3008/7134 completed (loss: 0.10102394968271255, acc: 0.9727891087532043)
[2025-02-13 20:42:43,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:43,309][root][INFO] - Training Epoch: 2/2, step 3009/7134 completed (loss: 0.12475553154945374, acc: 0.9702380895614624)
[2025-02-13 20:42:43,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:43,667][root][INFO] - Training Epoch: 2/2, step 3010/7134 completed (loss: 0.10237853229045868, acc: 0.9657142758369446)
[2025-02-13 20:42:43,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:44,024][root][INFO] - Training Epoch: 2/2, step 3011/7134 completed (loss: 0.36297187209129333, acc: 0.9151515364646912)
[2025-02-13 20:42:44,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:44,379][root][INFO] - Training Epoch: 2/2, step 3012/7134 completed (loss: 0.1624429076910019, acc: 0.971222996711731)
[2025-02-13 20:42:44,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:44,763][root][INFO] - Training Epoch: 2/2, step 3013/7134 completed (loss: 0.019209392368793488, acc: 0.9941860437393188)
[2025-02-13 20:42:44,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:45,147][root][INFO] - Training Epoch: 2/2, step 3014/7134 completed (loss: 0.07208050042390823, acc: 0.9810126423835754)
[2025-02-13 20:42:45,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:45,511][root][INFO] - Training Epoch: 2/2, step 3015/7134 completed (loss: 0.07607370615005493, acc: 0.9788732528686523)
[2025-02-13 20:42:45,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:45,901][root][INFO] - Training Epoch: 2/2, step 3016/7134 completed (loss: 0.06310727447271347, acc: 0.9794520735740662)
[2025-02-13 20:42:46,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:46,258][root][INFO] - Training Epoch: 2/2, step 3017/7134 completed (loss: 0.025359129533171654, acc: 0.9917355179786682)
[2025-02-13 20:42:46,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:46,613][root][INFO] - Training Epoch: 2/2, step 3018/7134 completed (loss: 0.048596207052469254, acc: 0.9847328066825867)
[2025-02-13 20:42:46,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:47,017][root][INFO] - Training Epoch: 2/2, step 3019/7134 completed (loss: 0.09011338651180267, acc: 0.9926470518112183)
[2025-02-13 20:42:47,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:47,419][root][INFO] - Training Epoch: 2/2, step 3020/7134 completed (loss: 0.03456299379467964, acc: 1.0)
[2025-02-13 20:42:47,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:47,771][root][INFO] - Training Epoch: 2/2, step 3021/7134 completed (loss: 0.03815889358520508, acc: 0.9924242496490479)
[2025-02-13 20:42:47,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:48,180][root][INFO] - Training Epoch: 2/2, step 3022/7134 completed (loss: 0.11493005603551865, acc: 0.9774436354637146)
[2025-02-13 20:42:48,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:48,550][root][INFO] - Training Epoch: 2/2, step 3023/7134 completed (loss: 0.07661541551351547, acc: 0.9769230484962463)
[2025-02-13 20:42:48,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:48,919][root][INFO] - Training Epoch: 2/2, step 3024/7134 completed (loss: 0.029320504516363144, acc: 1.0)
[2025-02-13 20:42:49,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:49,278][root][INFO] - Training Epoch: 2/2, step 3025/7134 completed (loss: 0.13991393148899078, acc: 0.9923076629638672)
[2025-02-13 20:42:49,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:49,634][root][INFO] - Training Epoch: 2/2, step 3026/7134 completed (loss: 0.12957437336444855, acc: 0.9590163826942444)
[2025-02-13 20:42:49,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:49,988][root][INFO] - Training Epoch: 2/2, step 3027/7134 completed (loss: 0.10991856455802917, acc: 0.9895833134651184)
[2025-02-13 20:42:50,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:50,329][root][INFO] - Training Epoch: 2/2, step 3028/7134 completed (loss: 0.021333932876586914, acc: 1.0)
[2025-02-13 20:42:50,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:50,681][root][INFO] - Training Epoch: 2/2, step 3029/7134 completed (loss: 0.12199592590332031, acc: 0.9863013625144958)
[2025-02-13 20:42:50,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:51,056][root][INFO] - Training Epoch: 2/2, step 3030/7134 completed (loss: 0.07033927738666534, acc: 0.9864864945411682)
[2025-02-13 20:42:51,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:51,403][root][INFO] - Training Epoch: 2/2, step 3031/7134 completed (loss: 0.04973249137401581, acc: 0.9797979593276978)
[2025-02-13 20:42:51,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:51,767][root][INFO] - Training Epoch: 2/2, step 3032/7134 completed (loss: 0.04530636966228485, acc: 0.9838709831237793)
[2025-02-13 20:42:51,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:52,124][root][INFO] - Training Epoch: 2/2, step 3033/7134 completed (loss: 0.06884017586708069, acc: 0.96875)
[2025-02-13 20:42:52,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:52,497][root][INFO] - Training Epoch: 2/2, step 3034/7134 completed (loss: 0.0657128393650055, acc: 0.9819819927215576)
[2025-02-13 20:42:52,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:52,848][root][INFO] - Training Epoch: 2/2, step 3035/7134 completed (loss: 0.05291600897908211, acc: 0.9931972622871399)
[2025-02-13 20:42:52,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:53,206][root][INFO] - Training Epoch: 2/2, step 3036/7134 completed (loss: 0.12820422649383545, acc: 0.9520000219345093)
[2025-02-13 20:42:53,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:53,572][root][INFO] - Training Epoch: 2/2, step 3037/7134 completed (loss: 0.09706682711839676, acc: 0.9731543660163879)
[2025-02-13 20:42:53,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:53,937][root][INFO] - Training Epoch: 2/2, step 3038/7134 completed (loss: 0.08060171455144882, acc: 0.9850746393203735)
[2025-02-13 20:42:54,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:54,293][root][INFO] - Training Epoch: 2/2, step 3039/7134 completed (loss: 0.05860641226172447, acc: 0.9864864945411682)
[2025-02-13 20:42:54,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:54,644][root][INFO] - Training Epoch: 2/2, step 3040/7134 completed (loss: 0.08216749131679535, acc: 0.9844961166381836)
[2025-02-13 20:42:54,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:55,004][root][INFO] - Training Epoch: 2/2, step 3041/7134 completed (loss: 0.1879405826330185, acc: 0.9642857313156128)
[2025-02-13 20:42:55,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:55,359][root][INFO] - Training Epoch: 2/2, step 3042/7134 completed (loss: 0.056723807007074356, acc: 0.9871794581413269)
[2025-02-13 20:42:55,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:55,705][root][INFO] - Training Epoch: 2/2, step 3043/7134 completed (loss: 0.04121912270784378, acc: 0.9821428656578064)
[2025-02-13 20:42:55,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:56,052][root][INFO] - Training Epoch: 2/2, step 3044/7134 completed (loss: 0.0488600991666317, acc: 0.9924812316894531)
[2025-02-13 20:42:56,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:56,399][root][INFO] - Training Epoch: 2/2, step 3045/7134 completed (loss: 0.02384508214890957, acc: 0.9922480583190918)
[2025-02-13 20:42:56,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:56,773][root][INFO] - Training Epoch: 2/2, step 3046/7134 completed (loss: 0.07799731194972992, acc: 0.9822485446929932)
[2025-02-13 20:42:56,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:57,146][root][INFO] - Training Epoch: 2/2, step 3047/7134 completed (loss: 0.09371477365493774, acc: 0.9717513918876648)
[2025-02-13 20:42:57,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:57,553][root][INFO] - Training Epoch: 2/2, step 3048/7134 completed (loss: 0.07048409432172775, acc: 0.9736841917037964)
[2025-02-13 20:42:57,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:57,902][root][INFO] - Training Epoch: 2/2, step 3049/7134 completed (loss: 0.06348291784524918, acc: 0.981249988079071)
[2025-02-13 20:42:58,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:58,267][root][INFO] - Training Epoch: 2/2, step 3050/7134 completed (loss: 0.08162350207567215, acc: 0.9796954393386841)
[2025-02-13 20:42:58,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:58,620][root][INFO] - Training Epoch: 2/2, step 3051/7134 completed (loss: 0.10236531496047974, acc: 0.9642857313156128)
[2025-02-13 20:42:58,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:58,979][root][INFO] - Training Epoch: 2/2, step 3052/7134 completed (loss: 0.06860852986574173, acc: 0.9783783555030823)
[2025-02-13 20:42:59,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:59,335][root][INFO] - Training Epoch: 2/2, step 3053/7134 completed (loss: 0.03456909582018852, acc: 0.9950248599052429)
[2025-02-13 20:42:59,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:59,706][root][INFO] - Training Epoch: 2/2, step 3054/7134 completed (loss: 0.032536689192056656, acc: 0.9946808218955994)
[2025-02-13 20:42:59,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:00,072][root][INFO] - Training Epoch: 2/2, step 3055/7134 completed (loss: 0.07659522444009781, acc: 0.9864864945411682)
[2025-02-13 20:43:00,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:00,446][root][INFO] - Training Epoch: 2/2, step 3056/7134 completed (loss: 0.05049964785575867, acc: 0.9945945739746094)
[2025-02-13 20:43:00,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:00,816][root][INFO] - Training Epoch: 2/2, step 3057/7134 completed (loss: 0.05464646965265274, acc: 0.9815950989723206)
[2025-02-13 20:43:00,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:01,166][root][INFO] - Training Epoch: 2/2, step 3058/7134 completed (loss: 0.09826017916202545, acc: 0.9736841917037964)
[2025-02-13 20:43:01,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:01,522][root][INFO] - Training Epoch: 2/2, step 3059/7134 completed (loss: 0.06298530101776123, acc: 0.9885714054107666)
[2025-02-13 20:43:01,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:01,889][root][INFO] - Training Epoch: 2/2, step 3060/7134 completed (loss: 0.08808556199073792, acc: 0.9685863852500916)
[2025-02-13 20:43:02,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:02,238][root][INFO] - Training Epoch: 2/2, step 3061/7134 completed (loss: 0.06527305394411087, acc: 0.9750000238418579)
[2025-02-13 20:43:02,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:02,597][root][INFO] - Training Epoch: 2/2, step 3062/7134 completed (loss: 0.027524052187800407, acc: 0.9937106966972351)
[2025-02-13 20:43:02,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:02,948][root][INFO] - Training Epoch: 2/2, step 3063/7134 completed (loss: 0.07489601522684097, acc: 0.9659090638160706)
[2025-02-13 20:43:03,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:03,307][root][INFO] - Training Epoch: 2/2, step 3064/7134 completed (loss: 0.02964601293206215, acc: 1.0)
[2025-02-13 20:43:03,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:03,669][root][INFO] - Training Epoch: 2/2, step 3065/7134 completed (loss: 0.015975186601281166, acc: 1.0)
[2025-02-13 20:43:03,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:04,032][root][INFO] - Training Epoch: 2/2, step 3066/7134 completed (loss: 0.02340780384838581, acc: 1.0)
[2025-02-13 20:43:04,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:04,425][root][INFO] - Training Epoch: 2/2, step 3067/7134 completed (loss: 0.016865219920873642, acc: 1.0)
[2025-02-13 20:43:04,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:04,794][root][INFO] - Training Epoch: 2/2, step 3068/7134 completed (loss: 0.11748023331165314, acc: 0.9553072452545166)
[2025-02-13 20:43:04,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:05,145][root][INFO] - Training Epoch: 2/2, step 3069/7134 completed (loss: 0.013390569016337395, acc: 1.0)
[2025-02-13 20:43:05,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:05,521][root][INFO] - Training Epoch: 2/2, step 3070/7134 completed (loss: 0.02217162773013115, acc: 0.9952152967453003)
[2025-02-13 20:43:05,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:05,888][root][INFO] - Training Epoch: 2/2, step 3071/7134 completed (loss: 0.056983958929777145, acc: 0.9833333492279053)
[2025-02-13 20:43:06,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:06,252][root][INFO] - Training Epoch: 2/2, step 3072/7134 completed (loss: 0.04523155093193054, acc: 0.98591548204422)
[2025-02-13 20:43:06,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:06,594][root][INFO] - Training Epoch: 2/2, step 3073/7134 completed (loss: 0.02625887840986252, acc: 0.9887005686759949)
[2025-02-13 20:43:06,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:06,949][root][INFO] - Training Epoch: 2/2, step 3074/7134 completed (loss: 0.025540702044963837, acc: 0.9933775067329407)
[2025-02-13 20:43:07,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:07,293][root][INFO] - Training Epoch: 2/2, step 3075/7134 completed (loss: 0.036643173545598984, acc: 0.9938271641731262)
[2025-02-13 20:43:07,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:07,648][root][INFO] - Training Epoch: 2/2, step 3076/7134 completed (loss: 0.037238623946905136, acc: 0.9946236610412598)
[2025-02-13 20:43:07,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:08,016][root][INFO] - Training Epoch: 2/2, step 3077/7134 completed (loss: 0.12635141611099243, acc: 0.963350772857666)
[2025-02-13 20:43:08,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:08,388][root][INFO] - Training Epoch: 2/2, step 3078/7134 completed (loss: 0.10833939909934998, acc: 0.9635416865348816)
[2025-02-13 20:43:08,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:08,754][root][INFO] - Training Epoch: 2/2, step 3079/7134 completed (loss: 0.08634401857852936, acc: 0.9735449552536011)
[2025-02-13 20:43:08,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:09,133][root][INFO] - Training Epoch: 2/2, step 3080/7134 completed (loss: 0.12710648775100708, acc: 0.9537037014961243)
[2025-02-13 20:43:09,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:09,477][root][INFO] - Training Epoch: 2/2, step 3081/7134 completed (loss: 0.04989492893218994, acc: 0.9900990128517151)
[2025-02-13 20:43:09,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:09,831][root][INFO] - Training Epoch: 2/2, step 3082/7134 completed (loss: 0.1900969296693802, acc: 0.9601989984512329)
[2025-02-13 20:43:09,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:10,191][root][INFO] - Training Epoch: 2/2, step 3083/7134 completed (loss: 0.07296621799468994, acc: 0.9675675630569458)
[2025-02-13 20:43:10,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:10,546][root][INFO] - Training Epoch: 2/2, step 3084/7134 completed (loss: 0.0898367315530777, acc: 0.9685863852500916)
[2025-02-13 20:43:10,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:10,903][root][INFO] - Training Epoch: 2/2, step 3085/7134 completed (loss: 0.15970996022224426, acc: 0.9548386931419373)
[2025-02-13 20:43:11,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:11,280][root][INFO] - Training Epoch: 2/2, step 3086/7134 completed (loss: 0.10747941583395004, acc: 0.9771689772605896)
[2025-02-13 20:43:11,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:11,644][root][INFO] - Training Epoch: 2/2, step 3087/7134 completed (loss: 0.0895664170384407, acc: 0.976331353187561)
[2025-02-13 20:43:11,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:12,005][root][INFO] - Training Epoch: 2/2, step 3088/7134 completed (loss: 0.11271904408931732, acc: 0.9672897458076477)
[2025-02-13 20:43:12,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:12,384][root][INFO] - Training Epoch: 2/2, step 3089/7134 completed (loss: 0.05395197495818138, acc: 0.9860464930534363)
[2025-02-13 20:43:12,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:12,747][root][INFO] - Training Epoch: 2/2, step 3090/7134 completed (loss: 0.2623184323310852, acc: 0.9378530979156494)
[2025-02-13 20:43:12,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:13,142][root][INFO] - Training Epoch: 2/2, step 3091/7134 completed (loss: 0.29931238293647766, acc: 0.9270386099815369)
[2025-02-13 20:43:13,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:13,525][root][INFO] - Training Epoch: 2/2, step 3092/7134 completed (loss: 0.20873785018920898, acc: 0.9618320465087891)
[2025-02-13 20:43:13,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:13,886][root][INFO] - Training Epoch: 2/2, step 3093/7134 completed (loss: 0.14652444422245026, acc: 0.9577465057373047)
[2025-02-13 20:43:14,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:14,264][root][INFO] - Training Epoch: 2/2, step 3094/7134 completed (loss: 0.0364106148481369, acc: 0.9942196607589722)
[2025-02-13 20:43:14,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:14,616][root][INFO] - Training Epoch: 2/2, step 3095/7134 completed (loss: 0.13991647958755493, acc: 0.9722222089767456)
[2025-02-13 20:43:14,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:14,993][root][INFO] - Training Epoch: 2/2, step 3096/7134 completed (loss: 0.10997775197029114, acc: 0.9842932224273682)
[2025-02-13 20:43:15,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:15,361][root][INFO] - Training Epoch: 2/2, step 3097/7134 completed (loss: 0.08065816015005112, acc: 0.9817073345184326)
[2025-02-13 20:43:15,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:15,718][root][INFO] - Training Epoch: 2/2, step 3098/7134 completed (loss: 0.12279605120420456, acc: 0.9673202633857727)
[2025-02-13 20:43:15,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:16,094][root][INFO] - Training Epoch: 2/2, step 3099/7134 completed (loss: 0.07352419942617416, acc: 0.9635036587715149)
[2025-02-13 20:43:16,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:16,466][root][INFO] - Training Epoch: 2/2, step 3100/7134 completed (loss: 0.06226605176925659, acc: 0.9820627570152283)
[2025-02-13 20:43:16,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:16,839][root][INFO] - Training Epoch: 2/2, step 3101/7134 completed (loss: 0.07068569958209991, acc: 0.9847715497016907)
[2025-02-13 20:43:16,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:17,224][root][INFO] - Training Epoch: 2/2, step 3102/7134 completed (loss: 0.0743074044585228, acc: 0.9815950989723206)
[2025-02-13 20:43:17,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:17,601][root][INFO] - Training Epoch: 2/2, step 3103/7134 completed (loss: 0.09264252334833145, acc: 0.9784482717514038)
[2025-02-13 20:43:17,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:17,993][root][INFO] - Training Epoch: 2/2, step 3104/7134 completed (loss: 0.07712315768003464, acc: 0.9844961166381836)
[2025-02-13 20:43:18,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:18,352][root][INFO] - Training Epoch: 2/2, step 3105/7134 completed (loss: 0.148703470826149, acc: 0.969565212726593)
[2025-02-13 20:43:18,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:18,729][root][INFO] - Training Epoch: 2/2, step 3106/7134 completed (loss: 0.06620505452156067, acc: 0.9798657894134521)
[2025-02-13 20:43:18,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:19,116][root][INFO] - Training Epoch: 2/2, step 3107/7134 completed (loss: 0.056222666054964066, acc: 0.9883720874786377)
[2025-02-13 20:43:19,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:19,459][root][INFO] - Training Epoch: 2/2, step 3108/7134 completed (loss: 0.08000790327787399, acc: 0.9795918464660645)
[2025-02-13 20:43:19,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:19,827][root][INFO] - Training Epoch: 2/2, step 3109/7134 completed (loss: 0.1126713901758194, acc: 0.9692307710647583)
[2025-02-13 20:43:19,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:20,194][root][INFO] - Training Epoch: 2/2, step 3110/7134 completed (loss: 0.11794139444828033, acc: 0.9611650705337524)
[2025-02-13 20:43:20,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:20,563][root][INFO] - Training Epoch: 2/2, step 3111/7134 completed (loss: 0.31256377696990967, acc: 0.9130434989929199)
[2025-02-13 20:43:20,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:20,986][root][INFO] - Training Epoch: 2/2, step 3112/7134 completed (loss: 0.21071898937225342, acc: 0.956204354763031)
[2025-02-13 20:43:21,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:21,385][root][INFO] - Training Epoch: 2/2, step 3113/7134 completed (loss: 0.0610065683722496, acc: 0.9817073345184326)
[2025-02-13 20:43:21,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:21,762][root][INFO] - Training Epoch: 2/2, step 3114/7134 completed (loss: 0.0555601641535759, acc: 0.9924812316894531)
[2025-02-13 20:43:21,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:22,121][root][INFO] - Training Epoch: 2/2, step 3115/7134 completed (loss: 0.09331931173801422, acc: 0.9802631735801697)
[2025-02-13 20:43:22,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:22,502][root][INFO] - Training Epoch: 2/2, step 3116/7134 completed (loss: 0.060535021126270294, acc: 0.9830508232116699)
[2025-02-13 20:43:22,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:22,880][root][INFO] - Training Epoch: 2/2, step 3117/7134 completed (loss: 0.1268007755279541, acc: 0.9775280952453613)
[2025-02-13 20:43:23,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:23,256][root][INFO] - Training Epoch: 2/2, step 3118/7134 completed (loss: 0.08017226308584213, acc: 0.9779005646705627)
[2025-02-13 20:43:23,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:23,628][root][INFO] - Training Epoch: 2/2, step 3119/7134 completed (loss: 0.1583680659532547, acc: 0.9417989253997803)
[2025-02-13 20:43:23,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:23,993][root][INFO] - Training Epoch: 2/2, step 3120/7134 completed (loss: 0.2277267426252365, acc: 0.9506173133850098)
[2025-02-13 20:43:24,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:24,359][root][INFO] - Training Epoch: 2/2, step 3121/7134 completed (loss: 0.08188562840223312, acc: 0.9886363744735718)
[2025-02-13 20:43:24,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:24,742][root][INFO] - Training Epoch: 2/2, step 3122/7134 completed (loss: 0.12202138453722, acc: 0.9789473414421082)
[2025-02-13 20:43:24,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:25,134][root][INFO] - Training Epoch: 2/2, step 3123/7134 completed (loss: 0.10641977190971375, acc: 0.9675675630569458)
[2025-02-13 20:43:25,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:25,463][root][INFO] - Training Epoch: 2/2, step 3124/7134 completed (loss: 0.15342670679092407, acc: 0.9611111283302307)
[2025-02-13 20:43:25,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:25,818][root][INFO] - Training Epoch: 2/2, step 3125/7134 completed (loss: 0.0488709919154644, acc: 0.9863945841789246)
[2025-02-13 20:43:25,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:26,210][root][INFO] - Training Epoch: 2/2, step 3126/7134 completed (loss: 0.10993468761444092, acc: 0.9776536226272583)
[2025-02-13 20:43:26,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:26,572][root][INFO] - Training Epoch: 2/2, step 3127/7134 completed (loss: 0.10873639583587646, acc: 0.9765258431434631)
[2025-02-13 20:43:26,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:26,940][root][INFO] - Training Epoch: 2/2, step 3128/7134 completed (loss: 0.08521095663309097, acc: 0.9839572310447693)
[2025-02-13 20:43:27,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:27,312][root][INFO] - Training Epoch: 2/2, step 3129/7134 completed (loss: 0.08398061245679855, acc: 0.9886363744735718)
[2025-02-13 20:43:27,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:27,702][root][INFO] - Training Epoch: 2/2, step 3130/7134 completed (loss: 0.051240548491477966, acc: 0.9847715497016907)
[2025-02-13 20:43:27,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:28,094][root][INFO] - Training Epoch: 2/2, step 3131/7134 completed (loss: 0.1014726310968399, acc: 0.9604519605636597)
[2025-02-13 20:43:28,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:28,472][root][INFO] - Training Epoch: 2/2, step 3132/7134 completed (loss: 0.03565334901213646, acc: 0.991150438785553)
[2025-02-13 20:43:28,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:28,863][root][INFO] - Training Epoch: 2/2, step 3133/7134 completed (loss: 0.21509943902492523, acc: 0.9659090638160706)
[2025-02-13 20:43:29,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:29,237][root][INFO] - Training Epoch: 2/2, step 3134/7134 completed (loss: 0.06139960139989853, acc: 0.9765625)
[2025-02-13 20:43:29,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:29,552][root][INFO] - Training Epoch: 2/2, step 3135/7134 completed (loss: 0.1117214784026146, acc: 0.969072163105011)
[2025-02-13 20:43:29,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:29,922][root][INFO] - Training Epoch: 2/2, step 3136/7134 completed (loss: 0.07626505196094513, acc: 0.9770992398262024)
[2025-02-13 20:43:30,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:30,303][root][INFO] - Training Epoch: 2/2, step 3137/7134 completed (loss: 0.11180119216442108, acc: 0.957446813583374)
[2025-02-13 20:43:30,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:30,660][root][INFO] - Training Epoch: 2/2, step 3138/7134 completed (loss: 0.22098541259765625, acc: 0.9666666388511658)
[2025-02-13 20:43:30,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:30,991][root][INFO] - Training Epoch: 2/2, step 3139/7134 completed (loss: 0.13343581557273865, acc: 0.9640287756919861)
[2025-02-13 20:43:31,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:31,358][root][INFO] - Training Epoch: 2/2, step 3140/7134 completed (loss: 0.09735538065433502, acc: 0.9794520735740662)
[2025-02-13 20:43:31,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:31,740][root][INFO] - Training Epoch: 2/2, step 3141/7134 completed (loss: 0.14438046514987946, acc: 0.9797297120094299)
[2025-02-13 20:43:31,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:32,097][root][INFO] - Training Epoch: 2/2, step 3142/7134 completed (loss: 0.0745864287018776, acc: 0.976047933101654)
[2025-02-13 20:43:32,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:32,472][root][INFO] - Training Epoch: 2/2, step 3143/7134 completed (loss: 0.10789746791124344, acc: 0.9741935729980469)
[2025-02-13 20:43:32,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:32,855][root][INFO] - Training Epoch: 2/2, step 3144/7134 completed (loss: 0.12140259146690369, acc: 0.9871794581413269)
[2025-02-13 20:43:33,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:33,227][root][INFO] - Training Epoch: 2/2, step 3145/7134 completed (loss: 0.11477547883987427, acc: 0.9639639854431152)
[2025-02-13 20:43:33,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:33,579][root][INFO] - Training Epoch: 2/2, step 3146/7134 completed (loss: 0.14594562351703644, acc: 0.9494949579238892)
[2025-02-13 20:43:33,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:33,934][root][INFO] - Training Epoch: 2/2, step 3147/7134 completed (loss: 0.055721744894981384, acc: 0.9863013625144958)
[2025-02-13 20:43:34,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:34,341][root][INFO] - Training Epoch: 2/2, step 3148/7134 completed (loss: 0.03402040898799896, acc: 0.9888268113136292)
[2025-02-13 20:43:34,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:34,726][root][INFO] - Training Epoch: 2/2, step 3149/7134 completed (loss: 0.029717983677983284, acc: 1.0)
[2025-02-13 20:43:34,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:35,099][root][INFO] - Training Epoch: 2/2, step 3150/7134 completed (loss: 0.40809521079063416, acc: 0.8857142925262451)
[2025-02-13 20:43:35,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:35,470][root][INFO] - Training Epoch: 2/2, step 3151/7134 completed (loss: 0.34099194407463074, acc: 0.9465649127960205)
[2025-02-13 20:43:35,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:35,833][root][INFO] - Training Epoch: 2/2, step 3152/7134 completed (loss: 0.09192188829183578, acc: 0.9788732528686523)
[2025-02-13 20:43:35,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:36,191][root][INFO] - Training Epoch: 2/2, step 3153/7134 completed (loss: 0.09195686876773834, acc: 0.966292142868042)
[2025-02-13 20:43:36,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:36,581][root][INFO] - Training Epoch: 2/2, step 3154/7134 completed (loss: 0.11256541311740875, acc: 0.969924807548523)
[2025-02-13 20:43:36,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:36,930][root][INFO] - Training Epoch: 2/2, step 3155/7134 completed (loss: 0.29206332564353943, acc: 0.932330846786499)
[2025-02-13 20:43:37,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:37,285][root][INFO] - Training Epoch: 2/2, step 3156/7134 completed (loss: 0.22583122551441193, acc: 0.9360465407371521)
[2025-02-13 20:43:37,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:37,657][root][INFO] - Training Epoch: 2/2, step 3157/7134 completed (loss: 0.10800278186798096, acc: 0.9739130139350891)
[2025-02-13 20:43:37,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:38,004][root][INFO] - Training Epoch: 2/2, step 3158/7134 completed (loss: 0.07543344795703888, acc: 0.9858155846595764)
[2025-02-13 20:43:38,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:38,359][root][INFO] - Training Epoch: 2/2, step 3159/7134 completed (loss: 0.11166886985301971, acc: 0.9793814420700073)
[2025-02-13 20:43:38,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:38,747][root][INFO] - Training Epoch: 2/2, step 3160/7134 completed (loss: 0.04528659209609032, acc: 0.9935897588729858)
[2025-02-13 20:43:38,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:39,112][root][INFO] - Training Epoch: 2/2, step 3161/7134 completed (loss: 0.11344333738088608, acc: 0.9808917045593262)
[2025-02-13 20:43:39,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:39,469][root][INFO] - Training Epoch: 2/2, step 3162/7134 completed (loss: 0.06062439829111099, acc: 0.9863013625144958)
[2025-02-13 20:43:39,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:39,812][root][INFO] - Training Epoch: 2/2, step 3163/7134 completed (loss: 0.030049050226807594, acc: 0.9918032884597778)
[2025-02-13 20:43:39,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:40,178][root][INFO] - Training Epoch: 2/2, step 3164/7134 completed (loss: 0.09025365114212036, acc: 0.9927536249160767)
[2025-02-13 20:43:40,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:40,561][root][INFO] - Training Epoch: 2/2, step 3165/7134 completed (loss: 0.06362749636173248, acc: 0.9930070042610168)
[2025-02-13 20:43:40,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:40,930][root][INFO] - Training Epoch: 2/2, step 3166/7134 completed (loss: 0.12623941898345947, acc: 0.9746835231781006)
[2025-02-13 20:43:41,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:41,317][root][INFO] - Training Epoch: 2/2, step 3167/7134 completed (loss: 0.13045082986354828, acc: 0.9629629850387573)
[2025-02-13 20:43:41,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:41,674][root][INFO] - Training Epoch: 2/2, step 3168/7134 completed (loss: 0.07762950658798218, acc: 0.9803921580314636)
[2025-02-13 20:43:41,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:42,035][root][INFO] - Training Epoch: 2/2, step 3169/7134 completed (loss: 0.10058088600635529, acc: 0.9754098653793335)
[2025-02-13 20:43:42,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:42,409][root][INFO] - Training Epoch: 2/2, step 3170/7134 completed (loss: 0.10559427738189697, acc: 0.9852941036224365)
[2025-02-13 20:43:42,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:42,790][root][INFO] - Training Epoch: 2/2, step 3171/7134 completed (loss: 0.09440917521715164, acc: 0.9835164546966553)
[2025-02-13 20:43:42,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:43,142][root][INFO] - Training Epoch: 2/2, step 3172/7134 completed (loss: 0.0716278925538063, acc: 0.9920634627342224)
[2025-02-13 20:43:43,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:43,501][root][INFO] - Training Epoch: 2/2, step 3173/7134 completed (loss: 0.08186525106430054, acc: 0.9898989796638489)
[2025-02-13 20:43:43,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:43,867][root][INFO] - Training Epoch: 2/2, step 3174/7134 completed (loss: 0.10333610326051712, acc: 0.9759036302566528)
[2025-02-13 20:43:44,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:44,251][root][INFO] - Training Epoch: 2/2, step 3175/7134 completed (loss: 0.07363688945770264, acc: 0.9825581312179565)
[2025-02-13 20:43:44,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:44,658][root][INFO] - Training Epoch: 2/2, step 3176/7134 completed (loss: 0.2191806435585022, acc: 0.9709302186965942)
[2025-02-13 20:43:44,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:45,034][root][INFO] - Training Epoch: 2/2, step 3177/7134 completed (loss: 0.13531804084777832, acc: 0.9691358208656311)
[2025-02-13 20:43:45,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:45,410][root][INFO] - Training Epoch: 2/2, step 3178/7134 completed (loss: 0.08475951850414276, acc: 0.9599999785423279)
[2025-02-13 20:43:45,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:45,800][root][INFO] - Training Epoch: 2/2, step 3179/7134 completed (loss: 0.10601800680160522, acc: 0.9642857313156128)
[2025-02-13 20:43:45,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:46,178][root][INFO] - Training Epoch: 2/2, step 3180/7134 completed (loss: 0.06800806522369385, acc: 0.9876543283462524)
[2025-02-13 20:43:46,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:46,562][root][INFO] - Training Epoch: 2/2, step 3181/7134 completed (loss: 0.08444466441869736, acc: 0.9779005646705627)
[2025-02-13 20:43:46,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:46,962][root][INFO] - Training Epoch: 2/2, step 3182/7134 completed (loss: 0.07610537111759186, acc: 0.9878048896789551)
[2025-02-13 20:43:47,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:47,344][root][INFO] - Training Epoch: 2/2, step 3183/7134 completed (loss: 0.08435853570699692, acc: 0.970802903175354)
[2025-02-13 20:43:47,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:47,743][root][INFO] - Training Epoch: 2/2, step 3184/7134 completed (loss: 0.07295244187116623, acc: 0.987500011920929)
[2025-02-13 20:43:47,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:48,111][root][INFO] - Training Epoch: 2/2, step 3185/7134 completed (loss: 0.03466300666332245, acc: 0.9937106966972351)
[2025-02-13 20:43:48,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:48,475][root][INFO] - Training Epoch: 2/2, step 3186/7134 completed (loss: 0.06257782131433487, acc: 0.9743589758872986)
[2025-02-13 20:43:48,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:48,890][root][INFO] - Training Epoch: 2/2, step 3187/7134 completed (loss: 0.20753180980682373, acc: 0.9714285731315613)
[2025-02-13 20:43:49,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:49,267][root][INFO] - Training Epoch: 2/2, step 3188/7134 completed (loss: 0.053357016295194626, acc: 0.9888268113136292)
[2025-02-13 20:43:49,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:49,651][root][INFO] - Training Epoch: 2/2, step 3189/7134 completed (loss: 0.06944848597049713, acc: 0.9876543283462524)
[2025-02-13 20:43:49,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:50,024][root][INFO] - Training Epoch: 2/2, step 3190/7134 completed (loss: 0.12341634929180145, acc: 0.9750000238418579)
[2025-02-13 20:43:50,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:50,396][root][INFO] - Training Epoch: 2/2, step 3191/7134 completed (loss: 0.06134264916181564, acc: 0.9937888383865356)
[2025-02-13 20:43:50,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:50,766][root][INFO] - Training Epoch: 2/2, step 3192/7134 completed (loss: 0.20052942633628845, acc: 0.970802903175354)
[2025-02-13 20:43:50,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:51,147][root][INFO] - Training Epoch: 2/2, step 3193/7134 completed (loss: 0.0982622280716896, acc: 0.9783783555030823)
[2025-02-13 20:43:51,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:51,515][root][INFO] - Training Epoch: 2/2, step 3194/7134 completed (loss: 0.10474678128957748, acc: 0.9757575988769531)
[2025-02-13 20:43:51,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:51,886][root][INFO] - Training Epoch: 2/2, step 3195/7134 completed (loss: 0.12586674094200134, acc: 0.9689440727233887)
[2025-02-13 20:43:52,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:52,259][root][INFO] - Training Epoch: 2/2, step 3196/7134 completed (loss: 0.07448913902044296, acc: 0.9855072498321533)
[2025-02-13 20:43:52,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:52,662][root][INFO] - Training Epoch: 2/2, step 3197/7134 completed (loss: 0.11070077866315842, acc: 0.976047933101654)
[2025-02-13 20:43:52,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:53,035][root][INFO] - Training Epoch: 2/2, step 3198/7134 completed (loss: 0.04024482145905495, acc: 1.0)
[2025-02-13 20:43:53,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:53,385][root][INFO] - Training Epoch: 2/2, step 3199/7134 completed (loss: 0.05177770182490349, acc: 0.9883720874786377)
[2025-02-13 20:43:53,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:53,749][root][INFO] - Training Epoch: 2/2, step 3200/7134 completed (loss: 0.11356168240308762, acc: 0.9858155846595764)
[2025-02-13 20:43:53,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:54,119][root][INFO] - Training Epoch: 2/2, step 3201/7134 completed (loss: 0.11715402454137802, acc: 0.9763779640197754)
[2025-02-13 20:43:54,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:54,482][root][INFO] - Training Epoch: 2/2, step 3202/7134 completed (loss: 0.0870356485247612, acc: 0.983146071434021)
[2025-02-13 20:43:54,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:54,856][root][INFO] - Training Epoch: 2/2, step 3203/7134 completed (loss: 0.06796030700206757, acc: 0.987500011920929)
[2025-02-13 20:43:54,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:55,202][root][INFO] - Training Epoch: 2/2, step 3204/7134 completed (loss: 0.0815814733505249, acc: 0.9726027250289917)
[2025-02-13 20:43:55,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:55,559][root][INFO] - Training Epoch: 2/2, step 3205/7134 completed (loss: 0.02020665444433689, acc: 0.9929078221321106)
[2025-02-13 20:43:55,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:55,930][root][INFO] - Training Epoch: 2/2, step 3206/7134 completed (loss: 0.07009438425302505, acc: 0.9696969985961914)
[2025-02-13 20:43:56,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:56,304][root][INFO] - Training Epoch: 2/2, step 3207/7134 completed (loss: 0.11264435946941376, acc: 0.9693251252174377)
[2025-02-13 20:43:56,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:56,691][root][INFO] - Training Epoch: 2/2, step 3208/7134 completed (loss: 0.08179516345262527, acc: 0.9928057789802551)
[2025-02-13 20:43:56,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:57,087][root][INFO] - Training Epoch: 2/2, step 3209/7134 completed (loss: 0.07009715586900711, acc: 0.987500011920929)
[2025-02-13 20:43:57,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:57,468][root][INFO] - Training Epoch: 2/2, step 3210/7134 completed (loss: 0.03579423204064369, acc: 0.9935483932495117)
[2025-02-13 20:43:57,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:57,838][root][INFO] - Training Epoch: 2/2, step 3211/7134 completed (loss: 0.06099695339798927, acc: 0.9915966391563416)
[2025-02-13 20:43:57,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:58,216][root][INFO] - Training Epoch: 2/2, step 3212/7134 completed (loss: 0.048281230032444, acc: 0.9824561476707458)
[2025-02-13 20:43:58,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:58,596][root][INFO] - Training Epoch: 2/2, step 3213/7134 completed (loss: 0.11568021774291992, acc: 0.9583333134651184)
[2025-02-13 20:43:58,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:58,967][root][INFO] - Training Epoch: 2/2, step 3214/7134 completed (loss: 0.07794269919395447, acc: 0.9707602262496948)
[2025-02-13 20:43:59,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:59,339][root][INFO] - Training Epoch: 2/2, step 3215/7134 completed (loss: 0.08667368441820145, acc: 0.976331353187561)
[2025-02-13 20:43:59,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:59,735][root][INFO] - Training Epoch: 2/2, step 3216/7134 completed (loss: 0.06537289172410965, acc: 0.9814814925193787)
[2025-02-13 20:43:59,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:00,118][root][INFO] - Training Epoch: 2/2, step 3217/7134 completed (loss: 0.07896668463945389, acc: 0.9719101190567017)
[2025-02-13 20:44:00,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:00,511][root][INFO] - Training Epoch: 2/2, step 3218/7134 completed (loss: 0.07033337652683258, acc: 0.9870967864990234)
[2025-02-13 20:44:00,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:00,851][root][INFO] - Training Epoch: 2/2, step 3219/7134 completed (loss: 0.10123597830533981, acc: 0.9523809552192688)
[2025-02-13 20:44:00,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:01,247][root][INFO] - Training Epoch: 2/2, step 3220/7134 completed (loss: 0.055970724672079086, acc: 0.9774011373519897)
[2025-02-13 20:44:01,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:01,667][root][INFO] - Training Epoch: 2/2, step 3221/7134 completed (loss: 0.04435352236032486, acc: 0.9838709831237793)
[2025-02-13 20:44:01,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:02,043][root][INFO] - Training Epoch: 2/2, step 3222/7134 completed (loss: 0.13731008768081665, acc: 0.9601989984512329)
[2025-02-13 20:44:02,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:02,424][root][INFO] - Training Epoch: 2/2, step 3223/7134 completed (loss: 0.20052765309810638, acc: 0.9693251252174377)
[2025-02-13 20:44:02,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:02,805][root][INFO] - Training Epoch: 2/2, step 3224/7134 completed (loss: 0.1306491196155548, acc: 0.9653179049491882)
[2025-02-13 20:44:02,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:03,199][root][INFO] - Training Epoch: 2/2, step 3225/7134 completed (loss: 0.08911755681037903, acc: 0.9700000286102295)
[2025-02-13 20:44:03,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:03,561][root][INFO] - Training Epoch: 2/2, step 3226/7134 completed (loss: 0.09378328919410706, acc: 0.9801324605941772)
[2025-02-13 20:44:03,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:03,966][root][INFO] - Training Epoch: 2/2, step 3227/7134 completed (loss: 0.1982131153345108, acc: 0.9547325372695923)
[2025-02-13 20:44:04,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:04,346][root][INFO] - Training Epoch: 2/2, step 3228/7134 completed (loss: 0.05763135850429535, acc: 0.9848484992980957)
[2025-02-13 20:44:04,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:04,731][root][INFO] - Training Epoch: 2/2, step 3229/7134 completed (loss: 0.1795060932636261, acc: 0.9595959782600403)
[2025-02-13 20:44:04,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:05,090][root][INFO] - Training Epoch: 2/2, step 3230/7134 completed (loss: 0.06678151339292526, acc: 0.981249988079071)
[2025-02-13 20:44:05,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:05,457][root][INFO] - Training Epoch: 2/2, step 3231/7134 completed (loss: 0.1001780778169632, acc: 0.9725274443626404)
[2025-02-13 20:44:05,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:05,835][root][INFO] - Training Epoch: 2/2, step 3232/7134 completed (loss: 0.028725016862154007, acc: 0.9953703880310059)
[2025-02-13 20:44:05,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:06,219][root][INFO] - Training Epoch: 2/2, step 3233/7134 completed (loss: 0.10732357949018478, acc: 0.9685039520263672)
[2025-02-13 20:44:06,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:06,614][root][INFO] - Training Epoch: 2/2, step 3234/7134 completed (loss: 0.03312673419713974, acc: 0.9937106966972351)
[2025-02-13 20:44:06,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:07,006][root][INFO] - Training Epoch: 2/2, step 3235/7134 completed (loss: 0.03697998821735382, acc: 0.9929078221321106)
[2025-02-13 20:44:07,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:07,399][root][INFO] - Training Epoch: 2/2, step 3236/7134 completed (loss: 0.18780748546123505, acc: 0.9604519605636597)
[2025-02-13 20:44:07,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:07,770][root][INFO] - Training Epoch: 2/2, step 3237/7134 completed (loss: 0.07598919421434402, acc: 0.9740259647369385)
[2025-02-13 20:44:07,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:08,183][root][INFO] - Training Epoch: 2/2, step 3238/7134 completed (loss: 0.1138424351811409, acc: 0.9611111283302307)
[2025-02-13 20:44:08,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:08,599][root][INFO] - Training Epoch: 2/2, step 3239/7134 completed (loss: 0.0374247282743454, acc: 0.9939024448394775)
[2025-02-13 20:44:08,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:08,959][root][INFO] - Training Epoch: 2/2, step 3240/7134 completed (loss: 0.15011928975582123, acc: 0.9776536226272583)
[2025-02-13 20:44:09,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:09,329][root][INFO] - Training Epoch: 2/2, step 3241/7134 completed (loss: 0.053433265537023544, acc: 0.9882352948188782)
[2025-02-13 20:44:09,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:09,722][root][INFO] - Training Epoch: 2/2, step 3242/7134 completed (loss: 0.027168171480298042, acc: 0.9935064911842346)
[2025-02-13 20:44:09,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:10,110][root][INFO] - Training Epoch: 2/2, step 3243/7134 completed (loss: 0.06255492568016052, acc: 0.9839572310447693)
[2025-02-13 20:44:10,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:10,486][root][INFO] - Training Epoch: 2/2, step 3244/7134 completed (loss: 0.08722086250782013, acc: 0.9826589822769165)
[2025-02-13 20:44:10,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:10,908][root][INFO] - Training Epoch: 2/2, step 3245/7134 completed (loss: 0.06670138984918594, acc: 0.9846153855323792)
[2025-02-13 20:44:11,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:11,317][root][INFO] - Training Epoch: 2/2, step 3246/7134 completed (loss: 0.09825574606657028, acc: 0.9794871807098389)
[2025-02-13 20:44:11,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:11,721][root][INFO] - Training Epoch: 2/2, step 3247/7134 completed (loss: 0.043604303151369095, acc: 0.9807692170143127)
[2025-02-13 20:44:11,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:12,081][root][INFO] - Training Epoch: 2/2, step 3248/7134 completed (loss: 0.013348824344575405, acc: 1.0)
[2025-02-13 20:44:12,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:12,449][root][INFO] - Training Epoch: 2/2, step 3249/7134 completed (loss: 0.03411856293678284, acc: 0.9881656765937805)
[2025-02-13 20:44:12,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:12,871][root][INFO] - Training Epoch: 2/2, step 3250/7134 completed (loss: 0.11679688096046448, acc: 0.9640718698501587)
[2025-02-13 20:44:13,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:13,242][root][INFO] - Training Epoch: 2/2, step 3251/7134 completed (loss: 0.05690598487854004, acc: 0.9871794581413269)
[2025-02-13 20:44:13,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:13,621][root][INFO] - Training Epoch: 2/2, step 3252/7134 completed (loss: 0.020387329161167145, acc: 1.0)
[2025-02-13 20:44:13,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:14,070][root][INFO] - Training Epoch: 2/2, step 3253/7134 completed (loss: 0.05608440190553665, acc: 0.9802631735801697)
[2025-02-13 20:44:14,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:14,502][root][INFO] - Training Epoch: 2/2, step 3254/7134 completed (loss: 0.05328822880983353, acc: 0.9870129823684692)
[2025-02-13 20:44:14,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:14,896][root][INFO] - Training Epoch: 2/2, step 3255/7134 completed (loss: 0.05263930931687355, acc: 0.9886363744735718)
[2025-02-13 20:44:15,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:15,325][root][INFO] - Training Epoch: 2/2, step 3256/7134 completed (loss: 0.06979303807020187, acc: 0.9937888383865356)
[2025-02-13 20:44:15,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:15,784][root][INFO] - Training Epoch: 2/2, step 3257/7134 completed (loss: 0.1570240706205368, acc: 0.9757575988769531)
[2025-02-13 20:44:15,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:16,221][root][INFO] - Training Epoch: 2/2, step 3258/7134 completed (loss: 0.12924860417842865, acc: 0.9593495726585388)
[2025-02-13 20:44:16,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:16,653][root][INFO] - Training Epoch: 2/2, step 3259/7134 completed (loss: 0.11211445182561874, acc: 0.9689440727233887)
[2025-02-13 20:44:16,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:17,090][root][INFO] - Training Epoch: 2/2, step 3260/7134 completed (loss: 0.20962919294834137, acc: 0.9586206674575806)
[2025-02-13 20:44:17,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:17,460][root][INFO] - Training Epoch: 2/2, step 3261/7134 completed (loss: 0.15244485437870026, acc: 0.9489051103591919)
[2025-02-13 20:44:17,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:17,836][root][INFO] - Training Epoch: 2/2, step 3262/7134 completed (loss: 0.09847167879343033, acc: 0.9691358208656311)
[2025-02-13 20:44:17,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:18,215][root][INFO] - Training Epoch: 2/2, step 3263/7134 completed (loss: 0.08618390560150146, acc: 0.9822485446929932)
[2025-02-13 20:44:18,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:18,635][root][INFO] - Training Epoch: 2/2, step 3264/7134 completed (loss: 0.0912100300192833, acc: 0.9695122241973877)
[2025-02-13 20:44:18,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:19,077][root][INFO] - Training Epoch: 2/2, step 3265/7134 completed (loss: 0.12696535885334015, acc: 0.9808917045593262)
[2025-02-13 20:44:19,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:19,539][root][INFO] - Training Epoch: 2/2, step 3266/7134 completed (loss: 0.06190064549446106, acc: 0.9862068891525269)
[2025-02-13 20:44:19,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:19,970][root][INFO] - Training Epoch: 2/2, step 3267/7134 completed (loss: 0.13499325513839722, acc: 0.9575757384300232)
[2025-02-13 20:44:20,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:20,405][root][INFO] - Training Epoch: 2/2, step 3268/7134 completed (loss: 0.0723167210817337, acc: 0.9794520735740662)
[2025-02-13 20:44:20,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:20,869][root][INFO] - Training Epoch: 2/2, step 3269/7134 completed (loss: 0.06764617562294006, acc: 0.9893617033958435)
[2025-02-13 20:44:21,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:21,284][root][INFO] - Training Epoch: 2/2, step 3270/7134 completed (loss: 0.07798027992248535, acc: 0.9729729890823364)
[2025-02-13 20:44:21,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:21,708][root][INFO] - Training Epoch: 2/2, step 3271/7134 completed (loss: 0.016635678708553314, acc: 1.0)
[2025-02-13 20:44:21,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:22,089][root][INFO] - Training Epoch: 2/2, step 3272/7134 completed (loss: 0.0715305283665657, acc: 0.9800000190734863)
[2025-02-13 20:44:22,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:22,529][root][INFO] - Training Epoch: 2/2, step 3273/7134 completed (loss: 0.0777682363986969, acc: 0.9714285731315613)
[2025-02-13 20:44:22,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:22,933][root][INFO] - Training Epoch: 2/2, step 3274/7134 completed (loss: 0.04793381690979004, acc: 0.9875776171684265)
[2025-02-13 20:44:23,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:23,353][root][INFO] - Training Epoch: 2/2, step 3275/7134 completed (loss: 0.43116244673728943, acc: 0.9041916131973267)
[2025-02-13 20:44:23,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:23,772][root][INFO] - Training Epoch: 2/2, step 3276/7134 completed (loss: 0.08480672538280487, acc: 0.9803921580314636)
[2025-02-13 20:44:23,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:24,162][root][INFO] - Training Epoch: 2/2, step 3277/7134 completed (loss: 0.11274444311857224, acc: 0.9640287756919861)
[2025-02-13 20:44:24,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:24,528][root][INFO] - Training Epoch: 2/2, step 3278/7134 completed (loss: 0.07377393543720245, acc: 0.9919354915618896)
[2025-02-13 20:44:24,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:24,930][root][INFO] - Training Epoch: 2/2, step 3279/7134 completed (loss: 0.12422100454568863, acc: 0.9588235020637512)
[2025-02-13 20:44:25,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:25,337][root][INFO] - Training Epoch: 2/2, step 3280/7134 completed (loss: 0.22068063914775848, acc: 0.932692289352417)
[2025-02-13 20:44:25,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:25,751][root][INFO] - Training Epoch: 2/2, step 3281/7134 completed (loss: 0.11569129675626755, acc: 0.9727891087532043)
[2025-02-13 20:44:25,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:26,154][root][INFO] - Training Epoch: 2/2, step 3282/7134 completed (loss: 0.04526330903172493, acc: 0.9923664331436157)
[2025-02-13 20:44:26,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:26,595][root][INFO] - Training Epoch: 2/2, step 3283/7134 completed (loss: 0.046317651867866516, acc: 0.9901960492134094)
[2025-02-13 20:44:26,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:26,991][root][INFO] - Training Epoch: 2/2, step 3284/7134 completed (loss: 0.046564992517232895, acc: 0.9862068891525269)
[2025-02-13 20:44:27,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:27,355][root][INFO] - Training Epoch: 2/2, step 3285/7134 completed (loss: 0.09067708998918533, acc: 0.970588207244873)
[2025-02-13 20:44:27,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:27,725][root][INFO] - Training Epoch: 2/2, step 3286/7134 completed (loss: 0.11256003379821777, acc: 0.9846153855323792)
[2025-02-13 20:44:27,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:28,085][root][INFO] - Training Epoch: 2/2, step 3287/7134 completed (loss: 0.07008612155914307, acc: 0.9777777791023254)
[2025-02-13 20:44:28,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:28,450][root][INFO] - Training Epoch: 2/2, step 3288/7134 completed (loss: 0.12514418363571167, acc: 0.9856114983558655)
[2025-02-13 20:44:28,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:28,798][root][INFO] - Training Epoch: 2/2, step 3289/7134 completed (loss: 0.0821741595864296, acc: 0.9779411554336548)
[2025-02-13 20:44:28,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:29,154][root][INFO] - Training Epoch: 2/2, step 3290/7134 completed (loss: 0.10795173794031143, acc: 0.9663865566253662)
[2025-02-13 20:44:29,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:29,534][root][INFO] - Training Epoch: 2/2, step 3291/7134 completed (loss: 0.09142765402793884, acc: 0.9900990128517151)
[2025-02-13 20:44:29,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:29,934][root][INFO] - Training Epoch: 2/2, step 3292/7134 completed (loss: 0.15544933080673218, acc: 0.9747899174690247)
[2025-02-13 20:44:30,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:30,330][root][INFO] - Training Epoch: 2/2, step 3293/7134 completed (loss: 0.09035235643386841, acc: 0.9848484992980957)
[2025-02-13 20:44:30,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:30,703][root][INFO] - Training Epoch: 2/2, step 3294/7134 completed (loss: 0.17559568583965302, acc: 0.9621211886405945)
[2025-02-13 20:44:30,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:31,055][root][INFO] - Training Epoch: 2/2, step 3295/7134 completed (loss: 0.023217443376779556, acc: 0.9919354915618896)
[2025-02-13 20:44:31,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:31,424][root][INFO] - Training Epoch: 2/2, step 3296/7134 completed (loss: 0.09699930250644684, acc: 0.9692307710647583)
[2025-02-13 20:44:31,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:31,810][root][INFO] - Training Epoch: 2/2, step 3297/7134 completed (loss: 0.07316842675209045, acc: 0.9905660152435303)
[2025-02-13 20:44:31,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:32,163][root][INFO] - Training Epoch: 2/2, step 3298/7134 completed (loss: 0.20313464105129242, acc: 0.9548872113227844)
[2025-02-13 20:44:32,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:32,510][root][INFO] - Training Epoch: 2/2, step 3299/7134 completed (loss: 0.04723271355032921, acc: 0.9842519760131836)
[2025-02-13 20:44:32,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:32,860][root][INFO] - Training Epoch: 2/2, step 3300/7134 completed (loss: 0.09965649992227554, acc: 0.9774436354637146)
[2025-02-13 20:44:32,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:33,218][root][INFO] - Training Epoch: 2/2, step 3301/7134 completed (loss: 0.03816172853112221, acc: 0.9902912378311157)
[2025-02-13 20:44:33,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:33,587][root][INFO] - Training Epoch: 2/2, step 3302/7134 completed (loss: 0.16532865166664124, acc: 0.9720279574394226)
[2025-02-13 20:44:33,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:33,922][root][INFO] - Training Epoch: 2/2, step 3303/7134 completed (loss: 0.09830228984355927, acc: 0.9702970385551453)
[2025-02-13 20:44:34,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:34,285][root][INFO] - Training Epoch: 2/2, step 3304/7134 completed (loss: 0.059456732124090195, acc: 0.9862385392189026)
[2025-02-13 20:44:34,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:34,652][root][INFO] - Training Epoch: 2/2, step 3305/7134 completed (loss: 0.09973055869340897, acc: 0.9874213933944702)
[2025-02-13 20:44:34,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:35,028][root][INFO] - Training Epoch: 2/2, step 3306/7134 completed (loss: 0.05938089266419411, acc: 0.982758641242981)
[2025-02-13 20:44:35,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:35,374][root][INFO] - Training Epoch: 2/2, step 3307/7134 completed (loss: 0.04714284464716911, acc: 0.9851852059364319)
[2025-02-13 20:44:35,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:35,719][root][INFO] - Training Epoch: 2/2, step 3308/7134 completed (loss: 0.15717089176177979, acc: 0.970588207244873)
[2025-02-13 20:44:35,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:36,057][root][INFO] - Training Epoch: 2/2, step 3309/7134 completed (loss: 0.13844697177410126, acc: 0.9679487347602844)
[2025-02-13 20:44:36,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:36,438][root][INFO] - Training Epoch: 2/2, step 3310/7134 completed (loss: 0.06381691247224808, acc: 0.9818181991577148)
[2025-02-13 20:44:36,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:36,775][root][INFO] - Training Epoch: 2/2, step 3311/7134 completed (loss: 0.09762870520353317, acc: 0.9702970385551453)
[2025-02-13 20:44:36,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:37,204][root][INFO] - Training Epoch: 2/2, step 3312/7134 completed (loss: 0.06290572881698608, acc: 0.9849624037742615)
[2025-02-13 20:44:37,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:37,562][root][INFO] - Training Epoch: 2/2, step 3313/7134 completed (loss: 0.07416719198226929, acc: 0.9897959232330322)
[2025-02-13 20:44:37,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:37,978][root][INFO] - Training Epoch: 2/2, step 3314/7134 completed (loss: 0.056284066289663315, acc: 0.9908257126808167)
[2025-02-13 20:44:38,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:38,374][root][INFO] - Training Epoch: 2/2, step 3315/7134 completed (loss: 0.09325233101844788, acc: 0.9822485446929932)
[2025-02-13 20:44:38,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:38,729][root][INFO] - Training Epoch: 2/2, step 3316/7134 completed (loss: 0.14990359544754028, acc: 0.9701492786407471)
[2025-02-13 20:44:38,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:39,136][root][INFO] - Training Epoch: 2/2, step 3317/7134 completed (loss: 0.15255527198314667, acc: 0.9586777091026306)
[2025-02-13 20:44:39,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:39,552][root][INFO] - Training Epoch: 2/2, step 3318/7134 completed (loss: 0.029597625136375427, acc: 1.0)
[2025-02-13 20:44:39,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:39,920][root][INFO] - Training Epoch: 2/2, step 3319/7134 completed (loss: 0.13094741106033325, acc: 0.95652174949646)
[2025-02-13 20:44:40,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:40,274][root][INFO] - Training Epoch: 2/2, step 3320/7134 completed (loss: 0.12359365820884705, acc: 0.9830508232116699)
[2025-02-13 20:44:40,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:40,644][root][INFO] - Training Epoch: 2/2, step 3321/7134 completed (loss: 0.21457131206989288, acc: 0.9428571462631226)
[2025-02-13 20:44:40,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:41,026][root][INFO] - Training Epoch: 2/2, step 3322/7134 completed (loss: 0.10147903114557266, acc: 0.9736841917037964)
[2025-02-13 20:44:41,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:41,423][root][INFO] - Training Epoch: 2/2, step 3323/7134 completed (loss: 0.08695941418409348, acc: 0.9824561476707458)
[2025-02-13 20:44:41,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:41,808][root][INFO] - Training Epoch: 2/2, step 3324/7134 completed (loss: 0.18927808105945587, acc: 0.9430052042007446)
[2025-02-13 20:44:41,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:42,201][root][INFO] - Training Epoch: 2/2, step 3325/7134 completed (loss: 0.1341800093650818, acc: 0.9580838084220886)
[2025-02-13 20:44:42,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:42,597][root][INFO] - Training Epoch: 2/2, step 3326/7134 completed (loss: 0.16796375811100006, acc: 0.9575757384300232)
[2025-02-13 20:44:42,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:42,957][root][INFO] - Training Epoch: 2/2, step 3327/7134 completed (loss: 0.17146752774715424, acc: 0.950276255607605)
[2025-02-13 20:44:43,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:43,338][root][INFO] - Training Epoch: 2/2, step 3328/7134 completed (loss: 0.07513175904750824, acc: 0.978723406791687)
[2025-02-13 20:44:43,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:43,705][root][INFO] - Training Epoch: 2/2, step 3329/7134 completed (loss: 0.13557226955890656, acc: 0.96875)
[2025-02-13 20:44:43,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:44,078][root][INFO] - Training Epoch: 2/2, step 3330/7134 completed (loss: 0.0884949341416359, acc: 0.9743589758872986)
[2025-02-13 20:44:44,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:44,446][root][INFO] - Training Epoch: 2/2, step 3331/7134 completed (loss: 0.2655720114707947, acc: 0.9289940595626831)
[2025-02-13 20:44:44,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:44,827][root][INFO] - Training Epoch: 2/2, step 3332/7134 completed (loss: 0.20223283767700195, acc: 0.9733333587646484)
[2025-02-13 20:44:44,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:45,196][root][INFO] - Training Epoch: 2/2, step 3333/7134 completed (loss: 0.1358496993780136, acc: 0.9652174115180969)
[2025-02-13 20:44:45,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:45,565][root][INFO] - Training Epoch: 2/2, step 3334/7134 completed (loss: 0.11397658288478851, acc: 0.967391312122345)
[2025-02-13 20:44:45,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:45,905][root][INFO] - Training Epoch: 2/2, step 3335/7134 completed (loss: 0.04481762647628784, acc: 0.9933333396911621)
[2025-02-13 20:44:46,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:46,274][root][INFO] - Training Epoch: 2/2, step 3336/7134 completed (loss: 0.029821626842021942, acc: 0.9940119981765747)
[2025-02-13 20:44:46,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:46,628][root][INFO] - Training Epoch: 2/2, step 3337/7134 completed (loss: 0.03719022125005722, acc: 0.9915966391563416)
[2025-02-13 20:44:46,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:46,979][root][INFO] - Training Epoch: 2/2, step 3338/7134 completed (loss: 0.05018482729792595, acc: 0.984455943107605)
[2025-02-13 20:44:47,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:47,339][root][INFO] - Training Epoch: 2/2, step 3339/7134 completed (loss: 0.10301243513822556, acc: 0.966183602809906)
[2025-02-13 20:44:47,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:47,696][root][INFO] - Training Epoch: 2/2, step 3340/7134 completed (loss: 0.05380893871188164, acc: 0.988095223903656)
[2025-02-13 20:44:47,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:48,052][root][INFO] - Training Epoch: 2/2, step 3341/7134 completed (loss: 0.0622212179005146, acc: 0.9880239367485046)
[2025-02-13 20:44:48,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:48,401][root][INFO] - Training Epoch: 2/2, step 3342/7134 completed (loss: 0.07613438367843628, acc: 0.9794520735740662)
[2025-02-13 20:44:48,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:48,785][root][INFO] - Training Epoch: 2/2, step 3343/7134 completed (loss: 0.03624499589204788, acc: 0.9913793206214905)
[2025-02-13 20:44:48,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:49,172][root][INFO] - Training Epoch: 2/2, step 3344/7134 completed (loss: 0.08754821866750717, acc: 0.9632353186607361)
[2025-02-13 20:44:49,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:49,535][root][INFO] - Training Epoch: 2/2, step 3345/7134 completed (loss: 0.11129846423864365, acc: 0.9860140085220337)
[2025-02-13 20:44:49,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:49,900][root][INFO] - Training Epoch: 2/2, step 3346/7134 completed (loss: 0.07474887371063232, acc: 0.9756097793579102)
[2025-02-13 20:44:50,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:50,249][root][INFO] - Training Epoch: 2/2, step 3347/7134 completed (loss: 0.19895218312740326, acc: 0.9542483687400818)
[2025-02-13 20:44:50,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:50,600][root][INFO] - Training Epoch: 2/2, step 3348/7134 completed (loss: 0.07290469855070114, acc: 0.9746835231781006)
[2025-02-13 20:44:50,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:50,942][root][INFO] - Training Epoch: 2/2, step 3349/7134 completed (loss: 0.057570960372686386, acc: 0.9922480583190918)
[2025-02-13 20:44:51,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:51,296][root][INFO] - Training Epoch: 2/2, step 3350/7134 completed (loss: 0.07288847118616104, acc: 0.9752066135406494)
[2025-02-13 20:44:51,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:51,649][root][INFO] - Training Epoch: 2/2, step 3351/7134 completed (loss: 0.1490442007780075, acc: 0.957446813583374)
[2025-02-13 20:44:51,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:52,008][root][INFO] - Training Epoch: 2/2, step 3352/7134 completed (loss: 0.04920891672372818, acc: 0.9851852059364319)
[2025-02-13 20:44:52,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:52,373][root][INFO] - Training Epoch: 2/2, step 3353/7134 completed (loss: 0.13121606409549713, acc: 0.9593495726585388)
[2025-02-13 20:44:52,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:52,735][root][INFO] - Training Epoch: 2/2, step 3354/7134 completed (loss: 0.056189339607954025, acc: 0.9683544039726257)
[2025-02-13 20:44:52,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:53,099][root][INFO] - Training Epoch: 2/2, step 3355/7134 completed (loss: 0.030881918966770172, acc: 0.9878048896789551)
[2025-02-13 20:44:53,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:53,483][root][INFO] - Training Epoch: 2/2, step 3356/7134 completed (loss: 0.0059102741070091724, acc: 1.0)
[2025-02-13 20:44:53,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:53,867][root][INFO] - Training Epoch: 2/2, step 3357/7134 completed (loss: 0.06654733419418335, acc: 0.9880239367485046)
[2025-02-13 20:44:54,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:54,239][root][INFO] - Training Epoch: 2/2, step 3358/7134 completed (loss: 0.14514325559139252, acc: 0.9411764740943909)
[2025-02-13 20:44:54,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:54,583][root][INFO] - Training Epoch: 2/2, step 3359/7134 completed (loss: 0.057626187801361084, acc: 0.9931507110595703)
[2025-02-13 20:44:54,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:54,932][root][INFO] - Training Epoch: 2/2, step 3360/7134 completed (loss: 0.12870946526527405, acc: 0.9739130139350891)
[2025-02-13 20:44:55,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:55,292][root][INFO] - Training Epoch: 2/2, step 3361/7134 completed (loss: 0.06732060760259628, acc: 0.9910714030265808)
[2025-02-13 20:44:55,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:55,631][root][INFO] - Training Epoch: 2/2, step 3362/7134 completed (loss: 0.10929806530475616, acc: 0.9727272987365723)
[2025-02-13 20:44:55,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:55,980][root][INFO] - Training Epoch: 2/2, step 3363/7134 completed (loss: 0.05927407369017601, acc: 0.9813084006309509)
[2025-02-13 20:44:56,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:56,337][root][INFO] - Training Epoch: 2/2, step 3364/7134 completed (loss: 0.19888408482074738, acc: 0.9346405267715454)
[2025-02-13 20:44:56,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:56,701][root][INFO] - Training Epoch: 2/2, step 3365/7134 completed (loss: 0.05127452686429024, acc: 0.991150438785553)
[2025-02-13 20:44:56,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:57,065][root][INFO] - Training Epoch: 2/2, step 3366/7134 completed (loss: 0.028376450762152672, acc: 1.0)
[2025-02-13 20:44:57,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:57,418][root][INFO] - Training Epoch: 2/2, step 3367/7134 completed (loss: 0.05380072444677353, acc: 0.9839572310447693)
[2025-02-13 20:44:57,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:57,793][root][INFO] - Training Epoch: 2/2, step 3368/7134 completed (loss: 0.06496059894561768, acc: 0.9813664555549622)
[2025-02-13 20:44:57,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:58,168][root][INFO] - Training Epoch: 2/2, step 3369/7134 completed (loss: 0.03929818794131279, acc: 0.9945945739746094)
[2025-02-13 20:44:58,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:58,528][root][INFO] - Training Epoch: 2/2, step 3370/7134 completed (loss: 0.03727858513593674, acc: 0.9938650131225586)
[2025-02-13 20:44:58,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:58,879][root][INFO] - Training Epoch: 2/2, step 3371/7134 completed (loss: 0.0310402549803257, acc: 0.9930555820465088)
[2025-02-13 20:44:59,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:59,242][root][INFO] - Training Epoch: 2/2, step 3372/7134 completed (loss: 0.08265457302331924, acc: 0.9931972622871399)
[2025-02-13 20:44:59,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:59,595][root][INFO] - Training Epoch: 2/2, step 3373/7134 completed (loss: 0.029506368562579155, acc: 0.9934640526771545)
[2025-02-13 20:44:59,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:59,969][root][INFO] - Training Epoch: 2/2, step 3374/7134 completed (loss: 0.0745396539568901, acc: 0.977142870426178)
[2025-02-13 20:45:00,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:00,336][root][INFO] - Training Epoch: 2/2, step 3375/7134 completed (loss: 0.05573649704456329, acc: 0.9820359349250793)
[2025-02-13 20:45:00,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:00,721][root][INFO] - Training Epoch: 2/2, step 3376/7134 completed (loss: 0.04689405858516693, acc: 0.9947090148925781)
[2025-02-13 20:45:00,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:01,119][root][INFO] - Training Epoch: 2/2, step 3377/7134 completed (loss: 0.03503658249974251, acc: 0.9949748516082764)
[2025-02-13 20:45:01,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:01,485][root][INFO] - Training Epoch: 2/2, step 3378/7134 completed (loss: 0.010998384095728397, acc: 1.0)
[2025-02-13 20:45:01,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:01,875][root][INFO] - Training Epoch: 2/2, step 3379/7134 completed (loss: 0.06155378744006157, acc: 0.9864864945411682)
[2025-02-13 20:45:02,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:02,306][root][INFO] - Training Epoch: 2/2, step 3380/7134 completed (loss: 0.08559897541999817, acc: 0.97826087474823)
[2025-02-13 20:45:02,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:02,693][root][INFO] - Training Epoch: 2/2, step 3381/7134 completed (loss: 0.06046747416257858, acc: 0.9878048896789551)
[2025-02-13 20:45:02,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:03,070][root][INFO] - Training Epoch: 2/2, step 3382/7134 completed (loss: 0.03943250700831413, acc: 0.984375)
[2025-02-13 20:45:03,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:03,468][root][INFO] - Training Epoch: 2/2, step 3383/7134 completed (loss: 0.07468806952238083, acc: 0.9790209531784058)
[2025-02-13 20:45:03,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:03,867][root][INFO] - Training Epoch: 2/2, step 3384/7134 completed (loss: 0.10433655977249146, acc: 0.970802903175354)
[2025-02-13 20:45:04,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:04,231][root][INFO] - Training Epoch: 2/2, step 3385/7134 completed (loss: 0.035363417118787766, acc: 0.9861111044883728)
[2025-02-13 20:45:04,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:04,593][root][INFO] - Training Epoch: 2/2, step 3386/7134 completed (loss: 0.06408185511827469, acc: 0.9704142212867737)
[2025-02-13 20:45:04,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:04,947][root][INFO] - Training Epoch: 2/2, step 3387/7134 completed (loss: 0.08441591262817383, acc: 0.9825581312179565)
[2025-02-13 20:45:05,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:05,319][root][INFO] - Training Epoch: 2/2, step 3388/7134 completed (loss: 0.053513478487730026, acc: 0.9743589758872986)
[2025-02-13 20:45:05,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:05,680][root][INFO] - Training Epoch: 2/2, step 3389/7134 completed (loss: 0.08715783804655075, acc: 0.9781420826911926)
[2025-02-13 20:45:05,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:06,028][root][INFO] - Training Epoch: 2/2, step 3390/7134 completed (loss: 0.05136996880173683, acc: 0.9829545617103577)
[2025-02-13 20:45:06,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:06,372][root][INFO] - Training Epoch: 2/2, step 3391/7134 completed (loss: 0.07582752406597137, acc: 0.9817073345184326)
[2025-02-13 20:45:06,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:06,735][root][INFO] - Training Epoch: 2/2, step 3392/7134 completed (loss: 0.02628568559885025, acc: 0.9923664331436157)
[2025-02-13 20:45:06,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:07,090][root][INFO] - Training Epoch: 2/2, step 3393/7134 completed (loss: 0.09720370173454285, acc: 0.9720279574394226)
[2025-02-13 20:45:07,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:07,457][root][INFO] - Training Epoch: 2/2, step 3394/7134 completed (loss: 0.04008263722062111, acc: 1.0)
[2025-02-13 20:45:07,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:07,803][root][INFO] - Training Epoch: 2/2, step 3395/7134 completed (loss: 0.1604893058538437, acc: 0.9539473652839661)
[2025-02-13 20:45:07,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:08,168][root][INFO] - Training Epoch: 2/2, step 3396/7134 completed (loss: 0.09492723643779755, acc: 0.9807692170143127)
[2025-02-13 20:45:08,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:08,563][root][INFO] - Training Epoch: 2/2, step 3397/7134 completed (loss: 0.03893887624144554, acc: 0.9900497794151306)
[2025-02-13 20:45:08,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:08,936][root][INFO] - Training Epoch: 2/2, step 3398/7134 completed (loss: 0.0666293278336525, acc: 0.97826087474823)
[2025-02-13 20:45:09,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:09,295][root][INFO] - Training Epoch: 2/2, step 3399/7134 completed (loss: 0.16878966987133026, acc: 0.9583333134651184)
[2025-02-13 20:45:09,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:09,681][root][INFO] - Training Epoch: 2/2, step 3400/7134 completed (loss: 0.0700409859418869, acc: 0.9685039520263672)
[2025-02-13 20:45:09,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:10,066][root][INFO] - Training Epoch: 2/2, step 3401/7134 completed (loss: 0.08773138374090195, acc: 0.9863945841789246)
[2025-02-13 20:45:10,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:10,465][root][INFO] - Training Epoch: 2/2, step 3402/7134 completed (loss: 0.2914786636829376, acc: 0.9518716335296631)
[2025-02-13 20:45:10,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:10,867][root][INFO] - Training Epoch: 2/2, step 3403/7134 completed (loss: 0.1711721271276474, acc: 0.9595959782600403)
[2025-02-13 20:45:11,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:11,247][root][INFO] - Training Epoch: 2/2, step 3404/7134 completed (loss: 0.03897722810506821, acc: 0.9874213933944702)
[2025-02-13 20:45:11,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:11,625][root][INFO] - Training Epoch: 2/2, step 3405/7134 completed (loss: 0.21397699415683746, acc: 0.9485294222831726)
[2025-02-13 20:45:11,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:12,012][root][INFO] - Training Epoch: 2/2, step 3406/7134 completed (loss: 0.21124190092086792, acc: 0.9350000023841858)
[2025-02-13 20:45:12,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:12,366][root][INFO] - Training Epoch: 2/2, step 3407/7134 completed (loss: 0.08312831819057465, acc: 0.987730085849762)
[2025-02-13 20:45:12,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:12,734][root][INFO] - Training Epoch: 2/2, step 3408/7134 completed (loss: 0.03452751040458679, acc: 0.9905660152435303)
[2025-02-13 20:45:12,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:13,135][root][INFO] - Training Epoch: 2/2, step 3409/7134 completed (loss: 0.14473740756511688, acc: 0.977011501789093)
[2025-02-13 20:45:13,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:13,504][root][INFO] - Training Epoch: 2/2, step 3410/7134 completed (loss: 0.17997632920742035, acc: 0.9634146094322205)
[2025-02-13 20:45:13,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:13,871][root][INFO] - Training Epoch: 2/2, step 3411/7134 completed (loss: 0.07858452945947647, acc: 0.9835164546966553)
[2025-02-13 20:45:13,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:14,222][root][INFO] - Training Epoch: 2/2, step 3412/7134 completed (loss: 0.1286897510290146, acc: 0.9750000238418579)
[2025-02-13 20:45:14,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:14,582][root][INFO] - Training Epoch: 2/2, step 3413/7134 completed (loss: 0.08411779999732971, acc: 0.9914529919624329)
[2025-02-13 20:45:14,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:14,935][root][INFO] - Training Epoch: 2/2, step 3414/7134 completed (loss: 0.0465460866689682, acc: 0.9791666865348816)
[2025-02-13 20:45:15,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:15,306][root][INFO] - Training Epoch: 2/2, step 3415/7134 completed (loss: 0.03723416477441788, acc: 0.9928571581840515)
[2025-02-13 20:45:15,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:15,694][root][INFO] - Training Epoch: 2/2, step 3416/7134 completed (loss: 0.13977651298046112, acc: 0.9617486596107483)
[2025-02-13 20:45:15,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:16,062][root][INFO] - Training Epoch: 2/2, step 3417/7134 completed (loss: 0.05968242883682251, acc: 0.9880239367485046)
[2025-02-13 20:45:16,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:16,426][root][INFO] - Training Epoch: 2/2, step 3418/7134 completed (loss: 0.0809803381562233, acc: 0.9754601120948792)
[2025-02-13 20:45:16,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:16,782][root][INFO] - Training Epoch: 2/2, step 3419/7134 completed (loss: 0.08429911732673645, acc: 0.9759036302566528)
[2025-02-13 20:45:16,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:17,155][root][INFO] - Training Epoch: 2/2, step 3420/7134 completed (loss: 0.01306402962654829, acc: 1.0)
[2025-02-13 20:45:17,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:17,523][root][INFO] - Training Epoch: 2/2, step 3421/7134 completed (loss: 0.06920115649700165, acc: 0.9743589758872986)
[2025-02-13 20:45:17,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:17,869][root][INFO] - Training Epoch: 2/2, step 3422/7134 completed (loss: 0.1386042833328247, acc: 0.9852941036224365)
[2025-02-13 20:45:18,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:18,237][root][INFO] - Training Epoch: 2/2, step 3423/7134 completed (loss: 0.022165572270751, acc: 1.0)
[2025-02-13 20:45:18,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:18,590][root][INFO] - Training Epoch: 2/2, step 3424/7134 completed (loss: 0.08519654721021652, acc: 0.9866666793823242)
[2025-02-13 20:45:18,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:18,947][root][INFO] - Training Epoch: 2/2, step 3425/7134 completed (loss: 0.08305124938488007, acc: 0.9789473414421082)
[2025-02-13 20:45:19,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:19,309][root][INFO] - Training Epoch: 2/2, step 3426/7134 completed (loss: 0.12913544476032257, acc: 0.988095223903656)
[2025-02-13 20:45:19,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:19,697][root][INFO] - Training Epoch: 2/2, step 3427/7134 completed (loss: 0.0757022425532341, acc: 0.9935064911842346)
[2025-02-13 20:45:19,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:20,088][root][INFO] - Training Epoch: 2/2, step 3428/7134 completed (loss: 0.09268045425415039, acc: 0.9764705896377563)
[2025-02-13 20:45:20,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:20,464][root][INFO] - Training Epoch: 2/2, step 3429/7134 completed (loss: 0.05159979313611984, acc: 0.9929577708244324)
[2025-02-13 20:45:20,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:20,823][root][INFO] - Training Epoch: 2/2, step 3430/7134 completed (loss: 0.1856611967086792, acc: 0.9589040875434875)
[2025-02-13 20:45:20,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:21,178][root][INFO] - Training Epoch: 2/2, step 3431/7134 completed (loss: 0.11198337376117706, acc: 0.97826087474823)
[2025-02-13 20:45:21,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:21,544][root][INFO] - Training Epoch: 2/2, step 3432/7134 completed (loss: 0.04888906702399254, acc: 0.987730085849762)
[2025-02-13 20:45:21,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:21,916][root][INFO] - Training Epoch: 2/2, step 3433/7134 completed (loss: 0.09750320762395859, acc: 0.9632353186607361)
[2025-02-13 20:45:22,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:22,262][root][INFO] - Training Epoch: 2/2, step 3434/7134 completed (loss: 0.07451024651527405, acc: 0.9857142567634583)
[2025-02-13 20:45:22,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:22,625][root][INFO] - Training Epoch: 2/2, step 3435/7134 completed (loss: 0.08402799069881439, acc: 0.9793103337287903)
[2025-02-13 20:45:22,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:22,994][root][INFO] - Training Epoch: 2/2, step 3436/7134 completed (loss: 0.09568030387163162, acc: 0.9781022071838379)
[2025-02-13 20:45:23,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:23,351][root][INFO] - Training Epoch: 2/2, step 3437/7134 completed (loss: 0.0758277103304863, acc: 0.9834710955619812)
[2025-02-13 20:45:23,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:23,734][root][INFO] - Training Epoch: 2/2, step 3438/7134 completed (loss: 0.13168969750404358, acc: 0.9824561476707458)
[2025-02-13 20:45:23,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:24,092][root][INFO] - Training Epoch: 2/2, step 3439/7134 completed (loss: 0.13333620131015778, acc: 0.976190447807312)
[2025-02-13 20:45:24,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:24,455][root][INFO] - Training Epoch: 2/2, step 3440/7134 completed (loss: 0.15128716826438904, acc: 0.9568965435028076)
[2025-02-13 20:45:24,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:24,849][root][INFO] - Training Epoch: 2/2, step 3441/7134 completed (loss: 0.1520768404006958, acc: 0.9534883499145508)
[2025-02-13 20:45:24,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:25,221][root][INFO] - Training Epoch: 2/2, step 3442/7134 completed (loss: 0.044105853885412216, acc: 0.9929078221321106)
[2025-02-13 20:45:25,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:25,600][root][INFO] - Training Epoch: 2/2, step 3443/7134 completed (loss: 0.2485058456659317, acc: 0.966292142868042)
[2025-02-13 20:45:25,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:25,998][root][INFO] - Training Epoch: 2/2, step 3444/7134 completed (loss: 0.12773582339286804, acc: 0.9490445852279663)
[2025-02-13 20:45:26,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:26,353][root][INFO] - Training Epoch: 2/2, step 3445/7134 completed (loss: 0.2477095127105713, acc: 0.9624060392379761)
[2025-02-13 20:45:26,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:26,727][root][INFO] - Training Epoch: 2/2, step 3446/7134 completed (loss: 0.15560747683048248, acc: 0.9599999785423279)
[2025-02-13 20:45:26,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:27,102][root][INFO] - Training Epoch: 2/2, step 3447/7134 completed (loss: 0.1787751317024231, acc: 0.9506173133850098)
[2025-02-13 20:45:27,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:27,458][root][INFO] - Training Epoch: 2/2, step 3448/7134 completed (loss: 0.1414620578289032, acc: 0.9595375657081604)
[2025-02-13 20:45:27,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:27,809][root][INFO] - Training Epoch: 2/2, step 3449/7134 completed (loss: 0.049349766224622726, acc: 1.0)
[2025-02-13 20:45:27,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:28,176][root][INFO] - Training Epoch: 2/2, step 3450/7134 completed (loss: 0.11212877184152603, acc: 0.9803921580314636)
[2025-02-13 20:45:28,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:28,538][root][INFO] - Training Epoch: 2/2, step 3451/7134 completed (loss: 0.13539892435073853, acc: 0.9503546357154846)
[2025-02-13 20:45:28,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:28,899][root][INFO] - Training Epoch: 2/2, step 3452/7134 completed (loss: 0.25778865814208984, acc: 0.9268292784690857)
[2025-02-13 20:45:29,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:29,248][root][INFO] - Training Epoch: 2/2, step 3453/7134 completed (loss: 0.11108272522687912, acc: 0.9722222089767456)
[2025-02-13 20:45:29,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:29,623][root][INFO] - Training Epoch: 2/2, step 3454/7134 completed (loss: 0.10445200651884079, acc: 0.9638554453849792)
[2025-02-13 20:45:29,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:29,980][root][INFO] - Training Epoch: 2/2, step 3455/7134 completed (loss: 0.0292369294911623, acc: 1.0)
[2025-02-13 20:45:30,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:30,351][root][INFO] - Training Epoch: 2/2, step 3456/7134 completed (loss: 0.04210821911692619, acc: 0.9899497628211975)
[2025-02-13 20:45:30,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:30,729][root][INFO] - Training Epoch: 2/2, step 3457/7134 completed (loss: 0.05203285440802574, acc: 0.9929577708244324)
[2025-02-13 20:45:30,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:31,146][root][INFO] - Training Epoch: 2/2, step 3458/7134 completed (loss: 0.13164907693862915, acc: 0.9780219793319702)
[2025-02-13 20:45:31,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:31,516][root][INFO] - Training Epoch: 2/2, step 3459/7134 completed (loss: 0.08736947178840637, acc: 0.9748427867889404)
[2025-02-13 20:45:31,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:31,859][root][INFO] - Training Epoch: 2/2, step 3460/7134 completed (loss: 0.4262830913066864, acc: 0.9090909361839294)
[2025-02-13 20:45:31,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:32,197][root][INFO] - Training Epoch: 2/2, step 3461/7134 completed (loss: 0.42828118801116943, acc: 0.8769230842590332)
[2025-02-13 20:45:32,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:32,587][root][INFO] - Training Epoch: 2/2, step 3462/7134 completed (loss: 0.21754984557628632, acc: 0.9707602262496948)
[2025-02-13 20:45:32,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:32,959][root][INFO] - Training Epoch: 2/2, step 3463/7134 completed (loss: 0.04079420119524002, acc: 0.9876543283462524)
[2025-02-13 20:45:33,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:33,318][root][INFO] - Training Epoch: 2/2, step 3464/7134 completed (loss: 0.05755472183227539, acc: 0.9775280952453613)
[2025-02-13 20:45:33,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:33,677][root][INFO] - Training Epoch: 2/2, step 3465/7134 completed (loss: 0.0779828205704689, acc: 0.9629629850387573)
[2025-02-13 20:45:33,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:34,044][root][INFO] - Training Epoch: 2/2, step 3466/7134 completed (loss: 0.036797378212213516, acc: 0.9938271641731262)
[2025-02-13 20:45:34,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:34,419][root][INFO] - Training Epoch: 2/2, step 3467/7134 completed (loss: 0.017450077459216118, acc: 1.0)
[2025-02-13 20:45:34,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:34,813][root][INFO] - Training Epoch: 2/2, step 3468/7134 completed (loss: 0.02027985267341137, acc: 1.0)
[2025-02-13 20:45:34,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:35,180][root][INFO] - Training Epoch: 2/2, step 3469/7134 completed (loss: 0.03053661249577999, acc: 1.0)
[2025-02-13 20:45:35,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:35,538][root][INFO] - Training Epoch: 2/2, step 3470/7134 completed (loss: 0.014258726499974728, acc: 1.0)
[2025-02-13 20:45:35,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:35,894][root][INFO] - Training Epoch: 2/2, step 3471/7134 completed (loss: 0.0397338904440403, acc: 0.9888888597488403)
[2025-02-13 20:45:36,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:36,266][root][INFO] - Training Epoch: 2/2, step 3472/7134 completed (loss: 0.027371812611818314, acc: 0.993630588054657)
[2025-02-13 20:45:36,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:36,690][root][INFO] - Training Epoch: 2/2, step 3473/7134 completed (loss: 0.04639481380581856, acc: 0.9868420958518982)
[2025-02-13 20:45:36,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:37,040][root][INFO] - Training Epoch: 2/2, step 3474/7134 completed (loss: 0.06702841073274612, acc: 0.9855072498321533)
[2025-02-13 20:45:37,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:37,392][root][INFO] - Training Epoch: 2/2, step 3475/7134 completed (loss: 0.0910324826836586, acc: 0.9772727489471436)
[2025-02-13 20:45:37,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:37,780][root][INFO] - Training Epoch: 2/2, step 3476/7134 completed (loss: 0.19288696348667145, acc: 0.9589040875434875)
[2025-02-13 20:45:37,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:38,135][root][INFO] - Training Epoch: 2/2, step 3477/7134 completed (loss: 0.09744193404912949, acc: 0.9932432174682617)
[2025-02-13 20:45:38,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:38,490][root][INFO] - Training Epoch: 2/2, step 3478/7134 completed (loss: 0.016069335862994194, acc: 1.0)
[2025-02-13 20:45:38,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:38,902][root][INFO] - Training Epoch: 2/2, step 3479/7134 completed (loss: 0.008584937080740929, acc: 1.0)
[2025-02-13 20:45:39,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:39,293][root][INFO] - Training Epoch: 2/2, step 3480/7134 completed (loss: 0.10594842582941055, acc: 0.9670329689979553)
[2025-02-13 20:45:39,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:39,663][root][INFO] - Training Epoch: 2/2, step 3481/7134 completed (loss: 0.06873438507318497, acc: 0.9849624037742615)
[2025-02-13 20:45:39,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:40,029][root][INFO] - Training Epoch: 2/2, step 3482/7134 completed (loss: 0.006506995763629675, acc: 1.0)
[2025-02-13 20:45:40,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:40,413][root][INFO] - Training Epoch: 2/2, step 3483/7134 completed (loss: 0.01526323426514864, acc: 1.0)
[2025-02-13 20:45:40,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:40,787][root][INFO] - Training Epoch: 2/2, step 3484/7134 completed (loss: 0.06030849739909172, acc: 0.9821428656578064)
[2025-02-13 20:45:40,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:41,144][root][INFO] - Training Epoch: 2/2, step 3485/7134 completed (loss: 0.09650108218193054, acc: 0.9696969985961914)
[2025-02-13 20:45:41,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:41,515][root][INFO] - Training Epoch: 2/2, step 3486/7134 completed (loss: 0.15224207937717438, acc: 0.9572649598121643)
[2025-02-13 20:45:41,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:41,908][root][INFO] - Training Epoch: 2/2, step 3487/7134 completed (loss: 0.10167323052883148, acc: 0.9698795080184937)
[2025-02-13 20:45:42,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:42,285][root][INFO] - Training Epoch: 2/2, step 3488/7134 completed (loss: 0.05920793116092682, acc: 0.9785714149475098)
[2025-02-13 20:45:42,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:42,675][root][INFO] - Training Epoch: 2/2, step 3489/7134 completed (loss: 0.16863508522510529, acc: 0.9407407641410828)
[2025-02-13 20:45:42,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:43,056][root][INFO] - Training Epoch: 2/2, step 3490/7134 completed (loss: 0.10932957381010056, acc: 0.9860140085220337)
[2025-02-13 20:45:43,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:43,420][root][INFO] - Training Epoch: 2/2, step 3491/7134 completed (loss: 0.117369145154953, acc: 0.987261176109314)
[2025-02-13 20:45:43,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:43,763][root][INFO] - Training Epoch: 2/2, step 3492/7134 completed (loss: 0.07077687233686447, acc: 0.9816513657569885)
[2025-02-13 20:45:43,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:44,146][root][INFO] - Training Epoch: 2/2, step 3493/7134 completed (loss: 0.07151797413825989, acc: 0.9814814925193787)
[2025-02-13 20:45:44,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:44,495][root][INFO] - Training Epoch: 2/2, step 3494/7134 completed (loss: 0.06699591875076294, acc: 0.9932432174682617)
[2025-02-13 20:45:44,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:44,831][root][INFO] - Training Epoch: 2/2, step 3495/7134 completed (loss: 0.03303348273038864, acc: 0.9896907210350037)
[2025-02-13 20:45:44,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:45,181][root][INFO] - Training Epoch: 2/2, step 3496/7134 completed (loss: 0.06817174702882767, acc: 0.9901960492134094)
[2025-02-13 20:45:45,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:45,556][root][INFO] - Training Epoch: 2/2, step 3497/7134 completed (loss: 0.07309825718402863, acc: 0.9873417615890503)
[2025-02-13 20:45:45,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:45,914][root][INFO] - Training Epoch: 2/2, step 3498/7134 completed (loss: 0.13754507899284363, acc: 0.9683544039726257)
[2025-02-13 20:45:46,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:46,309][root][INFO] - Training Epoch: 2/2, step 3499/7134 completed (loss: 0.13044673204421997, acc: 0.9803921580314636)
[2025-02-13 20:45:46,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:46,646][root][INFO] - Training Epoch: 2/2, step 3500/7134 completed (loss: 0.13227567076683044, acc: 0.9707602262496948)
[2025-02-13 20:45:46,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:47,003][root][INFO] - Training Epoch: 2/2, step 3501/7134 completed (loss: 0.12807343900203705, acc: 0.9545454382896423)
[2025-02-13 20:45:47,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:47,431][root][INFO] - Training Epoch: 2/2, step 3502/7134 completed (loss: 0.16433598101139069, acc: 0.9527027010917664)
[2025-02-13 20:45:47,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:47,809][root][INFO] - Training Epoch: 2/2, step 3503/7134 completed (loss: 0.2004404515028, acc: 0.9640287756919861)
[2025-02-13 20:45:47,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:48,187][root][INFO] - Training Epoch: 2/2, step 3504/7134 completed (loss: 0.013905787840485573, acc: 1.0)
[2025-02-13 20:45:48,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:48,554][root][INFO] - Training Epoch: 2/2, step 3505/7134 completed (loss: 0.0531512126326561, acc: 0.9841269850730896)
[2025-02-13 20:45:48,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:48,905][root][INFO] - Training Epoch: 2/2, step 3506/7134 completed (loss: 0.04595193639397621, acc: 0.9922480583190918)
[2025-02-13 20:45:49,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:49,290][root][INFO] - Training Epoch: 2/2, step 3507/7134 completed (loss: 0.04552198946475983, acc: 0.9861111044883728)
[2025-02-13 20:45:49,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:49,657][root][INFO] - Training Epoch: 2/2, step 3508/7134 completed (loss: 0.031549133360385895, acc: 0.9867549538612366)
[2025-02-13 20:45:49,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:50,031][root][INFO] - Training Epoch: 2/2, step 3509/7134 completed (loss: 0.09507518261671066, acc: 0.9631901979446411)
[2025-02-13 20:45:50,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:50,372][root][INFO] - Training Epoch: 2/2, step 3510/7134 completed (loss: 0.04320725426077843, acc: 0.9849624037742615)
[2025-02-13 20:45:50,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:50,749][root][INFO] - Training Epoch: 2/2, step 3511/7134 completed (loss: 0.02086258865892887, acc: 0.9926470518112183)
[2025-02-13 20:45:50,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:51,103][root][INFO] - Training Epoch: 2/2, step 3512/7134 completed (loss: 0.19494755566120148, acc: 0.9615384340286255)
[2025-02-13 20:45:51,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:51,479][root][INFO] - Training Epoch: 2/2, step 3513/7134 completed (loss: 0.020502768456935883, acc: 0.9927536249160767)
[2025-02-13 20:45:51,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:51,870][root][INFO] - Training Epoch: 2/2, step 3514/7134 completed (loss: 0.04749160632491112, acc: 0.9856459498405457)
[2025-02-13 20:45:52,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:52,247][root][INFO] - Training Epoch: 2/2, step 3515/7134 completed (loss: 0.017606589943170547, acc: 0.994350254535675)
[2025-02-13 20:45:52,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:52,633][root][INFO] - Training Epoch: 2/2, step 3516/7134 completed (loss: 0.014759695157408714, acc: 1.0)
[2025-02-13 20:45:52,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:53,013][root][INFO] - Training Epoch: 2/2, step 3517/7134 completed (loss: 0.023892156779766083, acc: 0.9949495196342468)
[2025-02-13 20:45:53,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:53,390][root][INFO] - Training Epoch: 2/2, step 3518/7134 completed (loss: 0.0525909960269928, acc: 0.9851852059364319)
[2025-02-13 20:45:53,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:53,767][root][INFO] - Training Epoch: 2/2, step 3519/7134 completed (loss: 0.048249755054712296, acc: 0.995555579662323)
[2025-02-13 20:45:53,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:54,143][root][INFO] - Training Epoch: 2/2, step 3520/7134 completed (loss: 0.10638203471899033, acc: 0.9848484992980957)
[2025-02-13 20:45:54,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:54,524][root][INFO] - Training Epoch: 2/2, step 3521/7134 completed (loss: 0.035896118730306625, acc: 0.9820359349250793)
[2025-02-13 20:45:54,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:54,875][root][INFO] - Training Epoch: 2/2, step 3522/7134 completed (loss: 0.03664286807179451, acc: 0.9888888597488403)
[2025-02-13 20:45:55,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:55,279][root][INFO] - Training Epoch: 2/2, step 3523/7134 completed (loss: 0.01143612153828144, acc: 1.0)
[2025-02-13 20:45:55,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:55,646][root][INFO] - Training Epoch: 2/2, step 3524/7134 completed (loss: 0.03708183020353317, acc: 0.9945054650306702)
[2025-02-13 20:45:55,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:56,009][root][INFO] - Training Epoch: 2/2, step 3525/7134 completed (loss: 0.02325516752898693, acc: 1.0)
[2025-02-13 20:45:56,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:56,385][root][INFO] - Training Epoch: 2/2, step 3526/7134 completed (loss: 0.07576858252286911, acc: 0.9848484992980957)
[2025-02-13 20:45:56,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:56,768][root][INFO] - Training Epoch: 2/2, step 3527/7134 completed (loss: 0.07475162297487259, acc: 0.9734042286872864)
[2025-02-13 20:45:56,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:57,134][root][INFO] - Training Epoch: 2/2, step 3528/7134 completed (loss: 0.0165855810046196, acc: 0.995192289352417)
[2025-02-13 20:45:57,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:57,488][root][INFO] - Training Epoch: 2/2, step 3529/7134 completed (loss: 0.0180096086114645, acc: 1.0)
[2025-02-13 20:45:57,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:57,910][root][INFO] - Training Epoch: 2/2, step 3530/7134 completed (loss: 0.08198043704032898, acc: 0.982300877571106)
[2025-02-13 20:45:58,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:58,269][root][INFO] - Training Epoch: 2/2, step 3531/7134 completed (loss: 0.04973817616701126, acc: 0.9937499761581421)
[2025-02-13 20:45:58,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:58,632][root][INFO] - Training Epoch: 2/2, step 3532/7134 completed (loss: 0.013382861390709877, acc: 1.0)
[2025-02-13 20:45:58,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:58,993][root][INFO] - Training Epoch: 2/2, step 3533/7134 completed (loss: 0.04570231959223747, acc: 0.9911110997200012)
[2025-02-13 20:45:59,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:59,365][root][INFO] - Training Epoch: 2/2, step 3534/7134 completed (loss: 0.043249599635601044, acc: 0.9824561476707458)
[2025-02-13 20:45:59,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:59,735][root][INFO] - Training Epoch: 2/2, step 3535/7134 completed (loss: 0.050910286605358124, acc: 0.9856114983558655)
[2025-02-13 20:45:59,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:00,117][root][INFO] - Training Epoch: 2/2, step 3536/7134 completed (loss: 0.0482381135225296, acc: 0.9938271641731262)
[2025-02-13 20:46:00,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:00,483][root][INFO] - Training Epoch: 2/2, step 3537/7134 completed (loss: 0.0728818029165268, acc: 0.984375)
[2025-02-13 20:46:00,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:00,843][root][INFO] - Training Epoch: 2/2, step 3538/7134 completed (loss: 0.05695553869009018, acc: 0.9831932783126831)
[2025-02-13 20:46:00,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:01,170][root][INFO] - Training Epoch: 2/2, step 3539/7134 completed (loss: 0.06023359298706055, acc: 0.977011501789093)
[2025-02-13 20:46:01,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:01,517][root][INFO] - Training Epoch: 2/2, step 3540/7134 completed (loss: 0.05833463743329048, acc: 0.9846153855323792)
[2025-02-13 20:46:01,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:01,887][root][INFO] - Training Epoch: 2/2, step 3541/7134 completed (loss: 0.04540839046239853, acc: 1.0)
[2025-02-13 20:46:02,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02,245][root][INFO] - Training Epoch: 2/2, step 3542/7134 completed (loss: 0.03736657276749611, acc: 0.9931507110595703)
[2025-02-13 20:46:02,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02,595][root][INFO] - Training Epoch: 2/2, step 3543/7134 completed (loss: 0.07758691906929016, acc: 0.9788732528686523)
[2025-02-13 20:46:02,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02,979][root][INFO] - Training Epoch: 2/2, step 3544/7134 completed (loss: 0.09643573313951492, acc: 0.9801324605941772)
[2025-02-13 20:46:03,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:03,359][root][INFO] - Training Epoch: 2/2, step 3545/7134 completed (loss: 0.08135344088077545, acc: 0.9861111044883728)
[2025-02-13 20:46:03,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:03,727][root][INFO] - Training Epoch: 2/2, step 3546/7134 completed (loss: 0.0671527311205864, acc: 0.9856114983558655)
[2025-02-13 20:46:03,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:04,111][root][INFO] - Training Epoch: 2/2, step 3547/7134 completed (loss: 0.10480806231498718, acc: 0.9717513918876648)
[2025-02-13 20:46:04,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:04,487][root][INFO] - Training Epoch: 2/2, step 3548/7134 completed (loss: 0.03254956379532814, acc: 0.9908257126808167)
[2025-02-13 20:46:04,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:04,857][root][INFO] - Training Epoch: 2/2, step 3549/7134 completed (loss: 0.036600906401872635, acc: 1.0)
[2025-02-13 20:46:04,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:05,213][root][INFO] - Training Epoch: 2/2, step 3550/7134 completed (loss: 0.1073775365948677, acc: 0.9781022071838379)
[2025-02-13 20:46:05,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:05,590][root][INFO] - Training Epoch: 2/2, step 3551/7134 completed (loss: 0.12843719124794006, acc: 0.9591836929321289)
[2025-02-13 20:46:05,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:05,977][root][INFO] - Training Epoch: 2/2, step 3552/7134 completed (loss: 0.10226450860500336, acc: 0.9842519760131836)
[2025-02-13 20:46:06,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:06,333][root][INFO] - Training Epoch: 2/2, step 3553/7134 completed (loss: 0.09727925062179565, acc: 0.9632353186607361)
[2025-02-13 20:46:06,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:06,696][root][INFO] - Training Epoch: 2/2, step 3554/7134 completed (loss: 0.21052336692810059, acc: 0.9509202241897583)
[2025-02-13 20:46:06,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:07,052][root][INFO] - Training Epoch: 2/2, step 3555/7134 completed (loss: 0.22771261632442474, acc: 0.9290322661399841)
[2025-02-13 20:46:07,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:07,389][root][INFO] - Training Epoch: 2/2, step 3556/7134 completed (loss: 0.128640815615654, acc: 0.9545454382896423)
[2025-02-13 20:46:07,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:07,765][root][INFO] - Training Epoch: 2/2, step 3557/7134 completed (loss: 0.15529528260231018, acc: 0.957446813583374)
[2025-02-13 20:46:07,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:08,155][root][INFO] - Training Epoch: 2/2, step 3558/7134 completed (loss: 0.13754691183567047, acc: 0.9520000219345093)
[2025-02-13 20:46:08,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:08,503][root][INFO] - Training Epoch: 2/2, step 3559/7134 completed (loss: 0.0803036317229271, acc: 0.976190447807312)
[2025-02-13 20:46:08,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:08,850][root][INFO] - Training Epoch: 2/2, step 3560/7134 completed (loss: 0.07420959323644638, acc: 0.9781420826911926)
[2025-02-13 20:46:08,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:09,214][root][INFO] - Training Epoch: 2/2, step 3561/7134 completed (loss: 0.07608073204755783, acc: 0.9774011373519897)
[2025-02-13 20:46:09,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:09,558][root][INFO] - Training Epoch: 2/2, step 3562/7134 completed (loss: 0.06031608581542969, acc: 0.9852941036224365)
[2025-02-13 20:46:09,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:09,902][root][INFO] - Training Epoch: 2/2, step 3563/7134 completed (loss: 0.08882198482751846, acc: 0.9716312289237976)
[2025-02-13 20:46:10,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:11,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:11,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:11,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:12,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:12,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:12,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:13,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:13,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:13,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:15,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:15,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:15,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:16,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:16,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:16,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:17,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:17,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:17,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:18,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:18,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:18,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:19,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:19,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:19,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:20,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:20,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:20,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:21,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:21,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:21,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:22,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:22,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:22,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:23,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:23,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:23,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:24,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:24,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:24,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:25,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:25,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:25,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:26,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:26,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:26,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:27,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:27,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:27,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:28,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:28,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:28,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:29,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:29,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:29,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:31,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:31,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:31,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:32,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:32,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:32,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:33,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:33,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:33,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:34,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:34,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:34,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:35,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:35,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:35,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:36,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:36,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:36,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:37,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:37,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:37,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:37,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:38,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:38,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:38,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:39,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:39,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:39,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:40,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:40,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:40,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:42,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:42,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:42,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:43,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:43,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:43,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:44,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:44,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:45,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:45,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:45,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:46,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:46,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:46,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:48,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:48,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:49,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:49,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:49,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:50,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:50,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:50,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:51,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:51,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:51,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:53,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:53,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:53,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:54,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:54,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:54,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:55,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:55,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:55,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:56,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:56,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:56,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:57,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:57,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:57,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:58,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:58,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:58,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:59,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:59,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:59,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:00,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:00,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:00,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:00,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:01,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:01,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:01,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:02,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:02,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:02,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:03,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:03,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:05,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:05,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:05,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:06,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:06,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:06,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:07,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:07,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:07,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:08,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:08,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:08,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:09,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:09,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:10,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:10,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:10,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:11,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:11,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:11,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:12,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:12,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:12,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:13,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:13,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:13,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:14,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:14,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:14,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:15,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:15,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:15,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:16,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:16,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:16,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:17,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:17,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:17,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:19,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:19,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:20,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:20,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:20,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:21,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:21,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:21,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:22,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:22,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:22,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:23,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:23,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:23,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:25,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:25,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:25,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:26,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:26,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:26,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:26,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:27,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:27,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:27,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:28,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:28,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:28,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:29,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:29,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:29,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:30,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:30,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:31,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:31,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:31,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:32,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:32,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:32,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:33,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:33,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:34,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:34,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:34,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:35,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:35,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:35,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:36,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:36,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:36,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:37,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:37,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:38,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:38,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:38,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:39,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:39,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:40,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:40,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:40,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:41,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:41,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:41,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:42,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:42,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:43,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:43,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:43,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:44,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:44,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:45,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:45,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:45,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:46,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:46,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:47,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:47,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:47,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:48,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:48,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:48,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:49,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:49,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:50,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:50,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:52,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:52,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:52,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:54,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:54,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:54,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:55,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:55,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:55,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:56,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:56,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:56,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:57,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:57,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:58,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:58,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:58,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:59,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:59,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:59,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:00,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:00,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:00,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:01,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:01,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:01,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:02,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:02,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:02,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:03,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:03,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:03,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:04,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:04,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:04,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:05,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:05,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:06,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:06,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:06,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:07,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:07,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:07,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:08,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:08,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:09,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:09,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:09,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:10,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:10,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:10,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:11,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:11,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:11,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:12,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:12,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:13,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:13,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:14,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:14,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:14,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:15,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:15,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:15,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:16,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:16,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:16,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:17,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:17,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:17,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:18,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:18,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:18,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:19,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:19,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:19,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:20,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:20,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:20,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:21,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:21,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:21,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:22,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:22,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:22,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:24,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:24,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:24,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:26,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:26,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:26,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:27,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:27,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:27,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:28,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:28,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:28,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:29,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:29,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:29,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:30,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:30,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:30,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:31,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:31,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:31,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:32,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:32,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:32,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:32,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:33,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:33,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:33,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:34,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:34,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:34,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:36,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:36,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:36,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:37,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:37,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:37,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:38,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:38,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:38,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:39,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:39,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:39,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:40,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:40,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:40,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:41,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:41,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:41,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:41,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:42,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:42,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:42,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:43,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:43,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:44,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:44,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:44,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:45,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:45,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:45,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:45,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:46,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:46,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:46,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:47,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:47,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:47,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:48,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:48,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:48,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:49,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:49,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:49,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:50,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:50,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:50,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:51,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:51,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:51,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:52,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:52,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:52,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:53,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:53,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:54,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:54,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:54,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:55,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:55,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:55,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:56,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:56,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:56,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:56,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:57,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:57,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:57,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:58,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:58,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:59,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:59,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:59,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:00,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:00,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:00,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:00,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:01,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:01,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:01,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:02,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:02,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:04,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:04,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:04,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:05,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:05,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:05,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:06,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:06,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:06,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:08,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:08,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:08,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:09,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:09,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:09,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:10,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:10,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:10,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:11,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:11,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:11,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:11,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:12,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:12,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:12,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:13,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:13,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:13,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:14,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:14,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:15,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:15,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:15,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:16,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:16,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:16,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:17,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:17,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:17,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:18,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:18,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:18,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:19,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:19,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:19,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:20,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:20,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:20,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:21,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:21,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:21,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:22,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:22,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:24,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:24,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:25,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:25,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:25,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:26,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:26,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:26,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:27,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:27,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:28,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:28,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:28,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:28,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:29,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:29,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:30,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:30,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:30,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:31,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:31,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:31,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:32,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:32,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:32,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:33,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:33,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:33,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:34,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:34,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:34,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:35,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:35,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:35,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:36,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:36,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:36,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:37,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:37,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:37,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:38,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:38,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:38,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:39,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:39,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:40,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:40,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:41,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:41,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:41,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:42,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:42,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:42,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:43,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:43,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:43,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:44,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:44,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:44,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:45,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:45,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:45,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:46,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:46,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:47,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:47,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:47,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:48,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:48,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:48,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:49,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:49,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:49,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:50,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:50,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:50,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:50,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:51,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:51,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:51,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:52,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:52,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:54,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:54,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:54,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:55,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:55,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:56,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:56,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:56,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:57,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:57,596][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2338, device='cuda:0') eval_epoch_loss=tensor(0.2101, device='cuda:0') eval_epoch_acc=tensor(0.9490, device='cuda:0')
[2025-02-13 20:49:57,598][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:49:57,598][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:49:57,849][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_3564_loss_0.21009458601474762/model.pt
[2025-02-13 20:49:57,860][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:49:57,861][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.21009458601474762
[2025-02-13 20:49:58,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:58,285][root][INFO] - Training Epoch: 2/2, step 3564/7134 completed (loss: 0.1137130856513977, acc: 0.9603174328804016)
[2025-02-13 20:49:58,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:58,644][root][INFO] - Training Epoch: 2/2, step 3565/7134 completed (loss: 0.16991202533245087, acc: 0.9485294222831726)
[2025-02-13 20:49:58,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:58,987][root][INFO] - Training Epoch: 2/2, step 3566/7134 completed (loss: 0.11696797609329224, acc: 0.9831932783126831)
[2025-02-13 20:49:59,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:59,332][root][INFO] - Training Epoch: 2/2, step 3567/7134 completed (loss: 0.12690934538841248, acc: 0.9775280952453613)
[2025-02-13 20:49:59,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:59,687][root][INFO] - Training Epoch: 2/2, step 3568/7134 completed (loss: 0.12066259235143661, acc: 0.9645389914512634)
[2025-02-13 20:49:59,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:00,034][root][INFO] - Training Epoch: 2/2, step 3569/7134 completed (loss: 0.11873963475227356, acc: 0.9750000238418579)
[2025-02-13 20:50:00,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:00,378][root][INFO] - Training Epoch: 2/2, step 3570/7134 completed (loss: 0.3213009536266327, acc: 0.9264705777168274)
[2025-02-13 20:50:00,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:00,737][root][INFO] - Training Epoch: 2/2, step 3571/7134 completed (loss: 0.19958993792533875, acc: 0.966292142868042)
[2025-02-13 20:50:00,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:01,099][root][INFO] - Training Epoch: 2/2, step 3572/7134 completed (loss: 0.05198677256703377, acc: 0.976190447807312)
[2025-02-13 20:50:01,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:01,460][root][INFO] - Training Epoch: 2/2, step 3573/7134 completed (loss: 0.13300177454948425, acc: 0.9605262875556946)
[2025-02-13 20:50:01,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:01,807][root][INFO] - Training Epoch: 2/2, step 3574/7134 completed (loss: 0.10458576679229736, acc: 0.9752066135406494)
[2025-02-13 20:50:01,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:02,164][root][INFO] - Training Epoch: 2/2, step 3575/7134 completed (loss: 0.338072270154953, acc: 0.9473684430122375)
[2025-02-13 20:50:02,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:02,540][root][INFO] - Training Epoch: 2/2, step 3576/7134 completed (loss: 0.07112863659858704, acc: 0.9925373196601868)
[2025-02-13 20:50:02,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:02,941][root][INFO] - Training Epoch: 2/2, step 3577/7134 completed (loss: 0.1128210499882698, acc: 0.9793814420700073)
[2025-02-13 20:50:03,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:03,282][root][INFO] - Training Epoch: 2/2, step 3578/7134 completed (loss: 0.0782635435461998, acc: 0.9698795080184937)
[2025-02-13 20:50:03,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:03,647][root][INFO] - Training Epoch: 2/2, step 3579/7134 completed (loss: 0.11987590789794922, acc: 0.9709302186965942)
[2025-02-13 20:50:03,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:04,008][root][INFO] - Training Epoch: 2/2, step 3580/7134 completed (loss: 0.24457062780857086, acc: 0.9421965479850769)
[2025-02-13 20:50:04,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:04,398][root][INFO] - Training Epoch: 2/2, step 3581/7134 completed (loss: 0.14516650140285492, acc: 0.9588235020637512)
[2025-02-13 20:50:04,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:04,752][root][INFO] - Training Epoch: 2/2, step 3582/7134 completed (loss: 0.12653353810310364, acc: 0.9658119678497314)
[2025-02-13 20:50:04,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:05,128][root][INFO] - Training Epoch: 2/2, step 3583/7134 completed (loss: 0.1648057997226715, acc: 0.9520958065986633)
[2025-02-13 20:50:05,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:05,514][root][INFO] - Training Epoch: 2/2, step 3584/7134 completed (loss: 0.18731683492660522, acc: 0.9468085169792175)
[2025-02-13 20:50:05,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:05,932][root][INFO] - Training Epoch: 2/2, step 3585/7134 completed (loss: 0.19130849838256836, acc: 0.9509202241897583)
[2025-02-13 20:50:06,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:06,292][root][INFO] - Training Epoch: 2/2, step 3586/7134 completed (loss: 0.21061255037784576, acc: 0.9490445852279663)
[2025-02-13 20:50:06,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:06,666][root][INFO] - Training Epoch: 2/2, step 3587/7134 completed (loss: 0.16615277528762817, acc: 0.9650349617004395)
[2025-02-13 20:50:06,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:07,027][root][INFO] - Training Epoch: 2/2, step 3588/7134 completed (loss: 0.11740275472402573, acc: 0.9595375657081604)
[2025-02-13 20:50:07,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:07,396][root][INFO] - Training Epoch: 2/2, step 3589/7134 completed (loss: 0.18864336609840393, acc: 0.9781420826911926)
[2025-02-13 20:50:07,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:07,759][root][INFO] - Training Epoch: 2/2, step 3590/7134 completed (loss: 0.049062080681324005, acc: 0.9833333492279053)
[2025-02-13 20:50:07,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:08,079][root][INFO] - Training Epoch: 2/2, step 3591/7134 completed (loss: 0.11691876500844955, acc: 0.9707602262496948)
[2025-02-13 20:50:08,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:08,427][root][INFO] - Training Epoch: 2/2, step 3592/7134 completed (loss: 0.039522211998701096, acc: 0.9928571581840515)
[2025-02-13 20:50:08,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:08,829][root][INFO] - Training Epoch: 2/2, step 3593/7134 completed (loss: 0.28291040658950806, acc: 0.9718309640884399)
[2025-02-13 20:50:08,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:09,186][root][INFO] - Training Epoch: 2/2, step 3594/7134 completed (loss: 0.08316615968942642, acc: 0.9801324605941772)
[2025-02-13 20:50:09,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:09,533][root][INFO] - Training Epoch: 2/2, step 3595/7134 completed (loss: 0.07930638641119003, acc: 0.9738562107086182)
[2025-02-13 20:50:09,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:09,875][root][INFO] - Training Epoch: 2/2, step 3596/7134 completed (loss: 0.1425483524799347, acc: 0.970588207244873)
[2025-02-13 20:50:10,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:10,225][root][INFO] - Training Epoch: 2/2, step 3597/7134 completed (loss: 0.06463844329118729, acc: 0.9934210777282715)
[2025-02-13 20:50:10,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:10,608][root][INFO] - Training Epoch: 2/2, step 3598/7134 completed (loss: 0.14034415781497955, acc: 0.9573459625244141)
[2025-02-13 20:50:10,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:10,972][root][INFO] - Training Epoch: 2/2, step 3599/7134 completed (loss: 0.19402626156806946, acc: 0.9243243336677551)
[2025-02-13 20:50:11,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:11,344][root][INFO] - Training Epoch: 2/2, step 3600/7134 completed (loss: 0.15618662536144257, acc: 0.9585492014884949)
[2025-02-13 20:50:11,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:11,710][root][INFO] - Training Epoch: 2/2, step 3601/7134 completed (loss: 0.34949734807014465, acc: 0.898876428604126)
[2025-02-13 20:50:11,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:12,067][root][INFO] - Training Epoch: 2/2, step 3602/7134 completed (loss: 0.2656060457229614, acc: 0.934883713722229)
[2025-02-13 20:50:12,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:12,412][root][INFO] - Training Epoch: 2/2, step 3603/7134 completed (loss: 0.10380033403635025, acc: 0.9685534834861755)
[2025-02-13 20:50:12,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:12,795][root][INFO] - Training Epoch: 2/2, step 3604/7134 completed (loss: 0.12391634285449982, acc: 0.9685039520263672)
[2025-02-13 20:50:12,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:13,171][root][INFO] - Training Epoch: 2/2, step 3605/7134 completed (loss: 0.15600921213626862, acc: 0.9641255736351013)
[2025-02-13 20:50:13,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:13,534][root][INFO] - Training Epoch: 2/2, step 3606/7134 completed (loss: 0.25151944160461426, acc: 0.9452054500579834)
[2025-02-13 20:50:13,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:13,886][root][INFO] - Training Epoch: 2/2, step 3607/7134 completed (loss: 0.15826290845870972, acc: 0.9589040875434875)
[2025-02-13 20:50:14,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:14,248][root][INFO] - Training Epoch: 2/2, step 3608/7134 completed (loss: 0.22856058180332184, acc: 0.946601927280426)
[2025-02-13 20:50:14,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:14,669][root][INFO] - Training Epoch: 2/2, step 3609/7134 completed (loss: 0.12174047529697418, acc: 0.9578947424888611)
[2025-02-13 20:50:14,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:15,039][root][INFO] - Training Epoch: 2/2, step 3610/7134 completed (loss: 0.1586088091135025, acc: 0.9615384340286255)
[2025-02-13 20:50:15,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:15,403][root][INFO] - Training Epoch: 2/2, step 3611/7134 completed (loss: 0.1392943412065506, acc: 0.9657142758369446)
[2025-02-13 20:50:15,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:15,800][root][INFO] - Training Epoch: 2/2, step 3612/7134 completed (loss: 0.3077831566333771, acc: 0.931034505367279)
[2025-02-13 20:50:15,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:16,199][root][INFO] - Training Epoch: 2/2, step 3613/7134 completed (loss: 0.1269957572221756, acc: 0.9562841653823853)
[2025-02-13 20:50:16,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:16,568][root][INFO] - Training Epoch: 2/2, step 3614/7134 completed (loss: 0.06958196312189102, acc: 0.9840425252914429)
[2025-02-13 20:50:16,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:16,933][root][INFO] - Training Epoch: 2/2, step 3615/7134 completed (loss: 0.0756540521979332, acc: 0.9729729890823364)
[2025-02-13 20:50:17,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:17,304][root][INFO] - Training Epoch: 2/2, step 3616/7134 completed (loss: 0.18017800152301788, acc: 0.9503105878829956)
[2025-02-13 20:50:17,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:17,685][root][INFO] - Training Epoch: 2/2, step 3617/7134 completed (loss: 0.07192014157772064, acc: 0.9945945739746094)
[2025-02-13 20:50:17,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:18,047][root][INFO] - Training Epoch: 2/2, step 3618/7134 completed (loss: 0.1856449991464615, acc: 0.9631901979446411)
[2025-02-13 20:50:18,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:18,418][root][INFO] - Training Epoch: 2/2, step 3619/7134 completed (loss: 0.3054027557373047, acc: 0.9301310181617737)
[2025-02-13 20:50:18,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:18,810][root][INFO] - Training Epoch: 2/2, step 3620/7134 completed (loss: 0.09964998066425323, acc: 0.976047933101654)
[2025-02-13 20:50:18,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:19,190][root][INFO] - Training Epoch: 2/2, step 3621/7134 completed (loss: 0.08791276067495346, acc: 0.9745222926139832)
[2025-02-13 20:50:19,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:19,584][root][INFO] - Training Epoch: 2/2, step 3622/7134 completed (loss: 0.06504614651203156, acc: 0.9870129823684692)
[2025-02-13 20:50:19,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:19,937][root][INFO] - Training Epoch: 2/2, step 3623/7134 completed (loss: 0.024218309670686722, acc: 1.0)
[2025-02-13 20:50:20,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:20,316][root][INFO] - Training Epoch: 2/2, step 3624/7134 completed (loss: 0.07212430983781815, acc: 0.9856114983558655)
[2025-02-13 20:50:20,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:20,711][root][INFO] - Training Epoch: 2/2, step 3625/7134 completed (loss: 0.21104712784290314, acc: 0.9495798349380493)
[2025-02-13 20:50:20,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:21,064][root][INFO] - Training Epoch: 2/2, step 3626/7134 completed (loss: 0.20289075374603271, acc: 0.966292142868042)
[2025-02-13 20:50:21,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:21,458][root][INFO] - Training Epoch: 2/2, step 3627/7134 completed (loss: 0.2645692229270935, acc: 0.9433962106704712)
[2025-02-13 20:50:21,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:21,818][root][INFO] - Training Epoch: 2/2, step 3628/7134 completed (loss: 0.1824006289243698, acc: 0.9523809552192688)
[2025-02-13 20:50:21,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:22,159][root][INFO] - Training Epoch: 2/2, step 3629/7134 completed (loss: 0.20039008557796478, acc: 0.9596773982048035)
[2025-02-13 20:50:22,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:22,503][root][INFO] - Training Epoch: 2/2, step 3630/7134 completed (loss: 0.0664592757821083, acc: 0.9784946441650391)
[2025-02-13 20:50:22,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:22,897][root][INFO] - Training Epoch: 2/2, step 3631/7134 completed (loss: 0.09438818693161011, acc: 0.9714285731315613)
[2025-02-13 20:50:23,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:23,281][root][INFO] - Training Epoch: 2/2, step 3632/7134 completed (loss: 0.06346612423658371, acc: 0.9918032884597778)
[2025-02-13 20:50:23,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:23,650][root][INFO] - Training Epoch: 2/2, step 3633/7134 completed (loss: 0.12500141561031342, acc: 0.9745222926139832)
[2025-02-13 20:50:23,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:24,004][root][INFO] - Training Epoch: 2/2, step 3634/7134 completed (loss: 0.10943207144737244, acc: 0.9719626307487488)
[2025-02-13 20:50:24,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:24,357][root][INFO] - Training Epoch: 2/2, step 3635/7134 completed (loss: 0.11479655653238297, acc: 0.9741379022598267)
[2025-02-13 20:50:24,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:24,717][root][INFO] - Training Epoch: 2/2, step 3636/7134 completed (loss: 0.07233429700136185, acc: 0.9811320900917053)
[2025-02-13 20:50:24,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:25,088][root][INFO] - Training Epoch: 2/2, step 3637/7134 completed (loss: 0.14349618554115295, acc: 0.9636363387107849)
[2025-02-13 20:50:25,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:25,449][root][INFO] - Training Epoch: 2/2, step 3638/7134 completed (loss: 0.05380146950483322, acc: 0.9890109896659851)
[2025-02-13 20:50:25,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:25,808][root][INFO] - Training Epoch: 2/2, step 3639/7134 completed (loss: 0.10438402742147446, acc: 0.978723406791687)
[2025-02-13 20:50:25,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:26,185][root][INFO] - Training Epoch: 2/2, step 3640/7134 completed (loss: 0.12770630419254303, acc: 0.9586777091026306)
[2025-02-13 20:50:26,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:26,524][root][INFO] - Training Epoch: 2/2, step 3641/7134 completed (loss: 0.04816800728440285, acc: 0.9838709831237793)
[2025-02-13 20:50:26,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:26,914][root][INFO] - Training Epoch: 2/2, step 3642/7134 completed (loss: 0.018905943259596825, acc: 1.0)
[2025-02-13 20:50:27,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:27,270][root][INFO] - Training Epoch: 2/2, step 3643/7134 completed (loss: 0.06790246069431305, acc: 0.984375)
[2025-02-13 20:50:27,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:27,663][root][INFO] - Training Epoch: 2/2, step 3644/7134 completed (loss: 0.13302311301231384, acc: 0.9662162065505981)
[2025-02-13 20:50:27,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:28,026][root][INFO] - Training Epoch: 2/2, step 3645/7134 completed (loss: 0.26124003529548645, acc: 0.9375)
[2025-02-13 20:50:28,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:28,381][root][INFO] - Training Epoch: 2/2, step 3646/7134 completed (loss: 0.31417885422706604, acc: 0.9318181872367859)
[2025-02-13 20:50:28,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:28,730][root][INFO] - Training Epoch: 2/2, step 3647/7134 completed (loss: 0.07268056273460388, acc: 0.9887640476226807)
[2025-02-13 20:50:28,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:29,097][root][INFO] - Training Epoch: 2/2, step 3648/7134 completed (loss: 0.14863669872283936, acc: 0.976331353187561)
[2025-02-13 20:50:29,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:29,449][root][INFO] - Training Epoch: 2/2, step 3649/7134 completed (loss: 0.1110888198018074, acc: 0.9798657894134521)
[2025-02-13 20:50:29,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:29,821][root][INFO] - Training Epoch: 2/2, step 3650/7134 completed (loss: 0.09554485976696014, acc: 0.9822485446929932)
[2025-02-13 20:50:29,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:30,170][root][INFO] - Training Epoch: 2/2, step 3651/7134 completed (loss: 0.17191733419895172, acc: 0.9698795080184937)
[2025-02-13 20:50:30,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:30,514][root][INFO] - Training Epoch: 2/2, step 3652/7134 completed (loss: 0.10048294812440872, acc: 0.9727891087532043)
[2025-02-13 20:50:30,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:30,861][root][INFO] - Training Epoch: 2/2, step 3653/7134 completed (loss: 0.2003025859594345, acc: 0.9570552110671997)
[2025-02-13 20:50:31,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:31,261][root][INFO] - Training Epoch: 2/2, step 3654/7134 completed (loss: 0.03897940367460251, acc: 0.9919999837875366)
[2025-02-13 20:50:31,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:31,685][root][INFO] - Training Epoch: 2/2, step 3655/7134 completed (loss: 0.2720995247364044, acc: 0.940119743347168)
[2025-02-13 20:50:31,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:32,036][root][INFO] - Training Epoch: 2/2, step 3656/7134 completed (loss: 0.26995036005973816, acc: 0.9420289993286133)
[2025-02-13 20:50:32,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:32,393][root][INFO] - Training Epoch: 2/2, step 3657/7134 completed (loss: 0.04147578403353691, acc: 0.9930070042610168)
[2025-02-13 20:50:32,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:32,752][root][INFO] - Training Epoch: 2/2, step 3658/7134 completed (loss: 0.1254940927028656, acc: 0.9652777910232544)
[2025-02-13 20:50:32,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:33,115][root][INFO] - Training Epoch: 2/2, step 3659/7134 completed (loss: 0.12549544870853424, acc: 0.9795918464660645)
[2025-02-13 20:50:33,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:33,431][root][INFO] - Training Epoch: 2/2, step 3660/7134 completed (loss: 0.17254558205604553, acc: 0.9629629850387573)
[2025-02-13 20:50:33,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:33,751][root][INFO] - Training Epoch: 2/2, step 3661/7134 completed (loss: 0.06929472833871841, acc: 0.9857142567634583)
[2025-02-13 20:50:33,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:34,108][root][INFO] - Training Epoch: 2/2, step 3662/7134 completed (loss: 0.26750704646110535, acc: 0.9440993666648865)
[2025-02-13 20:50:34,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:34,448][root][INFO] - Training Epoch: 2/2, step 3663/7134 completed (loss: 0.18451076745986938, acc: 0.9296875)
[2025-02-13 20:50:34,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:34,819][root][INFO] - Training Epoch: 2/2, step 3664/7134 completed (loss: 0.23372353613376617, acc: 0.9610389471054077)
[2025-02-13 20:50:34,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:35,180][root][INFO] - Training Epoch: 2/2, step 3665/7134 completed (loss: 0.14062045514583588, acc: 0.9594594836235046)
[2025-02-13 20:50:35,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:35,517][root][INFO] - Training Epoch: 2/2, step 3666/7134 completed (loss: 0.17552267014980316, acc: 0.9589040875434875)
[2025-02-13 20:50:35,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:35,902][root][INFO] - Training Epoch: 2/2, step 3667/7134 completed (loss: 0.3109314441680908, acc: 0.930232584476471)
[2025-02-13 20:50:36,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:36,248][root][INFO] - Training Epoch: 2/2, step 3668/7134 completed (loss: 0.22024478018283844, acc: 0.9724137783050537)
[2025-02-13 20:50:36,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:36,609][root][INFO] - Training Epoch: 2/2, step 3669/7134 completed (loss: 0.043369799852371216, acc: 0.9917355179786682)
[2025-02-13 20:50:36,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:36,967][root][INFO] - Training Epoch: 2/2, step 3670/7134 completed (loss: 0.03961535543203354, acc: 0.9879518151283264)
[2025-02-13 20:50:37,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:37,326][root][INFO] - Training Epoch: 2/2, step 3671/7134 completed (loss: 0.06694737076759338, acc: 0.9810126423835754)
[2025-02-13 20:50:37,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:37,697][root][INFO] - Training Epoch: 2/2, step 3672/7134 completed (loss: 0.0901314839720726, acc: 0.9834254384040833)
[2025-02-13 20:50:37,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:38,065][root][INFO] - Training Epoch: 2/2, step 3673/7134 completed (loss: 0.10651642084121704, acc: 0.9793103337287903)
[2025-02-13 20:50:38,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:38,430][root][INFO] - Training Epoch: 2/2, step 3674/7134 completed (loss: 0.03331497684121132, acc: 1.0)
[2025-02-13 20:50:38,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:38,845][root][INFO] - Training Epoch: 2/2, step 3675/7134 completed (loss: 0.08720562607049942, acc: 0.988950252532959)
[2025-02-13 20:50:38,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:39,232][root][INFO] - Training Epoch: 2/2, step 3676/7134 completed (loss: 0.08855625241994858, acc: 0.9871794581413269)
[2025-02-13 20:50:39,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:39,639][root][INFO] - Training Epoch: 2/2, step 3677/7134 completed (loss: 0.13892364501953125, acc: 0.9488636255264282)
[2025-02-13 20:50:39,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:39,992][root][INFO] - Training Epoch: 2/2, step 3678/7134 completed (loss: 0.1505671739578247, acc: 0.9620253443717957)
[2025-02-13 20:50:40,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:40,343][root][INFO] - Training Epoch: 2/2, step 3679/7134 completed (loss: 0.057450052350759506, acc: 0.9769230484962463)
[2025-02-13 20:50:40,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:40,703][root][INFO] - Training Epoch: 2/2, step 3680/7134 completed (loss: 0.11879348754882812, acc: 0.9666666388511658)
[2025-02-13 20:50:40,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:41,039][root][INFO] - Training Epoch: 2/2, step 3681/7134 completed (loss: 0.05130695551633835, acc: 0.9937499761581421)
[2025-02-13 20:50:41,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:41,409][root][INFO] - Training Epoch: 2/2, step 3682/7134 completed (loss: 0.020509250462055206, acc: 1.0)
[2025-02-13 20:50:41,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:41,819][root][INFO] - Training Epoch: 2/2, step 3683/7134 completed (loss: 0.026580501347780228, acc: 0.9926470518112183)
[2025-02-13 20:50:41,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:42,168][root][INFO] - Training Epoch: 2/2, step 3684/7134 completed (loss: 0.02242458425462246, acc: 1.0)
[2025-02-13 20:50:42,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:42,519][root][INFO] - Training Epoch: 2/2, step 3685/7134 completed (loss: 0.04819798842072487, acc: 0.9918699264526367)
[2025-02-13 20:50:42,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:42,836][root][INFO] - Training Epoch: 2/2, step 3686/7134 completed (loss: 0.05879981070756912, acc: 0.98591548204422)
[2025-02-13 20:50:42,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:43,202][root][INFO] - Training Epoch: 2/2, step 3687/7134 completed (loss: 0.03280017524957657, acc: 0.9937888383865356)
[2025-02-13 20:50:43,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:43,571][root][INFO] - Training Epoch: 2/2, step 3688/7134 completed (loss: 0.061165738850831985, acc: 0.9833333492279053)
[2025-02-13 20:50:43,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:43,936][root][INFO] - Training Epoch: 2/2, step 3689/7134 completed (loss: 0.07858351618051529, acc: 0.9858155846595764)
[2025-02-13 20:50:44,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:44,308][root][INFO] - Training Epoch: 2/2, step 3690/7134 completed (loss: 0.04184746369719505, acc: 0.9873417615890503)
[2025-02-13 20:50:44,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:44,657][root][INFO] - Training Epoch: 2/2, step 3691/7134 completed (loss: 0.05802619084715843, acc: 0.9900990128517151)
[2025-02-13 20:50:44,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:45,024][root][INFO] - Training Epoch: 2/2, step 3692/7134 completed (loss: 0.05041062831878662, acc: 0.9938650131225586)
[2025-02-13 20:50:45,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:45,410][root][INFO] - Training Epoch: 2/2, step 3693/7134 completed (loss: 0.04512808844447136, acc: 0.9924242496490479)
[2025-02-13 20:50:45,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:45,840][root][INFO] - Training Epoch: 2/2, step 3694/7134 completed (loss: 0.03941374272108078, acc: 0.987500011920929)
[2025-02-13 20:50:45,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:46,216][root][INFO] - Training Epoch: 2/2, step 3695/7134 completed (loss: 0.03836552053689957, acc: 0.993630588054657)
[2025-02-13 20:50:46,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:46,610][root][INFO] - Training Epoch: 2/2, step 3696/7134 completed (loss: 0.052336398512125015, acc: 0.9885057210922241)
[2025-02-13 20:50:46,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:46,972][root][INFO] - Training Epoch: 2/2, step 3697/7134 completed (loss: 0.05087963119149208, acc: 0.9878048896789551)
[2025-02-13 20:50:47,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:47,318][root][INFO] - Training Epoch: 2/2, step 3698/7134 completed (loss: 0.02852085791528225, acc: 1.0)
[2025-02-13 20:50:47,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:47,672][root][INFO] - Training Epoch: 2/2, step 3699/7134 completed (loss: 0.0311928391456604, acc: 0.9921259880065918)
[2025-02-13 20:50:47,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:48,045][root][INFO] - Training Epoch: 2/2, step 3700/7134 completed (loss: 0.08389246463775635, acc: 0.9937106966972351)
[2025-02-13 20:50:48,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:48,431][root][INFO] - Training Epoch: 2/2, step 3701/7134 completed (loss: 0.07190633565187454, acc: 0.9927007555961609)
[2025-02-13 20:50:48,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:48,771][root][INFO] - Training Epoch: 2/2, step 3702/7134 completed (loss: 0.07464119791984558, acc: 0.9939393997192383)
[2025-02-13 20:50:48,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:49,133][root][INFO] - Training Epoch: 2/2, step 3703/7134 completed (loss: 0.06210017576813698, acc: 0.9886363744735718)
[2025-02-13 20:50:49,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:49,487][root][INFO] - Training Epoch: 2/2, step 3704/7134 completed (loss: 0.031170297414064407, acc: 0.9900990128517151)
[2025-02-13 20:50:49,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:49,837][root][INFO] - Training Epoch: 2/2, step 3705/7134 completed (loss: 0.08031328022480011, acc: 0.9815950989723206)
[2025-02-13 20:50:49,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:50,191][root][INFO] - Training Epoch: 2/2, step 3706/7134 completed (loss: 0.18434615433216095, acc: 0.9405940771102905)
[2025-02-13 20:50:50,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:50,570][root][INFO] - Training Epoch: 2/2, step 3707/7134 completed (loss: 0.05321943014860153, acc: 0.9800000190734863)
[2025-02-13 20:50:50,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:50,938][root][INFO] - Training Epoch: 2/2, step 3708/7134 completed (loss: 0.1345449537038803, acc: 0.9692307710647583)
[2025-02-13 20:50:51,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:51,306][root][INFO] - Training Epoch: 2/2, step 3709/7134 completed (loss: 0.04342213645577431, acc: 0.9907407164573669)
[2025-02-13 20:50:51,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:51,674][root][INFO] - Training Epoch: 2/2, step 3710/7134 completed (loss: 0.10179832577705383, acc: 0.9722222089767456)
[2025-02-13 20:50:51,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:52,008][root][INFO] - Training Epoch: 2/2, step 3711/7134 completed (loss: 0.1646592915058136, acc: 0.9615384340286255)
[2025-02-13 20:50:52,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:52,386][root][INFO] - Training Epoch: 2/2, step 3712/7134 completed (loss: 0.12349124252796173, acc: 0.9619565010070801)
[2025-02-13 20:50:52,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:52,775][root][INFO] - Training Epoch: 2/2, step 3713/7134 completed (loss: 0.07757719606161118, acc: 0.971222996711731)
[2025-02-13 20:50:52,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:53,137][root][INFO] - Training Epoch: 2/2, step 3714/7134 completed (loss: 0.09712208062410355, acc: 0.9740932583808899)
[2025-02-13 20:50:53,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:53,493][root][INFO] - Training Epoch: 2/2, step 3715/7134 completed (loss: 0.08208226412534714, acc: 0.9680851101875305)
[2025-02-13 20:50:53,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:53,853][root][INFO] - Training Epoch: 2/2, step 3716/7134 completed (loss: 0.17680928111076355, acc: 0.9543147087097168)
[2025-02-13 20:50:53,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:54,250][root][INFO] - Training Epoch: 2/2, step 3717/7134 completed (loss: 0.055579256266355515, acc: 0.9878048896789551)
[2025-02-13 20:50:54,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:54,659][root][INFO] - Training Epoch: 2/2, step 3718/7134 completed (loss: 0.1461031436920166, acc: 0.984375)
[2025-02-13 20:50:54,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:55,029][root][INFO] - Training Epoch: 2/2, step 3719/7134 completed (loss: 0.10560574382543564, acc: 0.9766082167625427)
[2025-02-13 20:50:55,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:55,372][root][INFO] - Training Epoch: 2/2, step 3720/7134 completed (loss: 0.039245735853910446, acc: 0.9873417615890503)
[2025-02-13 20:50:55,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:55,748][root][INFO] - Training Epoch: 2/2, step 3721/7134 completed (loss: 0.04842785373330116, acc: 0.9884393215179443)
[2025-02-13 20:50:55,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:56,111][root][INFO] - Training Epoch: 2/2, step 3722/7134 completed (loss: 0.07445160299539566, acc: 0.984455943107605)
[2025-02-13 20:50:56,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:56,466][root][INFO] - Training Epoch: 2/2, step 3723/7134 completed (loss: 0.11065267771482468, acc: 0.9677419066429138)
[2025-02-13 20:50:56,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:56,827][root][INFO] - Training Epoch: 2/2, step 3724/7134 completed (loss: 0.049078319221735, acc: 0.9879518151283264)
[2025-02-13 20:50:56,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:57,195][root][INFO] - Training Epoch: 2/2, step 3725/7134 completed (loss: 0.18917037546634674, acc: 0.957317054271698)
[2025-02-13 20:50:57,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:57,590][root][INFO] - Training Epoch: 2/2, step 3726/7134 completed (loss: 0.08967513591051102, acc: 0.9648241400718689)
[2025-02-13 20:50:57,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:57,944][root][INFO] - Training Epoch: 2/2, step 3727/7134 completed (loss: 0.10383040457963943, acc: 0.9595375657081604)
[2025-02-13 20:50:58,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:58,307][root][INFO] - Training Epoch: 2/2, step 3728/7134 completed (loss: 0.021869545802474022, acc: 1.0)
[2025-02-13 20:50:58,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:58,672][root][INFO] - Training Epoch: 2/2, step 3729/7134 completed (loss: 0.07864634692668915, acc: 0.9764150977134705)
[2025-02-13 20:50:58,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:59,033][root][INFO] - Training Epoch: 2/2, step 3730/7134 completed (loss: 0.08449643105268478, acc: 0.9757575988769531)
[2025-02-13 20:50:59,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:59,395][root][INFO] - Training Epoch: 2/2, step 3731/7134 completed (loss: 0.029803341254591942, acc: 0.9942528605461121)
[2025-02-13 20:50:59,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:59,762][root][INFO] - Training Epoch: 2/2, step 3732/7134 completed (loss: 0.05025891587138176, acc: 0.9897435903549194)
[2025-02-13 20:50:59,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:00,109][root][INFO] - Training Epoch: 2/2, step 3733/7134 completed (loss: 0.09329777210950851, acc: 0.9875776171684265)
[2025-02-13 20:51:00,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:00,476][root][INFO] - Training Epoch: 2/2, step 3734/7134 completed (loss: 0.21329964697360992, acc: 0.9615384340286255)
[2025-02-13 20:51:00,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:00,837][root][INFO] - Training Epoch: 2/2, step 3735/7134 completed (loss: 0.08303741365671158, acc: 0.9675324559211731)
[2025-02-13 20:51:00,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:01,209][root][INFO] - Training Epoch: 2/2, step 3736/7134 completed (loss: 0.10806424915790558, acc: 0.9632353186607361)
[2025-02-13 20:51:01,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:01,570][root][INFO] - Training Epoch: 2/2, step 3737/7134 completed (loss: 0.1145426407456398, acc: 0.9636363387107849)
[2025-02-13 20:51:01,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:01,971][root][INFO] - Training Epoch: 2/2, step 3738/7134 completed (loss: 0.03806115314364433, acc: 0.9930555820465088)
[2025-02-13 20:51:02,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:02,329][root][INFO] - Training Epoch: 2/2, step 3739/7134 completed (loss: 0.038102224469184875, acc: 0.9893617033958435)
[2025-02-13 20:51:02,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:02,686][root][INFO] - Training Epoch: 2/2, step 3740/7134 completed (loss: 0.046333715319633484, acc: 0.993630588054657)
[2025-02-13 20:51:02,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:03,055][root][INFO] - Training Epoch: 2/2, step 3741/7134 completed (loss: 0.10638631880283356, acc: 0.9885057210922241)
[2025-02-13 20:51:03,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:03,407][root][INFO] - Training Epoch: 2/2, step 3742/7134 completed (loss: 0.1093566045165062, acc: 0.9727891087532043)
[2025-02-13 20:51:03,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:03,779][root][INFO] - Training Epoch: 2/2, step 3743/7134 completed (loss: 0.08952648192644119, acc: 0.9647058844566345)
[2025-02-13 20:51:03,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:04,161][root][INFO] - Training Epoch: 2/2, step 3744/7134 completed (loss: 0.12572428584098816, acc: 0.9593023061752319)
[2025-02-13 20:51:04,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:04,512][root][INFO] - Training Epoch: 2/2, step 3745/7134 completed (loss: 0.11315153539180756, acc: 0.9809523820877075)
[2025-02-13 20:51:04,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:04,909][root][INFO] - Training Epoch: 2/2, step 3746/7134 completed (loss: 0.12572871148586273, acc: 0.9717513918876648)
[2025-02-13 20:51:05,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:05,293][root][INFO] - Training Epoch: 2/2, step 3747/7134 completed (loss: 0.1089363545179367, acc: 0.9611111283302307)
[2025-02-13 20:51:05,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:05,635][root][INFO] - Training Epoch: 2/2, step 3748/7134 completed (loss: 0.08144979178905487, acc: 0.9733333587646484)
[2025-02-13 20:51:05,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:05,992][root][INFO] - Training Epoch: 2/2, step 3749/7134 completed (loss: 0.21994660794734955, acc: 0.9404761791229248)
[2025-02-13 20:51:06,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:06,354][root][INFO] - Training Epoch: 2/2, step 3750/7134 completed (loss: 0.028085989877581596, acc: 0.9938650131225586)
[2025-02-13 20:51:06,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:06,735][root][INFO] - Training Epoch: 2/2, step 3751/7134 completed (loss: 0.09735395014286041, acc: 0.9759036302566528)
[2025-02-13 20:51:06,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:07,105][root][INFO] - Training Epoch: 2/2, step 3752/7134 completed (loss: 0.1560101956129074, acc: 0.9496855139732361)
[2025-02-13 20:51:07,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:07,468][root][INFO] - Training Epoch: 2/2, step 3753/7134 completed (loss: 0.21082977950572968, acc: 0.9447852969169617)
[2025-02-13 20:51:07,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:07,826][root][INFO] - Training Epoch: 2/2, step 3754/7134 completed (loss: 0.025981200858950615, acc: 1.0)
[2025-02-13 20:51:07,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:08,182][root][INFO] - Training Epoch: 2/2, step 3755/7134 completed (loss: 0.10023503750562668, acc: 0.9752066135406494)
[2025-02-13 20:51:08,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:08,547][root][INFO] - Training Epoch: 2/2, step 3756/7134 completed (loss: 0.055933628231287, acc: 0.9884393215179443)
[2025-02-13 20:51:08,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:08,966][root][INFO] - Training Epoch: 2/2, step 3757/7134 completed (loss: 0.17769020795822144, acc: 0.9675324559211731)
[2025-02-13 20:51:09,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:09,338][root][INFO] - Training Epoch: 2/2, step 3758/7134 completed (loss: 0.1258794516324997, acc: 0.9685039520263672)
[2025-02-13 20:51:09,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:09,697][root][INFO] - Training Epoch: 2/2, step 3759/7134 completed (loss: 0.10402017831802368, acc: 0.9720279574394226)
[2025-02-13 20:51:09,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:10,090][root][INFO] - Training Epoch: 2/2, step 3760/7134 completed (loss: 0.11168606579303741, acc: 0.9756097793579102)
[2025-02-13 20:51:10,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:10,450][root][INFO] - Training Epoch: 2/2, step 3761/7134 completed (loss: 0.03787284344434738, acc: 0.9924242496490479)
[2025-02-13 20:51:10,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:10,818][root][INFO] - Training Epoch: 2/2, step 3762/7134 completed (loss: 0.03810109198093414, acc: 1.0)
[2025-02-13 20:51:10,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:11,174][root][INFO] - Training Epoch: 2/2, step 3763/7134 completed (loss: 0.03633103892207146, acc: 1.0)
[2025-02-13 20:51:11,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:11,562][root][INFO] - Training Epoch: 2/2, step 3764/7134 completed (loss: 0.06333820521831512, acc: 0.9820359349250793)
[2025-02-13 20:51:11,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:11,954][root][INFO] - Training Epoch: 2/2, step 3765/7134 completed (loss: 0.05975041165947914, acc: 0.9886363744735718)
[2025-02-13 20:51:12,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:12,326][root][INFO] - Training Epoch: 2/2, step 3766/7134 completed (loss: 0.1377228945493698, acc: 0.9731543660163879)
[2025-02-13 20:51:12,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:12,693][root][INFO] - Training Epoch: 2/2, step 3767/7134 completed (loss: 0.05905209481716156, acc: 0.9832402467727661)
[2025-02-13 20:51:12,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:13,054][root][INFO] - Training Epoch: 2/2, step 3768/7134 completed (loss: 0.09014757722616196, acc: 0.9711538553237915)
[2025-02-13 20:51:13,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:13,424][root][INFO] - Training Epoch: 2/2, step 3769/7134 completed (loss: 0.11266613006591797, acc: 0.970588207244873)
[2025-02-13 20:51:13,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:13,767][root][INFO] - Training Epoch: 2/2, step 3770/7134 completed (loss: 0.05288543179631233, acc: 0.9865771532058716)
[2025-02-13 20:51:13,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:14,133][root][INFO] - Training Epoch: 2/2, step 3771/7134 completed (loss: 0.026714617386460304, acc: 0.9934210777282715)
[2025-02-13 20:51:14,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:14,482][root][INFO] - Training Epoch: 2/2, step 3772/7134 completed (loss: 0.05684111639857292, acc: 0.9857142567634583)
[2025-02-13 20:51:14,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:14,849][root][INFO] - Training Epoch: 2/2, step 3773/7134 completed (loss: 0.07023200392723083, acc: 0.9893617033958435)
[2025-02-13 20:51:14,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:15,229][root][INFO] - Training Epoch: 2/2, step 3774/7134 completed (loss: 0.05309893935918808, acc: 0.9890109896659851)
[2025-02-13 20:51:15,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:15,615][root][INFO] - Training Epoch: 2/2, step 3775/7134 completed (loss: 0.08532886952161789, acc: 0.9875776171684265)
[2025-02-13 20:51:15,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:15,980][root][INFO] - Training Epoch: 2/2, step 3776/7134 completed (loss: 0.07795780152082443, acc: 0.9807692170143127)
[2025-02-13 20:51:16,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:16,367][root][INFO] - Training Epoch: 2/2, step 3777/7134 completed (loss: 0.027269627898931503, acc: 1.0)
[2025-02-13 20:51:16,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:16,741][root][INFO] - Training Epoch: 2/2, step 3778/7134 completed (loss: 0.12400931119918823, acc: 0.9709302186965942)
[2025-02-13 20:51:16,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:17,160][root][INFO] - Training Epoch: 2/2, step 3779/7134 completed (loss: 0.08225099742412567, acc: 0.9937499761581421)
[2025-02-13 20:51:17,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:17,527][root][INFO] - Training Epoch: 2/2, step 3780/7134 completed (loss: 0.03337141498923302, acc: 0.9939393997192383)
[2025-02-13 20:51:17,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:17,875][root][INFO] - Training Epoch: 2/2, step 3781/7134 completed (loss: 0.0854928269982338, acc: 0.9751552939414978)
[2025-02-13 20:51:18,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:18,191][root][INFO] - Training Epoch: 2/2, step 3782/7134 completed (loss: 0.06022912636399269, acc: 0.9874213933944702)
[2025-02-13 20:51:18,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:18,567][root][INFO] - Training Epoch: 2/2, step 3783/7134 completed (loss: 0.11573127657175064, acc: 0.976331353187561)
[2025-02-13 20:51:18,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:18,937][root][INFO] - Training Epoch: 2/2, step 3784/7134 completed (loss: 0.050297584384679794, acc: 0.9932885766029358)
[2025-02-13 20:51:19,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:19,303][root][INFO] - Training Epoch: 2/2, step 3785/7134 completed (loss: 0.058760177344083786, acc: 0.9929078221321106)
[2025-02-13 20:51:19,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:19,651][root][INFO] - Training Epoch: 2/2, step 3786/7134 completed (loss: 0.02761492319405079, acc: 0.9940476417541504)
[2025-02-13 20:51:19,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:19,982][root][INFO] - Training Epoch: 2/2, step 3787/7134 completed (loss: 0.027356980368494987, acc: 1.0)
[2025-02-13 20:51:20,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:20,357][root][INFO] - Training Epoch: 2/2, step 3788/7134 completed (loss: 0.14949007332324982, acc: 0.9693251252174377)
[2025-02-13 20:51:20,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:20,715][root][INFO] - Training Epoch: 2/2, step 3789/7134 completed (loss: 0.10614220052957535, acc: 0.970588207244873)
[2025-02-13 20:51:20,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:21,094][root][INFO] - Training Epoch: 2/2, step 3790/7134 completed (loss: 0.15152792632579803, acc: 0.970588207244873)
[2025-02-13 20:51:21,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:21,446][root][INFO] - Training Epoch: 2/2, step 3791/7134 completed (loss: 0.0642927810549736, acc: 0.9826589822769165)
[2025-02-13 20:51:21,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:21,830][root][INFO] - Training Epoch: 2/2, step 3792/7134 completed (loss: 0.04793410003185272, acc: 0.9924812316894531)
[2025-02-13 20:51:21,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:22,184][root][INFO] - Training Epoch: 2/2, step 3793/7134 completed (loss: 0.062307633459568024, acc: 0.9885057210922241)
[2025-02-13 20:51:22,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:22,546][root][INFO] - Training Epoch: 2/2, step 3794/7134 completed (loss: 0.16530445218086243, acc: 0.9666666388511658)
[2025-02-13 20:51:22,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:22,949][root][INFO] - Training Epoch: 2/2, step 3795/7134 completed (loss: 0.07551752775907516, acc: 0.9833333492279053)
[2025-02-13 20:51:23,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:23,345][root][INFO] - Training Epoch: 2/2, step 3796/7134 completed (loss: 0.06703238189220428, acc: 0.9824561476707458)
[2025-02-13 20:51:23,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:23,730][root][INFO] - Training Epoch: 2/2, step 3797/7134 completed (loss: 0.05687807872891426, acc: 0.9878787994384766)
[2025-02-13 20:51:23,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:24,125][root][INFO] - Training Epoch: 2/2, step 3798/7134 completed (loss: 0.2549367845058441, acc: 0.9308176040649414)
[2025-02-13 20:51:24,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:24,490][root][INFO] - Training Epoch: 2/2, step 3799/7134 completed (loss: 0.23098881542682648, acc: 0.9491525292396545)
[2025-02-13 20:51:24,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:24,855][root][INFO] - Training Epoch: 2/2, step 3800/7134 completed (loss: 0.21773844957351685, acc: 0.9586206674575806)
[2025-02-13 20:51:25,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:25,230][root][INFO] - Training Epoch: 2/2, step 3801/7134 completed (loss: 0.13485859334468842, acc: 0.9723756909370422)
[2025-02-13 20:51:25,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:25,597][root][INFO] - Training Epoch: 2/2, step 3802/7134 completed (loss: 0.1494356244802475, acc: 0.953125)
[2025-02-13 20:51:25,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:25,949][root][INFO] - Training Epoch: 2/2, step 3803/7134 completed (loss: 0.09479685127735138, acc: 0.9745222926139832)
[2025-02-13 20:51:26,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:26,319][root][INFO] - Training Epoch: 2/2, step 3804/7134 completed (loss: 0.0765162780880928, acc: 0.9929577708244324)
[2025-02-13 20:51:26,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:26,703][root][INFO] - Training Epoch: 2/2, step 3805/7134 completed (loss: 0.101378433406353, acc: 0.977142870426178)
[2025-02-13 20:51:26,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:27,082][root][INFO] - Training Epoch: 2/2, step 3806/7134 completed (loss: 0.03337731957435608, acc: 1.0)
[2025-02-13 20:51:27,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:27,435][root][INFO] - Training Epoch: 2/2, step 3807/7134 completed (loss: 0.0776836946606636, acc: 0.9726027250289917)
[2025-02-13 20:51:27,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:27,883][root][INFO] - Training Epoch: 2/2, step 3808/7134 completed (loss: 0.10521131008863449, acc: 0.97826087474823)
[2025-02-13 20:51:28,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:28,278][root][INFO] - Training Epoch: 2/2, step 3809/7134 completed (loss: 0.09248433262109756, acc: 0.9883720874786377)
[2025-02-13 20:51:28,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:28,631][root][INFO] - Training Epoch: 2/2, step 3810/7134 completed (loss: 0.0670405700802803, acc: 0.9805194735527039)
[2025-02-13 20:51:28,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:28,999][root][INFO] - Training Epoch: 2/2, step 3811/7134 completed (loss: 0.164473295211792, acc: 0.9640718698501587)
[2025-02-13 20:51:29,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:29,380][root][INFO] - Training Epoch: 2/2, step 3812/7134 completed (loss: 0.3175436556339264, acc: 0.9319728016853333)
[2025-02-13 20:51:29,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:29,735][root][INFO] - Training Epoch: 2/2, step 3813/7134 completed (loss: 0.29843342304229736, acc: 0.9513513445854187)
[2025-02-13 20:51:29,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:30,102][root][INFO] - Training Epoch: 2/2, step 3814/7134 completed (loss: 0.06337663531303406, acc: 0.977142870426178)
[2025-02-13 20:51:30,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:30,454][root][INFO] - Training Epoch: 2/2, step 3815/7134 completed (loss: 0.10148043930530548, acc: 0.9671052694320679)
[2025-02-13 20:51:30,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:30,843][root][INFO] - Training Epoch: 2/2, step 3816/7134 completed (loss: 0.05790937691926956, acc: 0.9850746393203735)
[2025-02-13 20:51:30,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:31,222][root][INFO] - Training Epoch: 2/2, step 3817/7134 completed (loss: 0.12284789234399796, acc: 0.9775280952453613)
[2025-02-13 20:51:31,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:31,592][root][INFO] - Training Epoch: 2/2, step 3818/7134 completed (loss: 0.17939110100269318, acc: 0.9561403393745422)
[2025-02-13 20:51:31,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:31,937][root][INFO] - Training Epoch: 2/2, step 3819/7134 completed (loss: 0.013567824847996235, acc: 1.0)
[2025-02-13 20:51:32,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:32,299][root][INFO] - Training Epoch: 2/2, step 3820/7134 completed (loss: 0.0733659565448761, acc: 0.9814814925193787)
[2025-02-13 20:51:32,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:32,649][root][INFO] - Training Epoch: 2/2, step 3821/7134 completed (loss: 0.03474194183945656, acc: 0.9931507110595703)
[2025-02-13 20:51:32,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:33,003][root][INFO] - Training Epoch: 2/2, step 3822/7134 completed (loss: 0.037293799221515656, acc: 0.9905660152435303)
[2025-02-13 20:51:33,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:33,392][root][INFO] - Training Epoch: 2/2, step 3823/7134 completed (loss: 0.04392741993069649, acc: 0.9865771532058716)
[2025-02-13 20:51:33,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:33,808][root][INFO] - Training Epoch: 2/2, step 3824/7134 completed (loss: 0.09406082332134247, acc: 0.984375)
[2025-02-13 20:51:33,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:34,182][root][INFO] - Training Epoch: 2/2, step 3825/7134 completed (loss: 0.2849431037902832, acc: 0.9722222089767456)
[2025-02-13 20:51:34,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:34,548][root][INFO] - Training Epoch: 2/2, step 3826/7134 completed (loss: 0.05778491869568825, acc: 0.98591548204422)
[2025-02-13 20:51:34,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:34,923][root][INFO] - Training Epoch: 2/2, step 3827/7134 completed (loss: 0.04144170507788658, acc: 0.987261176109314)
[2025-02-13 20:51:35,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:35,334][root][INFO] - Training Epoch: 2/2, step 3828/7134 completed (loss: 0.026552189141511917, acc: 1.0)
[2025-02-13 20:51:35,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:35,714][root][INFO] - Training Epoch: 2/2, step 3829/7134 completed (loss: 0.048770174384117126, acc: 0.9921875)
[2025-02-13 20:51:35,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:36,078][root][INFO] - Training Epoch: 2/2, step 3830/7134 completed (loss: 0.13222812116146088, acc: 0.9851852059364319)
[2025-02-13 20:51:36,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:36,447][root][INFO] - Training Epoch: 2/2, step 3831/7134 completed (loss: 0.12432263791561127, acc: 0.9671052694320679)
[2025-02-13 20:51:36,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:36,851][root][INFO] - Training Epoch: 2/2, step 3832/7134 completed (loss: 0.04267324134707451, acc: 0.9910714030265808)
[2025-02-13 20:51:37,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:37,246][root][INFO] - Training Epoch: 2/2, step 3833/7134 completed (loss: 0.061799727380275726, acc: 0.9836065769195557)
[2025-02-13 20:51:37,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:37,616][root][INFO] - Training Epoch: 2/2, step 3834/7134 completed (loss: 0.008346052840352058, acc: 1.0)
[2025-02-13 20:51:37,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:37,959][root][INFO] - Training Epoch: 2/2, step 3835/7134 completed (loss: 0.036161042749881744, acc: 0.9947916865348816)
[2025-02-13 20:51:38,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:38,302][root][INFO] - Training Epoch: 2/2, step 3836/7134 completed (loss: 0.06403274089097977, acc: 0.9753086566925049)
[2025-02-13 20:51:38,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:38,684][root][INFO] - Training Epoch: 2/2, step 3837/7134 completed (loss: 0.07666140049695969, acc: 0.993630588054657)
[2025-02-13 20:51:38,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:39,044][root][INFO] - Training Epoch: 2/2, step 3838/7134 completed (loss: 0.024607352912425995, acc: 1.0)
[2025-02-13 20:51:39,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:39,435][root][INFO] - Training Epoch: 2/2, step 3839/7134 completed (loss: 0.029511837288737297, acc: 0.9937888383865356)
[2025-02-13 20:51:39,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:39,822][root][INFO] - Training Epoch: 2/2, step 3840/7134 completed (loss: 0.022518018260598183, acc: 0.9939393997192383)
[2025-02-13 20:51:39,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:40,209][root][INFO] - Training Epoch: 2/2, step 3841/7134 completed (loss: 0.12742750346660614, acc: 0.9698492288589478)
[2025-02-13 20:51:40,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:40,537][root][INFO] - Training Epoch: 2/2, step 3842/7134 completed (loss: 0.01469446811825037, acc: 1.0)
[2025-02-13 20:51:40,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:40,931][root][INFO] - Training Epoch: 2/2, step 3843/7134 completed (loss: 0.11467205733060837, acc: 0.977477490901947)
[2025-02-13 20:51:41,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:41,295][root][INFO] - Training Epoch: 2/2, step 3844/7134 completed (loss: 0.12013223022222519, acc: 0.9738219976425171)
[2025-02-13 20:51:41,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:41,631][root][INFO] - Training Epoch: 2/2, step 3845/7134 completed (loss: 0.08795252442359924, acc: 0.9857142567634583)
[2025-02-13 20:51:41,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:41,965][root][INFO] - Training Epoch: 2/2, step 3846/7134 completed (loss: 0.022515200078487396, acc: 1.0)
[2025-02-13 20:51:42,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:42,318][root][INFO] - Training Epoch: 2/2, step 3847/7134 completed (loss: 0.06078428030014038, acc: 0.978723406791687)
[2025-02-13 20:51:42,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:42,698][root][INFO] - Training Epoch: 2/2, step 3848/7134 completed (loss: 0.07276502251625061, acc: 0.9886363744735718)
[2025-02-13 20:51:42,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:43,050][root][INFO] - Training Epoch: 2/2, step 3849/7134 completed (loss: 0.09484995901584625, acc: 0.9629629850387573)
[2025-02-13 20:51:43,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:43,397][root][INFO] - Training Epoch: 2/2, step 3850/7134 completed (loss: 0.09013353288173676, acc: 0.9836065769195557)
[2025-02-13 20:51:43,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:43,761][root][INFO] - Training Epoch: 2/2, step 3851/7134 completed (loss: 0.11462166160345078, acc: 0.9464285969734192)
[2025-02-13 20:51:43,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:44,128][root][INFO] - Training Epoch: 2/2, step 3852/7134 completed (loss: 0.02321162447333336, acc: 0.9928057789802551)
[2025-02-13 20:51:44,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:44,488][root][INFO] - Training Epoch: 2/2, step 3853/7134 completed (loss: 0.12287578731775284, acc: 0.9664804339408875)
[2025-02-13 20:51:44,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:44,834][root][INFO] - Training Epoch: 2/2, step 3854/7134 completed (loss: 0.079501673579216, acc: 0.9745222926139832)
[2025-02-13 20:51:44,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:45,203][root][INFO] - Training Epoch: 2/2, step 3855/7134 completed (loss: 0.0733608826994896, acc: 0.9880239367485046)
[2025-02-13 20:51:45,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:45,554][root][INFO] - Training Epoch: 2/2, step 3856/7134 completed (loss: 0.08595210313796997, acc: 0.9745222926139832)
[2025-02-13 20:51:45,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:45,933][root][INFO] - Training Epoch: 2/2, step 3857/7134 completed (loss: 0.08033037930727005, acc: 0.9729729890823364)
[2025-02-13 20:51:46,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:46,306][root][INFO] - Training Epoch: 2/2, step 3858/7134 completed (loss: 0.2073960304260254, acc: 0.949999988079071)
[2025-02-13 20:51:46,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:46,709][root][INFO] - Training Epoch: 2/2, step 3859/7134 completed (loss: 0.09932482242584229, acc: 0.9830508232116699)
[2025-02-13 20:51:46,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:47,085][root][INFO] - Training Epoch: 2/2, step 3860/7134 completed (loss: 0.035967420786619186, acc: 0.9947643876075745)
[2025-02-13 20:51:47,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:47,439][root][INFO] - Training Epoch: 2/2, step 3861/7134 completed (loss: 0.06934574991464615, acc: 0.9934640526771545)
[2025-02-13 20:51:47,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:47,839][root][INFO] - Training Epoch: 2/2, step 3862/7134 completed (loss: 0.054663822054862976, acc: 0.9923076629638672)
[2025-02-13 20:51:47,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:48,206][root][INFO] - Training Epoch: 2/2, step 3863/7134 completed (loss: 0.029758965596556664, acc: 0.9941860437393188)
[2025-02-13 20:51:48,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:48,574][root][INFO] - Training Epoch: 2/2, step 3864/7134 completed (loss: 0.06470394134521484, acc: 0.9805194735527039)
[2025-02-13 20:51:48,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:48,940][root][INFO] - Training Epoch: 2/2, step 3865/7134 completed (loss: 0.0961499884724617, acc: 0.9732142686843872)
[2025-02-13 20:51:49,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:49,298][root][INFO] - Training Epoch: 2/2, step 3866/7134 completed (loss: 0.13159050047397614, acc: 0.9700000286102295)
[2025-02-13 20:51:49,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:49,652][root][INFO] - Training Epoch: 2/2, step 3867/7134 completed (loss: 0.062151823192834854, acc: 0.9818181991577148)
[2025-02-13 20:51:49,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:50,002][root][INFO] - Training Epoch: 2/2, step 3868/7134 completed (loss: 0.04161730781197548, acc: 0.9926470518112183)
[2025-02-13 20:51:50,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:50,357][root][INFO] - Training Epoch: 2/2, step 3869/7134 completed (loss: 0.07160443067550659, acc: 0.9790209531784058)
[2025-02-13 20:51:50,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:50,711][root][INFO] - Training Epoch: 2/2, step 3870/7134 completed (loss: 0.15349608659744263, acc: 0.9738562107086182)
[2025-02-13 20:51:50,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:51,077][root][INFO] - Training Epoch: 2/2, step 3871/7134 completed (loss: 0.10201379656791687, acc: 0.977011501789093)
[2025-02-13 20:51:51,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:51,485][root][INFO] - Training Epoch: 2/2, step 3872/7134 completed (loss: 0.06357499212026596, acc: 0.9848484992980957)
[2025-02-13 20:51:51,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:51,843][root][INFO] - Training Epoch: 2/2, step 3873/7134 completed (loss: 0.06248648092150688, acc: 0.9818181991577148)
[2025-02-13 20:51:51,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:52,196][root][INFO] - Training Epoch: 2/2, step 3874/7134 completed (loss: 0.08429640531539917, acc: 0.988304078578949)
[2025-02-13 20:51:52,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:52,560][root][INFO] - Training Epoch: 2/2, step 3875/7134 completed (loss: 0.08763661235570908, acc: 0.9810126423835754)
[2025-02-13 20:51:52,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:52,903][root][INFO] - Training Epoch: 2/2, step 3876/7134 completed (loss: 0.04136577993631363, acc: 0.9919999837875366)
[2025-02-13 20:51:53,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:53,258][root][INFO] - Training Epoch: 2/2, step 3877/7134 completed (loss: 0.0972541868686676, acc: 0.9767441749572754)
[2025-02-13 20:51:53,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:53,609][root][INFO] - Training Epoch: 2/2, step 3878/7134 completed (loss: 0.4064379334449768, acc: 0.9244186282157898)
[2025-02-13 20:51:53,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:53,969][root][INFO] - Training Epoch: 2/2, step 3879/7134 completed (loss: 0.10606559365987778, acc: 0.9776119589805603)
[2025-02-13 20:51:54,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:54,330][root][INFO] - Training Epoch: 2/2, step 3880/7134 completed (loss: 0.09596948325634003, acc: 0.9551281929016113)
[2025-02-13 20:51:54,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:54,687][root][INFO] - Training Epoch: 2/2, step 3881/7134 completed (loss: 0.27856332063674927, acc: 0.9304812550544739)
[2025-02-13 20:51:54,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:55,052][root][INFO] - Training Epoch: 2/2, step 3882/7134 completed (loss: 0.13116848468780518, acc: 0.9587628841400146)
[2025-02-13 20:51:55,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:55,413][root][INFO] - Training Epoch: 2/2, step 3883/7134 completed (loss: 0.06656378507614136, acc: 0.9934640526771545)
[2025-02-13 20:51:55,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:55,771][root][INFO] - Training Epoch: 2/2, step 3884/7134 completed (loss: 0.05833577364683151, acc: 0.9885714054107666)
[2025-02-13 20:51:55,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:56,117][root][INFO] - Training Epoch: 2/2, step 3885/7134 completed (loss: 0.025072496384382248, acc: 1.0)
[2025-02-13 20:51:56,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:56,480][root][INFO] - Training Epoch: 2/2, step 3886/7134 completed (loss: 0.11151938140392303, acc: 0.9682539701461792)
[2025-02-13 20:51:56,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:56,829][root][INFO] - Training Epoch: 2/2, step 3887/7134 completed (loss: 0.12040114402770996, acc: 0.9811320900917053)
[2025-02-13 20:51:56,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:57,188][root][INFO] - Training Epoch: 2/2, step 3888/7134 completed (loss: 0.050064560025930405, acc: 0.9767441749572754)
[2025-02-13 20:51:57,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:57,568][root][INFO] - Training Epoch: 2/2, step 3889/7134 completed (loss: 0.04378015547990799, acc: 0.9774011373519897)
[2025-02-13 20:51:57,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:57,935][root][INFO] - Training Epoch: 2/2, step 3890/7134 completed (loss: 0.038266945630311966, acc: 0.9896907210350037)
[2025-02-13 20:51:58,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:58,313][root][INFO] - Training Epoch: 2/2, step 3891/7134 completed (loss: 0.05119907483458519, acc: 0.9884393215179443)
[2025-02-13 20:51:58,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:58,711][root][INFO] - Training Epoch: 2/2, step 3892/7134 completed (loss: 0.1027117446064949, acc: 0.9800000190734863)
[2025-02-13 20:51:58,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:59,075][root][INFO] - Training Epoch: 2/2, step 3893/7134 completed (loss: 0.15372370183467865, acc: 0.9587628841400146)
[2025-02-13 20:51:59,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:59,422][root][INFO] - Training Epoch: 2/2, step 3894/7134 completed (loss: 0.06740424782037735, acc: 0.9873417615890503)
[2025-02-13 20:51:59,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:59,775][root][INFO] - Training Epoch: 2/2, step 3895/7134 completed (loss: 0.09906433522701263, acc: 0.9779005646705627)
[2025-02-13 20:51:59,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:00,157][root][INFO] - Training Epoch: 2/2, step 3896/7134 completed (loss: 0.06950775533914566, acc: 0.9763033390045166)
[2025-02-13 20:52:00,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:00,564][root][INFO] - Training Epoch: 2/2, step 3897/7134 completed (loss: 0.2280057668685913, acc: 0.9651162624359131)
[2025-02-13 20:52:00,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:00,931][root][INFO] - Training Epoch: 2/2, step 3898/7134 completed (loss: 0.07805059105157852, acc: 0.977011501789093)
[2025-02-13 20:52:01,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:01,294][root][INFO] - Training Epoch: 2/2, step 3899/7134 completed (loss: 0.06972629576921463, acc: 0.9884393215179443)
[2025-02-13 20:52:01,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:01,693][root][INFO] - Training Epoch: 2/2, step 3900/7134 completed (loss: 0.07890595495700836, acc: 0.9793814420700073)
[2025-02-13 20:52:01,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:02,050][root][INFO] - Training Epoch: 2/2, step 3901/7134 completed (loss: 0.2105884999036789, acc: 0.9578313231468201)
[2025-02-13 20:52:02,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:02,446][root][INFO] - Training Epoch: 2/2, step 3902/7134 completed (loss: 0.1397656351327896, acc: 0.9739583134651184)
[2025-02-13 20:52:02,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:02,826][root][INFO] - Training Epoch: 2/2, step 3903/7134 completed (loss: 0.26558151841163635, acc: 0.9430052042007446)
[2025-02-13 20:52:02,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:03,176][root][INFO] - Training Epoch: 2/2, step 3904/7134 completed (loss: 0.13702571392059326, acc: 0.9649122953414917)
[2025-02-13 20:52:03,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:03,549][root][INFO] - Training Epoch: 2/2, step 3905/7134 completed (loss: 0.04248631373047829, acc: 0.9833333492279053)
[2025-02-13 20:52:03,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:03,914][root][INFO] - Training Epoch: 2/2, step 3906/7134 completed (loss: 0.04795832559466362, acc: 0.9887640476226807)
[2025-02-13 20:52:04,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:04,293][root][INFO] - Training Epoch: 2/2, step 3907/7134 completed (loss: 0.08760049939155579, acc: 0.9742268323898315)
[2025-02-13 20:52:04,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:04,688][root][INFO] - Training Epoch: 2/2, step 3908/7134 completed (loss: 0.1835842728614807, acc: 0.9595375657081604)
[2025-02-13 20:52:04,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:05,058][root][INFO] - Training Epoch: 2/2, step 3909/7134 completed (loss: 0.04373191297054291, acc: 1.0)
[2025-02-13 20:52:05,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:05,426][root][INFO] - Training Epoch: 2/2, step 3910/7134 completed (loss: 0.03465054929256439, acc: 0.9943181872367859)
[2025-02-13 20:52:05,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:05,795][root][INFO] - Training Epoch: 2/2, step 3911/7134 completed (loss: 0.08045919239521027, acc: 0.9685534834861755)
[2025-02-13 20:52:05,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:06,150][root][INFO] - Training Epoch: 2/2, step 3912/7134 completed (loss: 0.010866700671613216, acc: 1.0)
[2025-02-13 20:52:06,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:06,535][root][INFO] - Training Epoch: 2/2, step 3913/7134 completed (loss: 0.02984859235584736, acc: 0.9898989796638489)
[2025-02-13 20:52:06,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:06,921][root][INFO] - Training Epoch: 2/2, step 3914/7134 completed (loss: 0.07413327693939209, acc: 0.9748427867889404)
[2025-02-13 20:52:07,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:07,286][root][INFO] - Training Epoch: 2/2, step 3915/7134 completed (loss: 0.02539565972983837, acc: 0.9942528605461121)
[2025-02-13 20:52:07,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:07,653][root][INFO] - Training Epoch: 2/2, step 3916/7134 completed (loss: 0.05803889036178589, acc: 0.9897959232330322)
[2025-02-13 20:52:07,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:08,016][root][INFO] - Training Epoch: 2/2, step 3917/7134 completed (loss: 0.02271842584013939, acc: 0.9953488111495972)
[2025-02-13 20:52:08,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:08,386][root][INFO] - Training Epoch: 2/2, step 3918/7134 completed (loss: 0.06921657174825668, acc: 0.9666666388511658)
[2025-02-13 20:52:08,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:08,747][root][INFO] - Training Epoch: 2/2, step 3919/7134 completed (loss: 0.14240965247154236, acc: 0.9704142212867737)
[2025-02-13 20:52:08,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:09,106][root][INFO] - Training Epoch: 2/2, step 3920/7134 completed (loss: 0.18209759891033173, acc: 0.9520958065986633)
[2025-02-13 20:52:09,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:09,493][root][INFO] - Training Epoch: 2/2, step 3921/7134 completed (loss: 0.14649710059165955, acc: 0.9647887349128723)
[2025-02-13 20:52:09,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:09,816][root][INFO] - Training Epoch: 2/2, step 3922/7134 completed (loss: 0.18868164718151093, acc: 0.9610389471054077)
[2025-02-13 20:52:09,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:10,177][root][INFO] - Training Epoch: 2/2, step 3923/7134 completed (loss: 0.22090616822242737, acc: 0.9642857313156128)
[2025-02-13 20:52:10,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:10,534][root][INFO] - Training Epoch: 2/2, step 3924/7134 completed (loss: 0.11997084319591522, acc: 0.9484536051750183)
[2025-02-13 20:52:10,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:10,937][root][INFO] - Training Epoch: 2/2, step 3925/7134 completed (loss: 0.10772999376058578, acc: 0.9779005646705627)
[2025-02-13 20:52:11,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:11,303][root][INFO] - Training Epoch: 2/2, step 3926/7134 completed (loss: 0.07348920404911041, acc: 0.9810126423835754)
[2025-02-13 20:52:11,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:11,671][root][INFO] - Training Epoch: 2/2, step 3927/7134 completed (loss: 0.05272149667143822, acc: 0.9939393997192383)
[2025-02-13 20:52:11,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:12,044][root][INFO] - Training Epoch: 2/2, step 3928/7134 completed (loss: 0.14921776950359344, acc: 0.976047933101654)
[2025-02-13 20:52:12,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:12,406][root][INFO] - Training Epoch: 2/2, step 3929/7134 completed (loss: 0.09651046246290207, acc: 0.9805194735527039)
[2025-02-13 20:52:12,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:12,805][root][INFO] - Training Epoch: 2/2, step 3930/7134 completed (loss: 0.12913909554481506, acc: 0.9576271176338196)
[2025-02-13 20:52:12,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:13,179][root][INFO] - Training Epoch: 2/2, step 3931/7134 completed (loss: 0.05423698574304581, acc: 0.9810126423835754)
[2025-02-13 20:52:13,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:13,584][root][INFO] - Training Epoch: 2/2, step 3932/7134 completed (loss: 0.043450821191072464, acc: 1.0)
[2025-02-13 20:52:13,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:13,968][root][INFO] - Training Epoch: 2/2, step 3933/7134 completed (loss: 0.00756543455645442, acc: 1.0)
[2025-02-13 20:52:14,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:14,344][root][INFO] - Training Epoch: 2/2, step 3934/7134 completed (loss: 0.020560944452881813, acc: 1.0)
[2025-02-13 20:52:14,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:14,732][root][INFO] - Training Epoch: 2/2, step 3935/7134 completed (loss: 0.01582483947277069, acc: 1.0)
[2025-02-13 20:52:14,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:15,101][root][INFO] - Training Epoch: 2/2, step 3936/7134 completed (loss: 0.06355993449687958, acc: 0.9919354915618896)
[2025-02-13 20:52:15,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:15,473][root][INFO] - Training Epoch: 2/2, step 3937/7134 completed (loss: 0.15996518731117249, acc: 0.9661017060279846)
[2025-02-13 20:52:15,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:15,796][root][INFO] - Training Epoch: 2/2, step 3938/7134 completed (loss: 0.0771956741809845, acc: 0.9819819927215576)
[2025-02-13 20:52:15,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:16,181][root][INFO] - Training Epoch: 2/2, step 3939/7134 completed (loss: 0.13149836659431458, acc: 0.9629629850387573)
[2025-02-13 20:52:16,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:16,538][root][INFO] - Training Epoch: 2/2, step 3940/7134 completed (loss: 0.041118837893009186, acc: 1.0)
[2025-02-13 20:52:16,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:16,925][root][INFO] - Training Epoch: 2/2, step 3941/7134 completed (loss: 0.08517041802406311, acc: 0.9878048896789551)
[2025-02-13 20:52:17,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:17,310][root][INFO] - Training Epoch: 2/2, step 3942/7134 completed (loss: 0.06545612961053848, acc: 0.9919999837875366)
[2025-02-13 20:52:17,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:17,678][root][INFO] - Training Epoch: 2/2, step 3943/7134 completed (loss: 0.02658657170832157, acc: 1.0)
[2025-02-13 20:52:17,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:18,076][root][INFO] - Training Epoch: 2/2, step 3944/7134 completed (loss: 0.05534091591835022, acc: 0.9850746393203735)
[2025-02-13 20:52:18,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:18,432][root][INFO] - Training Epoch: 2/2, step 3945/7134 completed (loss: 0.023153433576226234, acc: 0.9934210777282715)
[2025-02-13 20:52:18,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:18,777][root][INFO] - Training Epoch: 2/2, step 3946/7134 completed (loss: 0.03608027845621109, acc: 0.9821428656578064)
[2025-02-13 20:52:18,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:19,111][root][INFO] - Training Epoch: 2/2, step 3947/7134 completed (loss: 0.007411051541566849, acc: 1.0)
[2025-02-13 20:52:19,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:19,507][root][INFO] - Training Epoch: 2/2, step 3948/7134 completed (loss: 0.012356895953416824, acc: 1.0)
[2025-02-13 20:52:19,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:19,868][root][INFO] - Training Epoch: 2/2, step 3949/7134 completed (loss: 0.07063666731119156, acc: 0.9765625)
[2025-02-13 20:52:19,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:20,195][root][INFO] - Training Epoch: 2/2, step 3950/7134 completed (loss: 0.02787269838154316, acc: 1.0)
[2025-02-13 20:52:20,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:20,549][root][INFO] - Training Epoch: 2/2, step 3951/7134 completed (loss: 0.028538675978779793, acc: 0.9896907210350037)
[2025-02-13 20:52:20,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:20,900][root][INFO] - Training Epoch: 2/2, step 3952/7134 completed (loss: 0.014255461283028126, acc: 1.0)
[2025-02-13 20:52:21,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:21,257][root][INFO] - Training Epoch: 2/2, step 3953/7134 completed (loss: 0.022223584353923798, acc: 0.9927536249160767)
[2025-02-13 20:52:21,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:21,600][root][INFO] - Training Epoch: 2/2, step 3954/7134 completed (loss: 0.0223956648260355, acc: 1.0)
[2025-02-13 20:52:21,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:21,972][root][INFO] - Training Epoch: 2/2, step 3955/7134 completed (loss: 0.028139658272266388, acc: 0.9921259880065918)
[2025-02-13 20:52:22,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:22,332][root][INFO] - Training Epoch: 2/2, step 3956/7134 completed (loss: 0.051781561225652695, acc: 0.9934640526771545)
[2025-02-13 20:52:22,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:22,683][root][INFO] - Training Epoch: 2/2, step 3957/7134 completed (loss: 0.027092190459370613, acc: 1.0)
[2025-02-13 20:52:22,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:23,033][root][INFO] - Training Epoch: 2/2, step 3958/7134 completed (loss: 0.07247067242860794, acc: 0.976190447807312)
[2025-02-13 20:52:23,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:23,394][root][INFO] - Training Epoch: 2/2, step 3959/7134 completed (loss: 0.020633284002542496, acc: 1.0)
[2025-02-13 20:52:23,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:23,751][root][INFO] - Training Epoch: 2/2, step 3960/7134 completed (loss: 0.056029707193374634, acc: 0.994413435459137)
[2025-02-13 20:52:23,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:24,111][root][INFO] - Training Epoch: 2/2, step 3961/7134 completed (loss: 0.013532896526157856, acc: 1.0)
[2025-02-13 20:52:24,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:24,490][root][INFO] - Training Epoch: 2/2, step 3962/7134 completed (loss: 0.033159106969833374, acc: 0.9932885766029358)
[2025-02-13 20:52:24,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:24,894][root][INFO] - Training Epoch: 2/2, step 3963/7134 completed (loss: 0.16022762656211853, acc: 0.9619565010070801)
[2025-02-13 20:52:25,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:25,248][root][INFO] - Training Epoch: 2/2, step 3964/7134 completed (loss: 0.16369789838790894, acc: 0.9615384340286255)
[2025-02-13 20:52:25,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:25,606][root][INFO] - Training Epoch: 2/2, step 3965/7134 completed (loss: 0.06422578543424606, acc: 0.9818181991577148)
[2025-02-13 20:52:25,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:25,990][root][INFO] - Training Epoch: 2/2, step 3966/7134 completed (loss: 0.034451548010110855, acc: 0.9921875)
[2025-02-13 20:52:26,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:26,351][root][INFO] - Training Epoch: 2/2, step 3967/7134 completed (loss: 0.16981685161590576, acc: 0.95652174949646)
[2025-02-13 20:52:26,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:26,726][root][INFO] - Training Epoch: 2/2, step 3968/7134 completed (loss: 0.01723388582468033, acc: 1.0)
[2025-02-13 20:52:26,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:27,073][root][INFO] - Training Epoch: 2/2, step 3969/7134 completed (loss: 0.10288742929697037, acc: 0.9622641801834106)
[2025-02-13 20:52:27,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:27,427][root][INFO] - Training Epoch: 2/2, step 3970/7134 completed (loss: 0.15018104016780853, acc: 0.9653179049491882)
[2025-02-13 20:52:27,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:27,782][root][INFO] - Training Epoch: 2/2, step 3971/7134 completed (loss: 0.2989489436149597, acc: 0.9384615421295166)
[2025-02-13 20:52:27,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:28,150][root][INFO] - Training Epoch: 2/2, step 3972/7134 completed (loss: 0.4319855272769928, acc: 0.9038461446762085)
[2025-02-13 20:52:28,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:28,520][root][INFO] - Training Epoch: 2/2, step 3973/7134 completed (loss: 0.14040738344192505, acc: 0.9647887349128723)
[2025-02-13 20:52:28,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:28,886][root][INFO] - Training Epoch: 2/2, step 3974/7134 completed (loss: 0.21681582927703857, acc: 0.9589743614196777)
[2025-02-13 20:52:29,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:29,242][root][INFO] - Training Epoch: 2/2, step 3975/7134 completed (loss: 0.11563561856746674, acc: 0.977142870426178)
[2025-02-13 20:52:29,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:29,616][root][INFO] - Training Epoch: 2/2, step 3976/7134 completed (loss: 0.22705549001693726, acc: 0.9518072009086609)
[2025-02-13 20:52:29,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:30,014][root][INFO] - Training Epoch: 2/2, step 3977/7134 completed (loss: 0.04386582225561142, acc: 0.9927536249160767)
[2025-02-13 20:52:30,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:30,433][root][INFO] - Training Epoch: 2/2, step 3978/7134 completed (loss: 0.04811061546206474, acc: 0.9900497794151306)
[2025-02-13 20:52:30,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:30,830][root][INFO] - Training Epoch: 2/2, step 3979/7134 completed (loss: 0.08310938626527786, acc: 0.985401451587677)
[2025-02-13 20:52:30,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:31,200][root][INFO] - Training Epoch: 2/2, step 3980/7134 completed (loss: 0.059895362704992294, acc: 0.9806451797485352)
[2025-02-13 20:52:31,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:31,559][root][INFO] - Training Epoch: 2/2, step 3981/7134 completed (loss: 0.11535625904798508, acc: 0.9698795080184937)
[2025-02-13 20:52:31,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:31,932][root][INFO] - Training Epoch: 2/2, step 3982/7134 completed (loss: 0.25357893109321594, acc: 0.9534883499145508)
[2025-02-13 20:52:32,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:32,300][root][INFO] - Training Epoch: 2/2, step 3983/7134 completed (loss: 0.5685885548591614, acc: 0.8604651093482971)
[2025-02-13 20:52:32,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:32,644][root][INFO] - Training Epoch: 2/2, step 3984/7134 completed (loss: 0.2569550573825836, acc: 0.9202127456665039)
[2025-02-13 20:52:32,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:32,992][root][INFO] - Training Epoch: 2/2, step 3985/7134 completed (loss: 0.07764242589473724, acc: 0.9769230484962463)
[2025-02-13 20:52:33,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:33,357][root][INFO] - Training Epoch: 2/2, step 3986/7134 completed (loss: 0.4821970760822296, acc: 0.8977272510528564)
[2025-02-13 20:52:33,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:33,718][root][INFO] - Training Epoch: 2/2, step 3987/7134 completed (loss: 0.15276598930358887, acc: 0.9682539701461792)
[2025-02-13 20:52:33,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:34,092][root][INFO] - Training Epoch: 2/2, step 3988/7134 completed (loss: 0.2184237539768219, acc: 0.9473684430122375)
[2025-02-13 20:52:34,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:34,405][root][INFO] - Training Epoch: 2/2, step 3989/7134 completed (loss: 0.21487750113010406, acc: 0.942148745059967)
[2025-02-13 20:52:34,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:34,803][root][INFO] - Training Epoch: 2/2, step 3990/7134 completed (loss: 0.1715957522392273, acc: 0.9602649211883545)
[2025-02-13 20:52:34,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:35,162][root][INFO] - Training Epoch: 2/2, step 3991/7134 completed (loss: 0.08928276598453522, acc: 0.9866666793823242)
[2025-02-13 20:52:35,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:35,513][root][INFO] - Training Epoch: 2/2, step 3992/7134 completed (loss: 0.13533155620098114, acc: 0.9780219793319702)
[2025-02-13 20:52:35,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:35,866][root][INFO] - Training Epoch: 2/2, step 3993/7134 completed (loss: 0.12076353281736374, acc: 0.9814814925193787)
[2025-02-13 20:52:35,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:36,211][root][INFO] - Training Epoch: 2/2, step 3994/7134 completed (loss: 0.09730220586061478, acc: 0.9901960492134094)
[2025-02-13 20:52:36,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:36,570][root][INFO] - Training Epoch: 2/2, step 3995/7134 completed (loss: 0.243587926030159, acc: 0.9069767594337463)
[2025-02-13 20:52:36,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:36,923][root][INFO] - Training Epoch: 2/2, step 3996/7134 completed (loss: 0.13074028491973877, acc: 0.9586777091026306)
[2025-02-13 20:52:37,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:37,280][root][INFO] - Training Epoch: 2/2, step 3997/7134 completed (loss: 0.17658205330371857, acc: 0.9568965435028076)
[2025-02-13 20:52:37,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:37,612][root][INFO] - Training Epoch: 2/2, step 3998/7134 completed (loss: 0.1675983965396881, acc: 0.9519230723381042)
[2025-02-13 20:52:37,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:37,975][root][INFO] - Training Epoch: 2/2, step 3999/7134 completed (loss: 0.09255266934633255, acc: 0.9838709831237793)
[2025-02-13 20:52:38,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:38,375][root][INFO] - Training Epoch: 2/2, step 4000/7134 completed (loss: 0.12935365736484528, acc: 0.9674796462059021)
[2025-02-13 20:52:38,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:38,740][root][INFO] - Training Epoch: 2/2, step 4001/7134 completed (loss: 0.1997937560081482, acc: 0.9596773982048035)
[2025-02-13 20:52:38,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:39,098][root][INFO] - Training Epoch: 2/2, step 4002/7134 completed (loss: 0.226765438914299, acc: 0.9532710313796997)
[2025-02-13 20:52:39,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:39,455][root][INFO] - Training Epoch: 2/2, step 4003/7134 completed (loss: 0.18390293419361115, acc: 0.9548872113227844)
[2025-02-13 20:52:39,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:39,851][root][INFO] - Training Epoch: 2/2, step 4004/7134 completed (loss: 0.12911655008792877, acc: 0.9700000286102295)
[2025-02-13 20:52:39,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:40,205][root][INFO] - Training Epoch: 2/2, step 4005/7134 completed (loss: 0.03153201565146446, acc: 0.9934640526771545)
[2025-02-13 20:52:40,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:40,573][root][INFO] - Training Epoch: 2/2, step 4006/7134 completed (loss: 0.032627977430820465, acc: 0.9954954981803894)
[2025-02-13 20:52:40,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:40,956][root][INFO] - Training Epoch: 2/2, step 4007/7134 completed (loss: 0.09715000540018082, acc: 0.976190447807312)
[2025-02-13 20:52:41,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:41,318][root][INFO] - Training Epoch: 2/2, step 4008/7134 completed (loss: 0.03320658579468727, acc: 0.9910314083099365)
[2025-02-13 20:52:41,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:41,692][root][INFO] - Training Epoch: 2/2, step 4009/7134 completed (loss: 0.021646913141012192, acc: 1.0)
[2025-02-13 20:52:41,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:42,058][root][INFO] - Training Epoch: 2/2, step 4010/7134 completed (loss: 0.047661639750003815, acc: 0.9856459498405457)
[2025-02-13 20:52:42,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:42,426][root][INFO] - Training Epoch: 2/2, step 4011/7134 completed (loss: 0.04922989010810852, acc: 0.9885714054107666)
[2025-02-13 20:52:42,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:42,767][root][INFO] - Training Epoch: 2/2, step 4012/7134 completed (loss: 0.0430750772356987, acc: 0.9879518151283264)
[2025-02-13 20:52:42,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:43,125][root][INFO] - Training Epoch: 2/2, step 4013/7134 completed (loss: 0.014058955013751984, acc: 1.0)
[2025-02-13 20:52:43,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:43,489][root][INFO] - Training Epoch: 2/2, step 4014/7134 completed (loss: 0.024403585121035576, acc: 0.9933333396911621)
[2025-02-13 20:52:43,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:43,846][root][INFO] - Training Epoch: 2/2, step 4015/7134 completed (loss: 0.026339532807469368, acc: 0.9862068891525269)
[2025-02-13 20:52:44,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:44,240][root][INFO] - Training Epoch: 2/2, step 4016/7134 completed (loss: 0.11741447448730469, acc: 0.9740932583808899)
[2025-02-13 20:52:44,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:44,611][root][INFO] - Training Epoch: 2/2, step 4017/7134 completed (loss: 0.054981108754873276, acc: 0.9896907210350037)
[2025-02-13 20:52:44,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:44,975][root][INFO] - Training Epoch: 2/2, step 4018/7134 completed (loss: 0.04036074876785278, acc: 0.9951691031455994)
[2025-02-13 20:52:45,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:45,327][root][INFO] - Training Epoch: 2/2, step 4019/7134 completed (loss: 0.054123762995004654, acc: 0.9944444298744202)
[2025-02-13 20:52:45,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:45,684][root][INFO] - Training Epoch: 2/2, step 4020/7134 completed (loss: 0.06826642900705338, acc: 0.9886363744735718)
[2025-02-13 20:52:45,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:46,035][root][INFO] - Training Epoch: 2/2, step 4021/7134 completed (loss: 0.03800820931792259, acc: 0.9852941036224365)
[2025-02-13 20:52:46,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:46,397][root][INFO] - Training Epoch: 2/2, step 4022/7134 completed (loss: 0.040534332394599915, acc: 0.9903846383094788)
[2025-02-13 20:52:46,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:46,761][root][INFO] - Training Epoch: 2/2, step 4023/7134 completed (loss: 0.1168484166264534, acc: 0.9657142758369446)
[2025-02-13 20:52:46,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:47,124][root][INFO] - Training Epoch: 2/2, step 4024/7134 completed (loss: 0.04328290745615959, acc: 0.9885057210922241)
[2025-02-13 20:52:47,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:47,484][root][INFO] - Training Epoch: 2/2, step 4025/7134 completed (loss: 0.16238471865653992, acc: 0.9581151604652405)
[2025-02-13 20:52:47,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:47,826][root][INFO] - Training Epoch: 2/2, step 4026/7134 completed (loss: 0.17390230298042297, acc: 0.9813664555549622)
[2025-02-13 20:52:47,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:48,181][root][INFO] - Training Epoch: 2/2, step 4027/7134 completed (loss: 0.1245071217417717, acc: 0.9550561904907227)
[2025-02-13 20:52:48,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:48,548][root][INFO] - Training Epoch: 2/2, step 4028/7134 completed (loss: 0.18833598494529724, acc: 0.9685039520263672)
[2025-02-13 20:52:48,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:48,925][root][INFO] - Training Epoch: 2/2, step 4029/7134 completed (loss: 0.05644439905881882, acc: 0.9837837815284729)
[2025-02-13 20:52:49,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:49,276][root][INFO] - Training Epoch: 2/2, step 4030/7134 completed (loss: 0.037670813500881195, acc: 0.9903846383094788)
[2025-02-13 20:52:49,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:49,624][root][INFO] - Training Epoch: 2/2, step 4031/7134 completed (loss: 0.05621260777115822, acc: 0.9875776171684265)
[2025-02-13 20:52:49,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:50,013][root][INFO] - Training Epoch: 2/2, step 4032/7134 completed (loss: 0.09989634156227112, acc: 0.9767441749572754)
[2025-02-13 20:52:50,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:50,385][root][INFO] - Training Epoch: 2/2, step 4033/7134 completed (loss: 0.01338291447609663, acc: 1.0)
[2025-02-13 20:52:50,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:50,737][root][INFO] - Training Epoch: 2/2, step 4034/7134 completed (loss: 0.05587188899517059, acc: 0.9948979616165161)
[2025-02-13 20:52:50,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:51,139][root][INFO] - Training Epoch: 2/2, step 4035/7134 completed (loss: 0.2082105129957199, acc: 0.9560975432395935)
[2025-02-13 20:52:51,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:51,485][root][INFO] - Training Epoch: 2/2, step 4036/7134 completed (loss: 0.07563915103673935, acc: 0.9846153855323792)
[2025-02-13 20:52:51,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:51,856][root][INFO] - Training Epoch: 2/2, step 4037/7134 completed (loss: 0.08960222452878952, acc: 0.9739583134651184)
[2025-02-13 20:52:51,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:52,206][root][INFO] - Training Epoch: 2/2, step 4038/7134 completed (loss: 0.07189588993787766, acc: 0.9886363744735718)
[2025-02-13 20:52:52,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:52,550][root][INFO] - Training Epoch: 2/2, step 4039/7134 completed (loss: 0.12411244213581085, acc: 0.976331353187561)
[2025-02-13 20:52:52,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:52,944][root][INFO] - Training Epoch: 2/2, step 4040/7134 completed (loss: 0.04224754497408867, acc: 0.9890710115432739)
[2025-02-13 20:52:53,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:53,311][root][INFO] - Training Epoch: 2/2, step 4041/7134 completed (loss: 0.11323198676109314, acc: 0.9710982441902161)
[2025-02-13 20:52:53,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:53,672][root][INFO] - Training Epoch: 2/2, step 4042/7134 completed (loss: 0.07617633789777756, acc: 0.9797297120094299)
[2025-02-13 20:52:53,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:53,994][root][INFO] - Training Epoch: 2/2, step 4043/7134 completed (loss: 0.10373128950595856, acc: 0.9683544039726257)
[2025-02-13 20:52:54,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:54,346][root][INFO] - Training Epoch: 2/2, step 4044/7134 completed (loss: 0.070035919547081, acc: 0.9835164546966553)
[2025-02-13 20:52:54,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:54,718][root][INFO] - Training Epoch: 2/2, step 4045/7134 completed (loss: 0.0745958536863327, acc: 0.977142870426178)
[2025-02-13 20:52:54,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:55,055][root][INFO] - Training Epoch: 2/2, step 4046/7134 completed (loss: 0.10389112681150436, acc: 0.9824561476707458)
[2025-02-13 20:52:55,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:55,411][root][INFO] - Training Epoch: 2/2, step 4047/7134 completed (loss: 0.09457935392856598, acc: 0.9890710115432739)
[2025-02-13 20:52:55,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:55,791][root][INFO] - Training Epoch: 2/2, step 4048/7134 completed (loss: 0.032508108764886856, acc: 1.0)
[2025-02-13 20:52:55,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:56,179][root][INFO] - Training Epoch: 2/2, step 4049/7134 completed (loss: 0.09391461312770844, acc: 0.9751243591308594)
[2025-02-13 20:52:56,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:56,566][root][INFO] - Training Epoch: 2/2, step 4050/7134 completed (loss: 0.12754620611667633, acc: 0.9624999761581421)
[2025-02-13 20:52:56,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:56,979][root][INFO] - Training Epoch: 2/2, step 4051/7134 completed (loss: 0.1415017992258072, acc: 0.9608938694000244)
[2025-02-13 20:52:57,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:57,329][root][INFO] - Training Epoch: 2/2, step 4052/7134 completed (loss: 0.08224701881408691, acc: 0.9830508232116699)
[2025-02-13 20:52:57,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:57,730][root][INFO] - Training Epoch: 2/2, step 4053/7134 completed (loss: 0.07497495412826538, acc: 0.9702380895614624)
[2025-02-13 20:52:57,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:58,108][root][INFO] - Training Epoch: 2/2, step 4054/7134 completed (loss: 0.23544958233833313, acc: 0.939130425453186)
[2025-02-13 20:52:58,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:58,478][root][INFO] - Training Epoch: 2/2, step 4055/7134 completed (loss: 0.09353573620319366, acc: 0.9680851101875305)
[2025-02-13 20:52:58,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:58,848][root][INFO] - Training Epoch: 2/2, step 4056/7134 completed (loss: 0.062149930745363235, acc: 0.9842105507850647)
[2025-02-13 20:52:58,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:59,212][root][INFO] - Training Epoch: 2/2, step 4057/7134 completed (loss: 0.06805519014596939, acc: 0.9823529124259949)
[2025-02-13 20:52:59,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:59,559][root][INFO] - Training Epoch: 2/2, step 4058/7134 completed (loss: 0.08365039527416229, acc: 0.9772727489471436)
[2025-02-13 20:52:59,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:59,938][root][INFO] - Training Epoch: 2/2, step 4059/7134 completed (loss: 0.022632239386439323, acc: 0.9925373196601868)
[2025-02-13 20:53:00,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:00,283][root][INFO] - Training Epoch: 2/2, step 4060/7134 completed (loss: 0.026275696232914925, acc: 0.9932432174682617)
[2025-02-13 20:53:00,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:00,681][root][INFO] - Training Epoch: 2/2, step 4061/7134 completed (loss: 0.018481751903891563, acc: 0.9941860437393188)
[2025-02-13 20:53:00,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:01,047][root][INFO] - Training Epoch: 2/2, step 4062/7134 completed (loss: 0.018462086096405983, acc: 0.9934210777282715)
[2025-02-13 20:53:01,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:01,413][root][INFO] - Training Epoch: 2/2, step 4063/7134 completed (loss: 0.029360460117459297, acc: 1.0)
[2025-02-13 20:53:01,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:01,794][root][INFO] - Training Epoch: 2/2, step 4064/7134 completed (loss: 0.019548753276467323, acc: 1.0)
[2025-02-13 20:53:01,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:02,160][root][INFO] - Training Epoch: 2/2, step 4065/7134 completed (loss: 0.013875015079975128, acc: 1.0)
[2025-02-13 20:53:02,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:02,561][root][INFO] - Training Epoch: 2/2, step 4066/7134 completed (loss: 0.014906211756169796, acc: 0.9934640526771545)
[2025-02-13 20:53:02,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:02,931][root][INFO] - Training Epoch: 2/2, step 4067/7134 completed (loss: 0.06311555206775665, acc: 0.9794520735740662)
[2025-02-13 20:53:03,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:03,296][root][INFO] - Training Epoch: 2/2, step 4068/7134 completed (loss: 0.13553838431835175, acc: 0.9669421315193176)
[2025-02-13 20:53:03,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:03,650][root][INFO] - Training Epoch: 2/2, step 4069/7134 completed (loss: 0.032567959278821945, acc: 0.9929577708244324)
[2025-02-13 20:53:03,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:04,013][root][INFO] - Training Epoch: 2/2, step 4070/7134 completed (loss: 0.061735861003398895, acc: 0.9867549538612366)
[2025-02-13 20:53:04,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:04,370][root][INFO] - Training Epoch: 2/2, step 4071/7134 completed (loss: 0.05809041112661362, acc: 0.9863013625144958)
[2025-02-13 20:53:04,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:04,750][root][INFO] - Training Epoch: 2/2, step 4072/7134 completed (loss: 0.04664670675992966, acc: 0.9878048896789551)
[2025-02-13 20:53:04,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:05,140][root][INFO] - Training Epoch: 2/2, step 4073/7134 completed (loss: 0.023780498653650284, acc: 0.9939758777618408)
[2025-02-13 20:53:05,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:05,499][root][INFO] - Training Epoch: 2/2, step 4074/7134 completed (loss: 0.0478702075779438, acc: 0.9867549538612366)
[2025-02-13 20:53:05,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:05,894][root][INFO] - Training Epoch: 2/2, step 4075/7134 completed (loss: 0.02693689987063408, acc: 0.9866666793823242)
[2025-02-13 20:53:06,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:06,279][root][INFO] - Training Epoch: 2/2, step 4076/7134 completed (loss: 0.02650490589439869, acc: 0.9939024448394775)
[2025-02-13 20:53:06,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:06,641][root][INFO] - Training Epoch: 2/2, step 4077/7134 completed (loss: 0.07811097055673599, acc: 0.9795918464660645)
[2025-02-13 20:53:06,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:07,026][root][INFO] - Training Epoch: 2/2, step 4078/7134 completed (loss: 0.05930833891034126, acc: 0.9874213933944702)
[2025-02-13 20:53:07,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:07,426][root][INFO] - Training Epoch: 2/2, step 4079/7134 completed (loss: 0.03961590304970741, acc: 0.9847328066825867)
[2025-02-13 20:53:07,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:07,776][root][INFO] - Training Epoch: 2/2, step 4080/7134 completed (loss: 0.02330159582197666, acc: 0.9931034445762634)
[2025-02-13 20:53:07,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:08,149][root][INFO] - Training Epoch: 2/2, step 4081/7134 completed (loss: 0.009852860122919083, acc: 1.0)
[2025-02-13 20:53:08,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:08,502][root][INFO] - Training Epoch: 2/2, step 4082/7134 completed (loss: 0.1739014834165573, acc: 0.9605262875556946)
[2025-02-13 20:53:08,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:08,863][root][INFO] - Training Epoch: 2/2, step 4083/7134 completed (loss: 0.19071368873119354, acc: 0.9356725215911865)
[2025-02-13 20:53:08,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:09,216][root][INFO] - Training Epoch: 2/2, step 4084/7134 completed (loss: 0.32155707478523254, acc: 0.9194630980491638)
[2025-02-13 20:53:09,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:09,592][root][INFO] - Training Epoch: 2/2, step 4085/7134 completed (loss: 0.04959073290228844, acc: 0.9894737005233765)
[2025-02-13 20:53:09,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:09,963][root][INFO] - Training Epoch: 2/2, step 4086/7134 completed (loss: 0.1334225833415985, acc: 0.9679487347602844)
[2025-02-13 20:53:10,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:10,344][root][INFO] - Training Epoch: 2/2, step 4087/7134 completed (loss: 0.08675675094127655, acc: 0.9946236610412598)
[2025-02-13 20:53:10,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:10,701][root][INFO] - Training Epoch: 2/2, step 4088/7134 completed (loss: 0.22804045677185059, acc: 0.9340101480484009)
[2025-02-13 20:53:10,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:11,051][root][INFO] - Training Epoch: 2/2, step 4089/7134 completed (loss: 0.17774154245853424, acc: 0.9469696879386902)
[2025-02-13 20:53:11,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:11,414][root][INFO] - Training Epoch: 2/2, step 4090/7134 completed (loss: 0.30050745606422424, acc: 0.9234449863433838)
[2025-02-13 20:53:11,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:11,782][root][INFO] - Training Epoch: 2/2, step 4091/7134 completed (loss: 0.43530598282814026, acc: 0.8876404762268066)
[2025-02-13 20:53:11,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:12,159][root][INFO] - Training Epoch: 2/2, step 4092/7134 completed (loss: 0.08148013055324554, acc: 0.9895287752151489)
[2025-02-13 20:53:12,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:12,518][root][INFO] - Training Epoch: 2/2, step 4093/7134 completed (loss: 0.07702911645174026, acc: 0.984000027179718)
[2025-02-13 20:53:12,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:12,917][root][INFO] - Training Epoch: 2/2, step 4094/7134 completed (loss: 0.12977470457553864, acc: 0.9634146094322205)
[2025-02-13 20:53:13,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:13,293][root][INFO] - Training Epoch: 2/2, step 4095/7134 completed (loss: 0.049934614449739456, acc: 0.987261176109314)
[2025-02-13 20:53:13,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:13,639][root][INFO] - Training Epoch: 2/2, step 4096/7134 completed (loss: 0.11998765170574188, acc: 0.948051929473877)
[2025-02-13 20:53:13,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:14,010][root][INFO] - Training Epoch: 2/2, step 4097/7134 completed (loss: 0.10943593084812164, acc: 0.9800000190734863)
[2025-02-13 20:53:14,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:14,369][root][INFO] - Training Epoch: 2/2, step 4098/7134 completed (loss: 0.054230015724897385, acc: 0.9942528605461121)
[2025-02-13 20:53:14,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:14,748][root][INFO] - Training Epoch: 2/2, step 4099/7134 completed (loss: 0.08271462470293045, acc: 0.9848484992980957)
[2025-02-13 20:53:14,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:15,156][root][INFO] - Training Epoch: 2/2, step 4100/7134 completed (loss: 0.01676342450082302, acc: 1.0)
[2025-02-13 20:53:15,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:15,527][root][INFO] - Training Epoch: 2/2, step 4101/7134 completed (loss: 0.05686960741877556, acc: 0.984000027179718)
[2025-02-13 20:53:15,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:15,924][root][INFO] - Training Epoch: 2/2, step 4102/7134 completed (loss: 0.07422028481960297, acc: 0.9837398529052734)
[2025-02-13 20:53:16,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:16,296][root][INFO] - Training Epoch: 2/2, step 4103/7134 completed (loss: 0.01928742788732052, acc: 1.0)
[2025-02-13 20:53:16,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:16,717][root][INFO] - Training Epoch: 2/2, step 4104/7134 completed (loss: 0.02720239944756031, acc: 1.0)
[2025-02-13 20:53:16,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:17,084][root][INFO] - Training Epoch: 2/2, step 4105/7134 completed (loss: 0.032327499240636826, acc: 0.9933333396911621)
[2025-02-13 20:53:17,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:17,450][root][INFO] - Training Epoch: 2/2, step 4106/7134 completed (loss: 0.04495555907487869, acc: 0.9931972622871399)
[2025-02-13 20:53:17,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:17,856][root][INFO] - Training Epoch: 2/2, step 4107/7134 completed (loss: 0.07178813964128494, acc: 0.9852941036224365)
[2025-02-13 20:53:17,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:18,247][root][INFO] - Training Epoch: 2/2, step 4108/7134 completed (loss: 0.03829486668109894, acc: 0.9863945841789246)
[2025-02-13 20:53:18,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:18,644][root][INFO] - Training Epoch: 2/2, step 4109/7134 completed (loss: 0.023528631776571274, acc: 1.0)
[2025-02-13 20:53:18,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:19,001][root][INFO] - Training Epoch: 2/2, step 4110/7134 completed (loss: 0.02127763442695141, acc: 0.9922480583190918)
[2025-02-13 20:53:19,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:19,374][root][INFO] - Training Epoch: 2/2, step 4111/7134 completed (loss: 0.07576434314250946, acc: 0.9793103337287903)
[2025-02-13 20:53:19,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:19,739][root][INFO] - Training Epoch: 2/2, step 4112/7134 completed (loss: 0.04923466220498085, acc: 0.9767441749572754)
[2025-02-13 20:53:19,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:20,116][root][INFO] - Training Epoch: 2/2, step 4113/7134 completed (loss: 0.0213109590113163, acc: 0.9930070042610168)
[2025-02-13 20:53:20,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:20,475][root][INFO] - Training Epoch: 2/2, step 4114/7134 completed (loss: 0.11202714592218399, acc: 0.9826086759567261)
[2025-02-13 20:53:20,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:20,877][root][INFO] - Training Epoch: 2/2, step 4115/7134 completed (loss: 0.022076496854424477, acc: 1.0)
[2025-02-13 20:53:21,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:21,255][root][INFO] - Training Epoch: 2/2, step 4116/7134 completed (loss: 0.016432106494903564, acc: 1.0)
[2025-02-13 20:53:21,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:21,622][root][INFO] - Training Epoch: 2/2, step 4117/7134 completed (loss: 0.021325981244444847, acc: 1.0)
[2025-02-13 20:53:21,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:21,989][root][INFO] - Training Epoch: 2/2, step 4118/7134 completed (loss: 0.04334087669849396, acc: 0.9895833134651184)
[2025-02-13 20:53:22,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:22,351][root][INFO] - Training Epoch: 2/2, step 4119/7134 completed (loss: 0.10900966823101044, acc: 0.9729729890823364)
[2025-02-13 20:53:22,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:22,735][root][INFO] - Training Epoch: 2/2, step 4120/7134 completed (loss: 0.11333830654621124, acc: 0.9924812316894531)
[2025-02-13 20:53:22,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:23,093][root][INFO] - Training Epoch: 2/2, step 4121/7134 completed (loss: 0.04796922951936722, acc: 0.9878048896789551)
[2025-02-13 20:53:23,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:23,443][root][INFO] - Training Epoch: 2/2, step 4122/7134 completed (loss: 0.00993704330176115, acc: 1.0)
[2025-02-13 20:53:23,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:23,809][root][INFO] - Training Epoch: 2/2, step 4123/7134 completed (loss: 0.01598634570837021, acc: 0.9935064911842346)
[2025-02-13 20:53:23,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:24,192][root][INFO] - Training Epoch: 2/2, step 4124/7134 completed (loss: 0.03316100314259529, acc: 0.9937499761581421)
[2025-02-13 20:53:24,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:24,551][root][INFO] - Training Epoch: 2/2, step 4125/7134 completed (loss: 0.032866738736629486, acc: 0.9937106966972351)
[2025-02-13 20:53:24,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:24,909][root][INFO] - Training Epoch: 2/2, step 4126/7134 completed (loss: 0.05939085781574249, acc: 0.9802631735801697)
[2025-02-13 20:53:25,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:25,256][root][INFO] - Training Epoch: 2/2, step 4127/7134 completed (loss: 0.04496697336435318, acc: 0.9865771532058716)
[2025-02-13 20:53:25,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:25,612][root][INFO] - Training Epoch: 2/2, step 4128/7134 completed (loss: 0.01778932847082615, acc: 1.0)
[2025-02-13 20:53:25,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:26,021][root][INFO] - Training Epoch: 2/2, step 4129/7134 completed (loss: 0.04174616560339928, acc: 0.9865771532058716)
[2025-02-13 20:53:26,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:26,378][root][INFO] - Training Epoch: 2/2, step 4130/7134 completed (loss: 0.027553457766771317, acc: 0.9913793206214905)
[2025-02-13 20:53:26,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:26,737][root][INFO] - Training Epoch: 2/2, step 4131/7134 completed (loss: 0.04713005572557449, acc: 0.9851852059364319)
[2025-02-13 20:53:26,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:27,087][root][INFO] - Training Epoch: 2/2, step 4132/7134 completed (loss: 0.020311638712882996, acc: 1.0)
[2025-02-13 20:53:27,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:27,439][root][INFO] - Training Epoch: 2/2, step 4133/7134 completed (loss: 0.06642759591341019, acc: 0.9930070042610168)
[2025-02-13 20:53:27,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:27,768][root][INFO] - Training Epoch: 2/2, step 4134/7134 completed (loss: 0.08290501683950424, acc: 0.9727272987365723)
[2025-02-13 20:53:27,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:28,122][root][INFO] - Training Epoch: 2/2, step 4135/7134 completed (loss: 0.03552925959229469, acc: 0.9928571581840515)
[2025-02-13 20:53:28,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:28,496][root][INFO] - Training Epoch: 2/2, step 4136/7134 completed (loss: 0.056792039424180984, acc: 0.9849624037742615)
[2025-02-13 20:53:28,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:28,858][root][INFO] - Training Epoch: 2/2, step 4137/7134 completed (loss: 0.03170540928840637, acc: 1.0)
[2025-02-13 20:53:29,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:29,242][root][INFO] - Training Epoch: 2/2, step 4138/7134 completed (loss: 0.11797159165143967, acc: 0.9607843160629272)
[2025-02-13 20:53:29,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:29,594][root][INFO] - Training Epoch: 2/2, step 4139/7134 completed (loss: 0.053506892174482346, acc: 0.9904761910438538)
[2025-02-13 20:53:29,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:29,949][root][INFO] - Training Epoch: 2/2, step 4140/7134 completed (loss: 0.121337890625, acc: 0.9604519605636597)
[2025-02-13 20:53:30,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:30,313][root][INFO] - Training Epoch: 2/2, step 4141/7134 completed (loss: 0.1139952689409256, acc: 0.9807692170143127)
[2025-02-13 20:53:30,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:30,697][root][INFO] - Training Epoch: 2/2, step 4142/7134 completed (loss: 0.13803011178970337, acc: 0.9549999833106995)
[2025-02-13 20:53:30,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:31,063][root][INFO] - Training Epoch: 2/2, step 4143/7134 completed (loss: 0.11611463129520416, acc: 0.9694322943687439)
[2025-02-13 20:53:31,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:31,462][root][INFO] - Training Epoch: 2/2, step 4144/7134 completed (loss: 0.14750248193740845, acc: 0.9589743614196777)
[2025-02-13 20:53:31,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:31,823][root][INFO] - Training Epoch: 2/2, step 4145/7134 completed (loss: 0.09358056634664536, acc: 0.9661017060279846)
[2025-02-13 20:53:31,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:32,158][root][INFO] - Training Epoch: 2/2, step 4146/7134 completed (loss: 0.2140944004058838, acc: 0.9285714030265808)
[2025-02-13 20:53:32,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:32,514][root][INFO] - Training Epoch: 2/2, step 4147/7134 completed (loss: 0.16692034900188446, acc: 0.9576271176338196)
[2025-02-13 20:53:32,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:32,949][root][INFO] - Training Epoch: 2/2, step 4148/7134 completed (loss: 0.4565076231956482, acc: 0.8791946172714233)
[2025-02-13 20:53:33,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:33,340][root][INFO] - Training Epoch: 2/2, step 4149/7134 completed (loss: 0.2171538770198822, acc: 0.9651162624359131)
[2025-02-13 20:53:33,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:33,727][root][INFO] - Training Epoch: 2/2, step 4150/7134 completed (loss: 0.4171893894672394, acc: 0.9316239356994629)
[2025-02-13 20:53:33,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:34,140][root][INFO] - Training Epoch: 2/2, step 4151/7134 completed (loss: 0.2086656242609024, acc: 0.942148745059967)
[2025-02-13 20:53:34,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:34,517][root][INFO] - Training Epoch: 2/2, step 4152/7134 completed (loss: 0.09622855484485626, acc: 0.9863945841789246)
[2025-02-13 20:53:34,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:34,886][root][INFO] - Training Epoch: 2/2, step 4153/7134 completed (loss: 0.1398259401321411, acc: 0.9714285731315613)
[2025-02-13 20:53:35,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:35,267][root][INFO] - Training Epoch: 2/2, step 4154/7134 completed (loss: 0.1638350486755371, acc: 0.9473684430122375)
[2025-02-13 20:53:35,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:35,665][root][INFO] - Training Epoch: 2/2, step 4155/7134 completed (loss: 0.18761013448238373, acc: 0.9731543660163879)
[2025-02-13 20:53:35,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:36,041][root][INFO] - Training Epoch: 2/2, step 4156/7134 completed (loss: 0.10146810114383698, acc: 0.9649122953414917)
[2025-02-13 20:53:36,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:36,394][root][INFO] - Training Epoch: 2/2, step 4157/7134 completed (loss: 0.0762321799993515, acc: 0.9831932783126831)
[2025-02-13 20:53:36,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:36,749][root][INFO] - Training Epoch: 2/2, step 4158/7134 completed (loss: 0.08829454332590103, acc: 0.9780219793319702)
[2025-02-13 20:53:36,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:37,104][root][INFO] - Training Epoch: 2/2, step 4159/7134 completed (loss: 0.10005660355091095, acc: 0.9849624037742615)
[2025-02-13 20:53:37,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:37,460][root][INFO] - Training Epoch: 2/2, step 4160/7134 completed (loss: 0.11994708329439163, acc: 0.9831932783126831)
[2025-02-13 20:53:37,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:37,865][root][INFO] - Training Epoch: 2/2, step 4161/7134 completed (loss: 0.05760477855801582, acc: 0.9913793206214905)
[2025-02-13 20:53:38,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:38,246][root][INFO] - Training Epoch: 2/2, step 4162/7134 completed (loss: 0.10209979116916656, acc: 0.970059871673584)
[2025-02-13 20:53:38,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:38,612][root][INFO] - Training Epoch: 2/2, step 4163/7134 completed (loss: 0.1057114228606224, acc: 0.9932885766029358)
[2025-02-13 20:53:38,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:38,982][root][INFO] - Training Epoch: 2/2, step 4164/7134 completed (loss: 0.05541307479143143, acc: 0.9751552939414978)
[2025-02-13 20:53:39,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:39,350][root][INFO] - Training Epoch: 2/2, step 4165/7134 completed (loss: 0.08838512003421783, acc: 0.9784946441650391)
[2025-02-13 20:53:39,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:39,736][root][INFO] - Training Epoch: 2/2, step 4166/7134 completed (loss: 0.14409790933132172, acc: 0.9615384340286255)
[2025-02-13 20:53:39,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:40,125][root][INFO] - Training Epoch: 2/2, step 4167/7134 completed (loss: 0.08624885231256485, acc: 0.9837398529052734)
[2025-02-13 20:53:40,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:40,489][root][INFO] - Training Epoch: 2/2, step 4168/7134 completed (loss: 0.11405649036169052, acc: 0.969924807548523)
[2025-02-13 20:53:40,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:40,864][root][INFO] - Training Epoch: 2/2, step 4169/7134 completed (loss: 0.10866936296224594, acc: 0.9769585132598877)
[2025-02-13 20:53:41,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:41,241][root][INFO] - Training Epoch: 2/2, step 4170/7134 completed (loss: 0.10657015442848206, acc: 0.961904764175415)
[2025-02-13 20:53:41,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:41,618][root][INFO] - Training Epoch: 2/2, step 4171/7134 completed (loss: 0.06602980196475983, acc: 0.9802955389022827)
[2025-02-13 20:53:41,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:41,995][root][INFO] - Training Epoch: 2/2, step 4172/7134 completed (loss: 0.06962034106254578, acc: 0.9888888597488403)
[2025-02-13 20:53:42,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:42,332][root][INFO] - Training Epoch: 2/2, step 4173/7134 completed (loss: 0.043010592460632324, acc: 1.0)
[2025-02-13 20:53:42,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:42,677][root][INFO] - Training Epoch: 2/2, step 4174/7134 completed (loss: 0.012426687404513359, acc: 1.0)
[2025-02-13 20:53:42,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:43,046][root][INFO] - Training Epoch: 2/2, step 4175/7134 completed (loss: 0.06892609596252441, acc: 0.9647887349128723)
[2025-02-13 20:53:43,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:43,410][root][INFO] - Training Epoch: 2/2, step 4176/7134 completed (loss: 0.03126966208219528, acc: 0.9870967864990234)
[2025-02-13 20:53:43,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:43,794][root][INFO] - Training Epoch: 2/2, step 4177/7134 completed (loss: 0.031053200364112854, acc: 0.9941520690917969)
[2025-02-13 20:53:43,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:44,164][root][INFO] - Training Epoch: 2/2, step 4178/7134 completed (loss: 0.05731627345085144, acc: 0.9868420958518982)
[2025-02-13 20:53:44,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:44,550][root][INFO] - Training Epoch: 2/2, step 4179/7134 completed (loss: 0.018053872510790825, acc: 1.0)
[2025-02-13 20:53:44,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:44,918][root][INFO] - Training Epoch: 2/2, step 4180/7134 completed (loss: 0.030976999551057816, acc: 0.9937499761581421)
[2025-02-13 20:53:45,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:45,265][root][INFO] - Training Epoch: 2/2, step 4181/7134 completed (loss: 0.01086110807955265, acc: 1.0)
[2025-02-13 20:53:45,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:45,620][root][INFO] - Training Epoch: 2/2, step 4182/7134 completed (loss: 0.011362436227500439, acc: 1.0)
[2025-02-13 20:53:45,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:45,988][root][INFO] - Training Epoch: 2/2, step 4183/7134 completed (loss: 0.03610403090715408, acc: 1.0)
[2025-02-13 20:53:46,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:46,340][root][INFO] - Training Epoch: 2/2, step 4184/7134 completed (loss: 0.021538151428103447, acc: 0.9941176176071167)
[2025-02-13 20:53:46,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:46,707][root][INFO] - Training Epoch: 2/2, step 4185/7134 completed (loss: 0.012928549200296402, acc: 1.0)
[2025-02-13 20:53:46,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:47,080][root][INFO] - Training Epoch: 2/2, step 4186/7134 completed (loss: 0.10149585455656052, acc: 0.9793814420700073)
[2025-02-13 20:53:47,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:47,426][root][INFO] - Training Epoch: 2/2, step 4187/7134 completed (loss: 0.013456705957651138, acc: 1.0)
[2025-02-13 20:53:47,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:47,790][root][INFO] - Training Epoch: 2/2, step 4188/7134 completed (loss: 0.015307694673538208, acc: 0.9947368502616882)
[2025-02-13 20:53:47,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:48,181][root][INFO] - Training Epoch: 2/2, step 4189/7134 completed (loss: 0.04868379980325699, acc: 0.9820359349250793)
[2025-02-13 20:53:48,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:48,542][root][INFO] - Training Epoch: 2/2, step 4190/7134 completed (loss: 0.05913655087351799, acc: 0.9766082167625427)
[2025-02-13 20:53:48,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:48,913][root][INFO] - Training Epoch: 2/2, step 4191/7134 completed (loss: 0.05298066511750221, acc: 0.9903846383094788)
[2025-02-13 20:53:49,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:49,270][root][INFO] - Training Epoch: 2/2, step 4192/7134 completed (loss: 0.015020857565104961, acc: 1.0)
[2025-02-13 20:53:49,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:49,634][root][INFO] - Training Epoch: 2/2, step 4193/7134 completed (loss: 0.054646022617816925, acc: 0.994413435459137)
[2025-02-13 20:53:49,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:50,007][root][INFO] - Training Epoch: 2/2, step 4194/7134 completed (loss: 0.008903071284294128, acc: 1.0)
[2025-02-13 20:53:50,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:50,405][root][INFO] - Training Epoch: 2/2, step 4195/7134 completed (loss: 0.0868532806634903, acc: 0.9885714054107666)
[2025-02-13 20:53:50,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:50,767][root][INFO] - Training Epoch: 2/2, step 4196/7134 completed (loss: 0.03283122554421425, acc: 0.9928057789802551)
[2025-02-13 20:53:50,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:51,131][root][INFO] - Training Epoch: 2/2, step 4197/7134 completed (loss: 0.04793528467416763, acc: 0.989130437374115)
[2025-02-13 20:53:51,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:51,492][root][INFO] - Training Epoch: 2/2, step 4198/7134 completed (loss: 0.07882744073867798, acc: 0.9942857027053833)
[2025-02-13 20:53:51,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:51,833][root][INFO] - Training Epoch: 2/2, step 4199/7134 completed (loss: 0.09310448169708252, acc: 0.9647887349128723)
[2025-02-13 20:53:51,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:52,190][root][INFO] - Training Epoch: 2/2, step 4200/7134 completed (loss: 0.03299842029809952, acc: 1.0)
[2025-02-13 20:53:52,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:52,575][root][INFO] - Training Epoch: 2/2, step 4201/7134 completed (loss: 0.025918077677488327, acc: 0.9929078221321106)
[2025-02-13 20:53:52,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:52,969][root][INFO] - Training Epoch: 2/2, step 4202/7134 completed (loss: 0.06989115476608276, acc: 0.9924812316894531)
[2025-02-13 20:53:53,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:53,354][root][INFO] - Training Epoch: 2/2, step 4203/7134 completed (loss: 0.054407380521297455, acc: 0.9873417615890503)
[2025-02-13 20:53:53,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:53,733][root][INFO] - Training Epoch: 2/2, step 4204/7134 completed (loss: 0.10568936914205551, acc: 0.977142870426178)
[2025-02-13 20:53:53,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:54,092][root][INFO] - Training Epoch: 2/2, step 4205/7134 completed (loss: 0.13550330698490143, acc: 0.9647058844566345)
[2025-02-13 20:53:54,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:54,463][root][INFO] - Training Epoch: 2/2, step 4206/7134 completed (loss: 0.12863785028457642, acc: 0.9784946441650391)
[2025-02-13 20:53:54,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:54,831][root][INFO] - Training Epoch: 2/2, step 4207/7134 completed (loss: 0.06233656778931618, acc: 0.9874213933944702)
[2025-02-13 20:53:54,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:55,182][root][INFO] - Training Epoch: 2/2, step 4208/7134 completed (loss: 0.11531274020671844, acc: 0.977011501789093)
[2025-02-13 20:53:55,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:55,543][root][INFO] - Training Epoch: 2/2, step 4209/7134 completed (loss: 0.08879557996988297, acc: 0.988950252532959)
[2025-02-13 20:53:55,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:55,943][root][INFO] - Training Epoch: 2/2, step 4210/7134 completed (loss: 0.04865596443414688, acc: 0.9912280440330505)
[2025-02-13 20:53:56,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:56,312][root][INFO] - Training Epoch: 2/2, step 4211/7134 completed (loss: 0.027953915297985077, acc: 1.0)
[2025-02-13 20:53:56,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:56,680][root][INFO] - Training Epoch: 2/2, step 4212/7134 completed (loss: 0.028257615864276886, acc: 1.0)
[2025-02-13 20:53:56,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:57,040][root][INFO] - Training Epoch: 2/2, step 4213/7134 completed (loss: 0.012110117822885513, acc: 1.0)
[2025-02-13 20:53:57,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:57,411][root][INFO] - Training Epoch: 2/2, step 4214/7134 completed (loss: 0.11537965387105942, acc: 0.9766082167625427)
[2025-02-13 20:53:57,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:57,788][root][INFO] - Training Epoch: 2/2, step 4215/7134 completed (loss: 0.07592704892158508, acc: 0.9842519760131836)
[2025-02-13 20:53:57,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:58,140][root][INFO] - Training Epoch: 2/2, step 4216/7134 completed (loss: 0.06946088373661041, acc: 0.9923076629638672)
[2025-02-13 20:53:58,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:58,492][root][INFO] - Training Epoch: 2/2, step 4217/7134 completed (loss: 0.022664954885840416, acc: 0.9944444298744202)
[2025-02-13 20:53:58,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:58,826][root][INFO] - Training Epoch: 2/2, step 4218/7134 completed (loss: 0.10661069303750992, acc: 0.9776119589805603)
[2025-02-13 20:53:58,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:59,136][root][INFO] - Training Epoch: 2/2, step 4219/7134 completed (loss: 0.028477299958467484, acc: 1.0)
[2025-02-13 20:53:59,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:59,497][root][INFO] - Training Epoch: 2/2, step 4220/7134 completed (loss: 0.18811799585819244, acc: 0.9602272510528564)
[2025-02-13 20:53:59,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:59,867][root][INFO] - Training Epoch: 2/2, step 4221/7134 completed (loss: 0.07481749355792999, acc: 0.9797297120094299)
[2025-02-13 20:53:59,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:00,202][root][INFO] - Training Epoch: 2/2, step 4222/7134 completed (loss: 0.1723651885986328, acc: 0.9824561476707458)
[2025-02-13 20:54:00,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:00,558][root][INFO] - Training Epoch: 2/2, step 4223/7134 completed (loss: 0.19562658667564392, acc: 0.9447236061096191)
[2025-02-13 20:54:00,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:00,949][root][INFO] - Training Epoch: 2/2, step 4224/7134 completed (loss: 0.1559785157442093, acc: 0.9596773982048035)
[2025-02-13 20:54:01,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:01,318][root][INFO] - Training Epoch: 2/2, step 4225/7134 completed (loss: 0.1078716516494751, acc: 0.9743589758872986)
[2025-02-13 20:54:01,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:01,685][root][INFO] - Training Epoch: 2/2, step 4226/7134 completed (loss: 0.17184053361415863, acc: 0.9644669890403748)
[2025-02-13 20:54:01,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:02,027][root][INFO] - Training Epoch: 2/2, step 4227/7134 completed (loss: 0.1867346465587616, acc: 0.9528301954269409)
[2025-02-13 20:54:02,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:02,386][root][INFO] - Training Epoch: 2/2, step 4228/7134 completed (loss: 0.24172939360141754, acc: 0.9487179517745972)
[2025-02-13 20:54:02,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:02,742][root][INFO] - Training Epoch: 2/2, step 4229/7134 completed (loss: 0.2007681429386139, acc: 0.9520000219345093)
[2025-02-13 20:54:02,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:03,115][root][INFO] - Training Epoch: 2/2, step 4230/7134 completed (loss: 0.2396266907453537, acc: 0.9418604373931885)
[2025-02-13 20:54:03,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:03,470][root][INFO] - Training Epoch: 2/2, step 4231/7134 completed (loss: 0.37097886204719543, acc: 0.9463087320327759)
[2025-02-13 20:54:03,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:03,822][root][INFO] - Training Epoch: 2/2, step 4232/7134 completed (loss: 0.19472439587116241, acc: 0.9444444179534912)
[2025-02-13 20:54:03,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:04,188][root][INFO] - Training Epoch: 2/2, step 4233/7134 completed (loss: 0.124937042593956, acc: 0.978723406791687)
[2025-02-13 20:54:04,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:04,557][root][INFO] - Training Epoch: 2/2, step 4234/7134 completed (loss: 0.08625472337007523, acc: 0.978723406791687)
[2025-02-13 20:54:04,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:04,970][root][INFO] - Training Epoch: 2/2, step 4235/7134 completed (loss: 0.058647915720939636, acc: 0.9855072498321533)
[2025-02-13 20:54:05,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:05,324][root][INFO] - Training Epoch: 2/2, step 4236/7134 completed (loss: 0.016965875402092934, acc: 1.0)
[2025-02-13 20:54:05,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:05,693][root][INFO] - Training Epoch: 2/2, step 4237/7134 completed (loss: 0.057455480098724365, acc: 1.0)
[2025-02-13 20:54:05,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:06,055][root][INFO] - Training Epoch: 2/2, step 4238/7134 completed (loss: 0.12812019884586334, acc: 0.9692307710647583)
[2025-02-13 20:54:06,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:06,402][root][INFO] - Training Epoch: 2/2, step 4239/7134 completed (loss: 0.035871293395757675, acc: 0.9919999837875366)
[2025-02-13 20:54:06,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:06,805][root][INFO] - Training Epoch: 2/2, step 4240/7134 completed (loss: 0.16729333996772766, acc: 0.9647887349128723)
[2025-02-13 20:54:06,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:07,183][root][INFO] - Training Epoch: 2/2, step 4241/7134 completed (loss: 0.025154704228043556, acc: 1.0)
[2025-02-13 20:54:07,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:07,540][root][INFO] - Training Epoch: 2/2, step 4242/7134 completed (loss: 0.06741601973772049, acc: 0.9897959232330322)
[2025-02-13 20:54:07,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:07,912][root][INFO] - Training Epoch: 2/2, step 4243/7134 completed (loss: 0.09462859481573105, acc: 0.9752066135406494)
[2025-02-13 20:54:08,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:08,248][root][INFO] - Training Epoch: 2/2, step 4244/7134 completed (loss: 0.02391473948955536, acc: 1.0)
[2025-02-13 20:54:08,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:08,602][root][INFO] - Training Epoch: 2/2, step 4245/7134 completed (loss: 0.03447885438799858, acc: 1.0)
[2025-02-13 20:54:08,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:08,975][root][INFO] - Training Epoch: 2/2, step 4246/7134 completed (loss: 0.07075731456279755, acc: 0.9931507110595703)
[2025-02-13 20:54:09,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:09,342][root][INFO] - Training Epoch: 2/2, step 4247/7134 completed (loss: 0.025212902575731277, acc: 1.0)
[2025-02-13 20:54:09,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:09,712][root][INFO] - Training Epoch: 2/2, step 4248/7134 completed (loss: 0.1633480042219162, acc: 0.9726027250289917)
[2025-02-13 20:54:09,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:10,132][root][INFO] - Training Epoch: 2/2, step 4249/7134 completed (loss: 0.1372375786304474, acc: 0.9779411554336548)
[2025-02-13 20:54:10,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:10,493][root][INFO] - Training Epoch: 2/2, step 4250/7134 completed (loss: 0.2531399726867676, acc: 0.9545454382896423)
[2025-02-13 20:54:10,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:10,853][root][INFO] - Training Epoch: 2/2, step 4251/7134 completed (loss: 0.06397388875484467, acc: 0.9849624037742615)
[2025-02-13 20:54:10,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11,211][root][INFO] - Training Epoch: 2/2, step 4252/7134 completed (loss: 0.06393471360206604, acc: 0.9838709831237793)
[2025-02-13 20:54:11,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11,578][root][INFO] - Training Epoch: 2/2, step 4253/7134 completed (loss: 0.08044421672821045, acc: 0.9739130139350891)
[2025-02-13 20:54:11,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11,930][root][INFO] - Training Epoch: 2/2, step 4254/7134 completed (loss: 0.01714889146387577, acc: 1.0)
[2025-02-13 20:54:12,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:12,335][root][INFO] - Training Epoch: 2/2, step 4255/7134 completed (loss: 0.09862953424453735, acc: 0.9692307710647583)
[2025-02-13 20:54:12,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:12,714][root][INFO] - Training Epoch: 2/2, step 4256/7134 completed (loss: 0.06993374228477478, acc: 0.9743589758872986)
[2025-02-13 20:54:12,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:13,086][root][INFO] - Training Epoch: 2/2, step 4257/7134 completed (loss: 0.022540580481290817, acc: 1.0)
[2025-02-13 20:54:13,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:13,448][root][INFO] - Training Epoch: 2/2, step 4258/7134 completed (loss: 0.015141426585614681, acc: 1.0)
[2025-02-13 20:54:13,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:13,818][root][INFO] - Training Epoch: 2/2, step 4259/7134 completed (loss: 0.1485351324081421, acc: 0.9750000238418579)
[2025-02-13 20:54:13,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:14,196][root][INFO] - Training Epoch: 2/2, step 4260/7134 completed (loss: 0.051154766231775284, acc: 0.9908257126808167)
[2025-02-13 20:54:14,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:14,556][root][INFO] - Training Epoch: 2/2, step 4261/7134 completed (loss: 0.03523235395550728, acc: 0.991304337978363)
[2025-02-13 20:54:14,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:14,928][root][INFO] - Training Epoch: 2/2, step 4262/7134 completed (loss: 0.05229966342449188, acc: 0.9930070042610168)
[2025-02-13 20:54:15,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:15,281][root][INFO] - Training Epoch: 2/2, step 4263/7134 completed (loss: 0.0968519002199173, acc: 0.9777777791023254)
[2025-02-13 20:54:15,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:15,641][root][INFO] - Training Epoch: 2/2, step 4264/7134 completed (loss: 0.12891560792922974, acc: 0.9775280952453613)
[2025-02-13 20:54:15,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:16,001][root][INFO] - Training Epoch: 2/2, step 4265/7134 completed (loss: 0.11154269427061081, acc: 0.9709302186965942)
[2025-02-13 20:54:16,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:16,380][root][INFO] - Training Epoch: 2/2, step 4266/7134 completed (loss: 0.05252508074045181, acc: 0.9842932224273682)
[2025-02-13 20:54:16,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:16,764][root][INFO] - Training Epoch: 2/2, step 4267/7134 completed (loss: 0.05262274667620659, acc: 0.9797979593276978)
[2025-02-13 20:54:16,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:17,137][root][INFO] - Training Epoch: 2/2, step 4268/7134 completed (loss: 0.07220765203237534, acc: 0.9918699264526367)
[2025-02-13 20:54:17,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:17,523][root][INFO] - Training Epoch: 2/2, step 4269/7134 completed (loss: 0.17071449756622314, acc: 0.9537572264671326)
[2025-02-13 20:54:17,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:17,884][root][INFO] - Training Epoch: 2/2, step 4270/7134 completed (loss: 0.056877896189689636, acc: 0.9895833134651184)
[2025-02-13 20:54:18,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:18,251][root][INFO] - Training Epoch: 2/2, step 4271/7134 completed (loss: 0.04892995208501816, acc: 0.9945945739746094)
[2025-02-13 20:54:18,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:18,623][root][INFO] - Training Epoch: 2/2, step 4272/7134 completed (loss: 0.0934443473815918, acc: 0.9800994992256165)
[2025-02-13 20:54:18,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:19,007][root][INFO] - Training Epoch: 2/2, step 4273/7134 completed (loss: 0.035436853766441345, acc: 0.9894179701805115)
[2025-02-13 20:54:19,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:19,392][root][INFO] - Training Epoch: 2/2, step 4274/7134 completed (loss: 0.14937710762023926, acc: 0.9754902124404907)
[2025-02-13 20:54:19,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:19,774][root][INFO] - Training Epoch: 2/2, step 4275/7134 completed (loss: 0.06550589948892593, acc: 0.9832402467727661)
[2025-02-13 20:54:19,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:20,151][root][INFO] - Training Epoch: 2/2, step 4276/7134 completed (loss: 0.08490017056465149, acc: 0.9891892075538635)
[2025-02-13 20:54:20,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:20,523][root][INFO] - Training Epoch: 2/2, step 4277/7134 completed (loss: 0.022789742797613144, acc: 1.0)
[2025-02-13 20:54:20,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:20,884][root][INFO] - Training Epoch: 2/2, step 4278/7134 completed (loss: 0.09818607568740845, acc: 0.9795918464660645)
[2025-02-13 20:54:21,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:21,242][root][INFO] - Training Epoch: 2/2, step 4279/7134 completed (loss: 0.046316854655742645, acc: 0.9937888383865356)
[2025-02-13 20:54:21,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:21,604][root][INFO] - Training Epoch: 2/2, step 4280/7134 completed (loss: 0.009406620636582375, acc: 1.0)
[2025-02-13 20:54:21,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:21,979][root][INFO] - Training Epoch: 2/2, step 4281/7134 completed (loss: 0.07810370624065399, acc: 0.9892473220825195)
[2025-02-13 20:54:22,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:22,381][root][INFO] - Training Epoch: 2/2, step 4282/7134 completed (loss: 0.04403379186987877, acc: 0.9741379022598267)
[2025-02-13 20:54:22,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:22,759][root][INFO] - Training Epoch: 2/2, step 4283/7134 completed (loss: 0.03888025879859924, acc: 0.9897959232330322)
[2025-02-13 20:54:22,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:23,160][root][INFO] - Training Epoch: 2/2, step 4284/7134 completed (loss: 0.07712511718273163, acc: 0.9844961166381836)
[2025-02-13 20:54:23,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:23,530][root][INFO] - Training Epoch: 2/2, step 4285/7134 completed (loss: 0.018807103857398033, acc: 1.0)
[2025-02-13 20:54:23,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:23,913][root][INFO] - Training Epoch: 2/2, step 4286/7134 completed (loss: 0.043808676302433014, acc: 0.9831932783126831)
[2025-02-13 20:54:24,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:24,284][root][INFO] - Training Epoch: 2/2, step 4287/7134 completed (loss: 0.04664428159594536, acc: 0.989130437374115)
[2025-02-13 20:54:24,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:24,652][root][INFO] - Training Epoch: 2/2, step 4288/7134 completed (loss: 0.047079574316740036, acc: 0.9882352948188782)
[2025-02-13 20:54:24,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:25,039][root][INFO] - Training Epoch: 2/2, step 4289/7134 completed (loss: 0.10836587101221085, acc: 0.9754601120948792)
[2025-02-13 20:54:25,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:25,426][root][INFO] - Training Epoch: 2/2, step 4290/7134 completed (loss: 0.05798499286174774, acc: 0.9826589822769165)
[2025-02-13 20:54:25,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:25,800][root][INFO] - Training Epoch: 2/2, step 4291/7134 completed (loss: 0.02611229382455349, acc: 0.9942196607589722)
[2025-02-13 20:54:25,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:26,203][root][INFO] - Training Epoch: 2/2, step 4292/7134 completed (loss: 0.06823213398456573, acc: 0.9935483932495117)
[2025-02-13 20:54:26,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:26,597][root][INFO] - Training Epoch: 2/2, step 4293/7134 completed (loss: 0.04213116317987442, acc: 0.9916666746139526)
[2025-02-13 20:54:26,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:26,957][root][INFO] - Training Epoch: 2/2, step 4294/7134 completed (loss: 0.09110832214355469, acc: 0.977142870426178)
[2025-02-13 20:54:27,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:27,340][root][INFO] - Training Epoch: 2/2, step 4295/7134 completed (loss: 0.0842699259519577, acc: 0.9714285731315613)
[2025-02-13 20:54:27,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:27,726][root][INFO] - Training Epoch: 2/2, step 4296/7134 completed (loss: 0.02517366223037243, acc: 0.9900990128517151)
[2025-02-13 20:54:27,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:28,105][root][INFO] - Training Epoch: 2/2, step 4297/7134 completed (loss: 0.005855708848685026, acc: 1.0)
[2025-02-13 20:54:28,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:28,483][root][INFO] - Training Epoch: 2/2, step 4298/7134 completed (loss: 0.11639959365129471, acc: 0.9659090638160706)
[2025-02-13 20:54:28,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:28,830][root][INFO] - Training Epoch: 2/2, step 4299/7134 completed (loss: 0.10285748541355133, acc: 0.9731183052062988)
[2025-02-13 20:54:28,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:29,189][root][INFO] - Training Epoch: 2/2, step 4300/7134 completed (loss: 0.13636203110218048, acc: 0.9831932783126831)
[2025-02-13 20:54:29,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:29,564][root][INFO] - Training Epoch: 2/2, step 4301/7134 completed (loss: 0.035569701343774796, acc: 0.9929577708244324)
[2025-02-13 20:54:29,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:29,938][root][INFO] - Training Epoch: 2/2, step 4302/7134 completed (loss: 0.02668563835322857, acc: 1.0)
[2025-02-13 20:54:30,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:30,333][root][INFO] - Training Epoch: 2/2, step 4303/7134 completed (loss: 0.09637027978897095, acc: 0.9653465151786804)
[2025-02-13 20:54:30,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:30,719][root][INFO] - Training Epoch: 2/2, step 4304/7134 completed (loss: 0.028569040820002556, acc: 1.0)
[2025-02-13 20:54:30,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:31,113][root][INFO] - Training Epoch: 2/2, step 4305/7134 completed (loss: 0.06536263227462769, acc: 0.9850000143051147)
[2025-02-13 20:54:31,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:31,491][root][INFO] - Training Epoch: 2/2, step 4306/7134 completed (loss: 0.1252700835466385, acc: 0.9738219976425171)
[2025-02-13 20:54:31,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:31,863][root][INFO] - Training Epoch: 2/2, step 4307/7134 completed (loss: 0.13395434617996216, acc: 0.9673202633857727)
[2025-02-13 20:54:32,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:32,244][root][INFO] - Training Epoch: 2/2, step 4308/7134 completed (loss: 0.07034869492053986, acc: 0.9893048405647278)
[2025-02-13 20:54:32,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:32,617][root][INFO] - Training Epoch: 2/2, step 4309/7134 completed (loss: 0.1155344694852829, acc: 0.9715909361839294)
[2025-02-13 20:54:32,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:32,980][root][INFO] - Training Epoch: 2/2, step 4310/7134 completed (loss: 0.061654482036828995, acc: 0.9847715497016907)
[2025-02-13 20:54:33,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:33,368][root][INFO] - Training Epoch: 2/2, step 4311/7134 completed (loss: 0.04906098172068596, acc: 0.9929078221321106)
[2025-02-13 20:54:33,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:33,746][root][INFO] - Training Epoch: 2/2, step 4312/7134 completed (loss: 0.07655283808708191, acc: 0.9815950989723206)
[2025-02-13 20:54:33,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:34,115][root][INFO] - Training Epoch: 2/2, step 4313/7134 completed (loss: 0.020013706758618355, acc: 1.0)
[2025-02-13 20:54:34,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:34,483][root][INFO] - Training Epoch: 2/2, step 4314/7134 completed (loss: 0.05926566570997238, acc: 0.9803921580314636)
[2025-02-13 20:54:34,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:34,847][root][INFO] - Training Epoch: 2/2, step 4315/7134 completed (loss: 0.03463629260659218, acc: 0.9940476417541504)
[2025-02-13 20:54:34,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:35,213][root][INFO] - Training Epoch: 2/2, step 4316/7134 completed (loss: 0.02626081369817257, acc: 1.0)
[2025-02-13 20:54:35,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:35,588][root][INFO] - Training Epoch: 2/2, step 4317/7134 completed (loss: 0.048855360597372055, acc: 0.9767441749572754)
[2025-02-13 20:54:35,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:35,972][root][INFO] - Training Epoch: 2/2, step 4318/7134 completed (loss: 0.08606678992509842, acc: 0.9846938848495483)
[2025-02-13 20:54:36,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:36,378][root][INFO] - Training Epoch: 2/2, step 4319/7134 completed (loss: 0.04535127803683281, acc: 0.9852216839790344)
[2025-02-13 20:54:36,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:36,749][root][INFO] - Training Epoch: 2/2, step 4320/7134 completed (loss: 0.035809412598609924, acc: 0.9935897588729858)
[2025-02-13 20:54:36,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:37,116][root][INFO] - Training Epoch: 2/2, step 4321/7134 completed (loss: 0.1300407499074936, acc: 0.9801980257034302)
[2025-02-13 20:54:37,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:37,489][root][INFO] - Training Epoch: 2/2, step 4322/7134 completed (loss: 0.10849025100469589, acc: 0.9748427867889404)
[2025-02-13 20:54:37,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:37,846][root][INFO] - Training Epoch: 2/2, step 4323/7134 completed (loss: 0.03541824594140053, acc: 0.9924812316894531)
[2025-02-13 20:54:37,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:38,204][root][INFO] - Training Epoch: 2/2, step 4324/7134 completed (loss: 0.04439229145646095, acc: 0.991304337978363)
[2025-02-13 20:54:38,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:38,571][root][INFO] - Training Epoch: 2/2, step 4325/7134 completed (loss: 0.13218426704406738, acc: 0.9726027250289917)
[2025-02-13 20:54:38,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:38,939][root][INFO] - Training Epoch: 2/2, step 4326/7134 completed (loss: 0.13428688049316406, acc: 0.9621211886405945)
[2025-02-13 20:54:39,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:39,290][root][INFO] - Training Epoch: 2/2, step 4327/7134 completed (loss: 0.052160389721393585, acc: 0.985401451587677)
[2025-02-13 20:54:39,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:39,663][root][INFO] - Training Epoch: 2/2, step 4328/7134 completed (loss: 0.03271172568202019, acc: 0.9921875)
[2025-02-13 20:54:39,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:40,039][root][INFO] - Training Epoch: 2/2, step 4329/7134 completed (loss: 0.07429394125938416, acc: 0.988304078578949)
[2025-02-13 20:54:40,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:40,390][root][INFO] - Training Epoch: 2/2, step 4330/7134 completed (loss: 0.03989250212907791, acc: 0.9909909963607788)
[2025-02-13 20:54:40,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:40,756][root][INFO] - Training Epoch: 2/2, step 4331/7134 completed (loss: 0.07468878477811813, acc: 0.9753086566925049)
[2025-02-13 20:54:40,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:41,122][root][INFO] - Training Epoch: 2/2, step 4332/7134 completed (loss: 0.07407713681459427, acc: 0.9841269850730896)
[2025-02-13 20:54:41,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:41,493][root][INFO] - Training Epoch: 2/2, step 4333/7134 completed (loss: 0.0555863194167614, acc: 0.987500011920929)
[2025-02-13 20:54:41,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:41,911][root][INFO] - Training Epoch: 2/2, step 4334/7134 completed (loss: 0.11051904410123825, acc: 0.9815950989723206)
[2025-02-13 20:54:42,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:42,296][root][INFO] - Training Epoch: 2/2, step 4335/7134 completed (loss: 0.08790574222803116, acc: 0.9834710955619812)
[2025-02-13 20:54:42,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:42,658][root][INFO] - Training Epoch: 2/2, step 4336/7134 completed (loss: 0.01804148219525814, acc: 0.9928057789802551)
[2025-02-13 20:54:42,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:43,016][root][INFO] - Training Epoch: 2/2, step 4337/7134 completed (loss: 0.05681143328547478, acc: 0.9790209531784058)
[2025-02-13 20:54:43,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:43,371][root][INFO] - Training Epoch: 2/2, step 4338/7134 completed (loss: 0.045759085565805435, acc: 0.9878787994384766)
[2025-02-13 20:54:43,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:43,746][root][INFO] - Training Epoch: 2/2, step 4339/7134 completed (loss: 0.04302426055073738, acc: 0.9882352948188782)
[2025-02-13 20:54:43,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:44,141][root][INFO] - Training Epoch: 2/2, step 4340/7134 completed (loss: 0.051816847175359726, acc: 0.9793103337287903)
[2025-02-13 20:54:44,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:44,523][root][INFO] - Training Epoch: 2/2, step 4341/7134 completed (loss: 0.07025329023599625, acc: 0.9863945841789246)
[2025-02-13 20:54:44,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:44,924][root][INFO] - Training Epoch: 2/2, step 4342/7134 completed (loss: 0.11455221474170685, acc: 0.9668874144554138)
[2025-02-13 20:54:45,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:45,329][root][INFO] - Training Epoch: 2/2, step 4343/7134 completed (loss: 0.05838314816355705, acc: 0.9777777791023254)
[2025-02-13 20:54:45,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:45,753][root][INFO] - Training Epoch: 2/2, step 4344/7134 completed (loss: 0.039201341569423676, acc: 0.991304337978363)
[2025-02-13 20:54:45,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:46,109][root][INFO] - Training Epoch: 2/2, step 4345/7134 completed (loss: 0.04714592918753624, acc: 0.9909909963607788)
[2025-02-13 20:54:46,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:46,460][root][INFO] - Training Epoch: 2/2, step 4346/7134 completed (loss: 0.0467926561832428, acc: 0.9819819927215576)
[2025-02-13 20:54:46,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:46,818][root][INFO] - Training Epoch: 2/2, step 4347/7134 completed (loss: 0.1055721566081047, acc: 0.9704142212867737)
[2025-02-13 20:54:46,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:47,165][root][INFO] - Training Epoch: 2/2, step 4348/7134 completed (loss: 0.055082645267248154, acc: 0.9795918464660645)
[2025-02-13 20:54:47,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:47,498][root][INFO] - Training Epoch: 2/2, step 4349/7134 completed (loss: 0.10868796706199646, acc: 0.96875)
[2025-02-13 20:54:47,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:47,859][root][INFO] - Training Epoch: 2/2, step 4350/7134 completed (loss: 0.10482900589704514, acc: 0.9775280952453613)
[2025-02-13 20:54:47,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:48,216][root][INFO] - Training Epoch: 2/2, step 4351/7134 completed (loss: 0.051033101975917816, acc: 1.0)
[2025-02-13 20:54:48,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:48,593][root][INFO] - Training Epoch: 2/2, step 4352/7134 completed (loss: 0.09363465756177902, acc: 0.9751552939414978)
[2025-02-13 20:54:48,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:48,961][root][INFO] - Training Epoch: 2/2, step 4353/7134 completed (loss: 0.05843764543533325, acc: 0.9820359349250793)
[2025-02-13 20:54:49,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:49,333][root][INFO] - Training Epoch: 2/2, step 4354/7134 completed (loss: 0.041381821036338806, acc: 0.9938650131225586)
[2025-02-13 20:54:49,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:49,727][root][INFO] - Training Epoch: 2/2, step 4355/7134 completed (loss: 0.09974808990955353, acc: 0.9751552939414978)
[2025-02-13 20:54:49,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:50,130][root][INFO] - Training Epoch: 2/2, step 4356/7134 completed (loss: 0.030225668102502823, acc: 1.0)
[2025-02-13 20:54:50,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:50,504][root][INFO] - Training Epoch: 2/2, step 4357/7134 completed (loss: 0.17590977251529694, acc: 0.9731543660163879)
[2025-02-13 20:54:50,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:50,919][root][INFO] - Training Epoch: 2/2, step 4358/7134 completed (loss: 0.10052094608545303, acc: 0.9817073345184326)
[2025-02-13 20:54:51,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:51,300][root][INFO] - Training Epoch: 2/2, step 4359/7134 completed (loss: 0.08738817274570465, acc: 0.9682539701461792)
[2025-02-13 20:54:51,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:51,646][root][INFO] - Training Epoch: 2/2, step 4360/7134 completed (loss: 0.04446558654308319, acc: 0.993630588054657)
[2025-02-13 20:54:51,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:52,006][root][INFO] - Training Epoch: 2/2, step 4361/7134 completed (loss: 0.06230118125677109, acc: 0.9735099077224731)
[2025-02-13 20:54:52,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:52,375][root][INFO] - Training Epoch: 2/2, step 4362/7134 completed (loss: 0.04351291432976723, acc: 0.9945651888847351)
[2025-02-13 20:54:52,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:52,749][root][INFO] - Training Epoch: 2/2, step 4363/7134 completed (loss: 0.07246359437704086, acc: 0.9772727489471436)
[2025-02-13 20:54:52,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:53,112][root][INFO] - Training Epoch: 2/2, step 4364/7134 completed (loss: 0.043974559754133224, acc: 0.9922480583190918)
[2025-02-13 20:54:53,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:53,481][root][INFO] - Training Epoch: 2/2, step 4365/7134 completed (loss: 0.04762798175215721, acc: 0.9856114983558655)
[2025-02-13 20:54:53,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:53,862][root][INFO] - Training Epoch: 2/2, step 4366/7134 completed (loss: 0.07152307033538818, acc: 0.9767441749572754)
[2025-02-13 20:54:53,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:54,216][root][INFO] - Training Epoch: 2/2, step 4367/7134 completed (loss: 0.04352908954024315, acc: 1.0)
[2025-02-13 20:54:54,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:54,598][root][INFO] - Training Epoch: 2/2, step 4368/7134 completed (loss: 0.06265008449554443, acc: 0.9820359349250793)
[2025-02-13 20:54:54,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:54,987][root][INFO] - Training Epoch: 2/2, step 4369/7134 completed (loss: 0.04519375041127205, acc: 0.9918699264526367)
[2025-02-13 20:54:55,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:55,361][root][INFO] - Training Epoch: 2/2, step 4370/7134 completed (loss: 0.13768716156482697, acc: 0.9883720874786377)
[2025-02-13 20:54:55,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:55,757][root][INFO] - Training Epoch: 2/2, step 4371/7134 completed (loss: 0.06758255511522293, acc: 0.9878048896789551)
[2025-02-13 20:54:55,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:56,113][root][INFO] - Training Epoch: 2/2, step 4372/7134 completed (loss: 0.06294232606887817, acc: 0.9836065769195557)
[2025-02-13 20:54:56,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:56,468][root][INFO] - Training Epoch: 2/2, step 4373/7134 completed (loss: 0.07258854061365128, acc: 0.9834254384040833)
[2025-02-13 20:54:56,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:56,835][root][INFO] - Training Epoch: 2/2, step 4374/7134 completed (loss: 0.08884937316179276, acc: 0.9695122241973877)
[2025-02-13 20:54:56,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:57,197][root][INFO] - Training Epoch: 2/2, step 4375/7134 completed (loss: 0.06826051324605942, acc: 0.976190447807312)
[2025-02-13 20:54:57,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:57,557][root][INFO] - Training Epoch: 2/2, step 4376/7134 completed (loss: 0.05342048406600952, acc: 0.9895833134651184)
[2025-02-13 20:54:57,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:57,924][root][INFO] - Training Epoch: 2/2, step 4377/7134 completed (loss: 0.1833627074956894, acc: 0.970588207244873)
[2025-02-13 20:54:58,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:58,277][root][INFO] - Training Epoch: 2/2, step 4378/7134 completed (loss: 0.11126916855573654, acc: 0.9774436354637146)
[2025-02-13 20:54:58,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:58,669][root][INFO] - Training Epoch: 2/2, step 4379/7134 completed (loss: 0.2329150289297104, acc: 0.9485294222831726)
[2025-02-13 20:54:58,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:59,031][root][INFO] - Training Epoch: 2/2, step 4380/7134 completed (loss: 0.08857458084821701, acc: 0.9650349617004395)
[2025-02-13 20:54:59,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:59,405][root][INFO] - Training Epoch: 2/2, step 4381/7134 completed (loss: 0.041363365948200226, acc: 0.9940119981765747)
[2025-02-13 20:54:59,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:59,756][root][INFO] - Training Epoch: 2/2, step 4382/7134 completed (loss: 0.0937112346291542, acc: 0.9806451797485352)
[2025-02-13 20:54:59,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:00,128][root][INFO] - Training Epoch: 2/2, step 4383/7134 completed (loss: 0.13694944977760315, acc: 0.9593908786773682)
[2025-02-13 20:55:00,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:00,486][root][INFO] - Training Epoch: 2/2, step 4384/7134 completed (loss: 0.032911647111177444, acc: 0.9943181872367859)
[2025-02-13 20:55:00,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:00,845][root][INFO] - Training Epoch: 2/2, step 4385/7134 completed (loss: 0.11206857115030289, acc: 0.9740932583808899)
[2025-02-13 20:55:00,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:01,203][root][INFO] - Training Epoch: 2/2, step 4386/7134 completed (loss: 0.08515891432762146, acc: 0.9805825352668762)
[2025-02-13 20:55:01,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:01,584][root][INFO] - Training Epoch: 2/2, step 4387/7134 completed (loss: 0.13862866163253784, acc: 0.9588235020637512)
[2025-02-13 20:55:01,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:01,940][root][INFO] - Training Epoch: 2/2, step 4388/7134 completed (loss: 0.16566644608974457, acc: 0.9675324559211731)
[2025-02-13 20:55:02,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:02,297][root][INFO] - Training Epoch: 2/2, step 4389/7134 completed (loss: 0.14364896714687347, acc: 0.97826087474823)
[2025-02-13 20:55:02,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:02,668][root][INFO] - Training Epoch: 2/2, step 4390/7134 completed (loss: 0.15075035393238068, acc: 0.9754601120948792)
[2025-02-13 20:55:02,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:03,072][root][INFO] - Training Epoch: 2/2, step 4391/7134 completed (loss: 0.13705849647521973, acc: 0.9726027250289917)
[2025-02-13 20:55:03,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:03,457][root][INFO] - Training Epoch: 2/2, step 4392/7134 completed (loss: 0.03411383926868439, acc: 0.9947090148925781)
[2025-02-13 20:55:03,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:03,827][root][INFO] - Training Epoch: 2/2, step 4393/7134 completed (loss: 0.06618525832891464, acc: 0.9866666793823242)
[2025-02-13 20:55:03,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:04,193][root][INFO] - Training Epoch: 2/2, step 4394/7134 completed (loss: 0.0507638193666935, acc: 0.9932885766029358)
[2025-02-13 20:55:04,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:04,561][root][INFO] - Training Epoch: 2/2, step 4395/7134 completed (loss: 0.12104830890893936, acc: 0.9875776171684265)
[2025-02-13 20:55:04,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:04,908][root][INFO] - Training Epoch: 2/2, step 4396/7134 completed (loss: 0.2907572090625763, acc: 0.9153439402580261)
[2025-02-13 20:55:05,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:05,261][root][INFO] - Training Epoch: 2/2, step 4397/7134 completed (loss: 0.42911094427108765, acc: 0.9099099040031433)
[2025-02-13 20:55:05,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:05,612][root][INFO] - Training Epoch: 2/2, step 4398/7134 completed (loss: 0.09765442460775375, acc: 0.9666666388511658)
[2025-02-13 20:55:05,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:06,027][root][INFO] - Training Epoch: 2/2, step 4399/7134 completed (loss: 0.15682917833328247, acc: 0.9791666865348816)
[2025-02-13 20:55:06,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:06,413][root][INFO] - Training Epoch: 2/2, step 4400/7134 completed (loss: 0.06299247592687607, acc: 0.9894737005233765)
[2025-02-13 20:55:06,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:06,797][root][INFO] - Training Epoch: 2/2, step 4401/7134 completed (loss: 0.11433223634958267, acc: 0.9612902998924255)
[2025-02-13 20:55:06,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:07,160][root][INFO] - Training Epoch: 2/2, step 4402/7134 completed (loss: 0.2106415331363678, acc: 0.9506173133850098)
[2025-02-13 20:55:07,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:07,535][root][INFO] - Training Epoch: 2/2, step 4403/7134 completed (loss: 0.2782942056655884, acc: 0.9318181872367859)
[2025-02-13 20:55:07,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:07,936][root][INFO] - Training Epoch: 2/2, step 4404/7134 completed (loss: 0.2481604665517807, acc: 0.9536082744598389)
[2025-02-13 20:55:08,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:08,319][root][INFO] - Training Epoch: 2/2, step 4405/7134 completed (loss: 0.1406828910112381, acc: 0.9653465151786804)
[2025-02-13 20:55:08,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:08,681][root][INFO] - Training Epoch: 2/2, step 4406/7134 completed (loss: 0.05898720771074295, acc: 0.9853658676147461)
[2025-02-13 20:55:08,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:09,029][root][INFO] - Training Epoch: 2/2, step 4407/7134 completed (loss: 0.14754296839237213, acc: 0.9693877696990967)
[2025-02-13 20:55:09,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:09,395][root][INFO] - Training Epoch: 2/2, step 4408/7134 completed (loss: 0.1659490466117859, acc: 0.9615384340286255)
[2025-02-13 20:55:09,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:09,789][root][INFO] - Training Epoch: 2/2, step 4409/7134 completed (loss: 0.10912901163101196, acc: 0.9635416865348816)
[2025-02-13 20:55:09,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:10,168][root][INFO] - Training Epoch: 2/2, step 4410/7134 completed (loss: 0.21795989573001862, acc: 0.9440559148788452)
[2025-02-13 20:55:10,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:10,536][root][INFO] - Training Epoch: 2/2, step 4411/7134 completed (loss: 0.22401635348796844, acc: 0.9569892287254333)
[2025-02-13 20:55:10,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:10,945][root][INFO] - Training Epoch: 2/2, step 4412/7134 completed (loss: 0.5456967353820801, acc: 0.8592965006828308)
[2025-02-13 20:55:11,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:11,314][root][INFO] - Training Epoch: 2/2, step 4413/7134 completed (loss: 0.2919781804084778, acc: 0.9274611473083496)
[2025-02-13 20:55:11,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:11,714][root][INFO] - Training Epoch: 2/2, step 4414/7134 completed (loss: 0.0874362364411354, acc: 0.9731183052062988)
[2025-02-13 20:55:11,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:12,079][root][INFO] - Training Epoch: 2/2, step 4415/7134 completed (loss: 0.12043449282646179, acc: 0.9606741666793823)
[2025-02-13 20:55:12,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:12,447][root][INFO] - Training Epoch: 2/2, step 4416/7134 completed (loss: 0.1332131177186966, acc: 0.9585492014884949)
[2025-02-13 20:55:12,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:12,806][root][INFO] - Training Epoch: 2/2, step 4417/7134 completed (loss: 0.2225763350725174, acc: 0.9494949579238892)
[2025-02-13 20:55:12,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:13,171][root][INFO] - Training Epoch: 2/2, step 4418/7134 completed (loss: 0.07392291724681854, acc: 0.9945651888847351)
[2025-02-13 20:55:13,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:13,528][root][INFO] - Training Epoch: 2/2, step 4419/7134 completed (loss: 0.22066271305084229, acc: 0.9463414549827576)
[2025-02-13 20:55:13,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:13,916][root][INFO] - Training Epoch: 2/2, step 4420/7134 completed (loss: 0.2310706377029419, acc: 0.9481865167617798)
[2025-02-13 20:55:14,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:14,277][root][INFO] - Training Epoch: 2/2, step 4421/7134 completed (loss: 0.1583152413368225, acc: 0.9541284441947937)
[2025-02-13 20:55:14,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:14,672][root][INFO] - Training Epoch: 2/2, step 4422/7134 completed (loss: 0.13911861181259155, acc: 0.946107804775238)
[2025-02-13 20:55:14,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:15,036][root][INFO] - Training Epoch: 2/2, step 4423/7134 completed (loss: 0.13477368652820587, acc: 0.9598214030265808)
[2025-02-13 20:55:15,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:15,413][root][INFO] - Training Epoch: 2/2, step 4424/7134 completed (loss: 0.09022720158100128, acc: 0.9829545617103577)
[2025-02-13 20:55:15,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:15,799][root][INFO] - Training Epoch: 2/2, step 4425/7134 completed (loss: 0.05466796085238457, acc: 0.9801980257034302)
[2025-02-13 20:55:15,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:16,197][root][INFO] - Training Epoch: 2/2, step 4426/7134 completed (loss: 0.14673683047294617, acc: 0.9679999947547913)
[2025-02-13 20:55:16,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:16,583][root][INFO] - Training Epoch: 2/2, step 4427/7134 completed (loss: 0.24549788236618042, acc: 0.9741379022598267)
[2025-02-13 20:55:16,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:16,988][root][INFO] - Training Epoch: 2/2, step 4428/7134 completed (loss: 0.10742906481027603, acc: 0.9768785834312439)
[2025-02-13 20:55:17,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:17,328][root][INFO] - Training Epoch: 2/2, step 4429/7134 completed (loss: 0.10759959369897842, acc: 0.9714285731315613)
[2025-02-13 20:55:17,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:17,721][root][INFO] - Training Epoch: 2/2, step 4430/7134 completed (loss: 0.10880834609270096, acc: 0.949438214302063)
[2025-02-13 20:55:17,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:18,083][root][INFO] - Training Epoch: 2/2, step 4431/7134 completed (loss: 0.14625369012355804, acc: 0.9709302186965942)
[2025-02-13 20:55:18,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:18,438][root][INFO] - Training Epoch: 2/2, step 4432/7134 completed (loss: 0.10431249439716339, acc: 0.9808917045593262)
[2025-02-13 20:55:18,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:18,871][root][INFO] - Training Epoch: 2/2, step 4433/7134 completed (loss: 0.0794324204325676, acc: 0.976190447807312)
[2025-02-13 20:55:19,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:19,251][root][INFO] - Training Epoch: 2/2, step 4434/7134 completed (loss: 0.12954677641391754, acc: 0.9714285731315613)
[2025-02-13 20:55:19,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:19,612][root][INFO] - Training Epoch: 2/2, step 4435/7134 completed (loss: 0.0756305605173111, acc: 0.9861111044883728)
[2025-02-13 20:55:19,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:19,982][root][INFO] - Training Epoch: 2/2, step 4436/7134 completed (loss: 0.07500969618558884, acc: 0.976331353187561)
[2025-02-13 20:55:20,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:20,344][root][INFO] - Training Epoch: 2/2, step 4437/7134 completed (loss: 0.028859378769993782, acc: 0.994535505771637)
[2025-02-13 20:55:20,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:20,704][root][INFO] - Training Epoch: 2/2, step 4438/7134 completed (loss: 0.17144536972045898, acc: 0.9850746393203735)
[2025-02-13 20:55:20,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:21,071][root][INFO] - Training Epoch: 2/2, step 4439/7134 completed (loss: 0.023614535108208656, acc: 0.9929078221321106)
[2025-02-13 20:55:21,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:21,418][root][INFO] - Training Epoch: 2/2, step 4440/7134 completed (loss: 0.05585518106818199, acc: 0.9879518151283264)
[2025-02-13 20:55:21,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:21,804][root][INFO] - Training Epoch: 2/2, step 4441/7134 completed (loss: 0.06030058115720749, acc: 0.9874213933944702)
[2025-02-13 20:55:21,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:22,196][root][INFO] - Training Epoch: 2/2, step 4442/7134 completed (loss: 0.12025514990091324, acc: 0.9719101190567017)
[2025-02-13 20:55:22,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:22,596][root][INFO] - Training Epoch: 2/2, step 4443/7134 completed (loss: 0.14249353110790253, acc: 0.9647058844566345)
[2025-02-13 20:55:22,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:22,984][root][INFO] - Training Epoch: 2/2, step 4444/7134 completed (loss: 0.061409804970026016, acc: 0.9927007555961609)
[2025-02-13 20:55:23,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:23,331][root][INFO] - Training Epoch: 2/2, step 4445/7134 completed (loss: 0.10067874193191528, acc: 0.9704142212867737)
[2025-02-13 20:55:23,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:23,696][root][INFO] - Training Epoch: 2/2, step 4446/7134 completed (loss: 0.03653879836201668, acc: 1.0)
[2025-02-13 20:55:23,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:24,058][root][INFO] - Training Epoch: 2/2, step 4447/7134 completed (loss: 0.06125589460134506, acc: 0.9885057210922241)
[2025-02-13 20:55:24,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:24,414][root][INFO] - Training Epoch: 2/2, step 4448/7134 completed (loss: 0.17059317231178284, acc: 0.9689922332763672)
[2025-02-13 20:55:24,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:24,762][root][INFO] - Training Epoch: 2/2, step 4449/7134 completed (loss: 0.1462547481060028, acc: 0.9576271176338196)
[2025-02-13 20:55:24,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:25,162][root][INFO] - Training Epoch: 2/2, step 4450/7134 completed (loss: 0.1105273962020874, acc: 0.9680851101875305)
[2025-02-13 20:55:25,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:25,517][root][INFO] - Training Epoch: 2/2, step 4451/7134 completed (loss: 0.25872743129730225, acc: 0.9490445852279663)
[2025-02-13 20:55:25,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:25,879][root][INFO] - Training Epoch: 2/2, step 4452/7134 completed (loss: 0.08932391554117203, acc: 0.970370352268219)
[2025-02-13 20:55:26,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26,236][root][INFO] - Training Epoch: 2/2, step 4453/7134 completed (loss: 0.045281946659088135, acc: 0.9869281053543091)
[2025-02-13 20:55:26,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26,622][root][INFO] - Training Epoch: 2/2, step 4454/7134 completed (loss: 0.17751552164554596, acc: 0.9751552939414978)
[2025-02-13 20:55:26,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26,991][root][INFO] - Training Epoch: 2/2, step 4455/7134 completed (loss: 0.2277563065290451, acc: 0.9722222089767456)
[2025-02-13 20:55:27,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:27,350][root][INFO] - Training Epoch: 2/2, step 4456/7134 completed (loss: 0.03808106109499931, acc: 0.9818181991577148)
[2025-02-13 20:55:27,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:27,689][root][INFO] - Training Epoch: 2/2, step 4457/7134 completed (loss: 0.02030343748629093, acc: 1.0)
[2025-02-13 20:55:27,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:28,050][root][INFO] - Training Epoch: 2/2, step 4458/7134 completed (loss: 0.016262777149677277, acc: 0.9930070042610168)
[2025-02-13 20:55:28,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:28,407][root][INFO] - Training Epoch: 2/2, step 4459/7134 completed (loss: 0.020909393206238747, acc: 1.0)
[2025-02-13 20:55:28,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:28,777][root][INFO] - Training Epoch: 2/2, step 4460/7134 completed (loss: 0.03633309528231621, acc: 0.9901477694511414)
[2025-02-13 20:55:28,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:29,137][root][INFO] - Training Epoch: 2/2, step 4461/7134 completed (loss: 0.03122956119477749, acc: 0.9895833134651184)
[2025-02-13 20:55:29,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:29,506][root][INFO] - Training Epoch: 2/2, step 4462/7134 completed (loss: 0.04045115038752556, acc: 0.9885714054107666)
[2025-02-13 20:55:29,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:29,879][root][INFO] - Training Epoch: 2/2, step 4463/7134 completed (loss: 0.03679239749908447, acc: 0.9894179701805115)
[2025-02-13 20:55:30,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:30,281][root][INFO] - Training Epoch: 2/2, step 4464/7134 completed (loss: 0.07351798564195633, acc: 0.9931507110595703)
[2025-02-13 20:55:30,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:30,660][root][INFO] - Training Epoch: 2/2, step 4465/7134 completed (loss: 0.07090926170349121, acc: 0.9898989796638489)
[2025-02-13 20:55:30,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:31,034][root][INFO] - Training Epoch: 2/2, step 4466/7134 completed (loss: 0.043481893837451935, acc: 0.9898989796638489)
[2025-02-13 20:55:31,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:31,409][root][INFO] - Training Epoch: 2/2, step 4467/7134 completed (loss: 0.0385950431227684, acc: 0.9893048405647278)
[2025-02-13 20:55:31,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:31,769][root][INFO] - Training Epoch: 2/2, step 4468/7134 completed (loss: 0.026164166629314423, acc: 0.9893048405647278)
[2025-02-13 20:55:31,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:32,142][root][INFO] - Training Epoch: 2/2, step 4469/7134 completed (loss: 0.010558301582932472, acc: 1.0)
[2025-02-13 20:55:32,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:32,521][root][INFO] - Training Epoch: 2/2, step 4470/7134 completed (loss: 0.04894425347447395, acc: 0.9855769276618958)
[2025-02-13 20:55:32,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:32,904][root][INFO] - Training Epoch: 2/2, step 4471/7134 completed (loss: 0.019555067643523216, acc: 0.9949495196342468)
[2025-02-13 20:55:33,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33,257][root][INFO] - Training Epoch: 2/2, step 4472/7134 completed (loss: 0.024249065667390823, acc: 1.0)
[2025-02-13 20:55:33,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33,615][root][INFO] - Training Epoch: 2/2, step 4473/7134 completed (loss: 0.0927039384841919, acc: 0.976331353187561)
[2025-02-13 20:55:33,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33,973][root][INFO] - Training Epoch: 2/2, step 4474/7134 completed (loss: 0.11844243854284286, acc: 0.9751552939414978)
[2025-02-13 20:55:34,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:34,335][root][INFO] - Training Epoch: 2/2, step 4475/7134 completed (loss: 0.20027682185173035, acc: 0.9521276354789734)
[2025-02-13 20:55:34,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:34,756][root][INFO] - Training Epoch: 2/2, step 4476/7134 completed (loss: 0.10702545195817947, acc: 0.9725274443626404)
[2025-02-13 20:55:34,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:35,199][root][INFO] - Training Epoch: 2/2, step 4477/7134 completed (loss: 0.0290824044495821, acc: 0.9897959232330322)
[2025-02-13 20:55:35,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:35,585][root][INFO] - Training Epoch: 2/2, step 4478/7134 completed (loss: 0.06643371284008026, acc: 0.9842932224273682)
[2025-02-13 20:55:35,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:35,938][root][INFO] - Training Epoch: 2/2, step 4479/7134 completed (loss: 0.029815662652254105, acc: 0.9944444298744202)
[2025-02-13 20:55:36,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:36,310][root][INFO] - Training Epoch: 2/2, step 4480/7134 completed (loss: 0.08142482489347458, acc: 0.9752475023269653)
[2025-02-13 20:55:36,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:36,683][root][INFO] - Training Epoch: 2/2, step 4481/7134 completed (loss: 0.05777052789926529, acc: 0.9900000095367432)
[2025-02-13 20:55:36,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:37,056][root][INFO] - Training Epoch: 2/2, step 4482/7134 completed (loss: 0.08603180199861526, acc: 0.9851484894752502)
[2025-02-13 20:55:37,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:37,404][root][INFO] - Training Epoch: 2/2, step 4483/7134 completed (loss: 0.10337293148040771, acc: 0.95652174949646)
[2025-02-13 20:55:37,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:37,778][root][INFO] - Training Epoch: 2/2, step 4484/7134 completed (loss: 0.032802075147628784, acc: 0.9886363744735718)
[2025-02-13 20:55:37,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:38,132][root][INFO] - Training Epoch: 2/2, step 4485/7134 completed (loss: 0.02561352215707302, acc: 0.9934640526771545)
[2025-02-13 20:55:38,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:38,489][root][INFO] - Training Epoch: 2/2, step 4486/7134 completed (loss: 0.029460405930876732, acc: 0.9875776171684265)
[2025-02-13 20:55:38,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:38,855][root][INFO] - Training Epoch: 2/2, step 4487/7134 completed (loss: 0.05318234860897064, acc: 0.987730085849762)
[2025-02-13 20:55:38,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:39,190][root][INFO] - Training Epoch: 2/2, step 4488/7134 completed (loss: 0.04616670310497284, acc: 0.9922480583190918)
[2025-02-13 20:55:39,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:39,563][root][INFO] - Training Epoch: 2/2, step 4489/7134 completed (loss: 0.04712677747011185, acc: 0.9862068891525269)
[2025-02-13 20:55:39,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:39,939][root][INFO] - Training Epoch: 2/2, step 4490/7134 completed (loss: 0.041850000619888306, acc: 0.9919999837875366)
[2025-02-13 20:55:40,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:40,303][root][INFO] - Training Epoch: 2/2, step 4491/7134 completed (loss: 0.04143881797790527, acc: 0.9847328066825867)
[2025-02-13 20:55:40,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:40,658][root][INFO] - Training Epoch: 2/2, step 4492/7134 completed (loss: 0.04334735497832298, acc: 0.9924812316894531)
[2025-02-13 20:55:40,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:41,012][root][INFO] - Training Epoch: 2/2, step 4493/7134 completed (loss: 0.043008267879486084, acc: 0.9941860437393188)
[2025-02-13 20:55:41,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:41,382][root][INFO] - Training Epoch: 2/2, step 4494/7134 completed (loss: 0.04208683595061302, acc: 0.9887640476226807)
[2025-02-13 20:55:41,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:41,773][root][INFO] - Training Epoch: 2/2, step 4495/7134 completed (loss: 0.02101931907236576, acc: 1.0)
[2025-02-13 20:55:41,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:42,139][root][INFO] - Training Epoch: 2/2, step 4496/7134 completed (loss: 0.055256061255931854, acc: 0.988304078578949)
[2025-02-13 20:55:42,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:42,517][root][INFO] - Training Epoch: 2/2, step 4497/7134 completed (loss: 0.020448878407478333, acc: 0.9947090148925781)
[2025-02-13 20:55:42,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:42,890][root][INFO] - Training Epoch: 2/2, step 4498/7134 completed (loss: 0.012013142928481102, acc: 1.0)
[2025-02-13 20:55:43,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:43,216][root][INFO] - Training Epoch: 2/2, step 4499/7134 completed (loss: 0.02700822800397873, acc: 0.9929078221321106)
[2025-02-13 20:55:43,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:43,587][root][INFO] - Training Epoch: 2/2, step 4500/7134 completed (loss: 0.029231080785393715, acc: 0.9839572310447693)
[2025-02-13 20:55:43,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:43,950][root][INFO] - Training Epoch: 2/2, step 4501/7134 completed (loss: 0.021053096279501915, acc: 0.988950252532959)
[2025-02-13 20:55:44,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:44,306][root][INFO] - Training Epoch: 2/2, step 4502/7134 completed (loss: 0.014228826388716698, acc: 1.0)
[2025-02-13 20:55:44,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:44,648][root][INFO] - Training Epoch: 2/2, step 4503/7134 completed (loss: 0.04624496027827263, acc: 0.9937106966972351)
[2025-02-13 20:55:44,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:45,023][root][INFO] - Training Epoch: 2/2, step 4504/7134 completed (loss: 0.056297942996025085, acc: 1.0)
[2025-02-13 20:55:45,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:45,426][root][INFO] - Training Epoch: 2/2, step 4505/7134 completed (loss: 0.08165215700864792, acc: 0.9729729890823364)
[2025-02-13 20:55:45,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:45,820][root][INFO] - Training Epoch: 2/2, step 4506/7134 completed (loss: 0.1016717329621315, acc: 0.9570552110671997)
[2025-02-13 20:55:45,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:46,207][root][INFO] - Training Epoch: 2/2, step 4507/7134 completed (loss: 0.019806325435638428, acc: 1.0)
[2025-02-13 20:55:46,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:46,578][root][INFO] - Training Epoch: 2/2, step 4508/7134 completed (loss: 0.09766155481338501, acc: 0.9627329111099243)
[2025-02-13 20:55:46,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:46,949][root][INFO] - Training Epoch: 2/2, step 4509/7134 completed (loss: 0.052981872111558914, acc: 0.9878787994384766)
[2025-02-13 20:55:47,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:47,319][root][INFO] - Training Epoch: 2/2, step 4510/7134 completed (loss: 0.11051753908395767, acc: 0.976190447807312)
[2025-02-13 20:55:47,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:47,693][root][INFO] - Training Epoch: 2/2, step 4511/7134 completed (loss: 0.06152402237057686, acc: 0.9886363744735718)
[2025-02-13 20:55:47,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:48,030][root][INFO] - Training Epoch: 2/2, step 4512/7134 completed (loss: 0.09081219881772995, acc: 0.9777777791023254)
[2025-02-13 20:55:48,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:48,392][root][INFO] - Training Epoch: 2/2, step 4513/7134 completed (loss: 0.09003590047359467, acc: 0.9824561476707458)
[2025-02-13 20:55:48,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:48,770][root][INFO] - Training Epoch: 2/2, step 4514/7134 completed (loss: 0.310482919216156, acc: 0.9479768872261047)
[2025-02-13 20:55:48,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:49,131][root][INFO] - Training Epoch: 2/2, step 4515/7134 completed (loss: 0.062484730035066605, acc: 0.9868420958518982)
[2025-02-13 20:55:49,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:49,481][root][INFO] - Training Epoch: 2/2, step 4516/7134 completed (loss: 0.10455048829317093, acc: 0.9679144620895386)
[2025-02-13 20:55:49,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:49,836][root][INFO] - Training Epoch: 2/2, step 4517/7134 completed (loss: 0.06404908746480942, acc: 0.9750000238418579)
[2025-02-13 20:55:49,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:50,205][root][INFO] - Training Epoch: 2/2, step 4518/7134 completed (loss: 0.08846055716276169, acc: 0.9793814420700073)
[2025-02-13 20:55:50,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:50,568][root][INFO] - Training Epoch: 2/2, step 4519/7134 completed (loss: 0.030393190681934357, acc: 1.0)
[2025-02-13 20:55:50,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:50,935][root][INFO] - Training Epoch: 2/2, step 4520/7134 completed (loss: 0.04666024446487427, acc: 0.9893048405647278)
[2025-02-13 20:55:51,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:51,308][root][INFO] - Training Epoch: 2/2, step 4521/7134 completed (loss: 0.10851075500249863, acc: 0.9850000143051147)
[2025-02-13 20:55:51,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:51,674][root][INFO] - Training Epoch: 2/2, step 4522/7134 completed (loss: 0.10662325471639633, acc: 0.971563994884491)
[2025-02-13 20:55:51,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:52,030][root][INFO] - Training Epoch: 2/2, step 4523/7134 completed (loss: 0.04356594383716583, acc: 0.9765258431434631)
[2025-02-13 20:55:52,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:52,396][root][INFO] - Training Epoch: 2/2, step 4524/7134 completed (loss: 0.0703258365392685, acc: 0.9842932224273682)
[2025-02-13 20:55:52,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:52,752][root][INFO] - Training Epoch: 2/2, step 4525/7134 completed (loss: 0.10199929028749466, acc: 0.9704433679580688)
[2025-02-13 20:55:52,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:53,106][root][INFO] - Training Epoch: 2/2, step 4526/7134 completed (loss: 0.11405697464942932, acc: 0.9725274443626404)
[2025-02-13 20:55:53,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:53,475][root][INFO] - Training Epoch: 2/2, step 4527/7134 completed (loss: 0.09487109631299973, acc: 0.9820627570152283)
[2025-02-13 20:55:53,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:53,850][root][INFO] - Training Epoch: 2/2, step 4528/7134 completed (loss: 0.07456564158201218, acc: 0.9901477694511414)
[2025-02-13 20:55:53,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:54,222][root][INFO] - Training Epoch: 2/2, step 4529/7134 completed (loss: 0.10972629487514496, acc: 0.9685863852500916)
[2025-02-13 20:55:54,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:54,587][root][INFO] - Training Epoch: 2/2, step 4530/7134 completed (loss: 0.032742612063884735, acc: 0.9939024448394775)
[2025-02-13 20:55:54,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:54,945][root][INFO] - Training Epoch: 2/2, step 4531/7134 completed (loss: 0.01128895953297615, acc: 1.0)
[2025-02-13 20:55:55,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:55,307][root][INFO] - Training Epoch: 2/2, step 4532/7134 completed (loss: 0.08645501732826233, acc: 0.9831223487854004)
[2025-02-13 20:55:55,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:55,670][root][INFO] - Training Epoch: 2/2, step 4533/7134 completed (loss: 0.022635813802480698, acc: 0.9953703880310059)
[2025-02-13 20:55:55,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:56,028][root][INFO] - Training Epoch: 2/2, step 4534/7134 completed (loss: 0.04984346777200699, acc: 0.9796954393386841)
[2025-02-13 20:55:56,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:56,378][root][INFO] - Training Epoch: 2/2, step 4535/7134 completed (loss: 0.04077664762735367, acc: 0.9903846383094788)
[2025-02-13 20:55:56,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:56,757][root][INFO] - Training Epoch: 2/2, step 4536/7134 completed (loss: 0.036459267139434814, acc: 0.9879518151283264)
[2025-02-13 20:55:56,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:57,124][root][INFO] - Training Epoch: 2/2, step 4537/7134 completed (loss: 0.029355430975556374, acc: 0.9909502267837524)
[2025-02-13 20:55:57,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:57,488][root][INFO] - Training Epoch: 2/2, step 4538/7134 completed (loss: 0.05603174492716789, acc: 0.9951691031455994)
[2025-02-13 20:55:57,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:57,886][root][INFO] - Training Epoch: 2/2, step 4539/7134 completed (loss: 0.04837484657764435, acc: 0.9906103014945984)
[2025-02-13 20:55:58,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:58,245][root][INFO] - Training Epoch: 2/2, step 4540/7134 completed (loss: 0.15419334173202515, acc: 0.9642857313156128)
[2025-02-13 20:55:58,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:58,601][root][INFO] - Training Epoch: 2/2, step 4541/7134 completed (loss: 0.117411307990551, acc: 0.9672130942344666)
[2025-02-13 20:55:58,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:58,981][root][INFO] - Training Epoch: 2/2, step 4542/7134 completed (loss: 0.12484748661518097, acc: 0.9561403393745422)
[2025-02-13 20:55:59,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:59,346][root][INFO] - Training Epoch: 2/2, step 4543/7134 completed (loss: 0.247598797082901, acc: 0.9285714030265808)
[2025-02-13 20:55:59,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:59,723][root][INFO] - Training Epoch: 2/2, step 4544/7134 completed (loss: 0.08116801083087921, acc: 0.9797297120094299)
[2025-02-13 20:55:59,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:00,085][root][INFO] - Training Epoch: 2/2, step 4545/7134 completed (loss: 0.047833915799856186, acc: 0.9814814925193787)
[2025-02-13 20:56:00,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:00,445][root][INFO] - Training Epoch: 2/2, step 4546/7134 completed (loss: 0.09095863252878189, acc: 0.9763779640197754)
[2025-02-13 20:56:00,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:00,816][root][INFO] - Training Epoch: 2/2, step 4547/7134 completed (loss: 0.057827915996313095, acc: 0.9830508232116699)
[2025-02-13 20:56:00,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:01,209][root][INFO] - Training Epoch: 2/2, step 4548/7134 completed (loss: 0.12009383738040924, acc: 0.9763779640197754)
[2025-02-13 20:56:01,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:01,581][root][INFO] - Training Epoch: 2/2, step 4549/7134 completed (loss: 0.12342758476734161, acc: 0.9736841917037964)
[2025-02-13 20:56:01,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:01,974][root][INFO] - Training Epoch: 2/2, step 4550/7134 completed (loss: 0.12105884402990341, acc: 0.969924807548523)
[2025-02-13 20:56:02,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:02,338][root][INFO] - Training Epoch: 2/2, step 4551/7134 completed (loss: 0.15860876441001892, acc: 0.9729729890823364)
[2025-02-13 20:56:02,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:02,695][root][INFO] - Training Epoch: 2/2, step 4552/7134 completed (loss: 0.04080259054899216, acc: 0.9824561476707458)
[2025-02-13 20:56:02,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:03,034][root][INFO] - Training Epoch: 2/2, step 4553/7134 completed (loss: 0.01902262307703495, acc: 1.0)
[2025-02-13 20:56:03,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:03,382][root][INFO] - Training Epoch: 2/2, step 4554/7134 completed (loss: 0.08668438345193863, acc: 0.9781022071838379)
[2025-02-13 20:56:03,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:03,748][root][INFO] - Training Epoch: 2/2, step 4555/7134 completed (loss: 0.0781613141298294, acc: 1.0)
[2025-02-13 20:56:03,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:04,131][root][INFO] - Training Epoch: 2/2, step 4556/7134 completed (loss: 0.21073180437088013, acc: 0.9354838728904724)
[2025-02-13 20:56:04,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:04,517][root][INFO] - Training Epoch: 2/2, step 4557/7134 completed (loss: 0.02818177454173565, acc: 1.0)
[2025-02-13 20:56:04,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:04,876][root][INFO] - Training Epoch: 2/2, step 4558/7134 completed (loss: 0.16554158926010132, acc: 0.961240291595459)
[2025-02-13 20:56:05,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:05,240][root][INFO] - Training Epoch: 2/2, step 4559/7134 completed (loss: 0.10760747641324997, acc: 0.9621211886405945)
[2025-02-13 20:56:05,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:05,607][root][INFO] - Training Epoch: 2/2, step 4560/7134 completed (loss: 0.08041241019964218, acc: 0.975806474685669)
[2025-02-13 20:56:05,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:05,969][root][INFO] - Training Epoch: 2/2, step 4561/7134 completed (loss: 0.053025808185338974, acc: 0.9918032884597778)
[2025-02-13 20:56:06,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:06,326][root][INFO] - Training Epoch: 2/2, step 4562/7134 completed (loss: 0.08965206891298294, acc: 0.9918699264526367)
[2025-02-13 20:56:06,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:06,701][root][INFO] - Training Epoch: 2/2, step 4563/7134 completed (loss: 0.045057814568281174, acc: 0.9929078221321106)
[2025-02-13 20:56:06,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:07,070][root][INFO] - Training Epoch: 2/2, step 4564/7134 completed (loss: 0.18224893510341644, acc: 0.948051929473877)
[2025-02-13 20:56:07,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:07,427][root][INFO] - Training Epoch: 2/2, step 4565/7134 completed (loss: 0.05692121386528015, acc: 0.991150438785553)
[2025-02-13 20:56:07,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:07,804][root][INFO] - Training Epoch: 2/2, step 4566/7134 completed (loss: 0.0751100555062294, acc: 0.9774436354637146)
[2025-02-13 20:56:07,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:08,176][root][INFO] - Training Epoch: 2/2, step 4567/7134 completed (loss: 0.041585877537727356, acc: 0.9909909963607788)
[2025-02-13 20:56:08,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:08,542][root][INFO] - Training Epoch: 2/2, step 4568/7134 completed (loss: 0.1141209676861763, acc: 0.9578947424888611)
[2025-02-13 20:56:08,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:08,977][root][INFO] - Training Epoch: 2/2, step 4569/7134 completed (loss: 0.22239701449871063, acc: 0.982300877571106)
[2025-02-13 20:56:09,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:09,339][root][INFO] - Training Epoch: 2/2, step 4570/7134 completed (loss: 0.05455901846289635, acc: 0.9805194735527039)
[2025-02-13 20:56:09,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:09,718][root][INFO] - Training Epoch: 2/2, step 4571/7134 completed (loss: 0.07696554064750671, acc: 0.977011501789093)
[2025-02-13 20:56:09,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:10,106][root][INFO] - Training Epoch: 2/2, step 4572/7134 completed (loss: 0.07111265510320663, acc: 0.9868420958518982)
[2025-02-13 20:56:10,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:10,483][root][INFO] - Training Epoch: 2/2, step 4573/7134 completed (loss: 0.06632055342197418, acc: 0.9807692170143127)
[2025-02-13 20:56:10,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:10,889][root][INFO] - Training Epoch: 2/2, step 4574/7134 completed (loss: 0.029693013057112694, acc: 1.0)
[2025-02-13 20:56:11,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:11,265][root][INFO] - Training Epoch: 2/2, step 4575/7134 completed (loss: 0.08648640662431717, acc: 0.9723756909370422)
[2025-02-13 20:56:11,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:11,638][root][INFO] - Training Epoch: 2/2, step 4576/7134 completed (loss: 0.027818448841571808, acc: 0.9942857027053833)
[2025-02-13 20:56:11,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:12,006][root][INFO] - Training Epoch: 2/2, step 4577/7134 completed (loss: 0.10427788645029068, acc: 0.9651162624359131)
[2025-02-13 20:56:12,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:12,386][root][INFO] - Training Epoch: 2/2, step 4578/7134 completed (loss: 0.047356605529785156, acc: 0.9878787994384766)
[2025-02-13 20:56:12,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:12,776][root][INFO] - Training Epoch: 2/2, step 4579/7134 completed (loss: 0.12289348989725113, acc: 0.9746835231781006)
[2025-02-13 20:56:12,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:13,184][root][INFO] - Training Epoch: 2/2, step 4580/7134 completed (loss: 0.15259188413619995, acc: 0.9746835231781006)
[2025-02-13 20:56:13,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:13,562][root][INFO] - Training Epoch: 2/2, step 4581/7134 completed (loss: 0.02623048610985279, acc: 0.9930070042610168)
[2025-02-13 20:56:13,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:13,932][root][INFO] - Training Epoch: 2/2, step 4582/7134 completed (loss: 0.05345381423830986, acc: 0.9861111044883728)
[2025-02-13 20:56:14,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:14,275][root][INFO] - Training Epoch: 2/2, step 4583/7134 completed (loss: 0.07147568464279175, acc: 0.9800000190734863)
[2025-02-13 20:56:14,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:14,645][root][INFO] - Training Epoch: 2/2, step 4584/7134 completed (loss: 0.054257649928331375, acc: 0.9862068891525269)
[2025-02-13 20:56:14,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:15,014][root][INFO] - Training Epoch: 2/2, step 4585/7134 completed (loss: 0.1066286638379097, acc: 0.9743589758872986)
[2025-02-13 20:56:15,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:15,385][root][INFO] - Training Epoch: 2/2, step 4586/7134 completed (loss: 0.04386717453598976, acc: 0.9881656765937805)
[2025-02-13 20:56:15,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:15,747][root][INFO] - Training Epoch: 2/2, step 4587/7134 completed (loss: 0.21076828241348267, acc: 0.9825581312179565)
[2025-02-13 20:56:15,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:16,125][root][INFO] - Training Epoch: 2/2, step 4588/7134 completed (loss: 0.10262817144393921, acc: 0.9750000238418579)
[2025-02-13 20:56:16,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:16,520][root][INFO] - Training Epoch: 2/2, step 4589/7134 completed (loss: 0.07790663093328476, acc: 0.9712643623352051)
[2025-02-13 20:56:16,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:16,892][root][INFO] - Training Epoch: 2/2, step 4590/7134 completed (loss: 0.11294518411159515, acc: 0.9642857313156128)
[2025-02-13 20:56:17,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:17,271][root][INFO] - Training Epoch: 2/2, step 4591/7134 completed (loss: 0.1248156726360321, acc: 0.9545454382896423)
[2025-02-13 20:56:17,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:17,637][root][INFO] - Training Epoch: 2/2, step 4592/7134 completed (loss: 0.08660997450351715, acc: 0.9756097793579102)
[2025-02-13 20:56:17,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:17,989][root][INFO] - Training Epoch: 2/2, step 4593/7134 completed (loss: 0.12185446918010712, acc: 0.9407894611358643)
[2025-02-13 20:56:18,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:18,365][root][INFO] - Training Epoch: 2/2, step 4594/7134 completed (loss: 0.1345585733652115, acc: 0.9576719403266907)
[2025-02-13 20:56:18,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:18,745][root][INFO] - Training Epoch: 2/2, step 4595/7134 completed (loss: 0.08724461495876312, acc: 0.9873417615890503)
[2025-02-13 20:56:18,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:19,103][root][INFO] - Training Epoch: 2/2, step 4596/7134 completed (loss: 0.05497089773416519, acc: 0.987500011920929)
[2025-02-13 20:56:19,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:19,470][root][INFO] - Training Epoch: 2/2, step 4597/7134 completed (loss: 0.07233772426843643, acc: 0.9743589758872986)
[2025-02-13 20:56:19,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:19,854][root][INFO] - Training Epoch: 2/2, step 4598/7134 completed (loss: 0.15846963226795197, acc: 0.9567567706108093)
[2025-02-13 20:56:19,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:20,226][root][INFO] - Training Epoch: 2/2, step 4599/7134 completed (loss: 0.01779560185968876, acc: 1.0)
[2025-02-13 20:56:20,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:20,569][root][INFO] - Training Epoch: 2/2, step 4600/7134 completed (loss: 0.05795516446232796, acc: 0.9874213933944702)
[2025-02-13 20:56:20,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:20,942][root][INFO] - Training Epoch: 2/2, step 4601/7134 completed (loss: 0.029770350083708763, acc: 0.994350254535675)
[2025-02-13 20:56:21,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:21,300][root][INFO] - Training Epoch: 2/2, step 4602/7134 completed (loss: 0.0881633311510086, acc: 0.9666666388511658)
[2025-02-13 20:56:21,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:21,657][root][INFO] - Training Epoch: 2/2, step 4603/7134 completed (loss: 0.05419767647981644, acc: 0.9825581312179565)
[2025-02-13 20:56:21,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:22,019][root][INFO] - Training Epoch: 2/2, step 4604/7134 completed (loss: 0.03849771246314049, acc: 1.0)
[2025-02-13 20:56:22,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:22,389][root][INFO] - Training Epoch: 2/2, step 4605/7134 completed (loss: 0.09247591346502304, acc: 0.9636363387107849)
[2025-02-13 20:56:22,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:22,778][root][INFO] - Training Epoch: 2/2, step 4606/7134 completed (loss: 0.03725326061248779, acc: 0.9874213933944702)
[2025-02-13 20:56:22,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:23,163][root][INFO] - Training Epoch: 2/2, step 4607/7134 completed (loss: 0.03786073252558708, acc: 0.9929078221321106)
[2025-02-13 20:56:23,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:23,538][root][INFO] - Training Epoch: 2/2, step 4608/7134 completed (loss: 0.07266777753829956, acc: 0.988304078578949)
[2025-02-13 20:56:23,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:23,917][root][INFO] - Training Epoch: 2/2, step 4609/7134 completed (loss: 0.027981378138065338, acc: 0.9923664331436157)
[2025-02-13 20:56:24,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:24,283][root][INFO] - Training Epoch: 2/2, step 4610/7134 completed (loss: 0.03172151371836662, acc: 0.9888268113136292)
[2025-02-13 20:56:24,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:24,653][root][INFO] - Training Epoch: 2/2, step 4611/7134 completed (loss: 0.04006132483482361, acc: 0.9931034445762634)
[2025-02-13 20:56:24,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:25,003][root][INFO] - Training Epoch: 2/2, step 4612/7134 completed (loss: 0.07863101363182068, acc: 0.9851852059364319)
[2025-02-13 20:56:25,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:25,351][root][INFO] - Training Epoch: 2/2, step 4613/7134 completed (loss: 0.05822065845131874, acc: 0.9938271641731262)
[2025-02-13 20:56:25,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:25,677][root][INFO] - Training Epoch: 2/2, step 4614/7134 completed (loss: 0.03540915623307228, acc: 0.9923076629638672)
[2025-02-13 20:56:25,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:26,047][root][INFO] - Training Epoch: 2/2, step 4615/7134 completed (loss: 0.08584380894899368, acc: 0.9880239367485046)
[2025-02-13 20:56:26,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:26,413][root][INFO] - Training Epoch: 2/2, step 4616/7134 completed (loss: 0.09089135378599167, acc: 0.9767441749572754)
[2025-02-13 20:56:26,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:26,761][root][INFO] - Training Epoch: 2/2, step 4617/7134 completed (loss: 0.12144551426172256, acc: 0.9820359349250793)
[2025-02-13 20:56:26,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:27,127][root][INFO] - Training Epoch: 2/2, step 4618/7134 completed (loss: 0.08229187875986099, acc: 0.9689922332763672)
[2025-02-13 20:56:27,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:27,504][root][INFO] - Training Epoch: 2/2, step 4619/7134 completed (loss: 0.11917964369058609, acc: 0.981249988079071)
[2025-02-13 20:56:27,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:27,876][root][INFO] - Training Epoch: 2/2, step 4620/7134 completed (loss: 0.02680938132107258, acc: 0.9941176176071167)
[2025-02-13 20:56:28,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:28,239][root][INFO] - Training Epoch: 2/2, step 4621/7134 completed (loss: 0.031413640826940536, acc: 0.9939024448394775)
[2025-02-13 20:56:28,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:28,601][root][INFO] - Training Epoch: 2/2, step 4622/7134 completed (loss: 0.04112989827990532, acc: 0.9888888597488403)
[2025-02-13 20:56:28,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:28,966][root][INFO] - Training Epoch: 2/2, step 4623/7134 completed (loss: 0.07851184904575348, acc: 0.9882352948188782)
[2025-02-13 20:56:29,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:29,390][root][INFO] - Training Epoch: 2/2, step 4624/7134 completed (loss: 0.16201597452163696, acc: 0.970588207244873)
[2025-02-13 20:56:29,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:29,761][root][INFO] - Training Epoch: 2/2, step 4625/7134 completed (loss: 0.15232035517692566, acc: 0.9509202241897583)
[2025-02-13 20:56:29,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:30,142][root][INFO] - Training Epoch: 2/2, step 4626/7134 completed (loss: 0.1873091608285904, acc: 0.9239130616188049)
[2025-02-13 20:56:30,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:30,496][root][INFO] - Training Epoch: 2/2, step 4627/7134 completed (loss: 0.11023634672164917, acc: 0.9677419066429138)
[2025-02-13 20:56:30,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:30,876][root][INFO] - Training Epoch: 2/2, step 4628/7134 completed (loss: 0.05579259991645813, acc: 0.9878048896789551)
[2025-02-13 20:56:31,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:31,230][root][INFO] - Training Epoch: 2/2, step 4629/7134 completed (loss: 0.0736149474978447, acc: 0.9935064911842346)
[2025-02-13 20:56:31,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:31,606][root][INFO] - Training Epoch: 2/2, step 4630/7134 completed (loss: 0.14046157896518707, acc: 0.9777777791023254)
[2025-02-13 20:56:31,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:31,992][root][INFO] - Training Epoch: 2/2, step 4631/7134 completed (loss: 0.06684231758117676, acc: 0.9906976819038391)
[2025-02-13 20:56:32,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:32,388][root][INFO] - Training Epoch: 2/2, step 4632/7134 completed (loss: 0.1204657182097435, acc: 0.9819276928901672)
[2025-02-13 20:56:32,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:32,767][root][INFO] - Training Epoch: 2/2, step 4633/7134 completed (loss: 0.20433716475963593, acc: 0.9452054500579834)
[2025-02-13 20:56:32,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:33,134][root][INFO] - Training Epoch: 2/2, step 4634/7134 completed (loss: 0.08822046965360641, acc: 0.9777777791023254)
[2025-02-13 20:56:33,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:33,506][root][INFO] - Training Epoch: 2/2, step 4635/7134 completed (loss: 0.1306651085615158, acc: 0.9817073345184326)
[2025-02-13 20:56:33,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:33,828][root][INFO] - Training Epoch: 2/2, step 4636/7134 completed (loss: 0.028463102877140045, acc: 0.9893617033958435)
[2025-02-13 20:56:33,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:34,203][root][INFO] - Training Epoch: 2/2, step 4637/7134 completed (loss: 0.0867641344666481, acc: 0.9841269850730896)
[2025-02-13 20:56:34,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:34,565][root][INFO] - Training Epoch: 2/2, step 4638/7134 completed (loss: 0.10387147217988968, acc: 0.9768785834312439)
[2025-02-13 20:56:34,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:34,921][root][INFO] - Training Epoch: 2/2, step 4639/7134 completed (loss: 0.07508581131696701, acc: 0.9729729890823364)
[2025-02-13 20:56:35,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:35,299][root][INFO] - Training Epoch: 2/2, step 4640/7134 completed (loss: 0.09207111597061157, acc: 0.9682539701461792)
[2025-02-13 20:56:35,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:35,656][root][INFO] - Training Epoch: 2/2, step 4641/7134 completed (loss: 0.05954059585928917, acc: 0.9930555820465088)
[2025-02-13 20:56:35,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:36,011][root][INFO] - Training Epoch: 2/2, step 4642/7134 completed (loss: 0.02675078809261322, acc: 0.994350254535675)
[2025-02-13 20:56:36,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:36,393][root][INFO] - Training Epoch: 2/2, step 4643/7134 completed (loss: 0.03929085284471512, acc: 0.9853658676147461)
[2025-02-13 20:56:36,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:36,781][root][INFO] - Training Epoch: 2/2, step 4644/7134 completed (loss: 0.026110736653208733, acc: 1.0)
[2025-02-13 20:56:36,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:37,150][root][INFO] - Training Epoch: 2/2, step 4645/7134 completed (loss: 0.014496461488306522, acc: 1.0)
[2025-02-13 20:56:37,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:37,507][root][INFO] - Training Epoch: 2/2, step 4646/7134 completed (loss: 0.05532141029834747, acc: 0.9857142567634583)
[2025-02-13 20:56:37,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:37,859][root][INFO] - Training Epoch: 2/2, step 4647/7134 completed (loss: 0.12745973467826843, acc: 0.9672130942344666)
[2025-02-13 20:56:37,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:38,237][root][INFO] - Training Epoch: 2/2, step 4648/7134 completed (loss: 0.12725843489170074, acc: 0.9745222926139832)
[2025-02-13 20:56:38,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:38,620][root][INFO] - Training Epoch: 2/2, step 4649/7134 completed (loss: 0.0640811175107956, acc: 0.9674796462059021)
[2025-02-13 20:56:38,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:38,989][root][INFO] - Training Epoch: 2/2, step 4650/7134 completed (loss: 0.0875338539481163, acc: 0.9729729890823364)
[2025-02-13 20:56:39,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:39,373][root][INFO] - Training Epoch: 2/2, step 4651/7134 completed (loss: 0.1534510850906372, acc: 0.9553072452545166)
[2025-02-13 20:56:39,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:39,795][root][INFO] - Training Epoch: 2/2, step 4652/7134 completed (loss: 0.0793398916721344, acc: 0.9691358208656311)
[2025-02-13 20:56:39,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:40,177][root][INFO] - Training Epoch: 2/2, step 4653/7134 completed (loss: 0.047842271625995636, acc: 0.990338146686554)
[2025-02-13 20:56:40,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:40,523][root][INFO] - Training Epoch: 2/2, step 4654/7134 completed (loss: 0.20369042456150055, acc: 0.9459459185600281)
[2025-02-13 20:56:40,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:40,886][root][INFO] - Training Epoch: 2/2, step 4655/7134 completed (loss: 0.06680949777364731, acc: 0.9828571677207947)
[2025-02-13 20:56:41,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:41,267][root][INFO] - Training Epoch: 2/2, step 4656/7134 completed (loss: 0.06325210630893707, acc: 0.9830508232116699)
[2025-02-13 20:56:41,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:41,618][root][INFO] - Training Epoch: 2/2, step 4657/7134 completed (loss: 0.13329440355300903, acc: 0.954023003578186)
[2025-02-13 20:56:41,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:41,977][root][INFO] - Training Epoch: 2/2, step 4658/7134 completed (loss: 0.060948606580495834, acc: 0.9829545617103577)
[2025-02-13 20:56:42,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:42,352][root][INFO] - Training Epoch: 2/2, step 4659/7134 completed (loss: 0.12326222658157349, acc: 0.9576719403266907)
[2025-02-13 20:56:42,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:42,715][root][INFO] - Training Epoch: 2/2, step 4660/7134 completed (loss: 0.13724516332149506, acc: 0.9743589758872986)
[2025-02-13 20:56:42,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:43,065][root][INFO] - Training Epoch: 2/2, step 4661/7134 completed (loss: 0.10213427245616913, acc: 0.9447852969169617)
[2025-02-13 20:56:43,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:43,419][root][INFO] - Training Epoch: 2/2, step 4662/7134 completed (loss: 0.13150295615196228, acc: 0.978723406791687)
[2025-02-13 20:56:43,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:43,800][root][INFO] - Training Epoch: 2/2, step 4663/7134 completed (loss: 0.20138660073280334, acc: 0.9398906826972961)
[2025-02-13 20:56:43,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:44,183][root][INFO] - Training Epoch: 2/2, step 4664/7134 completed (loss: 0.12634459137916565, acc: 0.9729729890823364)
[2025-02-13 20:56:44,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:44,548][root][INFO] - Training Epoch: 2/2, step 4665/7134 completed (loss: 0.05390070378780365, acc: 0.9890109896659851)
[2025-02-13 20:56:44,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:44,926][root][INFO] - Training Epoch: 2/2, step 4666/7134 completed (loss: 0.19372448325157166, acc: 0.9585492014884949)
[2025-02-13 20:56:45,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:45,288][root][INFO] - Training Epoch: 2/2, step 4667/7134 completed (loss: 0.15830959379673004, acc: 0.9729729890823364)
[2025-02-13 20:56:45,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:45,645][root][INFO] - Training Epoch: 2/2, step 4668/7134 completed (loss: 0.07776346057653427, acc: 0.9865771532058716)
[2025-02-13 20:56:45,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:46,015][root][INFO] - Training Epoch: 2/2, step 4669/7134 completed (loss: 0.08900958299636841, acc: 0.9774011373519897)
[2025-02-13 20:56:46,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:46,374][root][INFO] - Training Epoch: 2/2, step 4670/7134 completed (loss: 0.04264675825834274, acc: 1.0)
[2025-02-13 20:56:46,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:46,742][root][INFO] - Training Epoch: 2/2, step 4671/7134 completed (loss: 0.07986462861299515, acc: 0.9781420826911926)
[2025-02-13 20:56:46,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:47,132][root][INFO] - Training Epoch: 2/2, step 4672/7134 completed (loss: 0.09595593810081482, acc: 0.9724137783050537)
[2025-02-13 20:56:47,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:47,497][root][INFO] - Training Epoch: 2/2, step 4673/7134 completed (loss: 0.10310926288366318, acc: 0.9811320900917053)
[2025-02-13 20:56:47,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:47,850][root][INFO] - Training Epoch: 2/2, step 4674/7134 completed (loss: 0.04283317178487778, acc: 0.9935064911842346)
[2025-02-13 20:56:47,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:48,215][root][INFO] - Training Epoch: 2/2, step 4675/7134 completed (loss: 0.10916357487440109, acc: 0.9743589758872986)
[2025-02-13 20:56:48,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:48,571][root][INFO] - Training Epoch: 2/2, step 4676/7134 completed (loss: 0.03977774828672409, acc: 0.9935064911842346)
[2025-02-13 20:56:48,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:48,936][root][INFO] - Training Epoch: 2/2, step 4677/7134 completed (loss: 0.03763652592897415, acc: 0.9915966391563416)
[2025-02-13 20:56:49,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:49,335][root][INFO] - Training Epoch: 2/2, step 4678/7134 completed (loss: 0.09727992862462997, acc: 0.976047933101654)
[2025-02-13 20:56:49,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:49,710][root][INFO] - Training Epoch: 2/2, step 4679/7134 completed (loss: 0.05002712085843086, acc: 0.9942196607589722)
[2025-02-13 20:56:49,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:50,056][root][INFO] - Training Epoch: 2/2, step 4680/7134 completed (loss: 0.11834602057933807, acc: 0.9636363387107849)
[2025-02-13 20:56:50,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:50,404][root][INFO] - Training Epoch: 2/2, step 4681/7134 completed (loss: 0.16523465514183044, acc: 0.948051929473877)
[2025-02-13 20:56:50,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:50,774][root][INFO] - Training Epoch: 2/2, step 4682/7134 completed (loss: 0.08799109607934952, acc: 0.9668508172035217)
[2025-02-13 20:56:50,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:51,161][root][INFO] - Training Epoch: 2/2, step 4683/7134 completed (loss: 0.0768439993262291, acc: 0.9800000190734863)
[2025-02-13 20:56:51,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:51,539][root][INFO] - Training Epoch: 2/2, step 4684/7134 completed (loss: 0.05738682672381401, acc: 0.9948453903198242)
[2025-02-13 20:56:51,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:51,915][root][INFO] - Training Epoch: 2/2, step 4685/7134 completed (loss: 0.06454779952764511, acc: 0.9852216839790344)
[2025-02-13 20:56:52,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:52,289][root][INFO] - Training Epoch: 2/2, step 4686/7134 completed (loss: 0.05771980062127113, acc: 0.9894179701805115)
[2025-02-13 20:56:52,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:52,617][root][INFO] - Training Epoch: 2/2, step 4687/7134 completed (loss: 0.11162517219781876, acc: 0.95652174949646)
[2025-02-13 20:56:52,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:52,983][root][INFO] - Training Epoch: 2/2, step 4688/7134 completed (loss: 0.4065698981285095, acc: 0.9140625)
[2025-02-13 20:56:53,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:53,357][root][INFO] - Training Epoch: 2/2, step 4689/7134 completed (loss: 0.2075340896844864, acc: 0.959770143032074)
[2025-02-13 20:56:53,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:53,723][root][INFO] - Training Epoch: 2/2, step 4690/7134 completed (loss: 0.2420956939458847, acc: 0.9473684430122375)
[2025-02-13 20:56:53,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:54,104][root][INFO] - Training Epoch: 2/2, step 4691/7134 completed (loss: 0.05823741480708122, acc: 0.9846153855323792)
[2025-02-13 20:56:54,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:54,407][root][INFO] - Training Epoch: 2/2, step 4692/7134 completed (loss: 0.33672037720680237, acc: 0.9365079402923584)
[2025-02-13 20:56:54,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:54,793][root][INFO] - Training Epoch: 2/2, step 4693/7134 completed (loss: 0.2956511080265045, acc: 0.9314285516738892)
[2025-02-13 20:56:54,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:55,209][root][INFO] - Training Epoch: 2/2, step 4694/7134 completed (loss: 0.2652031183242798, acc: 0.9603174328804016)
[2025-02-13 20:56:55,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:55,589][root][INFO] - Training Epoch: 2/2, step 4695/7134 completed (loss: 0.23916083574295044, acc: 0.95333331823349)
[2025-02-13 20:56:55,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:55,950][root][INFO] - Training Epoch: 2/2, step 4696/7134 completed (loss: 0.026384243741631508, acc: 1.0)
[2025-02-13 20:56:56,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:56,291][root][INFO] - Training Epoch: 2/2, step 4697/7134 completed (loss: 0.05296783521771431, acc: 0.9920634627342224)
[2025-02-13 20:56:56,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:56,636][root][INFO] - Training Epoch: 2/2, step 4698/7134 completed (loss: 0.19703678786754608, acc: 0.9571428298950195)
[2025-02-13 20:56:56,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:57,002][root][INFO] - Training Epoch: 2/2, step 4699/7134 completed (loss: 0.14581026136875153, acc: 0.9568345546722412)
[2025-02-13 20:56:57,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:57,366][root][INFO] - Training Epoch: 2/2, step 4700/7134 completed (loss: 0.13163022696971893, acc: 0.9467455744743347)
[2025-02-13 20:56:57,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:57,697][root][INFO] - Training Epoch: 2/2, step 4701/7134 completed (loss: 0.22659388184547424, acc: 0.9375)
[2025-02-13 20:56:57,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:58,074][root][INFO] - Training Epoch: 2/2, step 4702/7134 completed (loss: 0.056682899594306946, acc: 0.9806451797485352)
[2025-02-13 20:56:58,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:58,434][root][INFO] - Training Epoch: 2/2, step 4703/7134 completed (loss: 0.05344183370471001, acc: 0.9882352948188782)
[2025-02-13 20:56:58,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:58,773][root][INFO] - Training Epoch: 2/2, step 4704/7134 completed (loss: 0.023083413019776344, acc: 1.0)
[2025-02-13 20:56:58,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:59,141][root][INFO] - Training Epoch: 2/2, step 4705/7134 completed (loss: 0.11931922286748886, acc: 0.9605911374092102)
[2025-02-13 20:56:59,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:59,509][root][INFO] - Training Epoch: 2/2, step 4706/7134 completed (loss: 0.17772343754768372, acc: 0.9487179517745972)
[2025-02-13 20:56:59,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:59,897][root][INFO] - Training Epoch: 2/2, step 4707/7134 completed (loss: 0.36184951663017273, acc: 0.9119496941566467)
[2025-02-13 20:57:00,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:00,254][root][INFO] - Training Epoch: 2/2, step 4708/7134 completed (loss: 0.0937562957406044, acc: 0.9748743772506714)
[2025-02-13 20:57:00,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:00,604][root][INFO] - Training Epoch: 2/2, step 4709/7134 completed (loss: 0.06200655177235603, acc: 0.9837837815284729)
[2025-02-13 20:57:00,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:00,959][root][INFO] - Training Epoch: 2/2, step 4710/7134 completed (loss: 0.10260496288537979, acc: 0.9757575988769531)
[2025-02-13 20:57:01,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:01,334][root][INFO] - Training Epoch: 2/2, step 4711/7134 completed (loss: 0.14552801847457886, acc: 0.9391891956329346)
[2025-02-13 20:57:01,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:01,683][root][INFO] - Training Epoch: 2/2, step 4712/7134 completed (loss: 0.2579677700996399, acc: 0.9411764740943909)
[2025-02-13 20:57:01,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:02,043][root][INFO] - Training Epoch: 2/2, step 4713/7134 completed (loss: 0.11339662224054337, acc: 0.9701492786407471)
[2025-02-13 20:57:02,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:02,365][root][INFO] - Training Epoch: 2/2, step 4714/7134 completed (loss: 0.10937011986970901, acc: 0.9694656729698181)
[2025-02-13 20:57:02,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:02,721][root][INFO] - Training Epoch: 2/2, step 4715/7134 completed (loss: 0.2942352592945099, acc: 0.9583333134651184)
[2025-02-13 20:57:02,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:03,101][root][INFO] - Training Epoch: 2/2, step 4716/7134 completed (loss: 0.08197397738695145, acc: 0.9870967864990234)
[2025-02-13 20:57:03,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:03,459][root][INFO] - Training Epoch: 2/2, step 4717/7134 completed (loss: 0.0527033656835556, acc: 0.9800000190734863)
[2025-02-13 20:57:03,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:03,822][root][INFO] - Training Epoch: 2/2, step 4718/7134 completed (loss: 0.023530060425400734, acc: 0.9928057789802551)
[2025-02-13 20:57:03,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:04,199][root][INFO] - Training Epoch: 2/2, step 4719/7134 completed (loss: 0.08953091502189636, acc: 0.976331353187561)
[2025-02-13 20:57:04,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:04,546][root][INFO] - Training Epoch: 2/2, step 4720/7134 completed (loss: 0.013704662211239338, acc: 1.0)
[2025-02-13 20:57:04,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:04,926][root][INFO] - Training Epoch: 2/2, step 4721/7134 completed (loss: 0.018173569813370705, acc: 0.9938271641731262)
[2025-02-13 20:57:05,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:05,277][root][INFO] - Training Epoch: 2/2, step 4722/7134 completed (loss: 0.08100169897079468, acc: 0.9748427867889404)
[2025-02-13 20:57:05,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:05,624][root][INFO] - Training Epoch: 2/2, step 4723/7134 completed (loss: 0.08604054152965546, acc: 0.9808917045593262)
[2025-02-13 20:57:05,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:05,998][root][INFO] - Training Epoch: 2/2, step 4724/7134 completed (loss: 0.0744514986872673, acc: 0.9695122241973877)
[2025-02-13 20:57:06,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:06,329][root][INFO] - Training Epoch: 2/2, step 4725/7134 completed (loss: 0.058650389313697815, acc: 0.9825581312179565)
[2025-02-13 20:57:06,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:06,701][root][INFO] - Training Epoch: 2/2, step 4726/7134 completed (loss: 0.13637498021125793, acc: 0.9655172228813171)
[2025-02-13 20:57:06,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:07,053][root][INFO] - Training Epoch: 2/2, step 4727/7134 completed (loss: 0.09101172536611557, acc: 0.9933775067329407)
[2025-02-13 20:57:07,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:07,412][root][INFO] - Training Epoch: 2/2, step 4728/7134 completed (loss: 0.07685211300849915, acc: 0.9801324605941772)
[2025-02-13 20:57:07,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:07,743][root][INFO] - Training Epoch: 2/2, step 4729/7134 completed (loss: 0.04954753816127777, acc: 0.9884393215179443)
[2025-02-13 20:57:07,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08,085][root][INFO] - Training Epoch: 2/2, step 4730/7134 completed (loss: 0.11085145175457001, acc: 0.9707602262496948)
[2025-02-13 20:57:08,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08,437][root][INFO] - Training Epoch: 2/2, step 4731/7134 completed (loss: 0.2166012078523636, acc: 0.9655172228813171)
[2025-02-13 20:57:08,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08,786][root][INFO] - Training Epoch: 2/2, step 4732/7134 completed (loss: 0.09399411827325821, acc: 0.9558823704719543)
[2025-02-13 20:57:08,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:09,176][root][INFO] - Training Epoch: 2/2, step 4733/7134 completed (loss: 0.06895885616540909, acc: 0.97826087474823)
[2025-02-13 20:57:09,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:09,554][root][INFO] - Training Epoch: 2/2, step 4734/7134 completed (loss: 0.19863304495811462, acc: 0.9496855139732361)
[2025-02-13 20:57:09,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:09,929][root][INFO] - Training Epoch: 2/2, step 4735/7134 completed (loss: 0.06790867447853088, acc: 0.9811320900917053)
[2025-02-13 20:57:10,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:10,277][root][INFO] - Training Epoch: 2/2, step 4736/7134 completed (loss: 0.21006140112876892, acc: 0.9666666388511658)
[2025-02-13 20:57:10,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:10,656][root][INFO] - Training Epoch: 2/2, step 4737/7134 completed (loss: 0.08260580152273178, acc: 0.9793103337287903)
[2025-02-13 20:57:10,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:11,057][root][INFO] - Training Epoch: 2/2, step 4738/7134 completed (loss: 0.053942423313856125, acc: 0.981249988079071)
[2025-02-13 20:57:11,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:11,415][root][INFO] - Training Epoch: 2/2, step 4739/7134 completed (loss: 0.24472388625144958, acc: 0.932584285736084)
[2025-02-13 20:57:11,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:11,791][root][INFO] - Training Epoch: 2/2, step 4740/7134 completed (loss: 0.2124156951904297, acc: 0.9444444179534912)
[2025-02-13 20:57:11,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:12,141][root][INFO] - Training Epoch: 2/2, step 4741/7134 completed (loss: 0.09717245399951935, acc: 0.9714285731315613)
[2025-02-13 20:57:12,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:12,511][root][INFO] - Training Epoch: 2/2, step 4742/7134 completed (loss: 0.3253594636917114, acc: 0.9272727370262146)
[2025-02-13 20:57:12,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:12,876][root][INFO] - Training Epoch: 2/2, step 4743/7134 completed (loss: 0.2008446902036667, acc: 0.9342105388641357)
[2025-02-13 20:57:12,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:13,208][root][INFO] - Training Epoch: 2/2, step 4744/7134 completed (loss: 0.07992155104875565, acc: 0.9724770784378052)
[2025-02-13 20:57:13,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:13,533][root][INFO] - Training Epoch: 2/2, step 4745/7134 completed (loss: 0.026740286499261856, acc: 1.0)
[2025-02-13 20:57:13,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:13,886][root][INFO] - Training Epoch: 2/2, step 4746/7134 completed (loss: 0.0492277555167675, acc: 0.9767441749572754)
[2025-02-13 20:57:14,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:14,257][root][INFO] - Training Epoch: 2/2, step 4747/7134 completed (loss: 0.08476876467466354, acc: 0.9651162624359131)
[2025-02-13 20:57:14,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:14,613][root][INFO] - Training Epoch: 2/2, step 4748/7134 completed (loss: 0.17537342011928558, acc: 0.9585798978805542)
[2025-02-13 20:57:14,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:14,977][root][INFO] - Training Epoch: 2/2, step 4749/7134 completed (loss: 0.028568904846906662, acc: 0.9941860437393188)
[2025-02-13 20:57:15,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:15,330][root][INFO] - Training Epoch: 2/2, step 4750/7134 completed (loss: 0.10643857717514038, acc: 0.9756097793579102)
[2025-02-13 20:57:15,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:15,735][root][INFO] - Training Epoch: 2/2, step 4751/7134 completed (loss: 0.049922745674848557, acc: 0.9878048896789551)
[2025-02-13 20:57:15,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:16,108][root][INFO] - Training Epoch: 2/2, step 4752/7134 completed (loss: 0.14052215218544006, acc: 0.9751552939414978)
[2025-02-13 20:57:16,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:16,460][root][INFO] - Training Epoch: 2/2, step 4753/7134 completed (loss: 0.13207639753818512, acc: 0.976331353187561)
[2025-02-13 20:57:16,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:16,818][root][INFO] - Training Epoch: 2/2, step 4754/7134 completed (loss: 0.03276878222823143, acc: 0.9935064911842346)
[2025-02-13 20:57:16,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:17,178][root][INFO] - Training Epoch: 2/2, step 4755/7134 completed (loss: 0.07684043049812317, acc: 0.9780219793319702)
[2025-02-13 20:57:17,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:17,523][root][INFO] - Training Epoch: 2/2, step 4756/7134 completed (loss: 0.03236880525946617, acc: 0.9919354915618896)
[2025-02-13 20:57:17,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:17,897][root][INFO] - Training Epoch: 2/2, step 4757/7134 completed (loss: 0.05554153770208359, acc: 0.9750000238418579)
[2025-02-13 20:57:18,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:18,271][root][INFO] - Training Epoch: 2/2, step 4758/7134 completed (loss: 0.039577145129442215, acc: 0.9944751262664795)
[2025-02-13 20:57:18,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:18,633][root][INFO] - Training Epoch: 2/2, step 4759/7134 completed (loss: 0.03934304416179657, acc: 0.9929078221321106)
[2025-02-13 20:57:18,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:18,983][root][INFO] - Training Epoch: 2/2, step 4760/7134 completed (loss: 0.05512915179133415, acc: 0.9915966391563416)
[2025-02-13 20:57:19,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:19,330][root][INFO] - Training Epoch: 2/2, step 4761/7134 completed (loss: 0.10557085275650024, acc: 0.9817073345184326)
[2025-02-13 20:57:19,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:19,682][root][INFO] - Training Epoch: 2/2, step 4762/7134 completed (loss: 0.20584458112716675, acc: 0.9670329689979553)
[2025-02-13 20:57:19,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:20,034][root][INFO] - Training Epoch: 2/2, step 4763/7134 completed (loss: 0.0667216032743454, acc: 0.9867549538612366)
[2025-02-13 20:57:20,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:20,398][root][INFO] - Training Epoch: 2/2, step 4764/7134 completed (loss: 0.10847733914852142, acc: 0.9742268323898315)
[2025-02-13 20:57:20,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:20,769][root][INFO] - Training Epoch: 2/2, step 4765/7134 completed (loss: 0.15163609385490417, acc: 0.9666666388511658)
[2025-02-13 20:57:20,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:21,134][root][INFO] - Training Epoch: 2/2, step 4766/7134 completed (loss: 0.06856106966733932, acc: 0.9833333492279053)
[2025-02-13 20:57:21,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:21,461][root][INFO] - Training Epoch: 2/2, step 4767/7134 completed (loss: 0.03401133045554161, acc: 0.9897959232330322)
[2025-02-13 20:57:21,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:21,825][root][INFO] - Training Epoch: 2/2, step 4768/7134 completed (loss: 0.09051350504159927, acc: 0.9764705896377563)
[2025-02-13 20:57:21,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:22,161][root][INFO] - Training Epoch: 2/2, step 4769/7134 completed (loss: 0.11027877032756805, acc: 0.9736841917037964)
[2025-02-13 20:57:22,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:22,558][root][INFO] - Training Epoch: 2/2, step 4770/7134 completed (loss: 0.03847045451402664, acc: 1.0)
[2025-02-13 20:57:22,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:22,947][root][INFO] - Training Epoch: 2/2, step 4771/7134 completed (loss: 0.17097115516662598, acc: 0.9754601120948792)
[2025-02-13 20:57:23,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:23,316][root][INFO] - Training Epoch: 2/2, step 4772/7134 completed (loss: 0.13548552989959717, acc: 0.9677419066429138)
[2025-02-13 20:57:23,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:23,653][root][INFO] - Training Epoch: 2/2, step 4773/7134 completed (loss: 0.06493229418992996, acc: 0.9807692170143127)
[2025-02-13 20:57:23,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:23,992][root][INFO] - Training Epoch: 2/2, step 4774/7134 completed (loss: 0.05646388977766037, acc: 0.9881656765937805)
[2025-02-13 20:57:24,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:24,352][root][INFO] - Training Epoch: 2/2, step 4775/7134 completed (loss: 0.1321018487215042, acc: 0.9811320900917053)
[2025-02-13 20:57:24,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:24,716][root][INFO] - Training Epoch: 2/2, step 4776/7134 completed (loss: 0.13547760248184204, acc: 0.9718309640884399)
[2025-02-13 20:57:24,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:25,080][root][INFO] - Training Epoch: 2/2, step 4777/7134 completed (loss: 0.1345939338207245, acc: 0.9726027250289917)
[2025-02-13 20:57:25,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:25,457][root][INFO] - Training Epoch: 2/2, step 4778/7134 completed (loss: 0.0639735609292984, acc: 0.9910714030265808)
[2025-02-13 20:57:25,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:25,829][root][INFO] - Training Epoch: 2/2, step 4779/7134 completed (loss: 0.05598260089755058, acc: 0.9851852059364319)
[2025-02-13 20:57:25,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:26,208][root][INFO] - Training Epoch: 2/2, step 4780/7134 completed (loss: 0.028227027505636215, acc: 0.9807692170143127)
[2025-02-13 20:57:26,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:26,579][root][INFO] - Training Epoch: 2/2, step 4781/7134 completed (loss: 0.03784654289484024, acc: 1.0)
[2025-02-13 20:57:26,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:26,988][root][INFO] - Training Epoch: 2/2, step 4782/7134 completed (loss: 0.040934912860393524, acc: 0.987500011920929)
[2025-02-13 20:57:27,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:27,360][root][INFO] - Training Epoch: 2/2, step 4783/7134 completed (loss: 0.14301657676696777, acc: 0.9492753744125366)
[2025-02-13 20:57:27,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:27,729][root][INFO] - Training Epoch: 2/2, step 4784/7134 completed (loss: 0.06254050135612488, acc: 0.9833333492279053)
[2025-02-13 20:57:27,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:28,087][root][INFO] - Training Epoch: 2/2, step 4785/7134 completed (loss: 0.09497834742069244, acc: 0.9850746393203735)
[2025-02-13 20:57:28,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:28,451][root][INFO] - Training Epoch: 2/2, step 4786/7134 completed (loss: 0.049203552305698395, acc: 0.9876543283462524)
[2025-02-13 20:57:28,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:28,816][root][INFO] - Training Epoch: 2/2, step 4787/7134 completed (loss: 0.04169890284538269, acc: 0.9937106966972351)
[2025-02-13 20:57:28,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:29,204][root][INFO] - Training Epoch: 2/2, step 4788/7134 completed (loss: 0.028677452355623245, acc: 0.9923076629638672)
[2025-02-13 20:57:29,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:29,590][root][INFO] - Training Epoch: 2/2, step 4789/7134 completed (loss: 0.04141857102513313, acc: 0.9935897588729858)
[2025-02-13 20:57:29,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:30,011][root][INFO] - Training Epoch: 2/2, step 4790/7134 completed (loss: 0.06744453310966492, acc: 0.983146071434021)
[2025-02-13 20:57:30,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:30,397][root][INFO] - Training Epoch: 2/2, step 4791/7134 completed (loss: 0.03067816235125065, acc: 0.9925373196601868)
[2025-02-13 20:57:30,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:30,792][root][INFO] - Training Epoch: 2/2, step 4792/7134 completed (loss: 0.031008463352918625, acc: 0.9932432174682617)
[2025-02-13 20:57:30,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:31,167][root][INFO] - Training Epoch: 2/2, step 4793/7134 completed (loss: 0.06176582723855972, acc: 0.9789473414421082)
[2025-02-13 20:57:31,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:31,551][root][INFO] - Training Epoch: 2/2, step 4794/7134 completed (loss: 0.23331905901432037, acc: 0.9432623982429504)
[2025-02-13 20:57:31,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:31,932][root][INFO] - Training Epoch: 2/2, step 4795/7134 completed (loss: 0.04455185681581497, acc: 0.987730085849762)
[2025-02-13 20:57:32,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:32,297][root][INFO] - Training Epoch: 2/2, step 4796/7134 completed (loss: 0.12033722549676895, acc: 0.9736841917037964)
[2025-02-13 20:57:32,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:32,679][root][INFO] - Training Epoch: 2/2, step 4797/7134 completed (loss: 0.03799816966056824, acc: 0.9890710115432739)
[2025-02-13 20:57:32,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:33,058][root][INFO] - Training Epoch: 2/2, step 4798/7134 completed (loss: 0.118134506046772, acc: 0.9880239367485046)
[2025-02-13 20:57:33,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:33,448][root][INFO] - Training Epoch: 2/2, step 4799/7134 completed (loss: 0.11160600930452347, acc: 0.9659090638160706)
[2025-02-13 20:57:33,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:33,811][root][INFO] - Training Epoch: 2/2, step 4800/7134 completed (loss: 0.06135820224881172, acc: 0.9868420958518982)
[2025-02-13 20:57:33,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:34,191][root][INFO] - Training Epoch: 2/2, step 4801/7134 completed (loss: 0.026886142790317535, acc: 1.0)
[2025-02-13 20:57:34,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:34,584][root][INFO] - Training Epoch: 2/2, step 4802/7134 completed (loss: 0.12160106748342514, acc: 0.9639175534248352)
[2025-02-13 20:57:34,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:34,948][root][INFO] - Training Epoch: 2/2, step 4803/7134 completed (loss: 0.051688045263290405, acc: 0.9878787994384766)
[2025-02-13 20:57:35,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:35,363][root][INFO] - Training Epoch: 2/2, step 4804/7134 completed (loss: 0.15777653455734253, acc: 0.9638554453849792)
[2025-02-13 20:57:35,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:35,760][root][INFO] - Training Epoch: 2/2, step 4805/7134 completed (loss: 0.0516185387969017, acc: 0.9933775067329407)
[2025-02-13 20:57:35,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:36,128][root][INFO] - Training Epoch: 2/2, step 4806/7134 completed (loss: 0.11218609660863876, acc: 0.9860140085220337)
[2025-02-13 20:57:36,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:36,438][root][INFO] - Training Epoch: 2/2, step 4807/7134 completed (loss: 0.04844067245721817, acc: 0.9873417615890503)
[2025-02-13 20:57:36,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:36,821][root][INFO] - Training Epoch: 2/2, step 4808/7134 completed (loss: 0.11970486491918564, acc: 0.9848484992980957)
[2025-02-13 20:57:36,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:37,246][root][INFO] - Training Epoch: 2/2, step 4809/7134 completed (loss: 0.08668293058872223, acc: 0.9724137783050537)
[2025-02-13 20:57:37,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:37,643][root][INFO] - Training Epoch: 2/2, step 4810/7134 completed (loss: 0.10041259229183197, acc: 0.970588207244873)
[2025-02-13 20:57:37,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:38,022][root][INFO] - Training Epoch: 2/2, step 4811/7134 completed (loss: 0.06521394103765488, acc: 0.9918032884597778)
[2025-02-13 20:57:38,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:38,402][root][INFO] - Training Epoch: 2/2, step 4812/7134 completed (loss: 0.1419695019721985, acc: 0.9774011373519897)
[2025-02-13 20:57:38,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:38,789][root][INFO] - Training Epoch: 2/2, step 4813/7134 completed (loss: 0.018068699166178703, acc: 1.0)
[2025-02-13 20:57:38,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:39,174][root][INFO] - Training Epoch: 2/2, step 4814/7134 completed (loss: 0.0574287474155426, acc: 0.989130437374115)
[2025-02-13 20:57:39,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:39,583][root][INFO] - Training Epoch: 2/2, step 4815/7134 completed (loss: 0.019796598702669144, acc: 0.9873417615890503)
[2025-02-13 20:57:39,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:39,991][root][INFO] - Training Epoch: 2/2, step 4816/7134 completed (loss: 0.019687842577695847, acc: 0.9949748516082764)
[2025-02-13 20:57:40,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:40,349][root][INFO] - Training Epoch: 2/2, step 4817/7134 completed (loss: 0.0710182934999466, acc: 0.9797979593276978)
[2025-02-13 20:57:40,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:40,747][root][INFO] - Training Epoch: 2/2, step 4818/7134 completed (loss: 0.026311039924621582, acc: 1.0)
[2025-02-13 20:57:40,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:41,133][root][INFO] - Training Epoch: 2/2, step 4819/7134 completed (loss: 0.022753996774554253, acc: 0.9916666746139526)
[2025-02-13 20:57:41,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:41,498][root][INFO] - Training Epoch: 2/2, step 4820/7134 completed (loss: 0.030390005558729172, acc: 0.9946523904800415)
[2025-02-13 20:57:41,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:41,848][root][INFO] - Training Epoch: 2/2, step 4821/7134 completed (loss: 0.027190426364541054, acc: 0.9942196607589722)
[2025-02-13 20:57:41,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:42,190][root][INFO] - Training Epoch: 2/2, step 4822/7134 completed (loss: 0.03668094053864479, acc: 0.9926470518112183)
[2025-02-13 20:57:42,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:42,568][root][INFO] - Training Epoch: 2/2, step 4823/7134 completed (loss: 0.1342054009437561, acc: 0.9622641801834106)
[2025-02-13 20:57:42,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:42,916][root][INFO] - Training Epoch: 2/2, step 4824/7134 completed (loss: 0.12162917852401733, acc: 0.9661017060279846)
[2025-02-13 20:57:43,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:43,276][root][INFO] - Training Epoch: 2/2, step 4825/7134 completed (loss: 0.0273943729698658, acc: 0.9926470518112183)
[2025-02-13 20:57:43,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:43,647][root][INFO] - Training Epoch: 2/2, step 4826/7134 completed (loss: 0.03763510659337044, acc: 1.0)
[2025-02-13 20:57:43,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:44,015][root][INFO] - Training Epoch: 2/2, step 4827/7134 completed (loss: 0.18405881524085999, acc: 0.9612902998924255)
[2025-02-13 20:57:44,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:44,386][root][INFO] - Training Epoch: 2/2, step 4828/7134 completed (loss: 0.020118897780776024, acc: 1.0)
[2025-02-13 20:57:44,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:44,769][root][INFO] - Training Epoch: 2/2, step 4829/7134 completed (loss: 0.01736264117062092, acc: 0.9950494766235352)
[2025-02-13 20:57:44,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:45,145][root][INFO] - Training Epoch: 2/2, step 4830/7134 completed (loss: 0.02754572592675686, acc: 0.9939393997192383)
[2025-02-13 20:57:45,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:45,539][root][INFO] - Training Epoch: 2/2, step 4831/7134 completed (loss: 0.00678935507312417, acc: 1.0)
[2025-02-13 20:57:45,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:45,918][root][INFO] - Training Epoch: 2/2, step 4832/7134 completed (loss: 0.022892063483595848, acc: 0.9869281053543091)
[2025-02-13 20:57:46,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:46,292][root][INFO] - Training Epoch: 2/2, step 4833/7134 completed (loss: 0.025155266746878624, acc: 1.0)
[2025-02-13 20:57:46,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:46,658][root][INFO] - Training Epoch: 2/2, step 4834/7134 completed (loss: 0.016551952809095383, acc: 0.9953488111495972)
[2025-02-13 20:57:46,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:47,024][root][INFO] - Training Epoch: 2/2, step 4835/7134 completed (loss: 0.029457606375217438, acc: 0.9846153855323792)
[2025-02-13 20:57:47,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:47,405][root][INFO] - Training Epoch: 2/2, step 4836/7134 completed (loss: 0.03981952369213104, acc: 0.9938271641731262)
[2025-02-13 20:57:47,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:47,770][root][INFO] - Training Epoch: 2/2, step 4837/7134 completed (loss: 0.19646596908569336, acc: 0.949999988079071)
[2025-02-13 20:57:47,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:48,145][root][INFO] - Training Epoch: 2/2, step 4838/7134 completed (loss: 0.11530455946922302, acc: 0.9784172773361206)
[2025-02-13 20:57:48,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:48,506][root][INFO] - Training Epoch: 2/2, step 4839/7134 completed (loss: 0.12655240297317505, acc: 0.9689922332763672)
[2025-02-13 20:57:48,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:48,876][root][INFO] - Training Epoch: 2/2, step 4840/7134 completed (loss: 0.0837668925523758, acc: 0.9868420958518982)
[2025-02-13 20:57:49,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:49,227][root][INFO] - Training Epoch: 2/2, step 4841/7134 completed (loss: 0.05358891561627388, acc: 0.9930555820465088)
[2025-02-13 20:57:49,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:49,611][root][INFO] - Training Epoch: 2/2, step 4842/7134 completed (loss: 0.09274592995643616, acc: 0.9722222089767456)
[2025-02-13 20:57:49,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:49,961][root][INFO] - Training Epoch: 2/2, step 4843/7134 completed (loss: 0.12276383489370346, acc: 0.9622641801834106)
[2025-02-13 20:57:50,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:50,296][root][INFO] - Training Epoch: 2/2, step 4844/7134 completed (loss: 0.05437784641981125, acc: 0.9846153855323792)
[2025-02-13 20:57:50,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:50,682][root][INFO] - Training Epoch: 2/2, step 4845/7134 completed (loss: 0.055478937923908234, acc: 0.987500011920929)
[2025-02-13 20:57:50,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:51,043][root][INFO] - Training Epoch: 2/2, step 4846/7134 completed (loss: 0.13787995278835297, acc: 0.96875)
[2025-02-13 20:57:51,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:51,389][root][INFO] - Training Epoch: 2/2, step 4847/7134 completed (loss: 0.04938101768493652, acc: 0.9940828680992126)
[2025-02-13 20:57:51,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:51,755][root][INFO] - Training Epoch: 2/2, step 4848/7134 completed (loss: 0.11614568531513214, acc: 0.9753086566925049)
[2025-02-13 20:57:51,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:52,113][root][INFO] - Training Epoch: 2/2, step 4849/7134 completed (loss: 0.041141536086797714, acc: 0.9931507110595703)
[2025-02-13 20:57:52,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:52,481][root][INFO] - Training Epoch: 2/2, step 4850/7134 completed (loss: 0.1148318499326706, acc: 0.9608938694000244)
[2025-02-13 20:57:52,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:52,843][root][INFO] - Training Epoch: 2/2, step 4851/7134 completed (loss: 0.056081876158714294, acc: 0.9891892075538635)
[2025-02-13 20:57:52,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:53,207][root][INFO] - Training Epoch: 2/2, step 4852/7134 completed (loss: 0.10887356847524643, acc: 0.9651162624359131)
[2025-02-13 20:57:53,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:53,581][root][INFO] - Training Epoch: 2/2, step 4853/7134 completed (loss: 0.10718392580747604, acc: 0.9640718698501587)
[2025-02-13 20:57:53,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:53,943][root][INFO] - Training Epoch: 2/2, step 4854/7134 completed (loss: 0.06266159564256668, acc: 0.9932885766029358)
[2025-02-13 20:57:54,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:54,350][root][INFO] - Training Epoch: 2/2, step 4855/7134 completed (loss: 0.13180731236934662, acc: 0.9712643623352051)
[2025-02-13 20:57:54,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:54,750][root][INFO] - Training Epoch: 2/2, step 4856/7134 completed (loss: 0.07103761285543442, acc: 0.9642857313156128)
[2025-02-13 20:57:54,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:55,106][root][INFO] - Training Epoch: 2/2, step 4857/7134 completed (loss: 0.05727379396557808, acc: 0.9876543283462524)
[2025-02-13 20:57:55,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:55,478][root][INFO] - Training Epoch: 2/2, step 4858/7134 completed (loss: 0.14297007024288177, acc: 0.9567901492118835)
[2025-02-13 20:57:55,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:55,835][root][INFO] - Training Epoch: 2/2, step 4859/7134 completed (loss: 0.049974579364061356, acc: 0.976190447807312)
[2025-02-13 20:57:55,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:56,195][root][INFO] - Training Epoch: 2/2, step 4860/7134 completed (loss: 0.06773573905229568, acc: 0.9941520690917969)
[2025-02-13 20:57:56,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:56,550][root][INFO] - Training Epoch: 2/2, step 4861/7134 completed (loss: 0.055560845881700516, acc: 0.9881656765937805)
[2025-02-13 20:57:56,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:56,906][root][INFO] - Training Epoch: 2/2, step 4862/7134 completed (loss: 0.07542000710964203, acc: 0.9736841917037964)
[2025-02-13 20:57:57,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:57,265][root][INFO] - Training Epoch: 2/2, step 4863/7134 completed (loss: 0.040127500891685486, acc: 0.9922480583190918)
[2025-02-13 20:57:57,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:57,655][root][INFO] - Training Epoch: 2/2, step 4864/7134 completed (loss: 0.08465105295181274, acc: 0.987261176109314)
[2025-02-13 20:57:57,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:58,030][root][INFO] - Training Epoch: 2/2, step 4865/7134 completed (loss: 0.0816749706864357, acc: 0.9623655676841736)
[2025-02-13 20:57:58,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:58,399][root][INFO] - Training Epoch: 2/2, step 4866/7134 completed (loss: 0.05949816480278969, acc: 0.9767441749572754)
[2025-02-13 20:57:58,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:58,768][root][INFO] - Training Epoch: 2/2, step 4867/7134 completed (loss: 0.09765060245990753, acc: 0.9832402467727661)
[2025-02-13 20:57:58,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:59,138][root][INFO] - Training Epoch: 2/2, step 4868/7134 completed (loss: 0.07485350966453552, acc: 0.9747899174690247)
[2025-02-13 20:57:59,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:59,459][root][INFO] - Training Epoch: 2/2, step 4869/7134 completed (loss: 0.08151307702064514, acc: 0.9795918464660645)
[2025-02-13 20:57:59,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:59,818][root][INFO] - Training Epoch: 2/2, step 4870/7134 completed (loss: 0.038779471069574356, acc: 0.9891892075538635)
[2025-02-13 20:57:59,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:00,167][root][INFO] - Training Epoch: 2/2, step 4871/7134 completed (loss: 0.06499043852090836, acc: 0.9736841917037964)
[2025-02-13 20:58:00,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:00,527][root][INFO] - Training Epoch: 2/2, step 4872/7134 completed (loss: 0.01580463908612728, acc: 0.9940476417541504)
[2025-02-13 20:58:00,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:00,905][root][INFO] - Training Epoch: 2/2, step 4873/7134 completed (loss: 0.07472603023052216, acc: 0.9781420826911926)
[2025-02-13 20:58:01,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:01,285][root][INFO] - Training Epoch: 2/2, step 4874/7134 completed (loss: 0.0738162249326706, acc: 0.9804878234863281)
[2025-02-13 20:58:01,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:01,694][root][INFO] - Training Epoch: 2/2, step 4875/7134 completed (loss: 0.03701755404472351, acc: 0.9937888383865356)
[2025-02-13 20:58:01,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:02,027][root][INFO] - Training Epoch: 2/2, step 4876/7134 completed (loss: 0.07791072130203247, acc: 0.9814814925193787)
[2025-02-13 20:58:02,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:02,400][root][INFO] - Training Epoch: 2/2, step 4877/7134 completed (loss: 0.051918186247348785, acc: 0.9846153855323792)
[2025-02-13 20:58:02,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:02,759][root][INFO] - Training Epoch: 2/2, step 4878/7134 completed (loss: 0.05448921024799347, acc: 0.9935064911842346)
[2025-02-13 20:58:02,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:03,136][root][INFO] - Training Epoch: 2/2, step 4879/7134 completed (loss: 0.03360460326075554, acc: 0.9935897588729858)
[2025-02-13 20:58:03,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:03,557][root][INFO] - Training Epoch: 2/2, step 4880/7134 completed (loss: 0.023921754211187363, acc: 1.0)
[2025-02-13 20:58:03,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:03,936][root][INFO] - Training Epoch: 2/2, step 4881/7134 completed (loss: 0.07372722029685974, acc: 0.9865771532058716)
[2025-02-13 20:58:04,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:04,289][root][INFO] - Training Epoch: 2/2, step 4882/7134 completed (loss: 0.028341293334960938, acc: 0.9935483932495117)
[2025-02-13 20:58:04,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:04,649][root][INFO] - Training Epoch: 2/2, step 4883/7134 completed (loss: 0.01474853791296482, acc: 1.0)
[2025-02-13 20:58:04,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:05,006][root][INFO] - Training Epoch: 2/2, step 4884/7134 completed (loss: 0.04630913585424423, acc: 0.9869281053543091)
[2025-02-13 20:58:05,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:05,364][root][INFO] - Training Epoch: 2/2, step 4885/7134 completed (loss: 0.01754899136722088, acc: 1.0)
[2025-02-13 20:58:05,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:05,713][root][INFO] - Training Epoch: 2/2, step 4886/7134 completed (loss: 0.03308164328336716, acc: 0.9903846383094788)
[2025-02-13 20:58:05,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:06,086][root][INFO] - Training Epoch: 2/2, step 4887/7134 completed (loss: 0.04541643708944321, acc: 0.9842932224273682)
[2025-02-13 20:58:06,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:06,473][root][INFO] - Training Epoch: 2/2, step 4888/7134 completed (loss: 0.08233443647623062, acc: 0.9835164546966553)
[2025-02-13 20:58:06,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:06,838][root][INFO] - Training Epoch: 2/2, step 4889/7134 completed (loss: 0.02043786086142063, acc: 1.0)
[2025-02-13 20:58:06,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:07,184][root][INFO] - Training Epoch: 2/2, step 4890/7134 completed (loss: 0.053555928170681, acc: 0.9905660152435303)
[2025-02-13 20:58:07,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:07,575][root][INFO] - Training Epoch: 2/2, step 4891/7134 completed (loss: 0.12050437182188034, acc: 0.954081654548645)
[2025-02-13 20:58:07,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:07,938][root][INFO] - Training Epoch: 2/2, step 4892/7134 completed (loss: 0.028209589421749115, acc: 1.0)
[2025-02-13 20:58:08,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:08,309][root][INFO] - Training Epoch: 2/2, step 4893/7134 completed (loss: 0.25657927989959717, acc: 0.9396551847457886)
[2025-02-13 20:58:08,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:08,701][root][INFO] - Training Epoch: 2/2, step 4894/7134 completed (loss: 0.06507450342178345, acc: 0.9764705896377563)
[2025-02-13 20:58:08,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:09,071][root][INFO] - Training Epoch: 2/2, step 4895/7134 completed (loss: 0.02024424448609352, acc: 0.9948979616165161)
[2025-02-13 20:58:09,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:09,414][root][INFO] - Training Epoch: 2/2, step 4896/7134 completed (loss: 0.08561314642429352, acc: 0.994413435459137)
[2025-02-13 20:58:09,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:09,778][root][INFO] - Training Epoch: 2/2, step 4897/7134 completed (loss: 0.045634180307388306, acc: 0.9802955389022827)
[2025-02-13 20:58:09,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:10,165][root][INFO] - Training Epoch: 2/2, step 4898/7134 completed (loss: 0.04456799849867821, acc: 0.9892473220825195)
[2025-02-13 20:58:10,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:10,532][root][INFO] - Training Epoch: 2/2, step 4899/7134 completed (loss: 0.031862933188676834, acc: 0.9939024448394775)
[2025-02-13 20:58:10,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:10,899][root][INFO] - Training Epoch: 2/2, step 4900/7134 completed (loss: 0.052689939737319946, acc: 0.9839572310447693)
[2025-02-13 20:58:11,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:11,265][root][INFO] - Training Epoch: 2/2, step 4901/7134 completed (loss: 0.043150644749403, acc: 0.9817073345184326)
[2025-02-13 20:58:11,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:11,643][root][INFO] - Training Epoch: 2/2, step 4902/7134 completed (loss: 0.021441342309117317, acc: 0.9948717951774597)
[2025-02-13 20:58:11,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:12,015][root][INFO] - Training Epoch: 2/2, step 4903/7134 completed (loss: 0.018193822354078293, acc: 1.0)
[2025-02-13 20:58:12,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:12,378][root][INFO] - Training Epoch: 2/2, step 4904/7134 completed (loss: 0.03794986382126808, acc: 0.9882352948188782)
[2025-02-13 20:58:12,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:12,743][root][INFO] - Training Epoch: 2/2, step 4905/7134 completed (loss: 0.027849826961755753, acc: 0.9869281053543091)
[2025-02-13 20:58:12,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:13,101][root][INFO] - Training Epoch: 2/2, step 4906/7134 completed (loss: 0.015043559484183788, acc: 1.0)
[2025-02-13 20:58:13,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:13,463][root][INFO] - Training Epoch: 2/2, step 4907/7134 completed (loss: 0.037871286273002625, acc: 0.9931972622871399)
[2025-02-13 20:58:13,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:13,831][root][INFO] - Training Epoch: 2/2, step 4908/7134 completed (loss: 0.009309890680015087, acc: 1.0)
[2025-02-13 20:58:13,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:14,230][root][INFO] - Training Epoch: 2/2, step 4909/7134 completed (loss: 0.018054550513625145, acc: 0.9932885766029358)
[2025-02-13 20:58:14,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:14,627][root][INFO] - Training Epoch: 2/2, step 4910/7134 completed (loss: 0.07625948637723923, acc: 0.9870967864990234)
[2025-02-13 20:58:14,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:14,999][root][INFO] - Training Epoch: 2/2, step 4911/7134 completed (loss: 0.025515003129839897, acc: 0.9946523904800415)
[2025-02-13 20:58:15,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:15,372][root][INFO] - Training Epoch: 2/2, step 4912/7134 completed (loss: 0.022419627755880356, acc: 0.9950248599052429)
[2025-02-13 20:58:15,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:15,740][root][INFO] - Training Epoch: 2/2, step 4913/7134 completed (loss: 0.06013540178537369, acc: 0.9860140085220337)
[2025-02-13 20:58:15,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:16,154][root][INFO] - Training Epoch: 2/2, step 4914/7134 completed (loss: 0.026932405307888985, acc: 1.0)
[2025-02-13 20:58:16,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:16,528][root][INFO] - Training Epoch: 2/2, step 4915/7134 completed (loss: 0.019274132326245308, acc: 0.9938271641731262)
[2025-02-13 20:58:16,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:16,880][root][INFO] - Training Epoch: 2/2, step 4916/7134 completed (loss: 0.04906150698661804, acc: 0.989130437374115)
[2025-02-13 20:58:17,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:17,242][root][INFO] - Training Epoch: 2/2, step 4917/7134 completed (loss: 0.020271258428692818, acc: 1.0)
[2025-02-13 20:58:17,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:17,600][root][INFO] - Training Epoch: 2/2, step 4918/7134 completed (loss: 0.012592064216732979, acc: 0.9938271641731262)
[2025-02-13 20:58:17,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:18,006][root][INFO] - Training Epoch: 2/2, step 4919/7134 completed (loss: 0.036681950092315674, acc: 0.9887005686759949)
[2025-02-13 20:58:18,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:18,372][root][INFO] - Training Epoch: 2/2, step 4920/7134 completed (loss: 0.05396214500069618, acc: 0.9813664555549622)
[2025-02-13 20:58:18,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:18,780][root][INFO] - Training Epoch: 2/2, step 4921/7134 completed (loss: 0.0812297835946083, acc: 0.985401451587677)
[2025-02-13 20:58:18,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:19,147][root][INFO] - Training Epoch: 2/2, step 4922/7134 completed (loss: 0.09318585693836212, acc: 0.9875776171684265)
[2025-02-13 20:58:19,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:19,513][root][INFO] - Training Epoch: 2/2, step 4923/7134 completed (loss: 0.013463450595736504, acc: 1.0)
[2025-02-13 20:58:19,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:19,860][root][INFO] - Training Epoch: 2/2, step 4924/7134 completed (loss: 0.015913404524326324, acc: 1.0)
[2025-02-13 20:58:19,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:20,221][root][INFO] - Training Epoch: 2/2, step 4925/7134 completed (loss: 0.04831542819738388, acc: 0.9932885766029358)
[2025-02-13 20:58:20,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:20,568][root][INFO] - Training Epoch: 2/2, step 4926/7134 completed (loss: 0.012955702841281891, acc: 1.0)
[2025-02-13 20:58:20,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:20,910][root][INFO] - Training Epoch: 2/2, step 4927/7134 completed (loss: 0.03496500849723816, acc: 0.9935483932495117)
[2025-02-13 20:58:21,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:21,252][root][INFO] - Training Epoch: 2/2, step 4928/7134 completed (loss: 0.01944185607135296, acc: 0.9947368502616882)
[2025-02-13 20:58:21,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:21,613][root][INFO] - Training Epoch: 2/2, step 4929/7134 completed (loss: 0.024766042828559875, acc: 1.0)
[2025-02-13 20:58:21,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:21,962][root][INFO] - Training Epoch: 2/2, step 4930/7134 completed (loss: 0.037628356367349625, acc: 0.9864864945411682)
[2025-02-13 20:58:22,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:22,333][root][INFO] - Training Epoch: 2/2, step 4931/7134 completed (loss: 0.025227822363376617, acc: 1.0)
[2025-02-13 20:58:22,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:22,697][root][INFO] - Training Epoch: 2/2, step 4932/7134 completed (loss: 0.029843024909496307, acc: 0.9941176176071167)
[2025-02-13 20:58:22,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:23,058][root][INFO] - Training Epoch: 2/2, step 4933/7134 completed (loss: 0.06529463082551956, acc: 0.9738219976425171)
[2025-02-13 20:58:23,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:23,422][root][INFO] - Training Epoch: 2/2, step 4934/7134 completed (loss: 0.08747744560241699, acc: 0.9823529124259949)
[2025-02-13 20:58:23,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:23,778][root][INFO] - Training Epoch: 2/2, step 4935/7134 completed (loss: 0.05941152572631836, acc: 0.9823529124259949)
[2025-02-13 20:58:23,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:24,141][root][INFO] - Training Epoch: 2/2, step 4936/7134 completed (loss: 0.014558127149939537, acc: 1.0)
[2025-02-13 20:58:24,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:24,547][root][INFO] - Training Epoch: 2/2, step 4937/7134 completed (loss: 0.04505638778209686, acc: 0.9930070042610168)
[2025-02-13 20:58:24,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:24,932][root][INFO] - Training Epoch: 2/2, step 4938/7134 completed (loss: 0.013551685959100723, acc: 1.0)
[2025-02-13 20:58:25,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:25,297][root][INFO] - Training Epoch: 2/2, step 4939/7134 completed (loss: 0.05707937851548195, acc: 0.9928571581840515)
[2025-02-13 20:58:25,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:25,680][root][INFO] - Training Epoch: 2/2, step 4940/7134 completed (loss: 0.12295990437269211, acc: 0.9922480583190918)
[2025-02-13 20:58:25,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:26,058][root][INFO] - Training Epoch: 2/2, step 4941/7134 completed (loss: 0.06872721016407013, acc: 0.9820359349250793)
[2025-02-13 20:58:26,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:26,432][root][INFO] - Training Epoch: 2/2, step 4942/7134 completed (loss: 0.048049814999103546, acc: 0.9810126423835754)
[2025-02-13 20:58:26,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:26,810][root][INFO] - Training Epoch: 2/2, step 4943/7134 completed (loss: 0.05580834671854973, acc: 0.987500011920929)
[2025-02-13 20:58:26,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:27,180][root][INFO] - Training Epoch: 2/2, step 4944/7134 completed (loss: 0.11705811321735382, acc: 0.9747899174690247)
[2025-02-13 20:58:27,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:27,536][root][INFO] - Training Epoch: 2/2, step 4945/7134 completed (loss: 0.08129259198904037, acc: 0.9772727489471436)
[2025-02-13 20:58:27,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:27,899][root][INFO] - Training Epoch: 2/2, step 4946/7134 completed (loss: 0.036907028406858444, acc: 0.9921875)
[2025-02-13 20:58:28,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:28,271][root][INFO] - Training Epoch: 2/2, step 4947/7134 completed (loss: 0.05972788855433464, acc: 0.9739130139350891)
[2025-02-13 20:58:28,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:28,631][root][INFO] - Training Epoch: 2/2, step 4948/7134 completed (loss: 0.05611353740096092, acc: 0.9904761910438538)
[2025-02-13 20:58:28,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:28,976][root][INFO] - Training Epoch: 2/2, step 4949/7134 completed (loss: 0.013458861038088799, acc: 1.0)
[2025-02-13 20:58:29,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:29,342][root][INFO] - Training Epoch: 2/2, step 4950/7134 completed (loss: 0.05483802407979965, acc: 0.9931972622871399)
[2025-02-13 20:58:29,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:29,727][root][INFO] - Training Epoch: 2/2, step 4951/7134 completed (loss: 0.046756159514188766, acc: 0.9887640476226807)
[2025-02-13 20:58:29,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:30,141][root][INFO] - Training Epoch: 2/2, step 4952/7134 completed (loss: 0.030076339840888977, acc: 0.9864864945411682)
[2025-02-13 20:58:30,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:30,485][root][INFO] - Training Epoch: 2/2, step 4953/7134 completed (loss: 0.024990133941173553, acc: 1.0)
[2025-02-13 20:58:30,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:30,891][root][INFO] - Training Epoch: 2/2, step 4954/7134 completed (loss: 0.017274441197514534, acc: 1.0)
[2025-02-13 20:58:31,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:31,283][root][INFO] - Training Epoch: 2/2, step 4955/7134 completed (loss: 0.022358665242791176, acc: 1.0)
[2025-02-13 20:58:31,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:31,653][root][INFO] - Training Epoch: 2/2, step 4956/7134 completed (loss: 0.10907787829637527, acc: 0.9647058844566345)
[2025-02-13 20:58:31,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:32,017][root][INFO] - Training Epoch: 2/2, step 4957/7134 completed (loss: 0.09334687143564224, acc: 0.9858155846595764)
[2025-02-13 20:58:32,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:32,377][root][INFO] - Training Epoch: 2/2, step 4958/7134 completed (loss: 0.0594579353928566, acc: 0.9811320900917053)
[2025-02-13 20:58:32,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:32,741][root][INFO] - Training Epoch: 2/2, step 4959/7134 completed (loss: 0.04003993421792984, acc: 0.9879518151283264)
[2025-02-13 20:58:32,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:33,100][root][INFO] - Training Epoch: 2/2, step 4960/7134 completed (loss: 0.05188513547182083, acc: 0.9852941036224365)
[2025-02-13 20:58:33,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:33,448][root][INFO] - Training Epoch: 2/2, step 4961/7134 completed (loss: 0.026057058945298195, acc: 1.0)
[2025-02-13 20:58:33,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:33,804][root][INFO] - Training Epoch: 2/2, step 4962/7134 completed (loss: 0.013997101224958897, acc: 1.0)
[2025-02-13 20:58:33,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:34,133][root][INFO] - Training Epoch: 2/2, step 4963/7134 completed (loss: 0.016464315354824066, acc: 1.0)
[2025-02-13 20:58:34,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:34,493][root][INFO] - Training Epoch: 2/2, step 4964/7134 completed (loss: 0.08524751663208008, acc: 0.9856114983558655)
[2025-02-13 20:58:34,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:34,873][root][INFO] - Training Epoch: 2/2, step 4965/7134 completed (loss: 0.017609987407922745, acc: 1.0)
[2025-02-13 20:58:35,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:35,235][root][INFO] - Training Epoch: 2/2, step 4966/7134 completed (loss: 0.15358372032642365, acc: 0.9670329689979553)
[2025-02-13 20:58:35,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:35,604][root][INFO] - Training Epoch: 2/2, step 4967/7134 completed (loss: 0.08032207190990448, acc: 0.9844961166381836)
[2025-02-13 20:58:35,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:35,971][root][INFO] - Training Epoch: 2/2, step 4968/7134 completed (loss: 0.1705227643251419, acc: 0.9503546357154846)
[2025-02-13 20:58:36,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:36,342][root][INFO] - Training Epoch: 2/2, step 4969/7134 completed (loss: 0.04976153373718262, acc: 0.9793103337287903)
[2025-02-13 20:58:36,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:36,688][root][INFO] - Training Epoch: 2/2, step 4970/7134 completed (loss: 0.039591703563928604, acc: 0.9931034445762634)
[2025-02-13 20:58:36,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:37,044][root][INFO] - Training Epoch: 2/2, step 4971/7134 completed (loss: 0.03815050050616264, acc: 0.9867549538612366)
[2025-02-13 20:58:37,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:37,416][root][INFO] - Training Epoch: 2/2, step 4972/7134 completed (loss: 0.06420847028493881, acc: 0.981249988079071)
[2025-02-13 20:58:37,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:37,776][root][INFO] - Training Epoch: 2/2, step 4973/7134 completed (loss: 0.03771211579442024, acc: 0.9924242496490479)
[2025-02-13 20:58:37,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:38,133][root][INFO] - Training Epoch: 2/2, step 4974/7134 completed (loss: 0.09509024024009705, acc: 0.9545454382896423)
[2025-02-13 20:58:38,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:38,488][root][INFO] - Training Epoch: 2/2, step 4975/7134 completed (loss: 0.1904386729001999, acc: 0.951724112033844)
[2025-02-13 20:58:38,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:38,812][root][INFO] - Training Epoch: 2/2, step 4976/7134 completed (loss: 0.05161738023161888, acc: 0.9789473414421082)
[2025-02-13 20:58:38,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:39,179][root][INFO] - Training Epoch: 2/2, step 4977/7134 completed (loss: 0.15443335473537445, acc: 0.9708737730979919)
[2025-02-13 20:58:39,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:39,561][root][INFO] - Training Epoch: 2/2, step 4978/7134 completed (loss: 0.02371091954410076, acc: 0.9942528605461121)
[2025-02-13 20:58:39,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:39,941][root][INFO] - Training Epoch: 2/2, step 4979/7134 completed (loss: 0.18217341601848602, acc: 0.9370629191398621)
[2025-02-13 20:58:40,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:40,314][root][INFO] - Training Epoch: 2/2, step 4980/7134 completed (loss: 0.1253320425748825, acc: 0.9714285731315613)
[2025-02-13 20:58:40,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:40,688][root][INFO] - Training Epoch: 2/2, step 4981/7134 completed (loss: 0.06013920530676842, acc: 0.9857142567634583)
[2025-02-13 20:58:40,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:41,085][root][INFO] - Training Epoch: 2/2, step 4982/7134 completed (loss: 0.07052022218704224, acc: 0.9819819927215576)
[2025-02-13 20:58:41,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:41,463][root][INFO] - Training Epoch: 2/2, step 4983/7134 completed (loss: 0.09330428391695023, acc: 0.9856114983558655)
[2025-02-13 20:58:41,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:41,841][root][INFO] - Training Epoch: 2/2, step 4984/7134 completed (loss: 0.03463820740580559, acc: 1.0)
[2025-02-13 20:58:41,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:42,189][root][INFO] - Training Epoch: 2/2, step 4985/7134 completed (loss: 0.01978548988699913, acc: 1.0)
[2025-02-13 20:58:42,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:42,548][root][INFO] - Training Epoch: 2/2, step 4986/7134 completed (loss: 0.06686824560165405, acc: 0.9930070042610168)
[2025-02-13 20:58:42,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:42,892][root][INFO] - Training Epoch: 2/2, step 4987/7134 completed (loss: 0.020582783967256546, acc: 1.0)
[2025-02-13 20:58:43,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:43,244][root][INFO] - Training Epoch: 2/2, step 4988/7134 completed (loss: 0.022051140666007996, acc: 1.0)
[2025-02-13 20:58:43,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:43,582][root][INFO] - Training Epoch: 2/2, step 4989/7134 completed (loss: 0.10279848426580429, acc: 0.9882352948188782)
[2025-02-13 20:58:43,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:43,926][root][INFO] - Training Epoch: 2/2, step 4990/7134 completed (loss: 0.020245803520083427, acc: 0.9914529919624329)
[2025-02-13 20:58:44,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:44,293][root][INFO] - Training Epoch: 2/2, step 4991/7134 completed (loss: 0.017435338348150253, acc: 1.0)
[2025-02-13 20:58:44,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:44,670][root][INFO] - Training Epoch: 2/2, step 4992/7134 completed (loss: 0.06047617644071579, acc: 0.982758641242981)
[2025-02-13 20:58:44,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:44,990][root][INFO] - Training Epoch: 2/2, step 4993/7134 completed (loss: 0.01574924774467945, acc: 1.0)
[2025-02-13 20:58:45,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:45,362][root][INFO] - Training Epoch: 2/2, step 4994/7134 completed (loss: 0.031356893479824066, acc: 0.9933775067329407)
[2025-02-13 20:58:45,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:45,736][root][INFO] - Training Epoch: 2/2, step 4995/7134 completed (loss: 0.06296653300523758, acc: 0.9814814925193787)
[2025-02-13 20:58:45,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:46,087][root][INFO] - Training Epoch: 2/2, step 4996/7134 completed (loss: 0.052187442779541016, acc: 0.9916666746139526)
[2025-02-13 20:58:46,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:46,491][root][INFO] - Training Epoch: 2/2, step 4997/7134 completed (loss: 0.030932750552892685, acc: 0.9847328066825867)
[2025-02-13 20:58:46,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:46,828][root][INFO] - Training Epoch: 2/2, step 4998/7134 completed (loss: 0.05899640545248985, acc: 0.9866666793823242)
[2025-02-13 20:58:46,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:47,212][root][INFO] - Training Epoch: 2/2, step 4999/7134 completed (loss: 0.05134206265211105, acc: 0.9842519760131836)
[2025-02-13 20:58:47,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:47,600][root][INFO] - Training Epoch: 2/2, step 5000/7134 completed (loss: 0.017883919179439545, acc: 1.0)
[2025-02-13 20:58:47,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:47,981][root][INFO] - Training Epoch: 2/2, step 5001/7134 completed (loss: 0.12715502083301544, acc: 0.9863945841789246)
[2025-02-13 20:58:48,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:48,329][root][INFO] - Training Epoch: 2/2, step 5002/7134 completed (loss: 0.020098192617297173, acc: 1.0)
[2025-02-13 20:58:48,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:48,714][root][INFO] - Training Epoch: 2/2, step 5003/7134 completed (loss: 0.02625337615609169, acc: 1.0)
[2025-02-13 20:58:48,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:49,066][root][INFO] - Training Epoch: 2/2, step 5004/7134 completed (loss: 0.009156366810202599, acc: 1.0)
[2025-02-13 20:58:49,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:49,427][root][INFO] - Training Epoch: 2/2, step 5005/7134 completed (loss: 0.06874136626720428, acc: 0.9890109896659851)
[2025-02-13 20:58:49,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:49,783][root][INFO] - Training Epoch: 2/2, step 5006/7134 completed (loss: 0.04590015113353729, acc: 0.9868420958518982)
[2025-02-13 20:58:49,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:50,119][root][INFO] - Training Epoch: 2/2, step 5007/7134 completed (loss: 0.019976068288087845, acc: 1.0)
[2025-02-13 20:58:50,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:50,502][root][INFO] - Training Epoch: 2/2, step 5008/7134 completed (loss: 0.1277931183576584, acc: 0.9833333492279053)
[2025-02-13 20:58:50,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:50,874][root][INFO] - Training Epoch: 2/2, step 5009/7134 completed (loss: 0.07860985398292542, acc: 0.9896907210350037)
[2025-02-13 20:58:50,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:51,216][root][INFO] - Training Epoch: 2/2, step 5010/7134 completed (loss: 0.08639129251241684, acc: 0.9914529919624329)
[2025-02-13 20:58:51,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:51,604][root][INFO] - Training Epoch: 2/2, step 5011/7134 completed (loss: 0.07436883449554443, acc: 0.9655172228813171)
[2025-02-13 20:58:51,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:51,908][root][INFO] - Training Epoch: 2/2, step 5012/7134 completed (loss: 0.03391963988542557, acc: 0.987500011920929)
[2025-02-13 20:58:52,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:52,253][root][INFO] - Training Epoch: 2/2, step 5013/7134 completed (loss: 0.045149464160203934, acc: 0.9829059839248657)
[2025-02-13 20:58:52,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:52,631][root][INFO] - Training Epoch: 2/2, step 5014/7134 completed (loss: 0.08549271523952484, acc: 0.9807692170143127)
[2025-02-13 20:58:52,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:52,992][root][INFO] - Training Epoch: 2/2, step 5015/7134 completed (loss: 0.15469448268413544, acc: 0.9693251252174377)
[2025-02-13 20:58:53,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:53,326][root][INFO] - Training Epoch: 2/2, step 5016/7134 completed (loss: 0.15940465033054352, acc: 0.9618320465087891)
[2025-02-13 20:58:53,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:53,672][root][INFO] - Training Epoch: 2/2, step 5017/7134 completed (loss: 0.027575816959142685, acc: 1.0)
[2025-02-13 20:58:53,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:54,019][root][INFO] - Training Epoch: 2/2, step 5018/7134 completed (loss: 0.04297715425491333, acc: 0.9924812316894531)
[2025-02-13 20:58:54,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:54,381][root][INFO] - Training Epoch: 2/2, step 5019/7134 completed (loss: 0.14911270141601562, acc: 0.9717513918876648)
[2025-02-13 20:58:54,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:54,750][root][INFO] - Training Epoch: 2/2, step 5020/7134 completed (loss: 0.12026071548461914, acc: 0.9629629850387573)
[2025-02-13 20:58:54,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:55,132][root][INFO] - Training Epoch: 2/2, step 5021/7134 completed (loss: 0.08914792537689209, acc: 0.9904761910438538)
[2025-02-13 20:58:55,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:55,504][root][INFO] - Training Epoch: 2/2, step 5022/7134 completed (loss: 0.057299233973026276, acc: 0.9927536249160767)
[2025-02-13 20:58:55,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:55,880][root][INFO] - Training Epoch: 2/2, step 5023/7134 completed (loss: 0.04077086225152016, acc: 0.9870967864990234)
[2025-02-13 20:58:56,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:56,258][root][INFO] - Training Epoch: 2/2, step 5024/7134 completed (loss: 0.04127323627471924, acc: 0.9819276928901672)
[2025-02-13 20:58:56,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:56,626][root][INFO] - Training Epoch: 2/2, step 5025/7134 completed (loss: 0.01574479602277279, acc: 1.0)
[2025-02-13 20:58:56,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:56,964][root][INFO] - Training Epoch: 2/2, step 5026/7134 completed (loss: 0.021887335926294327, acc: 1.0)
[2025-02-13 20:58:57,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:57,351][root][INFO] - Training Epoch: 2/2, step 5027/7134 completed (loss: 0.03330658748745918, acc: 0.9922480583190918)
[2025-02-13 20:58:57,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:57,711][root][INFO] - Training Epoch: 2/2, step 5028/7134 completed (loss: 0.061816416680812836, acc: 0.9788732528686523)
[2025-02-13 20:58:57,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:58,068][root][INFO] - Training Epoch: 2/2, step 5029/7134 completed (loss: 0.03373468667268753, acc: 0.9923076629638672)
[2025-02-13 20:58:58,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:58,469][root][INFO] - Training Epoch: 2/2, step 5030/7134 completed (loss: 0.03515658527612686, acc: 1.0)
[2025-02-13 20:58:58,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:58,831][root][INFO] - Training Epoch: 2/2, step 5031/7134 completed (loss: 0.15118199586868286, acc: 0.9710144996643066)
[2025-02-13 20:58:58,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:59,199][root][INFO] - Training Epoch: 2/2, step 5032/7134 completed (loss: 0.04803774133324623, acc: 0.9788732528686523)
[2025-02-13 20:58:59,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:59,559][root][INFO] - Training Epoch: 2/2, step 5033/7134 completed (loss: 0.01790454611182213, acc: 1.0)
[2025-02-13 20:58:59,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:59,919][root][INFO] - Training Epoch: 2/2, step 5034/7134 completed (loss: 0.03821428492665291, acc: 0.9849624037742615)
[2025-02-13 20:59:00,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:00,301][root][INFO] - Training Epoch: 2/2, step 5035/7134 completed (loss: 0.008861462585628033, acc: 1.0)
[2025-02-13 20:59:00,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:00,664][root][INFO] - Training Epoch: 2/2, step 5036/7134 completed (loss: 0.10916455090045929, acc: 0.9807692170143127)
[2025-02-13 20:59:00,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:01,028][root][INFO] - Training Epoch: 2/2, step 5037/7134 completed (loss: 0.12863299250602722, acc: 0.970588207244873)
[2025-02-13 20:59:01,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:01,365][root][INFO] - Training Epoch: 2/2, step 5038/7134 completed (loss: 0.18419209122657776, acc: 0.95652174949646)
[2025-02-13 20:59:01,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:01,710][root][INFO] - Training Epoch: 2/2, step 5039/7134 completed (loss: 0.01655031181871891, acc: 1.0)
[2025-02-13 20:59:01,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:02,031][root][INFO] - Training Epoch: 2/2, step 5040/7134 completed (loss: 0.0255852323025465, acc: 1.0)
[2025-02-13 20:59:02,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:02,392][root][INFO] - Training Epoch: 2/2, step 5041/7134 completed (loss: 0.016960807144641876, acc: 1.0)
[2025-02-13 20:59:02,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:02,815][root][INFO] - Training Epoch: 2/2, step 5042/7134 completed (loss: 0.05674812197685242, acc: 0.9805825352668762)
[2025-02-13 20:59:02,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:03,208][root][INFO] - Training Epoch: 2/2, step 5043/7134 completed (loss: 0.04487113282084465, acc: 0.991150438785553)
[2025-02-13 20:59:03,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:03,619][root][INFO] - Training Epoch: 2/2, step 5044/7134 completed (loss: 0.032520417124032974, acc: 0.993630588054657)
[2025-02-13 20:59:03,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:04,038][root][INFO] - Training Epoch: 2/2, step 5045/7134 completed (loss: 0.05667877569794655, acc: 0.9837837815284729)
[2025-02-13 20:59:04,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:04,397][root][INFO] - Training Epoch: 2/2, step 5046/7134 completed (loss: 0.09742472320795059, acc: 0.9677419066429138)
[2025-02-13 20:59:04,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:04,746][root][INFO] - Training Epoch: 2/2, step 5047/7134 completed (loss: 0.029783746227622032, acc: 1.0)
[2025-02-13 20:59:04,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:05,153][root][INFO] - Training Epoch: 2/2, step 5048/7134 completed (loss: 0.08075535297393799, acc: 0.9857142567634583)
[2025-02-13 20:59:05,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:05,506][root][INFO] - Training Epoch: 2/2, step 5049/7134 completed (loss: 0.07665478438138962, acc: 0.987261176109314)
[2025-02-13 20:59:05,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:05,875][root][INFO] - Training Epoch: 2/2, step 5050/7134 completed (loss: 0.16548818349838257, acc: 0.981249988079071)
[2025-02-13 20:59:06,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:06,240][root][INFO] - Training Epoch: 2/2, step 5051/7134 completed (loss: 0.11838652193546295, acc: 0.9727891087532043)
[2025-02-13 20:59:06,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:06,606][root][INFO] - Training Epoch: 2/2, step 5052/7134 completed (loss: 0.095025435090065, acc: 0.9743589758872986)
[2025-02-13 20:59:06,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:07,017][root][INFO] - Training Epoch: 2/2, step 5053/7134 completed (loss: 0.14352640509605408, acc: 0.949999988079071)
[2025-02-13 20:59:07,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:07,385][root][INFO] - Training Epoch: 2/2, step 5054/7134 completed (loss: 0.12755538523197174, acc: 0.9555555582046509)
[2025-02-13 20:59:07,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:07,753][root][INFO] - Training Epoch: 2/2, step 5055/7134 completed (loss: 0.13093774020671844, acc: 0.9646017551422119)
[2025-02-13 20:59:07,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:08,145][root][INFO] - Training Epoch: 2/2, step 5056/7134 completed (loss: 0.09491710364818573, acc: 0.9798657894134521)
[2025-02-13 20:59:08,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:08,515][root][INFO] - Training Epoch: 2/2, step 5057/7134 completed (loss: 0.21105621755123138, acc: 0.9617834687232971)
[2025-02-13 20:59:08,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:08,886][root][INFO] - Training Epoch: 2/2, step 5058/7134 completed (loss: 0.08714797347784042, acc: 0.9756097793579102)
[2025-02-13 20:59:09,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:09,898][root][INFO] - Training Epoch: 2/2, step 5059/7134 completed (loss: 0.04975564032793045, acc: 0.9820359349250793)
[2025-02-13 20:59:10,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:10,274][root][INFO] - Training Epoch: 2/2, step 5060/7134 completed (loss: 0.11231053620576859, acc: 0.9800000190734863)
[2025-02-13 20:59:10,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:10,623][root][INFO] - Training Epoch: 2/2, step 5061/7134 completed (loss: 0.05224982276558876, acc: 0.9904761910438538)
[2025-02-13 20:59:10,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:11,021][root][INFO] - Training Epoch: 2/2, step 5062/7134 completed (loss: 0.06217260658740997, acc: 0.9851852059364319)
[2025-02-13 20:59:11,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:11,400][root][INFO] - Training Epoch: 2/2, step 5063/7134 completed (loss: 0.06288573890924454, acc: 0.9860140085220337)
[2025-02-13 20:59:11,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:11,779][root][INFO] - Training Epoch: 2/2, step 5064/7134 completed (loss: 0.07964135706424713, acc: 0.9785714149475098)
[2025-02-13 20:59:11,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:12,175][root][INFO] - Training Epoch: 2/2, step 5065/7134 completed (loss: 0.09027169644832611, acc: 0.9748427867889404)
[2025-02-13 20:59:12,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:12,555][root][INFO] - Training Epoch: 2/2, step 5066/7134 completed (loss: 0.07682528346776962, acc: 0.9655172228813171)
[2025-02-13 20:59:12,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:12,934][root][INFO] - Training Epoch: 2/2, step 5067/7134 completed (loss: 0.08921569585800171, acc: 0.9836065769195557)
[2025-02-13 20:59:13,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:13,345][root][INFO] - Training Epoch: 2/2, step 5068/7134 completed (loss: 0.1245851144194603, acc: 0.9577465057373047)
[2025-02-13 20:59:13,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:13,766][root][INFO] - Training Epoch: 2/2, step 5069/7134 completed (loss: 0.03857817128300667, acc: 0.9864864945411682)
[2025-02-13 20:59:13,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:14,152][root][INFO] - Training Epoch: 2/2, step 5070/7134 completed (loss: 0.032618261873722076, acc: 0.9878048896789551)
[2025-02-13 20:59:14,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:14,514][root][INFO] - Training Epoch: 2/2, step 5071/7134 completed (loss: 0.026953041553497314, acc: 1.0)
[2025-02-13 20:59:14,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:14,871][root][INFO] - Training Epoch: 2/2, step 5072/7134 completed (loss: 0.06502576172351837, acc: 0.9890109896659851)
[2025-02-13 20:59:15,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:15,211][root][INFO] - Training Epoch: 2/2, step 5073/7134 completed (loss: 0.080132856965065, acc: 0.9694656729698181)
[2025-02-13 20:59:15,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:15,597][root][INFO] - Training Epoch: 2/2, step 5074/7134 completed (loss: 0.08581533282995224, acc: 0.9767441749572754)
[2025-02-13 20:59:15,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:15,959][root][INFO] - Training Epoch: 2/2, step 5075/7134 completed (loss: 0.15721650421619415, acc: 0.9652174115180969)
[2025-02-13 20:59:16,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:16,343][root][INFO] - Training Epoch: 2/2, step 5076/7134 completed (loss: 0.04710010439157486, acc: 0.9946236610412598)
[2025-02-13 20:59:16,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:16,719][root][INFO] - Training Epoch: 2/2, step 5077/7134 completed (loss: 0.056793078780174255, acc: 0.9852941036224365)
[2025-02-13 20:59:16,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:17,109][root][INFO] - Training Epoch: 2/2, step 5078/7134 completed (loss: 0.2015528380870819, acc: 0.949999988079071)
[2025-02-13 20:59:17,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:17,490][root][INFO] - Training Epoch: 2/2, step 5079/7134 completed (loss: 0.043130386620759964, acc: 0.9888888597488403)
[2025-02-13 20:59:17,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:17,868][root][INFO] - Training Epoch: 2/2, step 5080/7134 completed (loss: 0.021794119849801064, acc: 0.9950980544090271)
[2025-02-13 20:59:18,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:18,242][root][INFO] - Training Epoch: 2/2, step 5081/7134 completed (loss: 0.03637152537703514, acc: 0.9826589822769165)
[2025-02-13 20:59:18,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:18,627][root][INFO] - Training Epoch: 2/2, step 5082/7134 completed (loss: 0.06282058358192444, acc: 0.9931034445762634)
[2025-02-13 20:59:18,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:19,011][root][INFO] - Training Epoch: 2/2, step 5083/7134 completed (loss: 0.07322650402784348, acc: 0.9822485446929932)
[2025-02-13 20:59:19,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:19,411][root][INFO] - Training Epoch: 2/2, step 5084/7134 completed (loss: 0.04830923676490784, acc: 0.9900990128517151)
[2025-02-13 20:59:19,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:19,820][root][INFO] - Training Epoch: 2/2, step 5085/7134 completed (loss: 0.05487808212637901, acc: 0.9793814420700073)
[2025-02-13 20:59:19,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:20,194][root][INFO] - Training Epoch: 2/2, step 5086/7134 completed (loss: 0.043613266199827194, acc: 0.988950252532959)
[2025-02-13 20:59:20,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:20,583][root][INFO] - Training Epoch: 2/2, step 5087/7134 completed (loss: 0.07189803570508957, acc: 0.9837837815284729)
[2025-02-13 20:59:20,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:20,982][root][INFO] - Training Epoch: 2/2, step 5088/7134 completed (loss: 0.019092325121164322, acc: 0.9940828680992126)
[2025-02-13 20:59:21,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:21,359][root][INFO] - Training Epoch: 2/2, step 5089/7134 completed (loss: 0.057451874017715454, acc: 0.9897959232330322)
[2025-02-13 20:59:21,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:21,726][root][INFO] - Training Epoch: 2/2, step 5090/7134 completed (loss: 0.029314767569303513, acc: 0.9902912378311157)
[2025-02-13 20:59:21,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:22,114][root][INFO] - Training Epoch: 2/2, step 5091/7134 completed (loss: 0.037691887468099594, acc: 1.0)
[2025-02-13 20:59:22,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:22,485][root][INFO] - Training Epoch: 2/2, step 5092/7134 completed (loss: 0.06073024868965149, acc: 0.9836956262588501)
[2025-02-13 20:59:22,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:22,836][root][INFO] - Training Epoch: 2/2, step 5093/7134 completed (loss: 0.06034146621823311, acc: 0.9772727489471436)
[2025-02-13 20:59:22,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:23,201][root][INFO] - Training Epoch: 2/2, step 5094/7134 completed (loss: 0.04697391763329506, acc: 0.9923076629638672)
[2025-02-13 20:59:23,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:23,568][root][INFO] - Training Epoch: 2/2, step 5095/7134 completed (loss: 0.1013377457857132, acc: 0.9745222926139832)
[2025-02-13 20:59:23,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:23,944][root][INFO] - Training Epoch: 2/2, step 5096/7134 completed (loss: 0.10217039287090302, acc: 0.9893617033958435)
[2025-02-13 20:59:24,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:24,333][root][INFO] - Training Epoch: 2/2, step 5097/7134 completed (loss: 0.0464961975812912, acc: 0.9893048405647278)
[2025-02-13 20:59:24,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:24,718][root][INFO] - Training Epoch: 2/2, step 5098/7134 completed (loss: 0.08334880322217941, acc: 0.9795918464660645)
[2025-02-13 20:59:24,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:25,114][root][INFO] - Training Epoch: 2/2, step 5099/7134 completed (loss: 0.047889985144138336, acc: 0.9894179701805115)
[2025-02-13 20:59:25,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:25,490][root][INFO] - Training Epoch: 2/2, step 5100/7134 completed (loss: 0.04385324567556381, acc: 0.9803921580314636)
[2025-02-13 20:59:25,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:25,881][root][INFO] - Training Epoch: 2/2, step 5101/7134 completed (loss: 0.04120154678821564, acc: 0.9947643876075745)
[2025-02-13 20:59:26,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:26,310][root][INFO] - Training Epoch: 2/2, step 5102/7134 completed (loss: 0.05927148088812828, acc: 0.9878048896789551)
[2025-02-13 20:59:26,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:26,688][root][INFO] - Training Epoch: 2/2, step 5103/7134 completed (loss: 0.03664623945951462, acc: 1.0)
[2025-02-13 20:59:26,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:27,085][root][INFO] - Training Epoch: 2/2, step 5104/7134 completed (loss: 0.037949077785015106, acc: 0.9842105507850647)
[2025-02-13 20:59:27,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:27,457][root][INFO] - Training Epoch: 2/2, step 5105/7134 completed (loss: 0.10188739746809006, acc: 0.9801324605941772)
[2025-02-13 20:59:27,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:27,824][root][INFO] - Training Epoch: 2/2, step 5106/7134 completed (loss: 0.08524015545845032, acc: 0.9794520735740662)
[2025-02-13 20:59:27,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:28,188][root][INFO] - Training Epoch: 2/2, step 5107/7134 completed (loss: 0.1805143803358078, acc: 0.9523809552192688)
[2025-02-13 20:59:28,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:28,541][root][INFO] - Training Epoch: 2/2, step 5108/7134 completed (loss: 0.14948239922523499, acc: 0.957446813583374)
[2025-02-13 20:59:28,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:28,925][root][INFO] - Training Epoch: 2/2, step 5109/7134 completed (loss: 0.19477999210357666, acc: 0.9648241400718689)
[2025-02-13 20:59:29,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:29,284][root][INFO] - Training Epoch: 2/2, step 5110/7134 completed (loss: 0.13358259201049805, acc: 0.9560439586639404)
[2025-02-13 20:59:29,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:29,652][root][INFO] - Training Epoch: 2/2, step 5111/7134 completed (loss: 0.06395794451236725, acc: 0.977011501789093)
[2025-02-13 20:59:29,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:30,027][root][INFO] - Training Epoch: 2/2, step 5112/7134 completed (loss: 0.13432982563972473, acc: 0.9621621370315552)
[2025-02-13 20:59:30,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:30,400][root][INFO] - Training Epoch: 2/2, step 5113/7134 completed (loss: 0.14215898513793945, acc: 0.9602649211883545)
[2025-02-13 20:59:30,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:30,798][root][INFO] - Training Epoch: 2/2, step 5114/7134 completed (loss: 0.06394960731267929, acc: 0.9735099077224731)
[2025-02-13 20:59:30,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:31,184][root][INFO] - Training Epoch: 2/2, step 5115/7134 completed (loss: 0.06430067121982574, acc: 0.9735449552536011)
[2025-02-13 20:59:31,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:31,574][root][INFO] - Training Epoch: 2/2, step 5116/7134 completed (loss: 0.023284869268536568, acc: 1.0)
[2025-02-13 20:59:31,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:31,941][root][INFO] - Training Epoch: 2/2, step 5117/7134 completed (loss: 0.029065480455756187, acc: 0.9944751262664795)
[2025-02-13 20:59:32,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:32,306][root][INFO] - Training Epoch: 2/2, step 5118/7134 completed (loss: 0.07782591134309769, acc: 0.9759036302566528)
[2025-02-13 20:59:32,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:32,654][root][INFO] - Training Epoch: 2/2, step 5119/7134 completed (loss: 0.052923738956451416, acc: 0.9942528605461121)
[2025-02-13 20:59:32,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:33,006][root][INFO] - Training Epoch: 2/2, step 5120/7134 completed (loss: 0.08437429368495941, acc: 0.9788359999656677)
[2025-02-13 20:59:33,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:33,381][root][INFO] - Training Epoch: 2/2, step 5121/7134 completed (loss: 0.029960673302412033, acc: 0.9878787994384766)
[2025-02-13 20:59:33,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:33,746][root][INFO] - Training Epoch: 2/2, step 5122/7134 completed (loss: 0.16564816236495972, acc: 0.9534883499145508)
[2025-02-13 20:59:33,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:34,125][root][INFO] - Training Epoch: 2/2, step 5123/7134 completed (loss: 0.06532038003206253, acc: 0.9900990128517151)
[2025-02-13 20:59:34,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:34,495][root][INFO] - Training Epoch: 2/2, step 5124/7134 completed (loss: 0.06925726681947708, acc: 0.9852216839790344)
[2025-02-13 20:59:34,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:34,873][root][INFO] - Training Epoch: 2/2, step 5125/7134 completed (loss: 0.026361150667071342, acc: 0.9934640526771545)
[2025-02-13 20:59:35,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:35,252][root][INFO] - Training Epoch: 2/2, step 5126/7134 completed (loss: 0.07121911644935608, acc: 0.9724137783050537)
[2025-02-13 20:59:35,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:35,642][root][INFO] - Training Epoch: 2/2, step 5127/7134 completed (loss: 0.10535925626754761, acc: 0.977011501789093)
[2025-02-13 20:59:35,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:36,068][root][INFO] - Training Epoch: 2/2, step 5128/7134 completed (loss: 0.034847285598516464, acc: 0.9913793206214905)
[2025-02-13 20:59:36,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:36,439][root][INFO] - Training Epoch: 2/2, step 5129/7134 completed (loss: 0.08938995748758316, acc: 0.9844961166381836)
[2025-02-13 20:59:36,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:36,833][root][INFO] - Training Epoch: 2/2, step 5130/7134 completed (loss: 0.025839682668447495, acc: 1.0)
[2025-02-13 20:59:36,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:37,221][root][INFO] - Training Epoch: 2/2, step 5131/7134 completed (loss: 0.17294356226921082, acc: 0.9743589758872986)
[2025-02-13 20:59:37,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:37,569][root][INFO] - Training Epoch: 2/2, step 5132/7134 completed (loss: 0.07690819352865219, acc: 0.9765625)
[2025-02-13 20:59:37,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:37,941][root][INFO] - Training Epoch: 2/2, step 5133/7134 completed (loss: 0.0653805360198021, acc: 0.9817073345184326)
[2025-02-13 20:59:38,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:38,340][root][INFO] - Training Epoch: 2/2, step 5134/7134 completed (loss: 0.0854669064283371, acc: 0.97826087474823)
[2025-02-13 20:59:38,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:38,737][root][INFO] - Training Epoch: 2/2, step 5135/7134 completed (loss: 0.10296127945184708, acc: 0.9768785834312439)
[2025-02-13 20:59:38,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:39,105][root][INFO] - Training Epoch: 2/2, step 5136/7134 completed (loss: 0.059957943856716156, acc: 0.9892473220825195)
[2025-02-13 20:59:39,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:39,524][root][INFO] - Training Epoch: 2/2, step 5137/7134 completed (loss: 0.058142922818660736, acc: 0.97826087474823)
[2025-02-13 20:59:39,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:39,907][root][INFO] - Training Epoch: 2/2, step 5138/7134 completed (loss: 0.13272234797477722, acc: 0.9734042286872864)
[2025-02-13 20:59:40,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:40,299][root][INFO] - Training Epoch: 2/2, step 5139/7134 completed (loss: 0.04400084540247917, acc: 0.9882352948188782)
[2025-02-13 20:59:40,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:40,687][root][INFO] - Training Epoch: 2/2, step 5140/7134 completed (loss: 0.08121874928474426, acc: 0.9666666388511658)
[2025-02-13 20:59:40,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:41,072][root][INFO] - Training Epoch: 2/2, step 5141/7134 completed (loss: 0.01519443653523922, acc: 1.0)
[2025-02-13 20:59:41,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:41,462][root][INFO] - Training Epoch: 2/2, step 5142/7134 completed (loss: 0.024502942338585854, acc: 0.9950248599052429)
[2025-02-13 20:59:41,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:41,839][root][INFO] - Training Epoch: 2/2, step 5143/7134 completed (loss: 0.023526715114712715, acc: 0.9950248599052429)
[2025-02-13 20:59:41,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:42,232][root][INFO] - Training Epoch: 2/2, step 5144/7134 completed (loss: 0.02878657914698124, acc: 0.9894179701805115)
[2025-02-13 20:59:42,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:42,609][root][INFO] - Training Epoch: 2/2, step 5145/7134 completed (loss: 0.06823094934225082, acc: 0.9947916865348816)
[2025-02-13 20:59:42,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:42,976][root][INFO] - Training Epoch: 2/2, step 5146/7134 completed (loss: 0.08887419104576111, acc: 0.9726775884628296)
[2025-02-13 20:59:43,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:43,355][root][INFO] - Training Epoch: 2/2, step 5147/7134 completed (loss: 0.03566870838403702, acc: 0.9871794581413269)
[2025-02-13 20:59:43,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:43,737][root][INFO] - Training Epoch: 2/2, step 5148/7134 completed (loss: 0.1487579047679901, acc: 0.9624999761581421)
[2025-02-13 20:59:43,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:44,126][root][INFO] - Training Epoch: 2/2, step 5149/7134 completed (loss: 0.03474261611700058, acc: 0.9954751133918762)
[2025-02-13 20:59:44,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:44,509][root][INFO] - Training Epoch: 2/2, step 5150/7134 completed (loss: 0.10023774951696396, acc: 0.970059871673584)
[2025-02-13 20:59:44,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:44,904][root][INFO] - Training Epoch: 2/2, step 5151/7134 completed (loss: 0.09525851905345917, acc: 0.977142870426178)
[2025-02-13 20:59:45,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:45,287][root][INFO] - Training Epoch: 2/2, step 5152/7134 completed (loss: 0.03952353075146675, acc: 0.9882352948188782)
[2025-02-13 20:59:45,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:45,664][root][INFO] - Training Epoch: 2/2, step 5153/7134 completed (loss: 0.04932155832648277, acc: 1.0)
[2025-02-13 20:59:45,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:46,057][root][INFO] - Training Epoch: 2/2, step 5154/7134 completed (loss: 0.05134005472064018, acc: 0.9800000190734863)
[2025-02-13 20:59:46,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:46,467][root][INFO] - Training Epoch: 2/2, step 5155/7134 completed (loss: 0.11757612228393555, acc: 0.970802903175354)
[2025-02-13 20:59:46,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:46,871][root][INFO] - Training Epoch: 2/2, step 5156/7134 completed (loss: 0.05261329934000969, acc: 0.9768518805503845)
[2025-02-13 20:59:47,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:47,268][root][INFO] - Training Epoch: 2/2, step 5157/7134 completed (loss: 0.06069956719875336, acc: 0.9829545617103577)
[2025-02-13 20:59:47,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:47,671][root][INFO] - Training Epoch: 2/2, step 5158/7134 completed (loss: 0.12866882979869843, acc: 0.9679487347602844)
[2025-02-13 20:59:47,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:48,070][root][INFO] - Training Epoch: 2/2, step 5159/7134 completed (loss: 0.09571043401956558, acc: 0.9797979593276978)
[2025-02-13 20:59:48,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:48,440][root][INFO] - Training Epoch: 2/2, step 5160/7134 completed (loss: 0.12269481271505356, acc: 0.9666666388511658)
[2025-02-13 20:59:48,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:48,842][root][INFO] - Training Epoch: 2/2, step 5161/7134 completed (loss: 0.11741340905427933, acc: 0.9677419066429138)
[2025-02-13 20:59:48,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:49,222][root][INFO] - Training Epoch: 2/2, step 5162/7134 completed (loss: 0.06169890984892845, acc: 0.9885714054107666)
[2025-02-13 20:59:49,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:49,603][root][INFO] - Training Epoch: 2/2, step 5163/7134 completed (loss: 0.04963647201657295, acc: 0.9838709831237793)
[2025-02-13 20:59:49,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:50,022][root][INFO] - Training Epoch: 2/2, step 5164/7134 completed (loss: 0.02048211172223091, acc: 1.0)
[2025-02-13 20:59:50,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:50,390][root][INFO] - Training Epoch: 2/2, step 5165/7134 completed (loss: 0.07329186052083969, acc: 0.9818181991577148)
[2025-02-13 20:59:50,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:50,773][root][INFO] - Training Epoch: 2/2, step 5166/7134 completed (loss: 0.06580641120672226, acc: 0.9814814925193787)
[2025-02-13 20:59:50,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:51,155][root][INFO] - Training Epoch: 2/2, step 5167/7134 completed (loss: 0.09746016561985016, acc: 0.9668508172035217)
[2025-02-13 20:59:51,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:51,538][root][INFO] - Training Epoch: 2/2, step 5168/7134 completed (loss: 0.1275271773338318, acc: 0.9651162624359131)
[2025-02-13 20:59:51,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:51,917][root][INFO] - Training Epoch: 2/2, step 5169/7134 completed (loss: 0.10866548120975494, acc: 0.9695122241973877)
[2025-02-13 20:59:52,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:52,294][root][INFO] - Training Epoch: 2/2, step 5170/7134 completed (loss: 0.15659528970718384, acc: 0.9637305736541748)
[2025-02-13 20:59:52,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:52,689][root][INFO] - Training Epoch: 2/2, step 5171/7134 completed (loss: 0.05956433713436127, acc: 0.9837398529052734)
[2025-02-13 20:59:52,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:53,075][root][INFO] - Training Epoch: 2/2, step 5172/7134 completed (loss: 0.1833566278219223, acc: 0.957317054271698)
[2025-02-13 20:59:53,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:53,449][root][INFO] - Training Epoch: 2/2, step 5173/7134 completed (loss: 0.06460994482040405, acc: 0.9938650131225586)
[2025-02-13 20:59:53,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:53,821][root][INFO] - Training Epoch: 2/2, step 5174/7134 completed (loss: 0.10420241206884384, acc: 0.9826589822769165)
[2025-02-13 20:59:53,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:54,193][root][INFO] - Training Epoch: 2/2, step 5175/7134 completed (loss: 0.05371440201997757, acc: 0.9820359349250793)
[2025-02-13 20:59:54,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:54,571][root][INFO] - Training Epoch: 2/2, step 5176/7134 completed (loss: 0.05288555845618248, acc: 0.9802631735801697)
[2025-02-13 20:59:54,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:54,961][root][INFO] - Training Epoch: 2/2, step 5177/7134 completed (loss: 0.14555592834949493, acc: 0.9599999785423279)
[2025-02-13 20:59:55,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:55,324][root][INFO] - Training Epoch: 2/2, step 5178/7134 completed (loss: 0.31403255462646484, acc: 0.9466666579246521)
[2025-02-13 20:59:55,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:55,691][root][INFO] - Training Epoch: 2/2, step 5179/7134 completed (loss: 0.07773296535015106, acc: 0.976190447807312)
[2025-02-13 20:59:55,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:56,066][root][INFO] - Training Epoch: 2/2, step 5180/7134 completed (loss: 0.19173754751682281, acc: 0.9432623982429504)
[2025-02-13 20:59:56,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:56,420][root][INFO] - Training Epoch: 2/2, step 5181/7134 completed (loss: 0.1835707277059555, acc: 0.9550561904907227)
[2025-02-13 20:59:56,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:56,797][root][INFO] - Training Epoch: 2/2, step 5182/7134 completed (loss: 0.2554023563861847, acc: 0.9370078444480896)
[2025-02-13 20:59:56,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:57,154][root][INFO] - Training Epoch: 2/2, step 5183/7134 completed (loss: 0.14495038986206055, acc: 0.9558823704719543)
[2025-02-13 20:59:57,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:57,520][root][INFO] - Training Epoch: 2/2, step 5184/7134 completed (loss: 0.04029388725757599, acc: 0.9903846383094788)
[2025-02-13 20:59:57,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:57,907][root][INFO] - Training Epoch: 2/2, step 5185/7134 completed (loss: 0.10881586372852325, acc: 0.9801980257034302)
[2025-02-13 20:59:58,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:58,270][root][INFO] - Training Epoch: 2/2, step 5186/7134 completed (loss: 0.09503927081823349, acc: 0.9685039520263672)
[2025-02-13 20:59:58,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:58,625][root][INFO] - Training Epoch: 2/2, step 5187/7134 completed (loss: 0.03228960558772087, acc: 0.9919999837875366)
[2025-02-13 20:59:58,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:58,988][root][INFO] - Training Epoch: 2/2, step 5188/7134 completed (loss: 0.07226594537496567, acc: 0.9855072498321533)
[2025-02-13 20:59:59,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:59,307][root][INFO] - Training Epoch: 2/2, step 5189/7134 completed (loss: 0.10743429511785507, acc: 0.9900990128517151)
[2025-02-13 20:59:59,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:59,655][root][INFO] - Training Epoch: 2/2, step 5190/7134 completed (loss: 0.021487532183527946, acc: 0.9909090995788574)
[2025-02-13 20:59:59,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:00,052][root][INFO] - Training Epoch: 2/2, step 5191/7134 completed (loss: 0.1367252767086029, acc: 0.9691358208656311)
[2025-02-13 21:00:00,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:00,434][root][INFO] - Training Epoch: 2/2, step 5192/7134 completed (loss: 0.12517105042934418, acc: 0.9797979593276978)
[2025-02-13 21:00:00,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:00,803][root][INFO] - Training Epoch: 2/2, step 5193/7134 completed (loss: 0.02374330535531044, acc: 1.0)
[2025-02-13 21:00:00,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:01,198][root][INFO] - Training Epoch: 2/2, step 5194/7134 completed (loss: 0.049451086670160294, acc: 0.993630588054657)
[2025-02-13 21:00:01,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:01,572][root][INFO] - Training Epoch: 2/2, step 5195/7134 completed (loss: 0.12945659458637238, acc: 0.9814814925193787)
[2025-02-13 21:00:01,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:01,969][root][INFO] - Training Epoch: 2/2, step 5196/7134 completed (loss: 0.07867976278066635, acc: 0.9800000190734863)
[2025-02-13 21:00:02,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:02,353][root][INFO] - Training Epoch: 2/2, step 5197/7134 completed (loss: 0.11383409798145294, acc: 0.9603174328804016)
[2025-02-13 21:00:02,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:02,718][root][INFO] - Training Epoch: 2/2, step 5198/7134 completed (loss: 0.03927858918905258, acc: 1.0)
[2025-02-13 21:00:02,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:03,108][root][INFO] - Training Epoch: 2/2, step 5199/7134 completed (loss: 0.10936670750379562, acc: 0.9825581312179565)
[2025-02-13 21:00:03,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:03,476][root][INFO] - Training Epoch: 2/2, step 5200/7134 completed (loss: 0.0366954542696476, acc: 0.9920634627342224)
[2025-02-13 21:00:03,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:03,856][root][INFO] - Training Epoch: 2/2, step 5201/7134 completed (loss: 0.10876768082380295, acc: 0.9758453965187073)
[2025-02-13 21:00:03,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:04,230][root][INFO] - Training Epoch: 2/2, step 5202/7134 completed (loss: 0.08860573172569275, acc: 0.984375)
[2025-02-13 21:00:04,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:04,601][root][INFO] - Training Epoch: 2/2, step 5203/7134 completed (loss: 0.06171627342700958, acc: 0.9890109896659851)
[2025-02-13 21:00:04,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:04,946][root][INFO] - Training Epoch: 2/2, step 5204/7134 completed (loss: 0.02432277239859104, acc: 1.0)
[2025-02-13 21:00:05,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:05,326][root][INFO] - Training Epoch: 2/2, step 5205/7134 completed (loss: 0.03482140228152275, acc: 1.0)
[2025-02-13 21:00:05,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:05,715][root][INFO] - Training Epoch: 2/2, step 5206/7134 completed (loss: 0.21783587336540222, acc: 0.9590643048286438)
[2025-02-13 21:00:05,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:06,082][root][INFO] - Training Epoch: 2/2, step 5207/7134 completed (loss: 0.2680676281452179, acc: 0.9285714030265808)
[2025-02-13 21:00:06,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:06,461][root][INFO] - Training Epoch: 2/2, step 5208/7134 completed (loss: 0.0729290023446083, acc: 0.9885057210922241)
[2025-02-13 21:00:06,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:06,824][root][INFO] - Training Epoch: 2/2, step 5209/7134 completed (loss: 0.09823349863290787, acc: 0.969924807548523)
[2025-02-13 21:00:06,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:07,204][root][INFO] - Training Epoch: 2/2, step 5210/7134 completed (loss: 0.08300069719552994, acc: 0.9704142212867737)
[2025-02-13 21:00:07,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:07,576][root][INFO] - Training Epoch: 2/2, step 5211/7134 completed (loss: 0.11019832640886307, acc: 0.976190447807312)
[2025-02-13 21:00:07,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:07,969][root][INFO] - Training Epoch: 2/2, step 5212/7134 completed (loss: 0.08040980994701385, acc: 0.9740259647369385)
[2025-02-13 21:00:08,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:08,329][root][INFO] - Training Epoch: 2/2, step 5213/7134 completed (loss: 0.09855302423238754, acc: 0.9814814925193787)
[2025-02-13 21:00:08,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:08,674][root][INFO] - Training Epoch: 2/2, step 5214/7134 completed (loss: 0.08112873136997223, acc: 0.9915966391563416)
[2025-02-13 21:00:08,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:09,043][root][INFO] - Training Epoch: 2/2, step 5215/7134 completed (loss: 0.1067340150475502, acc: 0.9702970385551453)
[2025-02-13 21:00:09,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:09,341][root][INFO] - Training Epoch: 2/2, step 5216/7134 completed (loss: 0.048552557826042175, acc: 0.9861111044883728)
[2025-02-13 21:00:09,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:09,682][root][INFO] - Training Epoch: 2/2, step 5217/7134 completed (loss: 0.05897805094718933, acc: 0.989130437374115)
[2025-02-13 21:00:09,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:10,006][root][INFO] - Training Epoch: 2/2, step 5218/7134 completed (loss: 0.10865949094295502, acc: 0.9603960514068604)
[2025-02-13 21:00:10,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:10,342][root][INFO] - Training Epoch: 2/2, step 5219/7134 completed (loss: 0.1013927161693573, acc: 0.9814814925193787)
[2025-02-13 21:00:10,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:10,663][root][INFO] - Training Epoch: 2/2, step 5220/7134 completed (loss: 0.0574520006775856, acc: 0.9863013625144958)
[2025-02-13 21:00:10,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:11,011][root][INFO] - Training Epoch: 2/2, step 5221/7134 completed (loss: 0.07922198623418808, acc: 0.9876543283462524)
[2025-02-13 21:00:11,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:11,331][root][INFO] - Training Epoch: 2/2, step 5222/7134 completed (loss: 0.06386610865592957, acc: 0.988095223903656)
[2025-02-13 21:00:11,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:11,674][root][INFO] - Training Epoch: 2/2, step 5223/7134 completed (loss: 0.04784639924764633, acc: 0.982300877571106)
[2025-02-13 21:00:11,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:12,016][root][INFO] - Training Epoch: 2/2, step 5224/7134 completed (loss: 0.07873077690601349, acc: 0.9777777791023254)
[2025-02-13 21:00:12,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:12,379][root][INFO] - Training Epoch: 2/2, step 5225/7134 completed (loss: 0.12859968841075897, acc: 0.9802631735801697)
[2025-02-13 21:00:12,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:12,747][root][INFO] - Training Epoch: 2/2, step 5226/7134 completed (loss: 0.10477113723754883, acc: 0.9807692170143127)
[2025-02-13 21:00:12,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:13,104][root][INFO] - Training Epoch: 2/2, step 5227/7134 completed (loss: 0.08912178128957748, acc: 0.9897435903549194)
[2025-02-13 21:00:13,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:13,458][root][INFO] - Training Epoch: 2/2, step 5228/7134 completed (loss: 0.023346614092588425, acc: 1.0)
[2025-02-13 21:00:13,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:13,823][root][INFO] - Training Epoch: 2/2, step 5229/7134 completed (loss: 0.04374966770410538, acc: 0.9870967864990234)
[2025-02-13 21:00:13,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:14,183][root][INFO] - Training Epoch: 2/2, step 5230/7134 completed (loss: 0.03793903812766075, acc: 0.9944751262664795)
[2025-02-13 21:00:14,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:14,552][root][INFO] - Training Epoch: 2/2, step 5231/7134 completed (loss: 0.04441221430897713, acc: 0.9793814420700073)
[2025-02-13 21:00:14,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:14,929][root][INFO] - Training Epoch: 2/2, step 5232/7134 completed (loss: 0.05582109093666077, acc: 0.9887005686759949)
[2025-02-13 21:00:15,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:15,305][root][INFO] - Training Epoch: 2/2, step 5233/7134 completed (loss: 0.11499940603971481, acc: 0.9678899049758911)
[2025-02-13 21:00:15,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:15,674][root][INFO] - Training Epoch: 2/2, step 5234/7134 completed (loss: 0.05068770796060562, acc: 0.9894737005233765)
[2025-02-13 21:00:15,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:16,040][root][INFO] - Training Epoch: 2/2, step 5235/7134 completed (loss: 0.019536655396223068, acc: 0.9932432174682617)
[2025-02-13 21:00:16,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:16,389][root][INFO] - Training Epoch: 2/2, step 5236/7134 completed (loss: 0.023383120074868202, acc: 0.9927007555961609)
[2025-02-13 21:00:16,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:16,769][root][INFO] - Training Epoch: 2/2, step 5237/7134 completed (loss: 0.03003114089369774, acc: 0.9929577708244324)
[2025-02-13 21:00:16,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:17,143][root][INFO] - Training Epoch: 2/2, step 5238/7134 completed (loss: 0.20671990513801575, acc: 0.9776119589805603)
[2025-02-13 21:00:17,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:17,497][root][INFO] - Training Epoch: 2/2, step 5239/7134 completed (loss: 0.07197176665067673, acc: 0.9663865566253662)
[2025-02-13 21:00:17,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:17,856][root][INFO] - Training Epoch: 2/2, step 5240/7134 completed (loss: 0.03779640048742294, acc: 0.9883720874786377)
[2025-02-13 21:00:17,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:18,216][root][INFO] - Training Epoch: 2/2, step 5241/7134 completed (loss: 0.13987195491790771, acc: 0.9677419066429138)
[2025-02-13 21:00:18,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:18,595][root][INFO] - Training Epoch: 2/2, step 5242/7134 completed (loss: 0.031153468415141106, acc: 0.9934210777282715)
[2025-02-13 21:00:18,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:18,966][root][INFO] - Training Epoch: 2/2, step 5243/7134 completed (loss: 0.03223515674471855, acc: 0.9928571581840515)
[2025-02-13 21:00:19,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:19,319][root][INFO] - Training Epoch: 2/2, step 5244/7134 completed (loss: 0.06234170123934746, acc: 0.9825581312179565)
[2025-02-13 21:00:19,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:19,723][root][INFO] - Training Epoch: 2/2, step 5245/7134 completed (loss: 0.04725685343146324, acc: 0.9920634627342224)
[2025-02-13 21:00:19,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:20,078][root][INFO] - Training Epoch: 2/2, step 5246/7134 completed (loss: 0.08551033586263657, acc: 0.9683544039726257)
[2025-02-13 21:00:20,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:20,437][root][INFO] - Training Epoch: 2/2, step 5247/7134 completed (loss: 0.053046245127916336, acc: 0.9851852059364319)
[2025-02-13 21:00:20,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:20,790][root][INFO] - Training Epoch: 2/2, step 5248/7134 completed (loss: 0.06317108124494553, acc: 0.9847328066825867)
[2025-02-13 21:00:20,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:21,142][root][INFO] - Training Epoch: 2/2, step 5249/7134 completed (loss: 0.03642836958169937, acc: 1.0)
[2025-02-13 21:00:21,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:21,511][root][INFO] - Training Epoch: 2/2, step 5250/7134 completed (loss: 0.05446876212954521, acc: 0.9836956262588501)
[2025-02-13 21:00:21,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:21,871][root][INFO] - Training Epoch: 2/2, step 5251/7134 completed (loss: 0.03782102093100548, acc: 0.9947916865348816)
[2025-02-13 21:00:21,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:22,214][root][INFO] - Training Epoch: 2/2, step 5252/7134 completed (loss: 0.06784235686063766, acc: 0.982300877571106)
[2025-02-13 21:00:22,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:22,584][root][INFO] - Training Epoch: 2/2, step 5253/7134 completed (loss: 0.025763381272554398, acc: 0.9930555820465088)
[2025-02-13 21:00:22,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:22,956][root][INFO] - Training Epoch: 2/2, step 5254/7134 completed (loss: 0.16285920143127441, acc: 0.9510489702224731)
[2025-02-13 21:00:23,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:23,321][root][INFO] - Training Epoch: 2/2, step 5255/7134 completed (loss: 0.08757669478654861, acc: 0.977011501789093)
[2025-02-13 21:00:23,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:23,670][root][INFO] - Training Epoch: 2/2, step 5256/7134 completed (loss: 0.06237814575433731, acc: 0.9866666793823242)
[2025-02-13 21:00:23,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:24,040][root][INFO] - Training Epoch: 2/2, step 5257/7134 completed (loss: 0.13595877587795258, acc: 0.9640718698501587)
[2025-02-13 21:00:24,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:24,426][root][INFO] - Training Epoch: 2/2, step 5258/7134 completed (loss: 0.13887161016464233, acc: 0.9629629850387573)
[2025-02-13 21:00:24,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:24,801][root][INFO] - Training Epoch: 2/2, step 5259/7134 completed (loss: 0.16843974590301514, acc: 0.971222996711731)
[2025-02-13 21:00:24,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:25,169][root][INFO] - Training Epoch: 2/2, step 5260/7134 completed (loss: 0.04621896892786026, acc: 0.9947090148925781)
[2025-02-13 21:00:25,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:25,533][root][INFO] - Training Epoch: 2/2, step 5261/7134 completed (loss: 0.11856862157583237, acc: 0.9720279574394226)
[2025-02-13 21:00:25,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:25,925][root][INFO] - Training Epoch: 2/2, step 5262/7134 completed (loss: 0.032630424946546555, acc: 0.9939758777618408)
[2025-02-13 21:00:26,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:26,282][root][INFO] - Training Epoch: 2/2, step 5263/7134 completed (loss: 0.056569360196590424, acc: 0.9788732528686523)
[2025-02-13 21:00:26,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:26,652][root][INFO] - Training Epoch: 2/2, step 5264/7134 completed (loss: 0.06341437995433807, acc: 0.9795918464660645)
[2025-02-13 21:00:26,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:27,009][root][INFO] - Training Epoch: 2/2, step 5265/7134 completed (loss: 0.06145131215453148, acc: 0.9781420826911926)
[2025-02-13 21:00:27,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:27,367][root][INFO] - Training Epoch: 2/2, step 5266/7134 completed (loss: 0.03254104405641556, acc: 0.9938271641731262)
[2025-02-13 21:00:27,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:27,721][root][INFO] - Training Epoch: 2/2, step 5267/7134 completed (loss: 0.14968106150627136, acc: 0.9567901492118835)
[2025-02-13 21:00:27,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:28,095][root][INFO] - Training Epoch: 2/2, step 5268/7134 completed (loss: 0.09387757629156113, acc: 0.9767441749572754)
[2025-02-13 21:00:28,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:28,459][root][INFO] - Training Epoch: 2/2, step 5269/7134 completed (loss: 0.0677664577960968, acc: 0.9857142567634583)
[2025-02-13 21:00:28,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:28,841][root][INFO] - Training Epoch: 2/2, step 5270/7134 completed (loss: 0.07935384660959244, acc: 0.982758641242981)
[2025-02-13 21:00:28,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:29,214][root][INFO] - Training Epoch: 2/2, step 5271/7134 completed (loss: 0.059031251817941666, acc: 0.9856114983558655)
[2025-02-13 21:00:29,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:29,586][root][INFO] - Training Epoch: 2/2, step 5272/7134 completed (loss: 0.03942827135324478, acc: 0.9942196607589722)
[2025-02-13 21:00:29,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:29,963][root][INFO] - Training Epoch: 2/2, step 5273/7134 completed (loss: 0.10202280431985855, acc: 0.9652777910232544)
[2025-02-13 21:00:30,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:30,320][root][INFO] - Training Epoch: 2/2, step 5274/7134 completed (loss: 0.11767326295375824, acc: 0.9743589758872986)
[2025-02-13 21:00:30,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:30,683][root][INFO] - Training Epoch: 2/2, step 5275/7134 completed (loss: 0.06506592780351639, acc: 0.9918699264526367)
[2025-02-13 21:00:30,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:31,035][root][INFO] - Training Epoch: 2/2, step 5276/7134 completed (loss: 0.09544463455677032, acc: 0.9671052694320679)
[2025-02-13 21:00:31,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:31,380][root][INFO] - Training Epoch: 2/2, step 5277/7134 completed (loss: 0.04708531126379967, acc: 0.987261176109314)
[2025-02-13 21:00:31,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:31,749][root][INFO] - Training Epoch: 2/2, step 5278/7134 completed (loss: 0.10422318428754807, acc: 0.9696969985961914)
[2025-02-13 21:00:31,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:32,133][root][INFO] - Training Epoch: 2/2, step 5279/7134 completed (loss: 0.014004169031977654, acc: 1.0)
[2025-02-13 21:00:32,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:32,501][root][INFO] - Training Epoch: 2/2, step 5280/7134 completed (loss: 0.036257293075323105, acc: 0.9857142567634583)
[2025-02-13 21:00:32,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:32,864][root][INFO] - Training Epoch: 2/2, step 5281/7134 completed (loss: 0.06144900992512703, acc: 0.97826087474823)
[2025-02-13 21:00:32,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:33,219][root][INFO] - Training Epoch: 2/2, step 5282/7134 completed (loss: 0.10059860348701477, acc: 0.9825581312179565)
[2025-02-13 21:00:33,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:33,584][root][INFO] - Training Epoch: 2/2, step 5283/7134 completed (loss: 0.06016882508993149, acc: 0.9890710115432739)
[2025-02-13 21:00:33,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:33,948][root][INFO] - Training Epoch: 2/2, step 5284/7134 completed (loss: 0.18255434930324554, acc: 0.987500011920929)
[2025-02-13 21:00:34,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:34,316][root][INFO] - Training Epoch: 2/2, step 5285/7134 completed (loss: 0.1317129135131836, acc: 0.9569892287254333)
[2025-02-13 21:00:34,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:34,684][root][INFO] - Training Epoch: 2/2, step 5286/7134 completed (loss: 0.10278366506099701, acc: 0.9865771532058716)
[2025-02-13 21:00:34,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:35,048][root][INFO] - Training Epoch: 2/2, step 5287/7134 completed (loss: 0.09576880931854248, acc: 0.9747474789619446)
[2025-02-13 21:00:35,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:35,400][root][INFO] - Training Epoch: 2/2, step 5288/7134 completed (loss: 0.0484277680516243, acc: 0.9937499761581421)
[2025-02-13 21:00:35,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:35,767][root][INFO] - Training Epoch: 2/2, step 5289/7134 completed (loss: 0.05251335725188255, acc: 0.9886363744735718)
[2025-02-13 21:00:35,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:36,149][root][INFO] - Training Epoch: 2/2, step 5290/7134 completed (loss: 0.09689532220363617, acc: 0.9735449552536011)
[2025-02-13 21:00:36,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:36,523][root][INFO] - Training Epoch: 2/2, step 5291/7134 completed (loss: 0.022014226764440536, acc: 0.9945945739746094)
[2025-02-13 21:00:36,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:36,870][root][INFO] - Training Epoch: 2/2, step 5292/7134 completed (loss: 0.031152259558439255, acc: 0.994350254535675)
[2025-02-13 21:00:37,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:37,228][root][INFO] - Training Epoch: 2/2, step 5293/7134 completed (loss: 0.026879262179136276, acc: 0.993630588054657)
[2025-02-13 21:00:37,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:37,605][root][INFO] - Training Epoch: 2/2, step 5294/7134 completed (loss: 0.05255326256155968, acc: 0.9881656765937805)
[2025-02-13 21:00:37,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:37,960][root][INFO] - Training Epoch: 2/2, step 5295/7134 completed (loss: 0.09745912253856659, acc: 0.9922480583190918)
[2025-02-13 21:00:38,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:38,327][root][INFO] - Training Epoch: 2/2, step 5296/7134 completed (loss: 0.04878056421875954, acc: 0.9887005686759949)
[2025-02-13 21:00:38,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:38,689][root][INFO] - Training Epoch: 2/2, step 5297/7134 completed (loss: 0.061441414058208466, acc: 0.9879518151283264)
[2025-02-13 21:00:38,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:39,076][root][INFO] - Training Epoch: 2/2, step 5298/7134 completed (loss: 0.04065548628568649, acc: 0.9929078221321106)
[2025-02-13 21:00:39,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:39,426][root][INFO] - Training Epoch: 2/2, step 5299/7134 completed (loss: 0.03448811545968056, acc: 0.9931507110595703)
[2025-02-13 21:00:39,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:39,801][root][INFO] - Training Epoch: 2/2, step 5300/7134 completed (loss: 0.01749458909034729, acc: 0.9939758777618408)
[2025-02-13 21:00:39,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:40,181][root][INFO] - Training Epoch: 2/2, step 5301/7134 completed (loss: 0.036448948085308075, acc: 0.9817073345184326)
[2025-02-13 21:00:40,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:40,550][root][INFO] - Training Epoch: 2/2, step 5302/7134 completed (loss: 0.11185579001903534, acc: 0.9720670580863953)
[2025-02-13 21:00:40,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:40,913][root][INFO] - Training Epoch: 2/2, step 5303/7134 completed (loss: 0.20093829929828644, acc: 0.9469696879386902)
[2025-02-13 21:00:41,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:41,274][root][INFO] - Training Epoch: 2/2, step 5304/7134 completed (loss: 0.2768992483615875, acc: 0.9452054500579834)
[2025-02-13 21:00:41,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:41,633][root][INFO] - Training Epoch: 2/2, step 5305/7134 completed (loss: 0.09471471607685089, acc: 0.96875)
[2025-02-13 21:00:41,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:42,069][root][INFO] - Training Epoch: 2/2, step 5306/7134 completed (loss: 0.06984437257051468, acc: 0.9833333492279053)
[2025-02-13 21:00:42,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:42,437][root][INFO] - Training Epoch: 2/2, step 5307/7134 completed (loss: 0.2672251760959625, acc: 0.9473684430122375)
[2025-02-13 21:00:42,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:42,790][root][INFO] - Training Epoch: 2/2, step 5308/7134 completed (loss: 0.11097173392772675, acc: 0.9691358208656311)
[2025-02-13 21:00:42,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:43,149][root][INFO] - Training Epoch: 2/2, step 5309/7134 completed (loss: 0.124264657497406, acc: 0.9790209531784058)
[2025-02-13 21:00:43,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:43,517][root][INFO] - Training Epoch: 2/2, step 5310/7134 completed (loss: 0.1421372890472412, acc: 0.9659090638160706)
[2025-02-13 21:00:43,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:43,929][root][INFO] - Training Epoch: 2/2, step 5311/7134 completed (loss: 0.1849154382944107, acc: 0.95652174949646)
[2025-02-13 21:00:44,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:44,282][root][INFO] - Training Epoch: 2/2, step 5312/7134 completed (loss: 0.16835978627204895, acc: 0.9682539701461792)
[2025-02-13 21:00:44,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:44,652][root][INFO] - Training Epoch: 2/2, step 5313/7134 completed (loss: 0.07873863726854324, acc: 0.9918032884597778)
[2025-02-13 21:00:44,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:45,028][root][INFO] - Training Epoch: 2/2, step 5314/7134 completed (loss: 0.19599147140979767, acc: 0.9428571462631226)
[2025-02-13 21:00:45,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:45,406][root][INFO] - Training Epoch: 2/2, step 5315/7134 completed (loss: 0.10092665255069733, acc: 0.9784172773361206)
[2025-02-13 21:00:45,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:45,783][root][INFO] - Training Epoch: 2/2, step 5316/7134 completed (loss: 0.06861031800508499, acc: 0.9933333396911621)
[2025-02-13 21:00:45,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:46,148][root][INFO] - Training Epoch: 2/2, step 5317/7134 completed (loss: 0.05025898665189743, acc: 0.9920634627342224)
[2025-02-13 21:00:46,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:46,497][root][INFO] - Training Epoch: 2/2, step 5318/7134 completed (loss: 0.04609840363264084, acc: 0.993630588054657)
[2025-02-13 21:00:46,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:46,873][root][INFO] - Training Epoch: 2/2, step 5319/7134 completed (loss: 0.14594139158725739, acc: 0.9548872113227844)
[2025-02-13 21:00:47,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:47,248][root][INFO] - Training Epoch: 2/2, step 5320/7134 completed (loss: 0.20396026968955994, acc: 0.9496855139732361)
[2025-02-13 21:00:47,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:47,628][root][INFO] - Training Epoch: 2/2, step 5321/7134 completed (loss: 0.08399680256843567, acc: 0.9805194735527039)
[2025-02-13 21:00:47,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:48,001][root][INFO] - Training Epoch: 2/2, step 5322/7134 completed (loss: 0.04795940965414047, acc: 0.9800000190734863)
[2025-02-13 21:00:48,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:48,379][root][INFO] - Training Epoch: 2/2, step 5323/7134 completed (loss: 0.11149090528488159, acc: 0.9731543660163879)
[2025-02-13 21:00:48,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:48,751][root][INFO] - Training Epoch: 2/2, step 5324/7134 completed (loss: 0.07942953705787659, acc: 0.9802631735801697)
[2025-02-13 21:00:48,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:49,119][root][INFO] - Training Epoch: 2/2, step 5325/7134 completed (loss: 0.09094901382923126, acc: 0.9710144996643066)
[2025-02-13 21:00:49,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:49,487][root][INFO] - Training Epoch: 2/2, step 5326/7134 completed (loss: 0.13769039511680603, acc: 0.9655172228813171)
[2025-02-13 21:00:49,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:49,848][root][INFO] - Training Epoch: 2/2, step 5327/7134 completed (loss: 0.08072080463171005, acc: 0.9612902998924255)
[2025-02-13 21:00:49,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:50,215][root][INFO] - Training Epoch: 2/2, step 5328/7134 completed (loss: 0.04191068559885025, acc: 1.0)
[2025-02-13 21:00:50,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:50,577][root][INFO] - Training Epoch: 2/2, step 5329/7134 completed (loss: 0.13908445835113525, acc: 0.9800000190734863)
[2025-02-13 21:00:50,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:50,942][root][INFO] - Training Epoch: 2/2, step 5330/7134 completed (loss: 0.080106221139431, acc: 0.9873417615890503)
[2025-02-13 21:00:51,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:51,295][root][INFO] - Training Epoch: 2/2, step 5331/7134 completed (loss: 0.0846620500087738, acc: 0.9851852059364319)
[2025-02-13 21:00:51,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:51,663][root][INFO] - Training Epoch: 2/2, step 5332/7134 completed (loss: 0.06468106061220169, acc: 0.9863013625144958)
[2025-02-13 21:00:51,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:52,015][root][INFO] - Training Epoch: 2/2, step 5333/7134 completed (loss: 0.05297427996993065, acc: 0.9868420958518982)
[2025-02-13 21:00:52,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:52,366][root][INFO] - Training Epoch: 2/2, step 5334/7134 completed (loss: 0.09933875501155853, acc: 0.9785714149475098)
[2025-02-13 21:00:52,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:52,722][root][INFO] - Training Epoch: 2/2, step 5335/7134 completed (loss: 0.0523466132581234, acc: 0.9932885766029358)
[2025-02-13 21:00:52,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:53,080][root][INFO] - Training Epoch: 2/2, step 5336/7134 completed (loss: 0.0702570304274559, acc: 0.9929577708244324)
[2025-02-13 21:00:53,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:53,443][root][INFO] - Training Epoch: 2/2, step 5337/7134 completed (loss: 0.09484241902828217, acc: 0.9719626307487488)
[2025-02-13 21:00:53,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:53,819][root][INFO] - Training Epoch: 2/2, step 5338/7134 completed (loss: 0.05490460246801376, acc: 0.9857819676399231)
[2025-02-13 21:00:53,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:54,168][root][INFO] - Training Epoch: 2/2, step 5339/7134 completed (loss: 0.060393448919057846, acc: 0.9890109896659851)
[2025-02-13 21:00:54,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:54,546][root][INFO] - Training Epoch: 2/2, step 5340/7134 completed (loss: 0.15198536217212677, acc: 0.9597315192222595)
[2025-02-13 21:00:54,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:54,902][root][INFO] - Training Epoch: 2/2, step 5341/7134 completed (loss: 0.053140949457883835, acc: 0.9900000095367432)
[2025-02-13 21:00:55,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:55,273][root][INFO] - Training Epoch: 2/2, step 5342/7134 completed (loss: 0.15587200224399567, acc: 0.9578947424888611)
[2025-02-13 21:00:55,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:55,637][root][INFO] - Training Epoch: 2/2, step 5343/7134 completed (loss: 0.14107726514339447, acc: 0.9729729890823364)
[2025-02-13 21:00:55,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:56,005][root][INFO] - Training Epoch: 2/2, step 5344/7134 completed (loss: 0.11957159638404846, acc: 0.9605262875556946)
[2025-02-13 21:00:56,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:56,362][root][INFO] - Training Epoch: 2/2, step 5345/7134 completed (loss: 0.1270461231470108, acc: 0.9595959782600403)
[2025-02-13 21:00:56,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:56,710][root][INFO] - Training Epoch: 2/2, step 5346/7134 completed (loss: 0.09066801518201828, acc: 0.9816513657569885)
[2025-02-13 21:00:57,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:57,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:58,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:58,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:59,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:59,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:59,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:00,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:00,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:00,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:01,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:01,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:01,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:02,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:02,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:03,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:03,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:03,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:04,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:04,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:04,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:05,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:05,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:05,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:06,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:06,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:06,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:07,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:07,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:08,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:08,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:08,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:09,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:09,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:09,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:09,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:10,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:10,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:10,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:11,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:11,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:12,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:12,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:12,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:13,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:13,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:13,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:14,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:14,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:14,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:15,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:15,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:16,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:16,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:16,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:17,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:17,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:17,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:17,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:18,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:18,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:18,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:19,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:19,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:19,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:20,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:20,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:20,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:21,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:21,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:21,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:22,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:22,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:22,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:23,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:23,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:23,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:24,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:24,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:24,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:25,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:25,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:25,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:26,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:26,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:27,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:27,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:27,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:28,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:28,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:28,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:29,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:29,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:29,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:30,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:30,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:30,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:31,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:31,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:31,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:32,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:32,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:32,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:33,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:33,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:33,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:34,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:34,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:34,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:35,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:35,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:35,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:36,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:36,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:37,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:37,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:37,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:38,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:38,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:38,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:39,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:39,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:39,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:40,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:40,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:40,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:41,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:41,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:42,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:42,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:42,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:43,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:43,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:44,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:44,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:44,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:45,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:45,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:45,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:46,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:46,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:46,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:47,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:47,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:47,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:48,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:48,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:48,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:49,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:49,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:49,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:50,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:50,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:50,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:51,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:51,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:51,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:52,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:52,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:53,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:53,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:53,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:54,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:54,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:54,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:55,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:55,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:56,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:56,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:56,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:57,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:57,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:57,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:58,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:58,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:59,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:59,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:59,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:00,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:00,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:00,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:01,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:01,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:01,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:02,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:02,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:02,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:02,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:03,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:03,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:04,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:04,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:04,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:05,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:05,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:05,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:06,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:06,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:06,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:07,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:07,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:07,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:08,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:08,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:08,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:09,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:09,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:09,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:10,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:10,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:10,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:11,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:11,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:12,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:12,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:12,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:13,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:13,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:13,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:13,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:14,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:14,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:15,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:15,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:15,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:16,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:16,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:16,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:17,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:17,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:17,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:18,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:18,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:18,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:19,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:19,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:19,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:20,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:20,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:20,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:21,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:21,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:21,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:22,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:22,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:22,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:23,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:23,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:24,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:24,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:24,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:25,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:25,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:26,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:26,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:26,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:27,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:27,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:27,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:28,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:28,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:28,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:29,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:29,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:29,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:30,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:30,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:30,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:31,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:31,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:31,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:32,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:32,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:33,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:33,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:33,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:34,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:34,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:34,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:35,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:35,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:35,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:36,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:36,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:36,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:37,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:37,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:38,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:38,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:38,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:39,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:39,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:39,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:40,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:40,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:41,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:41,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:41,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:42,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:42,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:42,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:43,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:43,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:43,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:44,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:44,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:44,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:45,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:45,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:45,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:46,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:46,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:46,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:47,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:47,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:47,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:48,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:48,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:48,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:49,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:49,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:49,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:50,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:50,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:51,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:51,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:51,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:52,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:52,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:52,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:53,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:53,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:53,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:54,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:54,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:54,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:55,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:55,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:55,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:56,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:56,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:57,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:57,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:57,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:58,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:58,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:59,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:59,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:59,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:59,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:00,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:00,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:01,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:01,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:01,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:02,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:02,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:02,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:03,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:03,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:04,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:04,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:05,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:05,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:06,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:06,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:06,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:07,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:07,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:07,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:08,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:08,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:08,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:08,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:09,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:09,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:10,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:10,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:10,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:11,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:11,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:11,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:12,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:12,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:12,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:12,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:13,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:13,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:13,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:14,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:14,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:14,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:15,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:15,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:15,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:16,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:16,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:16,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:17,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:17,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:17,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:18,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:18,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:18,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:19,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:19,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:19,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:20,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:20,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:20,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:21,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:21,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:21,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:22,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:22,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:22,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:23,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:23,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:23,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:24,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:24,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:24,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:25,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:25,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:25,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:26,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:26,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:26,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:26,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:27,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:27,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:27,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:28,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:28,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:28,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:29,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:29,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:29,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:30,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:30,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:30,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:31,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:31,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:31,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:32,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:32,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:32,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:33,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:33,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:33,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:34,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:34,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:34,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:35,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:35,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:35,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:36,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:36,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:36,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:37,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:37,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:37,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:38,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:38,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:38,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:39,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:39,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:39,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:40,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:40,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:40,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:41,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:41,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:41,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:42,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:42,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:42,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:43,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:43,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:43,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:44,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:44,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:45,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:45,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:45,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:46,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:46,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:46,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:47,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:47,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:47,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:47,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:48,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:48,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:48,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:49,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:49,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:50,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:50,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:50,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:51,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:51,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:51,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:52,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:52,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:53,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:53,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:53,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:54,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:54,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:54,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:54,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:55,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:55,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:55,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:56,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:56,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:56,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:57,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:57,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:57,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:58,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:58,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:58,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:59,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:59,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:00,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:00,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:00,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:01,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:01,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:01,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:01,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:02,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:02,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:02,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:03,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:03,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:03,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:04,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:04,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:04,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:05,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:05,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:05,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:06,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:06,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:06,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:06,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:07,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:07,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:07,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:08,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:08,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:09,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:09,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:09,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:10,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:10,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:10,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:11,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:11,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:11,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:12,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:12,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:13,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:13,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:13,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:14,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:14,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:14,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:15,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:15,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:15,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:16,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:16,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:16,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:17,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:17,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:17,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:18,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:18,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:19,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:19,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:19,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:20,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:20,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:20,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:21,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:21,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:21,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:22,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:22,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:22,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:22,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:23,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:23,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:23,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:24,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:24,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:24,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:25,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:25,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:25,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:26,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:26,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:27,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:27,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:27,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:27,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:28,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:28,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:28,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:29,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:29,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:30,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:30,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:30,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:31,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:31,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:32,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:32,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:32,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:33,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:33,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:34,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:34,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:35,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:35,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:35,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:36,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:36,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:37,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:37,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:37,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:38,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:38,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:38,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:39,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:39,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:39,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:40,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:40,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:41,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:41,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:41,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:42,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:42,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:42,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:43,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:43,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:43,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:44,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:44,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:45,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:45,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:45,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:46,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:46,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:46,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:46,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:47,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:47,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:47,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:48,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:48,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:48,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:49,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:49,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:49,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:50,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:50,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:50,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:51,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:51,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:52,286][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2299, device='cuda:0') eval_epoch_loss=tensor(0.2069, device='cuda:0') eval_epoch_acc=tensor(0.9509, device='cuda:0')
[2025-02-13 21:04:52,287][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 21:04:52,288][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 21:04:52,594][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_5347_loss_0.20693475008010864/model.pt
[2025-02-13 21:04:52,603][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 21:04:52,604][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.20693475008010864
[2025-02-13 21:04:52,605][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9508558511734009
[2025-02-13 21:04:52,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:53,056][root][INFO] - Training Epoch: 2/2, step 5347/7134 completed (loss: 0.10720323026180267, acc: 0.9731183052062988)
[2025-02-13 21:04:53,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:53,428][root][INFO] - Training Epoch: 2/2, step 5348/7134 completed (loss: 0.17219415307044983, acc: 0.9698795080184937)
[2025-02-13 21:04:53,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:53,809][root][INFO] - Training Epoch: 2/2, step 5349/7134 completed (loss: 0.1334761679172516, acc: 0.9714285731315613)
[2025-02-13 21:04:53,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:54,245][root][INFO] - Training Epoch: 2/2, step 5350/7134 completed (loss: 0.07088460773229599, acc: 0.9764705896377563)
[2025-02-13 21:04:54,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:54,621][root][INFO] - Training Epoch: 2/2, step 5351/7134 completed (loss: 0.10984334349632263, acc: 0.9567567706108093)
[2025-02-13 21:04:54,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:54,996][root][INFO] - Training Epoch: 2/2, step 5352/7134 completed (loss: 0.053149543702602386, acc: 0.9820359349250793)
[2025-02-13 21:04:55,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:55,385][root][INFO] - Training Epoch: 2/2, step 5353/7134 completed (loss: 0.07992479205131531, acc: 0.9747474789619446)
[2025-02-13 21:04:55,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:55,810][root][INFO] - Training Epoch: 2/2, step 5354/7134 completed (loss: 0.10864392668008804, acc: 0.9629629850387573)
[2025-02-13 21:04:55,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:56,199][root][INFO] - Training Epoch: 2/2, step 5355/7134 completed (loss: 0.1371096521615982, acc: 0.9635416865348816)
[2025-02-13 21:04:56,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:56,576][root][INFO] - Training Epoch: 2/2, step 5356/7134 completed (loss: 0.11687419563531876, acc: 0.9627659320831299)
[2025-02-13 21:04:56,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:56,961][root][INFO] - Training Epoch: 2/2, step 5357/7134 completed (loss: 0.06358592957258224, acc: 0.9858155846595764)
[2025-02-13 21:04:57,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:57,359][root][INFO] - Training Epoch: 2/2, step 5358/7134 completed (loss: 0.30891796946525574, acc: 0.9340659379959106)
[2025-02-13 21:04:57,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:57,757][root][INFO] - Training Epoch: 2/2, step 5359/7134 completed (loss: 0.13391701877117157, acc: 0.9704433679580688)
[2025-02-13 21:04:57,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:58,153][root][INFO] - Training Epoch: 2/2, step 5360/7134 completed (loss: 0.05231022462248802, acc: 0.9885057210922241)
[2025-02-13 21:04:58,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:58,542][root][INFO] - Training Epoch: 2/2, step 5361/7134 completed (loss: 0.14002715051174164, acc: 0.9644669890403748)
[2025-02-13 21:04:58,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:58,980][root][INFO] - Training Epoch: 2/2, step 5362/7134 completed (loss: 0.20382818579673767, acc: 0.9729729890823364)
[2025-02-13 21:04:59,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:59,358][root][INFO] - Training Epoch: 2/2, step 5363/7134 completed (loss: 0.08794055879116058, acc: 0.9817073345184326)
[2025-02-13 21:04:59,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:59,753][root][INFO] - Training Epoch: 2/2, step 5364/7134 completed (loss: 0.05547080188989639, acc: 0.9884393215179443)
[2025-02-13 21:04:59,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:00,191][root][INFO] - Training Epoch: 2/2, step 5365/7134 completed (loss: 0.04338425397872925, acc: 0.9924242496490479)
[2025-02-13 21:05:00,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:00,567][root][INFO] - Training Epoch: 2/2, step 5366/7134 completed (loss: 0.19434186816215515, acc: 0.9701492786407471)
[2025-02-13 21:05:00,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:00,940][root][INFO] - Training Epoch: 2/2, step 5367/7134 completed (loss: 0.12889616191387177, acc: 0.9728260636329651)
[2025-02-13 21:05:01,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:01,336][root][INFO] - Training Epoch: 2/2, step 5368/7134 completed (loss: 0.08351083844900131, acc: 0.9689922332763672)
[2025-02-13 21:05:01,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:01,758][root][INFO] - Training Epoch: 2/2, step 5369/7134 completed (loss: 0.10089823603630066, acc: 0.9830508232116699)
[2025-02-13 21:05:01,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:02,146][root][INFO] - Training Epoch: 2/2, step 5370/7134 completed (loss: 0.16562722623348236, acc: 0.9740259647369385)
[2025-02-13 21:05:02,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:02,519][root][INFO] - Training Epoch: 2/2, step 5371/7134 completed (loss: 0.1935551017522812, acc: 0.9599999785423279)
[2025-02-13 21:05:02,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:02,889][root][INFO] - Training Epoch: 2/2, step 5372/7134 completed (loss: 0.03371693566441536, acc: 1.0)
[2025-02-13 21:05:03,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:03,257][root][INFO] - Training Epoch: 2/2, step 5373/7134 completed (loss: 0.04931483417749405, acc: 0.9920634627342224)
[2025-02-13 21:05:03,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:03,589][root][INFO] - Training Epoch: 2/2, step 5374/7134 completed (loss: 0.010063678957521915, acc: 1.0)
[2025-02-13 21:05:03,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:04,026][root][INFO] - Training Epoch: 2/2, step 5375/7134 completed (loss: 0.04834117740392685, acc: 0.9922480583190918)
[2025-02-13 21:05:04,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:04,395][root][INFO] - Training Epoch: 2/2, step 5376/7134 completed (loss: 0.042707763612270355, acc: 0.9797297120094299)
[2025-02-13 21:05:04,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:04,786][root][INFO] - Training Epoch: 2/2, step 5377/7134 completed (loss: 0.055578380823135376, acc: 0.9876543283462524)
[2025-02-13 21:05:04,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:05,164][root][INFO] - Training Epoch: 2/2, step 5378/7134 completed (loss: 0.025264911353588104, acc: 0.9947368502616882)
[2025-02-13 21:05:05,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:05,538][root][INFO] - Training Epoch: 2/2, step 5379/7134 completed (loss: 0.027985133230686188, acc: 0.9895833134651184)
[2025-02-13 21:05:05,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:05,909][root][INFO] - Training Epoch: 2/2, step 5380/7134 completed (loss: 0.06497377157211304, acc: 0.9851852059364319)
[2025-02-13 21:05:06,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:06,286][root][INFO] - Training Epoch: 2/2, step 5381/7134 completed (loss: 0.04797183722257614, acc: 0.9830508232116699)
[2025-02-13 21:05:06,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:06,641][root][INFO] - Training Epoch: 2/2, step 5382/7134 completed (loss: 0.03128645941615105, acc: 0.9855072498321533)
[2025-02-13 21:05:06,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:07,054][root][INFO] - Training Epoch: 2/2, step 5383/7134 completed (loss: 0.049374498426914215, acc: 0.9919354915618896)
[2025-02-13 21:05:07,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:07,469][root][INFO] - Training Epoch: 2/2, step 5384/7134 completed (loss: 0.03398476913571358, acc: 0.9949238300323486)
[2025-02-13 21:05:07,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:07,813][root][INFO] - Training Epoch: 2/2, step 5385/7134 completed (loss: 0.05321839079260826, acc: 0.9883720874786377)
[2025-02-13 21:05:07,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:08,191][root][INFO] - Training Epoch: 2/2, step 5386/7134 completed (loss: 0.022398486733436584, acc: 1.0)
[2025-02-13 21:05:08,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:08,579][root][INFO] - Training Epoch: 2/2, step 5387/7134 completed (loss: 0.01872452348470688, acc: 1.0)
[2025-02-13 21:05:08,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:08,929][root][INFO] - Training Epoch: 2/2, step 5388/7134 completed (loss: 0.01989785209298134, acc: 1.0)
[2025-02-13 21:05:09,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:09,354][root][INFO] - Training Epoch: 2/2, step 5389/7134 completed (loss: 0.015991197898983955, acc: 1.0)
[2025-02-13 21:05:09,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:09,804][root][INFO] - Training Epoch: 2/2, step 5390/7134 completed (loss: 0.07411512732505798, acc: 0.9685534834861755)
[2025-02-13 21:05:09,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:10,187][root][INFO] - Training Epoch: 2/2, step 5391/7134 completed (loss: 0.04935663565993309, acc: 0.9928057789802551)
[2025-02-13 21:05:10,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:10,609][root][INFO] - Training Epoch: 2/2, step 5392/7134 completed (loss: 0.08057239651679993, acc: 0.9886363744735718)
[2025-02-13 21:05:10,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:11,021][root][INFO] - Training Epoch: 2/2, step 5393/7134 completed (loss: 0.0839909017086029, acc: 0.9750000238418579)
[2025-02-13 21:05:11,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:11,386][root][INFO] - Training Epoch: 2/2, step 5394/7134 completed (loss: 0.20325621962547302, acc: 0.96875)
[2025-02-13 21:05:11,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:11,824][root][INFO] - Training Epoch: 2/2, step 5395/7134 completed (loss: 0.04321826994419098, acc: 0.9932432174682617)
[2025-02-13 21:05:12,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:12,235][root][INFO] - Training Epoch: 2/2, step 5396/7134 completed (loss: 0.034352052956819534, acc: 1.0)
[2025-02-13 21:05:12,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:12,614][root][INFO] - Training Epoch: 2/2, step 5397/7134 completed (loss: 0.05915599688887596, acc: 0.9716312289237976)
[2025-02-13 21:05:12,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:13,007][root][INFO] - Training Epoch: 2/2, step 5398/7134 completed (loss: 0.08375836163759232, acc: 0.9759036302566528)
[2025-02-13 21:05:13,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:13,357][root][INFO] - Training Epoch: 2/2, step 5399/7134 completed (loss: 0.07608847320079803, acc: 0.9714285731315613)
[2025-02-13 21:05:13,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:13,714][root][INFO] - Training Epoch: 2/2, step 5400/7134 completed (loss: 0.19207175076007843, acc: 0.9724137783050537)
[2025-02-13 21:05:13,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:14,099][root][INFO] - Training Epoch: 2/2, step 5401/7134 completed (loss: 0.06370201706886292, acc: 0.9861111044883728)
[2025-02-13 21:05:14,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:14,491][root][INFO] - Training Epoch: 2/2, step 5402/7134 completed (loss: 0.021701877936720848, acc: 1.0)
[2025-02-13 21:05:14,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:14,802][root][INFO] - Training Epoch: 2/2, step 5403/7134 completed (loss: 0.10930114984512329, acc: 0.9836065769195557)
[2025-02-13 21:05:14,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:15,176][root][INFO] - Training Epoch: 2/2, step 5404/7134 completed (loss: 0.18151627480983734, acc: 0.9664429426193237)
[2025-02-13 21:05:15,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:15,560][root][INFO] - Training Epoch: 2/2, step 5405/7134 completed (loss: 0.035088542848825455, acc: 0.991304337978363)
[2025-02-13 21:05:15,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:15,939][root][INFO] - Training Epoch: 2/2, step 5406/7134 completed (loss: 0.06850587576627731, acc: 0.9819819927215576)
[2025-02-13 21:05:16,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:16,270][root][INFO] - Training Epoch: 2/2, step 5407/7134 completed (loss: 0.03799992799758911, acc: 1.0)
[2025-02-13 21:05:16,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:16,666][root][INFO] - Training Epoch: 2/2, step 5408/7134 completed (loss: 0.15823493897914886, acc: 0.9716312289237976)
[2025-02-13 21:05:16,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:17,026][root][INFO] - Training Epoch: 2/2, step 5409/7134 completed (loss: 0.21106091141700745, acc: 0.9720279574394226)
[2025-02-13 21:05:17,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:17,398][root][INFO] - Training Epoch: 2/2, step 5410/7134 completed (loss: 0.0520436055958271, acc: 0.9879518151283264)
[2025-02-13 21:05:17,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:17,761][root][INFO] - Training Epoch: 2/2, step 5411/7134 completed (loss: 0.09721200913190842, acc: 0.9788732528686523)
[2025-02-13 21:05:17,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:18,153][root][INFO] - Training Epoch: 2/2, step 5412/7134 completed (loss: 0.16983596980571747, acc: 0.982300877571106)
[2025-02-13 21:05:18,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:18,511][root][INFO] - Training Epoch: 2/2, step 5413/7134 completed (loss: 0.1212148368358612, acc: 0.9583333134651184)
[2025-02-13 21:05:18,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:18,881][root][INFO] - Training Epoch: 2/2, step 5414/7134 completed (loss: 0.20080961287021637, acc: 0.9341317415237427)
[2025-02-13 21:05:19,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:19,212][root][INFO] - Training Epoch: 2/2, step 5415/7134 completed (loss: 0.16321787238121033, acc: 0.9805194735527039)
[2025-02-13 21:05:19,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:19,608][root][INFO] - Training Epoch: 2/2, step 5416/7134 completed (loss: 0.13021725416183472, acc: 0.9679144620895386)
[2025-02-13 21:05:19,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:19,991][root][INFO] - Training Epoch: 2/2, step 5417/7134 completed (loss: 0.03990525007247925, acc: 0.9935897588729858)
[2025-02-13 21:05:20,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:20,323][root][INFO] - Training Epoch: 2/2, step 5418/7134 completed (loss: 0.1241278350353241, acc: 0.9375)
[2025-02-13 21:05:20,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:20,687][root][INFO] - Training Epoch: 2/2, step 5419/7134 completed (loss: 0.21747614443302155, acc: 0.9343065619468689)
[2025-02-13 21:05:20,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:21,055][root][INFO] - Training Epoch: 2/2, step 5420/7134 completed (loss: 0.2122712880373001, acc: 0.9624060392379761)
[2025-02-13 21:05:21,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:21,399][root][INFO] - Training Epoch: 2/2, step 5421/7134 completed (loss: 0.15789344906806946, acc: 0.9437500238418579)
[2025-02-13 21:05:21,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:21,789][root][INFO] - Training Epoch: 2/2, step 5422/7134 completed (loss: 0.055619094520807266, acc: 1.0)
[2025-02-13 21:05:21,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:22,184][root][INFO] - Training Epoch: 2/2, step 5423/7134 completed (loss: 0.049670521169900894, acc: 0.987500011920929)
[2025-02-13 21:05:22,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:22,558][root][INFO] - Training Epoch: 2/2, step 5424/7134 completed (loss: 0.0730455294251442, acc: 0.9769230484962463)
[2025-02-13 21:05:22,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:22,975][root][INFO] - Training Epoch: 2/2, step 5425/7134 completed (loss: 0.0833481177687645, acc: 0.9765625)
[2025-02-13 21:05:23,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:23,348][root][INFO] - Training Epoch: 2/2, step 5426/7134 completed (loss: 0.1346537172794342, acc: 0.9836065769195557)
[2025-02-13 21:05:23,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:23,766][root][INFO] - Training Epoch: 2/2, step 5427/7134 completed (loss: 0.046623919159173965, acc: 1.0)
[2025-02-13 21:05:23,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:24,160][root][INFO] - Training Epoch: 2/2, step 5428/7134 completed (loss: 0.05657980591058731, acc: 0.9862068891525269)
[2025-02-13 21:05:24,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:24,549][root][INFO] - Training Epoch: 2/2, step 5429/7134 completed (loss: 0.06464657187461853, acc: 0.9798657894134521)
[2025-02-13 21:05:24,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:24,921][root][INFO] - Training Epoch: 2/2, step 5430/7134 completed (loss: 0.13523177802562714, acc: 0.9740259647369385)
[2025-02-13 21:05:25,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:25,288][root][INFO] - Training Epoch: 2/2, step 5431/7134 completed (loss: 0.08118898421525955, acc: 0.9624060392379761)
[2025-02-13 21:05:25,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:25,724][root][INFO] - Training Epoch: 2/2, step 5432/7134 completed (loss: 0.04110662266612053, acc: 0.9765625)
[2025-02-13 21:05:25,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:26,137][root][INFO] - Training Epoch: 2/2, step 5433/7134 completed (loss: 0.14521478116512299, acc: 0.9831932783126831)
[2025-02-13 21:05:26,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:26,531][root][INFO] - Training Epoch: 2/2, step 5434/7134 completed (loss: 0.06934770941734314, acc: 0.9677419066429138)
[2025-02-13 21:05:26,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:26,904][root][INFO] - Training Epoch: 2/2, step 5435/7134 completed (loss: 0.06766896694898605, acc: 0.9919999837875366)
[2025-02-13 21:05:27,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:27,259][root][INFO] - Training Epoch: 2/2, step 5436/7134 completed (loss: 0.0683882087469101, acc: 0.9741935729980469)
[2025-02-13 21:05:27,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:27,649][root][INFO] - Training Epoch: 2/2, step 5437/7134 completed (loss: 0.07179182767868042, acc: 0.9849624037742615)
[2025-02-13 21:05:27,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:28,035][root][INFO] - Training Epoch: 2/2, step 5438/7134 completed (loss: 0.10890626162290573, acc: 0.9666666388511658)
[2025-02-13 21:05:28,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:28,444][root][INFO] - Training Epoch: 2/2, step 5439/7134 completed (loss: 0.16664153337478638, acc: 0.9636363387107849)
[2025-02-13 21:05:28,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:28,828][root][INFO] - Training Epoch: 2/2, step 5440/7134 completed (loss: 0.12564700841903687, acc: 0.9666666388511658)
[2025-02-13 21:05:28,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:29,222][root][INFO] - Training Epoch: 2/2, step 5441/7134 completed (loss: 0.09569476544857025, acc: 0.9783783555030823)
[2025-02-13 21:05:29,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:29,676][root][INFO] - Training Epoch: 2/2, step 5442/7134 completed (loss: 0.06517194956541061, acc: 0.988095223903656)
[2025-02-13 21:05:29,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:30,037][root][INFO] - Training Epoch: 2/2, step 5443/7134 completed (loss: 0.059435877948999405, acc: 0.9939024448394775)
[2025-02-13 21:05:30,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:30,408][root][INFO] - Training Epoch: 2/2, step 5444/7134 completed (loss: 0.059795793145895004, acc: 0.9779411554336548)
[2025-02-13 21:05:30,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:30,856][root][INFO] - Training Epoch: 2/2, step 5445/7134 completed (loss: 0.12657897174358368, acc: 0.9779411554336548)
[2025-02-13 21:05:31,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:31,241][root][INFO] - Training Epoch: 2/2, step 5446/7134 completed (loss: 0.06709984689950943, acc: 0.9815950989723206)
[2025-02-13 21:05:31,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:31,614][root][INFO] - Training Epoch: 2/2, step 5447/7134 completed (loss: 0.0772474855184555, acc: 0.9726027250289917)
[2025-02-13 21:05:31,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:31,993][root][INFO] - Training Epoch: 2/2, step 5448/7134 completed (loss: 0.12664157152175903, acc: 0.9670329689979553)
[2025-02-13 21:05:32,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:32,437][root][INFO] - Training Epoch: 2/2, step 5449/7134 completed (loss: 0.08275267481803894, acc: 0.9836065769195557)
[2025-02-13 21:05:32,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:32,842][root][INFO] - Training Epoch: 2/2, step 5450/7134 completed (loss: 0.07856433838605881, acc: 0.9777777791023254)
[2025-02-13 21:05:32,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:33,262][root][INFO] - Training Epoch: 2/2, step 5451/7134 completed (loss: 0.03648250177502632, acc: 0.9940828680992126)
[2025-02-13 21:05:33,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:33,678][root][INFO] - Training Epoch: 2/2, step 5452/7134 completed (loss: 0.0546112060546875, acc: 0.9822485446929932)
[2025-02-13 21:05:33,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:34,115][root][INFO] - Training Epoch: 2/2, step 5453/7134 completed (loss: 0.1413644701242447, acc: 0.9777777791023254)
[2025-02-13 21:05:34,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:34,512][root][INFO] - Training Epoch: 2/2, step 5454/7134 completed (loss: 0.08074302971363068, acc: 0.9945651888847351)
[2025-02-13 21:05:34,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:34,995][root][INFO] - Training Epoch: 2/2, step 5455/7134 completed (loss: 0.10986892879009247, acc: 0.9839572310447693)
[2025-02-13 21:05:35,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:35,396][root][INFO] - Training Epoch: 2/2, step 5456/7134 completed (loss: 0.07400036603212357, acc: 0.9784946441650391)
[2025-02-13 21:05:35,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:35,810][root][INFO] - Training Epoch: 2/2, step 5457/7134 completed (loss: 0.08854172378778458, acc: 0.9885057210922241)
[2025-02-13 21:05:35,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:36,266][root][INFO] - Training Epoch: 2/2, step 5458/7134 completed (loss: 0.08449438214302063, acc: 0.9944751262664795)
[2025-02-13 21:05:36,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:36,715][root][INFO] - Training Epoch: 2/2, step 5459/7134 completed (loss: 0.03602439910173416, acc: 0.9815950989723206)
[2025-02-13 21:05:36,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:37,132][root][INFO] - Training Epoch: 2/2, step 5460/7134 completed (loss: 0.06339874863624573, acc: 0.9822485446929932)
[2025-02-13 21:05:37,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:37,506][root][INFO] - Training Epoch: 2/2, step 5461/7134 completed (loss: 0.11599394679069519, acc: 0.963302731513977)
[2025-02-13 21:05:37,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:37,878][root][INFO] - Training Epoch: 2/2, step 5462/7134 completed (loss: 0.04127953201532364, acc: 0.9938650131225586)
[2025-02-13 21:05:38,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:38,240][root][INFO] - Training Epoch: 2/2, step 5463/7134 completed (loss: 0.07532592117786407, acc: 0.9691358208656311)
[2025-02-13 21:05:38,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:38,616][root][INFO] - Training Epoch: 2/2, step 5464/7134 completed (loss: 0.08365673571825027, acc: 0.9781420826911926)
[2025-02-13 21:05:38,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:38,990][root][INFO] - Training Epoch: 2/2, step 5465/7134 completed (loss: 0.07563517242670059, acc: 0.981249988079071)
[2025-02-13 21:05:39,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:39,369][root][INFO] - Training Epoch: 2/2, step 5466/7134 completed (loss: 0.0826835185289383, acc: 0.9751243591308594)
[2025-02-13 21:05:39,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:39,756][root][INFO] - Training Epoch: 2/2, step 5467/7134 completed (loss: 0.03835131973028183, acc: 0.9950248599052429)
[2025-02-13 21:05:39,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:40,136][root][INFO] - Training Epoch: 2/2, step 5468/7134 completed (loss: 0.026159420609474182, acc: 0.9947643876075745)
[2025-02-13 21:05:40,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:40,546][root][INFO] - Training Epoch: 2/2, step 5469/7134 completed (loss: 0.08898889273405075, acc: 0.9682539701461792)
[2025-02-13 21:05:40,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:40,902][root][INFO] - Training Epoch: 2/2, step 5470/7134 completed (loss: 0.017028409987688065, acc: 1.0)
[2025-02-13 21:05:41,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:41,245][root][INFO] - Training Epoch: 2/2, step 5471/7134 completed (loss: 0.02955791726708412, acc: 0.9941176176071167)
[2025-02-13 21:05:41,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:41,650][root][INFO] - Training Epoch: 2/2, step 5472/7134 completed (loss: 0.03112187422811985, acc: 0.994413435459137)
[2025-02-13 21:05:41,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:42,045][root][INFO] - Training Epoch: 2/2, step 5473/7134 completed (loss: 0.019085047766566277, acc: 0.9949495196342468)
[2025-02-13 21:05:42,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:42,422][root][INFO] - Training Epoch: 2/2, step 5474/7134 completed (loss: 0.03957033157348633, acc: 0.9878787994384766)
[2025-02-13 21:05:42,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:42,824][root][INFO] - Training Epoch: 2/2, step 5475/7134 completed (loss: 0.04804026708006859, acc: 0.9814814925193787)
[2025-02-13 21:05:42,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:43,169][root][INFO] - Training Epoch: 2/2, step 5476/7134 completed (loss: 0.004063095431774855, acc: 1.0)
[2025-02-13 21:05:43,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:43,538][root][INFO] - Training Epoch: 2/2, step 5477/7134 completed (loss: 0.019663115963339806, acc: 1.0)
[2025-02-13 21:05:43,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:43,916][root][INFO] - Training Epoch: 2/2, step 5478/7134 completed (loss: 0.0546257421374321, acc: 0.9851852059364319)
[2025-02-13 21:05:44,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:44,291][root][INFO] - Training Epoch: 2/2, step 5479/7134 completed (loss: 0.09441160410642624, acc: 0.9797297120094299)
[2025-02-13 21:05:44,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:44,658][root][INFO] - Training Epoch: 2/2, step 5480/7134 completed (loss: 0.10821940749883652, acc: 0.9702380895614624)
[2025-02-13 21:05:44,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:45,010][root][INFO] - Training Epoch: 2/2, step 5481/7134 completed (loss: 0.053986772894859314, acc: 0.9836065769195557)
[2025-02-13 21:05:45,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:45,599][root][INFO] - Training Epoch: 2/2, step 5482/7134 completed (loss: 0.20785510540008545, acc: 0.9587628841400146)
[2025-02-13 21:05:45,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:46,016][root][INFO] - Training Epoch: 2/2, step 5483/7134 completed (loss: 0.08951041847467422, acc: 0.9757575988769531)
[2025-02-13 21:05:46,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:46,403][root][INFO] - Training Epoch: 2/2, step 5484/7134 completed (loss: 0.06723392754793167, acc: 0.9876543283462524)
[2025-02-13 21:05:46,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:46,816][root][INFO] - Training Epoch: 2/2, step 5485/7134 completed (loss: 0.09835844486951828, acc: 0.975806474685669)
[2025-02-13 21:05:46,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:47,206][root][INFO] - Training Epoch: 2/2, step 5486/7134 completed (loss: 0.0753689631819725, acc: 0.9772727489471436)
[2025-02-13 21:05:47,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:47,602][root][INFO] - Training Epoch: 2/2, step 5487/7134 completed (loss: 0.060971565544605255, acc: 0.9795918464660645)
[2025-02-13 21:05:47,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:48,016][root][INFO] - Training Epoch: 2/2, step 5488/7134 completed (loss: 0.09722261130809784, acc: 0.976047933101654)
[2025-02-13 21:05:48,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:48,398][root][INFO] - Training Epoch: 2/2, step 5489/7134 completed (loss: 0.1820470541715622, acc: 0.9512194991111755)
[2025-02-13 21:05:48,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:48,720][root][INFO] - Training Epoch: 2/2, step 5490/7134 completed (loss: 0.045623332262039185, acc: 0.9898989796638489)
[2025-02-13 21:05:48,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:49,081][root][INFO] - Training Epoch: 2/2, step 5491/7134 completed (loss: 0.06649086624383926, acc: 0.9824561476707458)
[2025-02-13 21:05:49,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:49,434][root][INFO] - Training Epoch: 2/2, step 5492/7134 completed (loss: 0.06986847519874573, acc: 0.9850746393203735)
[2025-02-13 21:05:49,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:49,810][root][INFO] - Training Epoch: 2/2, step 5493/7134 completed (loss: 0.15790602564811707, acc: 0.9635416865348816)
[2025-02-13 21:05:49,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:50,231][root][INFO] - Training Epoch: 2/2, step 5494/7134 completed (loss: 0.06202097237110138, acc: 0.9874213933944702)
[2025-02-13 21:05:50,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:50,619][root][INFO] - Training Epoch: 2/2, step 5495/7134 completed (loss: 0.03346879780292511, acc: 1.0)
[2025-02-13 21:05:50,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:51,011][root][INFO] - Training Epoch: 2/2, step 5496/7134 completed (loss: 0.020380329340696335, acc: 1.0)
[2025-02-13 21:05:51,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:51,395][root][INFO] - Training Epoch: 2/2, step 5497/7134 completed (loss: 0.10080224275588989, acc: 0.970588207244873)
[2025-02-13 21:05:51,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:51,797][root][INFO] - Training Epoch: 2/2, step 5498/7134 completed (loss: 0.12758032977581024, acc: 0.9627659320831299)
[2025-02-13 21:05:51,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:52,202][root][INFO] - Training Epoch: 2/2, step 5499/7134 completed (loss: 0.07488100975751877, acc: 0.9819819927215576)
[2025-02-13 21:05:52,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:52,592][root][INFO] - Training Epoch: 2/2, step 5500/7134 completed (loss: 0.061898160725831985, acc: 0.9792746305465698)
[2025-02-13 21:05:52,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:52,957][root][INFO] - Training Epoch: 2/2, step 5501/7134 completed (loss: 0.10355213284492493, acc: 0.9696969985961914)
[2025-02-13 21:05:53,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:53,345][root][INFO] - Training Epoch: 2/2, step 5502/7134 completed (loss: 0.07982398569583893, acc: 0.9774011373519897)
[2025-02-13 21:05:53,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:53,713][root][INFO] - Training Epoch: 2/2, step 5503/7134 completed (loss: 0.1173446998000145, acc: 0.9846153855323792)
[2025-02-13 21:05:53,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:54,091][root][INFO] - Training Epoch: 2/2, step 5504/7134 completed (loss: 0.02944079041481018, acc: 0.9879518151283264)
[2025-02-13 21:05:54,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:54,450][root][INFO] - Training Epoch: 2/2, step 5505/7134 completed (loss: 0.1694398671388626, acc: 0.9659090638160706)
[2025-02-13 21:05:54,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:54,814][root][INFO] - Training Epoch: 2/2, step 5506/7134 completed (loss: 0.09089537709951401, acc: 0.9797979593276978)
[2025-02-13 21:05:54,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:55,175][root][INFO] - Training Epoch: 2/2, step 5507/7134 completed (loss: 0.06987123936414719, acc: 0.9824561476707458)
[2025-02-13 21:05:55,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:55,545][root][INFO] - Training Epoch: 2/2, step 5508/7134 completed (loss: 0.05660906806588173, acc: 0.9952152967453003)
[2025-02-13 21:05:55,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:55,913][root][INFO] - Training Epoch: 2/2, step 5509/7134 completed (loss: 0.13735699653625488, acc: 0.971563994884491)
[2025-02-13 21:05:56,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:56,289][root][INFO] - Training Epoch: 2/2, step 5510/7134 completed (loss: 0.11679064482450485, acc: 0.9677419066429138)
[2025-02-13 21:05:56,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:56,673][root][INFO] - Training Epoch: 2/2, step 5511/7134 completed (loss: 0.16614744067192078, acc: 0.9660193920135498)
[2025-02-13 21:05:56,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:57,034][root][INFO] - Training Epoch: 2/2, step 5512/7134 completed (loss: 0.13567017018795013, acc: 0.9560439586639404)
[2025-02-13 21:05:57,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:57,403][root][INFO] - Training Epoch: 2/2, step 5513/7134 completed (loss: 0.10751239210367203, acc: 0.9704142212867737)
[2025-02-13 21:05:57,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:57,782][root][INFO] - Training Epoch: 2/2, step 5514/7134 completed (loss: 0.17592506110668182, acc: 0.9583333134651184)
[2025-02-13 21:05:57,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:58,151][root][INFO] - Training Epoch: 2/2, step 5515/7134 completed (loss: 0.2783355116844177, acc: 0.9642857313156128)
[2025-02-13 21:05:58,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:58,555][root][INFO] - Training Epoch: 2/2, step 5516/7134 completed (loss: 0.10458210855722427, acc: 0.9885057210922241)
[2025-02-13 21:05:58,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:58,915][root][INFO] - Training Epoch: 2/2, step 5517/7134 completed (loss: 0.07986214011907578, acc: 0.9888268113136292)
[2025-02-13 21:05:59,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:59,290][root][INFO] - Training Epoch: 2/2, step 5518/7134 completed (loss: 0.06706629693508148, acc: 0.9777777791023254)
[2025-02-13 21:05:59,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:59,660][root][INFO] - Training Epoch: 2/2, step 5519/7134 completed (loss: 0.08075851202011108, acc: 0.9753086566925049)
[2025-02-13 21:05:59,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:00,021][root][INFO] - Training Epoch: 2/2, step 5520/7134 completed (loss: 0.06749606132507324, acc: 0.9905213117599487)
[2025-02-13 21:06:00,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:00,400][root][INFO] - Training Epoch: 2/2, step 5521/7134 completed (loss: 0.1079171895980835, acc: 0.9738219976425171)
[2025-02-13 21:06:00,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:00,770][root][INFO] - Training Epoch: 2/2, step 5522/7134 completed (loss: 0.12534672021865845, acc: 0.963350772857666)
[2025-02-13 21:06:00,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:01,110][root][INFO] - Training Epoch: 2/2, step 5523/7134 completed (loss: 0.0968976691365242, acc: 0.9670329689979553)
[2025-02-13 21:06:01,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:01,456][root][INFO] - Training Epoch: 2/2, step 5524/7134 completed (loss: 0.1566838175058365, acc: 0.9620253443717957)
[2025-02-13 21:06:01,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:01,820][root][INFO] - Training Epoch: 2/2, step 5525/7134 completed (loss: 0.04477696865797043, acc: 0.9878048896789551)
[2025-02-13 21:06:01,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:02,182][root][INFO] - Training Epoch: 2/2, step 5526/7134 completed (loss: 0.05151665583252907, acc: 0.9813664555549622)
[2025-02-13 21:06:02,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:02,620][root][INFO] - Training Epoch: 2/2, step 5527/7134 completed (loss: 0.03859192878007889, acc: 0.9930070042610168)
[2025-02-13 21:06:02,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:02,991][root][INFO] - Training Epoch: 2/2, step 5528/7134 completed (loss: 0.07379579544067383, acc: 0.9848484992980957)
[2025-02-13 21:06:03,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:03,366][root][INFO] - Training Epoch: 2/2, step 5529/7134 completed (loss: 0.10725800693035126, acc: 0.9636363387107849)
[2025-02-13 21:06:03,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:03,805][root][INFO] - Training Epoch: 2/2, step 5530/7134 completed (loss: 0.06603170186281204, acc: 0.9863945841789246)
[2025-02-13 21:06:03,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:04,209][root][INFO] - Training Epoch: 2/2, step 5531/7134 completed (loss: 0.06912785768508911, acc: 0.9939758777618408)
[2025-02-13 21:06:04,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:04,620][root][INFO] - Training Epoch: 2/2, step 5532/7134 completed (loss: 0.08804137259721756, acc: 0.9774011373519897)
[2025-02-13 21:06:04,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:05,030][root][INFO] - Training Epoch: 2/2, step 5533/7134 completed (loss: 0.02916240692138672, acc: 0.9888888597488403)
[2025-02-13 21:06:05,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:05,433][root][INFO] - Training Epoch: 2/2, step 5534/7134 completed (loss: 0.12861108779907227, acc: 0.966292142868042)
[2025-02-13 21:06:05,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:05,842][root][INFO] - Training Epoch: 2/2, step 5535/7134 completed (loss: 0.041963301599025726, acc: 0.9896373152732849)
[2025-02-13 21:06:05,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:06,230][root][INFO] - Training Epoch: 2/2, step 5536/7134 completed (loss: 0.0682193711400032, acc: 0.9890710115432739)
[2025-02-13 21:06:06,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:06,599][root][INFO] - Training Epoch: 2/2, step 5537/7134 completed (loss: 0.040483761578798294, acc: 0.9885057210922241)
[2025-02-13 21:06:06,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:07,022][root][INFO] - Training Epoch: 2/2, step 5538/7134 completed (loss: 0.07496453821659088, acc: 0.9842932224273682)
[2025-02-13 21:06:07,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:07,435][root][INFO] - Training Epoch: 2/2, step 5539/7134 completed (loss: 0.07738010585308075, acc: 0.9781420826911926)
[2025-02-13 21:06:07,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:07,842][root][INFO] - Training Epoch: 2/2, step 5540/7134 completed (loss: 0.1096397340297699, acc: 0.9759036302566528)
[2025-02-13 21:06:07,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:08,202][root][INFO] - Training Epoch: 2/2, step 5541/7134 completed (loss: 0.03181162476539612, acc: 1.0)
[2025-02-13 21:06:08,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:08,555][root][INFO] - Training Epoch: 2/2, step 5542/7134 completed (loss: 0.014480415731668472, acc: 1.0)
[2025-02-13 21:06:08,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:08,932][root][INFO] - Training Epoch: 2/2, step 5543/7134 completed (loss: 0.06029466167092323, acc: 0.9893048405647278)
[2025-02-13 21:06:09,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:09,351][root][INFO] - Training Epoch: 2/2, step 5544/7134 completed (loss: 0.024236438795924187, acc: 0.994535505771637)
[2025-02-13 21:06:09,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:09,756][root][INFO] - Training Epoch: 2/2, step 5545/7134 completed (loss: 0.06212134659290314, acc: 0.988095223903656)
[2025-02-13 21:06:09,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:10,187][root][INFO] - Training Epoch: 2/2, step 5546/7134 completed (loss: 0.03180467709898949, acc: 1.0)
[2025-02-13 21:06:10,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:10,590][root][INFO] - Training Epoch: 2/2, step 5547/7134 completed (loss: 0.028021393343806267, acc: 1.0)
[2025-02-13 21:06:10,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:11,009][root][INFO] - Training Epoch: 2/2, step 5548/7134 completed (loss: 0.034653980284929276, acc: 0.9941176176071167)
[2025-02-13 21:06:11,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:11,402][root][INFO] - Training Epoch: 2/2, step 5549/7134 completed (loss: 0.08496973663568497, acc: 0.9798657894134521)
[2025-02-13 21:06:11,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:11,786][root][INFO] - Training Epoch: 2/2, step 5550/7134 completed (loss: 0.0436449870467186, acc: 0.9887640476226807)
[2025-02-13 21:06:11,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:12,152][root][INFO] - Training Epoch: 2/2, step 5551/7134 completed (loss: 0.16162221133708954, acc: 0.9677419066429138)
[2025-02-13 21:06:12,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:12,525][root][INFO] - Training Epoch: 2/2, step 5552/7134 completed (loss: 0.039379678666591644, acc: 0.9938650131225586)
[2025-02-13 21:06:12,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:12,894][root][INFO] - Training Epoch: 2/2, step 5553/7134 completed (loss: 0.09823314845561981, acc: 0.976331353187561)
[2025-02-13 21:06:13,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:13,256][root][INFO] - Training Epoch: 2/2, step 5554/7134 completed (loss: 0.041077543050050735, acc: 0.993630588054657)
[2025-02-13 21:06:13,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:13,621][root][INFO] - Training Epoch: 2/2, step 5555/7134 completed (loss: 0.03286783769726753, acc: 1.0)
[2025-02-13 21:06:13,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:13,988][root][INFO] - Training Epoch: 2/2, step 5556/7134 completed (loss: 0.07128968834877014, acc: 0.9720670580863953)
[2025-02-13 21:06:14,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:14,362][root][INFO] - Training Epoch: 2/2, step 5557/7134 completed (loss: 0.06087564304471016, acc: 0.9875776171684265)
[2025-02-13 21:06:14,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:14,796][root][INFO] - Training Epoch: 2/2, step 5558/7134 completed (loss: 0.0575091615319252, acc: 0.9777777791023254)
[2025-02-13 21:06:14,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:15,166][root][INFO] - Training Epoch: 2/2, step 5559/7134 completed (loss: 0.11198994517326355, acc: 0.9776536226272583)
[2025-02-13 21:06:15,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:15,535][root][INFO] - Training Epoch: 2/2, step 5560/7134 completed (loss: 0.100589320063591, acc: 0.9777777791023254)
[2025-02-13 21:06:15,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:15,889][root][INFO] - Training Epoch: 2/2, step 5561/7134 completed (loss: 0.07969190925359726, acc: 0.9754098653793335)
[2025-02-13 21:06:16,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:16,258][root][INFO] - Training Epoch: 2/2, step 5562/7134 completed (loss: 0.15086477994918823, acc: 0.9640287756919861)
[2025-02-13 21:06:16,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:16,625][root][INFO] - Training Epoch: 2/2, step 5563/7134 completed (loss: 0.045623525977134705, acc: 0.9861111044883728)
[2025-02-13 21:06:16,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:17,001][root][INFO] - Training Epoch: 2/2, step 5564/7134 completed (loss: 0.02618136629462242, acc: 0.987500011920929)
[2025-02-13 21:06:17,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:17,415][root][INFO] - Training Epoch: 2/2, step 5565/7134 completed (loss: 0.008857965469360352, acc: 1.0)
[2025-02-13 21:06:17,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:17,797][root][INFO] - Training Epoch: 2/2, step 5566/7134 completed (loss: 0.025980239734053612, acc: 0.9949238300323486)
[2025-02-13 21:06:17,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:18,182][root][INFO] - Training Epoch: 2/2, step 5567/7134 completed (loss: 0.030252834782004356, acc: 0.9900497794151306)
[2025-02-13 21:06:18,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:18,536][root][INFO] - Training Epoch: 2/2, step 5568/7134 completed (loss: 0.02630951814353466, acc: 0.9945054650306702)
[2025-02-13 21:06:18,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:18,912][root][INFO] - Training Epoch: 2/2, step 5569/7134 completed (loss: 0.030417615547776222, acc: 0.9895833134651184)
[2025-02-13 21:06:19,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:19,276][root][INFO] - Training Epoch: 2/2, step 5570/7134 completed (loss: 0.04475267976522446, acc: 0.989130437374115)
[2025-02-13 21:06:19,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:19,632][root][INFO] - Training Epoch: 2/2, step 5571/7134 completed (loss: 0.04114307463169098, acc: 0.9936708807945251)
[2025-02-13 21:06:19,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:19,998][root][INFO] - Training Epoch: 2/2, step 5572/7134 completed (loss: 0.06467913091182709, acc: 0.9938650131225586)
[2025-02-13 21:06:20,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:20,378][root][INFO] - Training Epoch: 2/2, step 5573/7134 completed (loss: 0.05792834609746933, acc: 0.9869281053543091)
[2025-02-13 21:06:20,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:20,722][root][INFO] - Training Epoch: 2/2, step 5574/7134 completed (loss: 0.020201507955789566, acc: 1.0)
[2025-02-13 21:06:20,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:21,091][root][INFO] - Training Epoch: 2/2, step 5575/7134 completed (loss: 0.06483588367700577, acc: 0.9933333396911621)
[2025-02-13 21:06:21,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:21,502][root][INFO] - Training Epoch: 2/2, step 5576/7134 completed (loss: 0.03460663929581642, acc: 0.9940476417541504)
[2025-02-13 21:06:21,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:21,880][root][INFO] - Training Epoch: 2/2, step 5577/7134 completed (loss: 0.08777347207069397, acc: 0.9851852059364319)
[2025-02-13 21:06:22,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:22,265][root][INFO] - Training Epoch: 2/2, step 5578/7134 completed (loss: 0.037616681307554245, acc: 0.9936708807945251)
[2025-02-13 21:06:22,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:22,714][root][INFO] - Training Epoch: 2/2, step 5579/7134 completed (loss: 0.05578257143497467, acc: 0.9815950989723206)
[2025-02-13 21:06:22,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:23,075][root][INFO] - Training Epoch: 2/2, step 5580/7134 completed (loss: 0.042677901685237885, acc: 0.9833333492279053)
[2025-02-13 21:06:23,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:23,448][root][INFO] - Training Epoch: 2/2, step 5581/7134 completed (loss: 0.04587921127676964, acc: 0.9878787994384766)
[2025-02-13 21:06:23,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:23,825][root][INFO] - Training Epoch: 2/2, step 5582/7134 completed (loss: 0.04319440573453903, acc: 0.988950252532959)
[2025-02-13 21:06:23,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:24,201][root][INFO] - Training Epoch: 2/2, step 5583/7134 completed (loss: 0.03422706574201584, acc: 0.9949748516082764)
[2025-02-13 21:06:24,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:24,569][root][INFO] - Training Epoch: 2/2, step 5584/7134 completed (loss: 0.063797727227211, acc: 0.9810126423835754)
[2025-02-13 21:06:24,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:24,955][root][INFO] - Training Epoch: 2/2, step 5585/7134 completed (loss: 0.07983275502920151, acc: 0.9756097793579102)
[2025-02-13 21:06:25,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:25,303][root][INFO] - Training Epoch: 2/2, step 5586/7134 completed (loss: 0.11027194559574127, acc: 0.9931972622871399)
[2025-02-13 21:06:25,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:25,668][root][INFO] - Training Epoch: 2/2, step 5587/7134 completed (loss: 0.045038655400276184, acc: 0.9828571677207947)
[2025-02-13 21:06:25,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:26,039][root][INFO] - Training Epoch: 2/2, step 5588/7134 completed (loss: 0.033468395471572876, acc: 0.9876543283462524)
[2025-02-13 21:06:26,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:26,409][root][INFO] - Training Epoch: 2/2, step 5589/7134 completed (loss: 0.084182009100914, acc: 0.9875776171684265)
[2025-02-13 21:06:26,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:26,764][root][INFO] - Training Epoch: 2/2, step 5590/7134 completed (loss: 0.0924733355641365, acc: 0.9710982441902161)
[2025-02-13 21:06:26,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:27,115][root][INFO] - Training Epoch: 2/2, step 5591/7134 completed (loss: 0.04555648937821388, acc: 0.9942196607589722)
[2025-02-13 21:06:27,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:27,461][root][INFO] - Training Epoch: 2/2, step 5592/7134 completed (loss: 0.046997908502817154, acc: 0.9941860437393188)
[2025-02-13 21:06:27,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:27,830][root][INFO] - Training Epoch: 2/2, step 5593/7134 completed (loss: 0.048059239983558655, acc: 0.9836956262588501)
[2025-02-13 21:06:27,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:28,192][root][INFO] - Training Epoch: 2/2, step 5594/7134 completed (loss: 0.03034631535410881, acc: 0.9886363744735718)
[2025-02-13 21:06:28,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:28,561][root][INFO] - Training Epoch: 2/2, step 5595/7134 completed (loss: 0.04079189896583557, acc: 0.9829545617103577)
[2025-02-13 21:06:28,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:28,918][root][INFO] - Training Epoch: 2/2, step 5596/7134 completed (loss: 0.03618267923593521, acc: 1.0)
[2025-02-13 21:06:29,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:29,287][root][INFO] - Training Epoch: 2/2, step 5597/7134 completed (loss: 0.07704408466815948, acc: 0.9819276928901672)
[2025-02-13 21:06:29,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:29,644][root][INFO] - Training Epoch: 2/2, step 5598/7134 completed (loss: 0.07177801430225372, acc: 0.9884393215179443)
[2025-02-13 21:06:29,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:30,019][root][INFO] - Training Epoch: 2/2, step 5599/7134 completed (loss: 0.11426973342895508, acc: 0.9727891087532043)
[2025-02-13 21:06:30,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:30,383][root][INFO] - Training Epoch: 2/2, step 5600/7134 completed (loss: 0.0322536826133728, acc: 1.0)
[2025-02-13 21:06:30,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:30,773][root][INFO] - Training Epoch: 2/2, step 5601/7134 completed (loss: 0.052985627204179764, acc: 0.9847328066825867)
[2025-02-13 21:06:30,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:31,144][root][INFO] - Training Epoch: 2/2, step 5602/7134 completed (loss: 0.04282493144273758, acc: 0.9929577708244324)
[2025-02-13 21:06:31,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:31,509][root][INFO] - Training Epoch: 2/2, step 5603/7134 completed (loss: 0.07499698549509048, acc: 0.9754098653793335)
[2025-02-13 21:06:31,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:31,868][root][INFO] - Training Epoch: 2/2, step 5604/7134 completed (loss: 0.08246730268001556, acc: 0.9918699264526367)
[2025-02-13 21:06:32,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:32,231][root][INFO] - Training Epoch: 2/2, step 5605/7134 completed (loss: 0.06512419879436493, acc: 0.9763779640197754)
[2025-02-13 21:06:32,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:32,597][root][INFO] - Training Epoch: 2/2, step 5606/7134 completed (loss: 0.07073013484477997, acc: 0.9813084006309509)
[2025-02-13 21:06:32,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:32,957][root][INFO] - Training Epoch: 2/2, step 5607/7134 completed (loss: 0.044791996479034424, acc: 0.9856114983558655)
[2025-02-13 21:06:33,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:33,308][root][INFO] - Training Epoch: 2/2, step 5608/7134 completed (loss: 0.04622362181544304, acc: 0.9922480583190918)
[2025-02-13 21:06:33,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:33,633][root][INFO] - Training Epoch: 2/2, step 5609/7134 completed (loss: 0.04072771593928337, acc: 0.9833333492279053)
[2025-02-13 21:06:33,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:33,951][root][INFO] - Training Epoch: 2/2, step 5610/7134 completed (loss: 0.046487465500831604, acc: 0.9909090995788574)
[2025-02-13 21:06:34,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:34,329][root][INFO] - Training Epoch: 2/2, step 5611/7134 completed (loss: 0.04162660241127014, acc: 0.9925373196601868)
[2025-02-13 21:06:34,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:34,724][root][INFO] - Training Epoch: 2/2, step 5612/7134 completed (loss: 0.0932290330529213, acc: 0.9788732528686523)
[2025-02-13 21:06:34,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:35,070][root][INFO] - Training Epoch: 2/2, step 5613/7134 completed (loss: 0.05082259327173233, acc: 0.9821428656578064)
[2025-02-13 21:06:35,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:35,462][root][INFO] - Training Epoch: 2/2, step 5614/7134 completed (loss: 0.12108764052391052, acc: 0.9865771532058716)
[2025-02-13 21:06:35,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:35,869][root][INFO] - Training Epoch: 2/2, step 5615/7134 completed (loss: 0.043109484016895294, acc: 1.0)
[2025-02-13 21:06:36,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:36,234][root][INFO] - Training Epoch: 2/2, step 5616/7134 completed (loss: 0.09798778593540192, acc: 0.9777777791023254)
[2025-02-13 21:06:36,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:36,603][root][INFO] - Training Epoch: 2/2, step 5617/7134 completed (loss: 0.03965598717331886, acc: 0.9922480583190918)
[2025-02-13 21:06:36,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:36,981][root][INFO] - Training Epoch: 2/2, step 5618/7134 completed (loss: 0.03696290776133537, acc: 1.0)
[2025-02-13 21:06:37,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:37,405][root][INFO] - Training Epoch: 2/2, step 5619/7134 completed (loss: 0.0759611502289772, acc: 0.9747899174690247)
[2025-02-13 21:06:37,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:37,766][root][INFO] - Training Epoch: 2/2, step 5620/7134 completed (loss: 0.04009140282869339, acc: 1.0)
[2025-02-13 21:06:37,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:38,133][root][INFO] - Training Epoch: 2/2, step 5621/7134 completed (loss: 0.04388072341680527, acc: 0.9935483932495117)
[2025-02-13 21:06:38,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:38,509][root][INFO] - Training Epoch: 2/2, step 5622/7134 completed (loss: 0.026659958064556122, acc: 0.9946523904800415)
[2025-02-13 21:06:38,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:38,920][root][INFO] - Training Epoch: 2/2, step 5623/7134 completed (loss: 0.06312202662229538, acc: 0.9861111044883728)
[2025-02-13 21:06:39,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:39,296][root][INFO] - Training Epoch: 2/2, step 5624/7134 completed (loss: 0.03709251433610916, acc: 0.9934640526771545)
[2025-02-13 21:06:39,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:39,696][root][INFO] - Training Epoch: 2/2, step 5625/7134 completed (loss: 0.025472993031144142, acc: 1.0)
[2025-02-13 21:06:39,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:40,068][root][INFO] - Training Epoch: 2/2, step 5626/7134 completed (loss: 0.02416745387017727, acc: 0.9931034445762634)
[2025-02-13 21:06:40,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:40,465][root][INFO] - Training Epoch: 2/2, step 5627/7134 completed (loss: 0.10143980383872986, acc: 0.9750000238418579)
[2025-02-13 21:06:40,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:40,849][root][INFO] - Training Epoch: 2/2, step 5628/7134 completed (loss: 0.0507231168448925, acc: 0.9910714030265808)
[2025-02-13 21:06:40,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:41,225][root][INFO] - Training Epoch: 2/2, step 5629/7134 completed (loss: 0.06301302462816238, acc: 0.9836065769195557)
[2025-02-13 21:06:41,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:41,577][root][INFO] - Training Epoch: 2/2, step 5630/7134 completed (loss: 0.07633097469806671, acc: 0.9803921580314636)
[2025-02-13 21:06:41,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:41,963][root][INFO] - Training Epoch: 2/2, step 5631/7134 completed (loss: 0.051859937608242035, acc: 0.9837398529052734)
[2025-02-13 21:06:42,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:42,389][root][INFO] - Training Epoch: 2/2, step 5632/7134 completed (loss: 0.07373479008674622, acc: 0.9807692170143127)
[2025-02-13 21:06:42,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:42,758][root][INFO] - Training Epoch: 2/2, step 5633/7134 completed (loss: 0.02597932144999504, acc: 0.9910714030265808)
[2025-02-13 21:06:42,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:43,118][root][INFO] - Training Epoch: 2/2, step 5634/7134 completed (loss: 0.04248255491256714, acc: 0.982300877571106)
[2025-02-13 21:06:43,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:43,502][root][INFO] - Training Epoch: 2/2, step 5635/7134 completed (loss: 0.04303508996963501, acc: 0.984375)
[2025-02-13 21:06:43,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:43,899][root][INFO] - Training Epoch: 2/2, step 5636/7134 completed (loss: 0.042297348380088806, acc: 0.9923076629638672)
[2025-02-13 21:06:44,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:44,269][root][INFO] - Training Epoch: 2/2, step 5637/7134 completed (loss: 0.025834165513515472, acc: 1.0)
[2025-02-13 21:06:44,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:44,635][root][INFO] - Training Epoch: 2/2, step 5638/7134 completed (loss: 0.05082735791802406, acc: 0.9824561476707458)
[2025-02-13 21:06:44,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:45,016][root][INFO] - Training Epoch: 2/2, step 5639/7134 completed (loss: 0.06198953837156296, acc: 0.9805194735527039)
[2025-02-13 21:06:45,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:45,367][root][INFO] - Training Epoch: 2/2, step 5640/7134 completed (loss: 0.11757435649633408, acc: 0.9645389914512634)
[2025-02-13 21:06:45,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:45,773][root][INFO] - Training Epoch: 2/2, step 5641/7134 completed (loss: 0.06596875190734863, acc: 0.9887640476226807)
[2025-02-13 21:06:45,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:46,168][root][INFO] - Training Epoch: 2/2, step 5642/7134 completed (loss: 0.02385784313082695, acc: 1.0)
[2025-02-13 21:06:46,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:46,528][root][INFO] - Training Epoch: 2/2, step 5643/7134 completed (loss: 0.09378015249967575, acc: 0.9789473414421082)
[2025-02-13 21:06:46,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:46,912][root][INFO] - Training Epoch: 2/2, step 5644/7134 completed (loss: 0.04905347526073456, acc: 0.9904761910438538)
[2025-02-13 21:06:47,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:47,275][root][INFO] - Training Epoch: 2/2, step 5645/7134 completed (loss: 0.029264777898788452, acc: 1.0)
[2025-02-13 21:06:47,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:47,664][root][INFO] - Training Epoch: 2/2, step 5646/7134 completed (loss: 0.02097753994166851, acc: 0.9928571581840515)
[2025-02-13 21:06:47,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:48,063][root][INFO] - Training Epoch: 2/2, step 5647/7134 completed (loss: 0.03552291914820671, acc: 1.0)
[2025-02-13 21:06:48,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:48,447][root][INFO] - Training Epoch: 2/2, step 5648/7134 completed (loss: 0.18750359117984772, acc: 0.9666666388511658)
[2025-02-13 21:06:48,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:48,824][root][INFO] - Training Epoch: 2/2, step 5649/7134 completed (loss: 0.06422444432973862, acc: 0.9797297120094299)
[2025-02-13 21:06:48,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:49,208][root][INFO] - Training Epoch: 2/2, step 5650/7134 completed (loss: 0.043902285397052765, acc: 0.9838709831237793)
[2025-02-13 21:06:49,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:49,608][root][INFO] - Training Epoch: 2/2, step 5651/7134 completed (loss: 0.04673362150788307, acc: 0.9931972622871399)
[2025-02-13 21:06:49,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:50,008][root][INFO] - Training Epoch: 2/2, step 5652/7134 completed (loss: 0.04810774698853493, acc: 0.9928057789802551)
[2025-02-13 21:06:50,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:50,374][root][INFO] - Training Epoch: 2/2, step 5653/7134 completed (loss: 0.04454195871949196, acc: 0.9870129823684692)
[2025-02-13 21:06:50,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:50,771][root][INFO] - Training Epoch: 2/2, step 5654/7134 completed (loss: 0.016122832894325256, acc: 1.0)
[2025-02-13 21:06:50,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:51,165][root][INFO] - Training Epoch: 2/2, step 5655/7134 completed (loss: 0.02324388548731804, acc: 1.0)
[2025-02-13 21:06:51,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:51,547][root][INFO] - Training Epoch: 2/2, step 5656/7134 completed (loss: 0.07576534152030945, acc: 0.9754098653793335)
[2025-02-13 21:06:51,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:51,932][root][INFO] - Training Epoch: 2/2, step 5657/7134 completed (loss: 0.15274573862552643, acc: 0.9668874144554138)
[2025-02-13 21:06:52,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:52,327][root][INFO] - Training Epoch: 2/2, step 5658/7134 completed (loss: 0.06775736063718796, acc: 0.9869281053543091)
[2025-02-13 21:06:52,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:52,713][root][INFO] - Training Epoch: 2/2, step 5659/7134 completed (loss: 0.04086223989725113, acc: 0.9928571581840515)
[2025-02-13 21:06:52,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:53,100][root][INFO] - Training Epoch: 2/2, step 5660/7134 completed (loss: 0.05717376247048378, acc: 0.9921875)
[2025-02-13 21:06:53,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:53,475][root][INFO] - Training Epoch: 2/2, step 5661/7134 completed (loss: 0.0657348558306694, acc: 0.9915966391563416)
[2025-02-13 21:06:53,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:53,867][root][INFO] - Training Epoch: 2/2, step 5662/7134 completed (loss: 0.06899706274271011, acc: 0.9856114983558655)
[2025-02-13 21:06:54,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:54,230][root][INFO] - Training Epoch: 2/2, step 5663/7134 completed (loss: 0.026413004845380783, acc: 1.0)
[2025-02-13 21:06:54,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:54,598][root][INFO] - Training Epoch: 2/2, step 5664/7134 completed (loss: 0.11525070667266846, acc: 0.9794520735740662)
[2025-02-13 21:06:54,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:54,982][root][INFO] - Training Epoch: 2/2, step 5665/7134 completed (loss: 0.02406136877834797, acc: 1.0)
[2025-02-13 21:06:55,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:55,352][root][INFO] - Training Epoch: 2/2, step 5666/7134 completed (loss: 0.05860525742173195, acc: 0.9869281053543091)
[2025-02-13 21:06:55,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:55,732][root][INFO] - Training Epoch: 2/2, step 5667/7134 completed (loss: 0.06570049375295639, acc: 0.9756097793579102)
[2025-02-13 21:06:55,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:56,083][root][INFO] - Training Epoch: 2/2, step 5668/7134 completed (loss: 0.11794311553239822, acc: 0.9777777791023254)
[2025-02-13 21:06:56,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:56,465][root][INFO] - Training Epoch: 2/2, step 5669/7134 completed (loss: 0.09619265049695969, acc: 0.987261176109314)
[2025-02-13 21:06:56,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:56,859][root][INFO] - Training Epoch: 2/2, step 5670/7134 completed (loss: 0.034754130989313126, acc: 0.9920634627342224)
[2025-02-13 21:06:57,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:57,239][root][INFO] - Training Epoch: 2/2, step 5671/7134 completed (loss: 0.041127707809209824, acc: 0.9943820238113403)
[2025-02-13 21:06:57,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:57,636][root][INFO] - Training Epoch: 2/2, step 5672/7134 completed (loss: 0.0669025182723999, acc: 0.9851852059364319)
[2025-02-13 21:06:57,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:57,989][root][INFO] - Training Epoch: 2/2, step 5673/7134 completed (loss: 0.08065395057201385, acc: 0.9794520735740662)
[2025-02-13 21:06:58,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:58,380][root][INFO] - Training Epoch: 2/2, step 5674/7134 completed (loss: 0.08762680739164352, acc: 0.9727891087532043)
[2025-02-13 21:06:58,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:58,768][root][INFO] - Training Epoch: 2/2, step 5675/7134 completed (loss: 0.13710784912109375, acc: 0.9666666388511658)
[2025-02-13 21:06:58,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:59,155][root][INFO] - Training Epoch: 2/2, step 5676/7134 completed (loss: 0.1729050576686859, acc: 0.9655172228813171)
[2025-02-13 21:06:59,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:59,561][root][INFO] - Training Epoch: 2/2, step 5677/7134 completed (loss: 0.19089238345623016, acc: 0.9505494236946106)
[2025-02-13 21:06:59,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:59,926][root][INFO] - Training Epoch: 2/2, step 5678/7134 completed (loss: 0.062418509274721146, acc: 0.9876543283462524)
[2025-02-13 21:07:00,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:00,325][root][INFO] - Training Epoch: 2/2, step 5679/7134 completed (loss: 0.21797628700733185, acc: 0.9620253443717957)
[2025-02-13 21:07:00,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:00,712][root][INFO] - Training Epoch: 2/2, step 5680/7134 completed (loss: 0.19595648348331451, acc: 0.9619565010070801)
[2025-02-13 21:07:00,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:01,098][root][INFO] - Training Epoch: 2/2, step 5681/7134 completed (loss: 0.12358658760786057, acc: 0.9636363387107849)
[2025-02-13 21:07:01,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:01,465][root][INFO] - Training Epoch: 2/2, step 5682/7134 completed (loss: 0.0758746787905693, acc: 0.9879518151283264)
[2025-02-13 21:07:01,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:01,855][root][INFO] - Training Epoch: 2/2, step 5683/7134 completed (loss: 0.10408773273229599, acc: 0.9777777791023254)
[2025-02-13 21:07:02,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:02,253][root][INFO] - Training Epoch: 2/2, step 5684/7134 completed (loss: 0.21356917917728424, acc: 0.9634146094322205)
[2025-02-13 21:07:02,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:02,649][root][INFO] - Training Epoch: 2/2, step 5685/7134 completed (loss: 0.11497267335653305, acc: 0.9685039520263672)
[2025-02-13 21:07:02,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:03,019][root][INFO] - Training Epoch: 2/2, step 5686/7134 completed (loss: 0.10094170272350311, acc: 0.9784172773361206)
[2025-02-13 21:07:03,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:03,396][root][INFO] - Training Epoch: 2/2, step 5687/7134 completed (loss: 0.16998666524887085, acc: 0.9696969985961914)
[2025-02-13 21:07:03,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:03,806][root][INFO] - Training Epoch: 2/2, step 5688/7134 completed (loss: 0.20053021609783173, acc: 0.9611111283302307)
[2025-02-13 21:07:03,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:04,195][root][INFO] - Training Epoch: 2/2, step 5689/7134 completed (loss: 0.1227436438202858, acc: 0.9756097793579102)
[2025-02-13 21:07:04,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:04,589][root][INFO] - Training Epoch: 2/2, step 5690/7134 completed (loss: 0.09468543529510498, acc: 0.969072163105011)
[2025-02-13 21:07:04,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:04,977][root][INFO] - Training Epoch: 2/2, step 5691/7134 completed (loss: 0.11639704555273056, acc: 0.9781420826911926)
[2025-02-13 21:07:05,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:05,360][root][INFO] - Training Epoch: 2/2, step 5692/7134 completed (loss: 0.1223112940788269, acc: 0.977142870426178)
[2025-02-13 21:07:05,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:05,742][root][INFO] - Training Epoch: 2/2, step 5693/7134 completed (loss: 0.031600043177604675, acc: 0.9874213933944702)
[2025-02-13 21:07:05,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:06,123][root][INFO] - Training Epoch: 2/2, step 5694/7134 completed (loss: 0.10161663591861725, acc: 0.9825581312179565)
[2025-02-13 21:07:06,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:06,535][root][INFO] - Training Epoch: 2/2, step 5695/7134 completed (loss: 0.041943829506635666, acc: 0.987730085849762)
[2025-02-13 21:07:06,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:06,909][root][INFO] - Training Epoch: 2/2, step 5696/7134 completed (loss: 0.06694311648607254, acc: 0.9851484894752502)
[2025-02-13 21:07:07,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:07,280][root][INFO] - Training Epoch: 2/2, step 5697/7134 completed (loss: 0.08733481913805008, acc: 0.9757575988769531)
[2025-02-13 21:07:07,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:07,644][root][INFO] - Training Epoch: 2/2, step 5698/7134 completed (loss: 0.07339399307966232, acc: 0.9829545617103577)
[2025-02-13 21:07:07,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:08,029][root][INFO] - Training Epoch: 2/2, step 5699/7134 completed (loss: 0.10396384447813034, acc: 0.982758641242981)
[2025-02-13 21:07:08,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:08,404][root][INFO] - Training Epoch: 2/2, step 5700/7134 completed (loss: 0.07376693189144135, acc: 0.9808917045593262)
[2025-02-13 21:07:08,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:08,796][root][INFO] - Training Epoch: 2/2, step 5701/7134 completed (loss: 0.07618311792612076, acc: 0.9891892075538635)
[2025-02-13 21:07:08,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:09,185][root][INFO] - Training Epoch: 2/2, step 5702/7134 completed (loss: 0.11043437570333481, acc: 0.9825581312179565)
[2025-02-13 21:07:09,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:09,574][root][INFO] - Training Epoch: 2/2, step 5703/7134 completed (loss: 0.019108489155769348, acc: 1.0)
[2025-02-13 21:07:09,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:09,967][root][INFO] - Training Epoch: 2/2, step 5704/7134 completed (loss: 0.0794939175248146, acc: 0.9647058844566345)
[2025-02-13 21:07:10,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:10,359][root][INFO] - Training Epoch: 2/2, step 5705/7134 completed (loss: 0.01056191697716713, acc: 1.0)
[2025-02-13 21:07:10,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:10,791][root][INFO] - Training Epoch: 2/2, step 5706/7134 completed (loss: 0.03951377794146538, acc: 0.9950494766235352)
[2025-02-13 21:07:10,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:11,178][root][INFO] - Training Epoch: 2/2, step 5707/7134 completed (loss: 0.0791425108909607, acc: 0.9760000109672546)
[2025-02-13 21:07:11,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:11,551][root][INFO] - Training Epoch: 2/2, step 5708/7134 completed (loss: 0.02167133428156376, acc: 0.9935897588729858)
[2025-02-13 21:07:11,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:11,936][root][INFO] - Training Epoch: 2/2, step 5709/7134 completed (loss: 0.12933093309402466, acc: 0.9813664555549622)
[2025-02-13 21:07:12,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:12,345][root][INFO] - Training Epoch: 2/2, step 5710/7134 completed (loss: 0.08261124044656754, acc: 0.977142870426178)
[2025-02-13 21:07:12,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:12,745][root][INFO] - Training Epoch: 2/2, step 5711/7134 completed (loss: 0.1246819943189621, acc: 0.9784946441650391)
[2025-02-13 21:07:12,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:13,117][root][INFO] - Training Epoch: 2/2, step 5712/7134 completed (loss: 0.02720216102898121, acc: 1.0)
[2025-02-13 21:07:13,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:13,498][root][INFO] - Training Epoch: 2/2, step 5713/7134 completed (loss: 0.035584598779678345, acc: 1.0)
[2025-02-13 21:07:13,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:13,903][root][INFO] - Training Epoch: 2/2, step 5714/7134 completed (loss: 0.027696475386619568, acc: 0.9945651888847351)
[2025-02-13 21:07:14,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:14,295][root][INFO] - Training Epoch: 2/2, step 5715/7134 completed (loss: 0.1710079461336136, acc: 0.9632353186607361)
[2025-02-13 21:07:14,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:14,676][root][INFO] - Training Epoch: 2/2, step 5716/7134 completed (loss: 0.05274733528494835, acc: 0.9930555820465088)
[2025-02-13 21:07:14,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:15,065][root][INFO] - Training Epoch: 2/2, step 5717/7134 completed (loss: 0.15015852451324463, acc: 0.9532163739204407)
[2025-02-13 21:07:15,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:15,453][root][INFO] - Training Epoch: 2/2, step 5718/7134 completed (loss: 0.04047440364956856, acc: 0.9868420958518982)
[2025-02-13 21:07:15,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:15,828][root][INFO] - Training Epoch: 2/2, step 5719/7134 completed (loss: 0.10178601741790771, acc: 0.9764705896377563)
[2025-02-13 21:07:15,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:16,201][root][INFO] - Training Epoch: 2/2, step 5720/7134 completed (loss: 0.08456901460886002, acc: 0.9743589758872986)
[2025-02-13 21:07:16,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:16,590][root][INFO] - Training Epoch: 2/2, step 5721/7134 completed (loss: 0.1089775413274765, acc: 0.9689119458198547)
[2025-02-13 21:07:16,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:16,949][root][INFO] - Training Epoch: 2/2, step 5722/7134 completed (loss: 0.02167925424873829, acc: 1.0)
[2025-02-13 21:07:17,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:17,307][root][INFO] - Training Epoch: 2/2, step 5723/7134 completed (loss: 0.07515129446983337, acc: 0.9894179701805115)
[2025-02-13 21:07:17,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:17,661][root][INFO] - Training Epoch: 2/2, step 5724/7134 completed (loss: 0.04375177621841431, acc: 0.9939393997192383)
[2025-02-13 21:07:17,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:18,053][root][INFO] - Training Epoch: 2/2, step 5725/7134 completed (loss: 0.10500326752662659, acc: 0.9657142758369446)
[2025-02-13 21:07:18,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:18,414][root][INFO] - Training Epoch: 2/2, step 5726/7134 completed (loss: 0.10108969360589981, acc: 0.970059871673584)
[2025-02-13 21:07:18,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:18,776][root][INFO] - Training Epoch: 2/2, step 5727/7134 completed (loss: 0.03002445213496685, acc: 1.0)
[2025-02-13 21:07:18,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:19,156][root][INFO] - Training Epoch: 2/2, step 5728/7134 completed (loss: 0.00727870175614953, acc: 1.0)
[2025-02-13 21:07:19,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:19,525][root][INFO] - Training Epoch: 2/2, step 5729/7134 completed (loss: 0.0249780286103487, acc: 1.0)
[2025-02-13 21:07:19,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:19,924][root][INFO] - Training Epoch: 2/2, step 5730/7134 completed (loss: 0.10003232955932617, acc: 0.9777777791023254)
[2025-02-13 21:07:20,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:20,297][root][INFO] - Training Epoch: 2/2, step 5731/7134 completed (loss: 0.08671845495700836, acc: 0.981249988079071)
[2025-02-13 21:07:20,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:20,682][root][INFO] - Training Epoch: 2/2, step 5732/7134 completed (loss: 0.46571025252342224, acc: 0.8674699068069458)
[2025-02-13 21:07:20,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:21,060][root][INFO] - Training Epoch: 2/2, step 5733/7134 completed (loss: 0.1115812137722969, acc: 0.9746835231781006)
[2025-02-13 21:07:21,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:21,433][root][INFO] - Training Epoch: 2/2, step 5734/7134 completed (loss: 0.04124417528510094, acc: 1.0)
[2025-02-13 21:07:21,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:21,836][root][INFO] - Training Epoch: 2/2, step 5735/7134 completed (loss: 0.08637787401676178, acc: 0.9867549538612366)
[2025-02-13 21:07:21,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:22,229][root][INFO] - Training Epoch: 2/2, step 5736/7134 completed (loss: 0.041476842015981674, acc: 0.9834254384040833)
[2025-02-13 21:07:22,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:22,611][root][INFO] - Training Epoch: 2/2, step 5737/7134 completed (loss: 0.03377638757228851, acc: 0.9941520690917969)
[2025-02-13 21:07:22,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:23,042][root][INFO] - Training Epoch: 2/2, step 5738/7134 completed (loss: 0.06107867509126663, acc: 0.9848484992980957)
[2025-02-13 21:07:23,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:23,422][root][INFO] - Training Epoch: 2/2, step 5739/7134 completed (loss: 0.05819788947701454, acc: 0.9916666746139526)
[2025-02-13 21:07:23,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:23,778][root][INFO] - Training Epoch: 2/2, step 5740/7134 completed (loss: 0.07834786921739578, acc: 0.9791666865348816)
[2025-02-13 21:07:23,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:24,158][root][INFO] - Training Epoch: 2/2, step 5741/7134 completed (loss: 0.05497466027736664, acc: 0.9829545617103577)
[2025-02-13 21:07:24,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:24,541][root][INFO] - Training Epoch: 2/2, step 5742/7134 completed (loss: 0.0413341261446476, acc: 0.9887640476226807)
[2025-02-13 21:07:24,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:24,928][root][INFO] - Training Epoch: 2/2, step 5743/7134 completed (loss: 0.03279425576329231, acc: 0.9947916865348816)
[2025-02-13 21:07:25,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:25,288][root][INFO] - Training Epoch: 2/2, step 5744/7134 completed (loss: 0.01256327424198389, acc: 1.0)
[2025-02-13 21:07:25,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:25,664][root][INFO] - Training Epoch: 2/2, step 5745/7134 completed (loss: 0.11949141323566437, acc: 0.9550561904907227)
[2025-02-13 21:07:25,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:26,011][root][INFO] - Training Epoch: 2/2, step 5746/7134 completed (loss: 0.06171906366944313, acc: 0.9876543283462524)
[2025-02-13 21:07:26,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:26,384][root][INFO] - Training Epoch: 2/2, step 5747/7134 completed (loss: 0.05997088924050331, acc: 0.9818181991577148)
[2025-02-13 21:07:26,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:26,762][root][INFO] - Training Epoch: 2/2, step 5748/7134 completed (loss: 0.023204347118735313, acc: 0.9921259880065918)
[2025-02-13 21:07:26,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:27,136][root][INFO] - Training Epoch: 2/2, step 5749/7134 completed (loss: 0.12892980873584747, acc: 0.9822485446929932)
[2025-02-13 21:07:27,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:27,505][root][INFO] - Training Epoch: 2/2, step 5750/7134 completed (loss: 0.033684756606817245, acc: 0.994413435459137)
[2025-02-13 21:07:27,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:27,871][root][INFO] - Training Epoch: 2/2, step 5751/7134 completed (loss: 0.07218366861343384, acc: 0.9867549538612366)
[2025-02-13 21:07:28,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:28,250][root][INFO] - Training Epoch: 2/2, step 5752/7134 completed (loss: 0.2492883950471878, acc: 0.9465649127960205)
[2025-02-13 21:07:28,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:28,605][root][INFO] - Training Epoch: 2/2, step 5753/7134 completed (loss: 0.08276082575321198, acc: 0.9597315192222595)
[2025-02-13 21:07:28,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:28,987][root][INFO] - Training Epoch: 2/2, step 5754/7134 completed (loss: 0.16310515999794006, acc: 0.954023003578186)
[2025-02-13 21:07:29,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:29,357][root][INFO] - Training Epoch: 2/2, step 5755/7134 completed (loss: 0.10718641430139542, acc: 0.9805194735527039)
[2025-02-13 21:07:29,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:29,725][root][INFO] - Training Epoch: 2/2, step 5756/7134 completed (loss: 0.14401672780513763, acc: 0.9476743936538696)
[2025-02-13 21:07:29,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:30,097][root][INFO] - Training Epoch: 2/2, step 5757/7134 completed (loss: 0.1159968450665474, acc: 0.9775280952453613)
[2025-02-13 21:07:30,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:30,417][root][INFO] - Training Epoch: 2/2, step 5758/7134 completed (loss: 0.20634329319000244, acc: 0.9320987462997437)
[2025-02-13 21:07:30,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:30,799][root][INFO] - Training Epoch: 2/2, step 5759/7134 completed (loss: 0.2024255394935608, acc: 0.9611650705337524)
[2025-02-13 21:07:30,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:31,168][root][INFO] - Training Epoch: 2/2, step 5760/7134 completed (loss: 0.14756081998348236, acc: 0.948051929473877)
[2025-02-13 21:07:31,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:31,548][root][INFO] - Training Epoch: 2/2, step 5761/7134 completed (loss: 0.057924240827560425, acc: 0.984000027179718)
[2025-02-13 21:07:31,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:31,923][root][INFO] - Training Epoch: 2/2, step 5762/7134 completed (loss: 0.05202918499708176, acc: 0.9922480583190918)
[2025-02-13 21:07:32,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:32,287][root][INFO] - Training Epoch: 2/2, step 5763/7134 completed (loss: 0.06442887336015701, acc: 0.9781420826911926)
[2025-02-13 21:07:32,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:32,640][root][INFO] - Training Epoch: 2/2, step 5764/7134 completed (loss: 0.014556041918694973, acc: 1.0)
[2025-02-13 21:07:32,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:33,005][root][INFO] - Training Epoch: 2/2, step 5765/7134 completed (loss: 0.1289360225200653, acc: 0.9622641801834106)
[2025-02-13 21:07:33,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:33,371][root][INFO] - Training Epoch: 2/2, step 5766/7134 completed (loss: 0.0654032751917839, acc: 0.9870967864990234)
[2025-02-13 21:07:33,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:33,741][root][INFO] - Training Epoch: 2/2, step 5767/7134 completed (loss: 0.01814805343747139, acc: 0.9937888383865356)
[2025-02-13 21:07:33,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:34,119][root][INFO] - Training Epoch: 2/2, step 5768/7134 completed (loss: 0.02265053801238537, acc: 0.9937499761581421)
[2025-02-13 21:07:34,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:34,492][root][INFO] - Training Epoch: 2/2, step 5769/7134 completed (loss: 0.014468305744230747, acc: 1.0)
[2025-02-13 21:07:34,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:34,867][root][INFO] - Training Epoch: 2/2, step 5770/7134 completed (loss: 0.03967657312750816, acc: 0.9891892075538635)
[2025-02-13 21:07:35,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:35,305][root][INFO] - Training Epoch: 2/2, step 5771/7134 completed (loss: 0.14699135720729828, acc: 0.9693251252174377)
[2025-02-13 21:07:35,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:35,688][root][INFO] - Training Epoch: 2/2, step 5772/7134 completed (loss: 0.0282421987503767, acc: 0.9916666746139526)
[2025-02-13 21:07:35,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:36,067][root][INFO] - Training Epoch: 2/2, step 5773/7134 completed (loss: 0.07189465314149857, acc: 0.9774436354637146)
[2025-02-13 21:07:36,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:36,434][root][INFO] - Training Epoch: 2/2, step 5774/7134 completed (loss: 0.06478358805179596, acc: 0.9753086566925049)
[2025-02-13 21:07:36,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:36,783][root][INFO] - Training Epoch: 2/2, step 5775/7134 completed (loss: 0.021516287699341774, acc: 0.9933333396911621)
[2025-02-13 21:07:36,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:37,151][root][INFO] - Training Epoch: 2/2, step 5776/7134 completed (loss: 0.06296131014823914, acc: 0.9887640476226807)
[2025-02-13 21:07:37,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:37,516][root][INFO] - Training Epoch: 2/2, step 5777/7134 completed (loss: 0.11598680913448334, acc: 0.9813084006309509)
[2025-02-13 21:07:37,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:37,890][root][INFO] - Training Epoch: 2/2, step 5778/7134 completed (loss: 0.18265536427497864, acc: 0.9634146094322205)
[2025-02-13 21:07:38,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:38,258][root][INFO] - Training Epoch: 2/2, step 5779/7134 completed (loss: 0.12849678099155426, acc: 0.9764705896377563)
[2025-02-13 21:07:38,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:38,663][root][INFO] - Training Epoch: 2/2, step 5780/7134 completed (loss: 0.08184201270341873, acc: 0.9601989984512329)
[2025-02-13 21:07:38,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:39,095][root][INFO] - Training Epoch: 2/2, step 5781/7134 completed (loss: 0.11117848008871078, acc: 0.9826589822769165)
[2025-02-13 21:07:39,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:39,472][root][INFO] - Training Epoch: 2/2, step 5782/7134 completed (loss: 0.039294589310884476, acc: 0.9944751262664795)
[2025-02-13 21:07:39,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:39,841][root][INFO] - Training Epoch: 2/2, step 5783/7134 completed (loss: 0.04187319427728653, acc: 0.9942196607589722)
[2025-02-13 21:07:39,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:40,209][root][INFO] - Training Epoch: 2/2, step 5784/7134 completed (loss: 0.04411858320236206, acc: 0.9717513918876648)
[2025-02-13 21:07:40,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:40,582][root][INFO] - Training Epoch: 2/2, step 5785/7134 completed (loss: 0.06713306158781052, acc: 0.9837837815284729)
[2025-02-13 21:07:40,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:40,945][root][INFO] - Training Epoch: 2/2, step 5786/7134 completed (loss: 0.037848617881536484, acc: 1.0)
[2025-02-13 21:07:41,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:41,322][root][INFO] - Training Epoch: 2/2, step 5787/7134 completed (loss: 0.062327418476343155, acc: 0.9790576100349426)
[2025-02-13 21:07:41,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:41,669][root][INFO] - Training Epoch: 2/2, step 5788/7134 completed (loss: 0.03170410916209221, acc: 0.9934210777282715)
[2025-02-13 21:07:41,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:42,035][root][INFO] - Training Epoch: 2/2, step 5789/7134 completed (loss: 0.05644378438591957, acc: 0.9898989796638489)
[2025-02-13 21:07:42,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:42,408][root][INFO] - Training Epoch: 2/2, step 5790/7134 completed (loss: 0.07842622697353363, acc: 0.976190447807312)
[2025-02-13 21:07:42,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:42,761][root][INFO] - Training Epoch: 2/2, step 5791/7134 completed (loss: 0.06359228491783142, acc: 0.9701492786407471)
[2025-02-13 21:07:42,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:43,127][root][INFO] - Training Epoch: 2/2, step 5792/7134 completed (loss: 0.15020790696144104, acc: 0.9530201554298401)
[2025-02-13 21:07:43,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:43,482][root][INFO] - Training Epoch: 2/2, step 5793/7134 completed (loss: 0.10814572870731354, acc: 0.9570552110671997)
[2025-02-13 21:07:43,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:43,835][root][INFO] - Training Epoch: 2/2, step 5794/7134 completed (loss: 0.2191026508808136, acc: 0.9490445852279663)
[2025-02-13 21:07:43,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:44,243][root][INFO] - Training Epoch: 2/2, step 5795/7134 completed (loss: 0.14290094375610352, acc: 0.9624060392379761)
[2025-02-13 21:07:44,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:44,610][root][INFO] - Training Epoch: 2/2, step 5796/7134 completed (loss: 0.07786988466978073, acc: 0.9770992398262024)
[2025-02-13 21:07:44,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:44,968][root][INFO] - Training Epoch: 2/2, step 5797/7134 completed (loss: 0.1058531105518341, acc: 0.9664429426193237)
[2025-02-13 21:07:45,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:45,339][root][INFO] - Training Epoch: 2/2, step 5798/7134 completed (loss: 0.6090513467788696, acc: 0.8779069781303406)
[2025-02-13 21:07:45,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:45,699][root][INFO] - Training Epoch: 2/2, step 5799/7134 completed (loss: 0.06957139074802399, acc: 0.9842519760131836)
[2025-02-13 21:07:45,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:46,057][root][INFO] - Training Epoch: 2/2, step 5800/7134 completed (loss: 0.21054303646087646, acc: 0.9640287756919861)
[2025-02-13 21:07:46,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:46,432][root][INFO] - Training Epoch: 2/2, step 5801/7134 completed (loss: 0.13871467113494873, acc: 0.9814814925193787)
[2025-02-13 21:07:46,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:46,795][root][INFO] - Training Epoch: 2/2, step 5802/7134 completed (loss: 0.064720019698143, acc: 0.9779411554336548)
[2025-02-13 21:07:46,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:47,162][root][INFO] - Training Epoch: 2/2, step 5803/7134 completed (loss: 0.03446822986006737, acc: 0.9942528605461121)
[2025-02-13 21:07:47,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:47,526][root][INFO] - Training Epoch: 2/2, step 5804/7134 completed (loss: 0.15256349742412567, acc: 0.9426751732826233)
[2025-02-13 21:07:47,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:47,895][root][INFO] - Training Epoch: 2/2, step 5805/7134 completed (loss: 0.09582111984491348, acc: 0.9735099077224731)
[2025-02-13 21:07:48,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:48,309][root][INFO] - Training Epoch: 2/2, step 5806/7134 completed (loss: 0.04444100335240364, acc: 0.9878787994384766)
[2025-02-13 21:07:48,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:48,712][root][INFO] - Training Epoch: 2/2, step 5807/7134 completed (loss: 0.12909847497940063, acc: 0.9718309640884399)
[2025-02-13 21:07:48,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:49,125][root][INFO] - Training Epoch: 2/2, step 5808/7134 completed (loss: 0.06256969273090363, acc: 0.9810126423835754)
[2025-02-13 21:07:49,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:49,482][root][INFO] - Training Epoch: 2/2, step 5809/7134 completed (loss: 0.04003021866083145, acc: 0.9901960492134094)
[2025-02-13 21:07:49,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:49,845][root][INFO] - Training Epoch: 2/2, step 5810/7134 completed (loss: 0.03285256400704384, acc: 0.9917355179786682)
[2025-02-13 21:07:49,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:50,176][root][INFO] - Training Epoch: 2/2, step 5811/7134 completed (loss: 0.037637170404195786, acc: 1.0)
[2025-02-13 21:07:50,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:50,556][root][INFO] - Training Epoch: 2/2, step 5812/7134 completed (loss: 0.034396469593048096, acc: 1.0)
[2025-02-13 21:07:50,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:50,906][root][INFO] - Training Epoch: 2/2, step 5813/7134 completed (loss: 0.09631913155317307, acc: 0.9672130942344666)
[2025-02-13 21:07:51,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:51,348][root][INFO] - Training Epoch: 2/2, step 5814/7134 completed (loss: 0.05944185331463814, acc: 0.984000027179718)
[2025-02-13 21:07:51,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:51,761][root][INFO] - Training Epoch: 2/2, step 5815/7134 completed (loss: 0.058537524193525314, acc: 0.982758641242981)
[2025-02-13 21:07:51,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:52,153][root][INFO] - Training Epoch: 2/2, step 5816/7134 completed (loss: 0.012707676738500595, acc: 1.0)
[2025-02-13 21:07:52,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:52,578][root][INFO] - Training Epoch: 2/2, step 5817/7134 completed (loss: 0.11961298435926437, acc: 0.9809523820877075)
[2025-02-13 21:07:52,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:53,002][root][INFO] - Training Epoch: 2/2, step 5818/7134 completed (loss: 0.02118483930826187, acc: 1.0)
[2025-02-13 21:07:53,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:53,363][root][INFO] - Training Epoch: 2/2, step 5819/7134 completed (loss: 0.02066853828728199, acc: 1.0)
[2025-02-13 21:07:53,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:53,710][root][INFO] - Training Epoch: 2/2, step 5820/7134 completed (loss: 0.06917881220579147, acc: 0.9917355179786682)
[2025-02-13 21:07:53,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:54,039][root][INFO] - Training Epoch: 2/2, step 5821/7134 completed (loss: 0.022583480924367905, acc: 0.9931034445762634)
[2025-02-13 21:07:54,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:54,397][root][INFO] - Training Epoch: 2/2, step 5822/7134 completed (loss: 0.05053211748600006, acc: 0.9865771532058716)
[2025-02-13 21:07:54,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:54,749][root][INFO] - Training Epoch: 2/2, step 5823/7134 completed (loss: 0.03729046881198883, acc: 0.9863945841789246)
[2025-02-13 21:07:54,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:55,131][root][INFO] - Training Epoch: 2/2, step 5824/7134 completed (loss: 0.044651128351688385, acc: 0.9818181991577148)
[2025-02-13 21:07:55,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:55,503][root][INFO] - Training Epoch: 2/2, step 5825/7134 completed (loss: 0.09941417723894119, acc: 0.9694656729698181)
[2025-02-13 21:07:55,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:55,879][root][INFO] - Training Epoch: 2/2, step 5826/7134 completed (loss: 0.014683091081678867, acc: 1.0)
[2025-02-13 21:07:56,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:56,233][root][INFO] - Training Epoch: 2/2, step 5827/7134 completed (loss: 0.027251822873950005, acc: 0.9838709831237793)
[2025-02-13 21:07:56,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:56,593][root][INFO] - Training Epoch: 2/2, step 5828/7134 completed (loss: 0.053239211440086365, acc: 0.9803921580314636)
[2025-02-13 21:07:56,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:56,961][root][INFO] - Training Epoch: 2/2, step 5829/7134 completed (loss: 0.029802147299051285, acc: 0.9939024448394775)
[2025-02-13 21:07:57,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:57,302][root][INFO] - Training Epoch: 2/2, step 5830/7134 completed (loss: 0.04158707335591316, acc: 0.9919999837875366)
[2025-02-13 21:07:57,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:57,650][root][INFO] - Training Epoch: 2/2, step 5831/7134 completed (loss: 0.0284523107111454, acc: 0.9897959232330322)
[2025-02-13 21:07:57,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:58,018][root][INFO] - Training Epoch: 2/2, step 5832/7134 completed (loss: 0.09089897572994232, acc: 0.9847328066825867)
[2025-02-13 21:07:58,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:58,359][root][INFO] - Training Epoch: 2/2, step 5833/7134 completed (loss: 0.03757837414741516, acc: 0.9849624037742615)
[2025-02-13 21:07:58,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:58,726][root][INFO] - Training Epoch: 2/2, step 5834/7134 completed (loss: 0.08419874310493469, acc: 0.9766082167625427)
[2025-02-13 21:07:58,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:59,089][root][INFO] - Training Epoch: 2/2, step 5835/7134 completed (loss: 0.04028083384037018, acc: 1.0)
[2025-02-13 21:07:59,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:59,467][root][INFO] - Training Epoch: 2/2, step 5836/7134 completed (loss: 0.11010952293872833, acc: 0.9808917045593262)
[2025-02-13 21:07:59,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:59,829][root][INFO] - Training Epoch: 2/2, step 5837/7134 completed (loss: 0.2715575695037842, acc: 0.9328858852386475)
[2025-02-13 21:07:59,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:00,236][root][INFO] - Training Epoch: 2/2, step 5838/7134 completed (loss: 0.09595093131065369, acc: 0.9928057789802551)
[2025-02-13 21:08:00,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:00,612][root][INFO] - Training Epoch: 2/2, step 5839/7134 completed (loss: 0.12152270972728729, acc: 0.95652174949646)
[2025-02-13 21:08:00,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:01,029][root][INFO] - Training Epoch: 2/2, step 5840/7134 completed (loss: 0.022503668442368507, acc: 0.9930070042610168)
[2025-02-13 21:08:01,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:01,435][root][INFO] - Training Epoch: 2/2, step 5841/7134 completed (loss: 0.1006801575422287, acc: 0.987500011920929)
[2025-02-13 21:08:01,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:01,822][root][INFO] - Training Epoch: 2/2, step 5842/7134 completed (loss: 0.02893471159040928, acc: 0.9917355179786682)
[2025-02-13 21:08:01,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:02,216][root][INFO] - Training Epoch: 2/2, step 5843/7134 completed (loss: 0.1289539337158203, acc: 0.9702380895614624)
[2025-02-13 21:08:02,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:02,652][root][INFO] - Training Epoch: 2/2, step 5844/7134 completed (loss: 0.04630979895591736, acc: 0.9879518151283264)
[2025-02-13 21:08:02,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:03,022][root][INFO] - Training Epoch: 2/2, step 5845/7134 completed (loss: 0.06960190832614899, acc: 0.9805194735527039)
[2025-02-13 21:08:03,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:03,390][root][INFO] - Training Epoch: 2/2, step 5846/7134 completed (loss: 0.026664847508072853, acc: 1.0)
[2025-02-13 21:08:03,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:03,768][root][INFO] - Training Epoch: 2/2, step 5847/7134 completed (loss: 0.03441794589161873, acc: 0.9939758777618408)
[2025-02-13 21:08:03,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:04,143][root][INFO] - Training Epoch: 2/2, step 5848/7134 completed (loss: 0.07225387543439865, acc: 0.9864864945411682)
[2025-02-13 21:08:04,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:04,498][root][INFO] - Training Epoch: 2/2, step 5849/7134 completed (loss: 0.09348368644714355, acc: 0.9740259647369385)
[2025-02-13 21:08:04,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:04,855][root][INFO] - Training Epoch: 2/2, step 5850/7134 completed (loss: 0.05684112384915352, acc: 0.9788732528686523)
[2025-02-13 21:08:04,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:05,211][root][INFO] - Training Epoch: 2/2, step 5851/7134 completed (loss: 0.11654677242040634, acc: 0.9520547986030579)
[2025-02-13 21:08:05,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:05,570][root][INFO] - Training Epoch: 2/2, step 5852/7134 completed (loss: 0.0831281989812851, acc: 0.9860140085220337)
[2025-02-13 21:08:05,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:05,938][root][INFO] - Training Epoch: 2/2, step 5853/7134 completed (loss: 0.12318123877048492, acc: 0.966292142868042)
[2025-02-13 21:08:06,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:06,324][root][INFO] - Training Epoch: 2/2, step 5854/7134 completed (loss: 0.06988121569156647, acc: 0.9923076629638672)
[2025-02-13 21:08:06,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:06,699][root][INFO] - Training Epoch: 2/2, step 5855/7134 completed (loss: 0.09606605023145676, acc: 0.991525411605835)
[2025-02-13 21:08:06,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:07,084][root][INFO] - Training Epoch: 2/2, step 5856/7134 completed (loss: 0.05043606832623482, acc: 0.9929577708244324)
[2025-02-13 21:08:07,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:07,467][root][INFO] - Training Epoch: 2/2, step 5857/7134 completed (loss: 0.08158861845731735, acc: 0.9774436354637146)
[2025-02-13 21:08:07,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:07,836][root][INFO] - Training Epoch: 2/2, step 5858/7134 completed (loss: 0.03316875919699669, acc: 0.9908257126808167)
[2025-02-13 21:08:07,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:08,208][root][INFO] - Training Epoch: 2/2, step 5859/7134 completed (loss: 0.02010810375213623, acc: 1.0)
[2025-02-13 21:08:08,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:08,601][root][INFO] - Training Epoch: 2/2, step 5860/7134 completed (loss: 0.035717885941267014, acc: 1.0)
[2025-02-13 21:08:08,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:08,973][root][INFO] - Training Epoch: 2/2, step 5861/7134 completed (loss: 0.014992787502706051, acc: 1.0)
[2025-02-13 21:08:09,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:09,322][root][INFO] - Training Epoch: 2/2, step 5862/7134 completed (loss: 0.03502508997917175, acc: 1.0)
[2025-02-13 21:08:09,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:09,696][root][INFO] - Training Epoch: 2/2, step 5863/7134 completed (loss: 0.17102155089378357, acc: 0.9677419066429138)
[2025-02-13 21:08:09,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:10,055][root][INFO] - Training Epoch: 2/2, step 5864/7134 completed (loss: 0.19564563035964966, acc: 0.9473684430122375)
[2025-02-13 21:08:10,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:10,404][root][INFO] - Training Epoch: 2/2, step 5865/7134 completed (loss: 0.05855025351047516, acc: 0.9791666865348816)
[2025-02-13 21:08:10,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:10,790][root][INFO] - Training Epoch: 2/2, step 5866/7134 completed (loss: 0.022524774074554443, acc: 0.9945054650306702)
[2025-02-13 21:08:10,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:11,212][root][INFO] - Training Epoch: 2/2, step 5867/7134 completed (loss: 0.04020693525671959, acc: 0.9864864945411682)
[2025-02-13 21:08:11,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:11,634][root][INFO] - Training Epoch: 2/2, step 5868/7134 completed (loss: 0.058484144508838654, acc: 0.9803921580314636)
[2025-02-13 21:08:11,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:12,077][root][INFO] - Training Epoch: 2/2, step 5869/7134 completed (loss: 0.06319401413202286, acc: 0.976047933101654)
[2025-02-13 21:08:12,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:12,455][root][INFO] - Training Epoch: 2/2, step 5870/7134 completed (loss: 0.04085099697113037, acc: 0.9907407164573669)
[2025-02-13 21:08:12,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:12,825][root][INFO] - Training Epoch: 2/2, step 5871/7134 completed (loss: 0.04910034313797951, acc: 0.9908257126808167)
[2025-02-13 21:08:12,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:13,193][root][INFO] - Training Epoch: 2/2, step 5872/7134 completed (loss: 0.04650077596306801, acc: 0.9751552939414978)
[2025-02-13 21:08:13,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:13,610][root][INFO] - Training Epoch: 2/2, step 5873/7134 completed (loss: 0.0749405100941658, acc: 0.9893048405647278)
[2025-02-13 21:08:13,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:13,965][root][INFO] - Training Epoch: 2/2, step 5874/7134 completed (loss: 0.03042147122323513, acc: 0.9920634627342224)
[2025-02-13 21:08:14,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:14,306][root][INFO] - Training Epoch: 2/2, step 5875/7134 completed (loss: 0.0345308855175972, acc: 0.9924242496490479)
[2025-02-13 21:08:14,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:14,723][root][INFO] - Training Epoch: 2/2, step 5876/7134 completed (loss: 0.03873025253415108, acc: 0.9775280952453613)
[2025-02-13 21:08:14,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:15,179][root][INFO] - Training Epoch: 2/2, step 5877/7134 completed (loss: 0.0679316520690918, acc: 0.9776536226272583)
[2025-02-13 21:08:15,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:15,663][root][INFO] - Training Epoch: 2/2, step 5878/7134 completed (loss: 0.07270161807537079, acc: 0.9818181991577148)
[2025-02-13 21:08:15,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:16,061][root][INFO] - Training Epoch: 2/2, step 5879/7134 completed (loss: 0.02163940854370594, acc: 1.0)
[2025-02-13 21:08:16,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:16,460][root][INFO] - Training Epoch: 2/2, step 5880/7134 completed (loss: 0.03901711851358414, acc: 0.9821428656578064)
[2025-02-13 21:08:16,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:16,845][root][INFO] - Training Epoch: 2/2, step 5881/7134 completed (loss: 0.0319741889834404, acc: 0.9914529919624329)
[2025-02-13 21:08:16,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:17,208][root][INFO] - Training Epoch: 2/2, step 5882/7134 completed (loss: 0.03158344700932503, acc: 0.9901960492134094)
[2025-02-13 21:08:17,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:17,590][root][INFO] - Training Epoch: 2/2, step 5883/7134 completed (loss: 0.04876362532377243, acc: 0.9769230484962463)
[2025-02-13 21:08:17,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:17,986][root][INFO] - Training Epoch: 2/2, step 5884/7134 completed (loss: 0.07326444983482361, acc: 0.9920634627342224)
[2025-02-13 21:08:18,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:18,364][root][INFO] - Training Epoch: 2/2, step 5885/7134 completed (loss: 0.17945009469985962, acc: 0.9509803652763367)
[2025-02-13 21:08:18,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:18,770][root][INFO] - Training Epoch: 2/2, step 5886/7134 completed (loss: 0.011154040694236755, acc: 1.0)
[2025-02-13 21:08:18,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:19,151][root][INFO] - Training Epoch: 2/2, step 5887/7134 completed (loss: 0.012396157719194889, acc: 1.0)
[2025-02-13 21:08:19,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:19,522][root][INFO] - Training Epoch: 2/2, step 5888/7134 completed (loss: 0.09599527716636658, acc: 0.9716312289237976)
[2025-02-13 21:08:19,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:19,978][root][INFO] - Training Epoch: 2/2, step 5889/7134 completed (loss: 0.06772373616695404, acc: 0.9615384340286255)
[2025-02-13 21:08:20,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:20,390][root][INFO] - Training Epoch: 2/2, step 5890/7134 completed (loss: 0.0391170009970665, acc: 0.9932432174682617)
[2025-02-13 21:08:20,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:20,806][root][INFO] - Training Epoch: 2/2, step 5891/7134 completed (loss: 0.14923416078090668, acc: 0.9642857313156128)
[2025-02-13 21:08:20,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:21,216][root][INFO] - Training Epoch: 2/2, step 5892/7134 completed (loss: 0.0659024566411972, acc: 0.9883720874786377)
[2025-02-13 21:08:21,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:21,593][root][INFO] - Training Epoch: 2/2, step 5893/7134 completed (loss: 0.04601743444800377, acc: 0.987730085849762)
[2025-02-13 21:08:21,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:21,934][root][INFO] - Training Epoch: 2/2, step 5894/7134 completed (loss: 0.019894108176231384, acc: 1.0)
[2025-02-13 21:08:22,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:22,364][root][INFO] - Training Epoch: 2/2, step 5895/7134 completed (loss: 0.02917657233774662, acc: 1.0)
[2025-02-13 21:08:22,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:22,752][root][INFO] - Training Epoch: 2/2, step 5896/7134 completed (loss: 0.05336800590157509, acc: 0.9937106966972351)
[2025-02-13 21:08:22,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:23,183][root][INFO] - Training Epoch: 2/2, step 5897/7134 completed (loss: 0.03208339959383011, acc: 0.9943181872367859)
[2025-02-13 21:08:23,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:23,612][root][INFO] - Training Epoch: 2/2, step 5898/7134 completed (loss: 0.05206040292978287, acc: 0.9821428656578064)
[2025-02-13 21:08:23,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:23,979][root][INFO] - Training Epoch: 2/2, step 5899/7134 completed (loss: 0.018391534686088562, acc: 1.0)
[2025-02-13 21:08:24,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:24,357][root][INFO] - Training Epoch: 2/2, step 5900/7134 completed (loss: 0.011041874065995216, acc: 1.0)
[2025-02-13 21:08:24,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:24,735][root][INFO] - Training Epoch: 2/2, step 5901/7134 completed (loss: 0.09444818645715714, acc: 0.9793814420700073)
[2025-02-13 21:08:24,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:25,118][root][INFO] - Training Epoch: 2/2, step 5902/7134 completed (loss: 0.04429924115538597, acc: 0.9897959232330322)
[2025-02-13 21:08:25,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:25,534][root][INFO] - Training Epoch: 2/2, step 5903/7134 completed (loss: 0.019334930926561356, acc: 1.0)
[2025-02-13 21:08:25,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:25,918][root][INFO] - Training Epoch: 2/2, step 5904/7134 completed (loss: 0.08615150302648544, acc: 0.98591548204422)
[2025-02-13 21:08:26,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:26,237][root][INFO] - Training Epoch: 2/2, step 5905/7134 completed (loss: 0.06339375674724579, acc: 0.9729729890823364)
[2025-02-13 21:08:26,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:26,656][root][INFO] - Training Epoch: 2/2, step 5906/7134 completed (loss: 0.05006221681833267, acc: 0.9869281053543091)
[2025-02-13 21:08:26,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:27,111][root][INFO] - Training Epoch: 2/2, step 5907/7134 completed (loss: 0.042820949107408524, acc: 0.9930555820465088)
[2025-02-13 21:08:27,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:27,529][root][INFO] - Training Epoch: 2/2, step 5908/7134 completed (loss: 0.03253648057579994, acc: 0.9935064911842346)
[2025-02-13 21:08:27,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:28,066][root][INFO] - Training Epoch: 2/2, step 5909/7134 completed (loss: 0.06761478632688522, acc: 0.970802903175354)
[2025-02-13 21:08:28,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:28,494][root][INFO] - Training Epoch: 2/2, step 5910/7134 completed (loss: 0.09102357923984528, acc: 0.9668874144554138)
[2025-02-13 21:08:28,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:28,878][root][INFO] - Training Epoch: 2/2, step 5911/7134 completed (loss: 0.09248232841491699, acc: 0.9739130139350891)
[2025-02-13 21:08:29,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:29,272][root][INFO] - Training Epoch: 2/2, step 5912/7134 completed (loss: 0.04106946289539337, acc: 0.9921875)
[2025-02-13 21:08:29,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:29,680][root][INFO] - Training Epoch: 2/2, step 5913/7134 completed (loss: 0.09700606763362885, acc: 0.9644970297813416)
[2025-02-13 21:08:29,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:30,067][root][INFO] - Training Epoch: 2/2, step 5914/7134 completed (loss: 0.08104565739631653, acc: 0.9857142567634583)
[2025-02-13 21:08:30,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:30,485][root][INFO] - Training Epoch: 2/2, step 5915/7134 completed (loss: 0.06785168498754501, acc: 0.9847328066825867)
[2025-02-13 21:08:30,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:30,846][root][INFO] - Training Epoch: 2/2, step 5916/7134 completed (loss: 0.09011183679103851, acc: 0.9801324605941772)
[2025-02-13 21:08:30,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:31,207][root][INFO] - Training Epoch: 2/2, step 5917/7134 completed (loss: 0.02611691690981388, acc: 0.9919999837875366)
[2025-02-13 21:08:31,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:31,596][root][INFO] - Training Epoch: 2/2, step 5918/7134 completed (loss: 0.11143394559621811, acc: 0.9720279574394226)
[2025-02-13 21:08:31,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:31,944][root][INFO] - Training Epoch: 2/2, step 5919/7134 completed (loss: 0.017128102481365204, acc: 1.0)
[2025-02-13 21:08:32,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:32,362][root][INFO] - Training Epoch: 2/2, step 5920/7134 completed (loss: 0.011484385468065739, acc: 1.0)
[2025-02-13 21:08:32,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:32,703][root][INFO] - Training Epoch: 2/2, step 5921/7134 completed (loss: 0.08670657128095627, acc: 0.9741379022598267)
[2025-02-13 21:08:32,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:33,079][root][INFO] - Training Epoch: 2/2, step 5922/7134 completed (loss: 0.2723236680030823, acc: 0.9487179517745972)
[2025-02-13 21:08:33,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:33,515][root][INFO] - Training Epoch: 2/2, step 5923/7134 completed (loss: 0.11398278176784515, acc: 0.9873417615890503)
[2025-02-13 21:08:33,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:33,897][root][INFO] - Training Epoch: 2/2, step 5924/7134 completed (loss: 0.04036223515868187, acc: 1.0)
[2025-02-13 21:08:34,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:34,244][root][INFO] - Training Epoch: 2/2, step 5925/7134 completed (loss: 0.2684876620769501, acc: 0.9333333373069763)
[2025-02-13 21:08:34,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:34,649][root][INFO] - Training Epoch: 2/2, step 5926/7134 completed (loss: 0.16584119200706482, acc: 0.9407894611358643)
[2025-02-13 21:08:34,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:35,095][root][INFO] - Training Epoch: 2/2, step 5927/7134 completed (loss: 0.09121301025152206, acc: 0.9821428656578064)
[2025-02-13 21:08:35,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:35,459][root][INFO] - Training Epoch: 2/2, step 5928/7134 completed (loss: 0.4022669792175293, acc: 0.9459459185600281)
[2025-02-13 21:08:35,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:35,845][root][INFO] - Training Epoch: 2/2, step 5929/7134 completed (loss: 0.15342488884925842, acc: 0.9677419066429138)
[2025-02-13 21:08:35,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:36,279][root][INFO] - Training Epoch: 2/2, step 5930/7134 completed (loss: 0.3217070400714874, acc: 0.942148745059967)
[2025-02-13 21:08:36,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:36,638][root][INFO] - Training Epoch: 2/2, step 5931/7134 completed (loss: 0.07949763536453247, acc: 0.9914529919624329)
[2025-02-13 21:08:36,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:37,020][root][INFO] - Training Epoch: 2/2, step 5932/7134 completed (loss: 0.29904380440711975, acc: 0.9136690497398376)
[2025-02-13 21:08:37,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:37,372][root][INFO] - Training Epoch: 2/2, step 5933/7134 completed (loss: 0.21734289824962616, acc: 0.9485294222831726)
[2025-02-13 21:08:37,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:37,736][root][INFO] - Training Epoch: 2/2, step 5934/7134 completed (loss: 0.10140302777290344, acc: 0.9857142567634583)
[2025-02-13 21:08:37,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:38,087][root][INFO] - Training Epoch: 2/2, step 5935/7134 completed (loss: 0.04915976524353027, acc: 0.9836065769195557)
[2025-02-13 21:08:38,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:38,447][root][INFO] - Training Epoch: 2/2, step 5936/7134 completed (loss: 0.22305592894554138, acc: 0.9693251252174377)
[2025-02-13 21:08:38,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:38,820][root][INFO] - Training Epoch: 2/2, step 5937/7134 completed (loss: 0.055510085076093674, acc: 0.9929577708244324)
[2025-02-13 21:08:38,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:39,190][root][INFO] - Training Epoch: 2/2, step 5938/7134 completed (loss: 0.05587727949023247, acc: 0.9849624037742615)
[2025-02-13 21:08:39,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:39,537][root][INFO] - Training Epoch: 2/2, step 5939/7134 completed (loss: 0.3342447578907013, acc: 0.931034505367279)
[2025-02-13 21:08:39,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:39,924][root][INFO] - Training Epoch: 2/2, step 5940/7134 completed (loss: 0.13292232155799866, acc: 0.9751552939414978)
[2025-02-13 21:08:40,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:40,280][root][INFO] - Training Epoch: 2/2, step 5941/7134 completed (loss: 0.06437759101390839, acc: 0.976190447807312)
[2025-02-13 21:08:40,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:40,636][root][INFO] - Training Epoch: 2/2, step 5942/7134 completed (loss: 0.02599405311048031, acc: 0.9919999837875366)
[2025-02-13 21:08:40,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:41,030][root][INFO] - Training Epoch: 2/2, step 5943/7134 completed (loss: 0.04785260185599327, acc: 0.9815950989723206)
[2025-02-13 21:08:41,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:41,420][root][INFO] - Training Epoch: 2/2, step 5944/7134 completed (loss: 0.08206123858690262, acc: 0.9846938848495483)
[2025-02-13 21:08:41,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:41,799][root][INFO] - Training Epoch: 2/2, step 5945/7134 completed (loss: 0.022976001724600792, acc: 1.0)
[2025-02-13 21:08:41,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:42,141][root][INFO] - Training Epoch: 2/2, step 5946/7134 completed (loss: 0.07843451201915741, acc: 0.9583333134651184)
[2025-02-13 21:08:42,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:42,521][root][INFO] - Training Epoch: 2/2, step 5947/7134 completed (loss: 0.16435201466083527, acc: 0.9663461446762085)
[2025-02-13 21:08:42,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:42,902][root][INFO] - Training Epoch: 2/2, step 5948/7134 completed (loss: 0.14111772179603577, acc: 0.9674796462059021)
[2025-02-13 21:08:43,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:43,272][root][INFO] - Training Epoch: 2/2, step 5949/7134 completed (loss: 0.023350100964307785, acc: 1.0)
[2025-02-13 21:08:43,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:43,646][root][INFO] - Training Epoch: 2/2, step 5950/7134 completed (loss: 0.09375887364149094, acc: 0.9801980257034302)
[2025-02-13 21:08:43,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:44,011][root][INFO] - Training Epoch: 2/2, step 5951/7134 completed (loss: 0.1339186578989029, acc: 0.9642857313156128)
[2025-02-13 21:08:44,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:44,386][root][INFO] - Training Epoch: 2/2, step 5952/7134 completed (loss: 0.40644413232803345, acc: 0.9276315569877625)
[2025-02-13 21:08:44,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:44,764][root][INFO] - Training Epoch: 2/2, step 5953/7134 completed (loss: 0.07144644856452942, acc: 0.9905660152435303)
[2025-02-13 21:08:44,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:45,156][root][INFO] - Training Epoch: 2/2, step 5954/7134 completed (loss: 0.025557979941368103, acc: 0.9900990128517151)
[2025-02-13 21:08:45,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:45,574][root][INFO] - Training Epoch: 2/2, step 5955/7134 completed (loss: 0.15178579092025757, acc: 0.9814814925193787)
[2025-02-13 21:08:45,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:45,929][root][INFO] - Training Epoch: 2/2, step 5956/7134 completed (loss: 0.10400620102882385, acc: 0.9833333492279053)
[2025-02-13 21:08:46,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:46,281][root][INFO] - Training Epoch: 2/2, step 5957/7134 completed (loss: 0.10049381852149963, acc: 0.9878048896789551)
[2025-02-13 21:08:46,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:46,674][root][INFO] - Training Epoch: 2/2, step 5958/7134 completed (loss: 0.09612448513507843, acc: 0.977142870426178)
[2025-02-13 21:08:46,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:47,055][root][INFO] - Training Epoch: 2/2, step 5959/7134 completed (loss: 0.12830357253551483, acc: 0.9734042286872864)
[2025-02-13 21:08:47,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:47,463][root][INFO] - Training Epoch: 2/2, step 5960/7134 completed (loss: 0.015564566478133202, acc: 0.9940476417541504)
[2025-02-13 21:08:47,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:47,829][root][INFO] - Training Epoch: 2/2, step 5961/7134 completed (loss: 0.07987764477729797, acc: 0.9722222089767456)
[2025-02-13 21:08:47,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:48,207][root][INFO] - Training Epoch: 2/2, step 5962/7134 completed (loss: 0.03992808237671852, acc: 0.9862068891525269)
[2025-02-13 21:08:48,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:48,571][root][INFO] - Training Epoch: 2/2, step 5963/7134 completed (loss: 0.03159507364034653, acc: 0.9942857027053833)
[2025-02-13 21:08:48,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:48,965][root][INFO] - Training Epoch: 2/2, step 5964/7134 completed (loss: 0.028201261535286903, acc: 0.9921259880065918)
[2025-02-13 21:08:49,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:49,330][root][INFO] - Training Epoch: 2/2, step 5965/7134 completed (loss: 0.051850490272045135, acc: 0.9871794581413269)
[2025-02-13 21:08:49,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:49,705][root][INFO] - Training Epoch: 2/2, step 5966/7134 completed (loss: 0.08249395340681076, acc: 0.97826087474823)
[2025-02-13 21:08:49,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:50,064][root][INFO] - Training Epoch: 2/2, step 5967/7134 completed (loss: 0.06897756457328796, acc: 0.9791666865348816)
[2025-02-13 21:08:50,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:50,412][root][INFO] - Training Epoch: 2/2, step 5968/7134 completed (loss: 0.051153793931007385, acc: 0.9900000095367432)
[2025-02-13 21:08:50,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:50,774][root][INFO] - Training Epoch: 2/2, step 5969/7134 completed (loss: 0.09780712425708771, acc: 0.9806451797485352)
[2025-02-13 21:08:50,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:51,170][root][INFO] - Training Epoch: 2/2, step 5970/7134 completed (loss: 0.13593456149101257, acc: 0.9765625)
[2025-02-13 21:08:51,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:51,615][root][INFO] - Training Epoch: 2/2, step 5971/7134 completed (loss: 0.035149771720170975, acc: 0.9894737005233765)
[2025-02-13 21:08:51,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:52,027][root][INFO] - Training Epoch: 2/2, step 5972/7134 completed (loss: 0.17651285231113434, acc: 0.9693251252174377)
[2025-02-13 21:08:52,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:52,408][root][INFO] - Training Epoch: 2/2, step 5973/7134 completed (loss: 0.14449815452098846, acc: 0.9599999785423279)
[2025-02-13 21:08:52,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:52,801][root][INFO] - Training Epoch: 2/2, step 5974/7134 completed (loss: 0.011390340514481068, acc: 1.0)
[2025-02-13 21:08:52,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:53,165][root][INFO] - Training Epoch: 2/2, step 5975/7134 completed (loss: 0.032491620630025864, acc: 0.9939024448394775)
[2025-02-13 21:08:53,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:53,541][root][INFO] - Training Epoch: 2/2, step 5976/7134 completed (loss: 0.02205776795744896, acc: 1.0)
[2025-02-13 21:08:53,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:53,932][root][INFO] - Training Epoch: 2/2, step 5977/7134 completed (loss: 0.07207372039556503, acc: 0.9850000143051147)
[2025-02-13 21:08:54,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:54,326][root][INFO] - Training Epoch: 2/2, step 5978/7134 completed (loss: 0.09037831425666809, acc: 0.9875776171684265)
[2025-02-13 21:08:54,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:54,717][root][INFO] - Training Epoch: 2/2, step 5979/7134 completed (loss: 0.10241662710905075, acc: 0.9894737005233765)
[2025-02-13 21:08:54,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:55,082][root][INFO] - Training Epoch: 2/2, step 5980/7134 completed (loss: 0.08530449122190475, acc: 0.97826087474823)
[2025-02-13 21:08:55,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:55,453][root][INFO] - Training Epoch: 2/2, step 5981/7134 completed (loss: 0.06981275975704193, acc: 0.989847719669342)
[2025-02-13 21:08:55,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:55,812][root][INFO] - Training Epoch: 2/2, step 5982/7134 completed (loss: 0.06592334061861038, acc: 0.9919354915618896)
[2025-02-13 21:08:55,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:56,184][root][INFO] - Training Epoch: 2/2, step 5983/7134 completed (loss: 0.06920144706964493, acc: 0.982758641242981)
[2025-02-13 21:08:56,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:56,559][root][INFO] - Training Epoch: 2/2, step 5984/7134 completed (loss: 0.050553251057863235, acc: 0.9878787994384766)
[2025-02-13 21:08:56,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:56,939][root][INFO] - Training Epoch: 2/2, step 5985/7134 completed (loss: 0.0878637284040451, acc: 0.983146071434021)
[2025-02-13 21:08:57,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:57,314][root][INFO] - Training Epoch: 2/2, step 5986/7134 completed (loss: 0.08788006752729416, acc: 0.9841269850730896)
[2025-02-13 21:08:57,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:57,683][root][INFO] - Training Epoch: 2/2, step 5987/7134 completed (loss: 0.0689791738986969, acc: 0.9756097793579102)
[2025-02-13 21:08:57,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:58,060][root][INFO] - Training Epoch: 2/2, step 5988/7134 completed (loss: 0.035197917371988297, acc: 0.9888888597488403)
[2025-02-13 21:08:58,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:58,417][root][INFO] - Training Epoch: 2/2, step 5989/7134 completed (loss: 0.0829189270734787, acc: 0.9846153855323792)
[2025-02-13 21:08:58,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:58,812][root][INFO] - Training Epoch: 2/2, step 5990/7134 completed (loss: 0.07278592139482498, acc: 0.9834254384040833)
[2025-02-13 21:08:58,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:59,134][root][INFO] - Training Epoch: 2/2, step 5991/7134 completed (loss: 0.1231849193572998, acc: 0.9659090638160706)
[2025-02-13 21:08:59,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:59,559][root][INFO] - Training Epoch: 2/2, step 5992/7134 completed (loss: 0.033760156482458115, acc: 0.9948979616165161)
[2025-02-13 21:08:59,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:59,925][root][INFO] - Training Epoch: 2/2, step 5993/7134 completed (loss: 0.043300796300172806, acc: 0.9887640476226807)
[2025-02-13 21:09:00,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:00,305][root][INFO] - Training Epoch: 2/2, step 5994/7134 completed (loss: 0.022837720811367035, acc: 0.9917355179786682)
[2025-02-13 21:09:00,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:00,707][root][INFO] - Training Epoch: 2/2, step 5995/7134 completed (loss: 0.03207242488861084, acc: 0.9884393215179443)
[2025-02-13 21:09:00,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:01,088][root][INFO] - Training Epoch: 2/2, step 5996/7134 completed (loss: 0.07838956266641617, acc: 0.9800000190734863)
[2025-02-13 21:09:01,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:01,453][root][INFO] - Training Epoch: 2/2, step 5997/7134 completed (loss: 0.10063386708498001, acc: 0.9772727489471436)
[2025-02-13 21:09:01,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:01,851][root][INFO] - Training Epoch: 2/2, step 5998/7134 completed (loss: 0.038895025849342346, acc: 0.9947090148925781)
[2025-02-13 21:09:01,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:02,218][root][INFO] - Training Epoch: 2/2, step 5999/7134 completed (loss: 0.045618828386068344, acc: 0.9855072498321533)
[2025-02-13 21:09:02,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:02,589][root][INFO] - Training Epoch: 2/2, step 6000/7134 completed (loss: 0.027175571769475937, acc: 1.0)
[2025-02-13 21:09:02,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:02,971][root][INFO] - Training Epoch: 2/2, step 6001/7134 completed (loss: 0.01968286745250225, acc: 1.0)
[2025-02-13 21:09:03,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:03,358][root][INFO] - Training Epoch: 2/2, step 6002/7134 completed (loss: 0.09621956944465637, acc: 0.9917355179786682)
[2025-02-13 21:09:03,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:03,784][root][INFO] - Training Epoch: 2/2, step 6003/7134 completed (loss: 0.05226154997944832, acc: 0.9870129823684692)
[2025-02-13 21:09:03,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:04,219][root][INFO] - Training Epoch: 2/2, step 6004/7134 completed (loss: 0.17843258380889893, acc: 0.9595375657081604)
[2025-02-13 21:09:04,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:04,562][root][INFO] - Training Epoch: 2/2, step 6005/7134 completed (loss: 0.06721854209899902, acc: 0.9801324605941772)
[2025-02-13 21:09:04,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:04,925][root][INFO] - Training Epoch: 2/2, step 6006/7134 completed (loss: 0.08081051707267761, acc: 0.984000027179718)
[2025-02-13 21:09:05,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:05,301][root][INFO] - Training Epoch: 2/2, step 6007/7134 completed (loss: 0.043093156069517136, acc: 1.0)
[2025-02-13 21:09:05,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:05,772][root][INFO] - Training Epoch: 2/2, step 6008/7134 completed (loss: 0.07825479656457901, acc: 0.9790209531784058)
[2025-02-13 21:09:05,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:06,145][root][INFO] - Training Epoch: 2/2, step 6009/7134 completed (loss: 0.07096626609563828, acc: 0.9924812316894531)
[2025-02-13 21:09:06,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:06,536][root][INFO] - Training Epoch: 2/2, step 6010/7134 completed (loss: 0.11159278452396393, acc: 0.9620253443717957)
[2025-02-13 21:09:06,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:06,927][root][INFO] - Training Epoch: 2/2, step 6011/7134 completed (loss: 0.17218869924545288, acc: 0.9702380895614624)
[2025-02-13 21:09:07,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:07,324][root][INFO] - Training Epoch: 2/2, step 6012/7134 completed (loss: 0.06219181418418884, acc: 0.9818181991577148)
[2025-02-13 21:09:07,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:07,694][root][INFO] - Training Epoch: 2/2, step 6013/7134 completed (loss: 0.09329129010438919, acc: 0.9719101190567017)
[2025-02-13 21:09:07,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:08,073][root][INFO] - Training Epoch: 2/2, step 6014/7134 completed (loss: 0.07233838737010956, acc: 0.9763779640197754)
[2025-02-13 21:09:08,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:08,486][root][INFO] - Training Epoch: 2/2, step 6015/7134 completed (loss: 0.0802590548992157, acc: 0.9819819927215576)
[2025-02-13 21:09:08,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:08,870][root][INFO] - Training Epoch: 2/2, step 6016/7134 completed (loss: 0.23755447566509247, acc: 0.9729729890823364)
[2025-02-13 21:09:08,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:09,245][root][INFO] - Training Epoch: 2/2, step 6017/7134 completed (loss: 0.02524017170071602, acc: 1.0)
[2025-02-13 21:09:09,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:09,618][root][INFO] - Training Epoch: 2/2, step 6018/7134 completed (loss: 0.13338603079319, acc: 0.9615384340286255)
[2025-02-13 21:09:09,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:09,958][root][INFO] - Training Epoch: 2/2, step 6019/7134 completed (loss: 0.022617071866989136, acc: 1.0)
[2025-02-13 21:09:10,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:10,358][root][INFO] - Training Epoch: 2/2, step 6020/7134 completed (loss: 0.009652047418057919, acc: 1.0)
[2025-02-13 21:09:10,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:10,694][root][INFO] - Training Epoch: 2/2, step 6021/7134 completed (loss: 0.019962327554821968, acc: 1.0)
[2025-02-13 21:09:10,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:11,052][root][INFO] - Training Epoch: 2/2, step 6022/7134 completed (loss: 0.024686608463525772, acc: 1.0)
[2025-02-13 21:09:11,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:11,408][root][INFO] - Training Epoch: 2/2, step 6023/7134 completed (loss: 0.0179153885692358, acc: 1.0)
[2025-02-13 21:09:11,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:11,747][root][INFO] - Training Epoch: 2/2, step 6024/7134 completed (loss: 0.016646672040224075, acc: 1.0)
[2025-02-13 21:09:11,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:12,123][root][INFO] - Training Epoch: 2/2, step 6025/7134 completed (loss: 0.06758486479520798, acc: 0.9836065769195557)
[2025-02-13 21:09:12,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:12,521][root][INFO] - Training Epoch: 2/2, step 6026/7134 completed (loss: 0.29278799891471863, acc: 0.9661017060279846)
[2025-02-13 21:09:12,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:12,900][root][INFO] - Training Epoch: 2/2, step 6027/7134 completed (loss: 0.04091596230864525, acc: 0.9887640476226807)
[2025-02-13 21:09:12,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:13,225][root][INFO] - Training Epoch: 2/2, step 6028/7134 completed (loss: 0.017835645005106926, acc: 1.0)
[2025-02-13 21:09:13,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:13,617][root][INFO] - Training Epoch: 2/2, step 6029/7134 completed (loss: 0.05965658649802208, acc: 0.98591548204422)
[2025-02-13 21:09:13,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:14,000][root][INFO] - Training Epoch: 2/2, step 6030/7134 completed (loss: 0.04462234303355217, acc: 0.9848484992980957)
[2025-02-13 21:09:14,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:14,377][root][INFO] - Training Epoch: 2/2, step 6031/7134 completed (loss: 0.020739590749144554, acc: 1.0)
[2025-02-13 21:09:14,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:14,700][root][INFO] - Training Epoch: 2/2, step 6032/7134 completed (loss: 0.025909962132573128, acc: 0.9900990128517151)
[2025-02-13 21:09:14,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:15,096][root][INFO] - Training Epoch: 2/2, step 6033/7134 completed (loss: 0.10436305403709412, acc: 0.9696969985961914)
[2025-02-13 21:09:15,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:15,441][root][INFO] - Training Epoch: 2/2, step 6034/7134 completed (loss: 0.12864483892917633, acc: 0.9552238583564758)
[2025-02-13 21:09:15,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:15,845][root][INFO] - Training Epoch: 2/2, step 6035/7134 completed (loss: 0.20612679421901703, acc: 0.9483568072319031)
[2025-02-13 21:09:15,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:16,234][root][INFO] - Training Epoch: 2/2, step 6036/7134 completed (loss: 0.21319925785064697, acc: 0.9438202381134033)
[2025-02-13 21:09:16,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:16,661][root][INFO] - Training Epoch: 2/2, step 6037/7134 completed (loss: 0.11151999235153198, acc: 0.9503546357154846)
[2025-02-13 21:09:16,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:17,039][root][INFO] - Training Epoch: 2/2, step 6038/7134 completed (loss: 0.09203984588384628, acc: 0.9754902124404907)
[2025-02-13 21:09:17,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:17,422][root][INFO] - Training Epoch: 2/2, step 6039/7134 completed (loss: 0.15314410626888275, acc: 0.9609755873680115)
[2025-02-13 21:09:17,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:17,811][root][INFO] - Training Epoch: 2/2, step 6040/7134 completed (loss: 0.08459113538265228, acc: 0.9779735803604126)
[2025-02-13 21:09:17,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:18,187][root][INFO] - Training Epoch: 2/2, step 6041/7134 completed (loss: 0.20766253769397736, acc: 0.9516128897666931)
[2025-02-13 21:09:18,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:18,552][root][INFO] - Training Epoch: 2/2, step 6042/7134 completed (loss: 0.08548236638307571, acc: 0.9724770784378052)
[2025-02-13 21:09:18,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:18,970][root][INFO] - Training Epoch: 2/2, step 6043/7134 completed (loss: 0.09591300785541534, acc: 0.9731183052062988)
[2025-02-13 21:09:19,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:19,340][root][INFO] - Training Epoch: 2/2, step 6044/7134 completed (loss: 0.06466669589281082, acc: 0.9857142567634583)
[2025-02-13 21:09:19,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:19,734][root][INFO] - Training Epoch: 2/2, step 6045/7134 completed (loss: 0.19349201023578644, acc: 0.9712918400764465)
[2025-02-13 21:09:19,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:20,116][root][INFO] - Training Epoch: 2/2, step 6046/7134 completed (loss: 0.13072533905506134, acc: 0.9520547986030579)
[2025-02-13 21:09:20,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:20,476][root][INFO] - Training Epoch: 2/2, step 6047/7134 completed (loss: 0.2175399214029312, acc: 0.918367326259613)
[2025-02-13 21:09:20,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:20,840][root][INFO] - Training Epoch: 2/2, step 6048/7134 completed (loss: 0.10248667746782303, acc: 0.9593495726585388)
[2025-02-13 21:09:20,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:21,193][root][INFO] - Training Epoch: 2/2, step 6049/7134 completed (loss: 0.10399782657623291, acc: 0.9790209531784058)
[2025-02-13 21:09:21,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:21,597][root][INFO] - Training Epoch: 2/2, step 6050/7134 completed (loss: 0.16173475980758667, acc: 0.9653179049491882)
[2025-02-13 21:09:21,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:21,994][root][INFO] - Training Epoch: 2/2, step 6051/7134 completed (loss: 0.17771290242671967, acc: 0.9488636255264282)
[2025-02-13 21:09:22,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:22,382][root][INFO] - Training Epoch: 2/2, step 6052/7134 completed (loss: 0.16001398861408234, acc: 0.9466666579246521)
[2025-02-13 21:09:22,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:22,764][root][INFO] - Training Epoch: 2/2, step 6053/7134 completed (loss: 0.14131204783916473, acc: 0.9646017551422119)
[2025-02-13 21:09:22,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:23,133][root][INFO] - Training Epoch: 2/2, step 6054/7134 completed (loss: 0.07520636171102524, acc: 0.984455943107605)
[2025-02-13 21:09:23,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:23,486][root][INFO] - Training Epoch: 2/2, step 6055/7134 completed (loss: 0.16596673429012299, acc: 0.9415584206581116)
[2025-02-13 21:09:23,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:23,861][root][INFO] - Training Epoch: 2/2, step 6056/7134 completed (loss: 0.08781613409519196, acc: 0.9909909963607788)
[2025-02-13 21:09:23,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:24,234][root][INFO] - Training Epoch: 2/2, step 6057/7134 completed (loss: 0.03885326907038689, acc: 0.9870129823684692)
[2025-02-13 21:09:24,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:24,603][root][INFO] - Training Epoch: 2/2, step 6058/7134 completed (loss: 0.017195457592606544, acc: 1.0)
[2025-02-13 21:09:24,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:24,962][root][INFO] - Training Epoch: 2/2, step 6059/7134 completed (loss: 0.1745898276567459, acc: 0.931506872177124)
[2025-02-13 21:09:25,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:25,329][root][INFO] - Training Epoch: 2/2, step 6060/7134 completed (loss: 0.10137160867452621, acc: 0.9938271641731262)
[2025-02-13 21:09:25,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:25,685][root][INFO] - Training Epoch: 2/2, step 6061/7134 completed (loss: 0.07512212544679642, acc: 0.9788732528686523)
[2025-02-13 21:09:25,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:26,051][root][INFO] - Training Epoch: 2/2, step 6062/7134 completed (loss: 0.1417054831981659, acc: 0.9576719403266907)
[2025-02-13 21:09:26,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:26,410][root][INFO] - Training Epoch: 2/2, step 6063/7134 completed (loss: 0.05338134244084358, acc: 0.991525411605835)
[2025-02-13 21:09:26,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:26,786][root][INFO] - Training Epoch: 2/2, step 6064/7134 completed (loss: 0.12307503819465637, acc: 0.9627329111099243)
[2025-02-13 21:09:26,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:27,164][root][INFO] - Training Epoch: 2/2, step 6065/7134 completed (loss: 0.07633736729621887, acc: 0.989847719669342)
[2025-02-13 21:09:27,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:27,527][root][INFO] - Training Epoch: 2/2, step 6066/7134 completed (loss: 0.11690380424261093, acc: 0.970370352268219)
[2025-02-13 21:09:27,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:27,907][root][INFO] - Training Epoch: 2/2, step 6067/7134 completed (loss: 0.16439442336559296, acc: 0.9407894611358643)
[2025-02-13 21:09:28,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:28,337][root][INFO] - Training Epoch: 2/2, step 6068/7134 completed (loss: 0.24698032438755035, acc: 0.9243243336677551)
[2025-02-13 21:09:28,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:28,700][root][INFO] - Training Epoch: 2/2, step 6069/7134 completed (loss: 0.169247567653656, acc: 0.9647058844566345)
[2025-02-13 21:09:28,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:29,081][root][INFO] - Training Epoch: 2/2, step 6070/7134 completed (loss: 0.11180049180984497, acc: 0.9657142758369446)
[2025-02-13 21:09:29,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:29,455][root][INFO] - Training Epoch: 2/2, step 6071/7134 completed (loss: 0.14336161315441132, acc: 0.9467455744743347)
[2025-02-13 21:09:29,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:29,797][root][INFO] - Training Epoch: 2/2, step 6072/7134 completed (loss: 0.18987877666950226, acc: 0.9416666626930237)
[2025-02-13 21:09:29,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:30,167][root][INFO] - Training Epoch: 2/2, step 6073/7134 completed (loss: 0.14400137960910797, acc: 0.9426751732826233)
[2025-02-13 21:09:30,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:30,544][root][INFO] - Training Epoch: 2/2, step 6074/7134 completed (loss: 0.22348324954509735, acc: 0.9677419066429138)
[2025-02-13 21:09:30,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:30,950][root][INFO] - Training Epoch: 2/2, step 6075/7134 completed (loss: 0.12417609244585037, acc: 0.9789473414421082)
[2025-02-13 21:09:31,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:31,427][root][INFO] - Training Epoch: 2/2, step 6076/7134 completed (loss: 0.21309970319271088, acc: 0.9358288645744324)
[2025-02-13 21:09:31,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:31,831][root][INFO] - Training Epoch: 2/2, step 6077/7134 completed (loss: 0.04107013717293739, acc: 0.9940828680992126)
[2025-02-13 21:09:31,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:32,296][root][INFO] - Training Epoch: 2/2, step 6078/7134 completed (loss: 0.1726510226726532, acc: 0.9774011373519897)
[2025-02-13 21:09:32,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:32,754][root][INFO] - Training Epoch: 2/2, step 6079/7134 completed (loss: 0.09538298845291138, acc: 0.9696969985961914)
[2025-02-13 21:09:32,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:33,188][root][INFO] - Training Epoch: 2/2, step 6080/7134 completed (loss: 0.07164811342954636, acc: 0.9885057210922241)
[2025-02-13 21:09:33,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:33,598][root][INFO] - Training Epoch: 2/2, step 6081/7134 completed (loss: 0.10774009674787521, acc: 0.9662162065505981)
[2025-02-13 21:09:33,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:33,977][root][INFO] - Training Epoch: 2/2, step 6082/7134 completed (loss: 0.05007326602935791, acc: 0.9798657894134521)
[2025-02-13 21:09:34,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:34,393][root][INFO] - Training Epoch: 2/2, step 6083/7134 completed (loss: 0.04720861837267876, acc: 0.9792746305465698)
[2025-02-13 21:09:34,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:34,770][root][INFO] - Training Epoch: 2/2, step 6084/7134 completed (loss: 0.05168294906616211, acc: 0.9866666793823242)
[2025-02-13 21:09:34,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:35,139][root][INFO] - Training Epoch: 2/2, step 6085/7134 completed (loss: 0.09184800088405609, acc: 0.9652777910232544)
[2025-02-13 21:09:35,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:35,540][root][INFO] - Training Epoch: 2/2, step 6086/7134 completed (loss: 0.04435023292899132, acc: 0.9890710115432739)
[2025-02-13 21:09:35,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:35,918][root][INFO] - Training Epoch: 2/2, step 6087/7134 completed (loss: 0.07778865844011307, acc: 0.9852941036224365)
[2025-02-13 21:09:36,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:36,328][root][INFO] - Training Epoch: 2/2, step 6088/7134 completed (loss: 0.10529854148626328, acc: 0.9675675630569458)
[2025-02-13 21:09:36,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:36,715][root][INFO] - Training Epoch: 2/2, step 6089/7134 completed (loss: 0.06321435421705246, acc: 0.9828571677207947)
[2025-02-13 21:09:36,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:37,163][root][INFO] - Training Epoch: 2/2, step 6090/7134 completed (loss: 0.04942407086491585, acc: 0.9893048405647278)
[2025-02-13 21:09:37,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:37,600][root][INFO] - Training Epoch: 2/2, step 6091/7134 completed (loss: 0.026730407029390335, acc: 1.0)
[2025-02-13 21:09:37,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:38,015][root][INFO] - Training Epoch: 2/2, step 6092/7134 completed (loss: 0.051606185734272, acc: 0.9878787994384766)
[2025-02-13 21:09:38,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:38,453][root][INFO] - Training Epoch: 2/2, step 6093/7134 completed (loss: 0.08801283687353134, acc: 0.9701492786407471)
[2025-02-13 21:09:38,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:38,913][root][INFO] - Training Epoch: 2/2, step 6094/7134 completed (loss: 0.21007056534290314, acc: 0.9375)
[2025-02-13 21:09:39,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:39,294][root][INFO] - Training Epoch: 2/2, step 6095/7134 completed (loss: 0.07713795453310013, acc: 0.987730085849762)
[2025-02-13 21:09:39,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:39,651][root][INFO] - Training Epoch: 2/2, step 6096/7134 completed (loss: 0.16537755727767944, acc: 0.9492385983467102)
[2025-02-13 21:09:39,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:40,024][root][INFO] - Training Epoch: 2/2, step 6097/7134 completed (loss: 0.07910168915987015, acc: 0.9689440727233887)
[2025-02-13 21:09:40,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:40,416][root][INFO] - Training Epoch: 2/2, step 6098/7134 completed (loss: 0.08217970281839371, acc: 0.9649122953414917)
[2025-02-13 21:09:40,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:40,787][root][INFO] - Training Epoch: 2/2, step 6099/7134 completed (loss: 0.06552086025476456, acc: 0.9863945841789246)
[2025-02-13 21:09:40,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:41,176][root][INFO] - Training Epoch: 2/2, step 6100/7134 completed (loss: 0.16769230365753174, acc: 0.9743589758872986)
[2025-02-13 21:09:41,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:41,547][root][INFO] - Training Epoch: 2/2, step 6101/7134 completed (loss: 0.016750456765294075, acc: 1.0)
[2025-02-13 21:09:41,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:41,986][root][INFO] - Training Epoch: 2/2, step 6102/7134 completed (loss: 0.22317394614219666, acc: 0.9453125)
[2025-02-13 21:09:42,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:42,402][root][INFO] - Training Epoch: 2/2, step 6103/7134 completed (loss: 0.059175439178943634, acc: 0.9818181991577148)
[2025-02-13 21:09:42,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:42,756][root][INFO] - Training Epoch: 2/2, step 6104/7134 completed (loss: 0.2505333125591278, acc: 0.9324324131011963)
[2025-02-13 21:09:42,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:43,121][root][INFO] - Training Epoch: 2/2, step 6105/7134 completed (loss: 0.39802610874176025, acc: 0.9212121367454529)
[2025-02-13 21:09:43,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:43,502][root][INFO] - Training Epoch: 2/2, step 6106/7134 completed (loss: 0.06107285991311073, acc: 0.9842519760131836)
[2025-02-13 21:09:43,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:43,860][root][INFO] - Training Epoch: 2/2, step 6107/7134 completed (loss: 0.022146454080939293, acc: 1.0)
[2025-02-13 21:09:43,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:44,239][root][INFO] - Training Epoch: 2/2, step 6108/7134 completed (loss: 0.12957721948623657, acc: 0.9586777091026306)
[2025-02-13 21:09:44,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:44,620][root][INFO] - Training Epoch: 2/2, step 6109/7134 completed (loss: 0.04205223172903061, acc: 0.9826086759567261)
[2025-02-13 21:09:44,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:44,985][root][INFO] - Training Epoch: 2/2, step 6110/7134 completed (loss: 0.1237611323595047, acc: 0.96875)
[2025-02-13 21:09:45,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:45,339][root][INFO] - Training Epoch: 2/2, step 6111/7134 completed (loss: 0.07140377163887024, acc: 0.989130437374115)
[2025-02-13 21:09:45,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:45,749][root][INFO] - Training Epoch: 2/2, step 6112/7134 completed (loss: 0.0527469664812088, acc: 0.9934640526771545)
[2025-02-13 21:09:45,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:46,122][root][INFO] - Training Epoch: 2/2, step 6113/7134 completed (loss: 0.08820437639951706, acc: 0.9756097793579102)
[2025-02-13 21:09:46,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:46,488][root][INFO] - Training Epoch: 2/2, step 6114/7134 completed (loss: 0.10454954206943512, acc: 0.9860140085220337)
[2025-02-13 21:09:46,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:46,860][root][INFO] - Training Epoch: 2/2, step 6115/7134 completed (loss: 0.04168287292122841, acc: 0.9919999837875366)
[2025-02-13 21:09:46,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:47,200][root][INFO] - Training Epoch: 2/2, step 6116/7134 completed (loss: 0.13936318457126617, acc: 0.9596773982048035)
[2025-02-13 21:09:47,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:47,571][root][INFO] - Training Epoch: 2/2, step 6117/7134 completed (loss: 0.12927542626857758, acc: 0.9520958065986633)
[2025-02-13 21:09:47,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:47,948][root][INFO] - Training Epoch: 2/2, step 6118/7134 completed (loss: 0.03894100338220596, acc: 0.9920634627342224)
[2025-02-13 21:09:48,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:48,325][root][INFO] - Training Epoch: 2/2, step 6119/7134 completed (loss: 0.0733267068862915, acc: 0.9729729890823364)
[2025-02-13 21:09:48,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:48,753][root][INFO] - Training Epoch: 2/2, step 6120/7134 completed (loss: 0.037166569381952286, acc: 0.9924242496490479)
[2025-02-13 21:09:48,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:49,145][root][INFO] - Training Epoch: 2/2, step 6121/7134 completed (loss: 0.125362828373909, acc: 0.9685039520263672)
[2025-02-13 21:09:49,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:49,533][root][INFO] - Training Epoch: 2/2, step 6122/7134 completed (loss: 0.04295387491583824, acc: 0.9919354915618896)
[2025-02-13 21:09:49,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:49,935][root][INFO] - Training Epoch: 2/2, step 6123/7134 completed (loss: 0.0667821541428566, acc: 0.9914529919624329)
[2025-02-13 21:09:50,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:50,331][root][INFO] - Training Epoch: 2/2, step 6124/7134 completed (loss: 0.024613581597805023, acc: 1.0)
[2025-02-13 21:09:50,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:50,737][root][INFO] - Training Epoch: 2/2, step 6125/7134 completed (loss: 0.2391129583120346, acc: 0.9626168012619019)
[2025-02-13 21:09:50,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:51,112][root][INFO] - Training Epoch: 2/2, step 6126/7134 completed (loss: 0.1342959702014923, acc: 0.9801324605941772)
[2025-02-13 21:09:51,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:51,473][root][INFO] - Training Epoch: 2/2, step 6127/7134 completed (loss: 0.033236805349588394, acc: 1.0)
[2025-02-13 21:09:51,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:51,885][root][INFO] - Training Epoch: 2/2, step 6128/7134 completed (loss: 0.04797522723674774, acc: 1.0)
[2025-02-13 21:09:52,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:52,326][root][INFO] - Training Epoch: 2/2, step 6129/7134 completed (loss: 0.14199066162109375, acc: 0.9704142212867737)
[2025-02-13 21:09:52,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:52,756][root][INFO] - Training Epoch: 2/2, step 6130/7134 completed (loss: 0.1185825988650322, acc: 0.9626865386962891)
[2025-02-13 21:09:52,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:53,159][root][INFO] - Training Epoch: 2/2, step 6131/7134 completed (loss: 0.1541147530078888, acc: 0.9611650705337524)
[2025-02-13 21:09:53,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:53,545][root][INFO] - Training Epoch: 2/2, step 6132/7134 completed (loss: 0.07486542314291, acc: 0.9807692170143127)
[2025-02-13 21:09:53,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:53,938][root][INFO] - Training Epoch: 2/2, step 6133/7134 completed (loss: 0.07386664301156998, acc: 0.9903846383094788)
[2025-02-13 21:09:54,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:54,326][root][INFO] - Training Epoch: 2/2, step 6134/7134 completed (loss: 0.18306154012680054, acc: 0.9479768872261047)
[2025-02-13 21:09:54,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:54,731][root][INFO] - Training Epoch: 2/2, step 6135/7134 completed (loss: 0.03786291182041168, acc: 0.9922480583190918)
[2025-02-13 21:09:54,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:55,115][root][INFO] - Training Epoch: 2/2, step 6136/7134 completed (loss: 0.1582203060388565, acc: 0.976047933101654)
[2025-02-13 21:09:55,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:55,502][root][INFO] - Training Epoch: 2/2, step 6137/7134 completed (loss: 0.11947001516819, acc: 0.9805194735527039)
[2025-02-13 21:09:55,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:55,874][root][INFO] - Training Epoch: 2/2, step 6138/7134 completed (loss: 0.13239459693431854, acc: 0.9693251252174377)
[2025-02-13 21:09:56,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:56,230][root][INFO] - Training Epoch: 2/2, step 6139/7134 completed (loss: 0.11761514842510223, acc: 0.9795918464660645)
[2025-02-13 21:09:56,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:56,626][root][INFO] - Training Epoch: 2/2, step 6140/7134 completed (loss: 0.05581061914563179, acc: 0.9923076629638672)
[2025-02-13 21:09:56,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:56,953][root][INFO] - Training Epoch: 2/2, step 6141/7134 completed (loss: 0.14806871116161346, acc: 0.9855072498321533)
[2025-02-13 21:09:57,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:57,336][root][INFO] - Training Epoch: 2/2, step 6142/7134 completed (loss: 0.0738498717546463, acc: 0.9828571677207947)
[2025-02-13 21:09:57,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:57,700][root][INFO] - Training Epoch: 2/2, step 6143/7134 completed (loss: 0.09445856511592865, acc: 0.9939393997192383)
[2025-02-13 21:09:57,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:58,053][root][INFO] - Training Epoch: 2/2, step 6144/7134 completed (loss: 0.05356566607952118, acc: 0.9931507110595703)
[2025-02-13 21:09:58,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:58,428][root][INFO] - Training Epoch: 2/2, step 6145/7134 completed (loss: 0.14611169695854187, acc: 0.9726775884628296)
[2025-02-13 21:09:58,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:58,785][root][INFO] - Training Epoch: 2/2, step 6146/7134 completed (loss: 0.12061988562345505, acc: 0.9759036302566528)
[2025-02-13 21:09:58,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:59,150][root][INFO] - Training Epoch: 2/2, step 6147/7134 completed (loss: 0.04082128033041954, acc: 0.9921259880065918)
[2025-02-13 21:09:59,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:59,529][root][INFO] - Training Epoch: 2/2, step 6148/7134 completed (loss: 0.04945052042603493, acc: 1.0)
[2025-02-13 21:09:59,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:59,891][root][INFO] - Training Epoch: 2/2, step 6149/7134 completed (loss: 0.14065539836883545, acc: 0.9735099077224731)
[2025-02-13 21:10:00,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:00,286][root][INFO] - Training Epoch: 2/2, step 6150/7134 completed (loss: 0.043723322451114655, acc: 0.9860140085220337)
[2025-02-13 21:10:00,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:00,625][root][INFO] - Training Epoch: 2/2, step 6151/7134 completed (loss: 0.03141094371676445, acc: 1.0)
[2025-02-13 21:10:00,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:01,001][root][INFO] - Training Epoch: 2/2, step 6152/7134 completed (loss: 0.2003161609172821, acc: 0.977142870426178)
[2025-02-13 21:10:01,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:01,378][root][INFO] - Training Epoch: 2/2, step 6153/7134 completed (loss: 0.05396876484155655, acc: 0.9876543283462524)
[2025-02-13 21:10:01,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:01,775][root][INFO] - Training Epoch: 2/2, step 6154/7134 completed (loss: 0.10584678500890732, acc: 0.988950252532959)
[2025-02-13 21:10:01,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:02,160][root][INFO] - Training Epoch: 2/2, step 6155/7134 completed (loss: 0.0892004445195198, acc: 0.976331353187561)
[2025-02-13 21:10:02,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:02,512][root][INFO] - Training Epoch: 2/2, step 6156/7134 completed (loss: 0.0578717477619648, acc: 0.9810126423835754)
[2025-02-13 21:10:02,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:02,879][root][INFO] - Training Epoch: 2/2, step 6157/7134 completed (loss: 0.0594661682844162, acc: 0.9873417615890503)
[2025-02-13 21:10:03,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:03,240][root][INFO] - Training Epoch: 2/2, step 6158/7134 completed (loss: 0.0820169746875763, acc: 0.9736841917037964)
[2025-02-13 21:10:03,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:03,609][root][INFO] - Training Epoch: 2/2, step 6159/7134 completed (loss: 0.1488664448261261, acc: 0.9642857313156128)
[2025-02-13 21:10:03,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:03,978][root][INFO] - Training Epoch: 2/2, step 6160/7134 completed (loss: 0.18482153117656708, acc: 0.9426751732826233)
[2025-02-13 21:10:04,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:04,353][root][INFO] - Training Epoch: 2/2, step 6161/7134 completed (loss: 0.1229911595582962, acc: 0.9685039520263672)
[2025-02-13 21:10:04,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:04,728][root][INFO] - Training Epoch: 2/2, step 6162/7134 completed (loss: 0.22954146564006805, acc: 0.9615384340286255)
[2025-02-13 21:10:04,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:05,103][root][INFO] - Training Epoch: 2/2, step 6163/7134 completed (loss: 0.1984943002462387, acc: 0.961240291595459)
[2025-02-13 21:10:05,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:05,490][root][INFO] - Training Epoch: 2/2, step 6164/7134 completed (loss: 0.15379923582077026, acc: 0.957317054271698)
[2025-02-13 21:10:05,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:05,874][root][INFO] - Training Epoch: 2/2, step 6165/7134 completed (loss: 0.05979615077376366, acc: 0.9923664331436157)
[2025-02-13 21:10:06,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:06,236][root][INFO] - Training Epoch: 2/2, step 6166/7134 completed (loss: 0.059110675007104874, acc: 0.9927007555961609)
[2025-02-13 21:10:06,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:06,619][root][INFO] - Training Epoch: 2/2, step 6167/7134 completed (loss: 0.03399232029914856, acc: 0.9825581312179565)
[2025-02-13 21:10:06,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:06,989][root][INFO] - Training Epoch: 2/2, step 6168/7134 completed (loss: 0.07342837750911713, acc: 0.9892473220825195)
[2025-02-13 21:10:07,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:07,391][root][INFO] - Training Epoch: 2/2, step 6169/7134 completed (loss: 0.10549475997686386, acc: 0.9759036302566528)
[2025-02-13 21:10:07,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:07,764][root][INFO] - Training Epoch: 2/2, step 6170/7134 completed (loss: 0.04945998266339302, acc: 0.9928057789802551)
[2025-02-13 21:10:07,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:08,148][root][INFO] - Training Epoch: 2/2, step 6171/7134 completed (loss: 0.04600612819194794, acc: 0.9835164546966553)
[2025-02-13 21:10:08,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:08,512][root][INFO] - Training Epoch: 2/2, step 6172/7134 completed (loss: 0.044716253876686096, acc: 0.984375)
[2025-02-13 21:10:08,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:08,924][root][INFO] - Training Epoch: 2/2, step 6173/7134 completed (loss: 0.025292333215475082, acc: 1.0)
[2025-02-13 21:10:09,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:09,308][root][INFO] - Training Epoch: 2/2, step 6174/7134 completed (loss: 0.017420509830117226, acc: 0.9939393997192383)
[2025-02-13 21:10:09,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:09,673][root][INFO] - Training Epoch: 2/2, step 6175/7134 completed (loss: 0.08268838375806808, acc: 0.9814814925193787)
[2025-02-13 21:10:09,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:10,046][root][INFO] - Training Epoch: 2/2, step 6176/7134 completed (loss: 0.04374246299266815, acc: 0.9939024448394775)
[2025-02-13 21:10:10,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:10,436][root][INFO] - Training Epoch: 2/2, step 6177/7134 completed (loss: 0.026832690462470055, acc: 1.0)
[2025-02-13 21:10:10,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:10,812][root][INFO] - Training Epoch: 2/2, step 6178/7134 completed (loss: 0.1552751362323761, acc: 0.9673202633857727)
[2025-02-13 21:10:10,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:11,193][root][INFO] - Training Epoch: 2/2, step 6179/7134 completed (loss: 0.13614541292190552, acc: 0.9599999785423279)
[2025-02-13 21:10:11,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:11,568][root][INFO] - Training Epoch: 2/2, step 6180/7134 completed (loss: 0.29494422674179077, acc: 0.9680851101875305)
[2025-02-13 21:10:11,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:11,941][root][INFO] - Training Epoch: 2/2, step 6181/7134 completed (loss: 0.2161923348903656, acc: 0.9583333134651184)
[2025-02-13 21:10:12,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:12,342][root][INFO] - Training Epoch: 2/2, step 6182/7134 completed (loss: 0.13692910969257355, acc: 0.9534883499145508)
[2025-02-13 21:10:12,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:12,713][root][INFO] - Training Epoch: 2/2, step 6183/7134 completed (loss: 0.3707996904850006, acc: 0.895061731338501)
[2025-02-13 21:10:12,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:13,118][root][INFO] - Training Epoch: 2/2, step 6184/7134 completed (loss: 0.2607274651527405, acc: 0.9304347634315491)
[2025-02-13 21:10:13,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:13,515][root][INFO] - Training Epoch: 2/2, step 6185/7134 completed (loss: 0.104415662586689, acc: 0.9677419066429138)
[2025-02-13 21:10:13,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:13,905][root][INFO] - Training Epoch: 2/2, step 6186/7134 completed (loss: 0.049254171550273895, acc: 0.9923664331436157)
[2025-02-13 21:10:14,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:14,265][root][INFO] - Training Epoch: 2/2, step 6187/7134 completed (loss: 0.10045096278190613, acc: 0.988304078578949)
[2025-02-13 21:10:14,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:14,694][root][INFO] - Training Epoch: 2/2, step 6188/7134 completed (loss: 0.1541491001844406, acc: 0.9607843160629272)
[2025-02-13 21:10:14,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:15,201][root][INFO] - Training Epoch: 2/2, step 6189/7134 completed (loss: 0.15518903732299805, acc: 0.9720279574394226)
[2025-02-13 21:10:15,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:15,585][root][INFO] - Training Epoch: 2/2, step 6190/7134 completed (loss: 0.06800075620412827, acc: 0.9928571581840515)
[2025-02-13 21:10:15,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:15,940][root][INFO] - Training Epoch: 2/2, step 6191/7134 completed (loss: 0.08281605690717697, acc: 0.9837398529052734)
[2025-02-13 21:10:16,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:16,309][root][INFO] - Training Epoch: 2/2, step 6192/7134 completed (loss: 0.042287420481443405, acc: 0.9902439117431641)
[2025-02-13 21:10:16,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:16,710][root][INFO] - Training Epoch: 2/2, step 6193/7134 completed (loss: 0.06764713674783707, acc: 0.9848484992980957)
[2025-02-13 21:10:16,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:17,090][root][INFO] - Training Epoch: 2/2, step 6194/7134 completed (loss: 0.03243030235171318, acc: 0.9918032884597778)
[2025-02-13 21:10:17,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:17,467][root][INFO] - Training Epoch: 2/2, step 6195/7134 completed (loss: 0.0409817099571228, acc: 0.9905660152435303)
[2025-02-13 21:10:17,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:17,849][root][INFO] - Training Epoch: 2/2, step 6196/7134 completed (loss: 0.050645217299461365, acc: 0.9876543283462524)
[2025-02-13 21:10:17,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:18,242][root][INFO] - Training Epoch: 2/2, step 6197/7134 completed (loss: 0.11974525451660156, acc: 0.9772727489471436)
[2025-02-13 21:10:18,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:18,615][root][INFO] - Training Epoch: 2/2, step 6198/7134 completed (loss: 0.08539502322673798, acc: 0.9734513163566589)
[2025-02-13 21:10:18,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:18,977][root][INFO] - Training Epoch: 2/2, step 6199/7134 completed (loss: 0.07273846864700317, acc: 0.9841269850730896)
[2025-02-13 21:10:19,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:19,385][root][INFO] - Training Epoch: 2/2, step 6200/7134 completed (loss: 0.0349830761551857, acc: 0.9948717951774597)
[2025-02-13 21:10:19,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:19,760][root][INFO] - Training Epoch: 2/2, step 6201/7134 completed (loss: 0.06377119570970535, acc: 0.9712918400764465)
[2025-02-13 21:10:19,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:20,194][root][INFO] - Training Epoch: 2/2, step 6202/7134 completed (loss: 0.10538066923618317, acc: 0.9714285731315613)
[2025-02-13 21:10:20,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:20,673][root][INFO] - Training Epoch: 2/2, step 6203/7134 completed (loss: 0.041934896260499954, acc: 0.9883720874786377)
[2025-02-13 21:10:20,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:21,132][root][INFO] - Training Epoch: 2/2, step 6204/7134 completed (loss: 0.01592697575688362, acc: 1.0)
[2025-02-13 21:10:21,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:21,533][root][INFO] - Training Epoch: 2/2, step 6205/7134 completed (loss: 0.07507485151290894, acc: 0.9784172773361206)
[2025-02-13 21:10:21,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:21,916][root][INFO] - Training Epoch: 2/2, step 6206/7134 completed (loss: 0.045076120644807816, acc: 0.9875776171684265)
[2025-02-13 21:10:22,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:22,307][root][INFO] - Training Epoch: 2/2, step 6207/7134 completed (loss: 0.2604514956474304, acc: 0.9333333373069763)
[2025-02-13 21:10:22,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:22,695][root][INFO] - Training Epoch: 2/2, step 6208/7134 completed (loss: 0.05575154349207878, acc: 0.984375)
[2025-02-13 21:10:22,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:23,074][root][INFO] - Training Epoch: 2/2, step 6209/7134 completed (loss: 0.06280747056007385, acc: 0.9821428656578064)
[2025-02-13 21:10:23,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:23,522][root][INFO] - Training Epoch: 2/2, step 6210/7134 completed (loss: 0.08808111399412155, acc: 0.9679144620895386)
[2025-02-13 21:10:23,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:23,958][root][INFO] - Training Epoch: 2/2, step 6211/7134 completed (loss: 0.052176348865032196, acc: 0.9939393997192383)
[2025-02-13 21:10:24,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:24,327][root][INFO] - Training Epoch: 2/2, step 6212/7134 completed (loss: 0.019520919770002365, acc: 1.0)
[2025-02-13 21:10:24,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:24,714][root][INFO] - Training Epoch: 2/2, step 6213/7134 completed (loss: 0.01991778239607811, acc: 1.0)
[2025-02-13 21:10:24,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:25,107][root][INFO] - Training Epoch: 2/2, step 6214/7134 completed (loss: 0.03215290978550911, acc: 0.9887005686759949)
[2025-02-13 21:10:25,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:25,523][root][INFO] - Training Epoch: 2/2, step 6215/7134 completed (loss: 0.12169937789440155, acc: 0.9758453965187073)
[2025-02-13 21:10:25,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:25,961][root][INFO] - Training Epoch: 2/2, step 6216/7134 completed (loss: 0.017638666555285454, acc: 1.0)
[2025-02-13 21:10:26,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:26,354][root][INFO] - Training Epoch: 2/2, step 6217/7134 completed (loss: 0.04043332487344742, acc: 0.9873417615890503)
[2025-02-13 21:10:26,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:26,833][root][INFO] - Training Epoch: 2/2, step 6218/7134 completed (loss: 0.04977943003177643, acc: 0.9870967864990234)
[2025-02-13 21:10:26,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:27,269][root][INFO] - Training Epoch: 2/2, step 6219/7134 completed (loss: 0.06124649941921234, acc: 0.9738562107086182)
[2025-02-13 21:10:27,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:27,721][root][INFO] - Training Epoch: 2/2, step 6220/7134 completed (loss: 0.06149746850132942, acc: 0.977011501789093)
[2025-02-13 21:10:27,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:28,216][root][INFO] - Training Epoch: 2/2, step 6221/7134 completed (loss: 0.011963386088609695, acc: 1.0)
[2025-02-13 21:10:28,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:28,635][root][INFO] - Training Epoch: 2/2, step 6222/7134 completed (loss: 0.08128773421049118, acc: 0.9819276928901672)
[2025-02-13 21:10:28,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:29,007][root][INFO] - Training Epoch: 2/2, step 6223/7134 completed (loss: 0.20011264085769653, acc: 0.9583333134651184)
[2025-02-13 21:10:29,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:29,384][root][INFO] - Training Epoch: 2/2, step 6224/7134 completed (loss: 0.026508353650569916, acc: 1.0)
[2025-02-13 21:10:29,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:29,783][root][INFO] - Training Epoch: 2/2, step 6225/7134 completed (loss: 0.08190213143825531, acc: 0.9720670580863953)
[2025-02-13 21:10:29,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:30,212][root][INFO] - Training Epoch: 2/2, step 6226/7134 completed (loss: 0.0889989510178566, acc: 0.97826087474823)
[2025-02-13 21:10:30,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:30,633][root][INFO] - Training Epoch: 2/2, step 6227/7134 completed (loss: 0.03702337667346001, acc: 0.9937499761581421)
[2025-02-13 21:10:30,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:31,066][root][INFO] - Training Epoch: 2/2, step 6228/7134 completed (loss: 0.0830058753490448, acc: 0.9840425252914429)
[2025-02-13 21:10:31,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:31,477][root][INFO] - Training Epoch: 2/2, step 6229/7134 completed (loss: 0.15912914276123047, acc: 0.9560975432395935)
[2025-02-13 21:10:31,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:31,893][root][INFO] - Training Epoch: 2/2, step 6230/7134 completed (loss: 0.11197229474782944, acc: 0.957446813583374)
[2025-02-13 21:10:32,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:32,263][root][INFO] - Training Epoch: 2/2, step 6231/7134 completed (loss: 0.07041066884994507, acc: 0.9707602262496948)
[2025-02-13 21:10:32,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:32,656][root][INFO] - Training Epoch: 2/2, step 6232/7134 completed (loss: 0.1106603741645813, acc: 0.9780219793319702)
[2025-02-13 21:10:32,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:33,027][root][INFO] - Training Epoch: 2/2, step 6233/7134 completed (loss: 0.04192536696791649, acc: 0.9942528605461121)
[2025-02-13 21:10:33,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:33,407][root][INFO] - Training Epoch: 2/2, step 6234/7134 completed (loss: 0.12294546514749527, acc: 0.9777777791023254)
[2025-02-13 21:10:33,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:33,781][root][INFO] - Training Epoch: 2/2, step 6235/7134 completed (loss: 0.04561196640133858, acc: 0.9821428656578064)
[2025-02-13 21:10:33,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:34,154][root][INFO] - Training Epoch: 2/2, step 6236/7134 completed (loss: 0.08528959006071091, acc: 0.9776536226272583)
[2025-02-13 21:10:34,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:34,546][root][INFO] - Training Epoch: 2/2, step 6237/7134 completed (loss: 0.14240901172161102, acc: 0.9661017060279846)
[2025-02-13 21:10:34,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:34,950][root][INFO] - Training Epoch: 2/2, step 6238/7134 completed (loss: 0.05899326503276825, acc: 0.9801324605941772)
[2025-02-13 21:10:35,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:35,319][root][INFO] - Training Epoch: 2/2, step 6239/7134 completed (loss: 0.10599815100431442, acc: 0.9689119458198547)
[2025-02-13 21:10:35,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:35,688][root][INFO] - Training Epoch: 2/2, step 6240/7134 completed (loss: 0.01791774481534958, acc: 0.9939758777618408)
[2025-02-13 21:10:35,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:36,099][root][INFO] - Training Epoch: 2/2, step 6241/7134 completed (loss: 0.038618508726358414, acc: 1.0)
[2025-02-13 21:10:36,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:36,475][root][INFO] - Training Epoch: 2/2, step 6242/7134 completed (loss: 0.03587495535612106, acc: 0.9863945841789246)
[2025-02-13 21:10:36,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:36,839][root][INFO] - Training Epoch: 2/2, step 6243/7134 completed (loss: 0.12210353463888168, acc: 0.9664429426193237)
[2025-02-13 21:10:36,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:37,207][root][INFO] - Training Epoch: 2/2, step 6244/7134 completed (loss: 0.06981948763132095, acc: 0.9820359349250793)
[2025-02-13 21:10:37,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:37,561][root][INFO] - Training Epoch: 2/2, step 6245/7134 completed (loss: 0.11065378040075302, acc: 0.9881656765937805)
[2025-02-13 21:10:37,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:37,937][root][INFO] - Training Epoch: 2/2, step 6246/7134 completed (loss: 0.017819644883275032, acc: 0.9946523904800415)
[2025-02-13 21:10:38,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:38,318][root][INFO] - Training Epoch: 2/2, step 6247/7134 completed (loss: 0.09694144129753113, acc: 0.97826087474823)
[2025-02-13 21:10:38,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:38,710][root][INFO] - Training Epoch: 2/2, step 6248/7134 completed (loss: 0.19876377284526825, acc: 0.946107804775238)
[2025-02-13 21:10:38,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:39,066][root][INFO] - Training Epoch: 2/2, step 6249/7134 completed (loss: 0.014947561547160149, acc: 1.0)
[2025-02-13 21:10:39,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:39,434][root][INFO] - Training Epoch: 2/2, step 6250/7134 completed (loss: 0.012586903758347034, acc: 1.0)
[2025-02-13 21:10:39,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:39,805][root][INFO] - Training Epoch: 2/2, step 6251/7134 completed (loss: 0.031957536935806274, acc: 0.9933775067329407)
[2025-02-13 21:10:39,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:40,180][root][INFO] - Training Epoch: 2/2, step 6252/7134 completed (loss: 0.006899239495396614, acc: 1.0)
[2025-02-13 21:10:40,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:40,584][root][INFO] - Training Epoch: 2/2, step 6253/7134 completed (loss: 0.042551249265670776, acc: 0.9930070042610168)
[2025-02-13 21:10:40,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:40,960][root][INFO] - Training Epoch: 2/2, step 6254/7134 completed (loss: 0.04255320131778717, acc: 0.9925373196601868)
[2025-02-13 21:10:41,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:41,331][root][INFO] - Training Epoch: 2/2, step 6255/7134 completed (loss: 0.023783816024661064, acc: 1.0)
[2025-02-13 21:10:41,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:41,742][root][INFO] - Training Epoch: 2/2, step 6256/7134 completed (loss: 0.009529412724077702, acc: 1.0)
[2025-02-13 21:10:41,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:42,149][root][INFO] - Training Epoch: 2/2, step 6257/7134 completed (loss: 0.024702690541744232, acc: 0.9940828680992126)
[2025-02-13 21:10:42,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:42,532][root][INFO] - Training Epoch: 2/2, step 6258/7134 completed (loss: 0.05808432400226593, acc: 0.9800000190734863)
[2025-02-13 21:10:42,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:42,908][root][INFO] - Training Epoch: 2/2, step 6259/7134 completed (loss: 0.018748179078102112, acc: 1.0)
[2025-02-13 21:10:43,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:43,255][root][INFO] - Training Epoch: 2/2, step 6260/7134 completed (loss: 0.04417591169476509, acc: 0.993630588054657)
[2025-02-13 21:10:43,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:43,647][root][INFO] - Training Epoch: 2/2, step 6261/7134 completed (loss: 0.04507286474108696, acc: 0.9813664555549622)
[2025-02-13 21:10:43,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:44,046][root][INFO] - Training Epoch: 2/2, step 6262/7134 completed (loss: 0.018899310380220413, acc: 0.9938271641731262)
[2025-02-13 21:10:44,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:44,438][root][INFO] - Training Epoch: 2/2, step 6263/7134 completed (loss: 0.015037615783512592, acc: 0.9935897588729858)
[2025-02-13 21:10:44,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:44,872][root][INFO] - Training Epoch: 2/2, step 6264/7134 completed (loss: 0.026913266628980637, acc: 0.9931507110595703)
[2025-02-13 21:10:45,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:45,259][root][INFO] - Training Epoch: 2/2, step 6265/7134 completed (loss: 0.01736585795879364, acc: 1.0)
[2025-02-13 21:10:45,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:45,602][root][INFO] - Training Epoch: 2/2, step 6266/7134 completed (loss: 0.010186930187046528, acc: 1.0)
[2025-02-13 21:10:45,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:45,984][root][INFO] - Training Epoch: 2/2, step 6267/7134 completed (loss: 0.02282700687646866, acc: 0.9934210777282715)
[2025-02-13 21:10:46,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:46,424][root][INFO] - Training Epoch: 2/2, step 6268/7134 completed (loss: 0.03277663514018059, acc: 0.9875776171684265)
[2025-02-13 21:10:46,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:46,796][root][INFO] - Training Epoch: 2/2, step 6269/7134 completed (loss: 0.005572419613599777, acc: 1.0)
[2025-02-13 21:10:46,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:47,168][root][INFO] - Training Epoch: 2/2, step 6270/7134 completed (loss: 0.01677638106048107, acc: 0.9934210777282715)
[2025-02-13 21:10:47,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:47,558][root][INFO] - Training Epoch: 2/2, step 6271/7134 completed (loss: 0.05070130527019501, acc: 0.9943181872367859)
[2025-02-13 21:10:47,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:47,978][root][INFO] - Training Epoch: 2/2, step 6272/7134 completed (loss: 0.004313287790864706, acc: 1.0)
[2025-02-13 21:10:48,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:48,367][root][INFO] - Training Epoch: 2/2, step 6273/7134 completed (loss: 0.016014864668250084, acc: 0.9940828680992126)
[2025-02-13 21:10:48,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:48,770][root][INFO] - Training Epoch: 2/2, step 6274/7134 completed (loss: 0.010472063906490803, acc: 1.0)
[2025-02-13 21:10:48,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:49,213][root][INFO] - Training Epoch: 2/2, step 6275/7134 completed (loss: 0.029912849888205528, acc: 0.9932885766029358)
[2025-02-13 21:10:49,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:49,580][root][INFO] - Training Epoch: 2/2, step 6276/7134 completed (loss: 0.03793365880846977, acc: 0.9918699264526367)
[2025-02-13 21:10:49,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:49,969][root][INFO] - Training Epoch: 2/2, step 6277/7134 completed (loss: 0.03689107671380043, acc: 1.0)
[2025-02-13 21:10:50,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:50,365][root][INFO] - Training Epoch: 2/2, step 6278/7134 completed (loss: 0.02182151935994625, acc: 0.9926470518112183)
[2025-02-13 21:10:50,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:50,763][root][INFO] - Training Epoch: 2/2, step 6279/7134 completed (loss: 0.03786010295152664, acc: 0.9846153855323792)
[2025-02-13 21:10:50,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:51,125][root][INFO] - Training Epoch: 2/2, step 6280/7134 completed (loss: 0.04841356351971626, acc: 0.9776119589805603)
[2025-02-13 21:10:51,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:51,496][root][INFO] - Training Epoch: 2/2, step 6281/7134 completed (loss: 0.01245847437530756, acc: 1.0)
[2025-02-13 21:10:51,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:51,868][root][INFO] - Training Epoch: 2/2, step 6282/7134 completed (loss: 0.06302135437726974, acc: 0.9861111044883728)
[2025-02-13 21:10:52,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:52,242][root][INFO] - Training Epoch: 2/2, step 6283/7134 completed (loss: 0.034350257366895676, acc: 1.0)
[2025-02-13 21:10:52,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:52,600][root][INFO] - Training Epoch: 2/2, step 6284/7134 completed (loss: 0.06769108772277832, acc: 0.9818181991577148)
[2025-02-13 21:10:52,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:53,001][root][INFO] - Training Epoch: 2/2, step 6285/7134 completed (loss: 0.1020764708518982, acc: 0.9578947424888611)
[2025-02-13 21:10:53,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:53,366][root][INFO] - Training Epoch: 2/2, step 6286/7134 completed (loss: 0.028144415467977524, acc: 0.9861111044883728)
[2025-02-13 21:10:53,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:53,724][root][INFO] - Training Epoch: 2/2, step 6287/7134 completed (loss: 0.03662959858775139, acc: 0.9848484992980957)
[2025-02-13 21:10:53,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:54,099][root][INFO] - Training Epoch: 2/2, step 6288/7134 completed (loss: 0.01571452058851719, acc: 0.9929078221321106)
[2025-02-13 21:10:54,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:54,477][root][INFO] - Training Epoch: 2/2, step 6289/7134 completed (loss: 0.01959039643406868, acc: 1.0)
[2025-02-13 21:10:54,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:54,844][root][INFO] - Training Epoch: 2/2, step 6290/7134 completed (loss: 0.019842077046632767, acc: 1.0)
[2025-02-13 21:10:54,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:55,207][root][INFO] - Training Epoch: 2/2, step 6291/7134 completed (loss: 0.018866894766688347, acc: 0.9924242496490479)
[2025-02-13 21:10:55,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:55,581][root][INFO] - Training Epoch: 2/2, step 6292/7134 completed (loss: 0.05008644983172417, acc: 0.9910714030265808)
[2025-02-13 21:10:55,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:56,045][root][INFO] - Training Epoch: 2/2, step 6293/7134 completed (loss: 0.09885472059249878, acc: 0.9696969985961914)
[2025-02-13 21:10:56,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:56,447][root][INFO] - Training Epoch: 2/2, step 6294/7134 completed (loss: 0.0907459482550621, acc: 0.9736841917037964)
[2025-02-13 21:10:56,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:56,816][root][INFO] - Training Epoch: 2/2, step 6295/7134 completed (loss: 0.014554096385836601, acc: 1.0)
[2025-02-13 21:10:56,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:57,168][root][INFO] - Training Epoch: 2/2, step 6296/7134 completed (loss: 0.16255685687065125, acc: 0.9640287756919861)
[2025-02-13 21:10:57,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:57,533][root][INFO] - Training Epoch: 2/2, step 6297/7134 completed (loss: 0.08390958607196808, acc: 0.9849624037742615)
[2025-02-13 21:10:57,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:57,903][root][INFO] - Training Epoch: 2/2, step 6298/7134 completed (loss: 0.07721119374036789, acc: 0.9722222089767456)
[2025-02-13 21:10:58,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:58,277][root][INFO] - Training Epoch: 2/2, step 6299/7134 completed (loss: 0.14491945505142212, acc: 0.9671052694320679)
[2025-02-13 21:10:58,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:58,645][root][INFO] - Training Epoch: 2/2, step 6300/7134 completed (loss: 0.0640549585223198, acc: 0.9802631735801697)
[2025-02-13 21:10:58,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:59,019][root][INFO] - Training Epoch: 2/2, step 6301/7134 completed (loss: 0.0310678631067276, acc: 0.9953703880310059)
[2025-02-13 21:10:59,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:59,381][root][INFO] - Training Epoch: 2/2, step 6302/7134 completed (loss: 0.01563597470521927, acc: 0.994413435459137)
[2025-02-13 21:10:59,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:59,762][root][INFO] - Training Epoch: 2/2, step 6303/7134 completed (loss: 0.020911892876029015, acc: 0.9910314083099365)
[2025-02-13 21:10:59,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:00,125][root][INFO] - Training Epoch: 2/2, step 6304/7134 completed (loss: 0.016828063875436783, acc: 0.9943181872367859)
[2025-02-13 21:11:00,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:00,481][root][INFO] - Training Epoch: 2/2, step 6305/7134 completed (loss: 0.057707831263542175, acc: 0.9794520735740662)
[2025-02-13 21:11:00,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:00,856][root][INFO] - Training Epoch: 2/2, step 6306/7134 completed (loss: 0.025865307077765465, acc: 0.9905213117599487)
[2025-02-13 21:11:00,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:01,221][root][INFO] - Training Epoch: 2/2, step 6307/7134 completed (loss: 0.057628463953733444, acc: 0.9923664331436157)
[2025-02-13 21:11:01,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:01,583][root][INFO] - Training Epoch: 2/2, step 6308/7134 completed (loss: 0.08972350507974625, acc: 0.984455943107605)
[2025-02-13 21:11:01,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:01,944][root][INFO] - Training Epoch: 2/2, step 6309/7134 completed (loss: 0.06633227318525314, acc: 0.9881656765937805)
[2025-02-13 21:11:02,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:02,354][root][INFO] - Training Epoch: 2/2, step 6310/7134 completed (loss: 0.060837749391794205, acc: 0.9824561476707458)
[2025-02-13 21:11:02,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:02,713][root][INFO] - Training Epoch: 2/2, step 6311/7134 completed (loss: 0.028045080602169037, acc: 0.9947916865348816)
[2025-02-13 21:11:02,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:03,068][root][INFO] - Training Epoch: 2/2, step 6312/7134 completed (loss: 0.02700691483914852, acc: 0.993630588054657)
[2025-02-13 21:11:03,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:03,429][root][INFO] - Training Epoch: 2/2, step 6313/7134 completed (loss: 0.01929851435124874, acc: 0.9945945739746094)
[2025-02-13 21:11:03,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:03,848][root][INFO] - Training Epoch: 2/2, step 6314/7134 completed (loss: 0.08277378976345062, acc: 0.9838709831237793)
[2025-02-13 21:11:03,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:04,205][root][INFO] - Training Epoch: 2/2, step 6315/7134 completed (loss: 0.14795972406864166, acc: 0.9798657894134521)
[2025-02-13 21:11:04,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:04,575][root][INFO] - Training Epoch: 2/2, step 6316/7134 completed (loss: 0.03443489223718643, acc: 0.9862068891525269)
[2025-02-13 21:11:04,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:04,961][root][INFO] - Training Epoch: 2/2, step 6317/7134 completed (loss: 0.021089274436235428, acc: 1.0)
[2025-02-13 21:11:05,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:05,320][root][INFO] - Training Epoch: 2/2, step 6318/7134 completed (loss: 0.030425598844885826, acc: 0.9868420958518982)
[2025-02-13 21:11:05,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:05,687][root][INFO] - Training Epoch: 2/2, step 6319/7134 completed (loss: 0.02684883028268814, acc: 0.9911110997200012)
[2025-02-13 21:11:05,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:06,071][root][INFO] - Training Epoch: 2/2, step 6320/7134 completed (loss: 0.022869333624839783, acc: 1.0)
[2025-02-13 21:11:06,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:06,450][root][INFO] - Training Epoch: 2/2, step 6321/7134 completed (loss: 0.025952190160751343, acc: 0.9952380657196045)
[2025-02-13 21:11:06,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:06,822][root][INFO] - Training Epoch: 2/2, step 6322/7134 completed (loss: 0.037687283009290695, acc: 0.993630588054657)
[2025-02-13 21:11:06,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:07,198][root][INFO] - Training Epoch: 2/2, step 6323/7134 completed (loss: 0.04817092418670654, acc: 0.987500011920929)
[2025-02-13 21:11:07,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:07,597][root][INFO] - Training Epoch: 2/2, step 6324/7134 completed (loss: 0.14325672388076782, acc: 0.9617486596107483)
[2025-02-13 21:11:07,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:07,985][root][INFO] - Training Epoch: 2/2, step 6325/7134 completed (loss: 0.04644383117556572, acc: 0.9919354915618896)
[2025-02-13 21:11:08,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:08,359][root][INFO] - Training Epoch: 2/2, step 6326/7134 completed (loss: 0.057382870465517044, acc: 0.9748427867889404)
[2025-02-13 21:11:08,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:08,722][root][INFO] - Training Epoch: 2/2, step 6327/7134 completed (loss: 0.01146220788359642, acc: 1.0)
[2025-02-13 21:11:08,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:09,115][root][INFO] - Training Epoch: 2/2, step 6328/7134 completed (loss: 0.0719122663140297, acc: 0.9813664555549622)
[2025-02-13 21:11:09,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:09,483][root][INFO] - Training Epoch: 2/2, step 6329/7134 completed (loss: 0.03734457865357399, acc: 0.9935897588729858)
[2025-02-13 21:11:09,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:09,842][root][INFO] - Training Epoch: 2/2, step 6330/7134 completed (loss: 0.031310781836509705, acc: 0.9905660152435303)
[2025-02-13 21:11:09,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:10,199][root][INFO] - Training Epoch: 2/2, step 6331/7134 completed (loss: 0.011496759951114655, acc: 1.0)
[2025-02-13 21:11:10,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:10,586][root][INFO] - Training Epoch: 2/2, step 6332/7134 completed (loss: 0.09932932257652283, acc: 0.9716312289237976)
[2025-02-13 21:11:10,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:10,963][root][INFO] - Training Epoch: 2/2, step 6333/7134 completed (loss: 0.10043839365243912, acc: 0.959770143032074)
[2025-02-13 21:11:11,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:11,312][root][INFO] - Training Epoch: 2/2, step 6334/7134 completed (loss: 0.025410978123545647, acc: 0.9926470518112183)
[2025-02-13 21:11:11,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:11,669][root][INFO] - Training Epoch: 2/2, step 6335/7134 completed (loss: 0.011511644348502159, acc: 1.0)
[2025-02-13 21:11:11,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:12,023][root][INFO] - Training Epoch: 2/2, step 6336/7134 completed (loss: 0.03802909702062607, acc: 0.9912280440330505)
[2025-02-13 21:11:12,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:12,405][root][INFO] - Training Epoch: 2/2, step 6337/7134 completed (loss: 0.021970652043819427, acc: 1.0)
[2025-02-13 21:11:12,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:12,776][root][INFO] - Training Epoch: 2/2, step 6338/7134 completed (loss: 0.11899318546056747, acc: 0.9874213933944702)
[2025-02-13 21:11:12,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:13,150][root][INFO] - Training Epoch: 2/2, step 6339/7134 completed (loss: 0.03960002213716507, acc: 0.9803921580314636)
[2025-02-13 21:11:13,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:13,523][root][INFO] - Training Epoch: 2/2, step 6340/7134 completed (loss: 0.02195262536406517, acc: 0.9938650131225586)
[2025-02-13 21:11:13,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:13,897][root][INFO] - Training Epoch: 2/2, step 6341/7134 completed (loss: 0.030773958191275597, acc: 0.9842519760131836)
[2025-02-13 21:11:14,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:14,278][root][INFO] - Training Epoch: 2/2, step 6342/7134 completed (loss: 0.04940091446042061, acc: 0.9768785834312439)
[2025-02-13 21:11:14,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:14,652][root][INFO] - Training Epoch: 2/2, step 6343/7134 completed (loss: 0.041348427534103394, acc: 0.9931507110595703)
[2025-02-13 21:11:14,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:15,032][root][INFO] - Training Epoch: 2/2, step 6344/7134 completed (loss: 0.04734399542212486, acc: 0.988950252532959)
[2025-02-13 21:11:15,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:15,407][root][INFO] - Training Epoch: 2/2, step 6345/7134 completed (loss: 0.044254206120967865, acc: 0.989130437374115)
[2025-02-13 21:11:15,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:15,766][root][INFO] - Training Epoch: 2/2, step 6346/7134 completed (loss: 0.1410875916481018, acc: 0.9651162624359131)
[2025-02-13 21:11:15,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:16,119][root][INFO] - Training Epoch: 2/2, step 6347/7134 completed (loss: 0.10153990238904953, acc: 0.9831932783126831)
[2025-02-13 21:11:16,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:16,488][root][INFO] - Training Epoch: 2/2, step 6348/7134 completed (loss: 0.10169103741645813, acc: 0.9930555820465088)
[2025-02-13 21:11:16,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:16,831][root][INFO] - Training Epoch: 2/2, step 6349/7134 completed (loss: 0.027666142210364342, acc: 0.9924242496490479)
[2025-02-13 21:11:16,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:17,185][root][INFO] - Training Epoch: 2/2, step 6350/7134 completed (loss: 0.07437137514352798, acc: 0.9735449552536011)
[2025-02-13 21:11:17,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:17,567][root][INFO] - Training Epoch: 2/2, step 6351/7134 completed (loss: 0.10537595301866531, acc: 0.976047933101654)
[2025-02-13 21:11:17,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:17,939][root][INFO] - Training Epoch: 2/2, step 6352/7134 completed (loss: 0.10973241180181503, acc: 0.9768785834312439)
[2025-02-13 21:11:18,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:18,314][root][INFO] - Training Epoch: 2/2, step 6353/7134 completed (loss: 0.1037602573633194, acc: 0.9693251252174377)
[2025-02-13 21:11:18,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:18,680][root][INFO] - Training Epoch: 2/2, step 6354/7134 completed (loss: 0.03149528428912163, acc: 0.9942528605461121)
[2025-02-13 21:11:18,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:19,036][root][INFO] - Training Epoch: 2/2, step 6355/7134 completed (loss: 0.029244881123304367, acc: 0.9933333396911621)
[2025-02-13 21:11:19,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:19,391][root][INFO] - Training Epoch: 2/2, step 6356/7134 completed (loss: 0.039335187524557114, acc: 0.987261176109314)
[2025-02-13 21:11:19,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:19,766][root][INFO] - Training Epoch: 2/2, step 6357/7134 completed (loss: 0.042912013828754425, acc: 0.9878787994384766)
[2025-02-13 21:11:19,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:20,158][root][INFO] - Training Epoch: 2/2, step 6358/7134 completed (loss: 0.08095569163560867, acc: 0.9824561476707458)
[2025-02-13 21:11:20,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:20,542][root][INFO] - Training Epoch: 2/2, step 6359/7134 completed (loss: 0.1297609955072403, acc: 0.9776536226272583)
[2025-02-13 21:11:20,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:20,962][root][INFO] - Training Epoch: 2/2, step 6360/7134 completed (loss: 0.05036177113652229, acc: 0.9863945841789246)
[2025-02-13 21:11:21,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:21,339][root][INFO] - Training Epoch: 2/2, step 6361/7134 completed (loss: 0.10085329413414001, acc: 0.971222996711731)
[2025-02-13 21:11:21,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:21,742][root][INFO] - Training Epoch: 2/2, step 6362/7134 completed (loss: 0.029735291376709938, acc: 1.0)
[2025-02-13 21:11:21,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:22,152][root][INFO] - Training Epoch: 2/2, step 6363/7134 completed (loss: 0.10051122307777405, acc: 0.9822485446929932)
[2025-02-13 21:11:22,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:22,523][root][INFO] - Training Epoch: 2/2, step 6364/7134 completed (loss: 0.03773504123091698, acc: 0.993630588054657)
[2025-02-13 21:11:22,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:22,881][root][INFO] - Training Epoch: 2/2, step 6365/7134 completed (loss: 0.03258505463600159, acc: 1.0)
[2025-02-13 21:11:23,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:23,247][root][INFO] - Training Epoch: 2/2, step 6366/7134 completed (loss: 0.06620360165834427, acc: 0.9940828680992126)
[2025-02-13 21:11:23,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:23,621][root][INFO] - Training Epoch: 2/2, step 6367/7134 completed (loss: 0.03105933591723442, acc: 1.0)
[2025-02-13 21:11:23,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:23,987][root][INFO] - Training Epoch: 2/2, step 6368/7134 completed (loss: 0.08238636702299118, acc: 0.9819276928901672)
[2025-02-13 21:11:24,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:24,362][root][INFO] - Training Epoch: 2/2, step 6369/7134 completed (loss: 0.07851332426071167, acc: 0.9821428656578064)
[2025-02-13 21:11:24,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:24,749][root][INFO] - Training Epoch: 2/2, step 6370/7134 completed (loss: 0.14500002562999725, acc: 0.9476743936538696)
[2025-02-13 21:11:24,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:25,110][root][INFO] - Training Epoch: 2/2, step 6371/7134 completed (loss: 0.10151012986898422, acc: 0.9803921580314636)
[2025-02-13 21:11:25,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:25,461][root][INFO] - Training Epoch: 2/2, step 6372/7134 completed (loss: 0.09331084787845612, acc: 0.982758641242981)
[2025-02-13 21:11:25,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:25,797][root][INFO] - Training Epoch: 2/2, step 6373/7134 completed (loss: 0.04977403208613396, acc: 0.9919354915618896)
[2025-02-13 21:11:25,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:26,209][root][INFO] - Training Epoch: 2/2, step 6374/7134 completed (loss: 0.029373135417699814, acc: 1.0)
[2025-02-13 21:11:26,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:26,614][root][INFO] - Training Epoch: 2/2, step 6375/7134 completed (loss: 0.11862411350011826, acc: 0.9723756909370422)
[2025-02-13 21:11:26,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:26,996][root][INFO] - Training Epoch: 2/2, step 6376/7134 completed (loss: 0.029864443466067314, acc: 1.0)
[2025-02-13 21:11:27,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:27,393][root][INFO] - Training Epoch: 2/2, step 6377/7134 completed (loss: 0.07564689218997955, acc: 0.984000027179718)
[2025-02-13 21:11:27,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:27,775][root][INFO] - Training Epoch: 2/2, step 6378/7134 completed (loss: 0.07228326052427292, acc: 0.9720930457115173)
[2025-02-13 21:11:27,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:28,134][root][INFO] - Training Epoch: 2/2, step 6379/7134 completed (loss: 0.061301182955503464, acc: 0.9842105507850647)
[2025-02-13 21:11:28,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:28,523][root][INFO] - Training Epoch: 2/2, step 6380/7134 completed (loss: 0.12340960651636124, acc: 0.9750000238418579)
[2025-02-13 21:11:28,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:28,896][root][INFO] - Training Epoch: 2/2, step 6381/7134 completed (loss: 0.019419850781559944, acc: 1.0)
[2025-02-13 21:11:29,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:29,281][root][INFO] - Training Epoch: 2/2, step 6382/7134 completed (loss: 0.019948411732912064, acc: 1.0)
[2025-02-13 21:11:29,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:29,641][root][INFO] - Training Epoch: 2/2, step 6383/7134 completed (loss: 0.18280938267707825, acc: 0.9710982441902161)
[2025-02-13 21:11:29,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:30,042][root][INFO] - Training Epoch: 2/2, step 6384/7134 completed (loss: 0.06011900678277016, acc: 0.9873417615890503)
[2025-02-13 21:11:30,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:30,457][root][INFO] - Training Epoch: 2/2, step 6385/7134 completed (loss: 0.16954928636550903, acc: 0.9658536314964294)
[2025-02-13 21:11:30,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:30,822][root][INFO] - Training Epoch: 2/2, step 6386/7134 completed (loss: 0.0561317577958107, acc: 0.9731183052062988)
[2025-02-13 21:11:30,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:31,179][root][INFO] - Training Epoch: 2/2, step 6387/7134 completed (loss: 0.07148484885692596, acc: 0.9939393997192383)
[2025-02-13 21:11:31,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:31,543][root][INFO] - Training Epoch: 2/2, step 6388/7134 completed (loss: 0.050982970744371414, acc: 0.9873417615890503)
[2025-02-13 21:11:31,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:31,918][root][INFO] - Training Epoch: 2/2, step 6389/7134 completed (loss: 0.07174647599458694, acc: 0.9807692170143127)
[2025-02-13 21:11:32,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:32,287][root][INFO] - Training Epoch: 2/2, step 6390/7134 completed (loss: 0.043220628052949905, acc: 0.991304337978363)
[2025-02-13 21:11:32,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:32,699][root][INFO] - Training Epoch: 2/2, step 6391/7134 completed (loss: 0.1890569031238556, acc: 0.9593023061752319)
[2025-02-13 21:11:32,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:33,105][root][INFO] - Training Epoch: 2/2, step 6392/7134 completed (loss: 0.14663304388523102, acc: 0.960869550704956)
[2025-02-13 21:11:33,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:33,521][root][INFO] - Training Epoch: 2/2, step 6393/7134 completed (loss: 0.04090718924999237, acc: 0.9907407164573669)
[2025-02-13 21:11:33,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:33,875][root][INFO] - Training Epoch: 2/2, step 6394/7134 completed (loss: 0.14225512742996216, acc: 0.9833333492279053)
[2025-02-13 21:11:34,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:34,242][root][INFO] - Training Epoch: 2/2, step 6395/7134 completed (loss: 0.06336227804422379, acc: 0.9851484894752502)
[2025-02-13 21:11:34,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:34,599][root][INFO] - Training Epoch: 2/2, step 6396/7134 completed (loss: 0.07090441137552261, acc: 0.9760765433311462)
[2025-02-13 21:11:34,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:34,963][root][INFO] - Training Epoch: 2/2, step 6397/7134 completed (loss: 0.024474378675222397, acc: 1.0)
[2025-02-13 21:11:35,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:35,328][root][INFO] - Training Epoch: 2/2, step 6398/7134 completed (loss: 0.05339938774704933, acc: 0.9862068891525269)
[2025-02-13 21:11:35,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:35,687][root][INFO] - Training Epoch: 2/2, step 6399/7134 completed (loss: 0.0775039866566658, acc: 0.9887640476226807)
[2025-02-13 21:11:35,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:36,017][root][INFO] - Training Epoch: 2/2, step 6400/7134 completed (loss: 0.015381836332380772, acc: 1.0)
[2025-02-13 21:11:36,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:36,394][root][INFO] - Training Epoch: 2/2, step 6401/7134 completed (loss: 0.045401446521282196, acc: 0.9904761910438538)
[2025-02-13 21:11:36,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:36,791][root][INFO] - Training Epoch: 2/2, step 6402/7134 completed (loss: 0.1022334024310112, acc: 0.9747474789619446)
[2025-02-13 21:11:36,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:37,198][root][INFO] - Training Epoch: 2/2, step 6403/7134 completed (loss: 0.02256815694272518, acc: 0.9948186278343201)
[2025-02-13 21:11:37,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:37,596][root][INFO] - Training Epoch: 2/2, step 6404/7134 completed (loss: 0.05806179717183113, acc: 0.9822485446929932)
[2025-02-13 21:11:37,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:38,046][root][INFO] - Training Epoch: 2/2, step 6405/7134 completed (loss: 0.05848397687077522, acc: 0.9903846383094788)
[2025-02-13 21:11:38,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:38,418][root][INFO] - Training Epoch: 2/2, step 6406/7134 completed (loss: 0.09032920002937317, acc: 0.9700000286102295)
[2025-02-13 21:11:38,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:38,799][root][INFO] - Training Epoch: 2/2, step 6407/7134 completed (loss: 0.05710679665207863, acc: 0.9888888597488403)
[2025-02-13 21:11:38,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:39,155][root][INFO] - Training Epoch: 2/2, step 6408/7134 completed (loss: 0.11177211254835129, acc: 0.9615384340286255)
[2025-02-13 21:11:39,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:39,520][root][INFO] - Training Epoch: 2/2, step 6409/7134 completed (loss: 0.130374938249588, acc: 0.9878048896789551)
[2025-02-13 21:11:39,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:39,887][root][INFO] - Training Epoch: 2/2, step 6410/7134 completed (loss: 0.07527093589305878, acc: 0.9916666746139526)
[2025-02-13 21:11:40,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:40,223][root][INFO] - Training Epoch: 2/2, step 6411/7134 completed (loss: 0.09820384532213211, acc: 0.9767441749572754)
[2025-02-13 21:11:40,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:40,581][root][INFO] - Training Epoch: 2/2, step 6412/7134 completed (loss: 0.023453451693058014, acc: 0.9935483932495117)
[2025-02-13 21:11:40,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:40,990][root][INFO] - Training Epoch: 2/2, step 6413/7134 completed (loss: 0.13934165239334106, acc: 0.9731543660163879)
[2025-02-13 21:11:41,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:41,360][root][INFO] - Training Epoch: 2/2, step 6414/7134 completed (loss: 0.036069709807634354, acc: 0.9802631735801697)
[2025-02-13 21:11:41,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:41,719][root][INFO] - Training Epoch: 2/2, step 6415/7134 completed (loss: 0.02673538774251938, acc: 0.9943181872367859)
[2025-02-13 21:11:41,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:42,085][root][INFO] - Training Epoch: 2/2, step 6416/7134 completed (loss: 0.08417234569787979, acc: 0.97826087474823)
[2025-02-13 21:11:42,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:42,443][root][INFO] - Training Epoch: 2/2, step 6417/7134 completed (loss: 0.11527658253908157, acc: 0.961240291595459)
[2025-02-13 21:11:42,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:42,817][root][INFO] - Training Epoch: 2/2, step 6418/7134 completed (loss: 0.1605854034423828, acc: 0.9530201554298401)
[2025-02-13 21:11:42,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:43,184][root][INFO] - Training Epoch: 2/2, step 6419/7134 completed (loss: 0.18180769681930542, acc: 0.9610389471054077)
[2025-02-13 21:11:43,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:43,574][root][INFO] - Training Epoch: 2/2, step 6420/7134 completed (loss: 0.11347775161266327, acc: 0.9746835231781006)
[2025-02-13 21:11:43,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:43,932][root][INFO] - Training Epoch: 2/2, step 6421/7134 completed (loss: 0.16757631301879883, acc: 0.9638554453849792)
[2025-02-13 21:11:44,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:44,328][root][INFO] - Training Epoch: 2/2, step 6422/7134 completed (loss: 0.11863839626312256, acc: 0.9608938694000244)
[2025-02-13 21:11:44,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:44,718][root][INFO] - Training Epoch: 2/2, step 6423/7134 completed (loss: 0.1143658310174942, acc: 0.9807692170143127)
[2025-02-13 21:11:44,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:45,092][root][INFO] - Training Epoch: 2/2, step 6424/7134 completed (loss: 0.05100390687584877, acc: 0.9893617033958435)
[2025-02-13 21:11:45,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:45,470][root][INFO] - Training Epoch: 2/2, step 6425/7134 completed (loss: 0.04841788485646248, acc: 0.9888268113136292)
[2025-02-13 21:11:45,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:45,847][root][INFO] - Training Epoch: 2/2, step 6426/7134 completed (loss: 0.0849672332406044, acc: 0.9776536226272583)
[2025-02-13 21:11:45,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:46,247][root][INFO] - Training Epoch: 2/2, step 6427/7134 completed (loss: 0.07534481585025787, acc: 0.9808917045593262)
[2025-02-13 21:11:46,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:46,647][root][INFO] - Training Epoch: 2/2, step 6428/7134 completed (loss: 0.08817531913518906, acc: 0.9808917045593262)
[2025-02-13 21:11:46,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:47,051][root][INFO] - Training Epoch: 2/2, step 6429/7134 completed (loss: 0.061270665377378464, acc: 0.9806451797485352)
[2025-02-13 21:11:47,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:47,423][root][INFO] - Training Epoch: 2/2, step 6430/7134 completed (loss: 0.17631593346595764, acc: 0.9624999761581421)
[2025-02-13 21:11:47,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:47,787][root][INFO] - Training Epoch: 2/2, step 6431/7134 completed (loss: 0.17225214838981628, acc: 0.9513513445854187)
[2025-02-13 21:11:47,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:48,154][root][INFO] - Training Epoch: 2/2, step 6432/7134 completed (loss: 0.48325756192207336, acc: 0.8774193525314331)
[2025-02-13 21:11:48,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:48,537][root][INFO] - Training Epoch: 2/2, step 6433/7134 completed (loss: 0.1387847512960434, acc: 0.9715909361839294)
[2025-02-13 21:11:48,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:48,893][root][INFO] - Training Epoch: 2/2, step 6434/7134 completed (loss: 0.050345148891210556, acc: 0.9826589822769165)
[2025-02-13 21:11:49,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:49,240][root][INFO] - Training Epoch: 2/2, step 6435/7134 completed (loss: 0.06950924545526505, acc: 0.97826087474823)
[2025-02-13 21:11:49,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:49,647][root][INFO] - Training Epoch: 2/2, step 6436/7134 completed (loss: 0.04154571145772934, acc: 0.9798657894134521)
[2025-02-13 21:11:49,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:50,039][root][INFO] - Training Epoch: 2/2, step 6437/7134 completed (loss: 0.14575618505477905, acc: 0.9671052694320679)
[2025-02-13 21:11:50,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:50,434][root][INFO] - Training Epoch: 2/2, step 6438/7134 completed (loss: 0.1527668535709381, acc: 0.9611650705337524)
[2025-02-13 21:11:50,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:50,824][root][INFO] - Training Epoch: 2/2, step 6439/7134 completed (loss: 0.06891797482967377, acc: 0.9813664555549622)
[2025-02-13 21:11:50,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:51,243][root][INFO] - Training Epoch: 2/2, step 6440/7134 completed (loss: 0.06704078614711761, acc: 0.9807692170143127)
[2025-02-13 21:11:51,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:51,634][root][INFO] - Training Epoch: 2/2, step 6441/7134 completed (loss: 0.036244072020053864, acc: 1.0)
[2025-02-13 21:11:51,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:52,041][root][INFO] - Training Epoch: 2/2, step 6442/7134 completed (loss: 0.03848673403263092, acc: 0.9951456189155579)
[2025-02-13 21:11:52,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:52,427][root][INFO] - Training Epoch: 2/2, step 6443/7134 completed (loss: 0.04045732691884041, acc: 0.9852941036224365)
[2025-02-13 21:11:52,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:52,838][root][INFO] - Training Epoch: 2/2, step 6444/7134 completed (loss: 0.05232089012861252, acc: 0.9833333492279053)
[2025-02-13 21:11:52,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:53,218][root][INFO] - Training Epoch: 2/2, step 6445/7134 completed (loss: 0.1413678228855133, acc: 0.9753694534301758)
[2025-02-13 21:11:53,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:53,605][root][INFO] - Training Epoch: 2/2, step 6446/7134 completed (loss: 0.04382322356104851, acc: 0.9950980544090271)
[2025-02-13 21:11:53,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:54,013][root][INFO] - Training Epoch: 2/2, step 6447/7134 completed (loss: 0.07304135710000992, acc: 0.9791666865348816)
[2025-02-13 21:11:54,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:54,423][root][INFO] - Training Epoch: 2/2, step 6448/7134 completed (loss: 0.23972339928150177, acc: 0.9419354796409607)
[2025-02-13 21:11:54,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:54,815][root][INFO] - Training Epoch: 2/2, step 6449/7134 completed (loss: 0.06775756925344467, acc: 0.984455943107605)
[2025-02-13 21:11:54,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:55,209][root][INFO] - Training Epoch: 2/2, step 6450/7134 completed (loss: 0.04074028506875038, acc: 0.9942196607589722)
[2025-02-13 21:11:55,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:55,654][root][INFO] - Training Epoch: 2/2, step 6451/7134 completed (loss: 0.1205761581659317, acc: 0.9747899174690247)
[2025-02-13 21:11:55,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:56,043][root][INFO] - Training Epoch: 2/2, step 6452/7134 completed (loss: 0.023250862956047058, acc: 0.9948979616165161)
[2025-02-13 21:11:56,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:56,473][root][INFO] - Training Epoch: 2/2, step 6453/7134 completed (loss: 0.03987906128168106, acc: 1.0)
[2025-02-13 21:11:56,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:56,903][root][INFO] - Training Epoch: 2/2, step 6454/7134 completed (loss: 0.09550482034683228, acc: 0.9836065769195557)
[2025-02-13 21:11:57,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:57,287][root][INFO] - Training Epoch: 2/2, step 6455/7134 completed (loss: 0.1191415935754776, acc: 0.9733333587646484)
[2025-02-13 21:11:57,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:57,777][root][INFO] - Training Epoch: 2/2, step 6456/7134 completed (loss: 0.11999940872192383, acc: 0.970059871673584)
[2025-02-13 21:11:57,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:58,241][root][INFO] - Training Epoch: 2/2, step 6457/7134 completed (loss: 0.02745089866220951, acc: 0.9908257126808167)
[2025-02-13 21:11:58,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:58,621][root][INFO] - Training Epoch: 2/2, step 6458/7134 completed (loss: 0.04159059375524521, acc: 1.0)
[2025-02-13 21:11:58,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:59,026][root][INFO] - Training Epoch: 2/2, step 6459/7134 completed (loss: 0.01105943787842989, acc: 1.0)
[2025-02-13 21:11:59,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:59,434][root][INFO] - Training Epoch: 2/2, step 6460/7134 completed (loss: 0.04599229618906975, acc: 0.9846938848495483)
[2025-02-13 21:11:59,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:59,796][root][INFO] - Training Epoch: 2/2, step 6461/7134 completed (loss: 0.025877952575683594, acc: 0.9942196607589722)
[2025-02-13 21:11:59,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:00,171][root][INFO] - Training Epoch: 2/2, step 6462/7134 completed (loss: 0.03389815241098404, acc: 0.9893617033958435)
[2025-02-13 21:12:00,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:00,562][root][INFO] - Training Epoch: 2/2, step 6463/7134 completed (loss: 0.09408124536275864, acc: 0.9757281541824341)
[2025-02-13 21:12:00,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:00,956][root][INFO] - Training Epoch: 2/2, step 6464/7134 completed (loss: 0.06879029422998428, acc: 0.9832402467727661)
[2025-02-13 21:12:01,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:01,346][root][INFO] - Training Epoch: 2/2, step 6465/7134 completed (loss: 0.015754610300064087, acc: 1.0)
[2025-02-13 21:12:01,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:01,730][root][INFO] - Training Epoch: 2/2, step 6466/7134 completed (loss: 0.0480811670422554, acc: 0.9841269850730896)
[2025-02-13 21:12:01,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:02,124][root][INFO] - Training Epoch: 2/2, step 6467/7134 completed (loss: 0.15229713916778564, acc: 0.9698795080184937)
[2025-02-13 21:12:02,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:02,537][root][INFO] - Training Epoch: 2/2, step 6468/7134 completed (loss: 0.047119978815317154, acc: 0.9852941036224365)
[2025-02-13 21:12:02,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:02,985][root][INFO] - Training Epoch: 2/2, step 6469/7134 completed (loss: 0.15106113255023956, acc: 0.9719626307487488)
[2025-02-13 21:12:03,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:03,402][root][INFO] - Training Epoch: 2/2, step 6470/7134 completed (loss: 0.06764047592878342, acc: 0.9803921580314636)
[2025-02-13 21:12:03,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:03,831][root][INFO] - Training Epoch: 2/2, step 6471/7134 completed (loss: 0.03298691287636757, acc: 1.0)
[2025-02-13 21:12:03,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:04,226][root][INFO] - Training Epoch: 2/2, step 6472/7134 completed (loss: 0.11428801715373993, acc: 0.9800000190734863)
[2025-02-13 21:12:04,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:04,611][root][INFO] - Training Epoch: 2/2, step 6473/7134 completed (loss: 0.055415842682123184, acc: 0.9722222089767456)
[2025-02-13 21:12:04,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:04,972][root][INFO] - Training Epoch: 2/2, step 6474/7134 completed (loss: 0.0625186339020729, acc: 0.9784946441650391)
[2025-02-13 21:12:05,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:05,327][root][INFO] - Training Epoch: 2/2, step 6475/7134 completed (loss: 0.03888363391160965, acc: 0.9928571581840515)
[2025-02-13 21:12:05,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:05,747][root][INFO] - Training Epoch: 2/2, step 6476/7134 completed (loss: 0.04006192833185196, acc: 0.9866666793823242)
[2025-02-13 21:12:05,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:06,195][root][INFO] - Training Epoch: 2/2, step 6477/7134 completed (loss: 0.030524790287017822, acc: 0.991304337978363)
[2025-02-13 21:12:06,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:06,592][root][INFO] - Training Epoch: 2/2, step 6478/7134 completed (loss: 0.06504891812801361, acc: 0.9710144996643066)
[2025-02-13 21:12:06,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:06,979][root][INFO] - Training Epoch: 2/2, step 6479/7134 completed (loss: 0.03525310754776001, acc: 0.9932885766029358)
[2025-02-13 21:12:07,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:07,373][root][INFO] - Training Epoch: 2/2, step 6480/7134 completed (loss: 0.03056158311665058, acc: 0.9928571581840515)
[2025-02-13 21:12:07,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:07,755][root][INFO] - Training Epoch: 2/2, step 6481/7134 completed (loss: 0.02717108279466629, acc: 0.9857142567634583)
[2025-02-13 21:12:07,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:08,169][root][INFO] - Training Epoch: 2/2, step 6482/7134 completed (loss: 0.011572586372494698, acc: 1.0)
[2025-02-13 21:12:08,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:08,590][root][INFO] - Training Epoch: 2/2, step 6483/7134 completed (loss: 0.09059645235538483, acc: 0.9930555820465088)
[2025-02-13 21:12:08,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:09,031][root][INFO] - Training Epoch: 2/2, step 6484/7134 completed (loss: 0.05049917846918106, acc: 0.988304078578949)
[2025-02-13 21:12:09,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:09,527][root][INFO] - Training Epoch: 2/2, step 6485/7134 completed (loss: 0.014990709722042084, acc: 1.0)
[2025-02-13 21:12:09,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:09,961][root][INFO] - Training Epoch: 2/2, step 6486/7134 completed (loss: 0.17315517365932465, acc: 0.95652174949646)
[2025-02-13 21:12:10,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:10,348][root][INFO] - Training Epoch: 2/2, step 6487/7134 completed (loss: 0.10974179953336716, acc: 0.970588207244873)
[2025-02-13 21:12:10,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:10,755][root][INFO] - Training Epoch: 2/2, step 6488/7134 completed (loss: 0.05402756109833717, acc: 0.9848484992980957)
[2025-02-13 21:12:10,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:11,145][root][INFO] - Training Epoch: 2/2, step 6489/7134 completed (loss: 0.13149307668209076, acc: 0.9743589758872986)
[2025-02-13 21:12:11,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:11,538][root][INFO] - Training Epoch: 2/2, step 6490/7134 completed (loss: 0.22493892908096313, acc: 0.9552238583564758)
[2025-02-13 21:12:11,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:11,967][root][INFO] - Training Epoch: 2/2, step 6491/7134 completed (loss: 0.12730520963668823, acc: 0.9740259647369385)
[2025-02-13 21:12:12,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:12,338][root][INFO] - Training Epoch: 2/2, step 6492/7134 completed (loss: 0.07105512917041779, acc: 0.9806451797485352)
[2025-02-13 21:12:12,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:12,716][root][INFO] - Training Epoch: 2/2, step 6493/7134 completed (loss: 0.060130421072244644, acc: 0.9821428656578064)
[2025-02-13 21:12:12,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:13,044][root][INFO] - Training Epoch: 2/2, step 6494/7134 completed (loss: 0.15746790170669556, acc: 0.9646017551422119)
[2025-02-13 21:12:13,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:13,421][root][INFO] - Training Epoch: 2/2, step 6495/7134 completed (loss: 0.06993332505226135, acc: 0.9876543283462524)
[2025-02-13 21:12:13,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:13,838][root][INFO] - Training Epoch: 2/2, step 6496/7134 completed (loss: 0.05821209400892258, acc: 0.9768785834312439)
[2025-02-13 21:12:13,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:14,220][root][INFO] - Training Epoch: 2/2, step 6497/7134 completed (loss: 0.16433773934841156, acc: 0.9583333134651184)
[2025-02-13 21:12:14,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:14,688][root][INFO] - Training Epoch: 2/2, step 6498/7134 completed (loss: 0.07703879475593567, acc: 0.9764705896377563)
[2025-02-13 21:12:14,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:15,139][root][INFO] - Training Epoch: 2/2, step 6499/7134 completed (loss: 0.09416840225458145, acc: 0.978723406791687)
[2025-02-13 21:12:15,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:15,582][root][INFO] - Training Epoch: 2/2, step 6500/7134 completed (loss: 0.1941361427307129, acc: 0.959770143032074)
[2025-02-13 21:12:15,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:15,965][root][INFO] - Training Epoch: 2/2, step 6501/7134 completed (loss: 0.3524898290634155, acc: 0.9226804375648499)
[2025-02-13 21:12:16,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:16,414][root][INFO] - Training Epoch: 2/2, step 6502/7134 completed (loss: 0.08873675763607025, acc: 0.9646464586257935)
[2025-02-13 21:12:16,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:16,791][root][INFO] - Training Epoch: 2/2, step 6503/7134 completed (loss: 0.11756674200296402, acc: 0.9689440727233887)
[2025-02-13 21:12:16,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:17,145][root][INFO] - Training Epoch: 2/2, step 6504/7134 completed (loss: 0.09416653215885162, acc: 0.9863013625144958)
[2025-02-13 21:12:17,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:17,506][root][INFO] - Training Epoch: 2/2, step 6505/7134 completed (loss: 0.1427968144416809, acc: 0.9666666388511658)
[2025-02-13 21:12:17,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:17,917][root][INFO] - Training Epoch: 2/2, step 6506/7134 completed (loss: 0.1510344296693802, acc: 0.969072163105011)
[2025-02-13 21:12:18,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:18,403][root][INFO] - Training Epoch: 2/2, step 6507/7134 completed (loss: 0.17011678218841553, acc: 0.9509202241897583)
[2025-02-13 21:12:18,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:18,804][root][INFO] - Training Epoch: 2/2, step 6508/7134 completed (loss: 0.05888281762599945, acc: 0.9888888597488403)
[2025-02-13 21:12:18,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:19,169][root][INFO] - Training Epoch: 2/2, step 6509/7134 completed (loss: 0.04587353393435478, acc: 0.9948453903198242)
[2025-02-13 21:12:19,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:19,556][root][INFO] - Training Epoch: 2/2, step 6510/7134 completed (loss: 0.09628192335367203, acc: 0.9805825352668762)
[2025-02-13 21:12:19,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:19,919][root][INFO] - Training Epoch: 2/2, step 6511/7134 completed (loss: 0.08935273438692093, acc: 0.9776119589805603)
[2025-02-13 21:12:20,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:20,284][root][INFO] - Training Epoch: 2/2, step 6512/7134 completed (loss: 0.018746227025985718, acc: 1.0)
[2025-02-13 21:12:20,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:20,642][root][INFO] - Training Epoch: 2/2, step 6513/7134 completed (loss: 0.06444443017244339, acc: 0.9851852059364319)
[2025-02-13 21:12:20,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:21,025][root][INFO] - Training Epoch: 2/2, step 6514/7134 completed (loss: 0.20997241139411926, acc: 0.9629629850387573)
[2025-02-13 21:12:21,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:21,416][root][INFO] - Training Epoch: 2/2, step 6515/7134 completed (loss: 0.10114181786775589, acc: 0.9670329689979553)
[2025-02-13 21:12:21,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:21,805][root][INFO] - Training Epoch: 2/2, step 6516/7134 completed (loss: 0.1091991737484932, acc: 0.9764705896377563)
[2025-02-13 21:12:21,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:22,195][root][INFO] - Training Epoch: 2/2, step 6517/7134 completed (loss: 0.03578566759824753, acc: 0.9937888383865356)
[2025-02-13 21:12:22,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:22,578][root][INFO] - Training Epoch: 2/2, step 6518/7134 completed (loss: 0.02083464153110981, acc: 1.0)
[2025-02-13 21:12:22,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:22,942][root][INFO] - Training Epoch: 2/2, step 6519/7134 completed (loss: 0.04457174614071846, acc: 0.9921259880065918)
[2025-02-13 21:12:23,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:23,345][root][INFO] - Training Epoch: 2/2, step 6520/7134 completed (loss: 0.03653884306550026, acc: 0.9870129823684692)
[2025-02-13 21:12:23,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:23,749][root][INFO] - Training Epoch: 2/2, step 6521/7134 completed (loss: 0.038332149386405945, acc: 0.9837837815284729)
[2025-02-13 21:12:23,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:24,125][root][INFO] - Training Epoch: 2/2, step 6522/7134 completed (loss: 0.046871818602085114, acc: 0.9934640526771545)
[2025-02-13 21:12:24,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:24,496][root][INFO] - Training Epoch: 2/2, step 6523/7134 completed (loss: 0.21368533372879028, acc: 0.9722222089767456)
[2025-02-13 21:12:24,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:24,948][root][INFO] - Training Epoch: 2/2, step 6524/7134 completed (loss: 0.12791579961776733, acc: 0.9668508172035217)
[2025-02-13 21:12:25,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:25,318][root][INFO] - Training Epoch: 2/2, step 6525/7134 completed (loss: 0.02494361624121666, acc: 0.9917355179786682)
[2025-02-13 21:12:25,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:25,681][root][INFO] - Training Epoch: 2/2, step 6526/7134 completed (loss: 0.06865440309047699, acc: 0.9847715497016907)
[2025-02-13 21:12:25,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:26,102][root][INFO] - Training Epoch: 2/2, step 6527/7134 completed (loss: 0.07549276202917099, acc: 0.9873417615890503)
[2025-02-13 21:12:26,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:26,505][root][INFO] - Training Epoch: 2/2, step 6528/7134 completed (loss: 0.16055577993392944, acc: 0.9637681245803833)
[2025-02-13 21:12:26,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:26,870][root][INFO] - Training Epoch: 2/2, step 6529/7134 completed (loss: 0.18505671620368958, acc: 0.9694656729698181)
[2025-02-13 21:12:27,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:27,242][root][INFO] - Training Epoch: 2/2, step 6530/7134 completed (loss: 0.07553940266370773, acc: 0.9779005646705627)
[2025-02-13 21:12:27,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:27,599][root][INFO] - Training Epoch: 2/2, step 6531/7134 completed (loss: 0.046035923063755035, acc: 0.9878048896789551)
[2025-02-13 21:12:27,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:27,957][root][INFO] - Training Epoch: 2/2, step 6532/7134 completed (loss: 0.161629781126976, acc: 0.9670329689979553)
[2025-02-13 21:12:28,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:28,323][root][INFO] - Training Epoch: 2/2, step 6533/7134 completed (loss: 0.01444210670888424, acc: 1.0)
[2025-02-13 21:12:28,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:28,671][root][INFO] - Training Epoch: 2/2, step 6534/7134 completed (loss: 0.18550555408000946, acc: 0.9653179049491882)
[2025-02-13 21:12:28,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:29,020][root][INFO] - Training Epoch: 2/2, step 6535/7134 completed (loss: 0.028990088030695915, acc: 0.9905660152435303)
[2025-02-13 21:12:29,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:29,371][root][INFO] - Training Epoch: 2/2, step 6536/7134 completed (loss: 0.03385185822844505, acc: 0.991304337978363)
[2025-02-13 21:12:29,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:29,713][root][INFO] - Training Epoch: 2/2, step 6537/7134 completed (loss: 0.020417379215359688, acc: 1.0)
[2025-02-13 21:12:29,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:30,067][root][INFO] - Training Epoch: 2/2, step 6538/7134 completed (loss: 0.1415434330701828, acc: 0.965753436088562)
[2025-02-13 21:12:30,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:30,418][root][INFO] - Training Epoch: 2/2, step 6539/7134 completed (loss: 0.014510799199342728, acc: 1.0)
[2025-02-13 21:12:30,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:30,810][root][INFO] - Training Epoch: 2/2, step 6540/7134 completed (loss: 0.33622679114341736, acc: 0.9204545617103577)
[2025-02-13 21:12:30,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:31,189][root][INFO] - Training Epoch: 2/2, step 6541/7134 completed (loss: 0.043848391622304916, acc: 0.988950252532959)
[2025-02-13 21:12:31,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:31,569][root][INFO] - Training Epoch: 2/2, step 6542/7134 completed (loss: 0.08340495824813843, acc: 0.9857142567634583)
[2025-02-13 21:12:31,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:31,950][root][INFO] - Training Epoch: 2/2, step 6543/7134 completed (loss: 0.05145951360464096, acc: 0.9912280440330505)
[2025-02-13 21:12:32,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:32,309][root][INFO] - Training Epoch: 2/2, step 6544/7134 completed (loss: 0.13402990996837616, acc: 0.9639639854431152)
[2025-02-13 21:12:32,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:32,659][root][INFO] - Training Epoch: 2/2, step 6545/7134 completed (loss: 0.021976349875330925, acc: 0.9834710955619812)
[2025-02-13 21:12:32,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:32,992][root][INFO] - Training Epoch: 2/2, step 6546/7134 completed (loss: 0.10422635078430176, acc: 0.9710144996643066)
[2025-02-13 21:12:33,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:33,355][root][INFO] - Training Epoch: 2/2, step 6547/7134 completed (loss: 0.17887702584266663, acc: 0.9710144996643066)
[2025-02-13 21:12:33,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:33,682][root][INFO] - Training Epoch: 2/2, step 6548/7134 completed (loss: 0.01696949452161789, acc: 1.0)
[2025-02-13 21:12:33,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:34,090][root][INFO] - Training Epoch: 2/2, step 6549/7134 completed (loss: 0.19532977044582367, acc: 0.9541984796524048)
[2025-02-13 21:12:34,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:34,455][root][INFO] - Training Epoch: 2/2, step 6550/7134 completed (loss: 0.18280506134033203, acc: 0.9617834687232971)
[2025-02-13 21:12:34,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:34,812][root][INFO] - Training Epoch: 2/2, step 6551/7134 completed (loss: 0.059205736964941025, acc: 0.9847715497016907)
[2025-02-13 21:12:34,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:35,177][root][INFO] - Training Epoch: 2/2, step 6552/7134 completed (loss: 0.14940954744815826, acc: 0.9559471607208252)
[2025-02-13 21:12:35,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:35,547][root][INFO] - Training Epoch: 2/2, step 6553/7134 completed (loss: 0.10234523564577103, acc: 0.9797979593276978)
[2025-02-13 21:12:35,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:35,921][root][INFO] - Training Epoch: 2/2, step 6554/7134 completed (loss: 0.09286627173423767, acc: 0.9906542301177979)
[2025-02-13 21:12:36,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:36,291][root][INFO] - Training Epoch: 2/2, step 6555/7134 completed (loss: 0.10006267577409744, acc: 0.9784482717514038)
[2025-02-13 21:12:36,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:36,641][root][INFO] - Training Epoch: 2/2, step 6556/7134 completed (loss: 0.13044637441635132, acc: 0.9819004535675049)
[2025-02-13 21:12:36,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:37,017][root][INFO] - Training Epoch: 2/2, step 6557/7134 completed (loss: 0.13201278448104858, acc: 0.9684684872627258)
[2025-02-13 21:12:37,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:37,435][root][INFO] - Training Epoch: 2/2, step 6558/7134 completed (loss: 0.1277717500925064, acc: 0.9813084006309509)
[2025-02-13 21:12:37,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:37,824][root][INFO] - Training Epoch: 2/2, step 6559/7134 completed (loss: 0.07605747133493423, acc: 0.9819276928901672)
[2025-02-13 21:12:38,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:38,270][root][INFO] - Training Epoch: 2/2, step 6560/7134 completed (loss: 0.044878605753183365, acc: 0.9895287752151489)
[2025-02-13 21:12:38,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:38,619][root][INFO] - Training Epoch: 2/2, step 6561/7134 completed (loss: 0.047195740044116974, acc: 0.9947643876075745)
[2025-02-13 21:12:38,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:39,016][root][INFO] - Training Epoch: 2/2, step 6562/7134 completed (loss: 0.04777104780077934, acc: 0.9937106966972351)
[2025-02-13 21:12:39,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:39,419][root][INFO] - Training Epoch: 2/2, step 6563/7134 completed (loss: 0.08405949175357819, acc: 0.9747474789619446)
[2025-02-13 21:12:39,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:39,816][root][INFO] - Training Epoch: 2/2, step 6564/7134 completed (loss: 0.05978243052959442, acc: 0.9939758777618408)
[2025-02-13 21:12:39,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:40,192][root][INFO] - Training Epoch: 2/2, step 6565/7134 completed (loss: 0.18054941296577454, acc: 0.9729729890823364)
[2025-02-13 21:12:40,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:40,573][root][INFO] - Training Epoch: 2/2, step 6566/7134 completed (loss: 0.06437930464744568, acc: 0.975806474685669)
[2025-02-13 21:12:40,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:40,990][root][INFO] - Training Epoch: 2/2, step 6567/7134 completed (loss: 0.07993310689926147, acc: 0.9822221994400024)
[2025-02-13 21:12:41,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:41,357][root][INFO] - Training Epoch: 2/2, step 6568/7134 completed (loss: 0.059036143124103546, acc: 0.9890109896659851)
[2025-02-13 21:12:41,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:41,705][root][INFO] - Training Epoch: 2/2, step 6569/7134 completed (loss: 0.058862004429101944, acc: 0.989847719669342)
[2025-02-13 21:12:41,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:42,078][root][INFO] - Training Epoch: 2/2, step 6570/7134 completed (loss: 0.05508538335561752, acc: 0.9950739145278931)
[2025-02-13 21:12:42,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:42,414][root][INFO] - Training Epoch: 2/2, step 6571/7134 completed (loss: 0.02566724829375744, acc: 1.0)
[2025-02-13 21:12:42,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:42,777][root][INFO] - Training Epoch: 2/2, step 6572/7134 completed (loss: 0.07014632970094681, acc: 0.9804878234863281)
[2025-02-13 21:12:42,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:43,181][root][INFO] - Training Epoch: 2/2, step 6573/7134 completed (loss: 0.028865348547697067, acc: 0.9902912378311157)
[2025-02-13 21:12:43,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:43,560][root][INFO] - Training Epoch: 2/2, step 6574/7134 completed (loss: 0.03302699327468872, acc: 0.9956896305084229)
[2025-02-13 21:12:43,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:43,992][root][INFO] - Training Epoch: 2/2, step 6575/7134 completed (loss: 0.11188793927431107, acc: 0.9624413251876831)
[2025-02-13 21:12:44,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:44,397][root][INFO] - Training Epoch: 2/2, step 6576/7134 completed (loss: 0.03926509991288185, acc: 0.9862385392189026)
[2025-02-13 21:12:44,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:44,797][root][INFO] - Training Epoch: 2/2, step 6577/7134 completed (loss: 0.03372858092188835, acc: 0.9939393997192383)
[2025-02-13 21:12:45,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:45,294][root][INFO] - Training Epoch: 2/2, step 6578/7134 completed (loss: 0.040484409779310226, acc: 0.9918032884597778)
[2025-02-13 21:12:45,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:45,681][root][INFO] - Training Epoch: 2/2, step 6579/7134 completed (loss: 0.1552753448486328, acc: 0.9774436354637146)
[2025-02-13 21:12:45,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:46,093][root][INFO] - Training Epoch: 2/2, step 6580/7134 completed (loss: 0.07575292885303497, acc: 0.9789473414421082)
[2025-02-13 21:12:46,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:46,464][root][INFO] - Training Epoch: 2/2, step 6581/7134 completed (loss: 0.06545373797416687, acc: 0.9783783555030823)
[2025-02-13 21:12:46,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:46,834][root][INFO] - Training Epoch: 2/2, step 6582/7134 completed (loss: 0.09813560545444489, acc: 0.9736841917037964)
[2025-02-13 21:12:46,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:47,225][root][INFO] - Training Epoch: 2/2, step 6583/7134 completed (loss: 0.05459606274962425, acc: 0.9899497628211975)
[2025-02-13 21:12:47,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:47,614][root][INFO] - Training Epoch: 2/2, step 6584/7134 completed (loss: 0.06079044193029404, acc: 0.9838709831237793)
[2025-02-13 21:12:47,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:47,981][root][INFO] - Training Epoch: 2/2, step 6585/7134 completed (loss: 0.04992518946528435, acc: 0.9932432174682617)
[2025-02-13 21:12:48,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:48,349][root][INFO] - Training Epoch: 2/2, step 6586/7134 completed (loss: 0.09349656850099564, acc: 0.9693251252174377)
[2025-02-13 21:12:48,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:48,765][root][INFO] - Training Epoch: 2/2, step 6587/7134 completed (loss: 0.1355535238981247, acc: 0.9845361113548279)
[2025-02-13 21:12:48,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:49,174][root][INFO] - Training Epoch: 2/2, step 6588/7134 completed (loss: 0.08027677983045578, acc: 0.9767441749572754)
[2025-02-13 21:12:49,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:49,552][root][INFO] - Training Epoch: 2/2, step 6589/7134 completed (loss: 0.08537819981575012, acc: 0.9813664555549622)
[2025-02-13 21:12:49,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:49,924][root][INFO] - Training Epoch: 2/2, step 6590/7134 completed (loss: 0.07843275368213654, acc: 0.9840425252914429)
[2025-02-13 21:12:50,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:50,329][root][INFO] - Training Epoch: 2/2, step 6591/7134 completed (loss: 0.06935426592826843, acc: 0.9766082167625427)
[2025-02-13 21:12:50,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:50,719][root][INFO] - Training Epoch: 2/2, step 6592/7134 completed (loss: 0.06625977903604507, acc: 0.9852216839790344)
[2025-02-13 21:12:50,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:51,096][root][INFO] - Training Epoch: 2/2, step 6593/7134 completed (loss: 0.08670639991760254, acc: 0.9800000190734863)
[2025-02-13 21:12:51,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:51,469][root][INFO] - Training Epoch: 2/2, step 6594/7134 completed (loss: 0.05173797160387039, acc: 0.9901477694511414)
[2025-02-13 21:12:51,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:51,829][root][INFO] - Training Epoch: 2/2, step 6595/7134 completed (loss: 0.06850778311491013, acc: 0.9938271641731262)
[2025-02-13 21:12:51,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:52,222][root][INFO] - Training Epoch: 2/2, step 6596/7134 completed (loss: 0.08461717516183853, acc: 0.9759036302566528)
[2025-02-13 21:12:52,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:52,622][root][INFO] - Training Epoch: 2/2, step 6597/7134 completed (loss: 0.06348218768835068, acc: 0.9895287752151489)
[2025-02-13 21:12:52,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:53,054][root][INFO] - Training Epoch: 2/2, step 6598/7134 completed (loss: 0.08921515196561813, acc: 0.9890710115432739)
[2025-02-13 21:12:53,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:53,437][root][INFO] - Training Epoch: 2/2, step 6599/7134 completed (loss: 0.07013155519962311, acc: 0.9756097793579102)
[2025-02-13 21:12:53,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:53,792][root][INFO] - Training Epoch: 2/2, step 6600/7134 completed (loss: 0.10103467851877213, acc: 0.9775280952453613)
[2025-02-13 21:12:53,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:54,168][root][INFO] - Training Epoch: 2/2, step 6601/7134 completed (loss: 0.18528231978416443, acc: 0.9460784196853638)
[2025-02-13 21:12:54,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:54,553][root][INFO] - Training Epoch: 2/2, step 6602/7134 completed (loss: 0.08029002696275711, acc: 0.9851484894752502)
[2025-02-13 21:12:54,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:54,933][root][INFO] - Training Epoch: 2/2, step 6603/7134 completed (loss: 0.10583995282649994, acc: 0.9811320900917053)
[2025-02-13 21:12:55,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:55,267][root][INFO] - Training Epoch: 2/2, step 6604/7134 completed (loss: 0.05706625059247017, acc: 0.9892473220825195)
[2025-02-13 21:12:55,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:55,622][root][INFO] - Training Epoch: 2/2, step 6605/7134 completed (loss: 0.14532791078090668, acc: 0.9677419066429138)
[2025-02-13 21:12:55,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:55,992][root][INFO] - Training Epoch: 2/2, step 6606/7134 completed (loss: 0.05551963299512863, acc: 0.9832402467727661)
[2025-02-13 21:12:56,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:56,354][root][INFO] - Training Epoch: 2/2, step 6607/7134 completed (loss: 0.10546423494815826, acc: 0.9612902998924255)
[2025-02-13 21:12:56,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:56,716][root][INFO] - Training Epoch: 2/2, step 6608/7134 completed (loss: 0.04167554900050163, acc: 0.9863013625144958)
[2025-02-13 21:12:56,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:57,073][root][INFO] - Training Epoch: 2/2, step 6609/7134 completed (loss: 0.07107443362474442, acc: 0.9685534834861755)
[2025-02-13 21:12:57,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:57,424][root][INFO] - Training Epoch: 2/2, step 6610/7134 completed (loss: 0.1136699765920639, acc: 0.9696969985961914)
[2025-02-13 21:12:57,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:57,774][root][INFO] - Training Epoch: 2/2, step 6611/7134 completed (loss: 0.0795719102025032, acc: 0.9726027250289917)
[2025-02-13 21:12:57,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:58,182][root][INFO] - Training Epoch: 2/2, step 6612/7134 completed (loss: 0.1745794266462326, acc: 0.9487179517745972)
[2025-02-13 21:12:58,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:58,587][root][INFO] - Training Epoch: 2/2, step 6613/7134 completed (loss: 0.09558337926864624, acc: 0.9731543660163879)
[2025-02-13 21:12:58,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:58,940][root][INFO] - Training Epoch: 2/2, step 6614/7134 completed (loss: 0.08345767110586166, acc: 0.9945054650306702)
[2025-02-13 21:12:59,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:59,285][root][INFO] - Training Epoch: 2/2, step 6615/7134 completed (loss: 0.06410913914442062, acc: 0.9864864945411682)
[2025-02-13 21:12:59,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:59,647][root][INFO] - Training Epoch: 2/2, step 6616/7134 completed (loss: 0.0659705400466919, acc: 0.9878787994384766)
[2025-02-13 21:12:59,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:59,999][root][INFO] - Training Epoch: 2/2, step 6617/7134 completed (loss: 0.13220135867595673, acc: 0.970370352268219)
[2025-02-13 21:13:00,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:00,338][root][INFO] - Training Epoch: 2/2, step 6618/7134 completed (loss: 0.023104125633835793, acc: 1.0)
[2025-02-13 21:13:00,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:00,708][root][INFO] - Training Epoch: 2/2, step 6619/7134 completed (loss: 0.01929098181426525, acc: 1.0)
[2025-02-13 21:13:00,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:01,071][root][INFO] - Training Epoch: 2/2, step 6620/7134 completed (loss: 0.11073150485754013, acc: 0.9652777910232544)
[2025-02-13 21:13:01,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:01,420][root][INFO] - Training Epoch: 2/2, step 6621/7134 completed (loss: 0.12595520913600922, acc: 0.977142870426178)
[2025-02-13 21:13:01,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:01,770][root][INFO] - Training Epoch: 2/2, step 6622/7134 completed (loss: 0.06021937355399132, acc: 0.9871794581413269)
[2025-02-13 21:13:01,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:02,124][root][INFO] - Training Epoch: 2/2, step 6623/7134 completed (loss: 0.05956704542040825, acc: 0.9820359349250793)
[2025-02-13 21:13:02,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:02,480][root][INFO] - Training Epoch: 2/2, step 6624/7134 completed (loss: 0.050621047616004944, acc: 0.993630588054657)
[2025-02-13 21:13:02,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:02,830][root][INFO] - Training Epoch: 2/2, step 6625/7134 completed (loss: 0.03562420979142189, acc: 0.9933775067329407)
[2025-02-13 21:13:02,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:03,197][root][INFO] - Training Epoch: 2/2, step 6626/7134 completed (loss: 0.046907827258110046, acc: 0.977142870426178)
[2025-02-13 21:13:03,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:03,578][root][INFO] - Training Epoch: 2/2, step 6627/7134 completed (loss: 0.04569779708981514, acc: 0.9892473220825195)
[2025-02-13 21:13:03,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:03,946][root][INFO] - Training Epoch: 2/2, step 6628/7134 completed (loss: 0.15858322381973267, acc: 0.9702380895614624)
[2025-02-13 21:13:04,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:04,287][root][INFO] - Training Epoch: 2/2, step 6629/7134 completed (loss: 0.15572892129421234, acc: 0.9624999761581421)
[2025-02-13 21:13:04,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:04,667][root][INFO] - Training Epoch: 2/2, step 6630/7134 completed (loss: 0.05020490288734436, acc: 0.9947916865348816)
[2025-02-13 21:13:04,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:05,021][root][INFO] - Training Epoch: 2/2, step 6631/7134 completed (loss: 0.045420341193675995, acc: 0.9937888383865356)
[2025-02-13 21:13:05,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:05,422][root][INFO] - Training Epoch: 2/2, step 6632/7134 completed (loss: 0.055470481514930725, acc: 0.989847719669342)
[2025-02-13 21:13:05,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:05,776][root][INFO] - Training Epoch: 2/2, step 6633/7134 completed (loss: 0.09303547441959381, acc: 0.9835164546966553)
[2025-02-13 21:13:05,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:06,131][root][INFO] - Training Epoch: 2/2, step 6634/7134 completed (loss: 0.04681158810853958, acc: 0.9849624037742615)
[2025-02-13 21:13:06,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:06,473][root][INFO] - Training Epoch: 2/2, step 6635/7134 completed (loss: 0.22250671684741974, acc: 0.9449999928474426)
[2025-02-13 21:13:06,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:06,829][root][INFO] - Training Epoch: 2/2, step 6636/7134 completed (loss: 0.09519127011299133, acc: 0.9658536314964294)
[2025-02-13 21:13:06,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:07,181][root][INFO] - Training Epoch: 2/2, step 6637/7134 completed (loss: 0.08203160017728806, acc: 0.981249988079071)
[2025-02-13 21:13:07,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:07,517][root][INFO] - Training Epoch: 2/2, step 6638/7134 completed (loss: 0.08628731966018677, acc: 0.9615384340286255)
[2025-02-13 21:13:07,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:07,855][root][INFO] - Training Epoch: 2/2, step 6639/7134 completed (loss: 0.0912221223115921, acc: 0.976047933101654)
[2025-02-13 21:13:07,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:08,218][root][INFO] - Training Epoch: 2/2, step 6640/7134 completed (loss: 0.0793110579252243, acc: 0.987261176109314)
[2025-02-13 21:13:08,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:08,570][root][INFO] - Training Epoch: 2/2, step 6641/7134 completed (loss: 0.19824405014514923, acc: 0.9557521939277649)
[2025-02-13 21:13:08,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:08,943][root][INFO] - Training Epoch: 2/2, step 6642/7134 completed (loss: 0.06831348687410355, acc: 0.9879518151283264)
[2025-02-13 21:13:09,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:09,300][root][INFO] - Training Epoch: 2/2, step 6643/7134 completed (loss: 0.06809309124946594, acc: 0.9838709831237793)
[2025-02-13 21:13:09,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:09,712][root][INFO] - Training Epoch: 2/2, step 6644/7134 completed (loss: 0.08106118440628052, acc: 0.9709543585777283)
[2025-02-13 21:13:09,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:10,105][root][INFO] - Training Epoch: 2/2, step 6645/7134 completed (loss: 0.04092045873403549, acc: 0.9947090148925781)
[2025-02-13 21:13:10,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:10,508][root][INFO] - Training Epoch: 2/2, step 6646/7134 completed (loss: 0.1330767720937729, acc: 0.9650654792785645)
[2025-02-13 21:13:10,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:10,864][root][INFO] - Training Epoch: 2/2, step 6647/7134 completed (loss: 0.039071694016456604, acc: 0.9941176176071167)
[2025-02-13 21:13:11,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:11,234][root][INFO] - Training Epoch: 2/2, step 6648/7134 completed (loss: 0.06714096665382385, acc: 0.9677419066429138)
[2025-02-13 21:13:11,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:11,613][root][INFO] - Training Epoch: 2/2, step 6649/7134 completed (loss: 0.08758348971605301, acc: 0.9710144996643066)
[2025-02-13 21:13:11,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:11,990][root][INFO] - Training Epoch: 2/2, step 6650/7134 completed (loss: 0.06283313781023026, acc: 0.9818181991577148)
[2025-02-13 21:13:12,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:12,363][root][INFO] - Training Epoch: 2/2, step 6651/7134 completed (loss: 0.135781928896904, acc: 0.9715909361839294)
[2025-02-13 21:13:12,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:12,725][root][INFO] - Training Epoch: 2/2, step 6652/7134 completed (loss: 0.22776740789413452, acc: 0.9411764740943909)
[2025-02-13 21:13:12,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:13,091][root][INFO] - Training Epoch: 2/2, step 6653/7134 completed (loss: 0.2125413864850998, acc: 0.9318181872367859)
[2025-02-13 21:13:13,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:13,446][root][INFO] - Training Epoch: 2/2, step 6654/7134 completed (loss: 0.11452177166938782, acc: 0.9620253443717957)
[2025-02-13 21:13:13,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:13,805][root][INFO] - Training Epoch: 2/2, step 6655/7134 completed (loss: 0.0932324007153511, acc: 0.9904761910438538)
[2025-02-13 21:13:13,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:14,151][root][INFO] - Training Epoch: 2/2, step 6656/7134 completed (loss: 0.08612029254436493, acc: 0.9736841917037964)
[2025-02-13 21:13:14,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:14,516][root][INFO] - Training Epoch: 2/2, step 6657/7134 completed (loss: 0.09934796392917633, acc: 0.978723406791687)
[2025-02-13 21:13:14,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:14,867][root][INFO] - Training Epoch: 2/2, step 6658/7134 completed (loss: 0.0648614689707756, acc: 0.9796954393386841)
[2025-02-13 21:13:15,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:15,241][root][INFO] - Training Epoch: 2/2, step 6659/7134 completed (loss: 0.059643056243658066, acc: 0.9781420826911926)
[2025-02-13 21:13:15,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:15,596][root][INFO] - Training Epoch: 2/2, step 6660/7134 completed (loss: 0.0765228196978569, acc: 0.9836065769195557)
[2025-02-13 21:13:15,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:15,958][root][INFO] - Training Epoch: 2/2, step 6661/7134 completed (loss: 0.07581938058137894, acc: 0.9850746393203735)
[2025-02-13 21:13:16,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:16,329][root][INFO] - Training Epoch: 2/2, step 6662/7134 completed (loss: 0.19874699413776398, acc: 0.9568345546722412)
[2025-02-13 21:13:16,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:16,696][root][INFO] - Training Epoch: 2/2, step 6663/7134 completed (loss: 0.09780466556549072, acc: 0.9696969985961914)
[2025-02-13 21:13:16,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:17,046][root][INFO] - Training Epoch: 2/2, step 6664/7134 completed (loss: 0.10972677916288376, acc: 0.9928571581840515)
[2025-02-13 21:13:17,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:17,391][root][INFO] - Training Epoch: 2/2, step 6665/7134 completed (loss: 0.06772252172231674, acc: 0.9937106966972351)
[2025-02-13 21:13:17,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:17,764][root][INFO] - Training Epoch: 2/2, step 6666/7134 completed (loss: 0.05632933974266052, acc: 0.9939393997192383)
[2025-02-13 21:13:17,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:18,132][root][INFO] - Training Epoch: 2/2, step 6667/7134 completed (loss: 0.1021391823887825, acc: 0.9795918464660645)
[2025-02-13 21:13:18,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:18,487][root][INFO] - Training Epoch: 2/2, step 6668/7134 completed (loss: 0.01423747930675745, acc: 1.0)
[2025-02-13 21:13:18,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:18,838][root][INFO] - Training Epoch: 2/2, step 6669/7134 completed (loss: 0.043157629668712616, acc: 0.9861111044883728)
[2025-02-13 21:13:18,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:19,178][root][INFO] - Training Epoch: 2/2, step 6670/7134 completed (loss: 0.03329622372984886, acc: 0.9928057789802551)
[2025-02-13 21:13:19,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:19,552][root][INFO] - Training Epoch: 2/2, step 6671/7134 completed (loss: 0.023394912481307983, acc: 0.9939024448394775)
[2025-02-13 21:13:19,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:19,922][root][INFO] - Training Epoch: 2/2, step 6672/7134 completed (loss: 0.02322804555296898, acc: 1.0)
[2025-02-13 21:13:20,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:20,295][root][INFO] - Training Epoch: 2/2, step 6673/7134 completed (loss: 0.03964252769947052, acc: 0.9874213933944702)
[2025-02-13 21:13:20,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:20,655][root][INFO] - Training Epoch: 2/2, step 6674/7134 completed (loss: 0.019707556813955307, acc: 0.991525411605835)
[2025-02-13 21:13:20,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:21,046][root][INFO] - Training Epoch: 2/2, step 6675/7134 completed (loss: 0.04233140870928764, acc: 0.9901960492134094)
[2025-02-13 21:13:21,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:21,421][root][INFO] - Training Epoch: 2/2, step 6676/7134 completed (loss: 0.03208877891302109, acc: 0.9900000095367432)
[2025-02-13 21:13:21,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:21,789][root][INFO] - Training Epoch: 2/2, step 6677/7134 completed (loss: 0.04873356595635414, acc: 0.9923664331436157)
[2025-02-13 21:13:21,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:22,152][root][INFO] - Training Epoch: 2/2, step 6678/7134 completed (loss: 0.032249823212623596, acc: 0.9925925731658936)
[2025-02-13 21:13:22,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:22,515][root][INFO] - Training Epoch: 2/2, step 6679/7134 completed (loss: 0.013288171961903572, acc: 1.0)
[2025-02-13 21:13:22,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:22,872][root][INFO] - Training Epoch: 2/2, step 6680/7134 completed (loss: 0.05109420418739319, acc: 0.9885057210922241)
[2025-02-13 21:13:23,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:23,235][root][INFO] - Training Epoch: 2/2, step 6681/7134 completed (loss: 0.0032470067963004112, acc: 1.0)
[2025-02-13 21:13:23,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:23,603][root][INFO] - Training Epoch: 2/2, step 6682/7134 completed (loss: 0.1253894567489624, acc: 0.9611650705337524)
[2025-02-13 21:13:23,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:23,999][root][INFO] - Training Epoch: 2/2, step 6683/7134 completed (loss: 0.10600385069847107, acc: 0.9655172228813171)
[2025-02-13 21:13:24,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:24,363][root][INFO] - Training Epoch: 2/2, step 6684/7134 completed (loss: 0.07344397902488708, acc: 0.9763779640197754)
[2025-02-13 21:13:24,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:24,715][root][INFO] - Training Epoch: 2/2, step 6685/7134 completed (loss: 0.04518142715096474, acc: 0.9919999837875366)
[2025-02-13 21:13:24,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:25,077][root][INFO] - Training Epoch: 2/2, step 6686/7134 completed (loss: 0.12687113881111145, acc: 0.982758641242981)
[2025-02-13 21:13:25,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:25,432][root][INFO] - Training Epoch: 2/2, step 6687/7134 completed (loss: 0.07584462314844131, acc: 0.9883720874786377)
[2025-02-13 21:13:25,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:25,814][root][INFO] - Training Epoch: 2/2, step 6688/7134 completed (loss: 0.10188114643096924, acc: 0.9784172773361206)
[2025-02-13 21:13:25,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:26,189][root][INFO] - Training Epoch: 2/2, step 6689/7134 completed (loss: 0.03581208363175392, acc: 1.0)
[2025-02-13 21:13:26,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:26,547][root][INFO] - Training Epoch: 2/2, step 6690/7134 completed (loss: 0.10307810455560684, acc: 0.9607843160629272)
[2025-02-13 21:13:26,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:26,903][root][INFO] - Training Epoch: 2/2, step 6691/7134 completed (loss: 0.04107867181301117, acc: 0.9930555820465088)
[2025-02-13 21:13:27,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:27,242][root][INFO] - Training Epoch: 2/2, step 6692/7134 completed (loss: 0.17514236271381378, acc: 0.9626168012619019)
[2025-02-13 21:13:27,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:27,597][root][INFO] - Training Epoch: 2/2, step 6693/7134 completed (loss: 0.0945998802781105, acc: 0.9851852059364319)
[2025-02-13 21:13:27,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:27,968][root][INFO] - Training Epoch: 2/2, step 6694/7134 completed (loss: 0.03564375266432762, acc: 0.9933775067329407)
[2025-02-13 21:13:28,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:28,326][root][INFO] - Training Epoch: 2/2, step 6695/7134 completed (loss: 0.05571315437555313, acc: 0.9772727489471436)
[2025-02-13 21:13:28,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:28,687][root][INFO] - Training Epoch: 2/2, step 6696/7134 completed (loss: 0.11736015230417252, acc: 0.9722222089767456)
[2025-02-13 21:13:28,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:29,040][root][INFO] - Training Epoch: 2/2, step 6697/7134 completed (loss: 0.13699349761009216, acc: 0.9642857313156128)
[2025-02-13 21:13:29,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:29,401][root][INFO] - Training Epoch: 2/2, step 6698/7134 completed (loss: 0.05039680376648903, acc: 0.9932885766029358)
[2025-02-13 21:13:29,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:29,787][root][INFO] - Training Epoch: 2/2, step 6699/7134 completed (loss: 0.07556262612342834, acc: 0.9809523820877075)
[2025-02-13 21:13:29,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:30,172][root][INFO] - Training Epoch: 2/2, step 6700/7134 completed (loss: 0.14289136230945587, acc: 0.970370352268219)
[2025-02-13 21:13:30,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:30,535][root][INFO] - Training Epoch: 2/2, step 6701/7134 completed (loss: 0.2469804733991623, acc: 0.9402984976768494)
[2025-02-13 21:13:30,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:30,893][root][INFO] - Training Epoch: 2/2, step 6702/7134 completed (loss: 0.08845782279968262, acc: 0.9739130139350891)
[2025-02-13 21:13:31,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:31,242][root][INFO] - Training Epoch: 2/2, step 6703/7134 completed (loss: 0.11640767008066177, acc: 0.9710144996643066)
[2025-02-13 21:13:31,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:31,611][root][INFO] - Training Epoch: 2/2, step 6704/7134 completed (loss: 0.18783354759216309, acc: 0.9482758641242981)
[2025-02-13 21:13:31,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:31,991][root][INFO] - Training Epoch: 2/2, step 6705/7134 completed (loss: 0.04811448976397514, acc: 0.9884393215179443)
[2025-02-13 21:13:32,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:32,365][root][INFO] - Training Epoch: 2/2, step 6706/7134 completed (loss: 0.023734189569950104, acc: 1.0)
[2025-02-13 21:13:32,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:32,773][root][INFO] - Training Epoch: 2/2, step 6707/7134 completed (loss: 0.1085112988948822, acc: 0.9658119678497314)
[2025-02-13 21:13:32,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:33,140][root][INFO] - Training Epoch: 2/2, step 6708/7134 completed (loss: 0.06154326722025871, acc: 0.9770992398262024)
[2025-02-13 21:13:33,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:33,505][root][INFO] - Training Epoch: 2/2, step 6709/7134 completed (loss: 0.14884215593338013, acc: 0.9878048896789551)
[2025-02-13 21:13:33,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:33,869][root][INFO] - Training Epoch: 2/2, step 6710/7134 completed (loss: 0.09878379106521606, acc: 0.9624060392379761)
[2025-02-13 21:13:34,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:34,241][root][INFO] - Training Epoch: 2/2, step 6711/7134 completed (loss: 0.045527372509241104, acc: 0.9936708807945251)
[2025-02-13 21:13:34,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:34,594][root][INFO] - Training Epoch: 2/2, step 6712/7134 completed (loss: 0.09083351492881775, acc: 0.9793103337287903)
[2025-02-13 21:13:34,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:35,025][root][INFO] - Training Epoch: 2/2, step 6713/7134 completed (loss: 0.13005997240543365, acc: 0.9693251252174377)
[2025-02-13 21:13:35,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:35,397][root][INFO] - Training Epoch: 2/2, step 6714/7134 completed (loss: 0.13693347573280334, acc: 0.9595375657081604)
[2025-02-13 21:13:35,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:35,760][root][INFO] - Training Epoch: 2/2, step 6715/7134 completed (loss: 0.18643797934055328, acc: 0.9594594836235046)
[2025-02-13 21:13:35,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:36,122][root][INFO] - Training Epoch: 2/2, step 6716/7134 completed (loss: 0.06436822563409805, acc: 0.9794520735740662)
[2025-02-13 21:13:36,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:36,469][root][INFO] - Training Epoch: 2/2, step 6717/7134 completed (loss: 0.05875563621520996, acc: 0.9864864945411682)
[2025-02-13 21:13:36,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:36,818][root][INFO] - Training Epoch: 2/2, step 6718/7134 completed (loss: 0.08056411892175674, acc: 0.9862068891525269)
[2025-02-13 21:13:36,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:37,165][root][INFO] - Training Epoch: 2/2, step 6719/7134 completed (loss: 0.04720992594957352, acc: 0.9932885766029358)
[2025-02-13 21:13:37,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:37,521][root][INFO] - Training Epoch: 2/2, step 6720/7134 completed (loss: 0.17086876928806305, acc: 0.959770143032074)
[2025-02-13 21:13:37,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:37,863][root][INFO] - Training Epoch: 2/2, step 6721/7134 completed (loss: 0.15477578341960907, acc: 0.9617834687232971)
[2025-02-13 21:13:37,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:38,193][root][INFO] - Training Epoch: 2/2, step 6722/7134 completed (loss: 0.18426570296287537, acc: 0.9510489702224731)
[2025-02-13 21:13:38,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:38,555][root][INFO] - Training Epoch: 2/2, step 6723/7134 completed (loss: 0.09981732815504074, acc: 0.9781420826911926)
[2025-02-13 21:13:38,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:38,907][root][INFO] - Training Epoch: 2/2, step 6724/7134 completed (loss: 0.1205035001039505, acc: 0.9685534834861755)
[2025-02-13 21:13:39,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:39,240][root][INFO] - Training Epoch: 2/2, step 6725/7134 completed (loss: 0.08624278008937836, acc: 0.9736841917037964)
[2025-02-13 21:13:39,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:39,572][root][INFO] - Training Epoch: 2/2, step 6726/7134 completed (loss: 0.19946953654289246, acc: 0.9669421315193176)
[2025-02-13 21:13:39,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:39,901][root][INFO] - Training Epoch: 2/2, step 6727/7134 completed (loss: 0.07774405181407928, acc: 0.9784172773361206)
[2025-02-13 21:13:40,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:40,253][root][INFO] - Training Epoch: 2/2, step 6728/7134 completed (loss: 0.16820712387561798, acc: 0.9548386931419373)
[2025-02-13 21:13:40,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:40,613][root][INFO] - Training Epoch: 2/2, step 6729/7134 completed (loss: 0.04917611554265022, acc: 0.9936708807945251)
[2025-02-13 21:13:40,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:40,986][root][INFO] - Training Epoch: 2/2, step 6730/7134 completed (loss: 0.16442649066448212, acc: 0.954023003578186)
[2025-02-13 21:13:41,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:41,363][root][INFO] - Training Epoch: 2/2, step 6731/7134 completed (loss: 0.06914057582616806, acc: 0.9759036302566528)
[2025-02-13 21:13:41,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:41,725][root][INFO] - Training Epoch: 2/2, step 6732/7134 completed (loss: 0.06505333632230759, acc: 0.9871794581413269)
[2025-02-13 21:13:41,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:42,071][root][INFO] - Training Epoch: 2/2, step 6733/7134 completed (loss: 0.10265649855136871, acc: 0.9856114983558655)
[2025-02-13 21:13:42,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:42,406][root][INFO] - Training Epoch: 2/2, step 6734/7134 completed (loss: 0.1573888510465622, acc: 0.9650349617004395)
[2025-02-13 21:13:42,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:42,766][root][INFO] - Training Epoch: 2/2, step 6735/7134 completed (loss: 0.10462694615125656, acc: 0.9807692170143127)
[2025-02-13 21:13:42,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:43,147][root][INFO] - Training Epoch: 2/2, step 6736/7134 completed (loss: 0.12746940553188324, acc: 0.9555555582046509)
[2025-02-13 21:13:43,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:43,522][root][INFO] - Training Epoch: 2/2, step 6737/7134 completed (loss: 0.1494528204202652, acc: 0.9470198750495911)
[2025-02-13 21:13:43,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:43,885][root][INFO] - Training Epoch: 2/2, step 6738/7134 completed (loss: 0.09372852742671967, acc: 0.9632353186607361)
[2025-02-13 21:13:43,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:44,207][root][INFO] - Training Epoch: 2/2, step 6739/7134 completed (loss: 0.09861721843481064, acc: 0.9785714149475098)
[2025-02-13 21:13:44,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:44,571][root][INFO] - Training Epoch: 2/2, step 6740/7134 completed (loss: 0.10864529758691788, acc: 0.9729729890823364)
[2025-02-13 21:13:44,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:44,917][root][INFO] - Training Epoch: 2/2, step 6741/7134 completed (loss: 0.05084019526839256, acc: 0.9819276928901672)
[2025-02-13 21:13:45,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:45,251][root][INFO] - Training Epoch: 2/2, step 6742/7134 completed (loss: 0.027923693880438805, acc: 0.9941176176071167)
[2025-02-13 21:13:45,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:45,629][root][INFO] - Training Epoch: 2/2, step 6743/7134 completed (loss: 0.03193679079413414, acc: 0.994350254535675)
[2025-02-13 21:13:45,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:45,983][root][INFO] - Training Epoch: 2/2, step 6744/7134 completed (loss: 0.027286821976304054, acc: 0.9932432174682617)
[2025-02-13 21:13:46,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:46,337][root][INFO] - Training Epoch: 2/2, step 6745/7134 completed (loss: 0.01755766198039055, acc: 1.0)
[2025-02-13 21:13:46,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:46,679][root][INFO] - Training Epoch: 2/2, step 6746/7134 completed (loss: 0.016394035890698433, acc: 1.0)
[2025-02-13 21:13:46,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:47,030][root][INFO] - Training Epoch: 2/2, step 6747/7134 completed (loss: 0.06642402708530426, acc: 0.9738219976425171)
[2025-02-13 21:13:47,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:47,406][root][INFO] - Training Epoch: 2/2, step 6748/7134 completed (loss: 0.06070047616958618, acc: 0.9901960492134094)
[2025-02-13 21:13:47,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:47,799][root][INFO] - Training Epoch: 2/2, step 6749/7134 completed (loss: 0.10578799992799759, acc: 0.9733333587646484)
[2025-02-13 21:13:47,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:48,167][root][INFO] - Training Epoch: 2/2, step 6750/7134 completed (loss: 0.07362905144691467, acc: 0.9851484894752502)
[2025-02-13 21:13:48,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:48,552][root][INFO] - Training Epoch: 2/2, step 6751/7134 completed (loss: 0.03365489840507507, acc: 0.9900497794151306)
[2025-02-13 21:13:48,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:48,921][root][INFO] - Training Epoch: 2/2, step 6752/7134 completed (loss: 0.03658413887023926, acc: 0.987730085849762)
[2025-02-13 21:13:49,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:49,285][root][INFO] - Training Epoch: 2/2, step 6753/7134 completed (loss: 0.021506767719984055, acc: 0.9898989796638489)
[2025-02-13 21:13:49,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:49,630][root][INFO] - Training Epoch: 2/2, step 6754/7134 completed (loss: 0.048995550721883774, acc: 0.9947643876075745)
[2025-02-13 21:13:49,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:49,996][root][INFO] - Training Epoch: 2/2, step 6755/7134 completed (loss: 0.03643243759870529, acc: 0.9947643876075745)
[2025-02-13 21:13:50,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:50,353][root][INFO] - Training Epoch: 2/2, step 6756/7134 completed (loss: 0.01970808207988739, acc: 0.9943820238113403)
[2025-02-13 21:13:50,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:50,702][root][INFO] - Training Epoch: 2/2, step 6757/7134 completed (loss: 0.020458528771996498, acc: 1.0)
[2025-02-13 21:13:50,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:51,094][root][INFO] - Training Epoch: 2/2, step 6758/7134 completed (loss: 0.024070192128419876, acc: 1.0)
[2025-02-13 21:13:51,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:51,464][root][INFO] - Training Epoch: 2/2, step 6759/7134 completed (loss: 0.04352870583534241, acc: 0.9866071343421936)
[2025-02-13 21:13:51,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:51,823][root][INFO] - Training Epoch: 2/2, step 6760/7134 completed (loss: 0.06252481788396835, acc: 0.9893617033958435)
[2025-02-13 21:13:51,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:52,185][root][INFO] - Training Epoch: 2/2, step 6761/7134 completed (loss: 0.05348564311861992, acc: 0.9857142567634583)
[2025-02-13 21:13:52,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:52,558][root][INFO] - Training Epoch: 2/2, step 6762/7134 completed (loss: 0.04155120998620987, acc: 0.9950000047683716)
[2025-02-13 21:13:52,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:52,929][root][INFO] - Training Epoch: 2/2, step 6763/7134 completed (loss: 0.02120349556207657, acc: 0.9946523904800415)
[2025-02-13 21:13:53,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:53,284][root][INFO] - Training Epoch: 2/2, step 6764/7134 completed (loss: 0.024946557357907295, acc: 0.9941176176071167)
[2025-02-13 21:13:53,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:53,656][root][INFO] - Training Epoch: 2/2, step 6765/7134 completed (loss: 0.05731029808521271, acc: 0.9835164546966553)
[2025-02-13 21:13:53,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:54,004][root][INFO] - Training Epoch: 2/2, step 6766/7134 completed (loss: 0.054000310599803925, acc: 0.9885714054107666)
[2025-02-13 21:13:54,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:54,326][root][INFO] - Training Epoch: 2/2, step 6767/7134 completed (loss: 0.05655766278505325, acc: 0.9821428656578064)
[2025-02-13 21:13:54,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:54,691][root][INFO] - Training Epoch: 2/2, step 6768/7134 completed (loss: 0.012598276138305664, acc: 1.0)
[2025-02-13 21:13:54,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:55,044][root][INFO] - Training Epoch: 2/2, step 6769/7134 completed (loss: 0.06950530409812927, acc: 0.9807692170143127)
[2025-02-13 21:13:55,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:55,427][root][INFO] - Training Epoch: 2/2, step 6770/7134 completed (loss: 0.033444248139858246, acc: 0.9947368502616882)
[2025-02-13 21:13:55,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:55,825][root][INFO] - Training Epoch: 2/2, step 6771/7134 completed (loss: 0.016027381643652916, acc: 1.0)
[2025-02-13 21:13:55,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:56,210][root][INFO] - Training Epoch: 2/2, step 6772/7134 completed (loss: 0.16888542473316193, acc: 0.9650349617004395)
[2025-02-13 21:13:56,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:56,575][root][INFO] - Training Epoch: 2/2, step 6773/7134 completed (loss: 0.07268986850976944, acc: 0.9876543283462524)
[2025-02-13 21:13:56,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:56,949][root][INFO] - Training Epoch: 2/2, step 6774/7134 completed (loss: 0.0730326697230339, acc: 0.976331353187561)
[2025-02-13 21:13:57,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:57,300][root][INFO] - Training Epoch: 2/2, step 6775/7134 completed (loss: 0.027229415252804756, acc: 1.0)
[2025-02-13 21:13:57,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:57,664][root][INFO] - Training Epoch: 2/2, step 6776/7134 completed (loss: 0.06424835324287415, acc: 0.984000027179718)
[2025-02-13 21:13:57,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:58,065][root][INFO] - Training Epoch: 2/2, step 6777/7134 completed (loss: 0.14073456823825836, acc: 0.9885714054107666)
[2025-02-13 21:13:58,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:58,439][root][INFO] - Training Epoch: 2/2, step 6778/7134 completed (loss: 0.026883484795689583, acc: 0.9943181872367859)
[2025-02-13 21:13:58,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:58,801][root][INFO] - Training Epoch: 2/2, step 6779/7134 completed (loss: 0.03627064451575279, acc: 0.988095223903656)
[2025-02-13 21:13:58,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:59,175][root][INFO] - Training Epoch: 2/2, step 6780/7134 completed (loss: 0.041470564901828766, acc: 0.9940119981765747)
[2025-02-13 21:13:59,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:59,548][root][INFO] - Training Epoch: 2/2, step 6781/7134 completed (loss: 0.015310118906199932, acc: 1.0)
[2025-02-13 21:13:59,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:59,918][root][INFO] - Training Epoch: 2/2, step 6782/7134 completed (loss: 0.01069041807204485, acc: 1.0)
[2025-02-13 21:14:00,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:00,262][root][INFO] - Training Epoch: 2/2, step 6783/7134 completed (loss: 0.05482299625873566, acc: 0.9724137783050537)
[2025-02-13 21:14:00,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:00,617][root][INFO] - Training Epoch: 2/2, step 6784/7134 completed (loss: 0.017307166010141373, acc: 0.9936708807945251)
[2025-02-13 21:14:00,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:00,980][root][INFO] - Training Epoch: 2/2, step 6785/7134 completed (loss: 0.045132119208574295, acc: 0.9803921580314636)
[2025-02-13 21:14:01,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:01,353][root][INFO] - Training Epoch: 2/2, step 6786/7134 completed (loss: 0.06231197714805603, acc: 0.9774011373519897)
[2025-02-13 21:14:01,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:01,717][root][INFO] - Training Epoch: 2/2, step 6787/7134 completed (loss: 0.02241678722202778, acc: 0.994535505771637)
[2025-02-13 21:14:01,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:02,082][root][INFO] - Training Epoch: 2/2, step 6788/7134 completed (loss: 0.02839111164212227, acc: 0.9928057789802551)
[2025-02-13 21:14:02,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:02,429][root][INFO] - Training Epoch: 2/2, step 6789/7134 completed (loss: 0.028692055493593216, acc: 0.9858155846595764)
[2025-02-13 21:14:02,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:02,802][root][INFO] - Training Epoch: 2/2, step 6790/7134 completed (loss: 0.11685796082019806, acc: 0.9715909361839294)
[2025-02-13 21:14:02,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:03,168][root][INFO] - Training Epoch: 2/2, step 6791/7134 completed (loss: 0.03265582025051117, acc: 0.9935897588729858)
[2025-02-13 21:14:03,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:03,529][root][INFO] - Training Epoch: 2/2, step 6792/7134 completed (loss: 0.014242070727050304, acc: 0.9925373196601868)
[2025-02-13 21:14:03,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:03,895][root][INFO] - Training Epoch: 2/2, step 6793/7134 completed (loss: 0.0186551995575428, acc: 1.0)
[2025-02-13 21:14:04,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:04,257][root][INFO] - Training Epoch: 2/2, step 6794/7134 completed (loss: 0.059038031846284866, acc: 0.9931972622871399)
[2025-02-13 21:14:04,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:04,617][root][INFO] - Training Epoch: 2/2, step 6795/7134 completed (loss: 0.028735367581248283, acc: 0.9942857027053833)
[2025-02-13 21:14:04,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:04,980][root][INFO] - Training Epoch: 2/2, step 6796/7134 completed (loss: 0.01675587147474289, acc: 0.9941860437393188)
[2025-02-13 21:14:05,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:05,346][root][INFO] - Training Epoch: 2/2, step 6797/7134 completed (loss: 0.06371785700321198, acc: 0.9878048896789551)
[2025-02-13 21:14:05,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:05,706][root][INFO] - Training Epoch: 2/2, step 6798/7134 completed (loss: 0.027114752680063248, acc: 0.9879518151283264)
[2025-02-13 21:14:05,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:06,063][root][INFO] - Training Epoch: 2/2, step 6799/7134 completed (loss: 0.03342224285006523, acc: 0.9880239367485046)
[2025-02-13 21:14:06,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:06,406][root][INFO] - Training Epoch: 2/2, step 6800/7134 completed (loss: 0.0443134680390358, acc: 0.9873417615890503)
[2025-02-13 21:14:06,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:06,779][root][INFO] - Training Epoch: 2/2, step 6801/7134 completed (loss: 0.037255048751831055, acc: 0.9928057789802551)
[2025-02-13 21:14:06,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:07,147][root][INFO] - Training Epoch: 2/2, step 6802/7134 completed (loss: 0.03193928301334381, acc: 0.9930070042610168)
[2025-02-13 21:14:07,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:07,522][root][INFO] - Training Epoch: 2/2, step 6803/7134 completed (loss: 0.03586756810545921, acc: 0.9833333492279053)
[2025-02-13 21:14:07,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:07,934][root][INFO] - Training Epoch: 2/2, step 6804/7134 completed (loss: 0.09136738628149033, acc: 0.9798657894134521)
[2025-02-13 21:14:08,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:08,305][root][INFO] - Training Epoch: 2/2, step 6805/7134 completed (loss: 0.034165918827056885, acc: 0.9851852059364319)
[2025-02-13 21:14:08,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:08,656][root][INFO] - Training Epoch: 2/2, step 6806/7134 completed (loss: 0.033759068697690964, acc: 0.9943820238113403)
[2025-02-13 21:14:08,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:09,027][root][INFO] - Training Epoch: 2/2, step 6807/7134 completed (loss: 0.03759419173002243, acc: 0.9937499761581421)
[2025-02-13 21:14:09,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:09,389][root][INFO] - Training Epoch: 2/2, step 6808/7134 completed (loss: 0.049196675419807434, acc: 0.9865771532058716)
[2025-02-13 21:14:09,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:09,767][root][INFO] - Training Epoch: 2/2, step 6809/7134 completed (loss: 0.06866883486509323, acc: 0.9871794581413269)
[2025-02-13 21:14:09,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:10,141][root][INFO] - Training Epoch: 2/2, step 6810/7134 completed (loss: 0.025684667751193047, acc: 0.9886363744735718)
[2025-02-13 21:14:10,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:10,515][root][INFO] - Training Epoch: 2/2, step 6811/7134 completed (loss: 0.03513441979885101, acc: 1.0)
[2025-02-13 21:14:10,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:10,881][root][INFO] - Training Epoch: 2/2, step 6812/7134 completed (loss: 0.02185993455350399, acc: 1.0)
[2025-02-13 21:14:11,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:11,247][root][INFO] - Training Epoch: 2/2, step 6813/7134 completed (loss: 0.04020851105451584, acc: 0.987500011920929)
[2025-02-13 21:14:11,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:11,616][root][INFO] - Training Epoch: 2/2, step 6814/7134 completed (loss: 0.027249449864029884, acc: 0.9942857027053833)
[2025-02-13 21:14:11,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:11,990][root][INFO] - Training Epoch: 2/2, step 6815/7134 completed (loss: 0.04880053550004959, acc: 0.9882352948188782)
[2025-02-13 21:14:12,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:12,354][root][INFO] - Training Epoch: 2/2, step 6816/7134 completed (loss: 0.038726530969142914, acc: 0.9861111044883728)
[2025-02-13 21:14:12,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:12,724][root][INFO] - Training Epoch: 2/2, step 6817/7134 completed (loss: 0.13129962980747223, acc: 0.9718309640884399)
[2025-02-13 21:14:12,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:13,093][root][INFO] - Training Epoch: 2/2, step 6818/7134 completed (loss: 0.028877153992652893, acc: 0.9931972622871399)
[2025-02-13 21:14:13,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:13,467][root][INFO] - Training Epoch: 2/2, step 6819/7134 completed (loss: 0.04113652929663658, acc: 0.985401451587677)
[2025-02-13 21:14:13,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:13,829][root][INFO] - Training Epoch: 2/2, step 6820/7134 completed (loss: 0.04971977695822716, acc: 0.985401451587677)
[2025-02-13 21:14:13,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:14,171][root][INFO] - Training Epoch: 2/2, step 6821/7134 completed (loss: 0.013531087897717953, acc: 1.0)
[2025-02-13 21:14:14,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:14,560][root][INFO] - Training Epoch: 2/2, step 6822/7134 completed (loss: 0.1535991132259369, acc: 0.9534883499145508)
[2025-02-13 21:14:14,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:14,926][root][INFO] - Training Epoch: 2/2, step 6823/7134 completed (loss: 0.05130666866898537, acc: 0.9879518151283264)
[2025-02-13 21:14:15,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:15,273][root][INFO] - Training Epoch: 2/2, step 6824/7134 completed (loss: 0.09216893464326859, acc: 0.9918699264526367)
[2025-02-13 21:14:15,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:15,619][root][INFO] - Training Epoch: 2/2, step 6825/7134 completed (loss: 0.040881115943193436, acc: 0.9892473220825195)
[2025-02-13 21:14:15,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:15,932][root][INFO] - Training Epoch: 2/2, step 6826/7134 completed (loss: 0.10030900686979294, acc: 0.9492753744125366)
[2025-02-13 21:14:16,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:16,293][root][INFO] - Training Epoch: 2/2, step 6827/7134 completed (loss: 0.17441876232624054, acc: 0.9635416865348816)
[2025-02-13 21:14:16,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:16,661][root][INFO] - Training Epoch: 2/2, step 6828/7134 completed (loss: 0.12176681309938431, acc: 0.9788732528686523)
[2025-02-13 21:14:16,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:17,026][root][INFO] - Training Epoch: 2/2, step 6829/7134 completed (loss: 0.10494493693113327, acc: 0.9704142212867737)
[2025-02-13 21:14:17,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:17,374][root][INFO] - Training Epoch: 2/2, step 6830/7134 completed (loss: 0.08889301866292953, acc: 0.9797297120094299)
[2025-02-13 21:14:17,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:17,740][root][INFO] - Training Epoch: 2/2, step 6831/7134 completed (loss: 0.06992408633232117, acc: 0.9720670580863953)
[2025-02-13 21:14:17,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:18,100][root][INFO] - Training Epoch: 2/2, step 6832/7134 completed (loss: 0.09351459890604019, acc: 0.9791666865348816)
[2025-02-13 21:14:18,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:18,451][root][INFO] - Training Epoch: 2/2, step 6833/7134 completed (loss: 0.07451508194208145, acc: 0.9808917045593262)
[2025-02-13 21:14:18,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:18,797][root][INFO] - Training Epoch: 2/2, step 6834/7134 completed (loss: 0.04998765513300896, acc: 0.9865771532058716)
[2025-02-13 21:14:18,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:19,153][root][INFO] - Training Epoch: 2/2, step 6835/7134 completed (loss: 0.03831614926457405, acc: 0.9869281053543091)
[2025-02-13 21:14:19,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:19,497][root][INFO] - Training Epoch: 2/2, step 6836/7134 completed (loss: 0.07854793220758438, acc: 0.9810426831245422)
[2025-02-13 21:14:19,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:19,867][root][INFO] - Training Epoch: 2/2, step 6837/7134 completed (loss: 0.08458197861909866, acc: 0.9848484992980957)
[2025-02-13 21:14:19,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:20,217][root][INFO] - Training Epoch: 2/2, step 6838/7134 completed (loss: 0.19456084072589874, acc: 0.9638554453849792)
[2025-02-13 21:14:20,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:20,566][root][INFO] - Training Epoch: 2/2, step 6839/7134 completed (loss: 0.09939078986644745, acc: 0.9599999785423279)
[2025-02-13 21:14:20,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:20,931][root][INFO] - Training Epoch: 2/2, step 6840/7134 completed (loss: 0.09112977236509323, acc: 0.9745222926139832)
[2025-02-13 21:14:21,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:21,277][root][INFO] - Training Epoch: 2/2, step 6841/7134 completed (loss: 0.030150242149829865, acc: 1.0)
[2025-02-13 21:14:21,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:21,643][root][INFO] - Training Epoch: 2/2, step 6842/7134 completed (loss: 0.07532370090484619, acc: 0.9780219793319702)
[2025-02-13 21:14:21,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:22,018][root][INFO] - Training Epoch: 2/2, step 6843/7134 completed (loss: 0.05676962807774544, acc: 0.9896373152732849)
[2025-02-13 21:14:22,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:22,376][root][INFO] - Training Epoch: 2/2, step 6844/7134 completed (loss: 0.0758080706000328, acc: 0.9798657894134521)
[2025-02-13 21:14:22,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:22,747][root][INFO] - Training Epoch: 2/2, step 6845/7134 completed (loss: 0.053061842918395996, acc: 0.9846153855323792)
[2025-02-13 21:14:22,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:23,106][root][INFO] - Training Epoch: 2/2, step 6846/7134 completed (loss: 0.06590370088815689, acc: 0.9753086566925049)
[2025-02-13 21:14:23,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:23,468][root][INFO] - Training Epoch: 2/2, step 6847/7134 completed (loss: 0.08040504157543182, acc: 0.9750000238418579)
[2025-02-13 21:14:23,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:23,810][root][INFO] - Training Epoch: 2/2, step 6848/7134 completed (loss: 0.031084392219781876, acc: 0.9887640476226807)
[2025-02-13 21:14:23,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:24,186][root][INFO] - Training Epoch: 2/2, step 6849/7134 completed (loss: 0.05544824153184891, acc: 0.9857142567634583)
[2025-02-13 21:14:24,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:24,550][root][INFO] - Training Epoch: 2/2, step 6850/7134 completed (loss: 0.05080915614962578, acc: 0.9904761910438538)
[2025-02-13 21:14:24,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:24,922][root][INFO] - Training Epoch: 2/2, step 6851/7134 completed (loss: 0.08195553719997406, acc: 0.9723756909370422)
[2025-02-13 21:14:25,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:25,291][root][INFO] - Training Epoch: 2/2, step 6852/7134 completed (loss: 0.09198691695928574, acc: 0.9714285731315613)
[2025-02-13 21:14:25,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:25,682][root][INFO] - Training Epoch: 2/2, step 6853/7134 completed (loss: 0.1336475908756256, acc: 0.9710982441902161)
[2025-02-13 21:14:25,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:26,027][root][INFO] - Training Epoch: 2/2, step 6854/7134 completed (loss: 0.015378124080598354, acc: 1.0)
[2025-02-13 21:14:26,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:26,383][root][INFO] - Training Epoch: 2/2, step 6855/7134 completed (loss: 0.02651241049170494, acc: 1.0)
[2025-02-13 21:14:26,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:26,745][root][INFO] - Training Epoch: 2/2, step 6856/7134 completed (loss: 0.02086505852639675, acc: 0.9939758777618408)
[2025-02-13 21:14:26,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:27,128][root][INFO] - Training Epoch: 2/2, step 6857/7134 completed (loss: 0.01593278907239437, acc: 0.9940476417541504)
[2025-02-13 21:14:27,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:27,462][root][INFO] - Training Epoch: 2/2, step 6858/7134 completed (loss: 0.010832560248672962, acc: 1.0)
[2025-02-13 21:14:27,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:27,829][root][INFO] - Training Epoch: 2/2, step 6859/7134 completed (loss: 0.02783789299428463, acc: 0.9887640476226807)
[2025-02-13 21:14:27,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:28,207][root][INFO] - Training Epoch: 2/2, step 6860/7134 completed (loss: 0.02424270287156105, acc: 1.0)
[2025-02-13 21:14:28,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:28,587][root][INFO] - Training Epoch: 2/2, step 6861/7134 completed (loss: 0.014580857940018177, acc: 1.0)
[2025-02-13 21:14:28,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:28,972][root][INFO] - Training Epoch: 2/2, step 6862/7134 completed (loss: 0.0888812467455864, acc: 0.9848484992980957)
[2025-02-13 21:14:29,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:29,316][root][INFO] - Training Epoch: 2/2, step 6863/7134 completed (loss: 0.018671274185180664, acc: 1.0)
[2025-02-13 21:14:29,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:29,687][root][INFO] - Training Epoch: 2/2, step 6864/7134 completed (loss: 0.02140698954463005, acc: 1.0)
[2025-02-13 21:14:29,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:30,027][root][INFO] - Training Epoch: 2/2, step 6865/7134 completed (loss: 0.055794671177864075, acc: 0.9756097793579102)
[2025-02-13 21:14:30,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:30,398][root][INFO] - Training Epoch: 2/2, step 6866/7134 completed (loss: 0.009741666726768017, acc: 0.9934640526771545)
[2025-02-13 21:14:30,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:30,799][root][INFO] - Training Epoch: 2/2, step 6867/7134 completed (loss: 0.005990577396005392, acc: 1.0)
[2025-02-13 21:14:30,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:31,145][root][INFO] - Training Epoch: 2/2, step 6868/7134 completed (loss: 0.0069640628062188625, acc: 1.0)
[2025-02-13 21:14:31,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:31,491][root][INFO] - Training Epoch: 2/2, step 6869/7134 completed (loss: 0.010981442406773567, acc: 1.0)
[2025-02-13 21:14:31,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:31,861][root][INFO] - Training Epoch: 2/2, step 6870/7134 completed (loss: 0.035388316959142685, acc: 0.9873417615890503)
[2025-02-13 21:14:32,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:32,231][root][INFO] - Training Epoch: 2/2, step 6871/7134 completed (loss: 0.007891800254583359, acc: 1.0)
[2025-02-13 21:14:32,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:32,622][root][INFO] - Training Epoch: 2/2, step 6872/7134 completed (loss: 0.04857894033193588, acc: 0.9943181872367859)
[2025-02-13 21:14:32,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:33,004][root][INFO] - Training Epoch: 2/2, step 6873/7134 completed (loss: 0.07001996785402298, acc: 0.9882352948188782)
[2025-02-13 21:14:33,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:33,358][root][INFO] - Training Epoch: 2/2, step 6874/7134 completed (loss: 0.008619822561740875, acc: 1.0)
[2025-02-13 21:14:33,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:33,742][root][INFO] - Training Epoch: 2/2, step 6875/7134 completed (loss: 0.06572073698043823, acc: 0.9798657894134521)
[2025-02-13 21:14:33,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:34,117][root][INFO] - Training Epoch: 2/2, step 6876/7134 completed (loss: 0.019600406289100647, acc: 1.0)
[2025-02-13 21:14:34,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:34,497][root][INFO] - Training Epoch: 2/2, step 6877/7134 completed (loss: 0.020119039341807365, acc: 1.0)
[2025-02-13 21:14:34,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:34,878][root][INFO] - Training Epoch: 2/2, step 6878/7134 completed (loss: 0.03782400116324425, acc: 0.9934640526771545)
[2025-02-13 21:14:35,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:35,222][root][INFO] - Training Epoch: 2/2, step 6879/7134 completed (loss: 0.022948523983359337, acc: 1.0)
[2025-02-13 21:14:35,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:35,618][root][INFO] - Training Epoch: 2/2, step 6880/7134 completed (loss: 0.03694157302379608, acc: 0.989847719669342)
[2025-02-13 21:14:35,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:35,983][root][INFO] - Training Epoch: 2/2, step 6881/7134 completed (loss: 0.12041793018579483, acc: 0.978723406791687)
[2025-02-13 21:14:36,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:36,359][root][INFO] - Training Epoch: 2/2, step 6882/7134 completed (loss: 0.061162687838077545, acc: 0.9929577708244324)
[2025-02-13 21:14:36,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:36,712][root][INFO] - Training Epoch: 2/2, step 6883/7134 completed (loss: 0.04595322534441948, acc: 0.9852941036224365)
[2025-02-13 21:14:36,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:37,074][root][INFO] - Training Epoch: 2/2, step 6884/7134 completed (loss: 0.08315060287714005, acc: 0.9829545617103577)
[2025-02-13 21:14:37,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:37,412][root][INFO] - Training Epoch: 2/2, step 6885/7134 completed (loss: 0.02524399384856224, acc: 1.0)
[2025-02-13 21:14:37,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:37,773][root][INFO] - Training Epoch: 2/2, step 6886/7134 completed (loss: 0.055729035288095474, acc: 0.9864864945411682)
[2025-02-13 21:14:37,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:38,124][root][INFO] - Training Epoch: 2/2, step 6887/7134 completed (loss: 0.0374356284737587, acc: 0.991304337978363)
[2025-02-13 21:14:38,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:38,490][root][INFO] - Training Epoch: 2/2, step 6888/7134 completed (loss: 0.07627368718385696, acc: 0.9866666793823242)
[2025-02-13 21:14:38,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:38,848][root][INFO] - Training Epoch: 2/2, step 6889/7134 completed (loss: 0.035190023481845856, acc: 1.0)
[2025-02-13 21:14:38,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:39,208][root][INFO] - Training Epoch: 2/2, step 6890/7134 completed (loss: 0.07896629720926285, acc: 0.9784946441650391)
[2025-02-13 21:14:39,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:39,572][root][INFO] - Training Epoch: 2/2, step 6891/7134 completed (loss: 0.025358697399497032, acc: 0.9938271641731262)
[2025-02-13 21:14:39,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:39,939][root][INFO] - Training Epoch: 2/2, step 6892/7134 completed (loss: 0.0517847016453743, acc: 0.9838709831237793)
[2025-02-13 21:14:40,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:40,296][root][INFO] - Training Epoch: 2/2, step 6893/7134 completed (loss: 0.07027480751276016, acc: 0.995192289352417)
[2025-02-13 21:14:40,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:40,660][root][INFO] - Training Epoch: 2/2, step 6894/7134 completed (loss: 0.0881582498550415, acc: 0.984375)
[2025-02-13 21:14:40,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:41,019][root][INFO] - Training Epoch: 2/2, step 6895/7134 completed (loss: 0.05170392245054245, acc: 0.9901477694511414)
[2025-02-13 21:14:41,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:41,393][root][INFO] - Training Epoch: 2/2, step 6896/7134 completed (loss: 0.11418601870536804, acc: 0.9653179049491882)
[2025-02-13 21:14:41,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:41,763][root][INFO] - Training Epoch: 2/2, step 6897/7134 completed (loss: 0.05812968313694, acc: 0.9900990128517151)
[2025-02-13 21:14:41,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:42,123][root][INFO] - Training Epoch: 2/2, step 6898/7134 completed (loss: 0.04308897256851196, acc: 0.9884393215179443)
[2025-02-13 21:14:42,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:42,483][root][INFO] - Training Epoch: 2/2, step 6899/7134 completed (loss: 0.048584893345832825, acc: 0.978723406791687)
[2025-02-13 21:14:42,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:42,845][root][INFO] - Training Epoch: 2/2, step 6900/7134 completed (loss: 0.2488984763622284, acc: 0.9595375657081604)
[2025-02-13 21:14:42,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:43,209][root][INFO] - Training Epoch: 2/2, step 6901/7134 completed (loss: 0.08620283007621765, acc: 0.9729729890823364)
[2025-02-13 21:14:43,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:43,581][root][INFO] - Training Epoch: 2/2, step 6902/7134 completed (loss: 0.011052882298827171, acc: 1.0)
[2025-02-13 21:14:43,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:43,945][root][INFO] - Training Epoch: 2/2, step 6903/7134 completed (loss: 0.025529131293296814, acc: 1.0)
[2025-02-13 21:14:44,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:44,323][root][INFO] - Training Epoch: 2/2, step 6904/7134 completed (loss: 0.046842772513628006, acc: 0.9896373152732849)
[2025-02-13 21:14:44,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:44,697][root][INFO] - Training Epoch: 2/2, step 6905/7134 completed (loss: 0.051629919558763504, acc: 0.9850746393203735)
[2025-02-13 21:14:44,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:45,063][root][INFO] - Training Epoch: 2/2, step 6906/7134 completed (loss: 0.10669233649969101, acc: 0.9852941036224365)
[2025-02-13 21:14:45,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:45,450][root][INFO] - Training Epoch: 2/2, step 6907/7134 completed (loss: 0.04400501400232315, acc: 0.9896373152732849)
[2025-02-13 21:14:45,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:45,813][root][INFO] - Training Epoch: 2/2, step 6908/7134 completed (loss: 0.07463157176971436, acc: 0.9809523820877075)
[2025-02-13 21:14:45,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:46,186][root][INFO] - Training Epoch: 2/2, step 6909/7134 completed (loss: 0.10249385982751846, acc: 0.9781420826911926)
[2025-02-13 21:14:46,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:46,553][root][INFO] - Training Epoch: 2/2, step 6910/7134 completed (loss: 0.04749583080410957, acc: 0.9895833134651184)
[2025-02-13 21:14:46,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:46,896][root][INFO] - Training Epoch: 2/2, step 6911/7134 completed (loss: 0.24510273337364197, acc: 0.950276255607605)
[2025-02-13 21:14:47,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:47,246][root][INFO] - Training Epoch: 2/2, step 6912/7134 completed (loss: 0.10871893912553787, acc: 0.9804878234863281)
[2025-02-13 21:14:47,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:47,647][root][INFO] - Training Epoch: 2/2, step 6913/7134 completed (loss: 0.06920036673545837, acc: 0.9855072498321533)
[2025-02-13 21:14:47,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:48,022][root][INFO] - Training Epoch: 2/2, step 6914/7134 completed (loss: 0.17991459369659424, acc: 0.9607843160629272)
[2025-02-13 21:14:48,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:48,398][root][INFO] - Training Epoch: 2/2, step 6915/7134 completed (loss: 0.08038020133972168, acc: 0.9891892075538635)
[2025-02-13 21:14:48,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:48,749][root][INFO] - Training Epoch: 2/2, step 6916/7134 completed (loss: 0.0946473479270935, acc: 0.9804878234863281)
[2025-02-13 21:14:48,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:49,133][root][INFO] - Training Epoch: 2/2, step 6917/7134 completed (loss: 0.12977083027362823, acc: 0.9793814420700073)
[2025-02-13 21:14:49,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:49,483][root][INFO] - Training Epoch: 2/2, step 6918/7134 completed (loss: 0.05873672291636467, acc: 0.9845361113548279)
[2025-02-13 21:14:49,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:49,892][root][INFO] - Training Epoch: 2/2, step 6919/7134 completed (loss: 0.1178356185555458, acc: 0.9796954393386841)
[2025-02-13 21:14:50,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:50,273][root][INFO] - Training Epoch: 2/2, step 6920/7134 completed (loss: 0.02630867063999176, acc: 0.9940119981765747)
[2025-02-13 21:14:50,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:50,672][root][INFO] - Training Epoch: 2/2, step 6921/7134 completed (loss: 0.08645332604646683, acc: 0.9754902124404907)
[2025-02-13 21:14:50,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:51,025][root][INFO] - Training Epoch: 2/2, step 6922/7134 completed (loss: 0.07788792997598648, acc: 0.9743589758872986)
[2025-02-13 21:14:51,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:51,392][root][INFO] - Training Epoch: 2/2, step 6923/7134 completed (loss: 0.07120242714881897, acc: 0.9912280440330505)
[2025-02-13 21:14:51,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:51,753][root][INFO] - Training Epoch: 2/2, step 6924/7134 completed (loss: 0.0495418906211853, acc: 0.9818181991577148)
[2025-02-13 21:14:51,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:52,106][root][INFO] - Training Epoch: 2/2, step 6925/7134 completed (loss: 0.013127071782946587, acc: 1.0)
[2025-02-13 21:14:52,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:52,452][root][INFO] - Training Epoch: 2/2, step 6926/7134 completed (loss: 0.10335047543048859, acc: 0.9603174328804016)
[2025-02-13 21:14:52,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:52,815][root][INFO] - Training Epoch: 2/2, step 6927/7134 completed (loss: 0.1229635551571846, acc: 0.9752475023269653)
[2025-02-13 21:14:52,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:53,151][root][INFO] - Training Epoch: 2/2, step 6928/7134 completed (loss: 0.1371564269065857, acc: 0.9765625)
[2025-02-13 21:14:53,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:53,531][root][INFO] - Training Epoch: 2/2, step 6929/7134 completed (loss: 0.041411951184272766, acc: 0.9886363744735718)
[2025-02-13 21:14:53,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:53,904][root][INFO] - Training Epoch: 2/2, step 6930/7134 completed (loss: 0.03375132009387016, acc: 0.9895833134651184)
[2025-02-13 21:14:54,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:54,284][root][INFO] - Training Epoch: 2/2, step 6931/7134 completed (loss: 0.0907835066318512, acc: 0.9788359999656677)
[2025-02-13 21:14:54,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:54,636][root][INFO] - Training Epoch: 2/2, step 6932/7134 completed (loss: 0.1355166882276535, acc: 0.976047933101654)
[2025-02-13 21:14:54,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:55,005][root][INFO] - Training Epoch: 2/2, step 6933/7134 completed (loss: 0.13014428317546844, acc: 0.976047933101654)
[2025-02-13 21:14:55,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:55,345][root][INFO] - Training Epoch: 2/2, step 6934/7134 completed (loss: 0.13684329390525818, acc: 0.9615384340286255)
[2025-02-13 21:14:55,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:55,710][root][INFO] - Training Epoch: 2/2, step 6935/7134 completed (loss: 0.0827137902379036, acc: 0.978723406791687)
[2025-02-13 21:14:55,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:56,074][root][INFO] - Training Epoch: 2/2, step 6936/7134 completed (loss: 0.03906533867120743, acc: 0.9946236610412598)
[2025-02-13 21:14:56,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:56,436][root][INFO] - Training Epoch: 2/2, step 6937/7134 completed (loss: 0.14203190803527832, acc: 0.9726027250289917)
[2025-02-13 21:14:56,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:56,811][root][INFO] - Training Epoch: 2/2, step 6938/7134 completed (loss: 0.21748720109462738, acc: 0.9425287246704102)
[2025-02-13 21:14:56,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:57,202][root][INFO] - Training Epoch: 2/2, step 6939/7134 completed (loss: 0.12023739516735077, acc: 0.9642857313156128)
[2025-02-13 21:14:57,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:57,556][root][INFO] - Training Epoch: 2/2, step 6940/7134 completed (loss: 0.17321977019309998, acc: 0.9448275566101074)
[2025-02-13 21:14:57,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:57,875][root][INFO] - Training Epoch: 2/2, step 6941/7134 completed (loss: 0.035981230437755585, acc: 0.9852941036224365)
[2025-02-13 21:14:58,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:58,195][root][INFO] - Training Epoch: 2/2, step 6942/7134 completed (loss: 0.13072435557842255, acc: 0.9745222926139832)
[2025-02-13 21:14:58,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:58,543][root][INFO] - Training Epoch: 2/2, step 6943/7134 completed (loss: 0.11164340376853943, acc: 0.9557521939277649)
[2025-02-13 21:14:58,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:58,930][root][INFO] - Training Epoch: 2/2, step 6944/7134 completed (loss: 0.08487861603498459, acc: 0.9862068891525269)
[2025-02-13 21:14:59,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:59,251][root][INFO] - Training Epoch: 2/2, step 6945/7134 completed (loss: 0.16953931748867035, acc: 0.9609375)
[2025-02-13 21:14:59,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:59,594][root][INFO] - Training Epoch: 2/2, step 6946/7134 completed (loss: 0.06039509177207947, acc: 0.9784946441650391)
[2025-02-13 21:14:59,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:59,943][root][INFO] - Training Epoch: 2/2, step 6947/7134 completed (loss: 0.09417951107025146, acc: 0.982300877571106)
[2025-02-13 21:15:00,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:00,293][root][INFO] - Training Epoch: 2/2, step 6948/7134 completed (loss: 0.14603674411773682, acc: 0.9571428298950195)
[2025-02-13 21:15:00,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:00,694][root][INFO] - Training Epoch: 2/2, step 6949/7134 completed (loss: 0.08378595858812332, acc: 0.9652174115180969)
[2025-02-13 21:15:00,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:01,036][root][INFO] - Training Epoch: 2/2, step 6950/7134 completed (loss: 0.06237264722585678, acc: 0.9873417615890503)
[2025-02-13 21:15:01,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:01,406][root][INFO] - Training Epoch: 2/2, step 6951/7134 completed (loss: 0.07626268267631531, acc: 0.9791666865348816)
[2025-02-13 21:15:01,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:01,749][root][INFO] - Training Epoch: 2/2, step 6952/7134 completed (loss: 0.07214553654193878, acc: 0.9714285731315613)
[2025-02-13 21:15:01,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:02,133][root][INFO] - Training Epoch: 2/2, step 6953/7134 completed (loss: 0.06997555494308472, acc: 0.9924812316894531)
[2025-02-13 21:15:02,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:02,528][root][INFO] - Training Epoch: 2/2, step 6954/7134 completed (loss: 0.17436033487319946, acc: 0.9655172228813171)
[2025-02-13 21:15:02,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:02,907][root][INFO] - Training Epoch: 2/2, step 6955/7134 completed (loss: 0.08420295268297195, acc: 0.9750000238418579)
[2025-02-13 21:15:03,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:03,292][root][INFO] - Training Epoch: 2/2, step 6956/7134 completed (loss: 0.05988062545657158, acc: 0.985401451587677)
[2025-02-13 21:15:03,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:03,663][root][INFO] - Training Epoch: 2/2, step 6957/7134 completed (loss: 0.050772976130247116, acc: 0.984000027179718)
[2025-02-13 21:15:03,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:04,051][root][INFO] - Training Epoch: 2/2, step 6958/7134 completed (loss: 0.10058416426181793, acc: 0.9658119678497314)
[2025-02-13 21:15:04,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:04,405][root][INFO] - Training Epoch: 2/2, step 6959/7134 completed (loss: 0.21286709606647491, acc: 0.9541284441947937)
[2025-02-13 21:15:04,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:04,797][root][INFO] - Training Epoch: 2/2, step 6960/7134 completed (loss: 0.11126963049173355, acc: 0.9791666865348816)
[2025-02-13 21:15:04,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:05,177][root][INFO] - Training Epoch: 2/2, step 6961/7134 completed (loss: 0.04838859662413597, acc: 0.9829059839248657)
[2025-02-13 21:15:05,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:05,557][root][INFO] - Training Epoch: 2/2, step 6962/7134 completed (loss: 0.06806369870901108, acc: 0.9855072498321533)
[2025-02-13 21:15:05,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:05,949][root][INFO] - Training Epoch: 2/2, step 6963/7134 completed (loss: 0.053299430757761, acc: 0.9801324605941772)
[2025-02-13 21:15:06,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:06,321][root][INFO] - Training Epoch: 2/2, step 6964/7134 completed (loss: 0.012598874047398567, acc: 1.0)
[2025-02-13 21:15:06,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:06,697][root][INFO] - Training Epoch: 2/2, step 6965/7134 completed (loss: 0.017389964312314987, acc: 1.0)
[2025-02-13 21:15:06,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:07,079][root][INFO] - Training Epoch: 2/2, step 6966/7134 completed (loss: 0.0442051887512207, acc: 0.9904761910438538)
[2025-02-13 21:15:07,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:07,460][root][INFO] - Training Epoch: 2/2, step 6967/7134 completed (loss: 0.02465415559709072, acc: 1.0)
[2025-02-13 21:15:07,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:07,827][root][INFO] - Training Epoch: 2/2, step 6968/7134 completed (loss: 0.05877798795700073, acc: 0.9865771532058716)
[2025-02-13 21:15:07,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:08,178][root][INFO] - Training Epoch: 2/2, step 6969/7134 completed (loss: 0.14700265228748322, acc: 0.9596773982048035)
[2025-02-13 21:15:08,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:08,514][root][INFO] - Training Epoch: 2/2, step 6970/7134 completed (loss: 0.17120341956615448, acc: 0.9741379022598267)
[2025-02-13 21:15:08,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:08,897][root][INFO] - Training Epoch: 2/2, step 6971/7134 completed (loss: 0.07815209776163101, acc: 0.9930555820465088)
[2025-02-13 21:15:09,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:09,274][root][INFO] - Training Epoch: 2/2, step 6972/7134 completed (loss: 0.08302785456180573, acc: 0.9767441749572754)
[2025-02-13 21:15:09,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:09,647][root][INFO] - Training Epoch: 2/2, step 6973/7134 completed (loss: 0.167398139834404, acc: 0.982758641242981)
[2025-02-13 21:15:09,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:10,039][root][INFO] - Training Epoch: 2/2, step 6974/7134 completed (loss: 0.11212743073701859, acc: 0.9607843160629272)
[2025-02-13 21:15:10,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:10,421][root][INFO] - Training Epoch: 2/2, step 6975/7134 completed (loss: 0.11465933173894882, acc: 0.9914529919624329)
[2025-02-13 21:15:10,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:10,789][root][INFO] - Training Epoch: 2/2, step 6976/7134 completed (loss: 0.05748452618718147, acc: 0.9785714149475098)
[2025-02-13 21:15:10,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:11,157][root][INFO] - Training Epoch: 2/2, step 6977/7134 completed (loss: 0.0838070660829544, acc: 0.9770992398262024)
[2025-02-13 21:15:11,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:11,557][root][INFO] - Training Epoch: 2/2, step 6978/7134 completed (loss: 0.05984731391072273, acc: 0.9873417615890503)
[2025-02-13 21:15:11,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:11,940][root][INFO] - Training Epoch: 2/2, step 6979/7134 completed (loss: 0.05548698082566261, acc: 0.9933775067329407)
[2025-02-13 21:15:12,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:12,321][root][INFO] - Training Epoch: 2/2, step 6980/7134 completed (loss: 0.17382371425628662, acc: 0.9717513918876648)
[2025-02-13 21:15:12,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:12,699][root][INFO] - Training Epoch: 2/2, step 6981/7134 completed (loss: 0.11319334805011749, acc: 0.9536082744598389)
[2025-02-13 21:15:12,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:13,119][root][INFO] - Training Epoch: 2/2, step 6982/7134 completed (loss: 0.19502967596054077, acc: 0.9581151604652405)
[2025-02-13 21:15:13,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:13,496][root][INFO] - Training Epoch: 2/2, step 6983/7134 completed (loss: 0.2208070009946823, acc: 0.9679487347602844)
[2025-02-13 21:15:13,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:13,883][root][INFO] - Training Epoch: 2/2, step 6984/7134 completed (loss: 0.12798328697681427, acc: 0.9674418568611145)
[2025-02-13 21:15:14,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:14,279][root][INFO] - Training Epoch: 2/2, step 6985/7134 completed (loss: 0.21210049092769623, acc: 0.9527897238731384)
[2025-02-13 21:15:14,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:14,655][root][INFO] - Training Epoch: 2/2, step 6986/7134 completed (loss: 0.08449415862560272, acc: 0.9753086566925049)
[2025-02-13 21:15:14,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:15,041][root][INFO] - Training Epoch: 2/2, step 6987/7134 completed (loss: 0.13659042119979858, acc: 0.9710744023323059)
[2025-02-13 21:15:15,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:15,383][root][INFO] - Training Epoch: 2/2, step 6988/7134 completed (loss: 0.05989481881260872, acc: 0.9921259880065918)
[2025-02-13 21:15:15,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:15,734][root][INFO] - Training Epoch: 2/2, step 6989/7134 completed (loss: 0.250728040933609, acc: 0.9459459185600281)
[2025-02-13 21:15:15,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:16,076][root][INFO] - Training Epoch: 2/2, step 6990/7134 completed (loss: 0.03888745233416557, acc: 0.9814814925193787)
[2025-02-13 21:15:16,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:16,465][root][INFO] - Training Epoch: 2/2, step 6991/7134 completed (loss: 0.22600097954273224, acc: 0.9504950642585754)
[2025-02-13 21:15:16,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:16,854][root][INFO] - Training Epoch: 2/2, step 6992/7134 completed (loss: 0.0709349513053894, acc: 0.9729729890823364)
[2025-02-13 21:15:16,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:17,210][root][INFO] - Training Epoch: 2/2, step 6993/7134 completed (loss: 0.11883313953876495, acc: 0.9554139971733093)
[2025-02-13 21:15:17,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:17,573][root][INFO] - Training Epoch: 2/2, step 6994/7134 completed (loss: 0.17423243820667267, acc: 0.9599999785423279)
[2025-02-13 21:15:17,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:17,952][root][INFO] - Training Epoch: 2/2, step 6995/7134 completed (loss: 0.08063182979822159, acc: 0.9709302186965942)
[2025-02-13 21:15:18,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:18,306][root][INFO] - Training Epoch: 2/2, step 6996/7134 completed (loss: 0.026676317676901817, acc: 0.9931507110595703)
[2025-02-13 21:15:18,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:18,655][root][INFO] - Training Epoch: 2/2, step 6997/7134 completed (loss: 0.05153194069862366, acc: 0.9870967864990234)
[2025-02-13 21:15:18,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:19,021][root][INFO] - Training Epoch: 2/2, step 6998/7134 completed (loss: 0.08607243746519089, acc: 0.9789473414421082)
[2025-02-13 21:15:19,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:19,407][root][INFO] - Training Epoch: 2/2, step 6999/7134 completed (loss: 0.04472656548023224, acc: 0.9830508232116699)
[2025-02-13 21:15:19,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:19,792][root][INFO] - Training Epoch: 2/2, step 7000/7134 completed (loss: 0.0481266975402832, acc: 0.9727272987365723)
[2025-02-13 21:15:19,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:20,161][root][INFO] - Training Epoch: 2/2, step 7001/7134 completed (loss: 0.22449594736099243, acc: 0.9740259647369385)
[2025-02-13 21:15:20,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:20,545][root][INFO] - Training Epoch: 2/2, step 7002/7134 completed (loss: 0.07942573726177216, acc: 0.9915966391563416)
[2025-02-13 21:15:20,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:20,912][root][INFO] - Training Epoch: 2/2, step 7003/7134 completed (loss: 0.04524245858192444, acc: 0.984455943107605)
[2025-02-13 21:15:21,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:21,285][root][INFO] - Training Epoch: 2/2, step 7004/7134 completed (loss: 0.08791816234588623, acc: 0.9756097793579102)
[2025-02-13 21:15:21,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:21,662][root][INFO] - Training Epoch: 2/2, step 7005/7134 completed (loss: 0.05271712318062782, acc: 0.9879518151283264)
[2025-02-13 21:15:21,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:22,043][root][INFO] - Training Epoch: 2/2, step 7006/7134 completed (loss: 0.06354865431785583, acc: 0.9751552939414978)
[2025-02-13 21:15:22,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:22,405][root][INFO] - Training Epoch: 2/2, step 7007/7134 completed (loss: 0.08196190744638443, acc: 0.9870967864990234)
[2025-02-13 21:15:22,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:22,759][root][INFO] - Training Epoch: 2/2, step 7008/7134 completed (loss: 0.037882354110479355, acc: 0.9944751262664795)
[2025-02-13 21:15:22,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:23,121][root][INFO] - Training Epoch: 2/2, step 7009/7134 completed (loss: 0.08598563075065613, acc: 0.96875)
[2025-02-13 21:15:23,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:23,470][root][INFO] - Training Epoch: 2/2, step 7010/7134 completed (loss: 0.05254516005516052, acc: 0.9870967864990234)
[2025-02-13 21:15:23,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:23,828][root][INFO] - Training Epoch: 2/2, step 7011/7134 completed (loss: 0.13392947614192963, acc: 0.9738219976425171)
[2025-02-13 21:15:23,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:24,200][root][INFO] - Training Epoch: 2/2, step 7012/7134 completed (loss: 0.02112959884107113, acc: 1.0)
[2025-02-13 21:15:24,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:24,581][root][INFO] - Training Epoch: 2/2, step 7013/7134 completed (loss: 0.02777133323252201, acc: 0.993630588054657)
[2025-02-13 21:15:24,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:24,947][root][INFO] - Training Epoch: 2/2, step 7014/7134 completed (loss: 0.10631220042705536, acc: 0.9662162065505981)
[2025-02-13 21:15:25,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:25,298][root][INFO] - Training Epoch: 2/2, step 7015/7134 completed (loss: 0.04089216887950897, acc: 0.9850746393203735)
[2025-02-13 21:15:25,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:25,707][root][INFO] - Training Epoch: 2/2, step 7016/7134 completed (loss: 0.0990302562713623, acc: 0.9807692170143127)
[2025-02-13 21:15:25,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:26,060][root][INFO] - Training Epoch: 2/2, step 7017/7134 completed (loss: 0.05659202113747597, acc: 0.982300877571106)
[2025-02-13 21:15:26,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:26,429][root][INFO] - Training Epoch: 2/2, step 7018/7134 completed (loss: 0.17624258995056152, acc: 0.9691358208656311)
[2025-02-13 21:15:26,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:26,777][root][INFO] - Training Epoch: 2/2, step 7019/7134 completed (loss: 0.30382832884788513, acc: 0.949367105960846)
[2025-02-13 21:15:26,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:27,126][root][INFO] - Training Epoch: 2/2, step 7020/7134 completed (loss: 0.03372238948941231, acc: 0.9939758777618408)
[2025-02-13 21:15:27,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:27,519][root][INFO] - Training Epoch: 2/2, step 7021/7134 completed (loss: 0.03963317349553108, acc: 0.9925925731658936)
[2025-02-13 21:15:27,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:27,908][root][INFO] - Training Epoch: 2/2, step 7022/7134 completed (loss: 0.12411054223775864, acc: 0.9583333134651184)
[2025-02-13 21:15:28,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:28,325][root][INFO] - Training Epoch: 2/2, step 7023/7134 completed (loss: 0.04514972120523453, acc: 1.0)
[2025-02-13 21:15:28,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:28,678][root][INFO] - Training Epoch: 2/2, step 7024/7134 completed (loss: 0.03613940253853798, acc: 1.0)
[2025-02-13 21:15:28,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:29,062][root][INFO] - Training Epoch: 2/2, step 7025/7134 completed (loss: 0.0935671329498291, acc: 0.9904761910438538)
[2025-02-13 21:15:29,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:29,418][root][INFO] - Training Epoch: 2/2, step 7026/7134 completed (loss: 0.06958907842636108, acc: 0.984375)
[2025-02-13 21:15:29,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:29,763][root][INFO] - Training Epoch: 2/2, step 7027/7134 completed (loss: 0.06425564736127853, acc: 1.0)
[2025-02-13 21:15:29,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:30,156][root][INFO] - Training Epoch: 2/2, step 7028/7134 completed (loss: 0.02516009286046028, acc: 0.9890109896659851)
[2025-02-13 21:15:30,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:30,537][root][INFO] - Training Epoch: 2/2, step 7029/7134 completed (loss: 0.06387721747159958, acc: 0.9775280952453613)
[2025-02-13 21:15:30,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:30,920][root][INFO] - Training Epoch: 2/2, step 7030/7134 completed (loss: 0.04032235965132713, acc: 1.0)
[2025-02-13 21:15:31,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:31,304][root][INFO] - Training Epoch: 2/2, step 7031/7134 completed (loss: 0.07619016617536545, acc: 1.0)
[2025-02-13 21:15:31,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:31,672][root][INFO] - Training Epoch: 2/2, step 7032/7134 completed (loss: 0.06404085457324982, acc: 1.0)
[2025-02-13 21:15:31,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:32,017][root][INFO] - Training Epoch: 2/2, step 7033/7134 completed (loss: 0.15173618495464325, acc: 0.9506173133850098)
[2025-02-13 21:15:32,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:32,370][root][INFO] - Training Epoch: 2/2, step 7034/7134 completed (loss: 0.035792551934719086, acc: 1.0)
[2025-02-13 21:15:32,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:32,754][root][INFO] - Training Epoch: 2/2, step 7035/7134 completed (loss: 0.11761008203029633, acc: 0.9841269850730896)
[2025-02-13 21:15:32,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:33,114][root][INFO] - Training Epoch: 2/2, step 7036/7134 completed (loss: 0.08392001688480377, acc: 0.9682539701461792)
[2025-02-13 21:15:33,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:33,475][root][INFO] - Training Epoch: 2/2, step 7037/7134 completed (loss: 0.0819505825638771, acc: 0.9701492786407471)
[2025-02-13 21:15:33,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:33,882][root][INFO] - Training Epoch: 2/2, step 7038/7134 completed (loss: 0.0821823924779892, acc: 0.9746835231781006)
[2025-02-13 21:15:34,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:34,259][root][INFO] - Training Epoch: 2/2, step 7039/7134 completed (loss: 0.032055530697107315, acc: 1.0)
[2025-02-13 21:15:34,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:34,606][root][INFO] - Training Epoch: 2/2, step 7040/7134 completed (loss: 0.1556614637374878, acc: 0.9599999785423279)
[2025-02-13 21:15:34,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:34,959][root][INFO] - Training Epoch: 2/2, step 7041/7134 completed (loss: 0.09024170786142349, acc: 0.9750000238418579)
[2025-02-13 21:15:35,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:35,339][root][INFO] - Training Epoch: 2/2, step 7042/7134 completed (loss: 0.07085233926773071, acc: 1.0)
[2025-02-13 21:15:35,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:35,685][root][INFO] - Training Epoch: 2/2, step 7043/7134 completed (loss: 0.11109516024589539, acc: 0.9733333587646484)
[2025-02-13 21:15:35,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:36,058][root][INFO] - Training Epoch: 2/2, step 7044/7134 completed (loss: 0.051478683948516846, acc: 1.0)
[2025-02-13 21:15:36,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:36,414][root][INFO] - Training Epoch: 2/2, step 7045/7134 completed (loss: 0.06438720226287842, acc: 0.9862068891525269)
[2025-02-13 21:15:36,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:36,785][root][INFO] - Training Epoch: 2/2, step 7046/7134 completed (loss: 0.03106890805065632, acc: 1.0)
[2025-02-13 21:15:36,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:37,149][root][INFO] - Training Epoch: 2/2, step 7047/7134 completed (loss: 0.049029164016246796, acc: 0.9953051805496216)
[2025-02-13 21:15:37,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:37,527][root][INFO] - Training Epoch: 2/2, step 7048/7134 completed (loss: 0.01905553974211216, acc: 0.9948453903198242)
[2025-02-13 21:15:37,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:37,898][root][INFO] - Training Epoch: 2/2, step 7049/7134 completed (loss: 0.03772762790322304, acc: 1.0)
[2025-02-13 21:15:38,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:38,273][root][INFO] - Training Epoch: 2/2, step 7050/7134 completed (loss: 0.02210211008787155, acc: 1.0)
[2025-02-13 21:15:38,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:38,639][root][INFO] - Training Epoch: 2/2, step 7051/7134 completed (loss: 0.0305694080889225, acc: 0.9945651888847351)
[2025-02-13 21:15:38,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:39,007][root][INFO] - Training Epoch: 2/2, step 7052/7134 completed (loss: 0.05676824599504471, acc: 0.9826589822769165)
[2025-02-13 21:15:39,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:39,369][root][INFO] - Training Epoch: 2/2, step 7053/7134 completed (loss: 0.019415924325585365, acc: 1.0)
[2025-02-13 21:15:39,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:39,746][root][INFO] - Training Epoch: 2/2, step 7054/7134 completed (loss: 0.12346769869327545, acc: 0.9677419066429138)
[2025-02-13 21:15:39,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:40,141][root][INFO] - Training Epoch: 2/2, step 7055/7134 completed (loss: 0.13447806239128113, acc: 0.9615384340286255)
[2025-02-13 21:15:40,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:40,528][root][INFO] - Training Epoch: 2/2, step 7056/7134 completed (loss: 0.10818714648485184, acc: 0.9722222089767456)
[2025-02-13 21:15:40,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:40,905][root][INFO] - Training Epoch: 2/2, step 7057/7134 completed (loss: 0.3452889919281006, acc: 0.9337016344070435)
[2025-02-13 21:15:41,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:41,282][root][INFO] - Training Epoch: 2/2, step 7058/7134 completed (loss: 0.11375529319047928, acc: 0.9689922332763672)
[2025-02-13 21:15:41,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:41,664][root][INFO] - Training Epoch: 2/2, step 7059/7134 completed (loss: 0.05223143473267555, acc: 0.9935897588729858)
[2025-02-13 21:15:41,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:42,018][root][INFO] - Training Epoch: 2/2, step 7060/7134 completed (loss: 0.12168042361736298, acc: 0.9647887349128723)
[2025-02-13 21:15:42,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:42,415][root][INFO] - Training Epoch: 2/2, step 7061/7134 completed (loss: 0.13158439099788666, acc: 0.9623655676841736)
[2025-02-13 21:15:42,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:42,781][root][INFO] - Training Epoch: 2/2, step 7062/7134 completed (loss: 0.11449187994003296, acc: 0.9745762944221497)
[2025-02-13 21:15:42,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:43,145][root][INFO] - Training Epoch: 2/2, step 7063/7134 completed (loss: 0.23498781025409698, acc: 0.9567567706108093)
[2025-02-13 21:15:43,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:43,567][root][INFO] - Training Epoch: 2/2, step 7064/7134 completed (loss: 0.029603291302919388, acc: 0.9939758777618408)
[2025-02-13 21:15:43,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:43,940][root][INFO] - Training Epoch: 2/2, step 7065/7134 completed (loss: 0.011485770344734192, acc: 1.0)
[2025-02-13 21:15:44,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:44,301][root][INFO] - Training Epoch: 2/2, step 7066/7134 completed (loss: 0.040845345705747604, acc: 0.9842932224273682)
[2025-02-13 21:15:44,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:44,666][root][INFO] - Training Epoch: 2/2, step 7067/7134 completed (loss: 0.03789876401424408, acc: 0.9947090148925781)
[2025-02-13 21:15:44,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:45,040][root][INFO] - Training Epoch: 2/2, step 7068/7134 completed (loss: 0.03456580638885498, acc: 0.995121955871582)
[2025-02-13 21:15:45,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:45,427][root][INFO] - Training Epoch: 2/2, step 7069/7134 completed (loss: 0.050750020891427994, acc: 0.9913420081138611)
[2025-02-13 21:15:45,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:45,857][root][INFO] - Training Epoch: 2/2, step 7070/7134 completed (loss: 0.037329450249671936, acc: 0.9884393215179443)
[2025-02-13 21:15:46,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:46,239][root][INFO] - Training Epoch: 2/2, step 7071/7134 completed (loss: 0.03687971085309982, acc: 0.9831932783126831)
[2025-02-13 21:15:46,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:46,620][root][INFO] - Training Epoch: 2/2, step 7072/7134 completed (loss: 0.059759899973869324, acc: 0.9848484992980957)
[2025-02-13 21:15:46,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:47,016][root][INFO] - Training Epoch: 2/2, step 7073/7134 completed (loss: 0.07870330661535263, acc: 0.9899497628211975)
[2025-02-13 21:15:47,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:47,392][root][INFO] - Training Epoch: 2/2, step 7074/7134 completed (loss: 0.17791296541690826, acc: 0.9593495726585388)
[2025-02-13 21:15:47,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:47,789][root][INFO] - Training Epoch: 2/2, step 7075/7134 completed (loss: 0.09803974628448486, acc: 0.9808917045593262)
[2025-02-13 21:15:47,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:48,187][root][INFO] - Training Epoch: 2/2, step 7076/7134 completed (loss: 0.1505695879459381, acc: 0.9550561904907227)
[2025-02-13 21:15:48,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:48,582][root][INFO] - Training Epoch: 2/2, step 7077/7134 completed (loss: 0.1286313235759735, acc: 0.965753436088562)
[2025-02-13 21:15:48,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:48,968][root][INFO] - Training Epoch: 2/2, step 7078/7134 completed (loss: 0.09804406762123108, acc: 0.9724137783050537)
[2025-02-13 21:15:49,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:49,339][root][INFO] - Training Epoch: 2/2, step 7079/7134 completed (loss: 0.04194926843047142, acc: 0.9868420958518982)
[2025-02-13 21:15:49,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:49,712][root][INFO] - Training Epoch: 2/2, step 7080/7134 completed (loss: 0.09455547481775284, acc: 0.9659863710403442)
[2025-02-13 21:15:49,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:50,099][root][INFO] - Training Epoch: 2/2, step 7081/7134 completed (loss: 0.032620564103126526, acc: 0.9884393215179443)
[2025-02-13 21:15:50,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:50,458][root][INFO] - Training Epoch: 2/2, step 7082/7134 completed (loss: 0.11057019233703613, acc: 0.9868420958518982)
[2025-02-13 21:15:50,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:50,828][root][INFO] - Training Epoch: 2/2, step 7083/7134 completed (loss: 0.03519316762685776, acc: 1.0)
[2025-02-13 21:15:50,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:51,212][root][INFO] - Training Epoch: 2/2, step 7084/7134 completed (loss: 0.06313607096672058, acc: 0.977011501789093)
[2025-02-13 21:15:51,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:51,593][root][INFO] - Training Epoch: 2/2, step 7085/7134 completed (loss: 0.0920691266655922, acc: 0.9800000190734863)
[2025-02-13 21:15:51,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:51,983][root][INFO] - Training Epoch: 2/2, step 7086/7134 completed (loss: 0.054613083600997925, acc: 0.9870967864990234)
[2025-02-13 21:15:52,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:52,354][root][INFO] - Training Epoch: 2/2, step 7087/7134 completed (loss: 0.09321212023496628, acc: 0.9844961166381836)
[2025-02-13 21:15:52,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:52,726][root][INFO] - Training Epoch: 2/2, step 7088/7134 completed (loss: 0.10967584699392319, acc: 0.957446813583374)
[2025-02-13 21:15:52,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:53,081][root][INFO] - Training Epoch: 2/2, step 7089/7134 completed (loss: 0.1450912058353424, acc: 0.9741935729980469)
[2025-02-13 21:15:53,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:53,468][root][INFO] - Training Epoch: 2/2, step 7090/7134 completed (loss: 0.0830167606472969, acc: 0.988095223903656)
[2025-02-13 21:15:53,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:53,829][root][INFO] - Training Epoch: 2/2, step 7091/7134 completed (loss: 0.05277280509471893, acc: 0.9757575988769531)
[2025-02-13 21:15:53,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:54,203][root][INFO] - Training Epoch: 2/2, step 7092/7134 completed (loss: 0.1378667801618576, acc: 0.969924807548523)
[2025-02-13 21:15:54,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:54,585][root][INFO] - Training Epoch: 2/2, step 7093/7134 completed (loss: 0.07034406810998917, acc: 0.9743589758872986)
[2025-02-13 21:15:54,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:54,960][root][INFO] - Training Epoch: 2/2, step 7094/7134 completed (loss: 0.021495329216122627, acc: 1.0)
[2025-02-13 21:15:55,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:55,324][root][INFO] - Training Epoch: 2/2, step 7095/7134 completed (loss: 0.0859474390745163, acc: 0.9830508232116699)
[2025-02-13 21:15:55,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:55,671][root][INFO] - Training Epoch: 2/2, step 7096/7134 completed (loss: 0.0466538742184639, acc: 0.9929577708244324)
[2025-02-13 21:15:55,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:56,039][root][INFO] - Training Epoch: 2/2, step 7097/7134 completed (loss: 0.11015372723340988, acc: 0.9487179517745972)
[2025-02-13 21:15:56,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:56,412][root][INFO] - Training Epoch: 2/2, step 7098/7134 completed (loss: 0.054752252995967865, acc: 0.9930070042610168)
[2025-02-13 21:15:56,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:56,756][root][INFO] - Training Epoch: 2/2, step 7099/7134 completed (loss: 0.12133176624774933, acc: 0.9710144996643066)
[2025-02-13 21:15:56,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:57,121][root][INFO] - Training Epoch: 2/2, step 7100/7134 completed (loss: 0.08858097344636917, acc: 0.976047933101654)
[2025-02-13 21:15:57,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:57,466][root][INFO] - Training Epoch: 2/2, step 7101/7134 completed (loss: 0.05878031998872757, acc: 0.9856114983558655)
[2025-02-13 21:15:57,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:57,853][root][INFO] - Training Epoch: 2/2, step 7102/7134 completed (loss: 0.06280556321144104, acc: 0.9661017060279846)
[2025-02-13 21:15:58,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:58,237][root][INFO] - Training Epoch: 2/2, step 7103/7134 completed (loss: 0.02898130752146244, acc: 1.0)
[2025-02-13 21:15:58,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:58,650][root][INFO] - Training Epoch: 2/2, step 7104/7134 completed (loss: 0.06868013739585876, acc: 0.9824561476707458)
[2025-02-13 21:15:58,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:59,038][root][INFO] - Training Epoch: 2/2, step 7105/7134 completed (loss: 0.04928312823176384, acc: 0.9930555820465088)
[2025-02-13 21:15:59,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:59,416][root][INFO] - Training Epoch: 2/2, step 7106/7134 completed (loss: 0.013897469267249107, acc: 1.0)
[2025-02-13 21:15:59,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:59,793][root][INFO] - Training Epoch: 2/2, step 7107/7134 completed (loss: 0.024877438321709633, acc: 0.9937888383865356)
[2025-02-13 21:15:59,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:00,155][root][INFO] - Training Epoch: 2/2, step 7108/7134 completed (loss: 0.03961528465151787, acc: 0.9927007555961609)
[2025-02-13 21:16:00,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:00,530][root][INFO] - Training Epoch: 2/2, step 7109/7134 completed (loss: 0.0916546881198883, acc: 0.9555555582046509)
[2025-02-13 21:16:00,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:00,931][root][INFO] - Training Epoch: 2/2, step 7110/7134 completed (loss: 0.01674998551607132, acc: 1.0)
[2025-02-13 21:16:01,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:01,270][root][INFO] - Training Epoch: 2/2, step 7111/7134 completed (loss: 0.051721666008234024, acc: 0.9907407164573669)
[2025-02-13 21:16:01,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:01,616][root][INFO] - Training Epoch: 2/2, step 7112/7134 completed (loss: 0.02337542176246643, acc: 1.0)
[2025-02-13 21:16:01,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:01,994][root][INFO] - Training Epoch: 2/2, step 7113/7134 completed (loss: 0.1483963429927826, acc: 0.9617834687232971)
[2025-02-13 21:16:02,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:02,344][root][INFO] - Training Epoch: 2/2, step 7114/7134 completed (loss: 0.04420780390501022, acc: 0.9908257126808167)
[2025-02-13 21:16:02,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:02,719][root][INFO] - Training Epoch: 2/2, step 7115/7134 completed (loss: 0.11611338704824448, acc: 0.9541984796524048)
[2025-02-13 21:16:02,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:03,072][root][INFO] - Training Epoch: 2/2, step 7116/7134 completed (loss: 0.10147347301244736, acc: 0.9852941036224365)
[2025-02-13 21:16:03,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:03,457][root][INFO] - Training Epoch: 2/2, step 7117/7134 completed (loss: 0.07884910702705383, acc: 0.9921259880065918)
[2025-02-13 21:16:03,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:03,824][root][INFO] - Training Epoch: 2/2, step 7118/7134 completed (loss: 0.08025725930929184, acc: 0.9857142567634583)
[2025-02-13 21:16:03,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:04,197][root][INFO] - Training Epoch: 2/2, step 7119/7134 completed (loss: 0.14745554327964783, acc: 0.9862068891525269)
[2025-02-13 21:16:04,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:04,569][root][INFO] - Training Epoch: 2/2, step 7120/7134 completed (loss: 0.05619114637374878, acc: 0.9805825352668762)
[2025-02-13 21:16:04,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:04,949][root][INFO] - Training Epoch: 2/2, step 7121/7134 completed (loss: 0.01448030211031437, acc: 1.0)
[2025-02-13 21:16:05,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:05,308][root][INFO] - Training Epoch: 2/2, step 7122/7134 completed (loss: 0.08317936956882477, acc: 0.9677419066429138)
[2025-02-13 21:16:05,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:05,641][root][INFO] - Training Epoch: 2/2, step 7123/7134 completed (loss: 0.016543004661798477, acc: 1.0)
[2025-02-13 21:16:05,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:06,032][root][INFO] - Training Epoch: 2/2, step 7124/7134 completed (loss: 0.08911009877920151, acc: 0.9754098653793335)
[2025-02-13 21:16:06,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:06,392][root][INFO] - Training Epoch: 2/2, step 7125/7134 completed (loss: 0.08661731332540512, acc: 0.9770992398262024)
[2025-02-13 21:16:06,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:06,757][root][INFO] - Training Epoch: 2/2, step 7126/7134 completed (loss: 0.1515541970729828, acc: 0.9635036587715149)
[2025-02-13 21:16:06,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:07,129][root][INFO] - Training Epoch: 2/2, step 7127/7134 completed (loss: 0.03582124784588814, acc: 0.9876543283462524)
[2025-02-13 21:16:07,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:07,489][root][INFO] - Training Epoch: 2/2, step 7128/7134 completed (loss: 0.07771466672420502, acc: 0.9726027250289917)
[2025-02-13 21:16:07,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:07,868][root][INFO] - Training Epoch: 2/2, step 7129/7134 completed (loss: 0.3068464994430542, acc: 0.9379844665527344)
[2025-02-13 21:16:08,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:09,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:09,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:09,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:10,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:10,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:10,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:11,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:11,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:11,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:11,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:12,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:12,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:12,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:13,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:13,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:13,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:14,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:14,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:14,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:15,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:15,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:15,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:16,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:16,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:16,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:16,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:17,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:17,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:18,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:18,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:18,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:19,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:19,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:19,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:20,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:20,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:20,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:21,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:21,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:21,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:21,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:22,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:22,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:22,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:23,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:23,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:24,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:24,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:24,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:25,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:25,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:25,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:26,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:26,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:26,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:27,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:27,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:27,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:28,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:28,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:29,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:29,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:30,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:30,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:30,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:31,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:31,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:31,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:32,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:32,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:32,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:33,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:33,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:33,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:34,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:34,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:35,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:35,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:35,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:36,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:36,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:36,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:37,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:37,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:37,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:38,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:38,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:39,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:39,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:39,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:40,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:40,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:40,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:40,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:41,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:41,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:42,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:42,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:42,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:43,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:43,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:43,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:44,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:44,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:44,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:45,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:45,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:45,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:46,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:46,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:46,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:47,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:47,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:47,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:48,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:48,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:48,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:49,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:49,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:49,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:50,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:50,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:50,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:51,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:51,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:51,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:52,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:52,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:52,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:53,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:53,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:53,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:54,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:54,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:54,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:55,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:55,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:56,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:56,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:56,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:57,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:57,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:57,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:58,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:58,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:58,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:59,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:59,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:59,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:00,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:00,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:00,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:01,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:01,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:01,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:02,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:02,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:02,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:02,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:03,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:03,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:03,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:04,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:04,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:04,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:05,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:05,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:06,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:06,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:06,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:07,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:07,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:07,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:08,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:08,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:09,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:09,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:09,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:10,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:10,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:10,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:11,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:11,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:11,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:12,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:12,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:12,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:13,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:13,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:13,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:14,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:14,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:14,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:14,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:15,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:15,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:15,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:16,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:16,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:16,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:17,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:17,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:17,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:18,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:18,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:18,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:19,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:19,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:19,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:20,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:20,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:21,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:21,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:21,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:21,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:22,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:22,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:22,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:23,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:23,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:23,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:24,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:24,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:24,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:25,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:25,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:25,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:26,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:26,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:26,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:27,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:27,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:27,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:28,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:28,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:28,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:29,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:29,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:29,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:30,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:30,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:30,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:30,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:31,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:31,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:32,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:32,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:33,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:33,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:33,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:34,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:34,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:34,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:35,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:35,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:35,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:36,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:36,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:36,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:37,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:37,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:37,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:38,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:38,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:39,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:39,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:39,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:39,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:40,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:40,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:41,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:41,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:41,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:42,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:42,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:42,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:43,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:43,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:43,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:44,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:44,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:45,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:45,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:45,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:46,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:46,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:47,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:47,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:47,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:48,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:48,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:49,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:49,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:49,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:50,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:50,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:50,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:51,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:51,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:51,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:52,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:52,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:52,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:53,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:53,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:53,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:54,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:54,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:54,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:55,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:55,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:55,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:56,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:56,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:56,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:57,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:57,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:57,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:58,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:58,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:58,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:59,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:59,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:59,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:00,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:00,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:00,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:01,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:01,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:02,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:02,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:02,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:03,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:03,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:04,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:04,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:04,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:05,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:05,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:05,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:06,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:06,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:07,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:07,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:07,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:08,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:08,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:08,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:09,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:09,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:09,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:10,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:10,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:10,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:11,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:11,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:11,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:12,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:12,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:13,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:13,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:14,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:14,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:15,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:15,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:15,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:16,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:16,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:16,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:17,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:17,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:17,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:18,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:18,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:18,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:18,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:19,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:19,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:19,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:20,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:20,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:20,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:21,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:21,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:21,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:21,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:22,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:22,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:22,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:23,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:23,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:23,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:24,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:24,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:24,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:25,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:25,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:25,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:25,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:26,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:26,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:26,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:27,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:27,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:27,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:28,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:28,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:29,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:29,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:29,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:30,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:30,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:30,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:30,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:31,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:31,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:31,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:32,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:32,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:32,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:33,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:33,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:33,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:34,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:34,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:34,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:35,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:35,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:35,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:36,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:36,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:36,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:36,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:37,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:37,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:37,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:38,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:38,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:38,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:39,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:39,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:39,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:40,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:40,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:40,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:41,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:41,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:41,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:42,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:42,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:42,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:43,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:43,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:43,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:44,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:44,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:44,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:45,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:45,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:45,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:46,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:46,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:46,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:47,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:47,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:47,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:48,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:48,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:48,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:49,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:49,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:49,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:50,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:50,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:50,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:51,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:51,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:51,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:52,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:52,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:52,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:53,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:53,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:53,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:54,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:54,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:54,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:55,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:55,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:55,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:56,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:56,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:56,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:57,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:57,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:58,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:58,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:58,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:59,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:59,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:59,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:00,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:00,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:01,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:01,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:01,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:02,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:02,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:02,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:03,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:03,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:03,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:04,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:04,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:04,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:05,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:05,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:05,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:06,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:06,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:06,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:07,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:07,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:07,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:08,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:08,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:08,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:08,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:09,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:09,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:09,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:10,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:10,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:10,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:11,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:11,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:11,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:12,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:12,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:12,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:13,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:13,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:13,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:14,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:14,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:14,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:15,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:15,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:15,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:16,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:16,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:16,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:17,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:17,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:17,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:18,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:18,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:18,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:19,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:19,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:19,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:20,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:20,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:20,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:21,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:21,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:22,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:22,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:22,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:23,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:23,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:23,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:24,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:24,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:25,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:25,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:25,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:26,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:26,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:26,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:27,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:27,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:27,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:28,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:28,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:28,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:29,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:29,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:29,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:30,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:30,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:31,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:31,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:31,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:32,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:32,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:32,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:33,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:33,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:33,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:34,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:34,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:34,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:35,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:35,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:35,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:36,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:36,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:36,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:37,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:37,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:37,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:38,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:38,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:38,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:39,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:39,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:39,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:40,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:40,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:40,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:41,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:41,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:42,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:42,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:43,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:43,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:44,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:44,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:44,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:44,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:45,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:45,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:46,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:46,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:46,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:47,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:47,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:47,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:48,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:48,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:48,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:49,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:49,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:49,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:50,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:50,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:51,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:51,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:51,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:52,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:52,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:52,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:53,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:53,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:53,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:54,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:54,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:54,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:55,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:55,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:55,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:55,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:56,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:56,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:56,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:57,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:57,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:58,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:58,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:58,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:59,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:59,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:59,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:00,475][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2268, device='cuda:0') eval_epoch_loss=tensor(0.2044, device='cuda:0') eval_epoch_acc=tensor(0.9521, device='cuda:0')
[2025-02-13 21:20:00,477][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 21:20:00,477][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 21:20:00,714][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_7130_loss_0.20444858074188232/model.pt
[2025-02-13 21:20:00,718][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 21:20:00,718][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.20444858074188232
[2025-02-13 21:20:00,718][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9520611763000488
[2025-02-13 21:20:00,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:01,133][root][INFO] - Training Epoch: 2/2, step 7130/7134 completed (loss: 0.26827287673950195, acc: 0.9200000166893005)
[2025-02-13 21:20:01,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:01,478][root][INFO] - Training Epoch: 2/2, step 7131/7134 completed (loss: 0.04423867538571358, acc: 0.9905660152435303)
[2025-02-13 21:20:01,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:01,859][root][INFO] - Training Epoch: 2/2, step 7132/7134 completed (loss: 0.017424480989575386, acc: 1.0)
[2025-02-13 21:20:01,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:02,217][root][INFO] - Training Epoch: 2/2, step 7133/7134 completed (loss: 0.1254364401102066, acc: 0.9849624037742615)
[2025-02-13 21:20:02,536][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.1078, train_epoch_loss=0.1024, epoch time 3612.5791520690545s
[2025-02-13 21:20:02,536][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-02-13 21:20:02,536][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 21 GB
[2025-02-13 21:20:02,537][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-02-13 21:20:02,537][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2025-02-13 21:20:02,537][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 8 GB
[2025-02-13 21:20:02,540][root][INFO] - Key: avg_train_prep, Value: 1.2068006992340088
[2025-02-13 21:20:02,541][root][INFO] - Key: avg_train_loss, Value: 0.1845998615026474
[2025-02-13 21:20:02,542][root][INFO] - Key: avg_train_acc, Value: 0.956378698348999
[2025-02-13 21:20:02,542][root][INFO] - Key: avg_eval_prep, Value: 1.2677905559539795
[2025-02-13 21:20:02,542][root][INFO] - Key: avg_eval_loss, Value: 0.23653744161128998
[2025-02-13 21:20:02,542][root][INFO] - Key: avg_eval_acc, Value: 0.9437452554702759
[2025-02-13 21:20:02,542][root][INFO] - Key: avg_epoch_time, Value: 3634.2287681815214
[2025-02-13 21:20:02,542][root][INFO] - Key: avg_checkpoint_time, Value: 0.2781551438383758
