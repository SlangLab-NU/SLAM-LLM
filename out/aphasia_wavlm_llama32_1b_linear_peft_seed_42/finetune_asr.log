[2025-02-16 13:51:59,660][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_seed_42', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False, 'test_flag': True}
[2025-02-16 13:51:59,661][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-16 13:51:59,661][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_wavlm_llama32_1b_linear_peft_seed_42'}
[2025-02-16 13:51:59,661][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'aphasia_wavlm_llama32_1b_linear_peft_seed_42', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-16_13-51-59.txt', 'log_interval': 5}
[2025-02-16 13:52:21,074][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-16 13:52:26,780][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-16 13:52:26,783][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-16 13:52:26,785][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-16 13:52:26,786][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-16 13:52:34,710][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-16 13:52:34,712][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-16 13:52:34,712][slam_llm.models.slam_model][INFO] - setup peft...
[2025-02-16 13:52:34,837][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-16 13:52:34,839][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-16 13:52:34,952][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-16 13:52:34,953][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-16 13:52:34,953][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-16 13:52:34,957][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-16 13:52:37,069][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-16 13:52:38,904][root][INFO] - --> Training Set Length = 95353
[2025-02-16 13:52:39,025][root][INFO] - --> Validation Set Length = 13162
[2025-02-16 13:52:39,025][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-16 13:52:39,026][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-16 13:52:41,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:42,549][root][INFO] - Training Epoch: 1/2, step 0/23838 completed (loss: 8.923311233520508, acc: 0.0)
[2025-02-16 13:52:42,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:42,935][root][INFO] - Training Epoch: 1/2, step 1/23838 completed (loss: 8.48862361907959, acc: 0.0833333358168602)
[2025-02-16 13:52:43,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:43,352][root][INFO] - Training Epoch: 1/2, step 2/23838 completed (loss: 7.502595901489258, acc: 0.07999999821186066)
[2025-02-16 13:52:43,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:43,893][root][INFO] - Training Epoch: 1/2, step 3/23838 completed (loss: 8.70085334777832, acc: 0.11764705926179886)
[2025-02-16 13:52:44,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:44,582][root][INFO] - Training Epoch: 1/2, step 4/23838 completed (loss: 8.335714340209961, acc: 0.0555555559694767)
[2025-02-16 13:52:44,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:45,062][root][INFO] - Training Epoch: 1/2, step 5/23838 completed (loss: 7.924537658691406, acc: 0.1818181872367859)
[2025-02-16 13:52:45,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:45,471][root][INFO] - Training Epoch: 1/2, step 6/23838 completed (loss: 8.028307914733887, acc: 0.0)
[2025-02-16 13:52:45,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:46,101][root][INFO] - Training Epoch: 1/2, step 7/23838 completed (loss: 7.8011980056762695, acc: 0.023255813866853714)
[2025-02-16 13:52:46,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:46,549][root][INFO] - Training Epoch: 1/2, step 8/23838 completed (loss: 7.45087194442749, acc: 0.1538461595773697)
[2025-02-16 13:52:46,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:46,927][root][INFO] - Training Epoch: 1/2, step 9/23838 completed (loss: 8.136008262634277, acc: 0.0476190485060215)
[2025-02-16 13:52:47,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:47,310][root][INFO] - Training Epoch: 1/2, step 10/23838 completed (loss: 8.952075958251953, acc: 0.1111111119389534)
[2025-02-16 13:52:47,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:47,686][root][INFO] - Training Epoch: 1/2, step 11/23838 completed (loss: 9.489426612854004, acc: 0.0476190485060215)
[2025-02-16 13:52:47,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:48,072][root][INFO] - Training Epoch: 1/2, step 12/23838 completed (loss: 8.788180351257324, acc: 0.0714285746216774)
[2025-02-16 13:52:48,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:48,458][root][INFO] - Training Epoch: 1/2, step 13/23838 completed (loss: 9.255790710449219, acc: 0.0833333358168602)
[2025-02-16 13:52:48,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:48,853][root][INFO] - Training Epoch: 1/2, step 14/23838 completed (loss: 8.874316215515137, acc: 0.0416666679084301)
[2025-02-16 13:52:49,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:49,393][root][INFO] - Training Epoch: 1/2, step 15/23838 completed (loss: 7.721263408660889, acc: 0.06060606241226196)
[2025-02-16 13:52:49,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:49,805][root][INFO] - Training Epoch: 1/2, step 16/23838 completed (loss: 9.519225120544434, acc: 0.0)
[2025-02-16 13:52:49,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:50,149][root][INFO] - Training Epoch: 1/2, step 17/23838 completed (loss: 6.740650177001953, acc: 0.1875)
[2025-02-16 13:52:50,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:50,582][root][INFO] - Training Epoch: 1/2, step 18/23838 completed (loss: 9.01861572265625, acc: 0.0)
[2025-02-16 13:52:50,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:51,042][root][INFO] - Training Epoch: 1/2, step 19/23838 completed (loss: 8.111373901367188, acc: 0.05000000074505806)
[2025-02-16 13:52:51,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:51,424][root][INFO] - Training Epoch: 1/2, step 20/23838 completed (loss: 7.566081523895264, acc: 0.1111111119389534)
[2025-02-16 13:52:51,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:51,798][root][INFO] - Training Epoch: 1/2, step 21/23838 completed (loss: 7.859306812286377, acc: 0.095238097012043)
[2025-02-16 13:52:51,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:52,132][root][INFO] - Training Epoch: 1/2, step 22/23838 completed (loss: 8.558947563171387, acc: 0.05263157933950424)
[2025-02-16 13:52:52,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:52,530][root][INFO] - Training Epoch: 1/2, step 23/23838 completed (loss: 7.295321941375732, acc: 0.125)
[2025-02-16 13:52:52,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:52,984][root][INFO] - Training Epoch: 1/2, step 24/23838 completed (loss: 7.92502498626709, acc: 0.1034482792019844)
[2025-02-16 13:52:53,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:53,369][root][INFO] - Training Epoch: 1/2, step 25/23838 completed (loss: 8.120734214782715, acc: 0.03703703731298447)
[2025-02-16 13:52:53,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:53,808][root][INFO] - Training Epoch: 1/2, step 26/23838 completed (loss: 6.515314102172852, acc: 0.13333334028720856)
[2025-02-16 13:52:53,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:54,189][root][INFO] - Training Epoch: 1/2, step 27/23838 completed (loss: 7.296704292297363, acc: 0.11999999731779099)
[2025-02-16 13:52:54,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:54,584][root][INFO] - Training Epoch: 1/2, step 28/23838 completed (loss: 7.63629150390625, acc: 0.043478261679410934)
[2025-02-16 13:52:54,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:54,991][root][INFO] - Training Epoch: 1/2, step 29/23838 completed (loss: 6.99611759185791, acc: 0.1111111119389534)
[2025-02-16 13:52:55,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:55,440][root][INFO] - Training Epoch: 1/2, step 30/23838 completed (loss: 9.68387222290039, acc: 0.0)
[2025-02-16 13:52:55,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:55,840][root][INFO] - Training Epoch: 1/2, step 31/23838 completed (loss: 8.90470027923584, acc: 0.1666666716337204)
[2025-02-16 13:52:55,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:56,189][root][INFO] - Training Epoch: 1/2, step 32/23838 completed (loss: 10.153646469116211, acc: 0.0)
[2025-02-16 13:52:56,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:56,532][root][INFO] - Training Epoch: 1/2, step 33/23838 completed (loss: 8.327704429626465, acc: 0.09090909361839294)
[2025-02-16 13:52:56,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:56,935][root][INFO] - Training Epoch: 1/2, step 34/23838 completed (loss: 9.170791625976562, acc: 0.1666666716337204)
[2025-02-16 13:52:57,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:57,285][root][INFO] - Training Epoch: 1/2, step 35/23838 completed (loss: 11.322883605957031, acc: 0.0)
[2025-02-16 13:52:57,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:57,630][root][INFO] - Training Epoch: 1/2, step 36/23838 completed (loss: 11.839996337890625, acc: 0.0)
[2025-02-16 13:52:57,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:58,251][root][INFO] - Training Epoch: 1/2, step 37/23838 completed (loss: 10.682265281677246, acc: 0.0)
[2025-02-16 13:52:58,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:58,867][root][INFO] - Training Epoch: 1/2, step 38/23838 completed (loss: 9.855891227722168, acc: 0.05263157933950424)
[2025-02-16 13:52:59,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:59,255][root][INFO] - Training Epoch: 1/2, step 39/23838 completed (loss: 8.393941879272461, acc: 0.0)
[2025-02-16 13:52:59,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:59,639][root][INFO] - Training Epoch: 1/2, step 40/23838 completed (loss: 8.151581764221191, acc: 0.0)
[2025-02-16 13:52:59,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:52:59,984][root][INFO] - Training Epoch: 1/2, step 41/23838 completed (loss: 8.092008590698242, acc: 0.06666667014360428)
[2025-02-16 13:53:00,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:00,338][root][INFO] - Training Epoch: 1/2, step 42/23838 completed (loss: 9.400967597961426, acc: 0.0)
[2025-02-16 13:53:00,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:00,742][root][INFO] - Training Epoch: 1/2, step 43/23838 completed (loss: 7.951184272766113, acc: 0.0)
[2025-02-16 13:53:00,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:01,091][root][INFO] - Training Epoch: 1/2, step 44/23838 completed (loss: 7.558956146240234, acc: 0.11764705926179886)
[2025-02-16 13:53:01,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:01,417][root][INFO] - Training Epoch: 1/2, step 45/23838 completed (loss: 7.2202301025390625, acc: 0.043478261679410934)
[2025-02-16 13:53:01,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:01,828][root][INFO] - Training Epoch: 1/2, step 46/23838 completed (loss: 7.3099541664123535, acc: 0.07692307978868484)
[2025-02-16 13:53:01,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:02,179][root][INFO] - Training Epoch: 1/2, step 47/23838 completed (loss: 8.823959350585938, acc: 0.0)
[2025-02-16 13:53:02,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:02,523][root][INFO] - Training Epoch: 1/2, step 48/23838 completed (loss: 5.571019172668457, acc: 0.1666666716337204)
[2025-02-16 13:53:02,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:02,953][root][INFO] - Training Epoch: 1/2, step 49/23838 completed (loss: 5.069390296936035, acc: 0.18918919563293457)
[2025-02-16 13:53:03,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:03,325][root][INFO] - Training Epoch: 1/2, step 50/23838 completed (loss: 6.904924392700195, acc: 0.03333333507180214)
[2025-02-16 13:53:03,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:03,740][root][INFO] - Training Epoch: 1/2, step 51/23838 completed (loss: 5.522862911224365, acc: 0.21276596188545227)
[2025-02-16 13:53:03,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:04,088][root][INFO] - Training Epoch: 1/2, step 52/23838 completed (loss: 5.592851161956787, acc: 0.1071428582072258)
[2025-02-16 13:53:04,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:04,433][root][INFO] - Training Epoch: 1/2, step 53/23838 completed (loss: 5.3516621589660645, acc: 0.25)
[2025-02-16 13:53:04,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:04,789][root][INFO] - Training Epoch: 1/2, step 54/23838 completed (loss: 5.246011257171631, acc: 0.16129031777381897)
[2025-02-16 13:53:04,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:05,110][root][INFO] - Training Epoch: 1/2, step 55/23838 completed (loss: 5.1982293128967285, acc: 0.1666666716337204)
[2025-02-16 13:53:05,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:05,476][root][INFO] - Training Epoch: 1/2, step 56/23838 completed (loss: 5.199990272521973, acc: 0.20000000298023224)
[2025-02-16 13:53:05,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:05,858][root][INFO] - Training Epoch: 1/2, step 57/23838 completed (loss: 5.580003261566162, acc: 0.07500000298023224)
[2025-02-16 13:53:05,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:06,241][root][INFO] - Training Epoch: 1/2, step 58/23838 completed (loss: 5.160689830780029, acc: 0.16129031777381897)
[2025-02-16 13:53:06,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:06,596][root][INFO] - Training Epoch: 1/2, step 59/23838 completed (loss: 5.90471076965332, acc: 0.06451612710952759)
[2025-02-16 13:53:06,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:07,126][root][INFO] - Training Epoch: 1/2, step 60/23838 completed (loss: 5.187988758087158, acc: 0.1538461595773697)
[2025-02-16 13:53:07,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:07,539][root][INFO] - Training Epoch: 1/2, step 61/23838 completed (loss: 5.467707633972168, acc: 0.11764705926179886)
[2025-02-16 13:53:07,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:08,026][root][INFO] - Training Epoch: 1/2, step 62/23838 completed (loss: 6.1372222900390625, acc: 0.10000000149011612)
[2025-02-16 13:53:08,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:08,437][root][INFO] - Training Epoch: 1/2, step 63/23838 completed (loss: 5.514958381652832, acc: 0.07500000298023224)
[2025-02-16 13:53:08,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:08,780][root][INFO] - Training Epoch: 1/2, step 64/23838 completed (loss: 5.179156303405762, acc: 0.03448275849223137)
[2025-02-16 13:53:08,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:09,145][root][INFO] - Training Epoch: 1/2, step 65/23838 completed (loss: 5.5718793869018555, acc: 0.1428571492433548)
[2025-02-16 13:53:09,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:09,529][root][INFO] - Training Epoch: 1/2, step 66/23838 completed (loss: 4.6021928787231445, acc: 0.20000000298023224)
[2025-02-16 13:53:09,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:09,852][root][INFO] - Training Epoch: 1/2, step 67/23838 completed (loss: 4.629431247711182, acc: 0.14705882966518402)
[2025-02-16 13:53:09,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:10,230][root][INFO] - Training Epoch: 1/2, step 68/23838 completed (loss: 4.271914958953857, acc: 0.1666666716337204)
[2025-02-16 13:53:10,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:10,592][root][INFO] - Training Epoch: 1/2, step 69/23838 completed (loss: 5.817154407501221, acc: 0.1818181872367859)
[2025-02-16 13:53:10,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:10,988][root][INFO] - Training Epoch: 1/2, step 70/23838 completed (loss: 6.152190208435059, acc: 0.095238097012043)
[2025-02-16 13:53:11,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:11,356][root][INFO] - Training Epoch: 1/2, step 71/23838 completed (loss: 5.849911689758301, acc: 0.20000000298023224)
[2025-02-16 13:53:11,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:11,819][root][INFO] - Training Epoch: 1/2, step 72/23838 completed (loss: 6.726399898529053, acc: 0.0833333358168602)
[2025-02-16 13:53:11,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:12,224][root][INFO] - Training Epoch: 1/2, step 73/23838 completed (loss: 5.771522045135498, acc: 0.10000000149011612)
[2025-02-16 13:53:12,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:12,600][root][INFO] - Training Epoch: 1/2, step 74/23838 completed (loss: 6.082006454467773, acc: 0.0)
[2025-02-16 13:53:12,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:13,004][root][INFO] - Training Epoch: 1/2, step 75/23838 completed (loss: 5.722850799560547, acc: 0.0833333358168602)
[2025-02-16 13:53:13,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:13,387][root][INFO] - Training Epoch: 1/2, step 76/23838 completed (loss: 4.920145511627197, acc: 0.1818181872367859)
[2025-02-16 13:53:13,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:13,837][root][INFO] - Training Epoch: 1/2, step 77/23838 completed (loss: 6.13737678527832, acc: 0.10526315867900848)
[2025-02-16 13:53:13,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:14,221][root][INFO] - Training Epoch: 1/2, step 78/23838 completed (loss: 6.4289398193359375, acc: 0.0)
[2025-02-16 13:53:14,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:14,746][root][INFO] - Training Epoch: 1/2, step 79/23838 completed (loss: 4.592864513397217, acc: 0.0833333358168602)
[2025-02-16 13:53:14,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:15,092][root][INFO] - Training Epoch: 1/2, step 80/23838 completed (loss: 4.742562770843506, acc: 0.3333333432674408)
[2025-02-16 13:53:15,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:15,423][root][INFO] - Training Epoch: 1/2, step 81/23838 completed (loss: 4.945333957672119, acc: 0.15789473056793213)
[2025-02-16 13:53:15,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:15,815][root][INFO] - Training Epoch: 1/2, step 82/23838 completed (loss: 5.470879554748535, acc: 0.0714285746216774)
[2025-02-16 13:53:15,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:16,184][root][INFO] - Training Epoch: 1/2, step 83/23838 completed (loss: 6.594515800476074, acc: 0.0625)
[2025-02-16 13:53:16,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:16,565][root][INFO] - Training Epoch: 1/2, step 84/23838 completed (loss: 3.67364239692688, acc: 0.2380952388048172)
[2025-02-16 13:53:16,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:17,005][root][INFO] - Training Epoch: 1/2, step 85/23838 completed (loss: 5.0183281898498535, acc: 0.1875)
[2025-02-16 13:53:17,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:17,430][root][INFO] - Training Epoch: 1/2, step 86/23838 completed (loss: 3.9666574001312256, acc: 0.2666666805744171)
[2025-02-16 13:53:17,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:17,871][root][INFO] - Training Epoch: 1/2, step 87/23838 completed (loss: 4.885735988616943, acc: 0.2083333283662796)
[2025-02-16 13:53:17,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:18,239][root][INFO] - Training Epoch: 1/2, step 88/23838 completed (loss: 3.4577157497406006, acc: 0.29411765933036804)
[2025-02-16 13:53:18,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:18,681][root][INFO] - Training Epoch: 1/2, step 89/23838 completed (loss: 5.123838424682617, acc: 0.25925925374031067)
[2025-02-16 13:53:18,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:19,131][root][INFO] - Training Epoch: 1/2, step 90/23838 completed (loss: 5.175074577331543, acc: 0.27272728085517883)
[2025-02-16 13:53:19,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:19,540][root][INFO] - Training Epoch: 1/2, step 91/23838 completed (loss: 3.3499817848205566, acc: 0.28125)
[2025-02-16 13:53:19,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:19,887][root][INFO] - Training Epoch: 1/2, step 92/23838 completed (loss: 3.6607563495635986, acc: 0.2857142984867096)
[2025-02-16 13:53:20,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:20,231][root][INFO] - Training Epoch: 1/2, step 93/23838 completed (loss: 3.6572256088256836, acc: 0.24242424964904785)
[2025-02-16 13:53:20,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:20,569][root][INFO] - Training Epoch: 1/2, step 94/23838 completed (loss: 3.779590129852295, acc: 0.3076923191547394)
[2025-02-16 13:53:20,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:21,022][root][INFO] - Training Epoch: 1/2, step 95/23838 completed (loss: 2.9231176376342773, acc: 0.5)
[2025-02-16 13:53:21,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:21,407][root][INFO] - Training Epoch: 1/2, step 96/23838 completed (loss: 1.2719773054122925, acc: 0.7142857313156128)
[2025-02-16 13:53:21,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:21,766][root][INFO] - Training Epoch: 1/2, step 97/23838 completed (loss: 2.009160041809082, acc: 0.5454545617103577)
[2025-02-16 13:53:21,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:22,110][root][INFO] - Training Epoch: 1/2, step 98/23838 completed (loss: 4.339921474456787, acc: 0.2142857164144516)
[2025-02-16 13:53:22,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:22,456][root][INFO] - Training Epoch: 1/2, step 99/23838 completed (loss: 1.9428070783615112, acc: 0.6666666865348816)
[2025-02-16 13:53:22,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:22,794][root][INFO] - Training Epoch: 1/2, step 100/23838 completed (loss: 3.370917797088623, acc: 0.5)
[2025-02-16 13:53:22,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:23,165][root][INFO] - Training Epoch: 1/2, step 101/23838 completed (loss: 3.5399715900421143, acc: 0.3333333432674408)
[2025-02-16 13:53:23,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:23,530][root][INFO] - Training Epoch: 1/2, step 102/23838 completed (loss: 3.1988279819488525, acc: 0.3636363744735718)
[2025-02-16 13:53:23,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:23,885][root][INFO] - Training Epoch: 1/2, step 103/23838 completed (loss: 2.9979734420776367, acc: 0.3333333432674408)
[2025-02-16 13:53:24,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:24,254][root][INFO] - Training Epoch: 1/2, step 104/23838 completed (loss: 2.007812023162842, acc: 0.5)
[2025-02-16 13:53:24,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:24,614][root][INFO] - Training Epoch: 1/2, step 105/23838 completed (loss: 2.8651063442230225, acc: 0.2857142984867096)
[2025-02-16 13:53:24,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:25,006][root][INFO] - Training Epoch: 1/2, step 106/23838 completed (loss: 2.4760758876800537, acc: 0.5384615659713745)
[2025-02-16 13:53:25,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:25,394][root][INFO] - Training Epoch: 1/2, step 107/23838 completed (loss: 1.0117124319076538, acc: 0.75)
[2025-02-16 13:53:25,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:25,721][root][INFO] - Training Epoch: 1/2, step 108/23838 completed (loss: 1.8807014226913452, acc: 0.46666666865348816)
[2025-02-16 13:53:25,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:26,034][root][INFO] - Training Epoch: 1/2, step 109/23838 completed (loss: 2.3768012523651123, acc: 0.4000000059604645)
[2025-02-16 13:53:26,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:26,416][root][INFO] - Training Epoch: 1/2, step 110/23838 completed (loss: 2.891801595687866, acc: 0.4000000059604645)
[2025-02-16 13:53:26,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:26,851][root][INFO] - Training Epoch: 1/2, step 111/23838 completed (loss: 2.8329899311065674, acc: 0.2857142984867096)
[2025-02-16 13:53:26,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:27,212][root][INFO] - Training Epoch: 1/2, step 112/23838 completed (loss: 2.091808557510376, acc: 0.3571428656578064)
[2025-02-16 13:53:27,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:27,568][root][INFO] - Training Epoch: 1/2, step 113/23838 completed (loss: 2.274719476699829, acc: 0.6363636255264282)
[2025-02-16 13:53:27,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:27,928][root][INFO] - Training Epoch: 1/2, step 114/23838 completed (loss: 2.8848941326141357, acc: 0.4285714328289032)
[2025-02-16 13:53:28,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:28,265][root][INFO] - Training Epoch: 1/2, step 115/23838 completed (loss: 5.989239692687988, acc: 0.15000000596046448)
[2025-02-16 13:53:28,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:28,628][root][INFO] - Training Epoch: 1/2, step 116/23838 completed (loss: 5.052674770355225, acc: 0.17391304671764374)
[2025-02-16 13:53:28,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:29,022][root][INFO] - Training Epoch: 1/2, step 117/23838 completed (loss: 5.269984722137451, acc: 0.2777777910232544)
[2025-02-16 13:53:29,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:29,420][root][INFO] - Training Epoch: 1/2, step 118/23838 completed (loss: 4.895733833312988, acc: 0.2142857164144516)
[2025-02-16 13:53:29,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:29,785][root][INFO] - Training Epoch: 1/2, step 119/23838 completed (loss: 5.803066730499268, acc: 0.10000000149011612)
[2025-02-16 13:53:29,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:30,132][root][INFO] - Training Epoch: 1/2, step 120/23838 completed (loss: 5.318136692047119, acc: 0.25)
[2025-02-16 13:53:30,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:30,536][root][INFO] - Training Epoch: 1/2, step 121/23838 completed (loss: 4.894484996795654, acc: 0.1538461595773697)
[2025-02-16 13:53:30,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:30,916][root][INFO] - Training Epoch: 1/2, step 122/23838 completed (loss: 4.635318756103516, acc: 0.25)
[2025-02-16 13:53:30,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:31,226][root][INFO] - Training Epoch: 1/2, step 123/23838 completed (loss: 3.0942306518554688, acc: 0.4285714328289032)
[2025-02-16 13:53:31,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:31,600][root][INFO] - Training Epoch: 1/2, step 124/23838 completed (loss: 5.1097493171691895, acc: 0.23076923191547394)
[2025-02-16 13:53:31,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:31,941][root][INFO] - Training Epoch: 1/2, step 125/23838 completed (loss: 5.114649772644043, acc: 0.19230769574642181)
[2025-02-16 13:53:32,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:32,280][root][INFO] - Training Epoch: 1/2, step 126/23838 completed (loss: 5.9819464683532715, acc: 0.2380952388048172)
[2025-02-16 13:53:32,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:32,628][root][INFO] - Training Epoch: 1/2, step 127/23838 completed (loss: 4.448441028594971, acc: 0.23999999463558197)
[2025-02-16 13:53:32,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:33,013][root][INFO] - Training Epoch: 1/2, step 128/23838 completed (loss: 5.605332851409912, acc: 0.11999999731779099)
[2025-02-16 13:53:33,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:33,348][root][INFO] - Training Epoch: 1/2, step 129/23838 completed (loss: 4.051798343658447, acc: 0.3529411852359772)
[2025-02-16 13:53:33,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:33,695][root][INFO] - Training Epoch: 1/2, step 130/23838 completed (loss: 4.40677547454834, acc: 0.3333333432674408)
[2025-02-16 13:53:33,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:34,091][root][INFO] - Training Epoch: 1/2, step 131/23838 completed (loss: 5.394349575042725, acc: 0.23333333432674408)
[2025-02-16 13:53:34,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:34,414][root][INFO] - Training Epoch: 1/2, step 132/23838 completed (loss: 4.299905300140381, acc: 0.38461539149284363)
[2025-02-16 13:53:34,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:34,809][root][INFO] - Training Epoch: 1/2, step 133/23838 completed (loss: 5.5259504318237305, acc: 0.1388888955116272)
[2025-02-16 13:53:34,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:35,133][root][INFO] - Training Epoch: 1/2, step 134/23838 completed (loss: 5.059266090393066, acc: 0.23529411852359772)
[2025-02-16 13:53:35,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:35,466][root][INFO] - Training Epoch: 1/2, step 135/23838 completed (loss: 3.5290679931640625, acc: 0.375)
[2025-02-16 13:53:35,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:35,892][root][INFO] - Training Epoch: 1/2, step 136/23838 completed (loss: 4.440161228179932, acc: 0.2916666567325592)
[2025-02-16 13:53:36,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:36,242][root][INFO] - Training Epoch: 1/2, step 137/23838 completed (loss: 4.318698883056641, acc: 0.190476194024086)
[2025-02-16 13:53:36,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:36,587][root][INFO] - Training Epoch: 1/2, step 138/23838 completed (loss: 4.126316547393799, acc: 0.4615384638309479)
[2025-02-16 13:53:36,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:36,942][root][INFO] - Training Epoch: 1/2, step 139/23838 completed (loss: 4.224960803985596, acc: 0.3181818127632141)
[2025-02-16 13:53:37,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:37,282][root][INFO] - Training Epoch: 1/2, step 140/23838 completed (loss: 4.929669380187988, acc: 0.25)
[2025-02-16 13:53:37,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:37,620][root][INFO] - Training Epoch: 1/2, step 141/23838 completed (loss: 3.6989457607269287, acc: 0.4166666567325592)
[2025-02-16 13:53:37,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:37,948][root][INFO] - Training Epoch: 1/2, step 142/23838 completed (loss: 4.096940517425537, acc: 0.2631579041481018)
[2025-02-16 13:53:38,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:38,253][root][INFO] - Training Epoch: 1/2, step 143/23838 completed (loss: 3.736907958984375, acc: 0.2380952388048172)
[2025-02-16 13:53:38,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:38,558][root][INFO] - Training Epoch: 1/2, step 144/23838 completed (loss: 3.385460615158081, acc: 0.4166666567325592)
[2025-02-16 13:53:38,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:38,907][root][INFO] - Training Epoch: 1/2, step 145/23838 completed (loss: 4.098265647888184, acc: 0.29032257199287415)
[2025-02-16 13:53:39,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:39,243][root][INFO] - Training Epoch: 1/2, step 146/23838 completed (loss: 4.433913707733154, acc: 0.25)
[2025-02-16 13:53:39,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:39,661][root][INFO] - Training Epoch: 1/2, step 147/23838 completed (loss: 6.320301532745361, acc: 0.1818181872367859)
[2025-02-16 13:53:39,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:39,946][root][INFO] - Training Epoch: 1/2, step 148/23838 completed (loss: 3.3584606647491455, acc: 0.3888888955116272)
[2025-02-16 13:53:40,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:40,266][root][INFO] - Training Epoch: 1/2, step 149/23838 completed (loss: 2.389202833175659, acc: 0.4000000059604645)
[2025-02-16 13:53:40,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:40,648][root][INFO] - Training Epoch: 1/2, step 150/23838 completed (loss: 4.646154403686523, acc: 0.40909090638160706)
[2025-02-16 13:53:40,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:40,989][root][INFO] - Training Epoch: 1/2, step 151/23838 completed (loss: 4.064942836761475, acc: 0.2916666567325592)
[2025-02-16 13:53:41,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:41,367][root][INFO] - Training Epoch: 1/2, step 152/23838 completed (loss: 3.849200963973999, acc: 0.4000000059604645)
[2025-02-16 13:53:41,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:41,714][root][INFO] - Training Epoch: 1/2, step 153/23838 completed (loss: 3.9432120323181152, acc: 0.2631579041481018)
[2025-02-16 13:53:41,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:42,071][root][INFO] - Training Epoch: 1/2, step 154/23838 completed (loss: 4.177474021911621, acc: 0.30000001192092896)
[2025-02-16 13:53:42,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:42,498][root][INFO] - Training Epoch: 1/2, step 155/23838 completed (loss: 4.156652927398682, acc: 0.37037035822868347)
[2025-02-16 13:53:42,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:42,930][root][INFO] - Training Epoch: 1/2, step 156/23838 completed (loss: 4.630622863769531, acc: 0.23529411852359772)
[2025-02-16 13:53:43,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:43,300][root][INFO] - Training Epoch: 1/2, step 157/23838 completed (loss: 3.622182607650757, acc: 0.3571428656578064)
[2025-02-16 13:53:43,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:43,622][root][INFO] - Training Epoch: 1/2, step 158/23838 completed (loss: 3.2815139293670654, acc: 0.5)
[2025-02-16 13:53:43,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:44,014][root][INFO] - Training Epoch: 1/2, step 159/23838 completed (loss: 5.086567401885986, acc: 0.27272728085517883)
[2025-02-16 13:53:44,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:44,450][root][INFO] - Training Epoch: 1/2, step 160/23838 completed (loss: 3.722999334335327, acc: 0.5)
[2025-02-16 13:53:44,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:44,784][root][INFO] - Training Epoch: 1/2, step 161/23838 completed (loss: 3.905282974243164, acc: 0.4000000059604645)
[2025-02-16 13:53:44,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:45,115][root][INFO] - Training Epoch: 1/2, step 162/23838 completed (loss: 3.2337965965270996, acc: 0.44999998807907104)
[2025-02-16 13:53:45,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:45,455][root][INFO] - Training Epoch: 1/2, step 163/23838 completed (loss: 3.7812459468841553, acc: 0.3913043439388275)
[2025-02-16 13:53:45,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:45,846][root][INFO] - Training Epoch: 1/2, step 164/23838 completed (loss: 4.594496250152588, acc: 0.20000000298023224)
[2025-02-16 13:53:45,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:46,214][root][INFO] - Training Epoch: 1/2, step 165/23838 completed (loss: 4.426572799682617, acc: 0.25)
[2025-02-16 13:53:46,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:46,595][root][INFO] - Training Epoch: 1/2, step 166/23838 completed (loss: 4.521782875061035, acc: 0.22727273404598236)
[2025-02-16 13:53:46,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:46,947][root][INFO] - Training Epoch: 1/2, step 167/23838 completed (loss: 5.1719865798950195, acc: 0.24242424964904785)
[2025-02-16 13:53:47,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:47,282][root][INFO] - Training Epoch: 1/2, step 168/23838 completed (loss: 3.29315447807312, acc: 0.4166666567325592)
[2025-02-16 13:53:47,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:47,616][root][INFO] - Training Epoch: 1/2, step 169/23838 completed (loss: 3.100614070892334, acc: 0.4117647111415863)
[2025-02-16 13:53:47,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:47,948][root][INFO] - Training Epoch: 1/2, step 170/23838 completed (loss: 4.452555179595947, acc: 0.2222222238779068)
[2025-02-16 13:53:48,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:48,280][root][INFO] - Training Epoch: 1/2, step 171/23838 completed (loss: 3.674978256225586, acc: 0.4285714328289032)
[2025-02-16 13:53:48,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:48,584][root][INFO] - Training Epoch: 1/2, step 172/23838 completed (loss: 2.796544313430786, acc: 0.699999988079071)
[2025-02-16 13:53:48,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:48,918][root][INFO] - Training Epoch: 1/2, step 173/23838 completed (loss: 4.724796295166016, acc: 0.30434781312942505)
[2025-02-16 13:53:49,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:49,246][root][INFO] - Training Epoch: 1/2, step 174/23838 completed (loss: 2.9087724685668945, acc: 0.3333333432674408)
[2025-02-16 13:53:49,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:49,573][root][INFO] - Training Epoch: 1/2, step 175/23838 completed (loss: 4.015725135803223, acc: 0.3461538553237915)
[2025-02-16 13:53:49,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:49,911][root][INFO] - Training Epoch: 1/2, step 176/23838 completed (loss: 3.313606023788452, acc: 0.5185185074806213)
[2025-02-16 13:53:50,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:50,251][root][INFO] - Training Epoch: 1/2, step 177/23838 completed (loss: 3.1458983421325684, acc: 0.4545454680919647)
[2025-02-16 13:53:50,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:50,577][root][INFO] - Training Epoch: 1/2, step 178/23838 completed (loss: 3.2981345653533936, acc: 0.38235294818878174)
[2025-02-16 13:53:50,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:50,906][root][INFO] - Training Epoch: 1/2, step 179/23838 completed (loss: 4.023932933807373, acc: 0.4545454680919647)
[2025-02-16 13:53:50,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:51,219][root][INFO] - Training Epoch: 1/2, step 180/23838 completed (loss: 2.4027740955352783, acc: 0.4285714328289032)
[2025-02-16 13:53:51,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:51,584][root][INFO] - Training Epoch: 1/2, step 181/23838 completed (loss: 2.8439152240753174, acc: 0.523809552192688)
[2025-02-16 13:53:51,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:51,939][root][INFO] - Training Epoch: 1/2, step 182/23838 completed (loss: 3.5915002822875977, acc: 0.4615384638309479)
[2025-02-16 13:53:52,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:52,238][root][INFO] - Training Epoch: 1/2, step 183/23838 completed (loss: 3.771794080734253, acc: 0.21739129722118378)
[2025-02-16 13:53:52,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:52,593][root][INFO] - Training Epoch: 1/2, step 184/23838 completed (loss: 4.274748802185059, acc: 0.27586206793785095)
[2025-02-16 13:53:52,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:52,979][root][INFO] - Training Epoch: 1/2, step 185/23838 completed (loss: 3.6805131435394287, acc: 0.2666666805744171)
[2025-02-16 13:53:53,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:53,305][root][INFO] - Training Epoch: 1/2, step 186/23838 completed (loss: 3.9795689582824707, acc: 0.3499999940395355)
[2025-02-16 13:53:53,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:53,625][root][INFO] - Training Epoch: 1/2, step 187/23838 completed (loss: 4.721776485443115, acc: 0.47999998927116394)
[2025-02-16 13:53:53,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:54,013][root][INFO] - Training Epoch: 1/2, step 188/23838 completed (loss: 4.413659572601318, acc: 0.3333333432674408)
[2025-02-16 13:53:54,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:54,354][root][INFO] - Training Epoch: 1/2, step 189/23838 completed (loss: 3.11576771736145, acc: 0.4736842215061188)
[2025-02-16 13:53:54,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:54,803][root][INFO] - Training Epoch: 1/2, step 190/23838 completed (loss: 2.798848867416382, acc: 0.4375)
[2025-02-16 13:53:54,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:55,145][root][INFO] - Training Epoch: 1/2, step 191/23838 completed (loss: 3.3783645629882812, acc: 0.5)
[2025-02-16 13:53:55,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:55,459][root][INFO] - Training Epoch: 1/2, step 192/23838 completed (loss: 2.961019992828369, acc: 0.5384615659713745)
[2025-02-16 13:53:55,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:55,787][root][INFO] - Training Epoch: 1/2, step 193/23838 completed (loss: 3.7804553508758545, acc: 0.4444444477558136)
[2025-02-16 13:53:55,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:56,128][root][INFO] - Training Epoch: 1/2, step 194/23838 completed (loss: 3.189824104309082, acc: 0.5384615659713745)
[2025-02-16 13:53:56,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:56,460][root][INFO] - Training Epoch: 1/2, step 195/23838 completed (loss: 4.14846134185791, acc: 0.24242424964904785)
[2025-02-16 13:53:56,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:56,792][root][INFO] - Training Epoch: 1/2, step 196/23838 completed (loss: 4.715272903442383, acc: 0.3199999928474426)
[2025-02-16 13:53:56,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:57,113][root][INFO] - Training Epoch: 1/2, step 197/23838 completed (loss: 4.262454032897949, acc: 0.3076923191547394)
[2025-02-16 13:53:57,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:57,436][root][INFO] - Training Epoch: 1/2, step 198/23838 completed (loss: 4.735742092132568, acc: 0.5)
[2025-02-16 13:53:57,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:57,732][root][INFO] - Training Epoch: 1/2, step 199/23838 completed (loss: 3.317018985748291, acc: 0.3333333432674408)
[2025-02-16 13:53:57,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:58,048][root][INFO] - Training Epoch: 1/2, step 200/23838 completed (loss: 2.811593770980835, acc: 0.4000000059604645)
[2025-02-16 13:53:58,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:58,395][root][INFO] - Training Epoch: 1/2, step 201/23838 completed (loss: 2.8950042724609375, acc: 0.3076923191547394)
[2025-02-16 13:53:58,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:58,714][root][INFO] - Training Epoch: 1/2, step 202/23838 completed (loss: 4.2801432609558105, acc: 0.3333333432674408)
[2025-02-16 13:53:58,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:59,058][root][INFO] - Training Epoch: 1/2, step 203/23838 completed (loss: 3.149628162384033, acc: 0.4285714328289032)
[2025-02-16 13:53:59,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:59,394][root][INFO] - Training Epoch: 1/2, step 204/23838 completed (loss: 3.7176125049591064, acc: 0.44999998807907104)
[2025-02-16 13:53:59,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:53:59,805][root][INFO] - Training Epoch: 1/2, step 205/23838 completed (loss: 4.1685686111450195, acc: 0.4615384638309479)
[2025-02-16 13:53:59,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:00,153][root][INFO] - Training Epoch: 1/2, step 206/23838 completed (loss: 4.098180294036865, acc: 0.38461539149284363)
[2025-02-16 13:54:00,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:00,508][root][INFO] - Training Epoch: 1/2, step 207/23838 completed (loss: 4.647862911224365, acc: 0.30000001192092896)
[2025-02-16 13:54:00,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:00,873][root][INFO] - Training Epoch: 1/2, step 208/23838 completed (loss: 3.9359734058380127, acc: 0.3125)
[2025-02-16 13:54:01,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:01,236][root][INFO] - Training Epoch: 1/2, step 209/23838 completed (loss: 3.276988983154297, acc: 0.4166666567325592)
[2025-02-16 13:54:01,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:01,575][root][INFO] - Training Epoch: 1/2, step 210/23838 completed (loss: 3.1778218746185303, acc: 0.4166666567325592)
[2025-02-16 13:54:01,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:01,936][root][INFO] - Training Epoch: 1/2, step 211/23838 completed (loss: 3.361795425415039, acc: 0.4000000059604645)
[2025-02-16 13:54:02,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:02,248][root][INFO] - Training Epoch: 1/2, step 212/23838 completed (loss: 3.414369821548462, acc: 0.5)
[2025-02-16 13:54:02,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:02,577][root][INFO] - Training Epoch: 1/2, step 213/23838 completed (loss: 1.743330478668213, acc: 0.5555555820465088)
[2025-02-16 13:54:02,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:02,924][root][INFO] - Training Epoch: 1/2, step 214/23838 completed (loss: 3.1942059993743896, acc: 0.5714285969734192)
[2025-02-16 13:54:03,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:03,357][root][INFO] - Training Epoch: 1/2, step 215/23838 completed (loss: 3.8188436031341553, acc: 0.27272728085517883)
[2025-02-16 13:54:03,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:03,702][root][INFO] - Training Epoch: 1/2, step 216/23838 completed (loss: 2.9860012531280518, acc: 0.5)
[2025-02-16 13:54:03,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:04,088][root][INFO] - Training Epoch: 1/2, step 217/23838 completed (loss: 4.432246685028076, acc: 0.3333333432674408)
[2025-02-16 13:54:04,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:04,433][root][INFO] - Training Epoch: 1/2, step 218/23838 completed (loss: 2.8534648418426514, acc: 0.5714285969734192)
[2025-02-16 13:54:04,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:04,748][root][INFO] - Training Epoch: 1/2, step 219/23838 completed (loss: 3.0990583896636963, acc: 0.47058823704719543)
[2025-02-16 13:54:04,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:05,065][root][INFO] - Training Epoch: 1/2, step 220/23838 completed (loss: 3.413156270980835, acc: 0.5)
[2025-02-16 13:54:05,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:05,397][root][INFO] - Training Epoch: 1/2, step 221/23838 completed (loss: 3.357513666152954, acc: 0.5333333611488342)
[2025-02-16 13:54:05,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:05,748][root][INFO] - Training Epoch: 1/2, step 222/23838 completed (loss: 4.083033084869385, acc: 0.6666666865348816)
[2025-02-16 13:54:05,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:06,119][root][INFO] - Training Epoch: 1/2, step 223/23838 completed (loss: 3.788541793823242, acc: 0.375)
[2025-02-16 13:54:06,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:06,527][root][INFO] - Training Epoch: 1/2, step 224/23838 completed (loss: 4.832406044006348, acc: 0.3125)
[2025-02-16 13:54:06,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:06,896][root][INFO] - Training Epoch: 1/2, step 225/23838 completed (loss: 4.34685754776001, acc: 0.3125)
[2025-02-16 13:54:07,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:07,261][root][INFO] - Training Epoch: 1/2, step 226/23838 completed (loss: 3.310343027114868, acc: 0.5625)
[2025-02-16 13:54:07,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:07,627][root][INFO] - Training Epoch: 1/2, step 227/23838 completed (loss: 2.036187171936035, acc: 0.625)
[2025-02-16 13:54:07,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:08,003][root][INFO] - Training Epoch: 1/2, step 228/23838 completed (loss: 3.3247783184051514, acc: 0.4375)
[2025-02-16 13:54:08,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:08,326][root][INFO] - Training Epoch: 1/2, step 229/23838 completed (loss: 3.4932665824890137, acc: 0.47058823704719543)
[2025-02-16 13:54:08,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:08,654][root][INFO] - Training Epoch: 1/2, step 230/23838 completed (loss: 2.923224687576294, acc: 0.5)
[2025-02-16 13:54:08,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:09,015][root][INFO] - Training Epoch: 1/2, step 231/23838 completed (loss: 5.5265421867370605, acc: 0.3333333432674408)
[2025-02-16 13:54:09,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:09,348][root][INFO] - Training Epoch: 1/2, step 232/23838 completed (loss: 6.9561872482299805, acc: 0.25)
[2025-02-16 13:54:09,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:09,743][root][INFO] - Training Epoch: 1/2, step 233/23838 completed (loss: 6.121603012084961, acc: 0.2222222238779068)
[2025-02-16 13:54:09,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:10,159][root][INFO] - Training Epoch: 1/2, step 234/23838 completed (loss: 5.781729221343994, acc: 0.3125)
[2025-02-16 13:54:10,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:10,532][root][INFO] - Training Epoch: 1/2, step 235/23838 completed (loss: 4.778539180755615, acc: 0.4117647111415863)
[2025-02-16 13:54:10,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:10,898][root][INFO] - Training Epoch: 1/2, step 236/23838 completed (loss: 4.218276023864746, acc: 0.3333333432674408)
[2025-02-16 13:54:11,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:11,229][root][INFO] - Training Epoch: 1/2, step 237/23838 completed (loss: 2.598662853240967, acc: 0.4545454680919647)
[2025-02-16 13:54:11,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:11,568][root][INFO] - Training Epoch: 1/2, step 238/23838 completed (loss: 3.7584474086761475, acc: 0.4444444477558136)
[2025-02-16 13:54:11,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:11,945][root][INFO] - Training Epoch: 1/2, step 239/23838 completed (loss: 3.545081377029419, acc: 0.25)
[2025-02-16 13:54:12,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:12,296][root][INFO] - Training Epoch: 1/2, step 240/23838 completed (loss: 1.4138461351394653, acc: 0.7272727489471436)
[2025-02-16 13:54:12,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:12,643][root][INFO] - Training Epoch: 1/2, step 241/23838 completed (loss: 3.656450033187866, acc: 0.3888888955116272)
[2025-02-16 13:54:12,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:13,150][root][INFO] - Training Epoch: 1/2, step 242/23838 completed (loss: 4.613709449768066, acc: 0.18518517911434174)
[2025-02-16 13:54:13,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:13,472][root][INFO] - Training Epoch: 1/2, step 243/23838 completed (loss: 2.912196159362793, acc: 0.5833333134651184)
[2025-02-16 13:54:13,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:13,831][root][INFO] - Training Epoch: 1/2, step 244/23838 completed (loss: 5.43919038772583, acc: 0.3076923191547394)
[2025-02-16 13:54:13,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:14,163][root][INFO] - Training Epoch: 1/2, step 245/23838 completed (loss: 4.4718804359436035, acc: 0.3333333432674408)
[2025-02-16 13:54:14,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:14,500][root][INFO] - Training Epoch: 1/2, step 246/23838 completed (loss: 3.411738157272339, acc: 0.4615384638309479)
[2025-02-16 13:54:14,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:14,798][root][INFO] - Training Epoch: 1/2, step 247/23838 completed (loss: 3.108954668045044, acc: 0.4166666567325592)
[2025-02-16 13:54:14,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:15,096][root][INFO] - Training Epoch: 1/2, step 248/23838 completed (loss: 3.951545000076294, acc: 0.21052631735801697)
[2025-02-16 13:54:15,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:15,394][root][INFO] - Training Epoch: 1/2, step 249/23838 completed (loss: 4.095262050628662, acc: 0.3529411852359772)
[2025-02-16 13:54:15,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:15,889][root][INFO] - Training Epoch: 1/2, step 250/23838 completed (loss: 4.103115558624268, acc: 0.3076923191547394)
[2025-02-16 13:54:15,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:16,200][root][INFO] - Training Epoch: 1/2, step 251/23838 completed (loss: 2.276211977005005, acc: 0.6875)
[2025-02-16 13:54:16,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:16,519][root][INFO] - Training Epoch: 1/2, step 252/23838 completed (loss: 3.6461069583892822, acc: 0.3125)
[2025-02-16 13:54:16,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:16,847][root][INFO] - Training Epoch: 1/2, step 253/23838 completed (loss: 3.2393569946289062, acc: 0.4166666567325592)
[2025-02-16 13:54:16,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:17,178][root][INFO] - Training Epoch: 1/2, step 254/23838 completed (loss: 3.3507049083709717, acc: 0.4166666567325592)
[2025-02-16 13:54:17,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:17,497][root][INFO] - Training Epoch: 1/2, step 255/23838 completed (loss: 0.9389658570289612, acc: 0.8181818127632141)
[2025-02-16 13:54:17,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:17,815][root][INFO] - Training Epoch: 1/2, step 256/23838 completed (loss: 2.6548993587493896, acc: 0.5714285969734192)
[2025-02-16 13:54:17,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:18,117][root][INFO] - Training Epoch: 1/2, step 257/23838 completed (loss: 2.975353956222534, acc: 0.5555555820465088)
[2025-02-16 13:54:18,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:18,440][root][INFO] - Training Epoch: 1/2, step 258/23838 completed (loss: 3.3060081005096436, acc: 0.3333333432674408)
[2025-02-16 13:54:18,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:18,732][root][INFO] - Training Epoch: 1/2, step 259/23838 completed (loss: 3.5646185874938965, acc: 0.29411765933036804)
[2025-02-16 13:54:18,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:19,101][root][INFO] - Training Epoch: 1/2, step 260/23838 completed (loss: 4.505965709686279, acc: 0.25)
[2025-02-16 13:54:19,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:19,449][root][INFO] - Training Epoch: 1/2, step 261/23838 completed (loss: 4.426278114318848, acc: 0.17241379618644714)
[2025-02-16 13:54:19,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:19,741][root][INFO] - Training Epoch: 1/2, step 262/23838 completed (loss: 3.609127998352051, acc: 0.3333333432674408)
[2025-02-16 13:54:19,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:20,083][root][INFO] - Training Epoch: 1/2, step 263/23838 completed (loss: 4.488139629364014, acc: 0.3636363744735718)
[2025-02-16 13:54:20,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:20,390][root][INFO] - Training Epoch: 1/2, step 264/23838 completed (loss: 5.5087480545043945, acc: 0.1875)
[2025-02-16 13:54:20,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:20,739][root][INFO] - Training Epoch: 1/2, step 265/23838 completed (loss: 3.888770341873169, acc: 0.10810811072587967)
[2025-02-16 13:54:20,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:21,045][root][INFO] - Training Epoch: 1/2, step 266/23838 completed (loss: 3.501457929611206, acc: 0.4736842215061188)
[2025-02-16 13:54:21,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:21,431][root][INFO] - Training Epoch: 1/2, step 267/23838 completed (loss: 3.391155958175659, acc: 0.3333333432674408)
[2025-02-16 13:54:21,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:21,871][root][INFO] - Training Epoch: 1/2, step 268/23838 completed (loss: 3.0202643871307373, acc: 0.3684210479259491)
[2025-02-16 13:54:22,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:22,246][root][INFO] - Training Epoch: 1/2, step 269/23838 completed (loss: 3.802018165588379, acc: 0.2083333283662796)
[2025-02-16 13:54:22,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:22,545][root][INFO] - Training Epoch: 1/2, step 270/23838 completed (loss: 3.9434027671813965, acc: 0.3333333432674408)
[2025-02-16 13:54:22,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:22,932][root][INFO] - Training Epoch: 1/2, step 271/23838 completed (loss: 3.226667881011963, acc: 0.27272728085517883)
[2025-02-16 13:54:23,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:23,256][root][INFO] - Training Epoch: 1/2, step 272/23838 completed (loss: 4.054917335510254, acc: 0.3333333432674408)
[2025-02-16 13:54:23,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:23,578][root][INFO] - Training Epoch: 1/2, step 273/23838 completed (loss: 2.9337856769561768, acc: 0.3333333432674408)
[2025-02-16 13:54:23,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:23,942][root][INFO] - Training Epoch: 1/2, step 274/23838 completed (loss: 3.115900993347168, acc: 0.46666666865348816)
[2025-02-16 13:54:24,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:24,278][root][INFO] - Training Epoch: 1/2, step 275/23838 completed (loss: 3.6655054092407227, acc: 0.3125)
[2025-02-16 13:54:24,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:24,644][root][INFO] - Training Epoch: 1/2, step 276/23838 completed (loss: 2.0920495986938477, acc: 0.7777777910232544)
[2025-02-16 13:54:24,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:25,005][root][INFO] - Training Epoch: 1/2, step 277/23838 completed (loss: 1.4786473512649536, acc: 0.5454545617103577)
[2025-02-16 13:54:25,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:25,340][root][INFO] - Training Epoch: 1/2, step 278/23838 completed (loss: 3.6117401123046875, acc: 0.32499998807907104)
[2025-02-16 13:54:25,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:25,673][root][INFO] - Training Epoch: 1/2, step 279/23838 completed (loss: 3.5206897258758545, acc: 0.32258063554763794)
[2025-02-16 13:54:25,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:26,036][root][INFO] - Training Epoch: 1/2, step 280/23838 completed (loss: 3.343407392501831, acc: 0.5)
[2025-02-16 13:54:26,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:26,420][root][INFO] - Training Epoch: 1/2, step 281/23838 completed (loss: 3.1805267333984375, acc: 0.39024388790130615)
[2025-02-16 13:54:26,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:26,732][root][INFO] - Training Epoch: 1/2, step 282/23838 completed (loss: 2.062103509902954, acc: 0.6842105388641357)
[2025-02-16 13:54:26,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:27,091][root][INFO] - Training Epoch: 1/2, step 283/23838 completed (loss: 2.924814462661743, acc: 0.4545454680919647)
[2025-02-16 13:54:27,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:27,393][root][INFO] - Training Epoch: 1/2, step 284/23838 completed (loss: 3.0942022800445557, acc: 0.5263158082962036)
[2025-02-16 13:54:27,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:27,722][root][INFO] - Training Epoch: 1/2, step 285/23838 completed (loss: 3.938603162765503, acc: 0.39024388790130615)
[2025-02-16 13:54:27,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:28,053][root][INFO] - Training Epoch: 1/2, step 286/23838 completed (loss: 2.6484568119049072, acc: 0.5263158082962036)
[2025-02-16 13:54:28,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:28,372][root][INFO] - Training Epoch: 1/2, step 287/23838 completed (loss: 4.315617561340332, acc: 0.3125)
[2025-02-16 13:54:28,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:28,687][root][INFO] - Training Epoch: 1/2, step 288/23838 completed (loss: 3.5286500453948975, acc: 0.3461538553237915)
[2025-02-16 13:54:28,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:29,036][root][INFO] - Training Epoch: 1/2, step 289/23838 completed (loss: 3.2838664054870605, acc: 0.4000000059604645)
[2025-02-16 13:54:29,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:29,375][root][INFO] - Training Epoch: 1/2, step 290/23838 completed (loss: 4.164699077606201, acc: 0.2222222238779068)
[2025-02-16 13:54:29,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:29,697][root][INFO] - Training Epoch: 1/2, step 291/23838 completed (loss: 3.480670928955078, acc: 0.3235294222831726)
[2025-02-16 13:54:29,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:30,067][root][INFO] - Training Epoch: 1/2, step 292/23838 completed (loss: 4.395181655883789, acc: 0.3199999928474426)
[2025-02-16 13:54:30,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:30,392][root][INFO] - Training Epoch: 1/2, step 293/23838 completed (loss: 2.8771629333496094, acc: 0.5)
[2025-02-16 13:54:30,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:30,721][root][INFO] - Training Epoch: 1/2, step 294/23838 completed (loss: 4.813419342041016, acc: 0.1944444477558136)
[2025-02-16 13:54:30,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:31,051][root][INFO] - Training Epoch: 1/2, step 295/23838 completed (loss: 4.257674217224121, acc: 0.3235294222831726)
[2025-02-16 13:54:31,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:31,380][root][INFO] - Training Epoch: 1/2, step 296/23838 completed (loss: 4.8814311027526855, acc: 0.2142857164144516)
[2025-02-16 13:54:31,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:31,721][root][INFO] - Training Epoch: 1/2, step 297/23838 completed (loss: 5.146162986755371, acc: 0.1764705926179886)
[2025-02-16 13:54:31,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:32,150][root][INFO] - Training Epoch: 1/2, step 298/23838 completed (loss: 4.942291259765625, acc: 0.2432432472705841)
[2025-02-16 13:54:32,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:32,559][root][INFO] - Training Epoch: 1/2, step 299/23838 completed (loss: 4.602280139923096, acc: 0.25531914830207825)
[2025-02-16 13:54:32,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:32,950][root][INFO] - Training Epoch: 1/2, step 300/23838 completed (loss: 4.47103214263916, acc: 0.22499999403953552)
[2025-02-16 13:54:33,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:33,312][root][INFO] - Training Epoch: 1/2, step 301/23838 completed (loss: 3.792017936706543, acc: 0.37037035822868347)
[2025-02-16 13:54:33,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:33,681][root][INFO] - Training Epoch: 1/2, step 302/23838 completed (loss: 4.584309101104736, acc: 0.27272728085517883)
[2025-02-16 13:54:33,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:34,063][root][INFO] - Training Epoch: 1/2, step 303/23838 completed (loss: 4.124290943145752, acc: 0.25925925374031067)
[2025-02-16 13:54:34,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:34,405][root][INFO] - Training Epoch: 1/2, step 304/23838 completed (loss: 3.8119311332702637, acc: 0.25806450843811035)
[2025-02-16 13:54:34,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:34,767][root][INFO] - Training Epoch: 1/2, step 305/23838 completed (loss: 3.0297698974609375, acc: 0.3333333432674408)
[2025-02-16 13:54:34,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:35,076][root][INFO] - Training Epoch: 1/2, step 306/23838 completed (loss: 4.6116156578063965, acc: 0.2666666805744171)
[2025-02-16 13:54:35,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:35,395][root][INFO] - Training Epoch: 1/2, step 307/23838 completed (loss: 3.4163830280303955, acc: 0.3333333432674408)
[2025-02-16 13:54:35,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:35,771][root][INFO] - Training Epoch: 1/2, step 308/23838 completed (loss: 4.287435531616211, acc: 0.21212121844291687)
[2025-02-16 13:54:35,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:36,123][root][INFO] - Training Epoch: 1/2, step 309/23838 completed (loss: 3.4201862812042236, acc: 0.3478260934352875)
[2025-02-16 13:54:36,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:36,522][root][INFO] - Training Epoch: 1/2, step 310/23838 completed (loss: 4.3791632652282715, acc: 0.27272728085517883)
[2025-02-16 13:54:36,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:36,886][root][INFO] - Training Epoch: 1/2, step 311/23838 completed (loss: 5.496823310852051, acc: 0.260869562625885)
[2025-02-16 13:54:37,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:37,254][root][INFO] - Training Epoch: 1/2, step 312/23838 completed (loss: 3.8777284622192383, acc: 0.1666666716337204)
[2025-02-16 13:54:37,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:37,580][root][INFO] - Training Epoch: 1/2, step 313/23838 completed (loss: 2.699317216873169, acc: 0.3636363744735718)
[2025-02-16 13:54:37,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:37,906][root][INFO] - Training Epoch: 1/2, step 314/23838 completed (loss: 4.400789737701416, acc: 0.375)
[2025-02-16 13:54:38,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:38,274][root][INFO] - Training Epoch: 1/2, step 315/23838 completed (loss: 3.7926228046417236, acc: 0.2666666805744171)
[2025-02-16 13:54:38,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:38,644][root][INFO] - Training Epoch: 1/2, step 316/23838 completed (loss: 4.381135940551758, acc: 0.25581395626068115)
[2025-02-16 13:54:38,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:39,011][root][INFO] - Training Epoch: 1/2, step 317/23838 completed (loss: 4.7163872718811035, acc: 0.25925925374031067)
[2025-02-16 13:54:39,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:39,340][root][INFO] - Training Epoch: 1/2, step 318/23838 completed (loss: 3.0822601318359375, acc: 0.4000000059604645)
[2025-02-16 13:54:39,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:39,768][root][INFO] - Training Epoch: 1/2, step 319/23838 completed (loss: 3.692430019378662, acc: 0.30434781312942505)
[2025-02-16 13:54:39,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:40,150][root][INFO] - Training Epoch: 1/2, step 320/23838 completed (loss: 3.6385509967803955, acc: 0.4285714328289032)
[2025-02-16 13:54:40,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:40,494][root][INFO] - Training Epoch: 1/2, step 321/23838 completed (loss: 4.5001912117004395, acc: 0.2083333283662796)
[2025-02-16 13:54:40,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:40,836][root][INFO] - Training Epoch: 1/2, step 322/23838 completed (loss: 3.0510928630828857, acc: 0.2777777910232544)
[2025-02-16 13:54:40,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:41,187][root][INFO] - Training Epoch: 1/2, step 323/23838 completed (loss: 3.4334447383880615, acc: 0.31578946113586426)
[2025-02-16 13:54:41,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:41,569][root][INFO] - Training Epoch: 1/2, step 324/23838 completed (loss: 3.8342862129211426, acc: 0.2222222238779068)
[2025-02-16 13:54:41,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:41,965][root][INFO] - Training Epoch: 1/2, step 325/23838 completed (loss: 3.9124534130096436, acc: 0.1818181872367859)
[2025-02-16 13:54:42,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:42,355][root][INFO] - Training Epoch: 1/2, step 326/23838 completed (loss: 3.1431384086608887, acc: 0.3199999928474426)
[2025-02-16 13:54:42,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:42,756][root][INFO] - Training Epoch: 1/2, step 327/23838 completed (loss: 3.5010359287261963, acc: 0.260869562625885)
[2025-02-16 13:54:42,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:43,100][root][INFO] - Training Epoch: 1/2, step 328/23838 completed (loss: 3.3056156635284424, acc: 0.25)
[2025-02-16 13:54:43,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:43,398][root][INFO] - Training Epoch: 1/2, step 329/23838 completed (loss: 4.681143283843994, acc: 0.25)
[2025-02-16 13:54:43,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:43,760][root][INFO] - Training Epoch: 1/2, step 330/23838 completed (loss: 3.718655824661255, acc: 0.28205129504203796)
[2025-02-16 13:54:43,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:44,097][root][INFO] - Training Epoch: 1/2, step 331/23838 completed (loss: 3.5375137329101562, acc: 0.3333333432674408)
[2025-02-16 13:54:44,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:44,471][root][INFO] - Training Epoch: 1/2, step 332/23838 completed (loss: 4.132389545440674, acc: 0.2068965584039688)
[2025-02-16 13:54:44,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:44,892][root][INFO] - Training Epoch: 1/2, step 333/23838 completed (loss: 4.003295421600342, acc: 0.2068965584039688)
[2025-02-16 13:54:45,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:45,307][root][INFO] - Training Epoch: 1/2, step 334/23838 completed (loss: 4.3339128494262695, acc: 0.3076923191547394)
[2025-02-16 13:54:45,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:45,673][root][INFO] - Training Epoch: 1/2, step 335/23838 completed (loss: 3.284843683242798, acc: 0.4166666567325592)
[2025-02-16 13:54:45,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:46,019][root][INFO] - Training Epoch: 1/2, step 336/23838 completed (loss: 4.321368217468262, acc: 0.27586206793785095)
[2025-02-16 13:54:46,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:46,330][root][INFO] - Training Epoch: 1/2, step 337/23838 completed (loss: 4.486906051635742, acc: 0.23529411852359772)
[2025-02-16 13:54:46,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:46,635][root][INFO] - Training Epoch: 1/2, step 338/23838 completed (loss: 4.283975124359131, acc: 0.26923078298568726)
[2025-02-16 13:54:46,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:46,995][root][INFO] - Training Epoch: 1/2, step 339/23838 completed (loss: 4.482123851776123, acc: 0.19354838132858276)
[2025-02-16 13:54:47,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:47,329][root][INFO] - Training Epoch: 1/2, step 340/23838 completed (loss: 3.3223049640655518, acc: 0.3928571343421936)
[2025-02-16 13:54:47,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:47,658][root][INFO] - Training Epoch: 1/2, step 341/23838 completed (loss: 2.982177734375, acc: 0.4375)
[2025-02-16 13:54:47,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:48,098][root][INFO] - Training Epoch: 1/2, step 342/23838 completed (loss: 4.3394551277160645, acc: 0.21875)
[2025-02-16 13:54:48,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:48,541][root][INFO] - Training Epoch: 1/2, step 343/23838 completed (loss: 4.40635871887207, acc: 0.1860465109348297)
[2025-02-16 13:54:48,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:48,894][root][INFO] - Training Epoch: 1/2, step 344/23838 completed (loss: 3.9345483779907227, acc: 0.2222222238779068)
[2025-02-16 13:54:48,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:49,200][root][INFO] - Training Epoch: 1/2, step 345/23838 completed (loss: 4.808716297149658, acc: 0.1111111119389534)
[2025-02-16 13:54:49,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:49,603][root][INFO] - Training Epoch: 1/2, step 346/23838 completed (loss: 3.928683042526245, acc: 0.19354838132858276)
[2025-02-16 13:54:49,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:49,982][root][INFO] - Training Epoch: 1/2, step 347/23838 completed (loss: 4.401923179626465, acc: 0.2916666567325592)
[2025-02-16 13:54:50,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:50,357][root][INFO] - Training Epoch: 1/2, step 348/23838 completed (loss: 3.3480725288391113, acc: 0.3928571343421936)
[2025-02-16 13:54:50,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:50,696][root][INFO] - Training Epoch: 1/2, step 349/23838 completed (loss: 4.970453262329102, acc: 0.20000000298023224)
[2025-02-16 13:54:50,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:51,020][root][INFO] - Training Epoch: 1/2, step 350/23838 completed (loss: 4.597909927368164, acc: 0.21875)
[2025-02-16 13:54:51,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:51,379][root][INFO] - Training Epoch: 1/2, step 351/23838 completed (loss: 4.024153232574463, acc: 0.3478260934352875)
[2025-02-16 13:54:51,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:51,720][root][INFO] - Training Epoch: 1/2, step 352/23838 completed (loss: 4.57854700088501, acc: 0.2142857164144516)
[2025-02-16 13:54:51,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:52,040][root][INFO] - Training Epoch: 1/2, step 353/23838 completed (loss: 4.515235424041748, acc: 0.17391304671764374)
[2025-02-16 13:54:52,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:52,409][root][INFO] - Training Epoch: 1/2, step 354/23838 completed (loss: 4.118498802185059, acc: 0.3199999928474426)
[2025-02-16 13:54:52,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:52,784][root][INFO] - Training Epoch: 1/2, step 355/23838 completed (loss: 4.107000827789307, acc: 0.24242424964904785)
[2025-02-16 13:54:52,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:53,152][root][INFO] - Training Epoch: 1/2, step 356/23838 completed (loss: 4.054865837097168, acc: 0.3214285671710968)
[2025-02-16 13:54:53,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:53,504][root][INFO] - Training Epoch: 1/2, step 357/23838 completed (loss: 3.7824044227600098, acc: 0.37931033968925476)
[2025-02-16 13:54:53,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:53,852][root][INFO] - Training Epoch: 1/2, step 358/23838 completed (loss: 4.875926971435547, acc: 0.095238097012043)
[2025-02-16 13:54:53,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:54,295][root][INFO] - Training Epoch: 1/2, step 359/23838 completed (loss: 3.6530909538269043, acc: 0.3103448152542114)
[2025-02-16 13:54:54,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:54,668][root][INFO] - Training Epoch: 1/2, step 360/23838 completed (loss: 4.337889671325684, acc: 0.17241379618644714)
[2025-02-16 13:54:54,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:55,000][root][INFO] - Training Epoch: 1/2, step 361/23838 completed (loss: 2.7864322662353516, acc: 0.30434781312942505)
[2025-02-16 13:54:55,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:55,358][root][INFO] - Training Epoch: 1/2, step 362/23838 completed (loss: 2.8042094707489014, acc: 0.4545454680919647)
[2025-02-16 13:54:55,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:55,706][root][INFO] - Training Epoch: 1/2, step 363/23838 completed (loss: 4.793145656585693, acc: 0.1599999964237213)
[2025-02-16 13:54:55,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:56,052][root][INFO] - Training Epoch: 1/2, step 364/23838 completed (loss: 4.293924808502197, acc: 0.2380952388048172)
[2025-02-16 13:54:56,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:56,465][root][INFO] - Training Epoch: 1/2, step 365/23838 completed (loss: 2.90678334236145, acc: 0.375)
[2025-02-16 13:54:56,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:56,817][root][INFO] - Training Epoch: 1/2, step 366/23838 completed (loss: 4.298091411590576, acc: 0.1818181872367859)
[2025-02-16 13:54:56,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:57,180][root][INFO] - Training Epoch: 1/2, step 367/23838 completed (loss: 4.716493606567383, acc: 0.2631579041481018)
[2025-02-16 13:54:57,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:57,536][root][INFO] - Training Epoch: 1/2, step 368/23838 completed (loss: 4.959282875061035, acc: 0.3333333432674408)
[2025-02-16 13:54:57,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:57,871][root][INFO] - Training Epoch: 1/2, step 369/23838 completed (loss: 4.1811347007751465, acc: 0.23999999463558197)
[2025-02-16 13:54:57,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:58,222][root][INFO] - Training Epoch: 1/2, step 370/23838 completed (loss: 3.913881540298462, acc: 0.1666666716337204)
[2025-02-16 13:54:58,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:58,590][root][INFO] - Training Epoch: 1/2, step 371/23838 completed (loss: 4.327743053436279, acc: 0.24242424964904785)
[2025-02-16 13:54:58,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:59,017][root][INFO] - Training Epoch: 1/2, step 372/23838 completed (loss: 3.1777069568634033, acc: 0.37837839126586914)
[2025-02-16 13:54:59,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:59,369][root][INFO] - Training Epoch: 1/2, step 373/23838 completed (loss: 3.5567100048065186, acc: 0.34210526943206787)
[2025-02-16 13:54:59,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:54:59,748][root][INFO] - Training Epoch: 1/2, step 374/23838 completed (loss: 3.9875686168670654, acc: 0.23333333432674408)
[2025-02-16 13:54:59,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:00,100][root][INFO] - Training Epoch: 1/2, step 375/23838 completed (loss: 3.5885555744171143, acc: 0.3611111044883728)
[2025-02-16 13:55:00,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:00,425][root][INFO] - Training Epoch: 1/2, step 376/23838 completed (loss: 3.5559849739074707, acc: 0.30000001192092896)
[2025-02-16 13:55:00,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:00,834][root][INFO] - Training Epoch: 1/2, step 377/23838 completed (loss: 3.5166945457458496, acc: 0.3333333432674408)
[2025-02-16 13:55:01,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:01,316][root][INFO] - Training Epoch: 1/2, step 378/23838 completed (loss: 3.8750882148742676, acc: 0.2950819730758667)
[2025-02-16 13:55:01,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:01,734][root][INFO] - Training Epoch: 1/2, step 379/23838 completed (loss: 3.283970594406128, acc: 0.39534884691238403)
[2025-02-16 13:55:01,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:02,124][root][INFO] - Training Epoch: 1/2, step 380/23838 completed (loss: 4.135368824005127, acc: 0.22807016968727112)
[2025-02-16 13:55:02,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:02,471][root][INFO] - Training Epoch: 1/2, step 381/23838 completed (loss: 3.8062994480133057, acc: 0.2291666716337204)
[2025-02-16 13:55:02,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:02,795][root][INFO] - Training Epoch: 1/2, step 382/23838 completed (loss: 3.580690860748291, acc: 0.3235294222831726)
[2025-02-16 13:55:03,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:03,293][root][INFO] - Training Epoch: 1/2, step 383/23838 completed (loss: 3.408726215362549, acc: 0.3529411852359772)
[2025-02-16 13:55:03,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:03,650][root][INFO] - Training Epoch: 1/2, step 384/23838 completed (loss: 3.657147169113159, acc: 0.3333333432674408)
[2025-02-16 13:55:03,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:03,997][root][INFO] - Training Epoch: 1/2, step 385/23838 completed (loss: 3.861971139907837, acc: 0.2380952388048172)
[2025-02-16 13:55:04,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:04,396][root][INFO] - Training Epoch: 1/2, step 386/23838 completed (loss: 3.6804072856903076, acc: 0.2857142984867096)
[2025-02-16 13:55:04,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:04,806][root][INFO] - Training Epoch: 1/2, step 387/23838 completed (loss: 3.8123810291290283, acc: 0.28947368264198303)
[2025-02-16 13:55:04,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:05,144][root][INFO] - Training Epoch: 1/2, step 388/23838 completed (loss: 4.070644855499268, acc: 0.21212121844291687)
[2025-02-16 13:55:05,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:05,479][root][INFO] - Training Epoch: 1/2, step 389/23838 completed (loss: 3.9753215312957764, acc: 0.2857142984867096)
[2025-02-16 13:55:05,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:05,861][root][INFO] - Training Epoch: 1/2, step 390/23838 completed (loss: 4.851231098175049, acc: 0.1515151560306549)
[2025-02-16 13:55:06,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:06,283][root][INFO] - Training Epoch: 1/2, step 391/23838 completed (loss: 2.9058868885040283, acc: 0.3684210479259491)
[2025-02-16 13:55:06,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:06,665][root][INFO] - Training Epoch: 1/2, step 392/23838 completed (loss: 3.9914681911468506, acc: 0.2291666716337204)
[2025-02-16 13:55:06,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:07,032][root][INFO] - Training Epoch: 1/2, step 393/23838 completed (loss: 3.3829152584075928, acc: 0.3333333432674408)
[2025-02-16 13:55:07,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:07,462][root][INFO] - Training Epoch: 1/2, step 394/23838 completed (loss: 3.607541561126709, acc: 0.26923078298568726)
[2025-02-16 13:55:07,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:07,803][root][INFO] - Training Epoch: 1/2, step 395/23838 completed (loss: 3.4314045906066895, acc: 0.44999998807907104)
[2025-02-16 13:55:07,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:08,161][root][INFO] - Training Epoch: 1/2, step 396/23838 completed (loss: 3.611419200897217, acc: 0.3571428656578064)
[2025-02-16 13:55:08,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:08,582][root][INFO] - Training Epoch: 1/2, step 397/23838 completed (loss: 4.001372814178467, acc: 0.2181818187236786)
[2025-02-16 13:55:08,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:08,969][root][INFO] - Training Epoch: 1/2, step 398/23838 completed (loss: 3.487884998321533, acc: 0.37142857909202576)
[2025-02-16 13:55:09,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:09,308][root][INFO] - Training Epoch: 1/2, step 399/23838 completed (loss: 4.138235092163086, acc: 0.27659574151039124)
[2025-02-16 13:55:09,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:09,707][root][INFO] - Training Epoch: 1/2, step 400/23838 completed (loss: 4.109483242034912, acc: 0.2978723347187042)
[2025-02-16 13:55:09,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:10,118][root][INFO] - Training Epoch: 1/2, step 401/23838 completed (loss: 3.980870485305786, acc: 0.27272728085517883)
[2025-02-16 13:55:10,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:10,421][root][INFO] - Training Epoch: 1/2, step 402/23838 completed (loss: 3.544111728668213, acc: 0.42424243688583374)
[2025-02-16 13:55:10,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:10,719][root][INFO] - Training Epoch: 1/2, step 403/23838 completed (loss: 3.234731435775757, acc: 0.380952388048172)
[2025-02-16 13:55:10,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:11,052][root][INFO] - Training Epoch: 1/2, step 404/23838 completed (loss: 3.0896244049072266, acc: 0.47999998927116394)
[2025-02-16 13:55:11,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:11,399][root][INFO] - Training Epoch: 1/2, step 405/23838 completed (loss: 2.5448238849639893, acc: 0.4399999976158142)
[2025-02-16 13:55:11,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:11,707][root][INFO] - Training Epoch: 1/2, step 406/23838 completed (loss: 4.31258487701416, acc: 0.4285714328289032)
[2025-02-16 13:55:11,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:12,074][root][INFO] - Training Epoch: 1/2, step 407/23838 completed (loss: 3.895984172821045, acc: 0.3499999940395355)
[2025-02-16 13:55:12,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:12,481][root][INFO] - Training Epoch: 1/2, step 408/23838 completed (loss: 4.668476581573486, acc: 0.3333333432674408)
[2025-02-16 13:55:12,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:12,819][root][INFO] - Training Epoch: 1/2, step 409/23838 completed (loss: 2.438310384750366, acc: 0.529411792755127)
[2025-02-16 13:55:12,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:13,134][root][INFO] - Training Epoch: 1/2, step 410/23838 completed (loss: 3.2807533740997314, acc: 0.4545454680919647)
[2025-02-16 13:55:13,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:13,454][root][INFO] - Training Epoch: 1/2, step 411/23838 completed (loss: 3.821956157684326, acc: 0.4761904776096344)
[2025-02-16 13:55:13,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:13,780][root][INFO] - Training Epoch: 1/2, step 412/23838 completed (loss: 3.4569053649902344, acc: 0.6428571343421936)
[2025-02-16 13:55:13,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:14,100][root][INFO] - Training Epoch: 1/2, step 413/23838 completed (loss: 3.6591689586639404, acc: 0.5)
[2025-02-16 13:55:14,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:14,435][root][INFO] - Training Epoch: 1/2, step 414/23838 completed (loss: 3.363568067550659, acc: 0.27586206793785095)
[2025-02-16 13:55:14,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:14,746][root][INFO] - Training Epoch: 1/2, step 415/23838 completed (loss: 4.187678813934326, acc: 0.38461539149284363)
[2025-02-16 13:55:14,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:15,067][root][INFO] - Training Epoch: 1/2, step 416/23838 completed (loss: 3.9981439113616943, acc: 0.4285714328289032)
[2025-02-16 13:55:15,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:15,374][root][INFO] - Training Epoch: 1/2, step 417/23838 completed (loss: 4.655934810638428, acc: 0.24137930572032928)
[2025-02-16 13:55:15,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:15,682][root][INFO] - Training Epoch: 1/2, step 418/23838 completed (loss: 3.3105309009552, acc: 0.30434781312942505)
[2025-02-16 13:55:15,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:16,036][root][INFO] - Training Epoch: 1/2, step 419/23838 completed (loss: 3.8402326107025146, acc: 0.4285714328289032)
[2025-02-16 13:55:16,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:16,369][root][INFO] - Training Epoch: 1/2, step 420/23838 completed (loss: 3.635424852371216, acc: 0.46666666865348816)
[2025-02-16 13:55:16,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:16,700][root][INFO] - Training Epoch: 1/2, step 421/23838 completed (loss: 3.679795742034912, acc: 0.3461538553237915)
[2025-02-16 13:55:16,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:17,000][root][INFO] - Training Epoch: 1/2, step 422/23838 completed (loss: 3.6718101501464844, acc: 0.40909090638160706)
[2025-02-16 13:55:17,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:17,326][root][INFO] - Training Epoch: 1/2, step 423/23838 completed (loss: 4.25847864151001, acc: 0.25925925374031067)
[2025-02-16 13:55:17,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:17,627][root][INFO] - Training Epoch: 1/2, step 424/23838 completed (loss: 2.9056434631347656, acc: 0.40625)
[2025-02-16 13:55:17,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:17,931][root][INFO] - Training Epoch: 1/2, step 425/23838 completed (loss: 3.2288050651550293, acc: 0.5263158082962036)
[2025-02-16 13:55:18,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:18,292][root][INFO] - Training Epoch: 1/2, step 426/23838 completed (loss: 6.189559459686279, acc: 0.2222222238779068)
[2025-02-16 13:55:18,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:18,628][root][INFO] - Training Epoch: 1/2, step 427/23838 completed (loss: 4.473184585571289, acc: 0.3888888955116272)
[2025-02-16 13:55:18,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:18,956][root][INFO] - Training Epoch: 1/2, step 428/23838 completed (loss: 4.935461521148682, acc: 0.3333333432674408)
[2025-02-16 13:55:19,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:19,296][root][INFO] - Training Epoch: 1/2, step 429/23838 completed (loss: 5.017776012420654, acc: 0.30000001192092896)
[2025-02-16 13:55:19,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:19,632][root][INFO] - Training Epoch: 1/2, step 430/23838 completed (loss: 3.7215123176574707, acc: 0.20000000298023224)
[2025-02-16 13:55:19,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:20,036][root][INFO] - Training Epoch: 1/2, step 431/23838 completed (loss: 4.056184768676758, acc: 0.22580644488334656)
[2025-02-16 13:55:20,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:20,389][root][INFO] - Training Epoch: 1/2, step 432/23838 completed (loss: 4.073897838592529, acc: 0.25581395626068115)
[2025-02-16 13:55:20,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:20,723][root][INFO] - Training Epoch: 1/2, step 433/23838 completed (loss: 3.8371641635894775, acc: 0.32608696818351746)
[2025-02-16 13:55:20,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:21,042][root][INFO] - Training Epoch: 1/2, step 434/23838 completed (loss: 3.805532217025757, acc: 0.21212121844291687)
[2025-02-16 13:55:21,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:21,396][root][INFO] - Training Epoch: 1/2, step 435/23838 completed (loss: 3.496067523956299, acc: 0.3076923191547394)
[2025-02-16 13:55:21,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:55:21,759][root][INFO] - Training Epoch: 1/2, step 436/23838 completed (loss: 3.5965073108673096, acc: 0.2666666805744171)
[2025-02-16 13:55:21,891][slam_llm.models.slam_model][INFO] - modality encoder
