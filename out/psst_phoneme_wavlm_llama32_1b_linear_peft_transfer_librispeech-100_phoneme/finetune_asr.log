[2025-02-12 22:48:33,068][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-12 22:48:33,069][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-12 22:48:33,069][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme'}
[2025-02-12 22:48:33,069][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-12_22-48-32.txt', 'log_interval': 5}
[2025-02-12 22:48:59,254][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-12 22:49:04,255][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-12 22:49:04,257][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-12 22:49:04,259][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-12 22:49:04,260][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-12 22:49:10,744][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-12 22:49:10,746][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-12 22:49:10,746][slam_llm.models.slam_model][INFO] - setup peft...
[2025-02-12 22:49:10,862][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-12 22:49:10,864][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-12 22:49:10,950][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-12 22:49:10,950][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-12 22:49:10,951][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_1.0206900835037231/model.pt
[2025-02-12 22:49:11,115][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-12 22:49:11,126][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-12 22:49:15,245][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-12 22:49:17,335][root][INFO] - --> Training Set Length = 28539
[2025-02-12 22:49:17,352][root][INFO] - --> Validation Set Length = 2703
[2025-02-12 22:49:17,352][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-12 22:49:17,353][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-12 22:49:20,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:21,998][root][INFO] - Training Epoch: 1/2, step 0/7134 completed (loss: 2.260176420211792, acc: 0.5557206273078918)
[2025-02-12 22:49:22,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:24,981][root][INFO] - Training Epoch: 1/2, step 1/7134 completed (loss: 2.431884288787842, acc: 0.509065568447113)
[2025-02-12 22:49:25,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:25,460][root][INFO] - Training Epoch: 1/2, step 2/7134 completed (loss: 2.3162989616394043, acc: 0.4926372170448303)
[2025-02-12 22:49:25,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:25,996][root][INFO] - Training Epoch: 1/2, step 3/7134 completed (loss: 2.1767072677612305, acc: 0.5198463797569275)
[2025-02-12 22:49:26,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:26,523][root][INFO] - Training Epoch: 1/2, step 4/7134 completed (loss: 2.304136037826538, acc: 0.48703956604003906)
[2025-02-12 22:49:26,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:27,008][root][INFO] - Training Epoch: 1/2, step 5/7134 completed (loss: 2.357591152191162, acc: 0.5038071274757385)
[2025-02-12 22:49:27,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:27,481][root][INFO] - Training Epoch: 1/2, step 6/7134 completed (loss: 2.2279090881347656, acc: 0.5072231292724609)
[2025-02-12 22:49:27,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:28,388][root][INFO] - Training Epoch: 1/2, step 7/7134 completed (loss: 2.46763277053833, acc: 0.4757160544395447)
[2025-02-12 22:49:28,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:28,907][root][INFO] - Training Epoch: 1/2, step 8/7134 completed (loss: 2.348681926727295, acc: 0.4814305305480957)
[2025-02-12 22:49:29,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:29,357][root][INFO] - Training Epoch: 1/2, step 9/7134 completed (loss: 2.3979854583740234, acc: 0.5240309834480286)
[2025-02-12 22:49:29,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:31,239][root][INFO] - Training Epoch: 1/2, step 10/7134 completed (loss: 2.59743595123291, acc: 0.4509246051311493)
[2025-02-12 22:49:31,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:31,781][root][INFO] - Training Epoch: 1/2, step 11/7134 completed (loss: 2.2375986576080322, acc: 0.5347092151641846)
[2025-02-12 22:49:31,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:32,265][root][INFO] - Training Epoch: 1/2, step 12/7134 completed (loss: 2.201674461364746, acc: 0.5020188689231873)
[2025-02-12 22:49:32,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:32,722][root][INFO] - Training Epoch: 1/2, step 13/7134 completed (loss: 2.1551504135131836, acc: 0.5158184170722961)
[2025-02-12 22:49:32,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:33,175][root][INFO] - Training Epoch: 1/2, step 14/7134 completed (loss: 2.2271945476531982, acc: 0.5512605309486389)
[2025-02-12 22:49:33,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:33,656][root][INFO] - Training Epoch: 1/2, step 15/7134 completed (loss: 2.143648386001587, acc: 0.5611015558242798)
[2025-02-12 22:49:33,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:34,145][root][INFO] - Training Epoch: 1/2, step 16/7134 completed (loss: 2.164863348007202, acc: 0.5528455376625061)
[2025-02-12 22:49:34,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:34,572][root][INFO] - Training Epoch: 1/2, step 17/7134 completed (loss: 2.304095983505249, acc: 0.5540540814399719)
[2025-02-12 22:49:34,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:35,046][root][INFO] - Training Epoch: 1/2, step 18/7134 completed (loss: 2.221116065979004, acc: 0.5255681872367859)
[2025-02-12 22:49:35,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:35,515][root][INFO] - Training Epoch: 1/2, step 19/7134 completed (loss: 2.293030023574829, acc: 0.5075861811637878)
[2025-02-12 22:49:35,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:35,998][root][INFO] - Training Epoch: 1/2, step 20/7134 completed (loss: 2.0939908027648926, acc: 0.5249999761581421)
[2025-02-12 22:49:36,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:36,489][root][INFO] - Training Epoch: 1/2, step 21/7134 completed (loss: 2.1168324947357178, acc: 0.5025773048400879)
[2025-02-12 22:49:36,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:37,000][root][INFO] - Training Epoch: 1/2, step 22/7134 completed (loss: 2.0579798221588135, acc: 0.5403458476066589)
[2025-02-12 22:49:37,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:37,494][root][INFO] - Training Epoch: 1/2, step 23/7134 completed (loss: 1.9900296926498413, acc: 0.55027174949646)
[2025-02-12 22:49:37,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:37,961][root][INFO] - Training Epoch: 1/2, step 24/7134 completed (loss: 1.9728442430496216, acc: 0.5515320301055908)
[2025-02-12 22:49:38,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:38,471][root][INFO] - Training Epoch: 1/2, step 25/7134 completed (loss: 1.9694594144821167, acc: 0.5340909361839294)
[2025-02-12 22:49:38,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:38,963][root][INFO] - Training Epoch: 1/2, step 26/7134 completed (loss: 1.8651843070983887, acc: 0.5648967623710632)
[2025-02-12 22:49:39,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:39,432][root][INFO] - Training Epoch: 1/2, step 27/7134 completed (loss: 1.7372978925704956, acc: 0.6129985451698303)
[2025-02-12 22:49:39,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:39,943][root][INFO] - Training Epoch: 1/2, step 28/7134 completed (loss: 2.1124675273895264, acc: 0.5313252806663513)
[2025-02-12 22:49:40,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:40,454][root][INFO] - Training Epoch: 1/2, step 29/7134 completed (loss: 2.188551664352417, acc: 0.5099149942398071)
[2025-02-12 22:49:40,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:41,029][root][INFO] - Training Epoch: 1/2, step 30/7134 completed (loss: 2.0453147888183594, acc: 0.5231638550758362)
[2025-02-12 22:49:41,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:41,933][root][INFO] - Training Epoch: 1/2, step 31/7134 completed (loss: 2.0599541664123535, acc: 0.4936440587043762)
[2025-02-12 22:49:42,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:42,456][root][INFO] - Training Epoch: 1/2, step 32/7134 completed (loss: 2.194636821746826, acc: 0.4564369320869446)
[2025-02-12 22:49:42,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:42,902][root][INFO] - Training Epoch: 1/2, step 33/7134 completed (loss: 2.119889974594116, acc: 0.5358565449714661)
[2025-02-12 22:49:43,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:43,431][root][INFO] - Training Epoch: 1/2, step 34/7134 completed (loss: 2.0986099243164062, acc: 0.5322391390800476)
[2025-02-12 22:49:43,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:44,772][root][INFO] - Training Epoch: 1/2, step 35/7134 completed (loss: 1.9442280530929565, acc: 0.5117813348770142)
[2025-02-12 22:49:44,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:45,296][root][INFO] - Training Epoch: 1/2, step 36/7134 completed (loss: 1.9566264152526855, acc: 0.5407165884971619)
[2025-02-12 22:49:45,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:45,835][root][INFO] - Training Epoch: 1/2, step 37/7134 completed (loss: 1.7448636293411255, acc: 0.5544430613517761)
[2025-02-12 22:49:46,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:46,335][root][INFO] - Training Epoch: 1/2, step 38/7134 completed (loss: 1.818229079246521, acc: 0.555059552192688)
[2025-02-12 22:49:46,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:46,882][root][INFO] - Training Epoch: 1/2, step 39/7134 completed (loss: 1.6196168661117554, acc: 0.5872800946235657)
[2025-02-12 22:49:47,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:47,343][root][INFO] - Training Epoch: 1/2, step 40/7134 completed (loss: 1.825260877609253, acc: 0.5896860957145691)
[2025-02-12 22:49:47,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:47,802][root][INFO] - Training Epoch: 1/2, step 41/7134 completed (loss: 1.7736191749572754, acc: 0.582524299621582)
[2025-02-12 22:49:48,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:48,308][root][INFO] - Training Epoch: 1/2, step 42/7134 completed (loss: 1.6466487646102905, acc: 0.5722300410270691)
[2025-02-12 22:49:48,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:48,770][root][INFO] - Training Epoch: 1/2, step 43/7134 completed (loss: 1.4633657932281494, acc: 0.6167742013931274)
[2025-02-12 22:49:48,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:49,249][root][INFO] - Training Epoch: 1/2, step 44/7134 completed (loss: 1.550189733505249, acc: 0.6203821897506714)
[2025-02-12 22:49:49,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:49,827][root][INFO] - Training Epoch: 1/2, step 45/7134 completed (loss: 1.585550308227539, acc: 0.5893824696540833)
[2025-02-12 22:49:51,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:51,806][root][INFO] - Training Epoch: 1/2, step 46/7134 completed (loss: 1.556700348854065, acc: 0.6346666812896729)
[2025-02-12 22:49:52,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:52,357][root][INFO] - Training Epoch: 1/2, step 47/7134 completed (loss: 1.6027731895446777, acc: 0.6435786485671997)
[2025-02-12 22:49:52,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:52,901][root][INFO] - Training Epoch: 1/2, step 48/7134 completed (loss: 1.4388998746871948, acc: 0.6235954761505127)
[2025-02-12 22:49:53,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:53,378][root][INFO] - Training Epoch: 1/2, step 49/7134 completed (loss: 1.3642698526382446, acc: 0.6255143880844116)
[2025-02-12 22:49:53,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:53,887][root][INFO] - Training Epoch: 1/2, step 50/7134 completed (loss: 1.4967403411865234, acc: 0.612456738948822)
[2025-02-12 22:49:54,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:54,314][root][INFO] - Training Epoch: 1/2, step 51/7134 completed (loss: 1.3282419443130493, acc: 0.6687816977500916)
[2025-02-12 22:49:54,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:54,758][root][INFO] - Training Epoch: 1/2, step 52/7134 completed (loss: 1.3972283601760864, acc: 0.6504064798355103)
[2025-02-12 22:49:54,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:55,206][root][INFO] - Training Epoch: 1/2, step 53/7134 completed (loss: 1.35702383518219, acc: 0.6338582634925842)
[2025-02-12 22:49:55,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:55,741][root][INFO] - Training Epoch: 1/2, step 54/7134 completed (loss: 1.2624632120132446, acc: 0.6651323437690735)
[2025-02-12 22:49:55,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:56,300][root][INFO] - Training Epoch: 1/2, step 55/7134 completed (loss: 1.2771167755126953, acc: 0.6586294174194336)
[2025-02-12 22:49:56,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:56,789][root][INFO] - Training Epoch: 1/2, step 56/7134 completed (loss: 1.2575215101242065, acc: 0.6660714149475098)
[2025-02-12 22:49:56,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:57,280][root][INFO] - Training Epoch: 1/2, step 57/7134 completed (loss: 1.306287407875061, acc: 0.6583850979804993)
[2025-02-12 22:49:57,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:57,784][root][INFO] - Training Epoch: 1/2, step 58/7134 completed (loss: 1.3552350997924805, acc: 0.6338028311729431)
[2025-02-12 22:49:57,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:58,257][root][INFO] - Training Epoch: 1/2, step 59/7134 completed (loss: 1.1593403816223145, acc: 0.6855791807174683)
[2025-02-12 22:49:58,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:58,774][root][INFO] - Training Epoch: 1/2, step 60/7134 completed (loss: 1.3836172819137573, acc: 0.6292613744735718)
[2025-02-12 22:49:58,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:59,294][root][INFO] - Training Epoch: 1/2, step 61/7134 completed (loss: 1.2817217111587524, acc: 0.6401468515396118)
[2025-02-12 22:49:59,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:49:59,779][root][INFO] - Training Epoch: 1/2, step 62/7134 completed (loss: 1.1979016065597534, acc: 0.6721727848052979)
[2025-02-12 22:49:59,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:00,275][root][INFO] - Training Epoch: 1/2, step 63/7134 completed (loss: 1.3369665145874023, acc: 0.6513075828552246)
[2025-02-12 22:50:00,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:00,779][root][INFO] - Training Epoch: 1/2, step 64/7134 completed (loss: 1.3561346530914307, acc: 0.6465053558349609)
[2025-02-12 22:50:00,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:01,291][root][INFO] - Training Epoch: 1/2, step 65/7134 completed (loss: 1.4268440008163452, acc: 0.601965606212616)
[2025-02-12 22:50:01,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:01,770][root][INFO] - Training Epoch: 1/2, step 66/7134 completed (loss: 1.3066307306289673, acc: 0.6559139490127563)
[2025-02-12 22:50:01,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:02,274][root][INFO] - Training Epoch: 1/2, step 67/7134 completed (loss: 1.2034592628479004, acc: 0.6705882549285889)
[2025-02-12 22:50:02,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:02,754][root][INFO] - Training Epoch: 1/2, step 68/7134 completed (loss: 1.3234829902648926, acc: 0.6580645442008972)
[2025-02-12 22:50:02,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:03,218][root][INFO] - Training Epoch: 1/2, step 69/7134 completed (loss: 1.2546401023864746, acc: 0.6645885109901428)
[2025-02-12 22:50:03,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:03,685][root][INFO] - Training Epoch: 1/2, step 70/7134 completed (loss: 1.213342308998108, acc: 0.6684210300445557)
[2025-02-12 22:50:03,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:04,145][root][INFO] - Training Epoch: 1/2, step 71/7134 completed (loss: 1.3621660470962524, acc: 0.6554622054100037)
[2025-02-12 22:50:04,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:04,649][root][INFO] - Training Epoch: 1/2, step 72/7134 completed (loss: 1.3390322923660278, acc: 0.6496163606643677)
[2025-02-12 22:50:04,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:05,080][root][INFO] - Training Epoch: 1/2, step 73/7134 completed (loss: 1.3268437385559082, acc: 0.6521739363670349)
[2025-02-12 22:50:05,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:05,551][root][INFO] - Training Epoch: 1/2, step 74/7134 completed (loss: 1.3764370679855347, acc: 0.6601671576499939)
[2025-02-12 22:50:05,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:06,078][root][INFO] - Training Epoch: 1/2, step 75/7134 completed (loss: 1.4211618900299072, acc: 0.6502242088317871)
[2025-02-12 22:50:06,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:06,572][root][INFO] - Training Epoch: 1/2, step 76/7134 completed (loss: 1.1975603103637695, acc: 0.6561712622642517)
[2025-02-12 22:50:06,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:07,067][root][INFO] - Training Epoch: 1/2, step 77/7134 completed (loss: 1.2554348707199097, acc: 0.6867167949676514)
[2025-02-12 22:50:07,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:07,592][root][INFO] - Training Epoch: 1/2, step 78/7134 completed (loss: 1.1322038173675537, acc: 0.7012578845024109)
[2025-02-12 22:50:07,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:08,066][root][INFO] - Training Epoch: 1/2, step 79/7134 completed (loss: 1.213165283203125, acc: 0.6604938507080078)
[2025-02-12 22:50:08,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:08,528][root][INFO] - Training Epoch: 1/2, step 80/7134 completed (loss: 1.0577828884124756, acc: 0.7109375)
[2025-02-12 22:50:08,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:09,022][root][INFO] - Training Epoch: 1/2, step 81/7134 completed (loss: 1.059593915939331, acc: 0.7159686088562012)
[2025-02-12 22:50:09,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:09,510][root][INFO] - Training Epoch: 1/2, step 82/7134 completed (loss: 1.0642292499542236, acc: 0.7199434041976929)
[2025-02-12 22:50:09,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:09,998][root][INFO] - Training Epoch: 1/2, step 83/7134 completed (loss: 1.154174566268921, acc: 0.6760925650596619)
[2025-02-12 22:50:10,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:10,487][root][INFO] - Training Epoch: 1/2, step 84/7134 completed (loss: 1.1915905475616455, acc: 0.6822106838226318)
[2025-02-12 22:50:10,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:10,898][root][INFO] - Training Epoch: 1/2, step 85/7134 completed (loss: 1.2739932537078857, acc: 0.6515513062477112)
[2025-02-12 22:50:11,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:11,366][root][INFO] - Training Epoch: 1/2, step 86/7134 completed (loss: 1.3349016904830933, acc: 0.6559633016586304)
[2025-02-12 22:50:11,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:11,874][root][INFO] - Training Epoch: 1/2, step 87/7134 completed (loss: 1.1148097515106201, acc: 0.6842877864837646)
[2025-02-12 22:50:12,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:12,349][root][INFO] - Training Epoch: 1/2, step 88/7134 completed (loss: 1.056235671043396, acc: 0.7012448310852051)
[2025-02-12 22:50:12,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:12,850][root][INFO] - Training Epoch: 1/2, step 89/7134 completed (loss: 1.0593931674957275, acc: 0.7047497034072876)
[2025-02-12 22:50:13,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:13,325][root][INFO] - Training Epoch: 1/2, step 90/7134 completed (loss: 1.1563940048217773, acc: 0.6730769276618958)
[2025-02-12 22:50:13,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:13,784][root][INFO] - Training Epoch: 1/2, step 91/7134 completed (loss: 1.006797194480896, acc: 0.7081632614135742)
[2025-02-12 22:50:13,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:14,298][root][INFO] - Training Epoch: 1/2, step 92/7134 completed (loss: 1.0635414123535156, acc: 0.6998597383499146)
[2025-02-12 22:50:14,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:14,790][root][INFO] - Training Epoch: 1/2, step 93/7134 completed (loss: 1.00943124294281, acc: 0.7314049601554871)
[2025-02-12 22:50:14,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:15,177][root][INFO] - Training Epoch: 1/2, step 94/7134 completed (loss: 1.2134822607040405, acc: 0.6728538274765015)
[2025-02-12 22:50:15,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:15,685][root][INFO] - Training Epoch: 1/2, step 95/7134 completed (loss: 1.0318583250045776, acc: 0.7085427045822144)
[2025-02-12 22:50:15,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:16,075][root][INFO] - Training Epoch: 1/2, step 96/7134 completed (loss: 1.0069220066070557, acc: 0.7355679869651794)
[2025-02-12 22:50:16,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:16,550][root][INFO] - Training Epoch: 1/2, step 97/7134 completed (loss: 1.7517505884170532, acc: 0.5903614163398743)
[2025-02-12 22:50:16,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:17,053][root][INFO] - Training Epoch: 1/2, step 98/7134 completed (loss: 1.3341847658157349, acc: 0.6407766938209534)
[2025-02-12 22:50:17,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:17,517][root][INFO] - Training Epoch: 1/2, step 99/7134 completed (loss: 0.9783185124397278, acc: 0.7394495606422424)
[2025-02-12 22:50:17,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:18,056][root][INFO] - Training Epoch: 1/2, step 100/7134 completed (loss: 0.953635036945343, acc: 0.7294871807098389)
[2025-02-12 22:50:18,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:18,543][root][INFO] - Training Epoch: 1/2, step 101/7134 completed (loss: 1.0240302085876465, acc: 0.7236841917037964)
[2025-02-12 22:50:18,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:19,006][root][INFO] - Training Epoch: 1/2, step 102/7134 completed (loss: 1.1070233583450317, acc: 0.6862027049064636)
[2025-02-12 22:50:19,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:19,440][root][INFO] - Training Epoch: 1/2, step 103/7134 completed (loss: 0.9187783598899841, acc: 0.7433333396911621)
[2025-02-12 22:50:19,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:19,951][root][INFO] - Training Epoch: 1/2, step 104/7134 completed (loss: 0.9421854019165039, acc: 0.7326284050941467)
[2025-02-12 22:50:20,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:20,456][root][INFO] - Training Epoch: 1/2, step 105/7134 completed (loss: 0.905887246131897, acc: 0.7145187854766846)
[2025-02-12 22:50:20,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:20,985][root][INFO] - Training Epoch: 1/2, step 106/7134 completed (loss: 0.954767107963562, acc: 0.7289837002754211)
[2025-02-12 22:50:21,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:21,482][root][INFO] - Training Epoch: 1/2, step 107/7134 completed (loss: 1.1081264019012451, acc: 0.6848137378692627)
[2025-02-12 22:50:21,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:22,000][root][INFO] - Training Epoch: 1/2, step 108/7134 completed (loss: 0.850854754447937, acc: 0.7494004964828491)
[2025-02-12 22:50:22,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:22,458][root][INFO] - Training Epoch: 1/2, step 109/7134 completed (loss: 0.9604400396347046, acc: 0.7352941036224365)
[2025-02-12 22:50:22,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:22,971][root][INFO] - Training Epoch: 1/2, step 110/7134 completed (loss: 0.8520013689994812, acc: 0.7544065713882446)
[2025-02-12 22:50:23,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:23,418][root][INFO] - Training Epoch: 1/2, step 111/7134 completed (loss: 0.9063637852668762, acc: 0.7386759519577026)
[2025-02-12 22:50:23,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:23,853][root][INFO] - Training Epoch: 1/2, step 112/7134 completed (loss: 0.8247298002243042, acc: 0.7411401867866516)
[2025-02-12 22:50:23,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:24,310][root][INFO] - Training Epoch: 1/2, step 113/7134 completed (loss: 0.8615198731422424, acc: 0.755696177482605)
[2025-02-12 22:50:24,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:24,778][root][INFO] - Training Epoch: 1/2, step 114/7134 completed (loss: 0.949637234210968, acc: 0.7506631016731262)
[2025-02-12 22:50:24,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:25,176][root][INFO] - Training Epoch: 1/2, step 115/7134 completed (loss: 1.0219910144805908, acc: 0.7139107584953308)
[2025-02-12 22:50:25,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:25,741][root][INFO] - Training Epoch: 1/2, step 116/7134 completed (loss: 0.8444747924804688, acc: 0.7455830574035645)
[2025-02-12 22:50:25,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:26,190][root][INFO] - Training Epoch: 1/2, step 117/7134 completed (loss: 0.839400589466095, acc: 0.7631579041481018)
[2025-02-12 22:50:26,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:26,707][root][INFO] - Training Epoch: 1/2, step 118/7134 completed (loss: 0.7923799753189087, acc: 0.7755101919174194)
[2025-02-12 22:50:26,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:27,207][root][INFO] - Training Epoch: 1/2, step 119/7134 completed (loss: 0.8336137533187866, acc: 0.7639225125312805)
[2025-02-12 22:50:27,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:27,662][root][INFO] - Training Epoch: 1/2, step 120/7134 completed (loss: 0.7510011792182922, acc: 0.7785059213638306)
[2025-02-12 22:50:27,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:28,123][root][INFO] - Training Epoch: 1/2, step 121/7134 completed (loss: 0.7539545297622681, acc: 0.7833553552627563)
[2025-02-12 22:50:28,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:28,608][root][INFO] - Training Epoch: 1/2, step 122/7134 completed (loss: 0.7462490200996399, acc: 0.800000011920929)
[2025-02-12 22:50:28,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:29,040][root][INFO] - Training Epoch: 1/2, step 123/7134 completed (loss: 0.7004615664482117, acc: 0.8033033013343811)
[2025-02-12 22:50:29,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:29,487][root][INFO] - Training Epoch: 1/2, step 124/7134 completed (loss: 0.8291175365447998, acc: 0.7706896662712097)
[2025-02-12 22:50:29,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:30,001][root][INFO] - Training Epoch: 1/2, step 125/7134 completed (loss: 0.8727390170097351, acc: 0.7577433586120605)
[2025-02-12 22:50:30,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:30,469][root][INFO] - Training Epoch: 1/2, step 126/7134 completed (loss: 0.7605260610580444, acc: 0.7799999713897705)
[2025-02-12 22:50:30,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:30,940][root][INFO] - Training Epoch: 1/2, step 127/7134 completed (loss: 0.7294554114341736, acc: 0.790123462677002)
[2025-02-12 22:50:31,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:31,429][root][INFO] - Training Epoch: 1/2, step 128/7134 completed (loss: 0.9212638735771179, acc: 0.7416378259658813)
[2025-02-12 22:50:31,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:31,913][root][INFO] - Training Epoch: 1/2, step 129/7134 completed (loss: 0.805852472782135, acc: 0.7683823704719543)
[2025-02-12 22:50:32,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:32,358][root][INFO] - Training Epoch: 1/2, step 130/7134 completed (loss: 0.7602077126502991, acc: 0.7697368264198303)
[2025-02-12 22:50:32,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:32,841][root][INFO] - Training Epoch: 1/2, step 131/7134 completed (loss: 0.7310370206832886, acc: 0.7986842393875122)
[2025-02-12 22:50:33,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:33,329][root][INFO] - Training Epoch: 1/2, step 132/7134 completed (loss: 0.655585765838623, acc: 0.8171262741088867)
[2025-02-12 22:50:33,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:33,810][root][INFO] - Training Epoch: 1/2, step 133/7134 completed (loss: 0.6724933981895447, acc: 0.8119800090789795)
[2025-02-12 22:50:33,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:34,299][root][INFO] - Training Epoch: 1/2, step 134/7134 completed (loss: 0.6161032915115356, acc: 0.8070796728134155)
[2025-02-12 22:50:34,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:34,787][root][INFO] - Training Epoch: 1/2, step 135/7134 completed (loss: 0.7410704493522644, acc: 0.7811111211776733)
[2025-02-12 22:50:34,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:35,254][root][INFO] - Training Epoch: 1/2, step 136/7134 completed (loss: 0.6335210204124451, acc: 0.8172042965888977)
[2025-02-12 22:50:35,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:35,734][root][INFO] - Training Epoch: 1/2, step 137/7134 completed (loss: 0.7565203309059143, acc: 0.7736777663230896)
[2025-02-12 22:50:35,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:36,236][root][INFO] - Training Epoch: 1/2, step 138/7134 completed (loss: 0.7794398069381714, acc: 0.7850821614265442)
[2025-02-12 22:50:36,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:36,765][root][INFO] - Training Epoch: 1/2, step 139/7134 completed (loss: 0.7242973446846008, acc: 0.8030690550804138)
[2025-02-12 22:50:36,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:37,241][root][INFO] - Training Epoch: 1/2, step 140/7134 completed (loss: 0.9142744541168213, acc: 0.7567164301872253)
[2025-02-12 22:50:37,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:37,744][root][INFO] - Training Epoch: 1/2, step 141/7134 completed (loss: 0.6952791810035706, acc: 0.8136792182922363)
[2025-02-12 22:50:37,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:38,251][root][INFO] - Training Epoch: 1/2, step 142/7134 completed (loss: 0.753121554851532, acc: 0.7885952591896057)
[2025-02-12 22:50:38,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:38,728][root][INFO] - Training Epoch: 1/2, step 143/7134 completed (loss: 0.5947988033294678, acc: 0.8200782537460327)
[2025-02-12 22:50:38,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:39,223][root][INFO] - Training Epoch: 1/2, step 144/7134 completed (loss: 0.7740670442581177, acc: 0.7929883003234863)
[2025-02-12 22:50:39,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:39,701][root][INFO] - Training Epoch: 1/2, step 145/7134 completed (loss: 1.016542673110962, acc: 0.7436619997024536)
[2025-02-12 22:50:39,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:40,191][root][INFO] - Training Epoch: 1/2, step 146/7134 completed (loss: 0.8404006361961365, acc: 0.7706552743911743)
[2025-02-12 22:50:40,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:40,673][root][INFO] - Training Epoch: 1/2, step 147/7134 completed (loss: 0.8708983659744263, acc: 0.7694973945617676)
[2025-02-12 22:50:40,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:41,120][root][INFO] - Training Epoch: 1/2, step 148/7134 completed (loss: 0.9026598930358887, acc: 0.7610208988189697)
[2025-02-12 22:50:41,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:41,580][root][INFO] - Training Epoch: 1/2, step 149/7134 completed (loss: 0.7989394068717957, acc: 0.7896774411201477)
[2025-02-12 22:50:41,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:42,069][root][INFO] - Training Epoch: 1/2, step 150/7134 completed (loss: 0.8683683276176453, acc: 0.7620320916175842)
[2025-02-12 22:50:42,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:42,551][root][INFO] - Training Epoch: 1/2, step 151/7134 completed (loss: 0.8363233804702759, acc: 0.776849627494812)
[2025-02-12 22:50:42,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:43,033][root][INFO] - Training Epoch: 1/2, step 152/7134 completed (loss: 0.9101243019104004, acc: 0.7494305372238159)
[2025-02-12 22:50:43,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:43,474][root][INFO] - Training Epoch: 1/2, step 153/7134 completed (loss: 0.9410735368728638, acc: 0.7374301552772522)
[2025-02-12 22:50:43,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:43,901][root][INFO] - Training Epoch: 1/2, step 154/7134 completed (loss: 0.8939772844314575, acc: 0.7537993788719177)
[2025-02-12 22:50:44,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:44,372][root][INFO] - Training Epoch: 1/2, step 155/7134 completed (loss: 0.7992010712623596, acc: 0.7818182110786438)
[2025-02-12 22:50:44,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:44,857][root][INFO] - Training Epoch: 1/2, step 156/7134 completed (loss: 0.8227593898773193, acc: 0.7747957706451416)
[2025-02-12 22:50:45,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:45,350][root][INFO] - Training Epoch: 1/2, step 157/7134 completed (loss: 0.8972786664962769, acc: 0.7734940052032471)
[2025-02-12 22:50:45,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:45,855][root][INFO] - Training Epoch: 1/2, step 158/7134 completed (loss: 0.7360041737556458, acc: 0.7912814021110535)
[2025-02-12 22:50:46,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:46,359][root][INFO] - Training Epoch: 1/2, step 159/7134 completed (loss: 0.710589587688446, acc: 0.789321780204773)
[2025-02-12 22:50:46,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:46,784][root][INFO] - Training Epoch: 1/2, step 160/7134 completed (loss: 0.9085890650749207, acc: 0.7550607323646545)
[2025-02-12 22:50:46,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:47,164][root][INFO] - Training Epoch: 1/2, step 161/7134 completed (loss: 0.7794387340545654, acc: 0.770348846912384)
[2025-02-12 22:50:47,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:47,568][root][INFO] - Training Epoch: 1/2, step 162/7134 completed (loss: 0.8002349734306335, acc: 0.7647058963775635)
[2025-02-12 22:50:47,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:48,090][root][INFO] - Training Epoch: 1/2, step 163/7134 completed (loss: 0.7590771317481995, acc: 0.7799113988876343)
[2025-02-12 22:50:48,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:48,506][root][INFO] - Training Epoch: 1/2, step 164/7134 completed (loss: 0.7529916763305664, acc: 0.7659574747085571)
[2025-02-12 22:50:48,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:48,958][root][INFO] - Training Epoch: 1/2, step 165/7134 completed (loss: 0.9109840393066406, acc: 0.7918660044670105)
[2025-02-12 22:50:49,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:49,324][root][INFO] - Training Epoch: 1/2, step 166/7134 completed (loss: 0.7516159415245056, acc: 0.7641791105270386)
[2025-02-12 22:50:49,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:49,797][root][INFO] - Training Epoch: 1/2, step 167/7134 completed (loss: 1.183514952659607, acc: 0.70944744348526)
[2025-02-12 22:50:49,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:50,205][root][INFO] - Training Epoch: 1/2, step 168/7134 completed (loss: 2.218839406967163, acc: 0.4941176474094391)
[2025-02-12 22:50:50,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:50,588][root][INFO] - Training Epoch: 1/2, step 169/7134 completed (loss: 1.8000317811965942, acc: 0.6162465214729309)
[2025-02-12 22:50:50,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:50,993][root][INFO] - Training Epoch: 1/2, step 170/7134 completed (loss: 1.171981692314148, acc: 0.6982921957969666)
[2025-02-12 22:50:51,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:51,389][root][INFO] - Training Epoch: 1/2, step 171/7134 completed (loss: 1.1699150800704956, acc: 0.7207792401313782)
[2025-02-12 22:50:51,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:51,820][root][INFO] - Training Epoch: 1/2, step 172/7134 completed (loss: 1.0009597539901733, acc: 0.7726161479949951)
[2025-02-12 22:50:51,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:52,203][root][INFO] - Training Epoch: 1/2, step 173/7134 completed (loss: 0.805873453617096, acc: 0.778761088848114)
[2025-02-12 22:50:52,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:52,631][root][INFO] - Training Epoch: 1/2, step 174/7134 completed (loss: 0.8033765554428101, acc: 0.76408451795578)
[2025-02-12 22:50:52,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:53,100][root][INFO] - Training Epoch: 1/2, step 175/7134 completed (loss: 0.8353074789047241, acc: 0.7945454716682434)
[2025-02-12 22:50:53,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:53,584][root][INFO] - Training Epoch: 1/2, step 176/7134 completed (loss: 0.8509865403175354, acc: 0.7753623127937317)
[2025-02-12 22:50:53,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:54,010][root][INFO] - Training Epoch: 1/2, step 177/7134 completed (loss: 0.8108093738555908, acc: 0.7722222208976746)
[2025-02-12 22:50:54,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:54,332][root][INFO] - Training Epoch: 1/2, step 178/7134 completed (loss: 0.8498935699462891, acc: 0.748062014579773)
[2025-02-12 22:50:54,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:54,755][root][INFO] - Training Epoch: 1/2, step 179/7134 completed (loss: 0.7554536461830139, acc: 0.8046421408653259)
[2025-02-12 22:50:54,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:55,238][root][INFO] - Training Epoch: 1/2, step 180/7134 completed (loss: 0.6291788816452026, acc: 0.8084833025932312)
[2025-02-12 22:50:55,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:55,679][root][INFO] - Training Epoch: 1/2, step 181/7134 completed (loss: 0.5520892143249512, acc: 0.8247694373130798)
[2025-02-12 22:50:55,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:56,177][root][INFO] - Training Epoch: 1/2, step 182/7134 completed (loss: 0.6161302924156189, acc: 0.8083028197288513)
[2025-02-12 22:50:56,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:56,659][root][INFO] - Training Epoch: 1/2, step 183/7134 completed (loss: 0.6051383018493652, acc: 0.8188011050224304)
[2025-02-12 22:50:56,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:57,087][root][INFO] - Training Epoch: 1/2, step 184/7134 completed (loss: 0.4946271777153015, acc: 0.8525345325469971)
[2025-02-12 22:50:57,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:57,523][root][INFO] - Training Epoch: 1/2, step 185/7134 completed (loss: 0.4603138864040375, acc: 0.8526466488838196)
[2025-02-12 22:50:57,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:58,006][root][INFO] - Training Epoch: 1/2, step 186/7134 completed (loss: 0.5518908500671387, acc: 0.8461538553237915)
[2025-02-12 22:50:58,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:58,449][root][INFO] - Training Epoch: 1/2, step 187/7134 completed (loss: 0.5668531656265259, acc: 0.8379888534545898)
[2025-02-12 22:50:58,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:58,891][root][INFO] - Training Epoch: 1/2, step 188/7134 completed (loss: 0.5600338578224182, acc: 0.8333333134651184)
[2025-02-12 22:50:59,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:59,273][root][INFO] - Training Epoch: 1/2, step 189/7134 completed (loss: 0.6758579015731812, acc: 0.8230769038200378)
[2025-02-12 22:50:59,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:50:59,733][root][INFO] - Training Epoch: 1/2, step 190/7134 completed (loss: 0.5941121578216553, acc: 0.8315132856369019)
[2025-02-12 22:50:59,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:00,151][root][INFO] - Training Epoch: 1/2, step 191/7134 completed (loss: 0.5202458500862122, acc: 0.8611111044883728)
[2025-02-12 22:51:00,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:00,653][root][INFO] - Training Epoch: 1/2, step 192/7134 completed (loss: 0.51795893907547, acc: 0.838487982749939)
[2025-02-12 22:51:00,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:01,086][root][INFO] - Training Epoch: 1/2, step 193/7134 completed (loss: 0.4915178418159485, acc: 0.8429530262947083)
[2025-02-12 22:51:01,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:01,574][root][INFO] - Training Epoch: 1/2, step 194/7134 completed (loss: 0.5639775991439819, acc: 0.8378766179084778)
[2025-02-12 22:51:01,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:02,032][root][INFO] - Training Epoch: 1/2, step 195/7134 completed (loss: 0.5775771737098694, acc: 0.848739504814148)
[2025-02-12 22:51:02,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:02,516][root][INFO] - Training Epoch: 1/2, step 196/7134 completed (loss: 0.4493488073348999, acc: 0.874331533908844)
[2025-02-12 22:51:02,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:02,992][root][INFO] - Training Epoch: 1/2, step 197/7134 completed (loss: 0.5647839307785034, acc: 0.8447293639183044)
[2025-02-12 22:51:03,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:03,479][root][INFO] - Training Epoch: 1/2, step 198/7134 completed (loss: 0.49701741337776184, acc: 0.8565940856933594)
[2025-02-12 22:51:03,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:03,941][root][INFO] - Training Epoch: 1/2, step 199/7134 completed (loss: 0.5296295881271362, acc: 0.8390804529190063)
[2025-02-12 22:51:04,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:04,426][root][INFO] - Training Epoch: 1/2, step 200/7134 completed (loss: 0.4804212749004364, acc: 0.8590604066848755)
[2025-02-12 22:51:04,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:04,874][root][INFO] - Training Epoch: 1/2, step 201/7134 completed (loss: 0.4820795953273773, acc: 0.8525280952453613)
[2025-02-12 22:51:05,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:05,351][root][INFO] - Training Epoch: 1/2, step 202/7134 completed (loss: 0.4438227713108063, acc: 0.860689640045166)
[2025-02-12 22:51:05,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:05,823][root][INFO] - Training Epoch: 1/2, step 203/7134 completed (loss: 0.4465674161911011, acc: 0.8842975497245789)
[2025-02-12 22:51:05,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:06,307][root][INFO] - Training Epoch: 1/2, step 204/7134 completed (loss: 0.4731771945953369, acc: 0.8784194588661194)
[2025-02-12 22:51:06,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:06,766][root][INFO] - Training Epoch: 1/2, step 205/7134 completed (loss: 0.5657077431678772, acc: 0.8473479747772217)
[2025-02-12 22:51:06,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:07,221][root][INFO] - Training Epoch: 1/2, step 206/7134 completed (loss: 0.5484153032302856, acc: 0.8523274660110474)
[2025-02-12 22:51:07,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:07,652][root][INFO] - Training Epoch: 1/2, step 207/7134 completed (loss: 0.5470958352088928, acc: 0.849829375743866)
[2025-02-12 22:51:07,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:08,081][root][INFO] - Training Epoch: 1/2, step 208/7134 completed (loss: 0.7206348180770874, acc: 0.8235294222831726)
[2025-02-12 22:51:08,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:08,550][root][INFO] - Training Epoch: 1/2, step 209/7134 completed (loss: 0.6578992009162903, acc: 0.8212435245513916)
[2025-02-12 22:51:08,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:09,037][root][INFO] - Training Epoch: 1/2, step 210/7134 completed (loss: 0.6691492795944214, acc: 0.8211284279823303)
[2025-02-12 22:51:09,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:09,415][root][INFO] - Training Epoch: 1/2, step 211/7134 completed (loss: 0.7466428279876709, acc: 0.8202614188194275)
[2025-02-12 22:51:09,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:09,847][root][INFO] - Training Epoch: 1/2, step 212/7134 completed (loss: 0.5744063258171082, acc: 0.8595041036605835)
[2025-02-12 22:51:10,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:10,299][root][INFO] - Training Epoch: 1/2, step 213/7134 completed (loss: 0.47069382667541504, acc: 0.8705501556396484)
[2025-02-12 22:51:10,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:10,707][root][INFO] - Training Epoch: 1/2, step 214/7134 completed (loss: 0.48136937618255615, acc: 0.8782051205635071)
[2025-02-12 22:51:10,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:11,165][root][INFO] - Training Epoch: 1/2, step 215/7134 completed (loss: 0.5477797985076904, acc: 0.8472222089767456)
[2025-02-12 22:51:11,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:11,594][root][INFO] - Training Epoch: 1/2, step 216/7134 completed (loss: 0.5005465745925903, acc: 0.8666666746139526)
[2025-02-12 22:51:11,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:12,103][root][INFO] - Training Epoch: 1/2, step 217/7134 completed (loss: 0.5025477409362793, acc: 0.8690909147262573)
[2025-02-12 22:51:12,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:12,586][root][INFO] - Training Epoch: 1/2, step 218/7134 completed (loss: 0.33705177903175354, acc: 0.8969465494155884)
[2025-02-12 22:51:12,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:13,053][root][INFO] - Training Epoch: 1/2, step 219/7134 completed (loss: 0.449644535779953, acc: 0.8791422843933105)
[2025-02-12 22:51:13,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:13,513][root][INFO] - Training Epoch: 1/2, step 220/7134 completed (loss: 0.4444459080696106, acc: 0.8828451633453369)
[2025-02-12 22:51:13,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:14,002][root][INFO] - Training Epoch: 1/2, step 221/7134 completed (loss: 0.4537971019744873, acc: 0.8805969953536987)
[2025-02-12 22:51:14,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:14,503][root][INFO] - Training Epoch: 1/2, step 222/7134 completed (loss: 0.43033117055892944, acc: 0.8874345421791077)
[2025-02-12 22:51:14,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:14,954][root][INFO] - Training Epoch: 1/2, step 223/7134 completed (loss: 0.485010027885437, acc: 0.8626444339752197)
[2025-02-12 22:51:15,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:15,390][root][INFO] - Training Epoch: 1/2, step 224/7134 completed (loss: 0.5983391404151917, acc: 0.8352788686752319)
[2025-02-12 22:51:15,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:15,854][root][INFO] - Training Epoch: 1/2, step 225/7134 completed (loss: 0.5579320192337036, acc: 0.8433889746665955)
[2025-02-12 22:51:16,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:16,345][root][INFO] - Training Epoch: 1/2, step 226/7134 completed (loss: 0.5466333031654358, acc: 0.859413206577301)
[2025-02-12 22:51:16,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:16,806][root][INFO] - Training Epoch: 1/2, step 227/7134 completed (loss: 0.6947139501571655, acc: 0.8325061798095703)
[2025-02-12 22:51:16,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:17,276][root][INFO] - Training Epoch: 1/2, step 228/7134 completed (loss: 0.5680384635925293, acc: 0.8509406447410583)
[2025-02-12 22:51:17,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:17,753][root][INFO] - Training Epoch: 1/2, step 229/7134 completed (loss: 0.4793878197669983, acc: 0.8607594966888428)
[2025-02-12 22:51:17,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:18,266][root][INFO] - Training Epoch: 1/2, step 230/7134 completed (loss: 0.4102514684200287, acc: 0.8783783912658691)
[2025-02-12 22:51:18,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:18,707][root][INFO] - Training Epoch: 1/2, step 231/7134 completed (loss: 0.38699233531951904, acc: 0.8813559412956238)
[2025-02-12 22:51:18,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:19,177][root][INFO] - Training Epoch: 1/2, step 232/7134 completed (loss: 0.43157660961151123, acc: 0.8731465935707092)
[2025-02-12 22:51:19,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:19,618][root][INFO] - Training Epoch: 1/2, step 233/7134 completed (loss: 0.35958993434906006, acc: 0.9017432928085327)
[2025-02-12 22:51:19,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:20,077][root][INFO] - Training Epoch: 1/2, step 234/7134 completed (loss: 0.4461800456047058, acc: 0.8800675868988037)
[2025-02-12 22:51:20,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:20,539][root][INFO] - Training Epoch: 1/2, step 235/7134 completed (loss: 0.4187588095664978, acc: 0.8917378783226013)
[2025-02-12 22:51:20,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:21,012][root][INFO] - Training Epoch: 1/2, step 236/7134 completed (loss: 0.3661169707775116, acc: 0.8999999761581421)
[2025-02-12 22:51:21,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:21,454][root][INFO] - Training Epoch: 1/2, step 237/7134 completed (loss: 0.4464840292930603, acc: 0.876304030418396)
[2025-02-12 22:51:21,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:21,877][root][INFO] - Training Epoch: 1/2, step 238/7134 completed (loss: 0.4238542914390564, acc: 0.8936567306518555)
[2025-02-12 22:51:22,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:22,326][root][INFO] - Training Epoch: 1/2, step 239/7134 completed (loss: 0.3050902783870697, acc: 0.8991596698760986)
[2025-02-12 22:51:22,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:22,786][root][INFO] - Training Epoch: 1/2, step 240/7134 completed (loss: 0.41813573241233826, acc: 0.883474588394165)
[2025-02-12 22:51:22,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:23,224][root][INFO] - Training Epoch: 1/2, step 241/7134 completed (loss: 0.4344958961009979, acc: 0.8872785568237305)
[2025-02-12 22:51:23,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:23,683][root][INFO] - Training Epoch: 1/2, step 242/7134 completed (loss: 0.47955936193466187, acc: 0.8804159164428711)
[2025-02-12 22:51:23,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:24,111][root][INFO] - Training Epoch: 1/2, step 243/7134 completed (loss: 0.5938252806663513, acc: 0.853741466999054)
[2025-02-12 22:51:24,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:24,539][root][INFO] - Training Epoch: 1/2, step 244/7134 completed (loss: 0.3244785666465759, acc: 0.9079939723014832)
[2025-02-12 22:51:24,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:24,974][root][INFO] - Training Epoch: 1/2, step 245/7134 completed (loss: 0.41308414936065674, acc: 0.889667272567749)
[2025-02-12 22:51:25,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:25,421][root][INFO] - Training Epoch: 1/2, step 246/7134 completed (loss: 0.4107908010482788, acc: 0.8848580718040466)
[2025-02-12 22:51:25,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:25,857][root][INFO] - Training Epoch: 1/2, step 247/7134 completed (loss: 0.3889525234699249, acc: 0.8983666300773621)
[2025-02-12 22:51:26,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:26,331][root][INFO] - Training Epoch: 1/2, step 248/7134 completed (loss: 0.3598397970199585, acc: 0.8972868323326111)
[2025-02-12 22:51:26,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:26,747][root][INFO] - Training Epoch: 1/2, step 249/7134 completed (loss: 0.5470276474952698, acc: 0.8686327338218689)
[2025-02-12 22:51:26,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:27,201][root][INFO] - Training Epoch: 1/2, step 250/7134 completed (loss: 0.4784940481185913, acc: 0.8734177350997925)
[2025-02-12 22:51:27,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:27,630][root][INFO] - Training Epoch: 1/2, step 251/7134 completed (loss: 0.37452438473701477, acc: 0.8940171003341675)
[2025-02-12 22:51:27,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:28,058][root][INFO] - Training Epoch: 1/2, step 252/7134 completed (loss: 0.34888955950737, acc: 0.8934426307678223)
[2025-02-12 22:51:28,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:28,523][root][INFO] - Training Epoch: 1/2, step 253/7134 completed (loss: 0.4077470302581787, acc: 0.8770053386688232)
[2025-02-12 22:51:28,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:28,959][root][INFO] - Training Epoch: 1/2, step 254/7134 completed (loss: 0.3879149258136749, acc: 0.8999999761581421)
[2025-02-12 22:51:29,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:29,422][root][INFO] - Training Epoch: 1/2, step 255/7134 completed (loss: 0.3866828382015228, acc: 0.8932748436927795)
[2025-02-12 22:51:29,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:29,832][root][INFO] - Training Epoch: 1/2, step 256/7134 completed (loss: 0.4592900574207306, acc: 0.8823529481887817)
[2025-02-12 22:51:30,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:30,342][root][INFO] - Training Epoch: 1/2, step 257/7134 completed (loss: 0.3586064875125885, acc: 0.8926380276679993)
[2025-02-12 22:51:30,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:30,782][root][INFO] - Training Epoch: 1/2, step 258/7134 completed (loss: 0.404709130525589, acc: 0.8876032829284668)
[2025-02-12 22:51:30,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:31,256][root][INFO] - Training Epoch: 1/2, step 259/7134 completed (loss: 0.3604840338230133, acc: 0.9043760299682617)
[2025-02-12 22:51:31,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:31,712][root][INFO] - Training Epoch: 1/2, step 260/7134 completed (loss: 0.3978666663169861, acc: 0.9001691937446594)
[2025-02-12 22:51:31,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:32,146][root][INFO] - Training Epoch: 1/2, step 261/7134 completed (loss: 0.3549831509590149, acc: 0.8997954726219177)
[2025-02-12 22:51:32,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:32,556][root][INFO] - Training Epoch: 1/2, step 262/7134 completed (loss: 0.3390413820743561, acc: 0.8985024690628052)
[2025-02-12 22:51:32,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:32,997][root][INFO] - Training Epoch: 1/2, step 263/7134 completed (loss: 0.38082247972488403, acc: 0.8863345980644226)
[2025-02-12 22:51:33,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:33,460][root][INFO] - Training Epoch: 1/2, step 264/7134 completed (loss: 0.4359067976474762, acc: 0.8738317489624023)
[2025-02-12 22:51:33,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:33,918][root][INFO] - Training Epoch: 1/2, step 265/7134 completed (loss: 0.38527536392211914, acc: 0.8878865838050842)
[2025-02-12 22:51:34,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:34,376][root][INFO] - Training Epoch: 1/2, step 266/7134 completed (loss: 0.42375439405441284, acc: 0.8795871734619141)
[2025-02-12 22:51:34,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:34,894][root][INFO] - Training Epoch: 1/2, step 267/7134 completed (loss: 0.4538995623588562, acc: 0.8666666746139526)
[2025-02-12 22:51:35,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:35,417][root][INFO] - Training Epoch: 1/2, step 268/7134 completed (loss: 0.45377102494239807, acc: 0.8560311198234558)
[2025-02-12 22:51:35,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:35,916][root][INFO] - Training Epoch: 1/2, step 269/7134 completed (loss: 0.39846131205558777, acc: 0.8845618963241577)
[2025-02-12 22:51:36,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:36,351][root][INFO] - Training Epoch: 1/2, step 270/7134 completed (loss: 0.27461591362953186, acc: 0.9268656969070435)
[2025-02-12 22:51:36,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:36,840][root][INFO] - Training Epoch: 1/2, step 271/7134 completed (loss: 0.3744184076786041, acc: 0.8915510773658752)
[2025-02-12 22:51:36,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:37,293][root][INFO] - Training Epoch: 1/2, step 272/7134 completed (loss: 0.3471163809299469, acc: 0.9096692204475403)
[2025-02-12 22:51:37,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:37,748][root][INFO] - Training Epoch: 1/2, step 273/7134 completed (loss: 0.31564176082611084, acc: 0.9077844023704529)
[2025-02-12 22:51:37,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:38,183][root][INFO] - Training Epoch: 1/2, step 274/7134 completed (loss: 0.2857835292816162, acc: 0.9202200770378113)
[2025-02-12 22:51:38,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:38,706][root][INFO] - Training Epoch: 1/2, step 275/7134 completed (loss: 0.4537702202796936, acc: 0.8816705346107483)
[2025-02-12 22:51:38,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:39,204][root][INFO] - Training Epoch: 1/2, step 276/7134 completed (loss: 0.31451863050460815, acc: 0.9232643246650696)
[2025-02-12 22:51:39,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:39,679][root][INFO] - Training Epoch: 1/2, step 277/7134 completed (loss: 0.4020141363143921, acc: 0.8880308866500854)
[2025-02-12 22:51:39,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:40,120][root][INFO] - Training Epoch: 1/2, step 278/7134 completed (loss: 0.3090735673904419, acc: 0.9042145609855652)
[2025-02-12 22:51:40,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:40,577][root][INFO] - Training Epoch: 1/2, step 279/7134 completed (loss: 0.30911219120025635, acc: 0.913385808467865)
[2025-02-12 22:51:40,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:41,006][root][INFO] - Training Epoch: 1/2, step 280/7134 completed (loss: 0.38271549344062805, acc: 0.8942652344703674)
[2025-02-12 22:51:41,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:41,425][root][INFO] - Training Epoch: 1/2, step 281/7134 completed (loss: 0.3044836223125458, acc: 0.9039999842643738)
[2025-02-12 22:51:41,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:41,868][root][INFO] - Training Epoch: 1/2, step 282/7134 completed (loss: 0.4698476195335388, acc: 0.8824427723884583)
[2025-02-12 22:51:42,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:42,300][root][INFO] - Training Epoch: 1/2, step 283/7134 completed (loss: 0.4436728358268738, acc: 0.8777589201927185)
[2025-02-12 22:51:42,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:42,745][root][INFO] - Training Epoch: 1/2, step 284/7134 completed (loss: 0.45559170842170715, acc: 0.8741610646247864)
[2025-02-12 22:51:42,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:43,204][root][INFO] - Training Epoch: 1/2, step 285/7134 completed (loss: 0.4038179814815521, acc: 0.8888888955116272)
[2025-02-12 22:51:43,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:43,611][root][INFO] - Training Epoch: 1/2, step 286/7134 completed (loss: 0.2980762720108032, acc: 0.913274347782135)
[2025-02-12 22:51:43,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:44,051][root][INFO] - Training Epoch: 1/2, step 287/7134 completed (loss: 0.4513760507106781, acc: 0.874799370765686)
[2025-02-12 22:51:44,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:44,532][root][INFO] - Training Epoch: 1/2, step 288/7134 completed (loss: 0.3769218921661377, acc: 0.8858194947242737)
[2025-02-12 22:51:44,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:45,057][root][INFO] - Training Epoch: 1/2, step 289/7134 completed (loss: 0.37272414565086365, acc: 0.8915008902549744)
[2025-02-12 22:51:45,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:45,490][root][INFO] - Training Epoch: 1/2, step 290/7134 completed (loss: 0.4023912847042084, acc: 0.8875380158424377)
[2025-02-12 22:51:45,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:45,943][root][INFO] - Training Epoch: 1/2, step 291/7134 completed (loss: 0.3007885813713074, acc: 0.9226973652839661)
[2025-02-12 22:51:46,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:46,396][root][INFO] - Training Epoch: 1/2, step 292/7134 completed (loss: 0.433941513299942, acc: 0.8773006200790405)
[2025-02-12 22:51:46,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:46,854][root][INFO] - Training Epoch: 1/2, step 293/7134 completed (loss: 0.29229626059532166, acc: 0.9196786880493164)
[2025-02-12 22:51:47,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:47,318][root][INFO] - Training Epoch: 1/2, step 294/7134 completed (loss: 0.341406911611557, acc: 0.9025341272354126)
[2025-02-12 22:51:47,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:47,806][root][INFO] - Training Epoch: 1/2, step 295/7134 completed (loss: 0.33196669816970825, acc: 0.9095563292503357)
[2025-02-12 22:51:47,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:48,259][root][INFO] - Training Epoch: 1/2, step 296/7134 completed (loss: 0.2741599380970001, acc: 0.9240310192108154)
[2025-02-12 22:51:48,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:48,716][root][INFO] - Training Epoch: 1/2, step 297/7134 completed (loss: 0.2657715380191803, acc: 0.9289520382881165)
[2025-02-12 22:51:48,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:49,175][root][INFO] - Training Epoch: 1/2, step 298/7134 completed (loss: 0.2872689664363861, acc: 0.9247999787330627)
[2025-02-12 22:51:49,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:49,619][root][INFO] - Training Epoch: 1/2, step 299/7134 completed (loss: 0.2820501923561096, acc: 0.927819550037384)
[2025-02-12 22:51:49,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:50,070][root][INFO] - Training Epoch: 1/2, step 300/7134 completed (loss: 0.2481580525636673, acc: 0.9255663156509399)
[2025-02-12 22:51:50,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:50,528][root][INFO] - Training Epoch: 1/2, step 301/7134 completed (loss: 0.27572101354599, acc: 0.9236209392547607)
[2025-02-12 22:51:50,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:50,938][root][INFO] - Training Epoch: 1/2, step 302/7134 completed (loss: 0.4053056538105011, acc: 0.9005848169326782)
[2025-02-12 22:51:51,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:51,346][root][INFO] - Training Epoch: 1/2, step 303/7134 completed (loss: 0.2775207757949829, acc: 0.9281553626060486)
[2025-02-12 22:51:51,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:51,795][root][INFO] - Training Epoch: 1/2, step 304/7134 completed (loss: 0.22345571219921112, acc: 0.9277777671813965)
[2025-02-12 22:51:51,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:52,233][root][INFO] - Training Epoch: 1/2, step 305/7134 completed (loss: 0.2046215683221817, acc: 0.9364069700241089)
[2025-02-12 22:51:52,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:52,665][root][INFO] - Training Epoch: 1/2, step 306/7134 completed (loss: 0.24841143190860748, acc: 0.9148550629615784)
[2025-02-12 22:51:52,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:53,132][root][INFO] - Training Epoch: 1/2, step 307/7134 completed (loss: 0.2729451656341553, acc: 0.9234875440597534)
[2025-02-12 22:51:53,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:53,558][root][INFO] - Training Epoch: 1/2, step 308/7134 completed (loss: 0.253896027803421, acc: 0.9226973652839661)
[2025-02-12 22:51:53,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:54,031][root][INFO] - Training Epoch: 1/2, step 309/7134 completed (loss: 0.27365851402282715, acc: 0.9241379499435425)
[2025-02-12 22:51:54,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:54,518][root][INFO] - Training Epoch: 1/2, step 310/7134 completed (loss: 0.23697252571582794, acc: 0.9342105388641357)
[2025-02-12 22:51:54,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:54,954][root][INFO] - Training Epoch: 1/2, step 311/7134 completed (loss: 0.26939696073532104, acc: 0.9253499507904053)
[2025-02-12 22:51:55,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:55,402][root][INFO] - Training Epoch: 1/2, step 312/7134 completed (loss: 0.23836645483970642, acc: 0.9306759238243103)
[2025-02-12 22:51:55,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:55,848][root][INFO] - Training Epoch: 1/2, step 313/7134 completed (loss: 0.2160150408744812, acc: 0.9269565343856812)
[2025-02-12 22:51:56,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:56,301][root][INFO] - Training Epoch: 1/2, step 314/7134 completed (loss: 0.30236372351646423, acc: 0.909375011920929)
[2025-02-12 22:51:56,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:56,759][root][INFO] - Training Epoch: 1/2, step 315/7134 completed (loss: 0.23321768641471863, acc: 0.9163934588432312)
[2025-02-12 22:51:56,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:57,195][root][INFO] - Training Epoch: 1/2, step 316/7134 completed (loss: 0.2872123718261719, acc: 0.911552369594574)
[2025-02-12 22:51:57,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:57,685][root][INFO] - Training Epoch: 1/2, step 317/7134 completed (loss: 0.21855521202087402, acc: 0.9377537369728088)
[2025-02-12 22:51:57,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:58,184][root][INFO] - Training Epoch: 1/2, step 318/7134 completed (loss: 0.37536704540252686, acc: 0.9023199081420898)
[2025-02-12 22:51:58,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:58,610][root][INFO] - Training Epoch: 1/2, step 319/7134 completed (loss: 0.3689144551753998, acc: 0.8934081196784973)
[2025-02-12 22:51:58,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:59,097][root][INFO] - Training Epoch: 1/2, step 320/7134 completed (loss: 0.33061689138412476, acc: 0.9158345460891724)
[2025-02-12 22:51:59,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:51:59,575][root][INFO] - Training Epoch: 1/2, step 321/7134 completed (loss: 0.37230226397514343, acc: 0.9042145609855652)
[2025-02-12 22:51:59,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:00,085][root][INFO] - Training Epoch: 1/2, step 322/7134 completed (loss: 0.3877471387386322, acc: 0.8888888955116272)
[2025-02-12 22:52:00,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:00,543][root][INFO] - Training Epoch: 1/2, step 323/7134 completed (loss: 0.4314732253551483, acc: 0.8844672441482544)
[2025-02-12 22:52:00,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:01,024][root][INFO] - Training Epoch: 1/2, step 324/7134 completed (loss: 0.3782418370246887, acc: 0.9010356664657593)
[2025-02-12 22:52:01,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:01,505][root][INFO] - Training Epoch: 1/2, step 325/7134 completed (loss: 0.2528414726257324, acc: 0.9241645336151123)
[2025-02-12 22:52:01,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:01,982][root][INFO] - Training Epoch: 1/2, step 326/7134 completed (loss: 0.29675909876823425, acc: 0.9074299931526184)
[2025-02-12 22:52:02,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:02,454][root][INFO] - Training Epoch: 1/2, step 327/7134 completed (loss: 0.3449450433254242, acc: 0.9020270109176636)
[2025-02-12 22:52:02,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:02,961][root][INFO] - Training Epoch: 1/2, step 328/7134 completed (loss: 0.3337019085884094, acc: 0.902222216129303)
[2025-02-12 22:52:03,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:03,437][root][INFO] - Training Epoch: 1/2, step 329/7134 completed (loss: 0.27699974179267883, acc: 0.9177438020706177)
[2025-02-12 22:52:03,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:03,925][root][INFO] - Training Epoch: 1/2, step 330/7134 completed (loss: 0.40204739570617676, acc: 0.8992950916290283)
[2025-02-12 22:52:04,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:04,428][root][INFO] - Training Epoch: 1/2, step 331/7134 completed (loss: 0.2818922996520996, acc: 0.9167616963386536)
[2025-02-12 22:52:04,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:04,922][root][INFO] - Training Epoch: 1/2, step 332/7134 completed (loss: 0.2755257189273834, acc: 0.9185360074043274)
[2025-02-12 22:52:05,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:05,385][root][INFO] - Training Epoch: 1/2, step 333/7134 completed (loss: 0.35516566038131714, acc: 0.9063260555267334)
[2025-02-12 22:52:05,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:05,865][root][INFO] - Training Epoch: 1/2, step 334/7134 completed (loss: 0.27598094940185547, acc: 0.9239543676376343)
[2025-02-12 22:52:06,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:06,321][root][INFO] - Training Epoch: 1/2, step 335/7134 completed (loss: 0.2523117959499359, acc: 0.9374090433120728)
[2025-02-12 22:52:06,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:06,783][root][INFO] - Training Epoch: 1/2, step 336/7134 completed (loss: 0.31042566895484924, acc: 0.9185628890991211)
[2025-02-12 22:52:06,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:07,239][root][INFO] - Training Epoch: 1/2, step 337/7134 completed (loss: 0.25666722655296326, acc: 0.9325153231620789)
[2025-02-12 22:52:07,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:07,689][root][INFO] - Training Epoch: 1/2, step 338/7134 completed (loss: 0.2728332281112671, acc: 0.9219143390655518)
[2025-02-12 22:52:07,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:08,140][root][INFO] - Training Epoch: 1/2, step 339/7134 completed (loss: 0.22070075571537018, acc: 0.9399999976158142)
[2025-02-12 22:52:08,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:08,586][root][INFO] - Training Epoch: 1/2, step 340/7134 completed (loss: 0.2750805914402008, acc: 0.9185750484466553)
[2025-02-12 22:52:08,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:09,063][root][INFO] - Training Epoch: 1/2, step 341/7134 completed (loss: 0.23605883121490479, acc: 0.9306431412696838)
[2025-02-12 22:52:09,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:09,536][root][INFO] - Training Epoch: 1/2, step 342/7134 completed (loss: 0.21850180625915527, acc: 0.9439393877983093)
[2025-02-12 22:52:09,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:10,004][root][INFO] - Training Epoch: 1/2, step 343/7134 completed (loss: 0.2515431046485901, acc: 0.9281663298606873)
[2025-02-12 22:52:10,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:10,435][root][INFO] - Training Epoch: 1/2, step 344/7134 completed (loss: 0.2045750766992569, acc: 0.9470803141593933)
[2025-02-12 22:52:10,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:10,875][root][INFO] - Training Epoch: 1/2, step 345/7134 completed (loss: 0.1875564008951187, acc: 0.9482142925262451)
[2025-02-12 22:52:11,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:11,399][root][INFO] - Training Epoch: 1/2, step 346/7134 completed (loss: 0.2075469195842743, acc: 0.9443561434745789)
[2025-02-12 22:52:11,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:11,842][root][INFO] - Training Epoch: 1/2, step 347/7134 completed (loss: 0.22716687619686127, acc: 0.948106586933136)
[2025-02-12 22:52:11,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:12,313][root][INFO] - Training Epoch: 1/2, step 348/7134 completed (loss: 0.17896272242069244, acc: 0.9552238583564758)
[2025-02-12 22:52:12,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:12,796][root][INFO] - Training Epoch: 1/2, step 349/7134 completed (loss: 0.19415336847305298, acc: 0.9409396052360535)
[2025-02-12 22:52:13,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:13,335][root][INFO] - Training Epoch: 1/2, step 350/7134 completed (loss: 0.1789054572582245, acc: 0.9494584798812866)
[2025-02-12 22:52:13,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:13,873][root][INFO] - Training Epoch: 1/2, step 351/7134 completed (loss: 0.3149075508117676, acc: 0.9300254583358765)
[2025-02-12 22:52:14,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:14,363][root][INFO] - Training Epoch: 1/2, step 352/7134 completed (loss: 0.19048872590065002, acc: 0.94972825050354)
[2025-02-12 22:52:14,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:14,881][root][INFO] - Training Epoch: 1/2, step 353/7134 completed (loss: 0.21106334030628204, acc: 0.9308510422706604)
[2025-02-12 22:52:15,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:15,339][root][INFO] - Training Epoch: 1/2, step 354/7134 completed (loss: 0.22090981900691986, acc: 0.9429569244384766)
[2025-02-12 22:52:15,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:15,815][root][INFO] - Training Epoch: 1/2, step 355/7134 completed (loss: 0.29647088050842285, acc: 0.9273743033409119)
[2025-02-12 22:52:16,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:16,320][root][INFO] - Training Epoch: 1/2, step 356/7134 completed (loss: 0.22685259580612183, acc: 0.9351851940155029)
[2025-02-12 22:52:16,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:16,818][root][INFO] - Training Epoch: 1/2, step 357/7134 completed (loss: 0.21988746523857117, acc: 0.9402035474777222)
[2025-02-12 22:52:16,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:17,286][root][INFO] - Training Epoch: 1/2, step 358/7134 completed (loss: 0.23084118962287903, acc: 0.944065511226654)
[2025-02-12 22:52:17,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:17,823][root][INFO] - Training Epoch: 1/2, step 359/7134 completed (loss: 0.18218564987182617, acc: 0.9364864826202393)
[2025-02-12 22:52:17,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:18,252][root][INFO] - Training Epoch: 1/2, step 360/7134 completed (loss: 0.20937202870845795, acc: 0.9442771077156067)
[2025-02-12 22:52:18,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:18,692][root][INFO] - Training Epoch: 1/2, step 361/7134 completed (loss: 0.19684024155139923, acc: 0.9500734210014343)
[2025-02-12 22:52:18,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:19,147][root][INFO] - Training Epoch: 1/2, step 362/7134 completed (loss: 0.22557109594345093, acc: 0.946107804775238)
[2025-02-12 22:52:19,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:19,628][root][INFO] - Training Epoch: 1/2, step 363/7134 completed (loss: 0.25982415676116943, acc: 0.9296875)
[2025-02-12 22:52:19,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:20,061][root][INFO] - Training Epoch: 1/2, step 364/7134 completed (loss: 0.22096915543079376, acc: 0.9345794320106506)
[2025-02-12 22:52:20,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:20,571][root][INFO] - Training Epoch: 1/2, step 365/7134 completed (loss: 0.20386892557144165, acc: 0.9462810158729553)
[2025-02-12 22:52:20,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:21,031][root][INFO] - Training Epoch: 1/2, step 366/7134 completed (loss: 0.3639408051967621, acc: 0.9059560894966125)
[2025-02-12 22:52:21,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:21,514][root][INFO] - Training Epoch: 1/2, step 367/7134 completed (loss: 0.2290009707212448, acc: 0.9287499785423279)
[2025-02-12 22:52:21,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:21,954][root][INFO] - Training Epoch: 1/2, step 368/7134 completed (loss: 0.189839169383049, acc: 0.9405646324157715)
[2025-02-12 22:52:22,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:22,386][root][INFO] - Training Epoch: 1/2, step 369/7134 completed (loss: 0.3094203770160675, acc: 0.9264931082725525)
[2025-02-12 22:52:22,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:22,868][root][INFO] - Training Epoch: 1/2, step 370/7134 completed (loss: 0.27358752489089966, acc: 0.9301310181617737)
[2025-02-12 22:52:23,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:23,337][root][INFO] - Training Epoch: 1/2, step 371/7134 completed (loss: 0.22585226595401764, acc: 0.9503759145736694)
[2025-02-12 22:52:23,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:23,775][root][INFO] - Training Epoch: 1/2, step 372/7134 completed (loss: 0.2039221227169037, acc: 0.9390787482261658)
[2025-02-12 22:52:23,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:24,235][root][INFO] - Training Epoch: 1/2, step 373/7134 completed (loss: 0.16916601359844208, acc: 0.9551569223403931)
[2025-02-12 22:52:24,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:24,693][root][INFO] - Training Epoch: 1/2, step 374/7134 completed (loss: 0.30035120248794556, acc: 0.9212218523025513)
[2025-02-12 22:52:24,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:25,151][root][INFO] - Training Epoch: 1/2, step 375/7134 completed (loss: 0.3088230490684509, acc: 0.9223560690879822)
[2025-02-12 22:52:25,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:25,610][root][INFO] - Training Epoch: 1/2, step 376/7134 completed (loss: 0.4368669092655182, acc: 0.8951724171638489)
[2025-02-12 22:52:25,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:26,071][root][INFO] - Training Epoch: 1/2, step 377/7134 completed (loss: 0.3203035295009613, acc: 0.9188445806503296)
[2025-02-12 22:52:26,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:26,516][root][INFO] - Training Epoch: 1/2, step 378/7134 completed (loss: 0.31197449564933777, acc: 0.919618546962738)
[2025-02-12 22:52:26,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:26,995][root][INFO] - Training Epoch: 1/2, step 379/7134 completed (loss: 0.30477187037467957, acc: 0.9195979833602905)
[2025-02-12 22:52:27,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:27,474][root][INFO] - Training Epoch: 1/2, step 380/7134 completed (loss: 0.39467906951904297, acc: 0.8975032567977905)
[2025-02-12 22:52:27,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:27,936][root][INFO] - Training Epoch: 1/2, step 381/7134 completed (loss: 0.33070850372314453, acc: 0.9134367108345032)
[2025-02-12 22:52:28,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:28,404][root][INFO] - Training Epoch: 1/2, step 382/7134 completed (loss: 0.36070385575294495, acc: 0.9062901139259338)
[2025-02-12 22:52:28,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:28,839][root][INFO] - Training Epoch: 1/2, step 383/7134 completed (loss: 0.317648708820343, acc: 0.9262759685516357)
[2025-02-12 22:52:29,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:29,322][root][INFO] - Training Epoch: 1/2, step 384/7134 completed (loss: 0.36051392555236816, acc: 0.9105473756790161)
[2025-02-12 22:52:29,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:29,795][root][INFO] - Training Epoch: 1/2, step 385/7134 completed (loss: 0.22295138239860535, acc: 0.9419152140617371)
[2025-02-12 22:52:29,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:30,175][root][INFO] - Training Epoch: 1/2, step 386/7134 completed (loss: 0.24265576899051666, acc: 0.9345602989196777)
[2025-02-12 22:52:30,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:30,660][root][INFO] - Training Epoch: 1/2, step 387/7134 completed (loss: 0.3916247487068176, acc: 0.8862974047660828)
[2025-02-12 22:52:30,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:31,118][root][INFO] - Training Epoch: 1/2, step 388/7134 completed (loss: 0.2370227873325348, acc: 0.9460093975067139)
[2025-02-12 22:52:31,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:31,599][root][INFO] - Training Epoch: 1/2, step 389/7134 completed (loss: 0.34684956073760986, acc: 0.9063360691070557)
[2025-02-12 22:52:31,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:32,027][root][INFO] - Training Epoch: 1/2, step 390/7134 completed (loss: 0.30499351024627686, acc: 0.9304206967353821)
[2025-02-12 22:52:32,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:32,441][root][INFO] - Training Epoch: 1/2, step 391/7134 completed (loss: 0.282092809677124, acc: 0.930272102355957)
[2025-02-12 22:52:32,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:32,918][root][INFO] - Training Epoch: 1/2, step 392/7134 completed (loss: 0.3442341983318329, acc: 0.897777795791626)
[2025-02-12 22:52:33,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:33,358][root][INFO] - Training Epoch: 1/2, step 393/7134 completed (loss: 0.3576393127441406, acc: 0.9083333611488342)
[2025-02-12 22:52:33,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:33,790][root][INFO] - Training Epoch: 1/2, step 394/7134 completed (loss: 0.44299814105033875, acc: 0.8891050815582275)
[2025-02-12 22:52:33,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:34,239][root][INFO] - Training Epoch: 1/2, step 395/7134 completed (loss: 0.3246309757232666, acc: 0.9218106865882874)
[2025-02-12 22:52:34,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:34,677][root][INFO] - Training Epoch: 1/2, step 396/7134 completed (loss: 0.25983715057373047, acc: 0.9397849440574646)
[2025-02-12 22:52:34,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:35,172][root][INFO] - Training Epoch: 1/2, step 397/7134 completed (loss: 0.32374832034111023, acc: 0.9021164178848267)
[2025-02-12 22:52:35,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:35,631][root][INFO] - Training Epoch: 1/2, step 398/7134 completed (loss: 0.2935832440853119, acc: 0.9252577424049377)
[2025-02-12 22:52:35,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:36,069][root][INFO] - Training Epoch: 1/2, step 399/7134 completed (loss: 0.3162074685096741, acc: 0.9049128293991089)
[2025-02-12 22:52:36,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:36,529][root][INFO] - Training Epoch: 1/2, step 400/7134 completed (loss: 0.3848399221897125, acc: 0.9029850959777832)
[2025-02-12 22:52:36,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:37,018][root][INFO] - Training Epoch: 1/2, step 401/7134 completed (loss: 0.27360808849334717, acc: 0.9129902124404907)
[2025-02-12 22:52:37,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:37,483][root][INFO] - Training Epoch: 1/2, step 402/7134 completed (loss: 0.29723432660102844, acc: 0.91128009557724)
[2025-02-12 22:52:37,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:38,016][root][INFO] - Training Epoch: 1/2, step 403/7134 completed (loss: 0.27482959628105164, acc: 0.9244060516357422)
[2025-02-12 22:52:38,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:38,473][root][INFO] - Training Epoch: 1/2, step 404/7134 completed (loss: 0.27781304717063904, acc: 0.9346153736114502)
[2025-02-12 22:52:38,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:38,970][root][INFO] - Training Epoch: 1/2, step 405/7134 completed (loss: 0.3286326229572296, acc: 0.9190031290054321)
[2025-02-12 22:52:39,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:39,457][root][INFO] - Training Epoch: 1/2, step 406/7134 completed (loss: 0.31464868783950806, acc: 0.9148629307746887)
[2025-02-12 22:52:39,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:39,870][root][INFO] - Training Epoch: 1/2, step 407/7134 completed (loss: 0.4209651052951813, acc: 0.8966101408004761)
[2025-02-12 22:52:40,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:40,323][root][INFO] - Training Epoch: 1/2, step 408/7134 completed (loss: 0.3476729989051819, acc: 0.9135802388191223)
[2025-02-12 22:52:40,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:40,790][root][INFO] - Training Epoch: 1/2, step 409/7134 completed (loss: 0.2624078691005707, acc: 0.9320755004882812)
[2025-02-12 22:52:40,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:41,316][root][INFO] - Training Epoch: 1/2, step 410/7134 completed (loss: 0.30375373363494873, acc: 0.924332320690155)
[2025-02-12 22:52:41,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:41,799][root][INFO] - Training Epoch: 1/2, step 411/7134 completed (loss: 0.31622642278671265, acc: 0.9148446321487427)
[2025-02-12 22:52:41,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:42,267][root][INFO] - Training Epoch: 1/2, step 412/7134 completed (loss: 0.3316193222999573, acc: 0.9118993282318115)
[2025-02-12 22:52:42,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:42,726][root][INFO] - Training Epoch: 1/2, step 413/7134 completed (loss: 0.2943226993083954, acc: 0.9225634336471558)
[2025-02-12 22:52:42,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:43,198][root][INFO] - Training Epoch: 1/2, step 414/7134 completed (loss: 0.2670835554599762, acc: 0.922848641872406)
[2025-02-12 22:52:43,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:43,662][root][INFO] - Training Epoch: 1/2, step 415/7134 completed (loss: 0.22164839506149292, acc: 0.9351851940155029)
[2025-02-12 22:52:43,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:44,158][root][INFO] - Training Epoch: 1/2, step 416/7134 completed (loss: 0.32932335138320923, acc: 0.9220778942108154)
[2025-02-12 22:52:44,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:44,659][root][INFO] - Training Epoch: 1/2, step 417/7134 completed (loss: 0.2757668197154999, acc: 0.9225543737411499)
[2025-02-12 22:52:44,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:45,157][root][INFO] - Training Epoch: 1/2, step 418/7134 completed (loss: 0.22991055250167847, acc: 0.9380022883415222)
[2025-02-12 22:52:45,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:45,625][root][INFO] - Training Epoch: 1/2, step 419/7134 completed (loss: 0.23407527804374695, acc: 0.9292804002761841)
[2025-02-12 22:52:45,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:46,069][root][INFO] - Training Epoch: 1/2, step 420/7134 completed (loss: 0.2415783405303955, acc: 0.9340101480484009)
[2025-02-12 22:52:46,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:46,524][root][INFO] - Training Epoch: 1/2, step 421/7134 completed (loss: 0.24161198735237122, acc: 0.9363057613372803)
[2025-02-12 22:52:46,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:46,988][root][INFO] - Training Epoch: 1/2, step 422/7134 completed (loss: 0.1448800414800644, acc: 0.9647058844566345)
[2025-02-12 22:52:47,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:47,458][root][INFO] - Training Epoch: 1/2, step 423/7134 completed (loss: 0.186019629240036, acc: 0.9514285922050476)
[2025-02-12 22:52:47,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:47,909][root][INFO] - Training Epoch: 1/2, step 424/7134 completed (loss: 0.21951641142368317, acc: 0.9351266026496887)
[2025-02-12 22:52:48,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:48,352][root][INFO] - Training Epoch: 1/2, step 425/7134 completed (loss: 0.19856610894203186, acc: 0.9476743936538696)
[2025-02-12 22:52:48,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:48,787][root][INFO] - Training Epoch: 1/2, step 426/7134 completed (loss: 0.15366706252098083, acc: 0.961002767086029)
[2025-02-12 22:52:48,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:49,211][root][INFO] - Training Epoch: 1/2, step 427/7134 completed (loss: 0.1693263202905655, acc: 0.9559939503669739)
[2025-02-12 22:52:49,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:49,678][root][INFO] - Training Epoch: 1/2, step 428/7134 completed (loss: 0.23465301096439362, acc: 0.9367470145225525)
[2025-02-12 22:52:49,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:50,120][root][INFO] - Training Epoch: 1/2, step 429/7134 completed (loss: 0.23821952939033508, acc: 0.9348171949386597)
[2025-02-12 22:52:50,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:50,577][root][INFO] - Training Epoch: 1/2, step 430/7134 completed (loss: 0.22268004715442657, acc: 0.9397417306900024)
[2025-02-12 22:52:50,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:51,029][root][INFO] - Training Epoch: 1/2, step 431/7134 completed (loss: 0.19507373869419098, acc: 0.9495677351951599)
[2025-02-12 22:52:51,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:51,461][root][INFO] - Training Epoch: 1/2, step 432/7134 completed (loss: 0.13211321830749512, acc: 0.9634369015693665)
[2025-02-12 22:52:51,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:51,960][root][INFO] - Training Epoch: 1/2, step 433/7134 completed (loss: 0.13032104074954987, acc: 0.9646739363670349)
[2025-02-12 22:52:52,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:52,418][root][INFO] - Training Epoch: 1/2, step 434/7134 completed (loss: 0.155716672539711, acc: 0.9633967876434326)
[2025-02-12 22:52:52,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:52,871][root][INFO] - Training Epoch: 1/2, step 435/7134 completed (loss: 0.1645282655954361, acc: 0.9517730474472046)
[2025-02-12 22:52:53,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:53,312][root][INFO] - Training Epoch: 1/2, step 436/7134 completed (loss: 0.20885710418224335, acc: 0.9468504190444946)
[2025-02-12 22:52:53,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:53,723][root][INFO] - Training Epoch: 1/2, step 437/7134 completed (loss: 0.2622525990009308, acc: 0.9381625652313232)
[2025-02-12 22:52:53,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:54,164][root][INFO] - Training Epoch: 1/2, step 438/7134 completed (loss: 0.22158534824848175, acc: 0.9461538195610046)
[2025-02-12 22:52:54,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:54,662][root][INFO] - Training Epoch: 1/2, step 439/7134 completed (loss: 0.17419439554214478, acc: 0.948952853679657)
[2025-02-12 22:52:54,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:55,089][root][INFO] - Training Epoch: 1/2, step 440/7134 completed (loss: 0.2373655140399933, acc: 0.9376083016395569)
[2025-02-12 22:52:55,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:55,547][root][INFO] - Training Epoch: 1/2, step 441/7134 completed (loss: 0.14167022705078125, acc: 0.9505247473716736)
[2025-02-12 22:52:55,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:56,003][root][INFO] - Training Epoch: 1/2, step 442/7134 completed (loss: 0.1310558319091797, acc: 0.9626865386962891)
[2025-02-12 22:52:56,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:56,470][root][INFO] - Training Epoch: 1/2, step 443/7134 completed (loss: 0.17213575541973114, acc: 0.9503875970840454)
[2025-02-12 22:52:56,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:56,915][root][INFO] - Training Epoch: 1/2, step 444/7134 completed (loss: 0.1348288655281067, acc: 0.9583333134651184)
[2025-02-12 22:52:57,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:57,352][root][INFO] - Training Epoch: 1/2, step 445/7134 completed (loss: 0.19682493805885315, acc: 0.9489796161651611)
[2025-02-12 22:52:57,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:57,807][root][INFO] - Training Epoch: 1/2, step 446/7134 completed (loss: 0.11979475617408752, acc: 0.9623955488204956)
[2025-02-12 22:52:57,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:58,265][root][INFO] - Training Epoch: 1/2, step 447/7134 completed (loss: 0.13394969701766968, acc: 0.9639389514923096)
[2025-02-12 22:52:58,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:58,737][root][INFO] - Training Epoch: 1/2, step 448/7134 completed (loss: 0.28921765089035034, acc: 0.9207161068916321)
[2025-02-12 22:52:58,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:59,210][root][INFO] - Training Epoch: 1/2, step 449/7134 completed (loss: 0.27568021416664124, acc: 0.9293193817138672)
[2025-02-12 22:52:59,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:52:59,688][root][INFO] - Training Epoch: 1/2, step 450/7134 completed (loss: 0.36212384700775146, acc: 0.9065817594528198)
[2025-02-12 22:52:59,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:00,163][root][INFO] - Training Epoch: 1/2, step 451/7134 completed (loss: 0.40708231925964355, acc: 0.8929845690727234)
[2025-02-12 22:53:00,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:00,634][root][INFO] - Training Epoch: 1/2, step 452/7134 completed (loss: 0.43696802854537964, acc: 0.8767123222351074)
[2025-02-12 22:53:00,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:01,055][root][INFO] - Training Epoch: 1/2, step 453/7134 completed (loss: 0.30770134925842285, acc: 0.9174311757087708)
[2025-02-12 22:53:01,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:01,530][root][INFO] - Training Epoch: 1/2, step 454/7134 completed (loss: 0.36068692803382874, acc: 0.9063509106636047)
[2025-02-12 22:53:01,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:01,999][root][INFO] - Training Epoch: 1/2, step 455/7134 completed (loss: 0.34570643305778503, acc: 0.9045751690864563)
[2025-02-12 22:53:02,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:02,470][root][INFO] - Training Epoch: 1/2, step 456/7134 completed (loss: 0.26588404178619385, acc: 0.9235127568244934)
[2025-02-12 22:53:02,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:02,939][root][INFO] - Training Epoch: 1/2, step 457/7134 completed (loss: 0.28140729665756226, acc: 0.9177852272987366)
[2025-02-12 22:53:03,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:03,376][root][INFO] - Training Epoch: 1/2, step 458/7134 completed (loss: 0.22247470915317535, acc: 0.9304174780845642)
[2025-02-12 22:53:03,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:03,821][root][INFO] - Training Epoch: 1/2, step 459/7134 completed (loss: 0.38624873757362366, acc: 0.9081481695175171)
[2025-02-12 22:53:03,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:04,270][root][INFO] - Training Epoch: 1/2, step 460/7134 completed (loss: 0.21426230669021606, acc: 0.9448819160461426)
[2025-02-12 22:53:04,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:04,735][root][INFO] - Training Epoch: 1/2, step 461/7134 completed (loss: 0.3076563775539398, acc: 0.9186535477638245)
[2025-02-12 22:53:04,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:05,247][root][INFO] - Training Epoch: 1/2, step 462/7134 completed (loss: 0.34462979435920715, acc: 0.9018691778182983)
[2025-02-12 22:53:05,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:05,766][root][INFO] - Training Epoch: 1/2, step 463/7134 completed (loss: 0.30267953872680664, acc: 0.9171894788742065)
[2025-02-12 22:53:05,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:06,254][root][INFO] - Training Epoch: 1/2, step 464/7134 completed (loss: 0.2456587255001068, acc: 0.9334277510643005)
[2025-02-12 22:53:06,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:06,692][root][INFO] - Training Epoch: 1/2, step 465/7134 completed (loss: 0.2484835386276245, acc: 0.9281984567642212)
[2025-02-12 22:53:06,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:07,181][root][INFO] - Training Epoch: 1/2, step 466/7134 completed (loss: 0.36858540773391724, acc: 0.9086229205131531)
[2025-02-12 22:53:07,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:07,650][root][INFO] - Training Epoch: 1/2, step 467/7134 completed (loss: 0.32727593183517456, acc: 0.9139297604560852)
[2025-02-12 22:53:07,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:08,083][root][INFO] - Training Epoch: 1/2, step 468/7134 completed (loss: 0.3580586612224579, acc: 0.9078404307365417)
[2025-02-12 22:53:08,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:08,527][root][INFO] - Training Epoch: 1/2, step 469/7134 completed (loss: 0.2354230433702469, acc: 0.9337176084518433)
[2025-02-12 22:53:08,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:08,963][root][INFO] - Training Epoch: 1/2, step 470/7134 completed (loss: 0.3484809994697571, acc: 0.8884353637695312)
[2025-02-12 22:53:09,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:09,420][root][INFO] - Training Epoch: 1/2, step 471/7134 completed (loss: 0.2688971161842346, acc: 0.9230769276618958)
[2025-02-12 22:53:09,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:09,881][root][INFO] - Training Epoch: 1/2, step 472/7134 completed (loss: 0.29358965158462524, acc: 0.9222874045372009)
[2025-02-12 22:53:10,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:10,325][root][INFO] - Training Epoch: 1/2, step 473/7134 completed (loss: 0.24384015798568726, acc: 0.9259259104728699)
[2025-02-12 22:53:10,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:10,781][root][INFO] - Training Epoch: 1/2, step 474/7134 completed (loss: 0.21197739243507385, acc: 0.9328063130378723)
[2025-02-12 22:53:10,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:11,257][root][INFO] - Training Epoch: 1/2, step 475/7134 completed (loss: 0.3795958161354065, acc: 0.9014706015586853)
[2025-02-12 22:53:11,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:11,716][root][INFO] - Training Epoch: 1/2, step 476/7134 completed (loss: 0.32218003273010254, acc: 0.9083094596862793)
[2025-02-12 22:53:11,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:12,208][root][INFO] - Training Epoch: 1/2, step 477/7134 completed (loss: 0.24220147728919983, acc: 0.932762861251831)
[2025-02-12 22:53:12,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:12,625][root][INFO] - Training Epoch: 1/2, step 478/7134 completed (loss: 0.3534422814846039, acc: 0.9133192300796509)
[2025-02-12 22:53:12,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:13,049][root][INFO] - Training Epoch: 1/2, step 479/7134 completed (loss: 0.16796468198299408, acc: 0.9540889263153076)
[2025-02-12 22:53:13,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:13,474][root][INFO] - Training Epoch: 1/2, step 480/7134 completed (loss: 0.16861289739608765, acc: 0.9518248438835144)
[2025-02-12 22:53:13,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:13,934][root][INFO] - Training Epoch: 1/2, step 481/7134 completed (loss: 0.22024433314800262, acc: 0.9401041865348816)
[2025-02-12 22:53:14,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:14,427][root][INFO] - Training Epoch: 1/2, step 482/7134 completed (loss: 0.15129387378692627, acc: 0.9628571271896362)
[2025-02-12 22:53:14,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:14,896][root][INFO] - Training Epoch: 1/2, step 483/7134 completed (loss: 0.17961934208869934, acc: 0.9508599638938904)
[2025-02-12 22:53:15,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:15,340][root][INFO] - Training Epoch: 1/2, step 484/7134 completed (loss: 0.22232024371623993, acc: 0.9371657967567444)
[2025-02-12 22:53:15,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:15,803][root][INFO] - Training Epoch: 1/2, step 485/7134 completed (loss: 0.2555858790874481, acc: 0.9364598989486694)
[2025-02-12 22:53:15,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:16,245][root][INFO] - Training Epoch: 1/2, step 486/7134 completed (loss: 0.34113049507141113, acc: 0.9292762875556946)
[2025-02-12 22:53:16,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:16,721][root][INFO] - Training Epoch: 1/2, step 487/7134 completed (loss: 0.18647249042987823, acc: 0.9456681609153748)
[2025-02-12 22:53:16,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:17,165][root][INFO] - Training Epoch: 1/2, step 488/7134 completed (loss: 0.283821702003479, acc: 0.9202898740768433)
[2025-02-12 22:53:17,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:17,628][root][INFO] - Training Epoch: 1/2, step 489/7134 completed (loss: 0.18752096593379974, acc: 0.9546218514442444)
[2025-02-12 22:53:17,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:18,067][root][INFO] - Training Epoch: 1/2, step 490/7134 completed (loss: 0.1655084639787674, acc: 0.956462562084198)
[2025-02-12 22:53:18,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:18,478][root][INFO] - Training Epoch: 1/2, step 491/7134 completed (loss: 0.14532437920570374, acc: 0.9621710777282715)
[2025-02-12 22:53:18,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:18,903][root][INFO] - Training Epoch: 1/2, step 492/7134 completed (loss: 0.12358967959880829, acc: 0.9606656432151794)
[2025-02-12 22:53:19,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:19,370][root][INFO] - Training Epoch: 1/2, step 493/7134 completed (loss: 0.2563221752643585, acc: 0.922636091709137)
[2025-02-12 22:53:19,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:19,828][root][INFO] - Training Epoch: 1/2, step 494/7134 completed (loss: 0.1692076027393341, acc: 0.9593750238418579)
[2025-02-12 22:53:19,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:20,281][root][INFO] - Training Epoch: 1/2, step 495/7134 completed (loss: 0.1559249609708786, acc: 0.9666666388511658)
[2025-02-12 22:53:20,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:20,679][root][INFO] - Training Epoch: 1/2, step 496/7134 completed (loss: 0.18427591025829315, acc: 0.9457994699478149)
[2025-02-12 22:53:20,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:21,119][root][INFO] - Training Epoch: 1/2, step 497/7134 completed (loss: 0.1696871668100357, acc: 0.9554263353347778)
[2025-02-12 22:53:21,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:21,562][root][INFO] - Training Epoch: 1/2, step 498/7134 completed (loss: 0.16744528710842133, acc: 0.9583333134651184)
[2025-02-12 22:53:21,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:22,040][root][INFO] - Training Epoch: 1/2, step 499/7134 completed (loss: 0.09597017616033554, acc: 0.9749652147293091)
[2025-02-12 22:53:22,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:22,472][root][INFO] - Training Epoch: 1/2, step 500/7134 completed (loss: 0.1566895693540573, acc: 0.9626666903495789)
[2025-02-12 22:53:22,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:22,957][root][INFO] - Training Epoch: 1/2, step 501/7134 completed (loss: 0.10691846162080765, acc: 0.9639889001846313)
[2025-02-12 22:53:23,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:23,376][root][INFO] - Training Epoch: 1/2, step 502/7134 completed (loss: 0.08529698848724365, acc: 0.9785932898521423)
[2025-02-12 22:53:23,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:23,813][root][INFO] - Training Epoch: 1/2, step 503/7134 completed (loss: 0.16292694211006165, acc: 0.9557046890258789)
[2025-02-12 22:53:23,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:24,269][root][INFO] - Training Epoch: 1/2, step 504/7134 completed (loss: 0.10164753347635269, acc: 0.9678770899772644)
[2025-02-12 22:53:24,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:24,739][root][INFO] - Training Epoch: 1/2, step 505/7134 completed (loss: 0.06589478254318237, acc: 0.981675386428833)
[2025-02-12 22:53:24,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:25,246][root][INFO] - Training Epoch: 1/2, step 506/7134 completed (loss: 0.16567951440811157, acc: 0.9490049481391907)
[2025-02-12 22:53:25,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:25,740][root][INFO] - Training Epoch: 1/2, step 507/7134 completed (loss: 0.1277385950088501, acc: 0.9684210419654846)
[2025-02-12 22:53:25,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:26,207][root][INFO] - Training Epoch: 1/2, step 508/7134 completed (loss: 0.17824526131153107, acc: 0.9539105892181396)
[2025-02-12 22:53:26,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:26,709][root][INFO] - Training Epoch: 1/2, step 509/7134 completed (loss: 0.2221803516149521, acc: 0.9397217631340027)
[2025-02-12 22:53:26,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:27,132][root][INFO] - Training Epoch: 1/2, step 510/7134 completed (loss: 0.34479212760925293, acc: 0.9168398976325989)
[2025-02-12 22:53:27,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:27,547][root][INFO] - Training Epoch: 1/2, step 511/7134 completed (loss: 0.32625243067741394, acc: 0.9168704152107239)
[2025-02-12 22:53:27,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:27,977][root][INFO] - Training Epoch: 1/2, step 512/7134 completed (loss: 0.2952621579170227, acc: 0.9298596978187561)
[2025-02-12 22:53:28,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:28,408][root][INFO] - Training Epoch: 1/2, step 513/7134 completed (loss: 0.23778773844242096, acc: 0.9242684841156006)
[2025-02-12 22:53:28,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:28,780][root][INFO] - Training Epoch: 1/2, step 514/7134 completed (loss: 0.3633720278739929, acc: 0.9145728349685669)
[2025-02-12 22:53:28,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:29,263][root][INFO] - Training Epoch: 1/2, step 515/7134 completed (loss: 0.20810624957084656, acc: 0.9448373317718506)
[2025-02-12 22:53:29,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:29,656][root][INFO] - Training Epoch: 1/2, step 516/7134 completed (loss: 0.1625848114490509, acc: 0.95652174949646)
[2025-02-12 22:53:29,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:30,131][root][INFO] - Training Epoch: 1/2, step 517/7134 completed (loss: 0.277385413646698, acc: 0.9354838728904724)
[2025-02-12 22:53:30,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:30,589][root][INFO] - Training Epoch: 1/2, step 518/7134 completed (loss: 0.1403483748435974, acc: 0.9595202207565308)
[2025-02-12 22:53:30,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:31,065][root][INFO] - Training Epoch: 1/2, step 519/7134 completed (loss: 0.19077736139297485, acc: 0.9422680139541626)
[2025-02-12 22:53:31,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:31,996][root][INFO] - Training Epoch: 1/2, step 520/7134 completed (loss: 0.18136684596538544, acc: 0.9462025165557861)
[2025-02-12 22:53:32,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:32,843][root][INFO] - Training Epoch: 1/2, step 521/7134 completed (loss: 0.21175870299339294, acc: 0.9412550330162048)
[2025-02-12 22:53:33,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:33,307][root][INFO] - Training Epoch: 1/2, step 522/7134 completed (loss: 0.1901717334985733, acc: 0.9557640552520752)
[2025-02-12 22:53:33,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:33,790][root][INFO] - Training Epoch: 1/2, step 523/7134 completed (loss: 0.19188281893730164, acc: 0.9473684430122375)
[2025-02-12 22:53:33,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:34,263][root][INFO] - Training Epoch: 1/2, step 524/7134 completed (loss: 0.15036015212535858, acc: 0.9610214829444885)
[2025-02-12 22:53:34,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:47,552][root][INFO] - Training Epoch: 1/2, step 525/7134 completed (loss: 0.251597136259079, acc: 0.9304482340812683)
[2025-02-12 22:53:50,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:50,509][root][INFO] - Training Epoch: 1/2, step 526/7134 completed (loss: 0.16603407263755798, acc: 0.9589040875434875)
[2025-02-12 22:53:50,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:50,939][root][INFO] - Training Epoch: 1/2, step 527/7134 completed (loss: 0.19092543423175812, acc: 0.9477611780166626)
[2025-02-12 22:53:51,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:51,402][root][INFO] - Training Epoch: 1/2, step 528/7134 completed (loss: 0.0888943076133728, acc: 0.9818913340568542)
[2025-02-12 22:53:51,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:51,992][root][INFO] - Training Epoch: 1/2, step 529/7134 completed (loss: 0.23447492718696594, acc: 0.9301587343215942)
[2025-02-12 22:53:52,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:52,433][root][INFO] - Training Epoch: 1/2, step 530/7134 completed (loss: 0.1791864037513733, acc: 0.9468267560005188)
[2025-02-12 22:53:52,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:52,909][root][INFO] - Training Epoch: 1/2, step 531/7134 completed (loss: 0.19798406958580017, acc: 0.952136754989624)
[2025-02-12 22:53:53,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:53,337][root][INFO] - Training Epoch: 1/2, step 532/7134 completed (loss: 0.10331506282091141, acc: 0.9729729890823364)
[2025-02-12 22:53:53,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:54,009][root][INFO] - Training Epoch: 1/2, step 533/7134 completed (loss: 0.1676742136478424, acc: 0.9450171589851379)
[2025-02-12 22:53:54,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:54,639][root][INFO] - Training Epoch: 1/2, step 534/7134 completed (loss: 0.12392441183328629, acc: 0.9615384340286255)
[2025-02-12 22:53:56,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:58,648][root][INFO] - Training Epoch: 1/2, step 535/7134 completed (loss: 0.15191322565078735, acc: 0.95782071352005)
[2025-02-12 22:53:58,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:53:59,277][root][INFO] - Training Epoch: 1/2, step 536/7134 completed (loss: 0.20396998524665833, acc: 0.9547445178031921)
[2025-02-12 22:53:59,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:04,335][root][INFO] - Training Epoch: 1/2, step 537/7134 completed (loss: 0.19541338086128235, acc: 0.9528718590736389)
[2025-02-12 22:54:07,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:20,138][root][INFO] - Training Epoch: 1/2, step 538/7134 completed (loss: 0.17420127987861633, acc: 0.9554896354675293)
[2025-02-12 22:54:20,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:20,857][root][INFO] - Training Epoch: 1/2, step 539/7134 completed (loss: 0.1670304238796234, acc: 0.9602076411247253)
[2025-02-12 22:54:20,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:21,291][root][INFO] - Training Epoch: 1/2, step 540/7134 completed (loss: 0.17873823642730713, acc: 0.9575757384300232)
[2025-02-12 22:54:21,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:21,779][root][INFO] - Training Epoch: 1/2, step 541/7134 completed (loss: 0.2631273865699768, acc: 0.9308510422706604)
[2025-02-12 22:54:21,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:22,183][root][INFO] - Training Epoch: 1/2, step 542/7134 completed (loss: 0.2758949398994446, acc: 0.9178403615951538)
[2025-02-12 22:54:22,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:22,633][root][INFO] - Training Epoch: 1/2, step 543/7134 completed (loss: 0.26780661940574646, acc: 0.9240506291389465)
[2025-02-12 22:54:22,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:23,278][root][INFO] - Training Epoch: 1/2, step 544/7134 completed (loss: 0.30544382333755493, acc: 0.9132491946220398)
[2025-02-12 22:54:23,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:23,981][root][INFO] - Training Epoch: 1/2, step 545/7134 completed (loss: 0.2825355529785156, acc: 0.9275362491607666)
[2025-02-12 22:54:24,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:24,440][root][INFO] - Training Epoch: 1/2, step 546/7134 completed (loss: 0.268411248922348, acc: 0.9243499040603638)
[2025-02-12 22:54:24,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:24,912][root][INFO] - Training Epoch: 1/2, step 547/7134 completed (loss: 0.20881865918636322, acc: 0.944847583770752)
[2025-02-12 22:54:25,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:25,358][root][INFO] - Training Epoch: 1/2, step 548/7134 completed (loss: 0.13284261524677277, acc: 0.9595828056335449)
[2025-02-12 22:54:25,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:25,805][root][INFO] - Training Epoch: 1/2, step 549/7134 completed (loss: 0.2377156764268875, acc: 0.9316770434379578)
[2025-02-12 22:54:25,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:26,248][root][INFO] - Training Epoch: 1/2, step 550/7134 completed (loss: 0.15413320064544678, acc: 0.9624573588371277)
[2025-02-12 22:54:26,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:26,637][root][INFO] - Training Epoch: 1/2, step 551/7134 completed (loss: 0.2242926061153412, acc: 0.9373849034309387)
[2025-02-12 22:54:26,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:27,064][root][INFO] - Training Epoch: 1/2, step 552/7134 completed (loss: 0.18450619280338287, acc: 0.9558404684066772)
[2025-02-12 22:54:27,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:27,477][root][INFO] - Training Epoch: 1/2, step 553/7134 completed (loss: 0.16617067158222198, acc: 0.9583333134651184)
[2025-02-12 22:54:27,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:27,974][root][INFO] - Training Epoch: 1/2, step 554/7134 completed (loss: 0.16719694435596466, acc: 0.9614835977554321)
[2025-02-12 22:54:28,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:28,413][root][INFO] - Training Epoch: 1/2, step 555/7134 completed (loss: 0.20463401079177856, acc: 0.9462875127792358)
[2025-02-12 22:54:28,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:28,849][root][INFO] - Training Epoch: 1/2, step 556/7134 completed (loss: 0.12456348538398743, acc: 0.9702970385551453)
[2025-02-12 22:54:28,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:29,274][root][INFO] - Training Epoch: 1/2, step 557/7134 completed (loss: 0.1851990818977356, acc: 0.9475465416908264)
[2025-02-12 22:54:29,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:29,724][root][INFO] - Training Epoch: 1/2, step 558/7134 completed (loss: 0.1614595204591751, acc: 0.9560283422470093)
[2025-02-12 22:54:30,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:30,342][root][INFO] - Training Epoch: 1/2, step 559/7134 completed (loss: 0.22835707664489746, acc: 0.9316239356994629)
[2025-02-12 22:54:30,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:30,806][root][INFO] - Training Epoch: 1/2, step 560/7134 completed (loss: 0.15854257345199585, acc: 0.9525483250617981)
[2025-02-12 22:54:31,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:31,321][root][INFO] - Training Epoch: 1/2, step 561/7134 completed (loss: 0.20072467625141144, acc: 0.9399744868278503)
[2025-02-12 22:54:31,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:31,751][root][INFO] - Training Epoch: 1/2, step 562/7134 completed (loss: 0.1496887505054474, acc: 0.9549669027328491)
[2025-02-12 22:54:31,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:32,175][root][INFO] - Training Epoch: 1/2, step 563/7134 completed (loss: 0.27359727025032043, acc: 0.922468364238739)
[2025-02-12 22:54:32,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:32,672][root][INFO] - Training Epoch: 1/2, step 564/7134 completed (loss: 0.17693398892879486, acc: 0.9493201375007629)
[2025-02-12 22:54:32,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:54:38,765][root][INFO] - Training Epoch: 1/2, step 565/7134 completed (loss: 0.17456604540348053, acc: 0.9564489126205444)
[2025-02-13 02:23:34,876][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-13 02:23:34,876][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-13 02:23:34,877][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme'}
[2025-02-13 02:23:34,877][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-13_02-23-34.txt', 'log_interval': 5}
[2025-02-13 02:23:54,191][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-13 02:24:00,855][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 02:24:00,858][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-13 02:24:00,860][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 02:24:00,861][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-13 02:24:05,093][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 02:24:05,094][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-13 02:24:05,094][slam_llm.models.slam_model][INFO] - setup peft...
[2025-02-13 02:24:05,216][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 02:24:05,218][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-13 02:24:05,323][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-13 02:24:05,323][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-13 02:24:05,324][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_1.0206900835037231/model.pt
[2025-02-13 02:24:05,504][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-13 02:24:05,508][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-13 02:24:08,552][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-13 02:24:09,450][root][INFO] - --> Training Set Length = 28539
[2025-02-13 02:24:09,462][root][INFO] - --> Validation Set Length = 2703
[2025-02-13 02:24:09,462][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 02:24:09,463][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 02:24:11,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:11,870][root][INFO] - Training Epoch: 1/2, step 0/7134 completed (loss: 2.260176420211792, acc: 0.5557206273078918)
[2025-02-13 02:24:12,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:12,481][root][INFO] - Training Epoch: 1/2, step 1/7134 completed (loss: 2.431884288787842, acc: 0.509065568447113)
[2025-02-13 02:24:12,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:12,949][root][INFO] - Training Epoch: 1/2, step 2/7134 completed (loss: 2.3166098594665527, acc: 0.4939759075641632)
[2025-02-13 02:24:13,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:13,392][root][INFO] - Training Epoch: 1/2, step 3/7134 completed (loss: 2.176783800125122, acc: 0.5211267471313477)
[2025-02-13 02:24:13,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:13,834][root][INFO] - Training Epoch: 1/2, step 4/7134 completed (loss: 2.3043711185455322, acc: 0.48567530512809753)
[2025-02-13 02:24:13,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:14,286][root][INFO] - Training Epoch: 1/2, step 5/7134 completed (loss: 2.358380079269409, acc: 0.5038071274757385)
[2025-02-13 02:24:14,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:14,727][root][INFO] - Training Epoch: 1/2, step 6/7134 completed (loss: 2.2279136180877686, acc: 0.5072231292724609)
[2025-02-13 02:24:14,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:15,198][root][INFO] - Training Epoch: 1/2, step 7/7134 completed (loss: 2.4676787853240967, acc: 0.4769614040851593)
[2025-02-13 02:24:15,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:15,688][root][INFO] - Training Epoch: 1/2, step 8/7134 completed (loss: 2.348905086517334, acc: 0.47867950797080994)
[2025-02-13 02:24:15,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:16,153][root][INFO] - Training Epoch: 1/2, step 9/7134 completed (loss: 2.397805690765381, acc: 0.5240309834480286)
[2025-02-13 02:24:16,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:16,665][root][INFO] - Training Epoch: 1/2, step 10/7134 completed (loss: 2.597505807876587, acc: 0.4509246051311493)
[2025-02-13 02:24:16,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:17,152][root][INFO] - Training Epoch: 1/2, step 11/7134 completed (loss: 2.2379262447357178, acc: 0.5365853905677795)
[2025-02-13 02:24:17,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:17,633][root][INFO] - Training Epoch: 1/2, step 12/7134 completed (loss: 2.201561212539673, acc: 0.5006729364395142)
[2025-02-13 02:24:17,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:18,071][root][INFO] - Training Epoch: 1/2, step 13/7134 completed (loss: 2.154956102371216, acc: 0.5158184170722961)
[2025-02-13 02:24:18,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:18,506][root][INFO] - Training Epoch: 1/2, step 14/7134 completed (loss: 2.2271029949188232, acc: 0.5512605309486389)
[2025-02-13 02:24:18,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:18,921][root][INFO] - Training Epoch: 1/2, step 15/7134 completed (loss: 2.1439809799194336, acc: 0.5611015558242798)
[2025-02-13 02:24:19,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:19,365][root][INFO] - Training Epoch: 1/2, step 16/7134 completed (loss: 2.1645631790161133, acc: 0.5528455376625061)
[2025-02-13 02:24:19,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:19,761][root][INFO] - Training Epoch: 1/2, step 17/7134 completed (loss: 2.3043055534362793, acc: 0.5540540814399719)
[2025-02-13 02:24:19,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:20,201][root][INFO] - Training Epoch: 1/2, step 18/7134 completed (loss: 2.2217345237731934, acc: 0.5255681872367859)
[2025-02-13 02:24:20,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:20,636][root][INFO] - Training Epoch: 1/2, step 19/7134 completed (loss: 2.293184280395508, acc: 0.5075861811637878)
[2025-02-13 02:24:20,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:21,060][root][INFO] - Training Epoch: 1/2, step 20/7134 completed (loss: 2.0943543910980225, acc: 0.5263158082962036)
[2025-02-13 02:24:21,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:21,527][root][INFO] - Training Epoch: 1/2, step 21/7134 completed (loss: 2.1166024208068848, acc: 0.5025773048400879)
[2025-02-13 02:24:21,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:21,979][root][INFO] - Training Epoch: 1/2, step 22/7134 completed (loss: 2.0580027103424072, acc: 0.5403458476066589)
[2025-02-13 02:24:22,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:22,410][root][INFO] - Training Epoch: 1/2, step 23/7134 completed (loss: 1.9904992580413818, acc: 0.55027174949646)
[2025-02-13 02:24:22,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:22,843][root][INFO] - Training Epoch: 1/2, step 24/7134 completed (loss: 1.9730256795883179, acc: 0.5529248118400574)
[2025-02-13 02:24:23,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:23,335][root][INFO] - Training Epoch: 1/2, step 25/7134 completed (loss: 1.9693783521652222, acc: 0.5340909361839294)
[2025-02-13 02:24:23,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:23,798][root][INFO] - Training Epoch: 1/2, step 26/7134 completed (loss: 1.8654817342758179, acc: 0.5648967623710632)
[2025-02-13 02:24:23,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:24,235][root][INFO] - Training Epoch: 1/2, step 27/7134 completed (loss: 1.7373212575912476, acc: 0.6129985451698303)
[2025-02-13 02:24:24,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:24,724][root][INFO] - Training Epoch: 1/2, step 28/7134 completed (loss: 2.1121156215667725, acc: 0.5325301289558411)
[2025-02-13 02:24:24,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:25,198][root][INFO] - Training Epoch: 1/2, step 29/7134 completed (loss: 2.1882026195526123, acc: 0.5099149942398071)
[2025-02-13 02:24:25,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:25,695][root][INFO] - Training Epoch: 1/2, step 30/7134 completed (loss: 2.04516863822937, acc: 0.5220338702201843)
[2025-02-13 02:24:25,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:26,155][root][INFO] - Training Epoch: 1/2, step 31/7134 completed (loss: 2.0604817867279053, acc: 0.4925847351551056)
[2025-02-13 02:24:26,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:26,634][root][INFO] - Training Epoch: 1/2, step 32/7134 completed (loss: 2.1946587562561035, acc: 0.459037721157074)
[2025-02-13 02:24:26,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:27,026][root][INFO] - Training Epoch: 1/2, step 33/7134 completed (loss: 2.1200785636901855, acc: 0.5358565449714661)
[2025-02-13 02:24:27,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:27,495][root][INFO] - Training Epoch: 1/2, step 34/7134 completed (loss: 2.0987069606781006, acc: 0.5310668349266052)
[2025-02-13 02:24:27,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:28,020][root][INFO] - Training Epoch: 1/2, step 35/7134 completed (loss: 1.9443883895874023, acc: 0.5117813348770142)
[2025-02-13 02:24:28,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:28,521][root][INFO] - Training Epoch: 1/2, step 36/7134 completed (loss: 1.9567351341247559, acc: 0.5407165884971619)
[2025-02-13 02:24:28,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:28,994][root][INFO] - Training Epoch: 1/2, step 37/7134 completed (loss: 1.7448383569717407, acc: 0.5544430613517761)
[2025-02-13 02:24:29,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:29,425][root][INFO] - Training Epoch: 1/2, step 38/7134 completed (loss: 1.817940354347229, acc: 0.5565476417541504)
[2025-02-13 02:24:29,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:29,882][root][INFO] - Training Epoch: 1/2, step 39/7134 completed (loss: 1.6201969385147095, acc: 0.5872800946235657)
[2025-02-13 02:24:30,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:30,298][root][INFO] - Training Epoch: 1/2, step 40/7134 completed (loss: 1.824886679649353, acc: 0.5896860957145691)
[2025-02-13 02:24:30,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:30,712][root][INFO] - Training Epoch: 1/2, step 41/7134 completed (loss: 1.7734125852584839, acc: 0.582524299621582)
[2025-02-13 02:24:30,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:31,154][root][INFO] - Training Epoch: 1/2, step 42/7134 completed (loss: 1.6469498872756958, acc: 0.5722300410270691)
[2025-02-13 02:24:31,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:31,570][root][INFO] - Training Epoch: 1/2, step 43/7134 completed (loss: 1.4634170532226562, acc: 0.6180645227432251)
[2025-02-13 02:24:31,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:32,010][root][INFO] - Training Epoch: 1/2, step 44/7134 completed (loss: 1.5499396324157715, acc: 0.6203821897506714)
[2025-02-13 02:24:32,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:32,502][root][INFO] - Training Epoch: 1/2, step 45/7134 completed (loss: 1.5852160453796387, acc: 0.5893824696540833)
[2025-02-13 02:24:32,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:32,956][root][INFO] - Training Epoch: 1/2, step 46/7134 completed (loss: 1.5565123558044434, acc: 0.6359999775886536)
[2025-02-13 02:24:33,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:33,435][root][INFO] - Training Epoch: 1/2, step 47/7134 completed (loss: 1.6031157970428467, acc: 0.6421356201171875)
[2025-02-13 02:24:33,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:33,943][root][INFO] - Training Epoch: 1/2, step 48/7134 completed (loss: 1.439233422279358, acc: 0.6235954761505127)
[2025-02-13 02:24:34,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:34,410][root][INFO] - Training Epoch: 1/2, step 49/7134 completed (loss: 1.3644226789474487, acc: 0.6241426467895508)
[2025-02-13 02:24:34,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:34,902][root][INFO] - Training Epoch: 1/2, step 50/7134 completed (loss: 1.4969511032104492, acc: 0.6136101484298706)
[2025-02-13 02:24:35,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:35,325][root][INFO] - Training Epoch: 1/2, step 51/7134 completed (loss: 1.3278170824050903, acc: 0.6713197827339172)
[2025-02-13 02:24:35,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:35,735][root][INFO] - Training Epoch: 1/2, step 52/7134 completed (loss: 1.3975400924682617, acc: 0.6520324945449829)
[2025-02-13 02:24:35,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:36,153][root][INFO] - Training Epoch: 1/2, step 53/7134 completed (loss: 1.3571503162384033, acc: 0.6325459480285645)
[2025-02-13 02:24:36,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:36,624][root][INFO] - Training Epoch: 1/2, step 54/7134 completed (loss: 1.2623183727264404, acc: 0.6651323437690735)
[2025-02-13 02:24:36,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:37,058][root][INFO] - Training Epoch: 1/2, step 55/7134 completed (loss: 1.2769818305969238, acc: 0.6598984599113464)
[2025-02-13 02:24:37,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:37,521][root][INFO] - Training Epoch: 1/2, step 56/7134 completed (loss: 1.25783109664917, acc: 0.6660714149475098)
[2025-02-13 02:24:37,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:37,989][root][INFO] - Training Epoch: 1/2, step 57/7134 completed (loss: 1.3061635494232178, acc: 0.6583850979804993)
[2025-02-13 02:24:38,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:38,468][root][INFO] - Training Epoch: 1/2, step 58/7134 completed (loss: 1.3548699617385864, acc: 0.6338028311729431)
[2025-02-13 02:24:38,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:38,941][root][INFO] - Training Epoch: 1/2, step 59/7134 completed (loss: 1.159551978111267, acc: 0.6843971610069275)
[2025-02-13 02:24:39,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:39,423][root][INFO] - Training Epoch: 1/2, step 60/7134 completed (loss: 1.3833616971969604, acc: 0.6292613744735718)
[2025-02-13 02:24:39,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:39,873][root][INFO] - Training Epoch: 1/2, step 61/7134 completed (loss: 1.2818470001220703, acc: 0.6413708925247192)
[2025-02-13 02:24:40,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:40,320][root][INFO] - Training Epoch: 1/2, step 62/7134 completed (loss: 1.1979732513427734, acc: 0.6721727848052979)
[2025-02-13 02:24:40,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:40,777][root][INFO] - Training Epoch: 1/2, step 63/7134 completed (loss: 1.3369488716125488, acc: 0.6525529026985168)
[2025-02-13 02:24:40,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:41,271][root][INFO] - Training Epoch: 1/2, step 64/7134 completed (loss: 1.3561406135559082, acc: 0.647849440574646)
[2025-02-13 02:24:41,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:41,776][root][INFO] - Training Epoch: 1/2, step 65/7134 completed (loss: 1.4270232915878296, acc: 0.601965606212616)
[2025-02-13 02:24:41,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:42,222][root][INFO] - Training Epoch: 1/2, step 66/7134 completed (loss: 1.3063743114471436, acc: 0.6559139490127563)
[2025-02-13 02:24:42,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:42,695][root][INFO] - Training Epoch: 1/2, step 67/7134 completed (loss: 1.203667163848877, acc: 0.671895444393158)
[2025-02-13 02:24:42,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:43,133][root][INFO] - Training Epoch: 1/2, step 68/7134 completed (loss: 1.3231706619262695, acc: 0.6567742228507996)
[2025-02-13 02:24:43,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:43,587][root][INFO] - Training Epoch: 1/2, step 69/7134 completed (loss: 1.2546907663345337, acc: 0.665835440158844)
[2025-02-13 02:24:43,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:44,036][root][INFO] - Training Epoch: 1/2, step 70/7134 completed (loss: 1.2133756875991821, acc: 0.6701754331588745)
[2025-02-13 02:24:44,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:44,476][root][INFO] - Training Epoch: 1/2, step 71/7134 completed (loss: 1.3619149923324585, acc: 0.6554622054100037)
[2025-02-13 02:24:44,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:44,933][root][INFO] - Training Epoch: 1/2, step 72/7134 completed (loss: 1.3391122817993164, acc: 0.6496163606643677)
[2025-02-13 02:24:45,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:45,347][root][INFO] - Training Epoch: 1/2, step 73/7134 completed (loss: 1.326568603515625, acc: 0.6521739363670349)
[2025-02-13 02:24:45,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:45,789][root][INFO] - Training Epoch: 1/2, step 74/7134 completed (loss: 1.3764866590499878, acc: 0.6601671576499939)
[2025-02-13 02:24:45,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:46,274][root][INFO] - Training Epoch: 1/2, step 75/7134 completed (loss: 1.42121422290802, acc: 0.6517189741134644)
[2025-02-13 02:24:46,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:46,735][root][INFO] - Training Epoch: 1/2, step 76/7134 completed (loss: 1.1978793144226074, acc: 0.6561712622642517)
[2025-02-13 02:24:46,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:47,186][root][INFO] - Training Epoch: 1/2, step 77/7134 completed (loss: 1.2558016777038574, acc: 0.6842105388641357)
[2025-02-13 02:24:47,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:47,668][root][INFO] - Training Epoch: 1/2, step 78/7134 completed (loss: 1.1323366165161133, acc: 0.704402506351471)
[2025-02-13 02:24:47,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:48,115][root][INFO] - Training Epoch: 1/2, step 79/7134 completed (loss: 1.2131632566452026, acc: 0.6604938507080078)
[2025-02-13 02:24:48,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:48,564][root][INFO] - Training Epoch: 1/2, step 80/7134 completed (loss: 1.0577462911605835, acc: 0.7122395634651184)
[2025-02-13 02:24:48,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:49,031][root][INFO] - Training Epoch: 1/2, step 81/7134 completed (loss: 1.0598846673965454, acc: 0.7146596908569336)
[2025-02-13 02:24:49,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:49,506][root][INFO] - Training Epoch: 1/2, step 82/7134 completed (loss: 1.0642740726470947, acc: 0.7213578224182129)
[2025-02-13 02:24:49,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:49,963][root][INFO] - Training Epoch: 1/2, step 83/7134 completed (loss: 1.154072880744934, acc: 0.6748071908950806)
[2025-02-13 02:24:50,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:50,411][root][INFO] - Training Epoch: 1/2, step 84/7134 completed (loss: 1.1912363767623901, acc: 0.6804835796356201)
[2025-02-13 02:24:50,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:50,778][root][INFO] - Training Epoch: 1/2, step 85/7134 completed (loss: 1.2738648653030396, acc: 0.6515513062477112)
[2025-02-13 02:24:50,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:51,198][root][INFO] - Training Epoch: 1/2, step 86/7134 completed (loss: 1.3349672555923462, acc: 0.6559633016586304)
[2025-02-13 02:24:51,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:51,669][root][INFO] - Training Epoch: 1/2, step 87/7134 completed (loss: 1.1146950721740723, acc: 0.685756266117096)
[2025-02-13 02:24:51,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:52,147][root][INFO] - Training Epoch: 1/2, step 88/7134 completed (loss: 1.056388020515442, acc: 0.7012448310852051)
[2025-02-13 02:24:52,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:52,612][root][INFO] - Training Epoch: 1/2, step 89/7134 completed (loss: 1.0593510866165161, acc: 0.7047497034072876)
[2025-02-13 02:24:52,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:53,046][root][INFO] - Training Epoch: 1/2, step 90/7134 completed (loss: 1.1565274000167847, acc: 0.6713286638259888)
[2025-02-13 02:24:53,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:53,470][root][INFO] - Training Epoch: 1/2, step 91/7134 completed (loss: 1.0066355466842651, acc: 0.7081632614135742)
[2025-02-13 02:24:53,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:53,935][root][INFO] - Training Epoch: 1/2, step 92/7134 completed (loss: 1.0631955862045288, acc: 0.6998597383499146)
[2025-02-13 02:24:54,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:54,392][root][INFO] - Training Epoch: 1/2, step 93/7134 completed (loss: 1.0093629360198975, acc: 0.7286501526832581)
[2025-02-13 02:24:54,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:54,742][root][INFO] - Training Epoch: 1/2, step 94/7134 completed (loss: 1.214046597480774, acc: 0.6728538274765015)
[2025-02-13 02:24:54,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:55,199][root][INFO] - Training Epoch: 1/2, step 95/7134 completed (loss: 1.0314124822616577, acc: 0.7097989916801453)
[2025-02-13 02:24:55,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:55,548][root][INFO] - Training Epoch: 1/2, step 96/7134 completed (loss: 1.0069177150726318, acc: 0.7355679869651794)
[2025-02-13 02:24:55,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:55,819][root][INFO] - Training Epoch: 1/2, step 97/7134 completed (loss: 1.752522349357605, acc: 0.5903614163398743)
[2025-02-13 02:24:55,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:56,287][root][INFO] - Training Epoch: 1/2, step 98/7134 completed (loss: 1.3345762491226196, acc: 0.6407766938209534)
[2025-02-13 02:24:56,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:56,720][root][INFO] - Training Epoch: 1/2, step 99/7134 completed (loss: 0.9784998297691345, acc: 0.7394495606422424)
[2025-02-13 02:24:56,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:57,230][root][INFO] - Training Epoch: 1/2, step 100/7134 completed (loss: 0.9534887075424194, acc: 0.7294871807098389)
[2025-02-13 02:24:57,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:57,652][root][INFO] - Training Epoch: 1/2, step 101/7134 completed (loss: 1.0240806341171265, acc: 0.7253289222717285)
[2025-02-13 02:24:57,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:58,094][root][INFO] - Training Epoch: 1/2, step 102/7134 completed (loss: 1.1067792177200317, acc: 0.6862027049064636)
[2025-02-13 02:24:58,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:58,492][root][INFO] - Training Epoch: 1/2, step 103/7134 completed (loss: 0.9186646342277527, acc: 0.7416666746139526)
[2025-02-13 02:24:58,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:58,949][root][INFO] - Training Epoch: 1/2, step 104/7134 completed (loss: 0.9423202872276306, acc: 0.7326284050941467)
[2025-02-13 02:24:59,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:59,408][root][INFO] - Training Epoch: 1/2, step 105/7134 completed (loss: 0.90606290102005, acc: 0.7145187854766846)
[2025-02-13 02:24:59,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:59,871][root][INFO] - Training Epoch: 1/2, step 106/7134 completed (loss: 0.9546143412590027, acc: 0.7289837002754211)
[2025-02-13 02:25:00,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:00,300][root][INFO] - Training Epoch: 1/2, step 107/7134 completed (loss: 1.1084234714508057, acc: 0.6848137378692627)
[2025-02-13 02:25:00,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:00,779][root][INFO] - Training Epoch: 1/2, step 108/7134 completed (loss: 0.8506556153297424, acc: 0.7494004964828491)
[2025-02-13 02:25:00,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:01,180][root][INFO] - Training Epoch: 1/2, step 109/7134 completed (loss: 0.9602189064025879, acc: 0.7352941036224365)
[2025-02-13 02:25:01,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:01,652][root][INFO] - Training Epoch: 1/2, step 110/7134 completed (loss: 0.8516778945922852, acc: 0.7544065713882446)
[2025-02-13 02:25:01,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:02,073][root][INFO] - Training Epoch: 1/2, step 111/7134 completed (loss: 0.9066793918609619, acc: 0.7386759519577026)
[2025-02-13 02:25:02,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:02,479][root][INFO] - Training Epoch: 1/2, step 112/7134 completed (loss: 0.8246921896934509, acc: 0.7411401867866516)
[2025-02-13 02:25:02,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:02,926][root][INFO] - Training Epoch: 1/2, step 113/7134 completed (loss: 0.8616845011711121, acc: 0.755696177482605)
[2025-02-13 02:25:03,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:03,373][root][INFO] - Training Epoch: 1/2, step 114/7134 completed (loss: 0.9495483040809631, acc: 0.7506631016731262)
[2025-02-13 02:25:03,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:03,716][root][INFO] - Training Epoch: 1/2, step 115/7134 completed (loss: 1.02203369140625, acc: 0.7139107584953308)
[2025-02-13 02:25:03,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:04,214][root][INFO] - Training Epoch: 1/2, step 116/7134 completed (loss: 0.8444240689277649, acc: 0.7455830574035645)
[2025-02-13 02:25:04,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:04,641][root][INFO] - Training Epoch: 1/2, step 117/7134 completed (loss: 0.8395592570304871, acc: 0.7631579041481018)
[2025-02-13 02:25:04,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:05,110][root][INFO] - Training Epoch: 1/2, step 118/7134 completed (loss: 0.792143702507019, acc: 0.7767857313156128)
[2025-02-13 02:25:05,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:05,572][root][INFO] - Training Epoch: 1/2, step 119/7134 completed (loss: 0.8335777521133423, acc: 0.7639225125312805)
[2025-02-13 02:25:05,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:06,007][root][INFO] - Training Epoch: 1/2, step 120/7134 completed (loss: 0.7510051131248474, acc: 0.7785059213638306)
[2025-02-13 02:25:06,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:06,440][root][INFO] - Training Epoch: 1/2, step 121/7134 completed (loss: 0.7541232705116272, acc: 0.7833553552627563)
[2025-02-13 02:25:06,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:06,887][root][INFO] - Training Epoch: 1/2, step 122/7134 completed (loss: 0.7462262511253357, acc: 0.800000011920929)
[2025-02-13 02:25:07,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:07,285][root][INFO] - Training Epoch: 1/2, step 123/7134 completed (loss: 0.7005439400672913, acc: 0.8033033013343811)
[2025-02-13 02:25:07,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:07,691][root][INFO] - Training Epoch: 1/2, step 124/7134 completed (loss: 0.8289502263069153, acc: 0.7706896662712097)
[2025-02-13 02:25:07,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:08,168][root][INFO] - Training Epoch: 1/2, step 125/7134 completed (loss: 0.8726658821105957, acc: 0.758849561214447)
[2025-02-13 02:25:08,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:08,609][root][INFO] - Training Epoch: 1/2, step 126/7134 completed (loss: 0.7603351473808289, acc: 0.7799999713897705)
[2025-02-13 02:25:08,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:09,072][root][INFO] - Training Epoch: 1/2, step 127/7134 completed (loss: 0.7296231985092163, acc: 0.790123462677002)
[2025-02-13 02:25:09,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:09,535][root][INFO] - Training Epoch: 1/2, step 128/7134 completed (loss: 0.9211060404777527, acc: 0.7416378259658813)
[2025-02-13 02:25:09,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:10,010][root][INFO] - Training Epoch: 1/2, step 129/7134 completed (loss: 0.8055824637413025, acc: 0.7683823704719543)
[2025-02-13 02:25:10,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:10,448][root][INFO] - Training Epoch: 1/2, step 130/7134 completed (loss: 0.759868323802948, acc: 0.7710526585578918)
[2025-02-13 02:25:10,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:10,909][root][INFO] - Training Epoch: 1/2, step 131/7134 completed (loss: 0.7313912510871887, acc: 0.7986842393875122)
[2025-02-13 02:25:11,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:11,348][root][INFO] - Training Epoch: 1/2, step 132/7134 completed (loss: 0.6555737257003784, acc: 0.8171262741088867)
[2025-02-13 02:25:11,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:11,774][root][INFO] - Training Epoch: 1/2, step 133/7134 completed (loss: 0.6725260019302368, acc: 0.8136439323425293)
[2025-02-13 02:25:11,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:12,229][root][INFO] - Training Epoch: 1/2, step 134/7134 completed (loss: 0.6160427331924438, acc: 0.8070796728134155)
[2025-02-13 02:25:12,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:12,695][root][INFO] - Training Epoch: 1/2, step 135/7134 completed (loss: 0.7412505149841309, acc: 0.7811111211776733)
[2025-02-13 02:25:12,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:13,143][root][INFO] - Training Epoch: 1/2, step 136/7134 completed (loss: 0.6337384581565857, acc: 0.8183990716934204)
[2025-02-13 02:25:13,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:13,607][root][INFO] - Training Epoch: 1/2, step 137/7134 completed (loss: 0.7565264105796814, acc: 0.7724477052688599)
[2025-02-13 02:25:13,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:14,081][root][INFO] - Training Epoch: 1/2, step 138/7134 completed (loss: 0.7792699337005615, acc: 0.7850821614265442)
[2025-02-13 02:25:14,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:14,571][root][INFO] - Training Epoch: 1/2, step 139/7134 completed (loss: 0.7245054244995117, acc: 0.8017902970314026)
[2025-02-13 02:25:14,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:15,023][root][INFO] - Training Epoch: 1/2, step 140/7134 completed (loss: 0.9140297174453735, acc: 0.7567164301872253)
[2025-02-13 02:25:15,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:15,459][root][INFO] - Training Epoch: 1/2, step 141/7134 completed (loss: 0.6953299641609192, acc: 0.8136792182922363)
[2025-02-13 02:25:15,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:15,934][root][INFO] - Training Epoch: 1/2, step 142/7134 completed (loss: 0.7532284259796143, acc: 0.7885952591896057)
[2025-02-13 02:25:16,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:16,387][root][INFO] - Training Epoch: 1/2, step 143/7134 completed (loss: 0.5945221781730652, acc: 0.8200782537460327)
[2025-02-13 02:25:16,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:16,844][root][INFO] - Training Epoch: 1/2, step 144/7134 completed (loss: 0.7742368578910828, acc: 0.7913188934326172)
[2025-02-13 02:25:16,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:17,289][root][INFO] - Training Epoch: 1/2, step 145/7134 completed (loss: 1.0167514085769653, acc: 0.7422535419464111)
[2025-02-13 02:25:17,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:17,750][root][INFO] - Training Epoch: 1/2, step 146/7134 completed (loss: 0.8402395844459534, acc: 0.7706552743911743)
[2025-02-13 02:25:17,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:18,173][root][INFO] - Training Epoch: 1/2, step 147/7134 completed (loss: 0.8711159825325012, acc: 0.7712305188179016)
[2025-02-13 02:25:18,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:18,584][root][INFO] - Training Epoch: 1/2, step 148/7134 completed (loss: 0.9031467437744141, acc: 0.7610208988189697)
[2025-02-13 02:25:18,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:19,023][root][INFO] - Training Epoch: 1/2, step 149/7134 completed (loss: 0.7988348007202148, acc: 0.7896774411201477)
[2025-02-13 02:25:19,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:19,501][root][INFO] - Training Epoch: 1/2, step 150/7134 completed (loss: 0.8684566020965576, acc: 0.7620320916175842)
[2025-02-13 02:25:19,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:19,979][root][INFO] - Training Epoch: 1/2, step 151/7134 completed (loss: 0.8362155556678772, acc: 0.776849627494812)
[2025-02-13 02:25:20,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:20,441][root][INFO] - Training Epoch: 1/2, step 152/7134 completed (loss: 0.9103264808654785, acc: 0.7494305372238159)
[2025-02-13 02:25:20,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:20,873][root][INFO] - Training Epoch: 1/2, step 153/7134 completed (loss: 0.94076007604599, acc: 0.7402234673500061)
[2025-02-13 02:25:21,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:21,289][root][INFO] - Training Epoch: 1/2, step 154/7134 completed (loss: 0.8940346240997314, acc: 0.7537993788719177)
[2025-02-13 02:25:21,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:21,752][root][INFO] - Training Epoch: 1/2, step 155/7134 completed (loss: 0.7993848323822021, acc: 0.78311687707901)
[2025-02-13 02:25:21,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:22,225][root][INFO] - Training Epoch: 1/2, step 156/7134 completed (loss: 0.8227267265319824, acc: 0.7759626507759094)
[2025-02-13 02:25:22,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:22,721][root][INFO] - Training Epoch: 1/2, step 157/7134 completed (loss: 0.8973734378814697, acc: 0.7722891569137573)
[2025-02-13 02:25:22,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:23,197][root][INFO] - Training Epoch: 1/2, step 158/7134 completed (loss: 0.7359259724617004, acc: 0.7912814021110535)
[2025-02-13 02:25:23,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:23,676][root][INFO] - Training Epoch: 1/2, step 159/7134 completed (loss: 0.71055006980896, acc: 0.789321780204773)
[2025-02-13 02:25:23,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:24,058][root][INFO] - Training Epoch: 1/2, step 160/7134 completed (loss: 0.9085018634796143, acc: 0.7550607323646545)
[2025-02-13 02:25:24,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:24,362][root][INFO] - Training Epoch: 1/2, step 161/7134 completed (loss: 0.7796516418457031, acc: 0.770348846912384)
[2025-02-13 02:25:24,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:24,693][root][INFO] - Training Epoch: 1/2, step 162/7134 completed (loss: 0.8000171184539795, acc: 0.7647058963775635)
[2025-02-13 02:25:24,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:25,174][root][INFO] - Training Epoch: 1/2, step 163/7134 completed (loss: 0.7588279843330383, acc: 0.7799113988876343)
[2025-02-13 02:25:25,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:25,522][root][INFO] - Training Epoch: 1/2, step 164/7134 completed (loss: 0.7531555891036987, acc: 0.7659574747085571)
[2025-02-13 02:25:25,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:25,921][root][INFO] - Training Epoch: 1/2, step 165/7134 completed (loss: 0.9105162024497986, acc: 0.7918660044670105)
[2025-02-13 02:25:26,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:26,197][root][INFO] - Training Epoch: 1/2, step 166/7134 completed (loss: 0.75174880027771, acc: 0.7641791105270386)
[2025-02-13 02:25:26,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:26,647][root][INFO] - Training Epoch: 1/2, step 167/7134 completed (loss: 1.1834332942962646, acc: 0.70944744348526)
[2025-02-13 02:25:26,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:26,924][root][INFO] - Training Epoch: 1/2, step 168/7134 completed (loss: 2.2191436290740967, acc: 0.4941176474094391)
[2025-02-13 02:25:27,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:27,275][root][INFO] - Training Epoch: 1/2, step 169/7134 completed (loss: 1.8000181913375854, acc: 0.6162465214729309)
[2025-02-13 02:25:27,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:27,668][root][INFO] - Training Epoch: 1/2, step 170/7134 completed (loss: 1.1717528104782104, acc: 0.6982921957969666)
[2025-02-13 02:25:27,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:28,034][root][INFO] - Training Epoch: 1/2, step 171/7134 completed (loss: 1.1694703102111816, acc: 0.7207792401313782)
[2025-02-13 02:25:28,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:28,437][root][INFO] - Training Epoch: 1/2, step 172/7134 completed (loss: 1.0007877349853516, acc: 0.7726161479949951)
[2025-02-13 02:25:28,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:28,775][root][INFO] - Training Epoch: 1/2, step 173/7134 completed (loss: 0.8059053421020508, acc: 0.778761088848114)
[2025-02-13 02:25:28,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:29,164][root][INFO] - Training Epoch: 1/2, step 174/7134 completed (loss: 0.803268313407898, acc: 0.76408451795578)
[2025-02-13 02:25:29,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:29,594][root][INFO] - Training Epoch: 1/2, step 175/7134 completed (loss: 0.8351854085922241, acc: 0.7927272915840149)
[2025-02-13 02:25:29,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:30,086][root][INFO] - Training Epoch: 1/2, step 176/7134 completed (loss: 0.8509477376937866, acc: 0.7753623127937317)
[2025-02-13 02:25:30,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:30,507][root][INFO] - Training Epoch: 1/2, step 177/7134 completed (loss: 0.811028242111206, acc: 0.7722222208976746)
[2025-02-13 02:25:30,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:30,785][root][INFO] - Training Epoch: 1/2, step 178/7134 completed (loss: 0.849933385848999, acc: 0.748062014579773)
[2025-02-13 02:25:30,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:31,200][root][INFO] - Training Epoch: 1/2, step 179/7134 completed (loss: 0.7554312348365784, acc: 0.8046421408653259)
[2025-02-13 02:25:31,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:31,665][root][INFO] - Training Epoch: 1/2, step 180/7134 completed (loss: 0.6292449831962585, acc: 0.8084833025932312)
[2025-02-13 02:25:31,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:32,090][root][INFO] - Training Epoch: 1/2, step 181/7134 completed (loss: 0.5519052743911743, acc: 0.8247694373130798)
[2025-02-13 02:25:32,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:32,587][root][INFO] - Training Epoch: 1/2, step 182/7134 completed (loss: 0.6160352230072021, acc: 0.8083028197288513)
[2025-02-13 02:25:32,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:33,048][root][INFO] - Training Epoch: 1/2, step 183/7134 completed (loss: 0.6050841212272644, acc: 0.8201634883880615)
[2025-02-13 02:25:33,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:33,467][root][INFO] - Training Epoch: 1/2, step 184/7134 completed (loss: 0.4945494830608368, acc: 0.8525345325469971)
[2025-02-13 02:25:33,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:33,892][root][INFO] - Training Epoch: 1/2, step 185/7134 completed (loss: 0.4603114426136017, acc: 0.8526466488838196)
[2025-02-13 02:25:34,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:34,361][root][INFO] - Training Epoch: 1/2, step 186/7134 completed (loss: 0.5518144369125366, acc: 0.8461538553237915)
[2025-02-13 02:25:34,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:34,790][root][INFO] - Training Epoch: 1/2, step 187/7134 completed (loss: 0.56703120470047, acc: 0.8379888534545898)
[2025-02-13 02:25:34,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:35,226][root][INFO] - Training Epoch: 1/2, step 188/7134 completed (loss: 0.5599114298820496, acc: 0.8333333134651184)
[2025-02-13 02:25:35,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:35,596][root][INFO] - Training Epoch: 1/2, step 189/7134 completed (loss: 0.67573481798172, acc: 0.8230769038200378)
[2025-02-13 02:25:35,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:36,045][root][INFO] - Training Epoch: 1/2, step 190/7134 completed (loss: 0.5945736169815063, acc: 0.8315132856369019)
[2025-02-13 02:25:36,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:36,443][root][INFO] - Training Epoch: 1/2, step 191/7134 completed (loss: 0.5203662514686584, acc: 0.8611111044883728)
[2025-02-13 02:25:36,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:36,949][root][INFO] - Training Epoch: 1/2, step 192/7134 completed (loss: 0.5180495381355286, acc: 0.838487982749939)
[2025-02-13 02:25:37,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:37,368][root][INFO] - Training Epoch: 1/2, step 193/7134 completed (loss: 0.49155962467193604, acc: 0.8429530262947083)
[2025-02-13 02:25:37,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:37,827][root][INFO] - Training Epoch: 1/2, step 194/7134 completed (loss: 0.5640105605125427, acc: 0.8378766179084778)
[2025-02-13 02:25:37,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:38,260][root][INFO] - Training Epoch: 1/2, step 195/7134 completed (loss: 0.5775476098060608, acc: 0.848739504814148)
[2025-02-13 02:25:38,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:38,723][root][INFO] - Training Epoch: 1/2, step 196/7134 completed (loss: 0.4494188725948334, acc: 0.874331533908844)
[2025-02-13 02:25:38,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:39,179][root][INFO] - Training Epoch: 1/2, step 197/7134 completed (loss: 0.5646904110908508, acc: 0.8447293639183044)
[2025-02-13 02:25:39,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:39,638][root][INFO] - Training Epoch: 1/2, step 198/7134 completed (loss: 0.4969663619995117, acc: 0.8565940856933594)
[2025-02-13 02:25:39,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:40,064][root][INFO] - Training Epoch: 1/2, step 199/7134 completed (loss: 0.5292457342147827, acc: 0.8390804529190063)
[2025-02-13 02:25:40,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:40,526][root][INFO] - Training Epoch: 1/2, step 200/7134 completed (loss: 0.480578750371933, acc: 0.8590604066848755)
[2025-02-13 02:25:40,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:40,955][root][INFO] - Training Epoch: 1/2, step 201/7134 completed (loss: 0.4825073480606079, acc: 0.8525280952453613)
[2025-02-13 02:25:41,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:41,386][root][INFO] - Training Epoch: 1/2, step 202/7134 completed (loss: 0.44395962357521057, acc: 0.860689640045166)
[2025-02-13 02:25:41,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:41,827][root][INFO] - Training Epoch: 1/2, step 203/7134 completed (loss: 0.44647619128227234, acc: 0.8856749534606934)
[2025-02-13 02:25:41,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:42,281][root][INFO] - Training Epoch: 1/2, step 204/7134 completed (loss: 0.4732474088668823, acc: 0.8784194588661194)
[2025-02-13 02:25:42,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:42,729][root][INFO] - Training Epoch: 1/2, step 205/7134 completed (loss: 0.5657241940498352, acc: 0.8473479747772217)
[2025-02-13 02:25:42,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:43,157][root][INFO] - Training Epoch: 1/2, step 206/7134 completed (loss: 0.5484351515769958, acc: 0.8523274660110474)
[2025-02-13 02:25:43,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:43,586][root][INFO] - Training Epoch: 1/2, step 207/7134 completed (loss: 0.5469732880592346, acc: 0.849829375743866)
[2025-02-13 02:25:43,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:44,018][root][INFO] - Training Epoch: 1/2, step 208/7134 completed (loss: 0.720470130443573, acc: 0.8235294222831726)
[2025-02-13 02:25:44,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:44,488][root][INFO] - Training Epoch: 1/2, step 209/7134 completed (loss: 0.65802001953125, acc: 0.8212435245513916)
[2025-02-13 02:25:44,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:44,988][root][INFO] - Training Epoch: 1/2, step 210/7134 completed (loss: 0.6694484949111938, acc: 0.8211284279823303)
[2025-02-13 02:25:45,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:45,349][root][INFO] - Training Epoch: 1/2, step 211/7134 completed (loss: 0.746973991394043, acc: 0.8202614188194275)
[2025-02-13 02:25:45,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:45,772][root][INFO] - Training Epoch: 1/2, step 212/7134 completed (loss: 0.5742572546005249, acc: 0.8595041036605835)
[2025-02-13 02:25:45,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:46,206][root][INFO] - Training Epoch: 1/2, step 213/7134 completed (loss: 0.4707237780094147, acc: 0.8705501556396484)
[2025-02-13 02:25:46,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:46,575][root][INFO] - Training Epoch: 1/2, step 214/7134 completed (loss: 0.4813534915447235, acc: 0.8803418874740601)
[2025-02-13 02:25:46,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:46,998][root][INFO] - Training Epoch: 1/2, step 215/7134 completed (loss: 0.5476793646812439, acc: 0.8472222089767456)
[2025-02-13 02:25:47,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:47,419][root][INFO] - Training Epoch: 1/2, step 216/7134 completed (loss: 0.5004048943519592, acc: 0.8680555820465088)
[2025-02-13 02:25:47,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:47,902][root][INFO] - Training Epoch: 1/2, step 217/7134 completed (loss: 0.5026820302009583, acc: 0.8703030347824097)
[2025-02-13 02:25:48,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:48,355][root][INFO] - Training Epoch: 1/2, step 218/7134 completed (loss: 0.3370971381664276, acc: 0.8969465494155884)
[2025-02-13 02:25:48,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:48,783][root][INFO] - Training Epoch: 1/2, step 219/7134 completed (loss: 0.4497421383857727, acc: 0.8791422843933105)
[2025-02-13 02:25:48,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:49,194][root][INFO] - Training Epoch: 1/2, step 220/7134 completed (loss: 0.44430527091026306, acc: 0.8828451633453369)
[2025-02-13 02:25:49,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:49,646][root][INFO] - Training Epoch: 1/2, step 221/7134 completed (loss: 0.4535428285598755, acc: 0.8805969953536987)
[2025-02-13 02:25:49,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:50,113][root][INFO] - Training Epoch: 1/2, step 222/7134 completed (loss: 0.43005290627479553, acc: 0.8874345421791077)
[2025-02-13 02:25:50,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:50,546][root][INFO] - Training Epoch: 1/2, step 223/7134 completed (loss: 0.48491114377975464, acc: 0.8613607287406921)
[2025-02-13 02:25:50,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:50,974][root][INFO] - Training Epoch: 1/2, step 224/7134 completed (loss: 0.5981045365333557, acc: 0.8352788686752319)
[2025-02-13 02:25:51,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:51,421][root][INFO] - Training Epoch: 1/2, step 225/7134 completed (loss: 0.5580864548683167, acc: 0.8433889746665955)
[2025-02-13 02:25:51,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:51,889][root][INFO] - Training Epoch: 1/2, step 226/7134 completed (loss: 0.5468762516975403, acc: 0.8606356978416443)
[2025-02-13 02:25:52,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:52,343][root][INFO] - Training Epoch: 1/2, step 227/7134 completed (loss: 0.6949070692062378, acc: 0.8325061798095703)
[2025-02-13 02:25:52,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:52,822][root][INFO] - Training Epoch: 1/2, step 228/7134 completed (loss: 0.5679065585136414, acc: 0.8523878455162048)
[2025-02-13 02:25:52,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:53,278][root][INFO] - Training Epoch: 1/2, step 229/7134 completed (loss: 0.47936350107192993, acc: 0.8607594966888428)
[2025-02-13 02:25:53,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:53,739][root][INFO] - Training Epoch: 1/2, step 230/7134 completed (loss: 0.4101102948188782, acc: 0.8783783912658691)
[2025-02-13 02:25:53,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:54,156][root][INFO] - Training Epoch: 1/2, step 231/7134 completed (loss: 0.387101948261261, acc: 0.8813559412956238)
[2025-02-13 02:25:54,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:54,577][root][INFO] - Training Epoch: 1/2, step 232/7134 completed (loss: 0.4314437508583069, acc: 0.8731465935707092)
[2025-02-13 02:25:54,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:55,001][root][INFO] - Training Epoch: 1/2, step 233/7134 completed (loss: 0.3593800663948059, acc: 0.9017432928085327)
[2025-02-13 02:25:55,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:55,436][root][INFO] - Training Epoch: 1/2, step 234/7134 completed (loss: 0.4459789991378784, acc: 0.8817567825317383)
[2025-02-13 02:25:55,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:55,910][root][INFO] - Training Epoch: 1/2, step 235/7134 completed (loss: 0.4185672402381897, acc: 0.8917378783226013)
[2025-02-13 02:25:56,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:56,341][root][INFO] - Training Epoch: 1/2, step 236/7134 completed (loss: 0.36614423990249634, acc: 0.8999999761581421)
[2025-02-13 02:25:56,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:56,759][root][INFO] - Training Epoch: 1/2, step 237/7134 completed (loss: 0.4466039836406708, acc: 0.876304030418396)
[2025-02-13 02:25:56,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:57,147][root][INFO] - Training Epoch: 1/2, step 238/7134 completed (loss: 0.4237233102321625, acc: 0.8936567306518555)
[2025-02-13 02:25:57,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:57,555][root][INFO] - Training Epoch: 1/2, step 239/7134 completed (loss: 0.30502375960350037, acc: 0.8970588445663452)
[2025-02-13 02:25:57,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:57,972][root][INFO] - Training Epoch: 1/2, step 240/7134 completed (loss: 0.41813769936561584, acc: 0.883474588394165)
[2025-02-13 02:25:58,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:58,402][root][INFO] - Training Epoch: 1/2, step 241/7134 completed (loss: 0.4344514310359955, acc: 0.8888888955116272)
[2025-02-13 02:25:58,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:58,816][root][INFO] - Training Epoch: 1/2, step 242/7134 completed (loss: 0.4795067310333252, acc: 0.8821490406990051)
[2025-02-13 02:25:58,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:59,228][root][INFO] - Training Epoch: 1/2, step 243/7134 completed (loss: 0.5937274694442749, acc: 0.853741466999054)
[2025-02-13 02:25:59,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:59,644][root][INFO] - Training Epoch: 1/2, step 244/7134 completed (loss: 0.3244030177593231, acc: 0.9095022678375244)
[2025-02-13 02:25:59,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:00,072][root][INFO] - Training Epoch: 1/2, step 245/7134 completed (loss: 0.4131011664867401, acc: 0.889667272567749)
[2025-02-13 02:26:00,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:00,491][root][INFO] - Training Epoch: 1/2, step 246/7134 completed (loss: 0.4108200669288635, acc: 0.8832807540893555)
[2025-02-13 02:26:00,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:00,904][root][INFO] - Training Epoch: 1/2, step 247/7134 completed (loss: 0.3886130154132843, acc: 0.8983666300773621)
[2025-02-13 02:26:01,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:01,360][root][INFO] - Training Epoch: 1/2, step 248/7134 completed (loss: 0.35999488830566406, acc: 0.8972868323326111)
[2025-02-13 02:26:01,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:01,717][root][INFO] - Training Epoch: 1/2, step 249/7134 completed (loss: 0.546938419342041, acc: 0.8713136911392212)
[2025-02-13 02:26:01,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:02,134][root][INFO] - Training Epoch: 1/2, step 250/7134 completed (loss: 0.47871336340904236, acc: 0.8734177350997925)
[2025-02-13 02:26:02,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:02,540][root][INFO] - Training Epoch: 1/2, step 251/7134 completed (loss: 0.3747141361236572, acc: 0.8940171003341675)
[2025-02-13 02:26:02,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:02,899][root][INFO] - Training Epoch: 1/2, step 252/7134 completed (loss: 0.34897294640541077, acc: 0.8934426307678223)
[2025-02-13 02:26:03,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:03,338][root][INFO] - Training Epoch: 1/2, step 253/7134 completed (loss: 0.40786048769950867, acc: 0.8770053386688232)
[2025-02-13 02:26:03,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:03,754][root][INFO] - Training Epoch: 1/2, step 254/7134 completed (loss: 0.38793236017227173, acc: 0.8999999761581421)
[2025-02-13 02:26:03,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:04,184][root][INFO] - Training Epoch: 1/2, step 255/7134 completed (loss: 0.38663390278816223, acc: 0.8932748436927795)
[2025-02-13 02:26:04,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:04,549][root][INFO] - Training Epoch: 1/2, step 256/7134 completed (loss: 0.45939525961875916, acc: 0.8823529481887817)
[2025-02-13 02:26:04,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:05,040][root][INFO] - Training Epoch: 1/2, step 257/7134 completed (loss: 0.35830405354499817, acc: 0.8926380276679993)
[2025-02-13 02:26:05,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:05,459][root][INFO] - Training Epoch: 1/2, step 258/7134 completed (loss: 0.40464189648628235, acc: 0.8876032829284668)
[2025-02-13 02:26:05,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:05,901][root][INFO] - Training Epoch: 1/2, step 259/7134 completed (loss: 0.36039459705352783, acc: 0.9043760299682617)
[2025-02-13 02:26:06,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:06,354][root][INFO] - Training Epoch: 1/2, step 260/7134 completed (loss: 0.3977467715740204, acc: 0.9001691937446594)
[2025-02-13 02:26:06,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:06,759][root][INFO] - Training Epoch: 1/2, step 261/7134 completed (loss: 0.35493627190589905, acc: 0.8997954726219177)
[2025-02-13 02:26:06,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:07,127][root][INFO] - Training Epoch: 1/2, step 262/7134 completed (loss: 0.33902645111083984, acc: 0.8985024690628052)
[2025-02-13 02:26:07,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:07,549][root][INFO] - Training Epoch: 1/2, step 263/7134 completed (loss: 0.3810350298881531, acc: 0.8850574493408203)
[2025-02-13 02:26:07,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:08,006][root][INFO] - Training Epoch: 1/2, step 264/7134 completed (loss: 0.43599480390548706, acc: 0.8738317489624023)
[2025-02-13 02:26:08,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:08,454][root][INFO] - Training Epoch: 1/2, step 265/7134 completed (loss: 0.38525253534317017, acc: 0.8891752362251282)
[2025-02-13 02:26:08,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:08,901][root][INFO] - Training Epoch: 1/2, step 266/7134 completed (loss: 0.42395803332328796, acc: 0.8807339668273926)
[2025-02-13 02:26:09,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:09,399][root][INFO] - Training Epoch: 1/2, step 267/7134 completed (loss: 0.45383498072624207, acc: 0.8643678426742554)
[2025-02-13 02:26:09,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:09,900][root][INFO] - Training Epoch: 1/2, step 268/7134 completed (loss: 0.4538377821445465, acc: 0.8547341227531433)
[2025-02-13 02:26:10,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:10,371][root][INFO] - Training Epoch: 1/2, step 269/7134 completed (loss: 0.39861807227134705, acc: 0.8845618963241577)
[2025-02-13 02:26:10,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:10,791][root][INFO] - Training Epoch: 1/2, step 270/7134 completed (loss: 0.27459716796875, acc: 0.9268656969070435)
[2025-02-13 02:26:10,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:11,253][root][INFO] - Training Epoch: 1/2, step 271/7134 completed (loss: 0.3744392991065979, acc: 0.8902900218963623)
[2025-02-13 02:26:11,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:11,690][root][INFO] - Training Epoch: 1/2, step 272/7134 completed (loss: 0.3471226692199707, acc: 0.910941481590271)
[2025-02-13 02:26:11,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:12,135][root][INFO] - Training Epoch: 1/2, step 273/7134 completed (loss: 0.31576359272003174, acc: 0.9077844023704529)
[2025-02-13 02:26:12,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:12,538][root][INFO] - Training Epoch: 1/2, step 274/7134 completed (loss: 0.2859543561935425, acc: 0.9202200770378113)
[2025-02-13 02:26:12,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:13,012][root][INFO] - Training Epoch: 1/2, step 275/7134 completed (loss: 0.4537599980831146, acc: 0.8805104494094849)
[2025-02-13 02:26:13,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:13,480][root][INFO] - Training Epoch: 1/2, step 276/7134 completed (loss: 0.3144763708114624, acc: 0.9220463037490845)
[2025-02-13 02:26:13,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:13,935][root][INFO] - Training Epoch: 1/2, step 277/7134 completed (loss: 0.40198805928230286, acc: 0.8880308866500854)
[2025-02-13 02:26:14,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:14,365][root][INFO] - Training Epoch: 1/2, step 278/7134 completed (loss: 0.30934715270996094, acc: 0.9042145609855652)
[2025-02-13 02:26:14,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:14,815][root][INFO] - Training Epoch: 1/2, step 279/7134 completed (loss: 0.3089154064655304, acc: 0.913385808467865)
[2025-02-13 02:26:14,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:15,232][root][INFO] - Training Epoch: 1/2, step 280/7134 completed (loss: 0.38235536217689514, acc: 0.8942652344703674)
[2025-02-13 02:26:15,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:15,644][root][INFO] - Training Epoch: 1/2, step 281/7134 completed (loss: 0.3044505715370178, acc: 0.9020000100135803)
[2025-02-13 02:26:15,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:16,070][root][INFO] - Training Epoch: 1/2, step 282/7134 completed (loss: 0.4698634147644043, acc: 0.8824427723884583)
[2025-02-13 02:26:16,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:16,490][root][INFO] - Training Epoch: 1/2, step 283/7134 completed (loss: 0.44345423579216003, acc: 0.8777589201927185)
[2025-02-13 02:26:16,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:16,893][root][INFO] - Training Epoch: 1/2, step 284/7134 completed (loss: 0.4552900195121765, acc: 0.8741610646247864)
[2025-02-13 02:26:17,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:17,307][root][INFO] - Training Epoch: 1/2, step 285/7134 completed (loss: 0.40400293469429016, acc: 0.8888888955116272)
[2025-02-13 02:26:17,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:17,702][root][INFO] - Training Epoch: 1/2, step 286/7134 completed (loss: 0.298051655292511, acc: 0.9150442481040955)
[2025-02-13 02:26:17,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:18,136][root][INFO] - Training Epoch: 1/2, step 287/7134 completed (loss: 0.45127901434898376, acc: 0.8764045238494873)
[2025-02-13 02:26:18,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:18,594][root][INFO] - Training Epoch: 1/2, step 288/7134 completed (loss: 0.3766275942325592, acc: 0.8858194947242737)
[2025-02-13 02:26:18,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:19,076][root][INFO] - Training Epoch: 1/2, step 289/7134 completed (loss: 0.37284529209136963, acc: 0.8915008902549744)
[2025-02-13 02:26:19,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:19,492][root][INFO] - Training Epoch: 1/2, step 290/7134 completed (loss: 0.4025338590145111, acc: 0.8860182166099548)
[2025-02-13 02:26:19,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:19,925][root][INFO] - Training Epoch: 1/2, step 291/7134 completed (loss: 0.3007246255874634, acc: 0.9226973652839661)
[2025-02-13 02:26:20,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:20,355][root][INFO] - Training Epoch: 1/2, step 292/7134 completed (loss: 0.43355345726013184, acc: 0.8773006200790405)
[2025-02-13 02:26:20,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:20,771][root][INFO] - Training Epoch: 1/2, step 293/7134 completed (loss: 0.29222163558006287, acc: 0.9196786880493164)
[2025-02-13 02:26:20,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:21,214][root][INFO] - Training Epoch: 1/2, step 294/7134 completed (loss: 0.3414256274700165, acc: 0.9005848169326782)
[2025-02-13 02:26:21,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:21,685][root][INFO] - Training Epoch: 1/2, step 295/7134 completed (loss: 0.33210310339927673, acc: 0.9095563292503357)
[2025-02-13 02:26:21,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:22,122][root][INFO] - Training Epoch: 1/2, step 296/7134 completed (loss: 0.274298757314682, acc: 0.9224806427955627)
[2025-02-13 02:26:22,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:22,588][root][INFO] - Training Epoch: 1/2, step 297/7134 completed (loss: 0.26571568846702576, acc: 0.9289520382881165)
[2025-02-13 02:26:22,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:23,033][root][INFO] - Training Epoch: 1/2, step 298/7134 completed (loss: 0.2875135838985443, acc: 0.9247999787330627)
[2025-02-13 02:26:23,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:23,455][root][INFO] - Training Epoch: 1/2, step 299/7134 completed (loss: 0.2819659113883972, acc: 0.927819550037384)
[2025-02-13 02:26:23,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:23,880][root][INFO] - Training Epoch: 1/2, step 300/7134 completed (loss: 0.2482994645833969, acc: 0.9255663156509399)
[2025-02-13 02:26:24,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:24,327][root][INFO] - Training Epoch: 1/2, step 301/7134 completed (loss: 0.27562034130096436, acc: 0.9236209392547607)
[2025-02-13 02:26:24,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:24,730][root][INFO] - Training Epoch: 1/2, step 302/7134 completed (loss: 0.405356764793396, acc: 0.9005848169326782)
[2025-02-13 02:26:24,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:25,107][root][INFO] - Training Epoch: 1/2, step 303/7134 completed (loss: 0.2775948643684387, acc: 0.9281553626060486)
[2025-02-13 02:26:25,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:25,522][root][INFO] - Training Epoch: 1/2, step 304/7134 completed (loss: 0.22342464327812195, acc: 0.9277777671813965)
[2025-02-13 02:26:25,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:25,948][root][INFO] - Training Epoch: 1/2, step 305/7134 completed (loss: 0.20463259518146515, acc: 0.9364069700241089)
[2025-02-13 02:26:26,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:26,362][root][INFO] - Training Epoch: 1/2, step 306/7134 completed (loss: 0.24864283204078674, acc: 0.9148550629615784)
[2025-02-13 02:26:26,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:26,806][root][INFO] - Training Epoch: 1/2, step 307/7134 completed (loss: 0.2731100916862488, acc: 0.9234875440597534)
[2025-02-13 02:26:26,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:27,225][root][INFO] - Training Epoch: 1/2, step 308/7134 completed (loss: 0.25395914912223816, acc: 0.9243420958518982)
[2025-02-13 02:26:27,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:27,685][root][INFO] - Training Epoch: 1/2, step 309/7134 completed (loss: 0.27354103326797485, acc: 0.9224137663841248)
[2025-02-13 02:26:27,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:28,166][root][INFO] - Training Epoch: 1/2, step 310/7134 completed (loss: 0.237062469124794, acc: 0.932748556137085)
[2025-02-13 02:26:28,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:28,582][root][INFO] - Training Epoch: 1/2, step 311/7134 completed (loss: 0.2693691849708557, acc: 0.923794686794281)
[2025-02-13 02:26:28,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:28,990][root][INFO] - Training Epoch: 1/2, step 312/7134 completed (loss: 0.23838025331497192, acc: 0.9306759238243103)
[2025-02-13 02:26:29,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:29,415][root][INFO] - Training Epoch: 1/2, step 313/7134 completed (loss: 0.21592384576797485, acc: 0.9269565343856812)
[2025-02-13 02:26:29,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:29,833][root][INFO] - Training Epoch: 1/2, step 314/7134 completed (loss: 0.30252981185913086, acc: 0.909375011920929)
[2025-02-13 02:26:29,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:30,266][root][INFO] - Training Epoch: 1/2, step 315/7134 completed (loss: 0.2332119345664978, acc: 0.9163934588432312)
[2025-02-13 02:26:30,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:30,638][root][INFO] - Training Epoch: 1/2, step 316/7134 completed (loss: 0.2873712480068207, acc: 0.911552369594574)
[2025-02-13 02:26:30,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:31,078][root][INFO] - Training Epoch: 1/2, step 317/7134 completed (loss: 0.2186964750289917, acc: 0.9377537369728088)
[2025-02-13 02:26:31,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:31,552][root][INFO] - Training Epoch: 1/2, step 318/7134 completed (loss: 0.37540197372436523, acc: 0.9023199081420898)
[2025-02-13 02:26:31,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:31,963][root][INFO] - Training Epoch: 1/2, step 319/7134 completed (loss: 0.36892974376678467, acc: 0.8934081196784973)
[2025-02-13 02:26:32,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:32,422][root][INFO] - Training Epoch: 1/2, step 320/7134 completed (loss: 0.3304632008075714, acc: 0.9144079685211182)
[2025-02-13 02:26:32,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:32,880][root][INFO] - Training Epoch: 1/2, step 321/7134 completed (loss: 0.3721316456794739, acc: 0.9042145609855652)
[2025-02-13 02:26:33,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:33,353][root][INFO] - Training Epoch: 1/2, step 322/7134 completed (loss: 0.3878288269042969, acc: 0.8888888955116272)
[2025-02-13 02:26:33,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:33,790][root][INFO] - Training Epoch: 1/2, step 323/7134 completed (loss: 0.43139490485191345, acc: 0.8844672441482544)
[2025-02-13 02:26:33,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:34,253][root][INFO] - Training Epoch: 1/2, step 324/7134 completed (loss: 0.37832126021385193, acc: 0.9010356664657593)
[2025-02-13 02:26:34,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:34,706][root][INFO] - Training Epoch: 1/2, step 325/7134 completed (loss: 0.2528364360332489, acc: 0.9241645336151123)
[2025-02-13 02:26:34,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:35,139][root][INFO] - Training Epoch: 1/2, step 326/7134 completed (loss: 0.29686540365219116, acc: 0.9086480140686035)
[2025-02-13 02:26:35,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:35,599][root][INFO] - Training Epoch: 1/2, step 327/7134 completed (loss: 0.3449731171131134, acc: 0.9020270109176636)
[2025-02-13 02:26:35,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:36,080][root][INFO] - Training Epoch: 1/2, step 328/7134 completed (loss: 0.3336453139781952, acc: 0.9007407426834106)
[2025-02-13 02:26:36,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:36,550][root][INFO] - Training Epoch: 1/2, step 329/7134 completed (loss: 0.27690020203590393, acc: 0.9177438020706177)
[2025-02-13 02:26:36,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:37,021][root][INFO] - Training Epoch: 1/2, step 330/7134 completed (loss: 0.4017413258552551, acc: 0.8992950916290283)
[2025-02-13 02:26:37,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:37,480][root][INFO] - Training Epoch: 1/2, step 331/7134 completed (loss: 0.281694620847702, acc: 0.9167616963386536)
[2025-02-13 02:26:37,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:37,925][root][INFO] - Training Epoch: 1/2, step 332/7134 completed (loss: 0.2753671109676361, acc: 0.9185360074043274)
[2025-02-13 02:26:38,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:38,365][root][INFO] - Training Epoch: 1/2, step 333/7134 completed (loss: 0.354947954416275, acc: 0.9063260555267334)
[2025-02-13 02:26:38,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:38,811][root][INFO] - Training Epoch: 1/2, step 334/7134 completed (loss: 0.2759164273738861, acc: 0.9252218008041382)
[2025-02-13 02:26:38,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:39,259][root][INFO] - Training Epoch: 1/2, step 335/7134 completed (loss: 0.25209230184555054, acc: 0.9374090433120728)
[2025-02-13 02:26:39,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:39,712][root][INFO] - Training Epoch: 1/2, step 336/7134 completed (loss: 0.3104094862937927, acc: 0.9197604656219482)
[2025-02-13 02:26:39,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:40,156][root][INFO] - Training Epoch: 1/2, step 337/7134 completed (loss: 0.25661274790763855, acc: 0.9325153231620789)
[2025-02-13 02:26:40,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:40,594][root][INFO] - Training Epoch: 1/2, step 338/7134 completed (loss: 0.2728104591369629, acc: 0.9219143390655518)
[2025-02-13 02:26:40,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:41,035][root][INFO] - Training Epoch: 1/2, step 339/7134 completed (loss: 0.22070658206939697, acc: 0.9399999976158142)
[2025-02-13 02:26:41,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:41,451][root][INFO] - Training Epoch: 1/2, step 340/7134 completed (loss: 0.27505454421043396, acc: 0.9211195707321167)
[2025-02-13 02:26:41,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:41,909][root][INFO] - Training Epoch: 1/2, step 341/7134 completed (loss: 0.2360978126525879, acc: 0.9306431412696838)
[2025-02-13 02:26:42,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:42,363][root][INFO] - Training Epoch: 1/2, step 342/7134 completed (loss: 0.2184537947177887, acc: 0.9439393877983093)
[2025-02-13 02:26:42,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:42,806][root][INFO] - Training Epoch: 1/2, step 343/7134 completed (loss: 0.25124073028564453, acc: 0.9281663298606873)
[2025-02-13 02:26:42,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:43,209][root][INFO] - Training Epoch: 1/2, step 344/7134 completed (loss: 0.20457319915294647, acc: 0.9470803141593933)
[2025-02-13 02:26:43,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:43,565][root][INFO] - Training Epoch: 1/2, step 345/7134 completed (loss: 0.18753695487976074, acc: 0.9482142925262451)
[2025-02-13 02:26:43,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:44,023][root][INFO] - Training Epoch: 1/2, step 346/7134 completed (loss: 0.2074313461780548, acc: 0.9443561434745789)
[2025-02-13 02:26:44,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:44,468][root][INFO] - Training Epoch: 1/2, step 347/7134 completed (loss: 0.22695158421993256, acc: 0.946704089641571)
[2025-02-13 02:26:44,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:44,921][root][INFO] - Training Epoch: 1/2, step 348/7134 completed (loss: 0.17898178100585938, acc: 0.9552238583564758)
[2025-02-13 02:26:45,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:45,354][root][INFO] - Training Epoch: 1/2, step 349/7134 completed (loss: 0.19421155750751495, acc: 0.9409396052360535)
[2025-02-13 02:26:45,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:45,847][root][INFO] - Training Epoch: 1/2, step 350/7134 completed (loss: 0.17901961505413055, acc: 0.9482551217079163)
[2025-02-13 02:26:46,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:46,355][root][INFO] - Training Epoch: 1/2, step 351/7134 completed (loss: 0.3149796426296234, acc: 0.9300254583358765)
[2025-02-13 02:26:46,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:46,791][root][INFO] - Training Epoch: 1/2, step 352/7134 completed (loss: 0.19051551818847656, acc: 0.94972825050354)
[2025-02-13 02:26:46,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:47,264][root][INFO] - Training Epoch: 1/2, step 353/7134 completed (loss: 0.21102631092071533, acc: 0.9308510422706604)
[2025-02-13 02:26:47,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:47,715][root][INFO] - Training Epoch: 1/2, step 354/7134 completed (loss: 0.22090919315814972, acc: 0.9429569244384766)
[2025-02-13 02:26:47,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:48,150][root][INFO] - Training Epoch: 1/2, step 355/7134 completed (loss: 0.29639390110969543, acc: 0.9273743033409119)
[2025-02-13 02:26:48,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:48,620][root][INFO] - Training Epoch: 1/2, step 356/7134 completed (loss: 0.22691579163074493, acc: 0.9351851940155029)
[2025-02-13 02:26:48,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:49,109][root][INFO] - Training Epoch: 1/2, step 357/7134 completed (loss: 0.21972908079624176, acc: 0.9402035474777222)
[2025-02-13 02:26:49,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:49,549][root][INFO] - Training Epoch: 1/2, step 358/7134 completed (loss: 0.2308143675327301, acc: 0.944065511226654)
[2025-02-13 02:26:49,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:50,017][root][INFO] - Training Epoch: 1/2, step 359/7134 completed (loss: 0.18227297067642212, acc: 0.9364864826202393)
[2025-02-13 02:26:50,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:50,435][root][INFO] - Training Epoch: 1/2, step 360/7134 completed (loss: 0.20932167768478394, acc: 0.9442771077156067)
[2025-02-13 02:26:50,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:50,857][root][INFO] - Training Epoch: 1/2, step 361/7134 completed (loss: 0.19690796732902527, acc: 0.9500734210014343)
[2025-02-13 02:26:50,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:51,277][root][INFO] - Training Epoch: 1/2, step 362/7134 completed (loss: 0.22569631040096283, acc: 0.946107804775238)
[2025-02-13 02:26:51,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:51,735][root][INFO] - Training Epoch: 1/2, step 363/7134 completed (loss: 0.2597355842590332, acc: 0.9296875)
[2025-02-13 02:26:51,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:52,137][root][INFO] - Training Epoch: 1/2, step 364/7134 completed (loss: 0.22070430219173431, acc: 0.9322429895401001)
[2025-02-13 02:26:52,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:52,622][root][INFO] - Training Epoch: 1/2, step 365/7134 completed (loss: 0.20399004220962524, acc: 0.9462810158729553)
[2025-02-13 02:26:52,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:53,067][root][INFO] - Training Epoch: 1/2, step 366/7134 completed (loss: 0.3639390766620636, acc: 0.9059560894966125)
[2025-02-13 02:26:53,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:53,530][root][INFO] - Training Epoch: 1/2, step 367/7134 completed (loss: 0.22894908487796783, acc: 0.9287499785423279)
[2025-02-13 02:26:53,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:53,955][root][INFO] - Training Epoch: 1/2, step 368/7134 completed (loss: 0.18990767002105713, acc: 0.9405646324157715)
[2025-02-13 02:26:54,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:54,360][root][INFO] - Training Epoch: 1/2, step 369/7134 completed (loss: 0.30944743752479553, acc: 0.9264931082725525)
[2025-02-13 02:26:54,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:54,801][root][INFO] - Training Epoch: 1/2, step 370/7134 completed (loss: 0.27365395426750183, acc: 0.9301310181617737)
[2025-02-13 02:26:54,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:55,238][root][INFO] - Training Epoch: 1/2, step 371/7134 completed (loss: 0.22563129663467407, acc: 0.9503759145736694)
[2025-02-13 02:26:55,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:55,661][root][INFO] - Training Epoch: 1/2, step 372/7134 completed (loss: 0.2039763480424881, acc: 0.9390787482261658)
[2025-02-13 02:26:55,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:56,093][root][INFO] - Training Epoch: 1/2, step 373/7134 completed (loss: 0.16918587684631348, acc: 0.9551569223403931)
[2025-02-13 02:26:56,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:56,532][root][INFO] - Training Epoch: 1/2, step 374/7134 completed (loss: 0.3003382086753845, acc: 0.9212218523025513)
[2025-02-13 02:26:56,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:56,948][root][INFO] - Training Epoch: 1/2, step 375/7134 completed (loss: 0.30858251452445984, acc: 0.9223560690879822)
[2025-02-13 02:26:57,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:57,398][root][INFO] - Training Epoch: 1/2, step 376/7134 completed (loss: 0.4368995428085327, acc: 0.8951724171638489)
[2025-02-13 02:26:57,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:57,847][root][INFO] - Training Epoch: 1/2, step 377/7134 completed (loss: 0.320189505815506, acc: 0.9188445806503296)
[2025-02-13 02:26:57,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:58,263][root][INFO] - Training Epoch: 1/2, step 378/7134 completed (loss: 0.3122116029262543, acc: 0.919618546962738)
[2025-02-13 02:26:58,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:58,712][root][INFO] - Training Epoch: 1/2, step 379/7134 completed (loss: 0.3047528862953186, acc: 0.9195979833602905)
[2025-02-13 02:26:58,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:59,154][root][INFO] - Training Epoch: 1/2, step 380/7134 completed (loss: 0.39455917477607727, acc: 0.8961892127990723)
[2025-02-13 02:26:59,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:59,624][root][INFO] - Training Epoch: 1/2, step 381/7134 completed (loss: 0.33069777488708496, acc: 0.9147287011146545)
[2025-02-13 02:26:59,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:00,078][root][INFO] - Training Epoch: 1/2, step 382/7134 completed (loss: 0.3607659935951233, acc: 0.9062901139259338)
[2025-02-13 02:27:00,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:00,494][root][INFO] - Training Epoch: 1/2, step 383/7134 completed (loss: 0.31748172640800476, acc: 0.9262759685516357)
[2025-02-13 02:27:00,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:00,914][root][INFO] - Training Epoch: 1/2, step 384/7134 completed (loss: 0.36058124899864197, acc: 0.9105473756790161)
[2025-02-13 02:27:01,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:01,334][root][INFO] - Training Epoch: 1/2, step 385/7134 completed (loss: 0.22289444506168365, acc: 0.9434850811958313)
[2025-02-13 02:27:01,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:01,675][root][INFO] - Training Epoch: 1/2, step 386/7134 completed (loss: 0.24258890748023987, acc: 0.9345602989196777)
[2025-02-13 02:27:01,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:02,136][root][INFO] - Training Epoch: 1/2, step 387/7134 completed (loss: 0.3914370834827423, acc: 0.8862974047660828)
[2025-02-13 02:27:02,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:02,606][root][INFO] - Training Epoch: 1/2, step 388/7134 completed (loss: 0.2371263951063156, acc: 0.9460093975067139)
[2025-02-13 02:27:02,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:03,080][root][INFO] - Training Epoch: 1/2, step 389/7134 completed (loss: 0.3468922972679138, acc: 0.9063360691070557)
[2025-02-13 02:27:03,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:03,495][root][INFO] - Training Epoch: 1/2, step 390/7134 completed (loss: 0.3049200475215912, acc: 0.9304206967353821)
[2025-02-13 02:27:03,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:03,898][root][INFO] - Training Epoch: 1/2, step 391/7134 completed (loss: 0.2820627689361572, acc: 0.930272102355957)
[2025-02-13 02:27:04,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:04,353][root][INFO] - Training Epoch: 1/2, step 392/7134 completed (loss: 0.3440741002559662, acc: 0.897777795791626)
[2025-02-13 02:27:04,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:04,769][root][INFO] - Training Epoch: 1/2, step 393/7134 completed (loss: 0.3573863208293915, acc: 0.9083333611488342)
[2025-02-13 02:27:04,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:05,181][root][INFO] - Training Epoch: 1/2, step 394/7134 completed (loss: 0.44311001896858215, acc: 0.887159526348114)
[2025-02-13 02:27:05,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:05,605][root][INFO] - Training Epoch: 1/2, step 395/7134 completed (loss: 0.324541300535202, acc: 0.9218106865882874)
[2025-02-13 02:27:05,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:06,021][root][INFO] - Training Epoch: 1/2, step 396/7134 completed (loss: 0.2599591612815857, acc: 0.9397849440574646)
[2025-02-13 02:27:06,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:06,491][root][INFO] - Training Epoch: 1/2, step 397/7134 completed (loss: 0.32365214824676514, acc: 0.9007936716079712)
[2025-02-13 02:27:06,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:06,936][root][INFO] - Training Epoch: 1/2, step 398/7134 completed (loss: 0.293443500995636, acc: 0.9252577424049377)
[2025-02-13 02:27:07,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:07,344][root][INFO] - Training Epoch: 1/2, step 399/7134 completed (loss: 0.3160209357738495, acc: 0.9033280611038208)
[2025-02-13 02:27:07,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:07,796][root][INFO] - Training Epoch: 1/2, step 400/7134 completed (loss: 0.3847815692424774, acc: 0.9029850959777832)
[2025-02-13 02:27:07,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:08,269][root][INFO] - Training Epoch: 1/2, step 401/7134 completed (loss: 0.2735460102558136, acc: 0.9129902124404907)
[2025-02-13 02:27:08,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:08,719][root][INFO] - Training Epoch: 1/2, step 402/7134 completed (loss: 0.297296941280365, acc: 0.9125475287437439)
[2025-02-13 02:27:08,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:09,217][root][INFO] - Training Epoch: 1/2, step 403/7134 completed (loss: 0.27485525608062744, acc: 0.9244060516357422)
[2025-02-13 02:27:09,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:09,639][root][INFO] - Training Epoch: 1/2, step 404/7134 completed (loss: 0.2778698801994324, acc: 0.9333333373069763)
[2025-02-13 02:27:09,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:10,121][root][INFO] - Training Epoch: 1/2, step 405/7134 completed (loss: 0.3287464380264282, acc: 0.9190031290054321)
[2025-02-13 02:27:10,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:10,584][root][INFO] - Training Epoch: 1/2, step 406/7134 completed (loss: 0.3146585524082184, acc: 0.9163058996200562)
[2025-02-13 02:27:10,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:10,975][root][INFO] - Training Epoch: 1/2, step 407/7134 completed (loss: 0.4211103022098541, acc: 0.8966101408004761)
[2025-02-13 02:27:11,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:11,401][root][INFO] - Training Epoch: 1/2, step 408/7134 completed (loss: 0.34745967388153076, acc: 0.9135802388191223)
[2025-02-13 02:27:11,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:11,852][root][INFO] - Training Epoch: 1/2, step 409/7134 completed (loss: 0.2624901533126831, acc: 0.9320755004882812)
[2025-02-13 02:27:12,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:12,337][root][INFO] - Training Epoch: 1/2, step 410/7134 completed (loss: 0.30421993136405945, acc: 0.924332320690155)
[2025-02-13 02:27:12,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:12,823][root][INFO] - Training Epoch: 1/2, step 411/7134 completed (loss: 0.31621915102005005, acc: 0.9148446321487427)
[2025-02-13 02:27:12,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:13,285][root][INFO] - Training Epoch: 1/2, step 412/7134 completed (loss: 0.3314942717552185, acc: 0.9130434989929199)
[2025-02-13 02:27:13,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:13,711][root][INFO] - Training Epoch: 1/2, step 413/7134 completed (loss: 0.2942539155483246, acc: 0.9225634336471558)
[2025-02-13 02:27:13,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:14,134][root][INFO] - Training Epoch: 1/2, step 414/7134 completed (loss: 0.2670258581638336, acc: 0.921364963054657)
[2025-02-13 02:27:14,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:14,583][root][INFO] - Training Epoch: 1/2, step 415/7134 completed (loss: 0.22180336713790894, acc: 0.9351851940155029)
[2025-02-13 02:27:14,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:15,063][root][INFO] - Training Epoch: 1/2, step 416/7134 completed (loss: 0.3294484317302704, acc: 0.9220778942108154)
[2025-02-13 02:27:15,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:15,523][root][INFO] - Training Epoch: 1/2, step 417/7134 completed (loss: 0.2757999002933502, acc: 0.9225543737411499)
[2025-02-13 02:27:15,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:16,001][root][INFO] - Training Epoch: 1/2, step 418/7134 completed (loss: 0.22983138263225555, acc: 0.9380022883415222)
[2025-02-13 02:27:16,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:16,462][root][INFO] - Training Epoch: 1/2, step 419/7134 completed (loss: 0.2340698093175888, acc: 0.9292804002761841)
[2025-02-13 02:27:16,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:16,878][root][INFO] - Training Epoch: 1/2, step 420/7134 completed (loss: 0.2417231798171997, acc: 0.9323180913925171)
[2025-02-13 02:27:17,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:17,308][root][INFO] - Training Epoch: 1/2, step 421/7134 completed (loss: 0.24159865081310272, acc: 0.9378980994224548)
[2025-02-13 02:27:17,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:17,753][root][INFO] - Training Epoch: 1/2, step 422/7134 completed (loss: 0.14489175379276276, acc: 0.9647058844566345)
[2025-02-13 02:27:17,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:18,226][root][INFO] - Training Epoch: 1/2, step 423/7134 completed (loss: 0.18595093488693237, acc: 0.9514285922050476)
[2025-02-13 02:27:18,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:18,643][root][INFO] - Training Epoch: 1/2, step 424/7134 completed (loss: 0.21938619017601013, acc: 0.9351266026496887)
[2025-02-13 02:27:18,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:19,057][root][INFO] - Training Epoch: 1/2, step 425/7134 completed (loss: 0.198316752910614, acc: 0.9476743936538696)
[2025-02-13 02:27:19,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:19,478][root][INFO] - Training Epoch: 1/2, step 426/7134 completed (loss: 0.15366369485855103, acc: 0.961002767086029)
[2025-02-13 02:27:19,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:19,891][root][INFO] - Training Epoch: 1/2, step 427/7134 completed (loss: 0.16920027136802673, acc: 0.9559939503669739)
[2025-02-13 02:27:20,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:20,329][root][INFO] - Training Epoch: 1/2, step 428/7134 completed (loss: 0.23466593027114868, acc: 0.9382529854774475)
[2025-02-13 02:27:20,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:20,753][root][INFO] - Training Epoch: 1/2, step 429/7134 completed (loss: 0.23819413781166077, acc: 0.9348171949386597)
[2025-02-13 02:27:20,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:21,184][root][INFO] - Training Epoch: 1/2, step 430/7134 completed (loss: 0.22279809415340424, acc: 0.9397417306900024)
[2025-02-13 02:27:21,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:21,603][root][INFO] - Training Epoch: 1/2, step 431/7134 completed (loss: 0.19498872756958008, acc: 0.9495677351951599)
[2025-02-13 02:27:21,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:22,011][root][INFO] - Training Epoch: 1/2, step 432/7134 completed (loss: 0.13220423460006714, acc: 0.9634369015693665)
[2025-02-13 02:27:22,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:22,477][root][INFO] - Training Epoch: 1/2, step 433/7134 completed (loss: 0.13037312030792236, acc: 0.9646739363670349)
[2025-02-13 02:27:22,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:22,915][root][INFO] - Training Epoch: 1/2, step 434/7134 completed (loss: 0.15570473670959473, acc: 0.9633967876434326)
[2025-02-13 02:27:23,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:23,375][root][INFO] - Training Epoch: 1/2, step 435/7134 completed (loss: 0.16452744603157043, acc: 0.9517730474472046)
[2025-02-13 02:27:23,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:23,785][root][INFO] - Training Epoch: 1/2, step 436/7134 completed (loss: 0.2087867707014084, acc: 0.9468504190444946)
[2025-02-13 02:27:23,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:24,191][root][INFO] - Training Epoch: 1/2, step 437/7134 completed (loss: 0.26206842064857483, acc: 0.9381625652313232)
[2025-02-13 02:27:24,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:24,586][root][INFO] - Training Epoch: 1/2, step 438/7134 completed (loss: 0.22158518433570862, acc: 0.9461538195610046)
[2025-02-13 02:27:24,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:25,051][root][INFO] - Training Epoch: 1/2, step 439/7134 completed (loss: 0.17402419447898865, acc: 0.948952853679657)
[2025-02-13 02:27:25,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:25,458][root][INFO] - Training Epoch: 1/2, step 440/7134 completed (loss: 0.2374785840511322, acc: 0.9376083016395569)
[2025-02-13 02:27:25,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:25,892][root][INFO] - Training Epoch: 1/2, step 441/7134 completed (loss: 0.1416441649198532, acc: 0.9505247473716736)
[2025-02-13 02:27:26,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:26,322][root][INFO] - Training Epoch: 1/2, step 442/7134 completed (loss: 0.13106384873390198, acc: 0.9626865386962891)
[2025-02-13 02:27:26,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:26,765][root][INFO] - Training Epoch: 1/2, step 443/7134 completed (loss: 0.172029510140419, acc: 0.9503875970840454)
[2025-02-13 02:27:26,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:27,173][root][INFO] - Training Epoch: 1/2, step 444/7134 completed (loss: 0.13473045825958252, acc: 0.9583333134651184)
[2025-02-13 02:27:27,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:27,603][root][INFO] - Training Epoch: 1/2, step 445/7134 completed (loss: 0.19680236279964447, acc: 0.9489796161651611)
[2025-02-13 02:27:27,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:28,045][root][INFO] - Training Epoch: 1/2, step 446/7134 completed (loss: 0.11979224532842636, acc: 0.961002767086029)
[2025-02-13 02:27:28,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:28,470][root][INFO] - Training Epoch: 1/2, step 447/7134 completed (loss: 0.13393057882785797, acc: 0.9639389514923096)
[2025-02-13 02:27:28,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:28,937][root][INFO] - Training Epoch: 1/2, step 448/7134 completed (loss: 0.28916412591934204, acc: 0.9207161068916321)
[2025-02-13 02:27:29,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:29,403][root][INFO] - Training Epoch: 1/2, step 449/7134 completed (loss: 0.27565997838974, acc: 0.9293193817138672)
[2025-02-13 02:27:29,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:29,874][root][INFO] - Training Epoch: 1/2, step 450/7134 completed (loss: 0.3620569705963135, acc: 0.9065817594528198)
[2025-02-13 02:27:30,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:30,342][root][INFO] - Training Epoch: 1/2, step 451/7134 completed (loss: 0.40711769461631775, acc: 0.8929845690727234)
[2025-02-13 02:27:30,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:30,806][root][INFO] - Training Epoch: 1/2, step 452/7134 completed (loss: 0.43680649995803833, acc: 0.8767123222351074)
[2025-02-13 02:27:30,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:31,199][root][INFO] - Training Epoch: 1/2, step 453/7134 completed (loss: 0.3076126277446747, acc: 0.9174311757087708)
[2025-02-13 02:27:31,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:31,672][root][INFO] - Training Epoch: 1/2, step 454/7134 completed (loss: 0.36074933409690857, acc: 0.9063509106636047)
[2025-02-13 02:27:31,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:32,133][root][INFO] - Training Epoch: 1/2, step 455/7134 completed (loss: 0.3457704484462738, acc: 0.9045751690864563)
[2025-02-13 02:27:32,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:32,583][root][INFO] - Training Epoch: 1/2, step 456/7134 completed (loss: 0.2655658423900604, acc: 0.9235127568244934)
[2025-02-13 02:27:32,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:33,031][root][INFO] - Training Epoch: 1/2, step 457/7134 completed (loss: 0.2811172604560852, acc: 0.9177852272987366)
[2025-02-13 02:27:33,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:33,442][root][INFO] - Training Epoch: 1/2, step 458/7134 completed (loss: 0.22246210277080536, acc: 0.9304174780845642)
[2025-02-13 02:27:33,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:33,845][root][INFO] - Training Epoch: 1/2, step 459/7134 completed (loss: 0.38620153069496155, acc: 0.9081481695175171)
[2025-02-13 02:27:33,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:34,260][root][INFO] - Training Epoch: 1/2, step 460/7134 completed (loss: 0.2142046093940735, acc: 0.9448819160461426)
[2025-02-13 02:27:34,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:34,723][root][INFO] - Training Epoch: 1/2, step 461/7134 completed (loss: 0.3075437545776367, acc: 0.9200561046600342)
[2025-02-13 02:27:34,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:35,230][root][INFO] - Training Epoch: 1/2, step 462/7134 completed (loss: 0.34468311071395874, acc: 0.9018691778182983)
[2025-02-13 02:27:35,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:35,718][root][INFO] - Training Epoch: 1/2, step 463/7134 completed (loss: 0.3026137053966522, acc: 0.9184441566467285)
[2025-02-13 02:27:35,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:36,182][root][INFO] - Training Epoch: 1/2, step 464/7134 completed (loss: 0.24554352462291718, acc: 0.9334277510643005)
[2025-02-13 02:27:36,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:36,607][root][INFO] - Training Epoch: 1/2, step 465/7134 completed (loss: 0.24843941628932953, acc: 0.9281984567642212)
[2025-02-13 02:27:36,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:37,065][root][INFO] - Training Epoch: 1/2, step 466/7134 completed (loss: 0.3686943054199219, acc: 0.9086229205131531)
[2025-02-13 02:27:37,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:37,522][root][INFO] - Training Epoch: 1/2, step 467/7134 completed (loss: 0.32707974314689636, acc: 0.9139297604560852)
[2025-02-13 02:27:37,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:37,910][root][INFO] - Training Epoch: 1/2, step 468/7134 completed (loss: 0.35792186856269836, acc: 0.9078404307365417)
[2025-02-13 02:27:38,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:38,344][root][INFO] - Training Epoch: 1/2, step 469/7134 completed (loss: 0.2355642169713974, acc: 0.9337176084518433)
[2025-02-13 02:27:38,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:38,762][root][INFO] - Training Epoch: 1/2, step 470/7134 completed (loss: 0.3485656678676605, acc: 0.8897958993911743)
[2025-02-13 02:27:38,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:39,205][root][INFO] - Training Epoch: 1/2, step 471/7134 completed (loss: 0.26878538727760315, acc: 0.9218934774398804)
[2025-02-13 02:27:39,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:39,653][root][INFO] - Training Epoch: 1/2, step 472/7134 completed (loss: 0.2934868037700653, acc: 0.9208211302757263)
[2025-02-13 02:27:39,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:40,061][root][INFO] - Training Epoch: 1/2, step 473/7134 completed (loss: 0.24401424825191498, acc: 0.9259259104728699)
[2025-02-13 02:27:40,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:40,507][root][INFO] - Training Epoch: 1/2, step 474/7134 completed (loss: 0.21198633313179016, acc: 0.9341238737106323)
[2025-02-13 02:27:40,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:40,930][root][INFO] - Training Epoch: 1/2, step 475/7134 completed (loss: 0.379740446805954, acc: 0.9014706015586853)
[2025-02-13 02:27:41,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:41,364][root][INFO] - Training Epoch: 1/2, step 476/7134 completed (loss: 0.322034627199173, acc: 0.9083094596862793)
[2025-02-13 02:27:41,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:41,860][root][INFO] - Training Epoch: 1/2, step 477/7134 completed (loss: 0.2419692575931549, acc: 0.932762861251831)
[2025-02-13 02:27:41,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:42,243][root][INFO] - Training Epoch: 1/2, step 478/7134 completed (loss: 0.3537032902240753, acc: 0.9112050533294678)
[2025-02-13 02:27:42,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:42,647][root][INFO] - Training Epoch: 1/2, step 479/7134 completed (loss: 0.16788215935230255, acc: 0.952654242515564)
[2025-02-13 02:27:42,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:43,059][root][INFO] - Training Epoch: 1/2, step 480/7134 completed (loss: 0.16859376430511475, acc: 0.9518248438835144)
[2025-02-13 02:27:43,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:43,506][root][INFO] - Training Epoch: 1/2, step 481/7134 completed (loss: 0.22029566764831543, acc: 0.9427083134651184)
[2025-02-13 02:27:43,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:43,929][root][INFO] - Training Epoch: 1/2, step 482/7134 completed (loss: 0.15125837922096252, acc: 0.9628571271896362)
[2025-02-13 02:27:44,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:44,405][root][INFO] - Training Epoch: 1/2, step 483/7134 completed (loss: 0.17952391505241394, acc: 0.9508599638938904)
[2025-02-13 02:27:44,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:44,836][root][INFO] - Training Epoch: 1/2, step 484/7134 completed (loss: 0.22232499718666077, acc: 0.9371657967567444)
[2025-02-13 02:27:44,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:45,256][root][INFO] - Training Epoch: 1/2, step 485/7134 completed (loss: 0.2557569146156311, acc: 0.9364598989486694)
[2025-02-13 02:27:45,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:45,683][root][INFO] - Training Epoch: 1/2, step 486/7134 completed (loss: 0.3411679267883301, acc: 0.9292762875556946)
[2025-02-13 02:27:45,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:46,108][root][INFO] - Training Epoch: 1/2, step 487/7134 completed (loss: 0.18640179932117462, acc: 0.9456681609153748)
[2025-02-13 02:27:46,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:46,504][root][INFO] - Training Epoch: 1/2, step 488/7134 completed (loss: 0.2838001549243927, acc: 0.9202898740768433)
[2025-02-13 02:27:46,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:46,926][root][INFO] - Training Epoch: 1/2, step 489/7134 completed (loss: 0.18764656782150269, acc: 0.9546218514442444)
[2025-02-13 02:27:47,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:47,355][root][INFO] - Training Epoch: 1/2, step 490/7134 completed (loss: 0.16546639800071716, acc: 0.956462562084198)
[2025-02-13 02:27:47,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:47,759][root][INFO] - Training Epoch: 1/2, step 491/7134 completed (loss: 0.1453867405653, acc: 0.9605262875556946)
[2025-02-13 02:27:47,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:48,175][root][INFO] - Training Epoch: 1/2, step 492/7134 completed (loss: 0.1234467551112175, acc: 0.9606656432151794)
[2025-02-13 02:27:48,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:48,631][root][INFO] - Training Epoch: 1/2, step 493/7134 completed (loss: 0.25613611936569214, acc: 0.9240687489509583)
[2025-02-13 02:27:48,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:49,061][root][INFO] - Training Epoch: 1/2, step 494/7134 completed (loss: 0.16923245787620544, acc: 0.9593750238418579)
[2025-02-13 02:27:49,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:49,479][root][INFO] - Training Epoch: 1/2, step 495/7134 completed (loss: 0.15592196583747864, acc: 0.9666666388511658)
[2025-02-13 02:27:49,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:49,813][root][INFO] - Training Epoch: 1/2, step 496/7134 completed (loss: 0.1842360496520996, acc: 0.9457994699478149)
[2025-02-13 02:27:49,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:50,235][root][INFO] - Training Epoch: 1/2, step 497/7134 completed (loss: 0.1696721762418747, acc: 0.9554263353347778)
[2025-02-13 02:27:50,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:50,685][root][INFO] - Training Epoch: 1/2, step 498/7134 completed (loss: 0.1674698442220688, acc: 0.9583333134651184)
[2025-02-13 02:27:50,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:51,158][root][INFO] - Training Epoch: 1/2, step 499/7134 completed (loss: 0.09606675803661346, acc: 0.9749652147293091)
[2025-02-13 02:27:51,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:51,589][root][INFO] - Training Epoch: 1/2, step 500/7134 completed (loss: 0.15664911270141602, acc: 0.9626666903495789)
[2025-02-13 02:27:51,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:52,057][root][INFO] - Training Epoch: 1/2, step 501/7134 completed (loss: 0.10682723671197891, acc: 0.9639889001846313)
[2025-02-13 02:27:52,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:52,459][root][INFO] - Training Epoch: 1/2, step 502/7134 completed (loss: 0.08532429486513138, acc: 0.9785932898521423)
[2025-02-13 02:27:52,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:52,864][root][INFO] - Training Epoch: 1/2, step 503/7134 completed (loss: 0.162911519408226, acc: 0.9557046890258789)
[2025-02-13 02:27:53,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:53,311][root][INFO] - Training Epoch: 1/2, step 504/7134 completed (loss: 0.10161270946264267, acc: 0.9678770899772644)
[2025-02-13 02:27:53,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:53,728][root][INFO] - Training Epoch: 1/2, step 505/7134 completed (loss: 0.06587622314691544, acc: 0.981675386428833)
[2025-02-13 02:27:53,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:54,200][root][INFO] - Training Epoch: 1/2, step 506/7134 completed (loss: 0.1656903475522995, acc: 0.9477611780166626)
[2025-02-13 02:27:54,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:54,699][root][INFO] - Training Epoch: 1/2, step 507/7134 completed (loss: 0.12779274582862854, acc: 0.9684210419654846)
[2025-02-13 02:27:54,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:55,158][root][INFO] - Training Epoch: 1/2, step 508/7134 completed (loss: 0.1781958043575287, acc: 0.9539105892181396)
[2025-02-13 02:27:55,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:55,654][root][INFO] - Training Epoch: 1/2, step 509/7134 completed (loss: 0.22212272882461548, acc: 0.9397217631340027)
[2025-02-13 02:27:55,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:56,033][root][INFO] - Training Epoch: 1/2, step 510/7134 completed (loss: 0.3449840247631073, acc: 0.9168398976325989)
[2025-02-13 02:27:56,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:56,439][root][INFO] - Training Epoch: 1/2, step 511/7134 completed (loss: 0.3264060318470001, acc: 0.9168704152107239)
[2025-02-13 02:27:56,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:56,860][root][INFO] - Training Epoch: 1/2, step 512/7134 completed (loss: 0.2948702573776245, acc: 0.9298596978187561)
[2025-02-13 02:27:57,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:57,271][root][INFO] - Training Epoch: 1/2, step 513/7134 completed (loss: 0.2379172146320343, acc: 0.9242684841156006)
[2025-02-13 02:27:57,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:57,599][root][INFO] - Training Epoch: 1/2, step 514/7134 completed (loss: 0.3632795214653015, acc: 0.9120603203773499)
[2025-02-13 02:27:57,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:58,073][root][INFO] - Training Epoch: 1/2, step 515/7134 completed (loss: 0.20788176357746124, acc: 0.9462517499923706)
[2025-02-13 02:27:58,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:58,398][root][INFO] - Training Epoch: 1/2, step 516/7134 completed (loss: 0.16259144246578217, acc: 0.95652174949646)
[2025-02-13 02:27:58,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:58,821][root][INFO] - Training Epoch: 1/2, step 517/7134 completed (loss: 0.27728837728500366, acc: 0.9354838728904724)
[2025-02-13 02:27:58,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:59,271][root][INFO] - Training Epoch: 1/2, step 518/7134 completed (loss: 0.1403169184923172, acc: 0.9610195159912109)
[2025-02-13 02:27:59,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:59,697][root][INFO] - Training Epoch: 1/2, step 519/7134 completed (loss: 0.19038709998130798, acc: 0.9422680139541626)
[2025-02-13 02:27:59,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:00,120][root][INFO] - Training Epoch: 1/2, step 520/7134 completed (loss: 0.18125653266906738, acc: 0.9462025165557861)
[2025-02-13 02:28:00,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:00,553][root][INFO] - Training Epoch: 1/2, step 521/7134 completed (loss: 0.21181683242321014, acc: 0.9412550330162048)
[2025-02-13 02:28:00,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:00,997][root][INFO] - Training Epoch: 1/2, step 522/7134 completed (loss: 0.18992799520492554, acc: 0.9557640552520752)
[2025-02-13 02:28:01,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:01,455][root][INFO] - Training Epoch: 1/2, step 523/7134 completed (loss: 0.1920412927865982, acc: 0.9473684430122375)
[2025-02-13 02:28:01,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:01,896][root][INFO] - Training Epoch: 1/2, step 524/7134 completed (loss: 0.15033391118049622, acc: 0.9610214829444885)
[2025-02-13 02:28:02,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:02,318][root][INFO] - Training Epoch: 1/2, step 525/7134 completed (loss: 0.2517574727535248, acc: 0.9304482340812683)
[2025-02-13 02:28:02,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:02,768][root][INFO] - Training Epoch: 1/2, step 526/7134 completed (loss: 0.16596472263336182, acc: 0.9589040875434875)
[2025-02-13 02:28:02,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:03,183][root][INFO] - Training Epoch: 1/2, step 527/7134 completed (loss: 0.19092130661010742, acc: 0.9477611780166626)
[2025-02-13 02:28:03,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:03,637][root][INFO] - Training Epoch: 1/2, step 528/7134 completed (loss: 0.0887971818447113, acc: 0.9818913340568542)
[2025-02-13 02:28:03,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:04,110][root][INFO] - Training Epoch: 1/2, step 529/7134 completed (loss: 0.2345631867647171, acc: 0.9317460060119629)
[2025-02-13 02:28:04,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:04,528][root][INFO] - Training Epoch: 1/2, step 530/7134 completed (loss: 0.179239884018898, acc: 0.9451115131378174)
[2025-02-13 02:28:04,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:04,893][root][INFO] - Training Epoch: 1/2, step 531/7134 completed (loss: 0.19793806970119476, acc: 0.952136754989624)
[2025-02-13 02:28:05,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:05,314][root][INFO] - Training Epoch: 1/2, step 532/7134 completed (loss: 0.1034039780497551, acc: 0.9729729890823364)
[2025-02-13 02:28:05,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:05,753][root][INFO] - Training Epoch: 1/2, step 533/7134 completed (loss: 0.16760146617889404, acc: 0.9450171589851379)
[2025-02-13 02:28:05,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:06,205][root][INFO] - Training Epoch: 1/2, step 534/7134 completed (loss: 0.12385625392198563, acc: 0.9615384340286255)
[2025-02-13 02:28:06,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:06,681][root][INFO] - Training Epoch: 1/2, step 535/7134 completed (loss: 0.15188556909561157, acc: 0.95782071352005)
[2025-02-13 02:28:06,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:07,128][root][INFO] - Training Epoch: 1/2, step 536/7134 completed (loss: 0.20399042963981628, acc: 0.9547445178031921)
[2025-02-13 02:28:07,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:07,543][root][INFO] - Training Epoch: 1/2, step 537/7134 completed (loss: 0.19532175362110138, acc: 0.9528718590736389)
[2025-02-13 02:28:07,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:07,975][root][INFO] - Training Epoch: 1/2, step 538/7134 completed (loss: 0.17417290806770325, acc: 0.9554896354675293)
[2025-02-13 02:28:08,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:08,400][root][INFO] - Training Epoch: 1/2, step 539/7134 completed (loss: 0.16705597937107086, acc: 0.9584774971008301)
[2025-02-13 02:28:08,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:08,804][root][INFO] - Training Epoch: 1/2, step 540/7134 completed (loss: 0.17875684797763824, acc: 0.9575757384300232)
[2025-02-13 02:28:08,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:09,257][root][INFO] - Training Epoch: 1/2, step 541/7134 completed (loss: 0.2632414698600769, acc: 0.9308510422706604)
[2025-02-13 02:28:09,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:09,595][root][INFO] - Training Epoch: 1/2, step 542/7134 completed (loss: 0.2755109965801239, acc: 0.9178403615951538)
[2025-02-13 02:28:09,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:10,006][root][INFO] - Training Epoch: 1/2, step 543/7134 completed (loss: 0.26789093017578125, acc: 0.9240506291389465)
[2025-02-13 02:28:10,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:10,443][root][INFO] - Training Epoch: 1/2, step 544/7134 completed (loss: 0.30539989471435547, acc: 0.9132491946220398)
[2025-02-13 02:28:10,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:10,922][root][INFO] - Training Epoch: 1/2, step 545/7134 completed (loss: 0.2827964425086975, acc: 0.9275362491607666)
[2025-02-13 02:28:11,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:11,346][root][INFO] - Training Epoch: 1/2, step 546/7134 completed (loss: 0.2681274116039276, acc: 0.9243499040603638)
[2025-02-13 02:28:11,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:11,806][root][INFO] - Training Epoch: 1/2, step 547/7134 completed (loss: 0.2088097482919693, acc: 0.944847583770752)
[2025-02-13 02:28:11,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:12,240][root][INFO] - Training Epoch: 1/2, step 548/7134 completed (loss: 0.13293714821338654, acc: 0.9595828056335449)
[2025-02-13 02:28:12,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:12,664][root][INFO] - Training Epoch: 1/2, step 549/7134 completed (loss: 0.23777088522911072, acc: 0.9316770434379578)
[2025-02-13 02:28:12,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:13,087][root][INFO] - Training Epoch: 1/2, step 550/7134 completed (loss: 0.1541302353143692, acc: 0.9624573588371277)
[2025-02-13 02:28:13,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:13,452][root][INFO] - Training Epoch: 1/2, step 551/7134 completed (loss: 0.22417381405830383, acc: 0.9373849034309387)
[2025-02-13 02:28:13,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:13,870][root][INFO] - Training Epoch: 1/2, step 552/7134 completed (loss: 0.18438223004341125, acc: 0.9558404684066772)
[2025-02-13 02:28:14,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:14,245][root][INFO] - Training Epoch: 1/2, step 553/7134 completed (loss: 0.16651864349842072, acc: 0.9583333134651184)
[2025-02-13 02:28:14,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:14,703][root][INFO] - Training Epoch: 1/2, step 554/7134 completed (loss: 0.1672097146511078, acc: 0.9614835977554321)
[2025-02-13 02:28:14,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:15,129][root][INFO] - Training Epoch: 1/2, step 555/7134 completed (loss: 0.2048666924238205, acc: 0.9478672742843628)
[2025-02-13 02:28:15,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:15,555][root][INFO] - Training Epoch: 1/2, step 556/7134 completed (loss: 0.124444380402565, acc: 0.9702970385551453)
[2025-02-13 02:28:15,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:15,969][root][INFO] - Training Epoch: 1/2, step 557/7134 completed (loss: 0.18538640439510345, acc: 0.9475465416908264)
[2025-02-13 02:28:16,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:16,401][root][INFO] - Training Epoch: 1/2, step 558/7134 completed (loss: 0.16142840683460236, acc: 0.9560283422470093)
[2025-02-13 02:28:16,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:16,865][root][INFO] - Training Epoch: 1/2, step 559/7134 completed (loss: 0.22849197685718536, acc: 0.9316239356994629)
[2025-02-13 02:28:16,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:17,278][root][INFO] - Training Epoch: 1/2, step 560/7134 completed (loss: 0.15850766003131866, acc: 0.9525483250617981)
[2025-02-13 02:28:17,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:17,763][root][INFO] - Training Epoch: 1/2, step 561/7134 completed (loss: 0.20069383084774017, acc: 0.9399744868278503)
[2025-02-13 02:28:17,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:18,182][root][INFO] - Training Epoch: 1/2, step 562/7134 completed (loss: 0.14976036548614502, acc: 0.9549669027328491)
[2025-02-13 02:28:18,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:18,584][root][INFO] - Training Epoch: 1/2, step 563/7134 completed (loss: 0.2737240791320801, acc: 0.922468364238739)
[2025-02-13 02:28:18,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:19,046][root][INFO] - Training Epoch: 1/2, step 564/7134 completed (loss: 0.1767401248216629, acc: 0.9493201375007629)
[2025-02-13 02:28:19,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:19,452][root][INFO] - Training Epoch: 1/2, step 565/7134 completed (loss: 0.1745561957359314, acc: 0.9564489126205444)
[2025-02-13 02:28:19,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:19,852][root][INFO] - Training Epoch: 1/2, step 566/7134 completed (loss: 0.23862458765506744, acc: 0.9466357231140137)
[2025-02-13 02:28:20,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:20,290][root][INFO] - Training Epoch: 1/2, step 567/7134 completed (loss: 0.17803826928138733, acc: 0.9607293009757996)
[2025-02-13 02:28:20,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:20,727][root][INFO] - Training Epoch: 1/2, step 568/7134 completed (loss: 0.18401886522769928, acc: 0.9464052319526672)
[2025-02-13 02:28:20,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:21,160][root][INFO] - Training Epoch: 1/2, step 569/7134 completed (loss: 0.1598767340183258, acc: 0.9555555582046509)
[2025-02-13 02:28:21,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:21,636][root][INFO] - Training Epoch: 1/2, step 570/7134 completed (loss: 0.19269618391990662, acc: 0.9388954043388367)
[2025-02-13 02:28:21,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:22,090][root][INFO] - Training Epoch: 1/2, step 571/7134 completed (loss: 0.1635274589061737, acc: 0.9559193849563599)
[2025-02-13 02:28:22,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:22,529][root][INFO] - Training Epoch: 1/2, step 572/7134 completed (loss: 0.11138032376766205, acc: 0.9626769423484802)
[2025-02-13 02:28:22,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:23,004][root][INFO] - Training Epoch: 1/2, step 573/7134 completed (loss: 0.12899711728096008, acc: 0.9629155993461609)
[2025-02-13 02:28:23,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:23,411][root][INFO] - Training Epoch: 1/2, step 574/7134 completed (loss: 0.1372065246105194, acc: 0.9683794379234314)
[2025-02-13 02:28:23,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:23,899][root][INFO] - Training Epoch: 1/2, step 575/7134 completed (loss: 0.14730539917945862, acc: 0.9627249240875244)
[2025-02-13 02:28:24,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:24,334][root][INFO] - Training Epoch: 1/2, step 576/7134 completed (loss: 0.12218122184276581, acc: 0.9678217768669128)
[2025-02-13 02:28:24,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:24,781][root][INFO] - Training Epoch: 1/2, step 577/7134 completed (loss: 0.15777915716171265, acc: 0.9552041888237)
[2025-02-13 02:28:24,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:25,263][root][INFO] - Training Epoch: 1/2, step 578/7134 completed (loss: 0.18783821165561676, acc: 0.9373549818992615)
[2025-02-13 02:28:25,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:25,682][root][INFO] - Training Epoch: 1/2, step 579/7134 completed (loss: 0.2701566219329834, acc: 0.9229781627655029)
[2025-02-13 02:28:25,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:26,119][root][INFO] - Training Epoch: 1/2, step 580/7134 completed (loss: 0.2058863788843155, acc: 0.9401820302009583)
[2025-02-13 02:28:26,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:26,522][root][INFO] - Training Epoch: 1/2, step 581/7134 completed (loss: 0.14108087122440338, acc: 0.9595537185668945)
[2025-02-13 02:28:26,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:26,974][root][INFO] - Training Epoch: 1/2, step 582/7134 completed (loss: 0.1686624437570572, acc: 0.9580838084220886)
[2025-02-13 02:28:27,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:27,426][root][INFO] - Training Epoch: 1/2, step 583/7134 completed (loss: 0.12759821116924286, acc: 0.9623376727104187)
[2025-02-13 02:28:27,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:27,889][root][INFO] - Training Epoch: 1/2, step 584/7134 completed (loss: 0.12452694028615952, acc: 0.9665071964263916)
[2025-02-13 02:28:28,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:28,323][root][INFO] - Training Epoch: 1/2, step 585/7134 completed (loss: 0.16954141855239868, acc: 0.9528985619544983)
[2025-02-13 02:28:28,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:28,742][root][INFO] - Training Epoch: 1/2, step 586/7134 completed (loss: 0.16834937036037445, acc: 0.9543676376342773)
[2025-02-13 02:28:28,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:29,183][root][INFO] - Training Epoch: 1/2, step 587/7134 completed (loss: 0.14479215443134308, acc: 0.9644218683242798)
[2025-02-13 02:28:29,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:29,633][root][INFO] - Training Epoch: 1/2, step 588/7134 completed (loss: 0.18824614584445953, acc: 0.9544740915298462)
[2025-02-13 02:28:29,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:30,071][root][INFO] - Training Epoch: 1/2, step 589/7134 completed (loss: 0.1547766774892807, acc: 0.9632892608642578)
[2025-02-13 02:28:30,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:30,505][root][INFO] - Training Epoch: 1/2, step 590/7134 completed (loss: 0.06903763860464096, acc: 0.9729323387145996)
[2025-02-13 02:28:30,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:30,952][root][INFO] - Training Epoch: 1/2, step 591/7134 completed (loss: 0.14499208331108093, acc: 0.9673123359680176)
[2025-02-13 02:28:31,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:31,380][root][INFO] - Training Epoch: 1/2, step 592/7134 completed (loss: 0.1397918313741684, acc: 0.9593908786773682)
[2025-02-13 02:28:31,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:31,807][root][INFO] - Training Epoch: 1/2, step 593/7134 completed (loss: 0.126477912068367, acc: 0.9566929340362549)
[2025-02-13 02:28:31,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:32,259][root][INFO] - Training Epoch: 1/2, step 594/7134 completed (loss: 0.15192359685897827, acc: 0.952201247215271)
[2025-02-13 02:28:32,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:32,752][root][INFO] - Training Epoch: 1/2, step 595/7134 completed (loss: 0.183546781539917, acc: 0.9532237648963928)
[2025-02-13 02:28:32,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:33,195][root][INFO] - Training Epoch: 1/2, step 596/7134 completed (loss: 0.10251060128211975, acc: 0.967783510684967)
[2025-02-13 02:28:33,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:33,670][root][INFO] - Training Epoch: 1/2, step 597/7134 completed (loss: 0.2941897213459015, acc: 0.9196310639381409)
[2025-02-13 02:28:33,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:34,118][root][INFO] - Training Epoch: 1/2, step 598/7134 completed (loss: 0.23835813999176025, acc: 0.9364005327224731)
[2025-02-13 02:28:34,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:34,559][root][INFO] - Training Epoch: 1/2, step 599/7134 completed (loss: 0.1767025887966156, acc: 0.9473684430122375)
[2025-02-13 02:28:34,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:35,013][root][INFO] - Training Epoch: 1/2, step 600/7134 completed (loss: 0.1687031239271164, acc: 0.9554753303527832)
[2025-02-13 02:28:35,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:35,482][root][INFO] - Training Epoch: 1/2, step 601/7134 completed (loss: 0.16695208847522736, acc: 0.954356849193573)
[2025-02-13 02:28:35,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:35,929][root][INFO] - Training Epoch: 1/2, step 602/7134 completed (loss: 0.19395411014556885, acc: 0.9489558935165405)
[2025-02-13 02:28:36,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:36,376][root][INFO] - Training Epoch: 1/2, step 603/7134 completed (loss: 0.20807263255119324, acc: 0.9460154175758362)
[2025-02-13 02:28:36,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:36,766][root][INFO] - Training Epoch: 1/2, step 604/7134 completed (loss: 0.19552665948867798, acc: 0.9355783462524414)
[2025-02-13 02:28:36,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:37,215][root][INFO] - Training Epoch: 1/2, step 605/7134 completed (loss: 0.2068033367395401, acc: 0.9443005323410034)
[2025-02-13 02:28:37,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:37,661][root][INFO] - Training Epoch: 1/2, step 606/7134 completed (loss: 0.2929333448410034, acc: 0.9344870448112488)
[2025-02-13 02:28:37,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:38,107][root][INFO] - Training Epoch: 1/2, step 607/7134 completed (loss: 0.1917106956243515, acc: 0.9479305744171143)
[2025-02-13 02:28:38,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:38,574][root][INFO] - Training Epoch: 1/2, step 608/7134 completed (loss: 0.15064287185668945, acc: 0.9598445892333984)
[2025-02-13 02:28:38,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:39,036][root][INFO] - Training Epoch: 1/2, step 609/7134 completed (loss: 0.22188059985637665, acc: 0.9332552552223206)
[2025-02-13 02:28:39,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:39,498][root][INFO] - Training Epoch: 1/2, step 610/7134 completed (loss: 0.21639131009578705, acc: 0.9355246424674988)
[2025-02-13 02:28:39,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:39,953][root][INFO] - Training Epoch: 1/2, step 611/7134 completed (loss: 0.18018752336502075, acc: 0.9512194991111755)
[2025-02-13 02:28:40,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:40,381][root][INFO] - Training Epoch: 1/2, step 612/7134 completed (loss: 0.16229093074798584, acc: 0.9459102749824524)
[2025-02-13 02:28:40,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:40,845][root][INFO] - Training Epoch: 1/2, step 613/7134 completed (loss: 0.20981323719024658, acc: 0.9378663301467896)
[2025-02-13 02:28:40,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:41,295][root][INFO] - Training Epoch: 1/2, step 614/7134 completed (loss: 0.15546640753746033, acc: 0.9580973982810974)
[2025-02-13 02:28:41,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:41,751][root][INFO] - Training Epoch: 1/2, step 615/7134 completed (loss: 0.18949252367019653, acc: 0.9496932625770569)
[2025-02-13 02:28:41,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:42,230][root][INFO] - Training Epoch: 1/2, step 616/7134 completed (loss: 0.17833825945854187, acc: 0.9511363506317139)
[2025-02-13 02:28:42,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:42,687][root][INFO] - Training Epoch: 1/2, step 617/7134 completed (loss: 0.16792775690555573, acc: 0.9589977264404297)
[2025-02-13 02:28:42,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:43,123][root][INFO] - Training Epoch: 1/2, step 618/7134 completed (loss: 0.18444491922855377, acc: 0.9491978883743286)
[2025-02-13 02:28:43,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:43,483][root][INFO] - Training Epoch: 1/2, step 619/7134 completed (loss: 0.22124451398849487, acc: 0.9453441500663757)
[2025-02-13 02:28:43,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:43,936][root][INFO] - Training Epoch: 1/2, step 620/7134 completed (loss: 0.2576168477535248, acc: 0.9442675113677979)
[2025-02-13 02:28:44,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:44,365][root][INFO] - Training Epoch: 1/2, step 621/7134 completed (loss: 0.15215946733951569, acc: 0.9543378949165344)
[2025-02-13 02:28:44,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:44,794][root][INFO] - Training Epoch: 1/2, step 622/7134 completed (loss: 0.16780687868595123, acc: 0.9512194991111755)
[2025-02-13 02:28:44,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:45,199][root][INFO] - Training Epoch: 1/2, step 623/7134 completed (loss: 0.21891288459300995, acc: 0.9518950581550598)
[2025-02-13 02:28:45,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:45,648][root][INFO] - Training Epoch: 1/2, step 624/7134 completed (loss: 0.19493865966796875, acc: 0.9527027010917664)
[2025-02-13 02:28:45,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:46,115][root][INFO] - Training Epoch: 1/2, step 625/7134 completed (loss: 0.18033641576766968, acc: 0.9445910453796387)
[2025-02-13 02:28:46,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:46,527][root][INFO] - Training Epoch: 1/2, step 626/7134 completed (loss: 0.1714807003736496, acc: 0.9553264379501343)
[2025-02-13 02:28:46,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:46,952][root][INFO] - Training Epoch: 1/2, step 627/7134 completed (loss: 0.166581392288208, acc: 0.9454277157783508)
[2025-02-13 02:28:47,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:47,374][root][INFO] - Training Epoch: 1/2, step 628/7134 completed (loss: 0.1401783525943756, acc: 0.957004189491272)
[2025-02-13 02:28:47,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:47,798][root][INFO] - Training Epoch: 1/2, step 629/7134 completed (loss: 0.1653779149055481, acc: 0.9570815563201904)
[2025-02-13 02:28:47,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:48,246][root][INFO] - Training Epoch: 1/2, step 630/7134 completed (loss: 0.16161295771598816, acc: 0.9559585452079773)
[2025-02-13 02:28:48,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:48,664][root][INFO] - Training Epoch: 1/2, step 631/7134 completed (loss: 0.13085904717445374, acc: 0.9605262875556946)
[2025-02-13 02:28:48,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:49,105][root][INFO] - Training Epoch: 1/2, step 632/7134 completed (loss: 0.14592626690864563, acc: 0.9611650705337524)
[2025-02-13 02:28:49,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:49,517][root][INFO] - Training Epoch: 1/2, step 633/7134 completed (loss: 0.16104631125926971, acc: 0.9535558819770813)
[2025-02-13 02:28:49,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:49,930][root][INFO] - Training Epoch: 1/2, step 634/7134 completed (loss: 0.1401321142911911, acc: 0.9588607549667358)
[2025-02-13 02:28:50,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:50,364][root][INFO] - Training Epoch: 1/2, step 635/7134 completed (loss: 0.17894689738750458, acc: 0.9450381398200989)
[2025-02-13 02:28:50,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:50,811][root][INFO] - Training Epoch: 1/2, step 636/7134 completed (loss: 0.1762671321630478, acc: 0.9572413563728333)
[2025-02-13 02:28:50,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:51,214][root][INFO] - Training Epoch: 1/2, step 637/7134 completed (loss: 0.13566331565380096, acc: 0.9702048301696777)
[2025-02-13 02:28:51,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:51,626][root][INFO] - Training Epoch: 1/2, step 638/7134 completed (loss: 0.2134999781847, acc: 0.9516380429267883)
[2025-02-13 02:28:51,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:52,033][root][INFO] - Training Epoch: 1/2, step 639/7134 completed (loss: 0.17783202230930328, acc: 0.9466666579246521)
[2025-02-13 02:28:52,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:52,430][root][INFO] - Training Epoch: 1/2, step 640/7134 completed (loss: 0.19105027616024017, acc: 0.947826087474823)
[2025-02-13 02:28:52,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:52,834][root][INFO] - Training Epoch: 1/2, step 641/7134 completed (loss: 0.11028873175382614, acc: 0.9705401062965393)
[2025-02-13 02:28:52,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:53,263][root][INFO] - Training Epoch: 1/2, step 642/7134 completed (loss: 0.12361599504947662, acc: 0.9625850319862366)
[2025-02-13 02:28:53,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:53,697][root][INFO] - Training Epoch: 1/2, step 643/7134 completed (loss: 0.09847338497638702, acc: 0.9640564918518066)
[2025-02-13 02:28:53,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:54,138][root][INFO] - Training Epoch: 1/2, step 644/7134 completed (loss: 0.13501203060150146, acc: 0.9555984735488892)
[2025-02-13 02:28:54,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:54,539][root][INFO] - Training Epoch: 1/2, step 645/7134 completed (loss: 0.17877952754497528, acc: 0.9528619647026062)
[2025-02-13 02:28:54,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:54,963][root][INFO] - Training Epoch: 1/2, step 646/7134 completed (loss: 0.19581739604473114, acc: 0.9403669834136963)
[2025-02-13 02:28:55,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:55,390][root][INFO] - Training Epoch: 1/2, step 647/7134 completed (loss: 0.13214249908924103, acc: 0.9678511023521423)
[2025-02-13 02:28:55,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:55,824][root][INFO] - Training Epoch: 1/2, step 648/7134 completed (loss: 0.3025844991207123, acc: 0.9284467697143555)
[2025-02-13 02:28:55,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:56,272][root][INFO] - Training Epoch: 1/2, step 649/7134 completed (loss: 0.1711151897907257, acc: 0.9487179517745972)
[2025-02-13 02:28:56,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:56,698][root][INFO] - Training Epoch: 1/2, step 650/7134 completed (loss: 0.21704037487506866, acc: 0.948194682598114)
[2025-02-13 02:28:56,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:57,128][root][INFO] - Training Epoch: 1/2, step 651/7134 completed (loss: 0.41102054715156555, acc: 0.9021406769752502)
[2025-02-13 02:28:57,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:57,561][root][INFO] - Training Epoch: 1/2, step 652/7134 completed (loss: 0.22138062119483948, acc: 0.9347078800201416)
[2025-02-13 02:28:57,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:57,980][root][INFO] - Training Epoch: 1/2, step 653/7134 completed (loss: 0.20496919751167297, acc: 0.937588632106781)
[2025-02-13 02:28:58,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:58,381][root][INFO] - Training Epoch: 1/2, step 654/7134 completed (loss: 0.3534645736217499, acc: 0.9071274399757385)
[2025-02-13 02:28:58,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:58,831][root][INFO] - Training Epoch: 1/2, step 655/7134 completed (loss: 0.22857415676116943, acc: 0.9395161271095276)
[2025-02-13 02:28:58,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:59,260][root][INFO] - Training Epoch: 1/2, step 656/7134 completed (loss: 0.26049768924713135, acc: 0.934036910533905)
[2025-02-13 02:28:59,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:59,716][root][INFO] - Training Epoch: 1/2, step 657/7134 completed (loss: 0.22712725400924683, acc: 0.9309791326522827)
[2025-02-13 02:28:59,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:00,148][root][INFO] - Training Epoch: 1/2, step 658/7134 completed (loss: 0.21271651983261108, acc: 0.9499341249465942)
[2025-02-13 02:29:00,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:00,600][root][INFO] - Training Epoch: 1/2, step 659/7134 completed (loss: 0.2169514149427414, acc: 0.9281525015830994)
[2025-02-13 02:29:00,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:01,063][root][INFO] - Training Epoch: 1/2, step 660/7134 completed (loss: 0.22642171382904053, acc: 0.9459064602851868)
[2025-02-13 02:29:01,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:01,538][root][INFO] - Training Epoch: 1/2, step 661/7134 completed (loss: 0.2545134425163269, acc: 0.939330518245697)
[2025-02-13 02:29:01,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:02,019][root][INFO] - Training Epoch: 1/2, step 662/7134 completed (loss: 0.166092187166214, acc: 0.9517426490783691)
[2025-02-13 02:29:02,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:02,524][root][INFO] - Training Epoch: 1/2, step 663/7134 completed (loss: 0.21555203199386597, acc: 0.9416385889053345)
[2025-02-13 02:29:02,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:02,964][root][INFO] - Training Epoch: 1/2, step 664/7134 completed (loss: 0.16541650891304016, acc: 0.9583333134651184)
[2025-02-13 02:29:03,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:03,398][root][INFO] - Training Epoch: 1/2, step 665/7134 completed (loss: 0.18853460252285004, acc: 0.9502018690109253)
[2025-02-13 02:29:03,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:03,873][root][INFO] - Training Epoch: 1/2, step 666/7134 completed (loss: 0.25234341621398926, acc: 0.9313187003135681)
[2025-02-13 02:29:04,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:04,327][root][INFO] - Training Epoch: 1/2, step 667/7134 completed (loss: 0.21986667811870575, acc: 0.9440353512763977)
[2025-02-13 02:29:04,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:04,776][root][INFO] - Training Epoch: 1/2, step 668/7134 completed (loss: 0.12808173894882202, acc: 0.9670469164848328)
[2025-02-13 02:29:04,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:05,250][root][INFO] - Training Epoch: 1/2, step 669/7134 completed (loss: 0.15752267837524414, acc: 0.9604365825653076)
[2025-02-13 02:29:05,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:05,741][root][INFO] - Training Epoch: 1/2, step 670/7134 completed (loss: 0.2138551026582718, acc: 0.9445585012435913)
[2025-02-13 02:29:05,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:06,244][root][INFO] - Training Epoch: 1/2, step 671/7134 completed (loss: 0.19853341579437256, acc: 0.9421221613883972)
[2025-02-13 02:29:06,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:06,677][root][INFO] - Training Epoch: 1/2, step 672/7134 completed (loss: 0.1715976744890213, acc: 0.9552906155586243)
[2025-02-13 02:29:06,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:07,122][root][INFO] - Training Epoch: 1/2, step 673/7134 completed (loss: 0.1961270123720169, acc: 0.952088475227356)
[2025-02-13 02:29:07,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:07,570][root][INFO] - Training Epoch: 1/2, step 674/7134 completed (loss: 0.23332522809505463, acc: 0.9396681785583496)
[2025-02-13 02:29:07,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:08,031][root][INFO] - Training Epoch: 1/2, step 675/7134 completed (loss: 0.19191208481788635, acc: 0.9530639052391052)
[2025-02-13 02:29:08,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:08,471][root][INFO] - Training Epoch: 1/2, step 676/7134 completed (loss: 0.16361463069915771, acc: 0.9597632884979248)
[2025-02-13 02:29:08,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:08,935][root][INFO] - Training Epoch: 1/2, step 677/7134 completed (loss: 0.11877705901861191, acc: 0.9642416834831238)
[2025-02-13 02:29:09,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:09,387][root][INFO] - Training Epoch: 1/2, step 678/7134 completed (loss: 0.21725215017795563, acc: 0.9367429614067078)
[2025-02-13 02:29:09,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:09,779][root][INFO] - Training Epoch: 1/2, step 679/7134 completed (loss: 0.09651636332273483, acc: 0.9754253029823303)
[2025-02-13 02:29:09,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:10,226][root][INFO] - Training Epoch: 1/2, step 680/7134 completed (loss: 0.2467256337404251, acc: 0.9353099465370178)
[2025-02-13 02:29:10,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:10,700][root][INFO] - Training Epoch: 1/2, step 681/7134 completed (loss: 0.13830094039440155, acc: 0.9582822322845459)
[2025-02-13 02:29:10,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:11,121][root][INFO] - Training Epoch: 1/2, step 682/7134 completed (loss: 0.1264866292476654, acc: 0.9711815714836121)
[2025-02-13 02:29:11,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:11,593][root][INFO] - Training Epoch: 1/2, step 683/7134 completed (loss: 0.14857375621795654, acc: 0.9753086566925049)
[2025-02-13 02:29:11,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:12,080][root][INFO] - Training Epoch: 1/2, step 684/7134 completed (loss: 0.16795188188552856, acc: 0.9612188339233398)
[2025-02-13 02:29:12,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:12,528][root][INFO] - Training Epoch: 1/2, step 685/7134 completed (loss: 0.17468424141407013, acc: 0.9513715505599976)
[2025-02-13 02:29:12,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:12,978][root][INFO] - Training Epoch: 1/2, step 686/7134 completed (loss: 0.10484246164560318, acc: 0.9696168899536133)
[2025-02-13 02:29:13,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:13,427][root][INFO] - Training Epoch: 1/2, step 687/7134 completed (loss: 0.18028457462787628, acc: 0.9477756023406982)
[2025-02-13 02:29:13,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:13,880][root][INFO] - Training Epoch: 1/2, step 688/7134 completed (loss: 0.07721526175737381, acc: 0.9778106212615967)
[2025-02-13 02:29:14,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:14,291][root][INFO] - Training Epoch: 1/2, step 689/7134 completed (loss: 0.08306704461574554, acc: 0.9709401726722717)
[2025-02-13 02:29:14,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:14,717][root][INFO] - Training Epoch: 1/2, step 690/7134 completed (loss: 0.17454107105731964, acc: 0.9570747017860413)
[2025-02-13 02:29:14,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:15,131][root][INFO] - Training Epoch: 1/2, step 691/7134 completed (loss: 0.1408659666776657, acc: 0.9640804529190063)
[2025-02-13 02:29:15,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:15,583][root][INFO] - Training Epoch: 1/2, step 692/7134 completed (loss: 0.16889409720897675, acc: 0.9506666660308838)
[2025-02-13 02:29:15,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:16,005][root][INFO] - Training Epoch: 1/2, step 693/7134 completed (loss: 0.15065020322799683, acc: 0.9563812613487244)
[2025-02-13 02:29:16,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:16,426][root][INFO] - Training Epoch: 1/2, step 694/7134 completed (loss: 0.1379646360874176, acc: 0.962043821811676)
[2025-02-13 02:29:16,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:16,872][root][INFO] - Training Epoch: 1/2, step 695/7134 completed (loss: 0.16125614941120148, acc: 0.9621342420578003)
[2025-02-13 02:29:17,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:17,279][root][INFO] - Training Epoch: 1/2, step 696/7134 completed (loss: 0.22493615746498108, acc: 0.9417475461959839)
[2025-02-13 02:29:17,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:17,736][root][INFO] - Training Epoch: 1/2, step 697/7134 completed (loss: 0.12717774510383606, acc: 0.9702233076095581)
[2025-02-13 02:29:17,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:18,138][root][INFO] - Training Epoch: 1/2, step 698/7134 completed (loss: 0.12230758368968964, acc: 0.9641693830490112)
[2025-02-13 02:29:18,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:18,610][root][INFO] - Training Epoch: 1/2, step 699/7134 completed (loss: 0.0777633860707283, acc: 0.9781931638717651)
[2025-02-13 02:29:18,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:19,055][root][INFO] - Training Epoch: 1/2, step 700/7134 completed (loss: 0.1532142460346222, acc: 0.9626865386962891)
[2025-02-13 02:29:19,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:19,460][root][INFO] - Training Epoch: 1/2, step 701/7134 completed (loss: 0.13633884489536285, acc: 0.9592198729515076)
[2025-02-13 02:29:19,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:19,900][root][INFO] - Training Epoch: 1/2, step 702/7134 completed (loss: 0.1179543063044548, acc: 0.9642857313156128)
[2025-02-13 02:29:20,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:20,314][root][INFO] - Training Epoch: 1/2, step 703/7134 completed (loss: 0.11984960734844208, acc: 0.9725363254547119)
[2025-02-13 02:29:20,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:20,772][root][INFO] - Training Epoch: 1/2, step 704/7134 completed (loss: 0.14661088585853577, acc: 0.9589665532112122)
[2025-02-13 02:29:20,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:21,244][root][INFO] - Training Epoch: 1/2, step 705/7134 completed (loss: 0.1950605809688568, acc: 0.9463686943054199)
[2025-02-13 02:29:21,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:21,707][root][INFO] - Training Epoch: 1/2, step 706/7134 completed (loss: 0.1447761356830597, acc: 0.9529540538787842)
[2025-02-13 02:29:21,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:22,149][root][INFO] - Training Epoch: 1/2, step 707/7134 completed (loss: 0.17390386760234833, acc: 0.9568345546722412)
[2025-02-13 02:29:22,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:22,619][root][INFO] - Training Epoch: 1/2, step 708/7134 completed (loss: 0.11856238543987274, acc: 0.9578686356544495)
[2025-02-13 02:29:22,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:23,062][root][INFO] - Training Epoch: 1/2, step 709/7134 completed (loss: 0.13281990587711334, acc: 0.9606205224990845)
[2025-02-13 02:29:23,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:23,526][root][INFO] - Training Epoch: 1/2, step 710/7134 completed (loss: 0.14590424299240112, acc: 0.962469756603241)
[2025-02-13 02:29:23,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:23,954][root][INFO] - Training Epoch: 1/2, step 711/7134 completed (loss: 0.14904344081878662, acc: 0.9622195959091187)
[2025-02-13 02:29:24,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:24,391][root][INFO] - Training Epoch: 1/2, step 712/7134 completed (loss: 0.10162688791751862, acc: 0.9694749712944031)
[2025-02-13 02:29:24,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:24,856][root][INFO] - Training Epoch: 1/2, step 713/7134 completed (loss: 0.10132142901420593, acc: 0.9723183512687683)
[2025-02-13 02:29:24,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:25,183][root][INFO] - Training Epoch: 1/2, step 714/7134 completed (loss: 0.10149835050106049, acc: 0.97826087474823)
[2025-02-13 02:29:25,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:25,658][root][INFO] - Training Epoch: 1/2, step 715/7134 completed (loss: 0.11060740798711777, acc: 0.96875)
[2025-02-13 02:29:25,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:26,109][root][INFO] - Training Epoch: 1/2, step 716/7134 completed (loss: 0.1208280473947525, acc: 0.9635949730873108)
[2025-02-13 02:29:26,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:26,546][root][INFO] - Training Epoch: 1/2, step 717/7134 completed (loss: 0.12519949674606323, acc: 0.9633375406265259)
[2025-02-13 02:29:26,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:27,013][root][INFO] - Training Epoch: 1/2, step 718/7134 completed (loss: 0.13739164173603058, acc: 0.9561403393745422)
[2025-02-13 02:29:27,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:27,370][root][INFO] - Training Epoch: 1/2, step 719/7134 completed (loss: 0.07643646746873856, acc: 0.9741379022598267)
[2025-02-13 02:29:27,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:27,820][root][INFO] - Training Epoch: 1/2, step 720/7134 completed (loss: 0.11023246496915817, acc: 0.9736841917037964)
[2025-02-13 02:29:27,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:28,288][root][INFO] - Training Epoch: 1/2, step 721/7134 completed (loss: 0.15956953167915344, acc: 0.9551856517791748)
[2025-02-13 02:29:28,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:28,801][root][INFO] - Training Epoch: 1/2, step 722/7134 completed (loss: 0.13152927160263062, acc: 0.9614936113357544)
[2025-02-13 02:29:28,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:29,249][root][INFO] - Training Epoch: 1/2, step 723/7134 completed (loss: 0.07898669689893723, acc: 0.9782082438468933)
[2025-02-13 02:29:29,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:29,673][root][INFO] - Training Epoch: 1/2, step 724/7134 completed (loss: 0.10639068484306335, acc: 0.9642346501350403)
[2025-02-13 02:29:29,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:30,121][root][INFO] - Training Epoch: 1/2, step 725/7134 completed (loss: 0.08782825618982315, acc: 0.9753979444503784)
[2025-02-13 02:29:30,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:30,567][root][INFO] - Training Epoch: 1/2, step 726/7134 completed (loss: 0.10432200133800507, acc: 0.9748954176902771)
[2025-02-13 02:29:30,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:31,000][root][INFO] - Training Epoch: 1/2, step 727/7134 completed (loss: 0.11239630728960037, acc: 0.9696969985961914)
[2025-02-13 02:29:31,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:31,433][root][INFO] - Training Epoch: 1/2, step 728/7134 completed (loss: 0.08111035078763962, acc: 0.9724518060684204)
[2025-02-13 02:29:31,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:31,896][root][INFO] - Training Epoch: 1/2, step 729/7134 completed (loss: 0.10718607157468796, acc: 0.9652295112609863)
[2025-02-13 02:29:32,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:32,350][root][INFO] - Training Epoch: 1/2, step 730/7134 completed (loss: 0.11199803650379181, acc: 0.9675745964050293)
[2025-02-13 02:29:32,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:32,801][root][INFO] - Training Epoch: 1/2, step 731/7134 completed (loss: 0.08331437408924103, acc: 0.9866666793823242)
[2025-02-13 02:29:32,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:33,222][root][INFO] - Training Epoch: 1/2, step 732/7134 completed (loss: 0.22013737261295319, acc: 0.9452829957008362)
[2025-02-13 02:29:33,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:33,601][root][INFO] - Training Epoch: 1/2, step 733/7134 completed (loss: 0.1273488849401474, acc: 0.962699830532074)
[2025-02-13 02:29:33,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:34,006][root][INFO] - Training Epoch: 1/2, step 734/7134 completed (loss: 0.11130916327238083, acc: 0.9723502397537231)
[2025-02-13 02:29:34,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:34,423][root][INFO] - Training Epoch: 1/2, step 735/7134 completed (loss: 0.12290451675653458, acc: 0.9646302461624146)
[2025-02-13 02:29:34,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:34,829][root][INFO] - Training Epoch: 1/2, step 736/7134 completed (loss: 0.18651288747787476, acc: 0.9472527503967285)
[2025-02-13 02:29:34,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:35,251][root][INFO] - Training Epoch: 1/2, step 737/7134 completed (loss: 0.25803765654563904, acc: 0.9350912570953369)
[2025-02-13 02:29:35,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:35,658][root][INFO] - Training Epoch: 1/2, step 738/7134 completed (loss: 0.07791446894407272, acc: 0.9750000238418579)
[2025-02-13 02:29:35,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:36,079][root][INFO] - Training Epoch: 1/2, step 739/7134 completed (loss: 0.14844000339508057, acc: 0.9522935748100281)
[2025-02-13 02:29:36,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:36,483][root][INFO] - Training Epoch: 1/2, step 740/7134 completed (loss: 0.12772250175476074, acc: 0.9539682269096375)
[2025-02-13 02:29:36,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:36,897][root][INFO] - Training Epoch: 1/2, step 741/7134 completed (loss: 0.1618538349866867, acc: 0.9574132561683655)
[2025-02-13 02:29:37,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:37,261][root][INFO] - Training Epoch: 1/2, step 742/7134 completed (loss: 0.06661137193441391, acc: 0.9769392013549805)
[2025-02-13 02:29:37,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:37,709][root][INFO] - Training Epoch: 1/2, step 743/7134 completed (loss: 0.1504373848438263, acc: 0.9623494148254395)
[2025-02-13 02:29:37,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:38,108][root][INFO] - Training Epoch: 1/2, step 744/7134 completed (loss: 0.16416007280349731, acc: 0.9563491940498352)
[2025-02-13 02:29:38,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:38,523][root][INFO] - Training Epoch: 1/2, step 745/7134 completed (loss: 0.104719378054142, acc: 0.9682779312133789)
[2025-02-13 02:29:38,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:38,956][root][INFO] - Training Epoch: 1/2, step 746/7134 completed (loss: 0.15200279653072357, acc: 0.9556650519371033)
[2025-02-13 02:29:39,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:39,329][root][INFO] - Training Epoch: 1/2, step 747/7134 completed (loss: 0.1890370100736618, acc: 0.945182740688324)
[2025-02-13 02:29:39,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:39,806][root][INFO] - Training Epoch: 1/2, step 748/7134 completed (loss: 0.1285121589899063, acc: 0.9563909769058228)
[2025-02-13 02:29:39,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:40,232][root][INFO] - Training Epoch: 1/2, step 749/7134 completed (loss: 0.08191518485546112, acc: 0.9755043387413025)
[2025-02-13 02:29:40,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:40,685][root][INFO] - Training Epoch: 1/2, step 750/7134 completed (loss: 0.12598419189453125, acc: 0.9747747778892517)
[2025-02-13 02:29:40,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:41,100][root][INFO] - Training Epoch: 1/2, step 751/7134 completed (loss: 0.1265607476234436, acc: 0.9642248749732971)
[2025-02-13 02:29:41,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:41,464][root][INFO] - Training Epoch: 1/2, step 752/7134 completed (loss: 0.1012648269534111, acc: 0.9819819927215576)
[2025-02-13 02:29:41,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:41,870][root][INFO] - Training Epoch: 1/2, step 753/7134 completed (loss: 0.1287556141614914, acc: 0.9645270109176636)
[2025-02-13 02:29:42,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:42,288][root][INFO] - Training Epoch: 1/2, step 754/7134 completed (loss: 0.10050725191831589, acc: 0.9713423848152161)
[2025-02-13 02:29:42,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:42,728][root][INFO] - Training Epoch: 1/2, step 755/7134 completed (loss: 0.1789141744375229, acc: 0.9570956826210022)
[2025-02-13 02:29:42,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:43,107][root][INFO] - Training Epoch: 1/2, step 756/7134 completed (loss: 0.12628495693206787, acc: 0.9700934290885925)
[2025-02-13 02:29:43,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:43,515][root][INFO] - Training Epoch: 1/2, step 757/7134 completed (loss: 0.10716409236192703, acc: 0.975095808506012)
[2025-02-13 02:29:43,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:43,928][root][INFO] - Training Epoch: 1/2, step 758/7134 completed (loss: 0.15563957393169403, acc: 0.9586918950080872)
[2025-02-13 02:29:44,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:44,298][root][INFO] - Training Epoch: 1/2, step 759/7134 completed (loss: 0.13686950504779816, acc: 0.961240291595459)
[2025-02-13 02:29:44,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:44,705][root][INFO] - Training Epoch: 1/2, step 760/7134 completed (loss: 0.0759148895740509, acc: 0.9792027473449707)
[2025-02-13 02:29:44,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:45,110][root][INFO] - Training Epoch: 1/2, step 761/7134 completed (loss: 0.09170505404472351, acc: 0.9749608635902405)
[2025-02-13 02:29:45,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:45,516][root][INFO] - Training Epoch: 1/2, step 762/7134 completed (loss: 0.0827174112200737, acc: 0.9820788502693176)
[2025-02-13 02:29:45,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:45,918][root][INFO] - Training Epoch: 1/2, step 763/7134 completed (loss: 0.11781379580497742, acc: 0.9706840515136719)
[2025-02-13 02:29:46,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:46,339][root][INFO] - Training Epoch: 1/2, step 764/7134 completed (loss: 0.06288761645555496, acc: 0.9799138903617859)
[2025-02-13 02:29:46,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:46,802][root][INFO] - Training Epoch: 1/2, step 765/7134 completed (loss: 0.10351467877626419, acc: 0.9677891731262207)
[2025-02-13 02:29:46,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:47,240][root][INFO] - Training Epoch: 1/2, step 766/7134 completed (loss: 0.08072856068611145, acc: 0.9777448177337646)
[2025-02-13 02:29:47,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:47,666][root][INFO] - Training Epoch: 1/2, step 767/7134 completed (loss: 0.08726229518651962, acc: 0.9692533016204834)
[2025-02-13 02:29:47,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:48,095][root][INFO] - Training Epoch: 1/2, step 768/7134 completed (loss: 0.13185438513755798, acc: 0.9688385128974915)
[2025-02-13 02:29:48,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:48,522][root][INFO] - Training Epoch: 1/2, step 769/7134 completed (loss: 0.0787578597664833, acc: 0.984240710735321)
[2025-02-13 02:29:48,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:48,923][root][INFO] - Training Epoch: 1/2, step 770/7134 completed (loss: 0.11348044127225876, acc: 0.972434937953949)
[2025-02-13 02:29:49,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:49,274][root][INFO] - Training Epoch: 1/2, step 771/7134 completed (loss: 0.10397060960531235, acc: 0.9762375950813293)
[2025-02-13 02:29:49,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:49,675][root][INFO] - Training Epoch: 1/2, step 772/7134 completed (loss: 0.0762827917933464, acc: 0.9765493869781494)
[2025-02-13 02:29:49,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:50,129][root][INFO] - Training Epoch: 1/2, step 773/7134 completed (loss: 0.11674454063177109, acc: 0.9678663015365601)
[2025-02-13 02:29:50,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:50,541][root][INFO] - Training Epoch: 1/2, step 774/7134 completed (loss: 0.06304585188627243, acc: 0.985049843788147)
[2025-02-13 02:29:50,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:50,997][root][INFO] - Training Epoch: 1/2, step 775/7134 completed (loss: 0.08389075845479965, acc: 0.9800266027450562)
[2025-02-13 02:29:51,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:51,431][root][INFO] - Training Epoch: 1/2, step 776/7134 completed (loss: 0.07194717973470688, acc: 0.9778106212615967)
[2025-02-13 02:29:51,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:51,846][root][INFO] - Training Epoch: 1/2, step 777/7134 completed (loss: 0.07156296819448471, acc: 0.9855538010597229)
[2025-02-13 02:29:51,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:52,272][root][INFO] - Training Epoch: 1/2, step 778/7134 completed (loss: 0.11310698091983795, acc: 0.9711538553237915)
[2025-02-13 02:29:52,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:52,695][root][INFO] - Training Epoch: 1/2, step 779/7134 completed (loss: 0.17514602839946747, acc: 0.9532846808433533)
[2025-02-13 02:29:52,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:53,131][root][INFO] - Training Epoch: 1/2, step 780/7134 completed (loss: 0.20306526124477386, acc: 0.9505703449249268)
[2025-02-13 02:29:53,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:53,612][root][INFO] - Training Epoch: 1/2, step 781/7134 completed (loss: 0.1909898966550827, acc: 0.9389499425888062)
[2025-02-13 02:29:53,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:54,009][root][INFO] - Training Epoch: 1/2, step 782/7134 completed (loss: 0.19065943360328674, acc: 0.9477847814559937)
[2025-02-13 02:29:54,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:54,470][root][INFO] - Training Epoch: 1/2, step 783/7134 completed (loss: 0.2583535611629486, acc: 0.9273885488510132)
[2025-02-13 02:29:54,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:54,904][root][INFO] - Training Epoch: 1/2, step 784/7134 completed (loss: 0.17723602056503296, acc: 0.9476248621940613)
[2025-02-13 02:29:55,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:55,346][root][INFO] - Training Epoch: 1/2, step 785/7134 completed (loss: 0.16549423336982727, acc: 0.953177273273468)
[2025-02-13 02:29:55,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:55,758][root][INFO] - Training Epoch: 1/2, step 786/7134 completed (loss: 0.1212063580751419, acc: 0.9702602028846741)
[2025-02-13 02:29:55,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:56,098][root][INFO] - Training Epoch: 1/2, step 787/7134 completed (loss: 0.11887949705123901, acc: 0.9689781069755554)
[2025-02-13 02:29:56,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:56,554][root][INFO] - Training Epoch: 1/2, step 788/7134 completed (loss: 0.18242427706718445, acc: 0.9515418410301208)
[2025-02-13 02:29:56,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:56,971][root][INFO] - Training Epoch: 1/2, step 789/7134 completed (loss: 0.11940784007310867, acc: 0.9679389595985413)
[2025-02-13 02:29:57,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:57,385][root][INFO] - Training Epoch: 1/2, step 790/7134 completed (loss: 0.19012588262557983, acc: 0.9452662467956543)
[2025-02-13 02:29:57,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:57,799][root][INFO] - Training Epoch: 1/2, step 791/7134 completed (loss: 0.11040020734071732, acc: 0.97826087474823)
[2025-02-13 02:29:57,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:58,214][root][INFO] - Training Epoch: 1/2, step 792/7134 completed (loss: 0.10808549076318741, acc: 0.9721792936325073)
[2025-02-13 02:29:58,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:58,639][root][INFO] - Training Epoch: 1/2, step 793/7134 completed (loss: 0.07846115529537201, acc: 0.9792899489402771)
[2025-02-13 02:29:58,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:59,054][root][INFO] - Training Epoch: 1/2, step 794/7134 completed (loss: 0.081670843064785, acc: 0.9813519716262817)
[2025-02-13 02:29:59,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:59,409][root][INFO] - Training Epoch: 1/2, step 795/7134 completed (loss: 0.16834400594234467, acc: 0.9595015645027161)
[2025-02-13 02:29:59,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:59,817][root][INFO] - Training Epoch: 1/2, step 796/7134 completed (loss: 0.18758787214756012, acc: 0.9520202279090881)
[2025-02-13 02:29:59,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:00,190][root][INFO] - Training Epoch: 1/2, step 797/7134 completed (loss: 0.13188254833221436, acc: 0.9738956093788147)
[2025-02-13 02:30:00,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:00,547][root][INFO] - Training Epoch: 1/2, step 798/7134 completed (loss: 0.0679364800453186, acc: 0.9867724776268005)
[2025-02-13 02:30:00,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:00,920][root][INFO] - Training Epoch: 1/2, step 799/7134 completed (loss: 0.10260210931301117, acc: 0.9741935729980469)
[2025-02-13 02:30:01,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:01,320][root][INFO] - Training Epoch: 1/2, step 800/7134 completed (loss: 0.15012586116790771, acc: 0.9590316414833069)
[2025-02-13 02:30:01,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:01,748][root][INFO] - Training Epoch: 1/2, step 801/7134 completed (loss: 0.17178967595100403, acc: 0.9567999839782715)
[2025-02-13 02:30:01,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:02,177][root][INFO] - Training Epoch: 1/2, step 802/7134 completed (loss: 0.09883374720811844, acc: 0.967793881893158)
[2025-02-13 02:30:02,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:02,554][root][INFO] - Training Epoch: 1/2, step 803/7134 completed (loss: 0.12544035911560059, acc: 0.9624999761581421)
[2025-02-13 02:30:02,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:02,967][root][INFO] - Training Epoch: 1/2, step 804/7134 completed (loss: 0.14904430508613586, acc: 0.951120138168335)
[2025-02-13 02:30:03,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:03,380][root][INFO] - Training Epoch: 1/2, step 805/7134 completed (loss: 0.1386815905570984, acc: 0.9531835317611694)
[2025-02-13 02:30:03,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:03,826][root][INFO] - Training Epoch: 1/2, step 806/7134 completed (loss: 0.11659277975559235, acc: 0.9664804339408875)
[2025-02-13 02:30:03,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:04,299][root][INFO] - Training Epoch: 1/2, step 807/7134 completed (loss: 0.15405818819999695, acc: 0.9622871279716492)
[2025-02-13 02:30:04,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:04,742][root][INFO] - Training Epoch: 1/2, step 808/7134 completed (loss: 0.09449470043182373, acc: 0.9712575078010559)
[2025-02-13 02:30:04,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:05,172][root][INFO] - Training Epoch: 1/2, step 809/7134 completed (loss: 0.09728958457708359, acc: 0.9717682003974915)
[2025-02-13 02:30:05,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:05,591][root][INFO] - Training Epoch: 1/2, step 810/7134 completed (loss: 0.11803476512432098, acc: 0.973009467124939)
[2025-02-13 02:30:05,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:05,983][root][INFO] - Training Epoch: 1/2, step 811/7134 completed (loss: 0.1559457778930664, acc: 0.9666081070899963)
[2025-02-13 02:30:06,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:06,424][root][INFO] - Training Epoch: 1/2, step 812/7134 completed (loss: 0.13056546449661255, acc: 0.9672130942344666)
[2025-02-13 02:30:06,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:06,854][root][INFO] - Training Epoch: 1/2, step 813/7134 completed (loss: 0.1383989155292511, acc: 0.9666182994842529)
[2025-02-13 02:30:07,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:07,315][root][INFO] - Training Epoch: 1/2, step 814/7134 completed (loss: 0.12804727256298065, acc: 0.9651612639427185)
[2025-02-13 02:30:07,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:07,763][root][INFO] - Training Epoch: 1/2, step 815/7134 completed (loss: 0.1302523910999298, acc: 0.9659969210624695)
[2025-02-13 02:30:07,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:08,208][root][INFO] - Training Epoch: 1/2, step 816/7134 completed (loss: 0.14227284491062164, acc: 0.9610169529914856)
[2025-02-13 02:30:08,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:08,638][root][INFO] - Training Epoch: 1/2, step 817/7134 completed (loss: 0.1332591027021408, acc: 0.9634369015693665)
[2025-02-13 02:30:08,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:09,061][root][INFO] - Training Epoch: 1/2, step 818/7134 completed (loss: 0.19679555296897888, acc: 0.9432989954948425)
[2025-02-13 02:30:09,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:09,467][root][INFO] - Training Epoch: 1/2, step 819/7134 completed (loss: 0.11356227844953537, acc: 0.9760000109672546)
[2025-02-13 02:30:09,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:09,924][root][INFO] - Training Epoch: 1/2, step 820/7134 completed (loss: 0.10292010754346848, acc: 0.9675425291061401)
[2025-02-13 02:30:10,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:10,414][root][INFO] - Training Epoch: 1/2, step 821/7134 completed (loss: 0.08249888569116592, acc: 0.9772117733955383)
[2025-02-13 02:30:10,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:10,818][root][INFO] - Training Epoch: 1/2, step 822/7134 completed (loss: 0.0969175472855568, acc: 0.9701727032661438)
[2025-02-13 02:30:10,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:11,233][root][INFO] - Training Epoch: 1/2, step 823/7134 completed (loss: 0.08367960900068283, acc: 0.9793621301651001)
[2025-02-13 02:30:11,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:11,647][root][INFO] - Training Epoch: 1/2, step 824/7134 completed (loss: 0.1234937459230423, acc: 0.962897539138794)
[2025-02-13 02:30:11,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:12,115][root][INFO] - Training Epoch: 1/2, step 825/7134 completed (loss: 0.11745449155569077, acc: 0.9670619368553162)
[2025-02-13 02:30:12,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:12,586][root][INFO] - Training Epoch: 1/2, step 826/7134 completed (loss: 0.10995505005121231, acc: 0.968664824962616)
[2025-02-13 02:30:12,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:13,019][root][INFO] - Training Epoch: 1/2, step 827/7134 completed (loss: 0.0971132218837738, acc: 0.9711285829544067)
[2025-02-13 02:30:13,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:13,427][root][INFO] - Training Epoch: 1/2, step 828/7134 completed (loss: 0.09087312966585159, acc: 0.9735614061355591)
[2025-02-13 02:30:13,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:13,845][root][INFO] - Training Epoch: 1/2, step 829/7134 completed (loss: 0.1235969215631485, acc: 0.9611650705337524)
[2025-02-13 02:30:13,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:14,307][root][INFO] - Training Epoch: 1/2, step 830/7134 completed (loss: 0.10367273539304733, acc: 0.9717608094215393)
[2025-02-13 02:30:14,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:14,743][root][INFO] - Training Epoch: 1/2, step 831/7134 completed (loss: 0.07471539825201035, acc: 0.9739726185798645)
[2025-02-13 02:30:14,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:15,163][root][INFO] - Training Epoch: 1/2, step 832/7134 completed (loss: 0.14531220495700836, acc: 0.9692832827568054)
[2025-02-13 02:30:15,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:15,620][root][INFO] - Training Epoch: 1/2, step 833/7134 completed (loss: 0.11418469250202179, acc: 0.970588207244873)
[2025-02-13 02:30:15,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:16,058][root][INFO] - Training Epoch: 1/2, step 834/7134 completed (loss: 0.05666124448180199, acc: 0.988811194896698)
[2025-02-13 02:30:16,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:16,498][root][INFO] - Training Epoch: 1/2, step 835/7134 completed (loss: 0.11056654900312424, acc: 0.9706293940544128)
[2025-02-13 02:30:16,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:16,972][root][INFO] - Training Epoch: 1/2, step 836/7134 completed (loss: 0.11505588889122009, acc: 0.9675173759460449)
[2025-02-13 02:30:17,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:17,412][root][INFO] - Training Epoch: 1/2, step 837/7134 completed (loss: 0.09151285886764526, acc: 0.9768392443656921)
[2025-02-13 02:30:17,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:17,843][root][INFO] - Training Epoch: 1/2, step 838/7134 completed (loss: 0.09462286531925201, acc: 0.9745628237724304)
[2025-02-13 02:30:17,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:18,297][root][INFO] - Training Epoch: 1/2, step 839/7134 completed (loss: 0.10218462347984314, acc: 0.9688995480537415)
[2025-02-13 02:30:18,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:18,716][root][INFO] - Training Epoch: 1/2, step 840/7134 completed (loss: 0.0791681557893753, acc: 0.9776847958564758)
[2025-02-13 02:30:18,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:19,147][root][INFO] - Training Epoch: 1/2, step 841/7134 completed (loss: 0.18451161682605743, acc: 0.9612724781036377)
[2025-02-13 02:30:19,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:19,597][root][INFO] - Training Epoch: 1/2, step 842/7134 completed (loss: 0.1194210946559906, acc: 0.9690027236938477)
[2025-02-13 02:30:19,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:20,048][root][INFO] - Training Epoch: 1/2, step 843/7134 completed (loss: 0.09048774838447571, acc: 0.9735743999481201)
[2025-02-13 02:30:20,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:20,462][root][INFO] - Training Epoch: 1/2, step 844/7134 completed (loss: 0.08193913102149963, acc: 0.9747235178947449)
[2025-02-13 02:30:20,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:20,877][root][INFO] - Training Epoch: 1/2, step 845/7134 completed (loss: 0.14527764916419983, acc: 0.9646017551422119)
[2025-02-13 02:30:21,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:21,330][root][INFO] - Training Epoch: 1/2, step 846/7134 completed (loss: 0.1334753930568695, acc: 0.9688427448272705)
[2025-02-13 02:30:21,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:21,766][root][INFO] - Training Epoch: 1/2, step 847/7134 completed (loss: 0.11040452867746353, acc: 0.9590268731117249)
[2025-02-13 02:30:21,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:22,193][root][INFO] - Training Epoch: 1/2, step 848/7134 completed (loss: 0.15482231974601746, acc: 0.955500602722168)
[2025-02-13 02:30:22,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:22,655][root][INFO] - Training Epoch: 1/2, step 849/7134 completed (loss: 0.10827169567346573, acc: 0.9673518538475037)
[2025-02-13 02:30:22,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:23,121][root][INFO] - Training Epoch: 1/2, step 850/7134 completed (loss: 0.10078917443752289, acc: 0.9664429426193237)
[2025-02-13 02:30:23,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:23,581][root][INFO] - Training Epoch: 1/2, step 851/7134 completed (loss: 0.080259770154953, acc: 0.980289101600647)
[2025-02-13 02:30:23,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:24,038][root][INFO] - Training Epoch: 1/2, step 852/7134 completed (loss: 0.09902027249336243, acc: 0.9694749712944031)
[2025-02-13 02:30:24,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:24,513][root][INFO] - Training Epoch: 1/2, step 853/7134 completed (loss: 0.08997122198343277, acc: 0.965859055519104)
[2025-02-13 02:30:24,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:24,944][root][INFO] - Training Epoch: 1/2, step 854/7134 completed (loss: 0.14775146543979645, acc: 0.9577114582061768)
[2025-02-13 02:30:25,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:25,417][root][INFO] - Training Epoch: 1/2, step 855/7134 completed (loss: 0.155244380235672, acc: 0.9613583087921143)
[2025-02-13 02:30:25,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:25,862][root][INFO] - Training Epoch: 1/2, step 856/7134 completed (loss: 0.11619456857442856, acc: 0.9685230255126953)
[2025-02-13 02:30:25,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:26,286][root][INFO] - Training Epoch: 1/2, step 857/7134 completed (loss: 0.1355660855770111, acc: 0.9647576808929443)
[2025-02-13 02:30:26,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:26,732][root][INFO] - Training Epoch: 1/2, step 858/7134 completed (loss: 0.1186976283788681, acc: 0.9676616787910461)
[2025-02-13 02:30:26,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:27,192][root][INFO] - Training Epoch: 1/2, step 859/7134 completed (loss: 0.19684374332427979, acc: 0.9383886456489563)
[2025-02-13 02:30:27,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:27,646][root][INFO] - Training Epoch: 1/2, step 860/7134 completed (loss: 0.15689484775066376, acc: 0.9494311213493347)
[2025-02-13 02:30:27,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:28,127][root][INFO] - Training Epoch: 1/2, step 861/7134 completed (loss: 0.15904037654399872, acc: 0.9577465057373047)
[2025-02-13 02:30:28,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:28,558][root][INFO] - Training Epoch: 1/2, step 862/7134 completed (loss: 0.17406794428825378, acc: 0.950661838054657)
[2025-02-13 02:30:28,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:29,021][root][INFO] - Training Epoch: 1/2, step 863/7134 completed (loss: 0.15866641700267792, acc: 0.9534342288970947)
[2025-02-13 02:30:29,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:29,465][root][INFO] - Training Epoch: 1/2, step 864/7134 completed (loss: 0.15958364307880402, acc: 0.9539822936058044)
[2025-02-13 02:30:29,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:29,918][root][INFO] - Training Epoch: 1/2, step 865/7134 completed (loss: 0.18379178643226624, acc: 0.956204354763031)
[2025-02-13 02:30:30,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:30,370][root][INFO] - Training Epoch: 1/2, step 866/7134 completed (loss: 0.20201852917671204, acc: 0.9407407641410828)
[2025-02-13 02:30:30,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:30,800][root][INFO] - Training Epoch: 1/2, step 867/7134 completed (loss: 0.21939460933208466, acc: 0.9474343061447144)
[2025-02-13 02:30:30,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:31,258][root][INFO] - Training Epoch: 1/2, step 868/7134 completed (loss: 0.18008233606815338, acc: 0.9502617716789246)
[2025-02-13 02:30:31,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:31,593][root][INFO] - Training Epoch: 1/2, step 869/7134 completed (loss: 0.2807022035121918, acc: 0.9299362897872925)
[2025-02-13 02:30:31,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:32,046][root][INFO] - Training Epoch: 1/2, step 870/7134 completed (loss: 0.1253785938024521, acc: 0.9676550030708313)
[2025-02-13 02:30:32,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:32,460][root][INFO] - Training Epoch: 1/2, step 871/7134 completed (loss: 0.17133736610412598, acc: 0.9592834115028381)
[2025-02-13 02:30:32,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:32,883][root][INFO] - Training Epoch: 1/2, step 872/7134 completed (loss: 0.09903990477323532, acc: 0.9733123779296875)
[2025-02-13 02:30:33,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:33,324][root][INFO] - Training Epoch: 1/2, step 873/7134 completed (loss: 0.16617292165756226, acc: 0.9535211324691772)
[2025-02-13 02:30:33,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:33,775][root][INFO] - Training Epoch: 1/2, step 874/7134 completed (loss: 0.1747477799654007, acc: 0.9522342085838318)
[2025-02-13 02:30:33,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:34,200][root][INFO] - Training Epoch: 1/2, step 875/7134 completed (loss: 0.18007564544677734, acc: 0.9502196311950684)
[2025-02-13 02:30:34,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:34,641][root][INFO] - Training Epoch: 1/2, step 876/7134 completed (loss: 0.14285415410995483, acc: 0.9590643048286438)
[2025-02-13 02:30:34,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:35,113][root][INFO] - Training Epoch: 1/2, step 877/7134 completed (loss: 0.2161562442779541, acc: 0.9443708658218384)
[2025-02-13 02:30:35,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:35,541][root][INFO] - Training Epoch: 1/2, step 878/7134 completed (loss: 0.17552591860294342, acc: 0.9522546529769897)
[2025-02-13 02:30:35,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:35,963][root][INFO] - Training Epoch: 1/2, step 879/7134 completed (loss: 0.14024990797042847, acc: 0.9636098742485046)
[2025-02-13 02:30:36,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:36,387][root][INFO] - Training Epoch: 1/2, step 880/7134 completed (loss: 0.06283474713563919, acc: 0.9883551597595215)
[2025-02-13 02:30:36,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:36,782][root][INFO] - Training Epoch: 1/2, step 881/7134 completed (loss: 0.07120407372713089, acc: 0.9799599051475525)
[2025-02-13 02:30:36,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:37,205][root][INFO] - Training Epoch: 1/2, step 882/7134 completed (loss: 0.12257207930088043, acc: 0.9694244861602783)
[2025-02-13 02:30:37,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:37,618][root][INFO] - Training Epoch: 1/2, step 883/7134 completed (loss: 0.2111474573612213, acc: 0.9403669834136963)
[2025-02-13 02:30:37,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:38,033][root][INFO] - Training Epoch: 1/2, step 884/7134 completed (loss: 0.12913647294044495, acc: 0.9557640552520752)
[2025-02-13 02:30:38,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:38,460][root][INFO] - Training Epoch: 1/2, step 885/7134 completed (loss: 0.10602988302707672, acc: 0.966292142868042)
[2025-02-13 02:30:38,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:38,904][root][INFO] - Training Epoch: 1/2, step 886/7134 completed (loss: 0.16003452241420746, acc: 0.9415204524993896)
[2025-02-13 02:30:39,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:39,281][root][INFO] - Training Epoch: 1/2, step 887/7134 completed (loss: 0.11591088026762009, acc: 0.9628318548202515)
[2025-02-13 02:30:39,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:39,704][root][INFO] - Training Epoch: 1/2, step 888/7134 completed (loss: 0.16108644008636475, acc: 0.9575113654136658)
[2025-02-13 02:30:39,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:40,109][root][INFO] - Training Epoch: 1/2, step 889/7134 completed (loss: 0.0794018805027008, acc: 0.9733333587646484)
[2025-02-13 02:30:40,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:40,523][root][INFO] - Training Epoch: 1/2, step 890/7134 completed (loss: 0.10710105299949646, acc: 0.971137523651123)
[2025-02-13 02:30:40,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:40,969][root][INFO] - Training Epoch: 1/2, step 891/7134 completed (loss: 0.11698678135871887, acc: 0.9805285334587097)
[2025-02-13 02:30:41,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:41,394][root][INFO] - Training Epoch: 1/2, step 892/7134 completed (loss: 0.11040998995304108, acc: 0.9738371968269348)
[2025-02-13 02:30:41,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:41,788][root][INFO] - Training Epoch: 1/2, step 893/7134 completed (loss: 0.08657483011484146, acc: 0.9767801761627197)
[2025-02-13 02:30:41,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:42,221][root][INFO] - Training Epoch: 1/2, step 894/7134 completed (loss: 0.15028469264507294, acc: 0.9622905254364014)
[2025-02-13 02:30:42,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:42,626][root][INFO] - Training Epoch: 1/2, step 895/7134 completed (loss: 0.08317650854587555, acc: 0.9811965823173523)
[2025-02-13 02:30:42,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:43,019][root][INFO] - Training Epoch: 1/2, step 896/7134 completed (loss: 0.12156613171100616, acc: 0.9660000205039978)
[2025-02-13 02:30:43,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:43,500][root][INFO] - Training Epoch: 1/2, step 897/7134 completed (loss: 0.14781831204891205, acc: 0.9596100449562073)
[2025-02-13 02:30:43,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:43,980][root][INFO] - Training Epoch: 1/2, step 898/7134 completed (loss: 0.08831627666950226, acc: 0.9701120853424072)
[2025-02-13 02:30:44,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:44,419][root][INFO] - Training Epoch: 1/2, step 899/7134 completed (loss: 0.11081955581903458, acc: 0.9738805890083313)
[2025-02-13 02:30:44,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:44,836][root][INFO] - Training Epoch: 1/2, step 900/7134 completed (loss: 0.0955699235200882, acc: 0.9690576791763306)
[2025-02-13 02:30:45,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:45,312][root][INFO] - Training Epoch: 1/2, step 901/7134 completed (loss: 0.11496283113956451, acc: 0.9679999947547913)
[2025-02-13 02:30:45,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:45,720][root][INFO] - Training Epoch: 1/2, step 902/7134 completed (loss: 0.05475340783596039, acc: 0.9813486337661743)
[2025-02-13 02:30:45,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:46,168][root][INFO] - Training Epoch: 1/2, step 903/7134 completed (loss: 0.05760524794459343, acc: 0.9775429368019104)
[2025-02-13 02:30:46,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:46,635][root][INFO] - Training Epoch: 1/2, step 904/7134 completed (loss: 0.0868559256196022, acc: 0.9778645634651184)
[2025-02-13 02:30:46,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:47,095][root][INFO] - Training Epoch: 1/2, step 905/7134 completed (loss: 0.07657809555530548, acc: 0.9770889282226562)
[2025-02-13 02:30:47,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:47,534][root][INFO] - Training Epoch: 1/2, step 906/7134 completed (loss: 0.09209534525871277, acc: 0.9772117733955383)
[2025-02-13 02:30:47,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:47,979][root][INFO] - Training Epoch: 1/2, step 907/7134 completed (loss: 0.0769738182425499, acc: 0.9789473414421082)
[2025-02-13 02:30:48,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:48,414][root][INFO] - Training Epoch: 1/2, step 908/7134 completed (loss: 0.07796357572078705, acc: 0.9739921689033508)
[2025-02-13 02:30:48,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:48,831][root][INFO] - Training Epoch: 1/2, step 909/7134 completed (loss: 0.09243188053369522, acc: 0.9714714884757996)
[2025-02-13 02:30:48,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:49,296][root][INFO] - Training Epoch: 1/2, step 910/7134 completed (loss: 0.11030620336532593, acc: 0.9639519453048706)
[2025-02-13 02:30:49,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:49,740][root][INFO] - Training Epoch: 1/2, step 911/7134 completed (loss: 0.11007259041070938, acc: 0.9692307710647583)
[2025-02-13 02:30:49,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:50,198][root][INFO] - Training Epoch: 1/2, step 912/7134 completed (loss: 0.1127709150314331, acc: 0.9659520983695984)
[2025-02-13 02:30:50,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:50,654][root][INFO] - Training Epoch: 1/2, step 913/7134 completed (loss: 0.06730371713638306, acc: 0.983668327331543)
[2025-02-13 02:30:50,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:51,083][root][INFO] - Training Epoch: 1/2, step 914/7134 completed (loss: 0.0764317512512207, acc: 0.980867326259613)
[2025-02-13 02:30:51,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:51,496][root][INFO] - Training Epoch: 1/2, step 915/7134 completed (loss: 0.16573460400104523, acc: 0.9579287767410278)
[2025-02-13 02:30:51,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:51,908][root][INFO] - Training Epoch: 1/2, step 916/7134 completed (loss: 0.11168576776981354, acc: 0.9650959968566895)
[2025-02-13 02:30:52,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:52,315][root][INFO] - Training Epoch: 1/2, step 917/7134 completed (loss: 0.11938349902629852, acc: 0.9698340892791748)
[2025-02-13 02:30:52,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:52,742][root][INFO] - Training Epoch: 1/2, step 918/7134 completed (loss: 0.05449919030070305, acc: 0.9770290851593018)
[2025-02-13 02:30:52,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:53,136][root][INFO] - Training Epoch: 1/2, step 919/7134 completed (loss: 0.05847703665494919, acc: 0.9797979593276978)
[2025-02-13 02:30:53,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:53,549][root][INFO] - Training Epoch: 1/2, step 920/7134 completed (loss: 0.10171390324831009, acc: 0.9677870869636536)
[2025-02-13 02:30:53,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:53,973][root][INFO] - Training Epoch: 1/2, step 921/7134 completed (loss: 0.08630027621984482, acc: 0.9742547273635864)
[2025-02-13 02:30:54,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:54,377][root][INFO] - Training Epoch: 1/2, step 922/7134 completed (loss: 0.13267794251441956, acc: 0.9672130942344666)
[2025-02-13 02:30:54,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:54,805][root][INFO] - Training Epoch: 1/2, step 923/7134 completed (loss: 0.10050061345100403, acc: 0.9656084775924683)
[2025-02-13 02:30:54,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:55,314][root][INFO] - Training Epoch: 1/2, step 924/7134 completed (loss: 0.07110611349344254, acc: 0.979468584060669)
[2025-02-13 02:30:55,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:55,790][root][INFO] - Training Epoch: 1/2, step 925/7134 completed (loss: 0.11587096750736237, acc: 0.9720998406410217)
[2025-02-13 02:30:55,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:56,256][root][INFO] - Training Epoch: 1/2, step 926/7134 completed (loss: 0.10484693199396133, acc: 0.9812734127044678)
[2025-02-13 02:30:56,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:56,709][root][INFO] - Training Epoch: 1/2, step 927/7134 completed (loss: 0.1053459420800209, acc: 0.9771241545677185)
[2025-02-13 02:30:56,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:57,164][root][INFO] - Training Epoch: 1/2, step 928/7134 completed (loss: 0.12181907147169113, acc: 0.9723865985870361)
[2025-02-13 02:30:57,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:57,570][root][INFO] - Training Epoch: 1/2, step 929/7134 completed (loss: 0.052820004522800446, acc: 0.9905213117599487)
[2025-02-13 02:30:57,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:57,901][root][INFO] - Training Epoch: 1/2, step 930/7134 completed (loss: 0.11937203258275986, acc: 0.9657257795333862)
[2025-02-13 02:30:58,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:58,323][root][INFO] - Training Epoch: 1/2, step 931/7134 completed (loss: 0.10914116352796555, acc: 0.971781313419342)
[2025-02-13 02:30:58,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:58,724][root][INFO] - Training Epoch: 1/2, step 932/7134 completed (loss: 0.08264728635549545, acc: 0.972779393196106)
[2025-02-13 02:30:58,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:59,142][root][INFO] - Training Epoch: 1/2, step 933/7134 completed (loss: 0.12302377074956894, acc: 0.9619181752204895)
[2025-02-13 02:30:59,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:59,573][root][INFO] - Training Epoch: 1/2, step 934/7134 completed (loss: 0.07023852318525314, acc: 0.9803664684295654)
[2025-02-13 02:30:59,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:59,990][root][INFO] - Training Epoch: 1/2, step 935/7134 completed (loss: 0.07096944004297256, acc: 0.9766277074813843)
[2025-02-13 02:31:00,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:00,416][root][INFO] - Training Epoch: 1/2, step 936/7134 completed (loss: 0.12191425263881683, acc: 0.9701230525970459)
[2025-02-13 02:31:00,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:00,846][root][INFO] - Training Epoch: 1/2, step 937/7134 completed (loss: 0.0973881259560585, acc: 0.9683098793029785)
[2025-02-13 02:31:00,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:01,287][root][INFO] - Training Epoch: 1/2, step 938/7134 completed (loss: 0.09103730320930481, acc: 0.9756447076797485)
[2025-02-13 02:31:01,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:01,689][root][INFO] - Training Epoch: 1/2, step 939/7134 completed (loss: 0.1079067811369896, acc: 0.9704049825668335)
[2025-02-13 02:31:01,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:02,149][root][INFO] - Training Epoch: 1/2, step 940/7134 completed (loss: 0.11367971450090408, acc: 0.9656652212142944)
[2025-02-13 02:31:02,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:02,629][root][INFO] - Training Epoch: 1/2, step 941/7134 completed (loss: 0.09359337389469147, acc: 0.9724137783050537)
[2025-02-13 02:31:02,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:03,062][root][INFO] - Training Epoch: 1/2, step 942/7134 completed (loss: 0.09822767972946167, acc: 0.9782293438911438)
[2025-02-13 02:31:03,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:03,490][root][INFO] - Training Epoch: 1/2, step 943/7134 completed (loss: 0.0700119212269783, acc: 0.9802131056785583)
[2025-02-13 02:31:03,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:03,886][root][INFO] - Training Epoch: 1/2, step 944/7134 completed (loss: 0.08818406611680984, acc: 0.9760119915008545)
[2025-02-13 02:31:04,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:04,294][root][INFO] - Training Epoch: 1/2, step 945/7134 completed (loss: 0.07774241268634796, acc: 0.988811194896698)
[2025-02-13 02:31:04,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:04,653][root][INFO] - Training Epoch: 1/2, step 946/7134 completed (loss: 0.05193105712532997, acc: 0.9871520400047302)
[2025-02-13 02:31:04,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:05,061][root][INFO] - Training Epoch: 1/2, step 947/7134 completed (loss: 0.08022872358560562, acc: 0.9810671210289001)
[2025-02-13 02:31:05,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:05,504][root][INFO] - Training Epoch: 1/2, step 948/7134 completed (loss: 0.0994715467095375, acc: 0.9735743999481201)
[2025-02-13 02:31:05,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:05,994][root][INFO] - Training Epoch: 1/2, step 949/7134 completed (loss: 0.07777318358421326, acc: 0.9817671775817871)
[2025-02-13 02:31:06,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:06,416][root][INFO] - Training Epoch: 1/2, step 950/7134 completed (loss: 0.08631376177072525, acc: 0.979938268661499)
[2025-02-13 02:31:06,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:06,825][root][INFO] - Training Epoch: 1/2, step 951/7134 completed (loss: 0.08714901655912399, acc: 0.9788732528686523)
[2025-02-13 02:31:06,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:07,235][root][INFO] - Training Epoch: 1/2, step 952/7134 completed (loss: 0.12603379786014557, acc: 0.960066556930542)
[2025-02-13 02:31:07,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:07,701][root][INFO] - Training Epoch: 1/2, step 953/7134 completed (loss: 0.17672249674797058, acc: 0.9509202241897583)
[2025-02-13 02:31:07,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:08,151][root][INFO] - Training Epoch: 1/2, step 954/7134 completed (loss: 0.1078190952539444, acc: 0.9676945805549622)
[2025-02-13 02:31:08,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:08,612][root][INFO] - Training Epoch: 1/2, step 955/7134 completed (loss: 0.09324607253074646, acc: 0.9772422909736633)
[2025-02-13 02:31:08,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:09,051][root][INFO] - Training Epoch: 1/2, step 956/7134 completed (loss: 0.09764984995126724, acc: 0.9670731425285339)
[2025-02-13 02:31:09,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:09,499][root][INFO] - Training Epoch: 1/2, step 957/7134 completed (loss: 0.07688336819410324, acc: 0.977246880531311)
[2025-02-13 02:31:09,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:09,904][root][INFO] - Training Epoch: 1/2, step 958/7134 completed (loss: 0.188791885972023, acc: 0.9503676295280457)
[2025-02-13 02:31:10,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:10,328][root][INFO] - Training Epoch: 1/2, step 959/7134 completed (loss: 0.21921297907829285, acc: 0.9465240836143494)
[2025-02-13 02:31:10,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:10,750][root][INFO] - Training Epoch: 1/2, step 960/7134 completed (loss: 0.2053368240594864, acc: 0.9470699429512024)
[2025-02-13 02:31:10,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:11,177][root][INFO] - Training Epoch: 1/2, step 961/7134 completed (loss: 0.1403406262397766, acc: 0.9636118412017822)
[2025-02-13 02:31:11,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:11,617][root][INFO] - Training Epoch: 1/2, step 962/7134 completed (loss: 0.20842352509498596, acc: 0.9558620452880859)
[2025-02-13 02:31:11,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:12,037][root][INFO] - Training Epoch: 1/2, step 963/7134 completed (loss: 0.15088219940662384, acc: 0.9634146094322205)
[2025-02-13 02:31:12,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:12,456][root][INFO] - Training Epoch: 1/2, step 964/7134 completed (loss: 0.24936170876026154, acc: 0.9460269808769226)
[2025-02-13 02:31:12,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:12,878][root][INFO] - Training Epoch: 1/2, step 965/7134 completed (loss: 0.20287497341632843, acc: 0.9569584131240845)
[2025-02-13 02:31:13,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:13,281][root][INFO] - Training Epoch: 1/2, step 966/7134 completed (loss: 0.11155959218740463, acc: 0.965573787689209)
[2025-02-13 02:31:13,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:13,735][root][INFO] - Training Epoch: 1/2, step 967/7134 completed (loss: 0.14823195338249207, acc: 0.9590722918510437)
[2025-02-13 02:31:13,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:14,143][root][INFO] - Training Epoch: 1/2, step 968/7134 completed (loss: 0.23085615038871765, acc: 0.9443339705467224)
[2025-02-13 02:31:14,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:14,539][root][INFO] - Training Epoch: 1/2, step 969/7134 completed (loss: 0.14381703734397888, acc: 0.965309202671051)
[2025-02-13 02:31:14,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:14,960][root][INFO] - Training Epoch: 1/2, step 970/7134 completed (loss: 0.2045671045780182, acc: 0.9389312863349915)
[2025-02-13 02:31:15,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:15,419][root][INFO] - Training Epoch: 1/2, step 971/7134 completed (loss: 0.20260007679462433, acc: 0.9496932625770569)
[2025-02-13 02:31:15,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:15,890][root][INFO] - Training Epoch: 1/2, step 972/7134 completed (loss: 0.1522868573665619, acc: 0.9551681280136108)
[2025-02-13 02:31:16,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:16,326][root][INFO] - Training Epoch: 1/2, step 973/7134 completed (loss: 0.16937550902366638, acc: 0.9589743614196777)
[2025-02-13 02:31:16,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:16,797][root][INFO] - Training Epoch: 1/2, step 974/7134 completed (loss: 0.15530365705490112, acc: 0.9646522402763367)
[2025-02-13 02:31:16,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:17,232][root][INFO] - Training Epoch: 1/2, step 975/7134 completed (loss: 0.15766793489456177, acc: 0.9509569406509399)
[2025-02-13 02:31:17,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:17,674][root][INFO] - Training Epoch: 1/2, step 976/7134 completed (loss: 0.14109569787979126, acc: 0.9579439163208008)
[2025-02-13 02:31:17,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:18,118][root][INFO] - Training Epoch: 1/2, step 977/7134 completed (loss: 0.16223108768463135, acc: 0.9545454382896423)
[2025-02-13 02:31:18,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:18,561][root][INFO] - Training Epoch: 1/2, step 978/7134 completed (loss: 0.08073865622282028, acc: 0.9783393740653992)
[2025-02-13 02:31:18,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:19,005][root][INFO] - Training Epoch: 1/2, step 979/7134 completed (loss: 0.1391316056251526, acc: 0.970588207244873)
[2025-02-13 02:31:19,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:19,436][root][INFO] - Training Epoch: 1/2, step 980/7134 completed (loss: 0.12210536748170853, acc: 0.9649999737739563)
[2025-02-13 02:31:19,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:19,830][root][INFO] - Training Epoch: 1/2, step 981/7134 completed (loss: 0.21439862251281738, acc: 0.9485530257225037)
[2025-02-13 02:31:19,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:20,274][root][INFO] - Training Epoch: 1/2, step 982/7134 completed (loss: 0.10377736389636993, acc: 0.9775280952453613)
[2025-02-13 02:31:20,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:20,696][root][INFO] - Training Epoch: 1/2, step 983/7134 completed (loss: 0.12393069267272949, acc: 0.9708904027938843)
[2025-02-13 02:31:20,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:21,115][root][INFO] - Training Epoch: 1/2, step 984/7134 completed (loss: 0.09658229351043701, acc: 0.9724264740943909)
[2025-02-13 02:31:21,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:21,565][root][INFO] - Training Epoch: 1/2, step 985/7134 completed (loss: 0.10351316630840302, acc: 0.9681122303009033)
[2025-02-13 02:31:21,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:21,996][root][INFO] - Training Epoch: 1/2, step 986/7134 completed (loss: 0.11295320838689804, acc: 0.9702549576759338)
[2025-02-13 02:31:22,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:22,390][root][INFO] - Training Epoch: 1/2, step 987/7134 completed (loss: 0.07212354242801666, acc: 0.9774919748306274)
[2025-02-13 02:31:22,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:22,806][root][INFO] - Training Epoch: 1/2, step 988/7134 completed (loss: 0.17886760830879211, acc: 0.9597780704498291)
[2025-02-13 02:31:22,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:23,237][root][INFO] - Training Epoch: 1/2, step 989/7134 completed (loss: 0.14008116722106934, acc: 0.9610570073127747)
[2025-02-13 02:31:23,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:23,671][root][INFO] - Training Epoch: 1/2, step 990/7134 completed (loss: 0.18903957307338715, acc: 0.9494311213493347)
[2025-02-13 02:31:23,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:24,096][root][INFO] - Training Epoch: 1/2, step 991/7134 completed (loss: 0.16384842991828918, acc: 0.956181526184082)
[2025-02-13 02:31:24,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:24,551][root][INFO] - Training Epoch: 1/2, step 992/7134 completed (loss: 0.14433518052101135, acc: 0.9513157606124878)
[2025-02-13 02:31:24,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:24,975][root][INFO] - Training Epoch: 1/2, step 993/7134 completed (loss: 0.11363767087459564, acc: 0.972176730632782)
[2025-02-13 02:31:25,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:25,461][root][INFO] - Training Epoch: 1/2, step 994/7134 completed (loss: 0.11834532022476196, acc: 0.9673423171043396)
[2025-02-13 02:31:25,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:25,899][root][INFO] - Training Epoch: 1/2, step 995/7134 completed (loss: 0.1619621366262436, acc: 0.9629629850387573)
[2025-02-13 02:31:26,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:26,315][root][INFO] - Training Epoch: 1/2, step 996/7134 completed (loss: 0.12746918201446533, acc: 0.9678770899772644)
[2025-02-13 02:31:26,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:26,742][root][INFO] - Training Epoch: 1/2, step 997/7134 completed (loss: 0.13693678379058838, acc: 0.9584527015686035)
[2025-02-13 02:31:26,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:27,169][root][INFO] - Training Epoch: 1/2, step 998/7134 completed (loss: 0.13704019784927368, acc: 0.9591549038887024)
[2025-02-13 02:31:27,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:27,585][root][INFO] - Training Epoch: 1/2, step 999/7134 completed (loss: 0.09650568664073944, acc: 0.9731183052062988)
[2025-02-13 02:31:27,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:28,000][root][INFO] - Training Epoch: 1/2, step 1000/7134 completed (loss: 0.10846716910600662, acc: 0.9760273694992065)
[2025-02-13 02:31:28,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:28,412][root][INFO] - Training Epoch: 1/2, step 1001/7134 completed (loss: 0.12100950628519058, acc: 0.9595959782600403)
[2025-02-13 02:31:28,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:28,861][root][INFO] - Training Epoch: 1/2, step 1002/7134 completed (loss: 0.1286163330078125, acc: 0.9657632112503052)
[2025-02-13 02:31:28,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:29,299][root][INFO] - Training Epoch: 1/2, step 1003/7134 completed (loss: 0.1358112245798111, acc: 0.9673105478286743)
[2025-02-13 02:31:29,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:29,701][root][INFO] - Training Epoch: 1/2, step 1004/7134 completed (loss: 0.1576738953590393, acc: 0.9670103192329407)
[2025-02-13 02:31:29,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:30,128][root][INFO] - Training Epoch: 1/2, step 1005/7134 completed (loss: 0.17073580622673035, acc: 0.9519867300987244)
[2025-02-13 02:31:30,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:30,538][root][INFO] - Training Epoch: 1/2, step 1006/7134 completed (loss: 0.12602785229682922, acc: 0.9637883305549622)
[2025-02-13 02:31:30,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:30,959][root][INFO] - Training Epoch: 1/2, step 1007/7134 completed (loss: 0.1511133313179016, acc: 0.9591836929321289)
[2025-02-13 02:31:31,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:31,397][root][INFO] - Training Epoch: 1/2, step 1008/7134 completed (loss: 0.13304823637008667, acc: 0.9602836966514587)
[2025-02-13 02:31:31,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:31,819][root][INFO] - Training Epoch: 1/2, step 1009/7134 completed (loss: 0.1351301074028015, acc: 0.9733570218086243)
[2025-02-13 02:31:31,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:32,230][root][INFO] - Training Epoch: 1/2, step 1010/7134 completed (loss: 0.09742289781570435, acc: 0.9664031863212585)
[2025-02-13 02:31:32,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:32,661][root][INFO] - Training Epoch: 1/2, step 1011/7134 completed (loss: 0.14723309874534607, acc: 0.9654218554496765)
[2025-02-13 02:31:32,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:33,090][root][INFO] - Training Epoch: 1/2, step 1012/7134 completed (loss: 0.12180541455745697, acc: 0.9688385128974915)
[2025-02-13 02:31:33,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:33,521][root][INFO] - Training Epoch: 1/2, step 1013/7134 completed (loss: 0.11500011384487152, acc: 0.9699140191078186)
[2025-02-13 02:31:33,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:33,941][root][INFO] - Training Epoch: 1/2, step 1014/7134 completed (loss: 0.1060202345252037, acc: 0.9655172228813171)
[2025-02-13 02:31:34,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:34,369][root][INFO] - Training Epoch: 1/2, step 1015/7134 completed (loss: 0.07584526389837265, acc: 0.9789156913757324)
[2025-02-13 02:31:34,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:34,797][root][INFO] - Training Epoch: 1/2, step 1016/7134 completed (loss: 0.11569744348526001, acc: 0.9751434326171875)
[2025-02-13 02:31:34,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:35,264][root][INFO] - Training Epoch: 1/2, step 1017/7134 completed (loss: 0.056953154504299164, acc: 0.9865771532058716)
[2025-02-13 02:31:35,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:35,720][root][INFO] - Training Epoch: 1/2, step 1018/7134 completed (loss: 0.11405618488788605, acc: 0.9751098155975342)
[2025-02-13 02:31:35,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:36,156][root][INFO] - Training Epoch: 1/2, step 1019/7134 completed (loss: 0.12364362180233002, acc: 0.9647436141967773)
[2025-02-13 02:31:36,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:36,583][root][INFO] - Training Epoch: 1/2, step 1020/7134 completed (loss: 0.10951614379882812, acc: 0.967164158821106)
[2025-02-13 02:31:36,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:37,010][root][INFO] - Training Epoch: 1/2, step 1021/7134 completed (loss: 0.06562381982803345, acc: 0.9808917045593262)
[2025-02-13 02:31:37,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:37,463][root][INFO] - Training Epoch: 1/2, step 1022/7134 completed (loss: 0.06542974710464478, acc: 0.9815546870231628)
[2025-02-13 02:31:37,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:37,904][root][INFO] - Training Epoch: 1/2, step 1023/7134 completed (loss: 0.08938528597354889, acc: 0.9806950092315674)
[2025-02-13 02:31:38,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:38,330][root][INFO] - Training Epoch: 1/2, step 1024/7134 completed (loss: 0.12890882790088654, acc: 0.9694793820381165)
[2025-02-13 02:31:38,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:38,742][root][INFO] - Training Epoch: 1/2, step 1025/7134 completed (loss: 0.12065233290195465, acc: 0.9768683314323425)
[2025-02-13 02:31:38,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:39,160][root][INFO] - Training Epoch: 1/2, step 1026/7134 completed (loss: 0.12763556838035583, acc: 0.9684210419654846)
[2025-02-13 02:31:39,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:39,576][root][INFO] - Training Epoch: 1/2, step 1027/7134 completed (loss: 0.06597007066011429, acc: 0.9797022938728333)
[2025-02-13 02:31:39,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:40,002][root][INFO] - Training Epoch: 1/2, step 1028/7134 completed (loss: 0.08318062126636505, acc: 0.9865471124649048)
[2025-02-13 02:31:40,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:40,455][root][INFO] - Training Epoch: 1/2, step 1029/7134 completed (loss: 0.07605478912591934, acc: 0.9732540845870972)
[2025-02-13 02:31:40,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:40,886][root][INFO] - Training Epoch: 1/2, step 1030/7134 completed (loss: 0.0362909771502018, acc: 0.9892183542251587)
[2025-02-13 02:31:41,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:41,286][root][INFO] - Training Epoch: 1/2, step 1031/7134 completed (loss: 0.1206742599606514, acc: 0.9664633870124817)
[2025-02-13 02:31:41,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:41,740][root][INFO] - Training Epoch: 1/2, step 1032/7134 completed (loss: 0.11783955246210098, acc: 0.9723502397537231)
[2025-02-13 02:31:41,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:42,197][root][INFO] - Training Epoch: 1/2, step 1033/7134 completed (loss: 0.08051152527332306, acc: 0.9756097793579102)
[2025-02-13 02:31:42,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:42,648][root][INFO] - Training Epoch: 1/2, step 1034/7134 completed (loss: 0.11305166035890579, acc: 0.9692058563232422)
[2025-02-13 02:31:42,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:43,000][root][INFO] - Training Epoch: 1/2, step 1035/7134 completed (loss: 0.29439714550971985, acc: 0.9456140398979187)
[2025-02-13 02:31:43,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:43,369][root][INFO] - Training Epoch: 1/2, step 1036/7134 completed (loss: 0.26381468772888184, acc: 0.9399999976158142)
[2025-02-13 02:31:43,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:43,819][root][INFO] - Training Epoch: 1/2, step 1037/7134 completed (loss: 0.06640356034040451, acc: 0.976190447807312)
[2025-02-13 02:31:43,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:44,264][root][INFO] - Training Epoch: 1/2, step 1038/7134 completed (loss: 0.08594711869955063, acc: 0.9731543660163879)
[2025-02-13 02:31:44,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:44,671][root][INFO] - Training Epoch: 1/2, step 1039/7134 completed (loss: 0.09865584969520569, acc: 0.977225661277771)
[2025-02-13 02:31:44,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:45,114][root][INFO] - Training Epoch: 1/2, step 1040/7134 completed (loss: 0.05192507058382034, acc: 0.9863013625144958)
[2025-02-13 02:31:45,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:45,536][root][INFO] - Training Epoch: 1/2, step 1041/7134 completed (loss: 0.07612910121679306, acc: 0.9813084006309509)
[2025-02-13 02:31:45,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:45,988][root][INFO] - Training Epoch: 1/2, step 1042/7134 completed (loss: 0.10482224822044373, acc: 0.9679487347602844)
[2025-02-13 02:31:46,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:46,444][root][INFO] - Training Epoch: 1/2, step 1043/7134 completed (loss: 0.08652424067258835, acc: 0.9711191058158875)
[2025-02-13 02:31:46,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:46,889][root][INFO] - Training Epoch: 1/2, step 1044/7134 completed (loss: 0.10060148686170578, acc: 0.9794520735740662)
[2025-02-13 02:31:47,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:47,287][root][INFO] - Training Epoch: 1/2, step 1045/7134 completed (loss: 0.15681906044483185, acc: 0.9660742878913879)
[2025-02-13 02:31:47,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:47,697][root][INFO] - Training Epoch: 1/2, step 1046/7134 completed (loss: 0.08373116701841354, acc: 0.9768339991569519)
[2025-02-13 02:31:47,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:48,126][root][INFO] - Training Epoch: 1/2, step 1047/7134 completed (loss: 0.16147112846374512, acc: 0.9585185050964355)
[2025-02-13 02:31:48,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:48,627][root][INFO] - Training Epoch: 1/2, step 1048/7134 completed (loss: 0.06367498636245728, acc: 0.9838709831237793)
[2025-02-13 02:31:48,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:49,058][root][INFO] - Training Epoch: 1/2, step 1049/7134 completed (loss: 0.12557163834571838, acc: 0.9677891731262207)
[2025-02-13 02:31:49,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:49,501][root][INFO] - Training Epoch: 1/2, step 1050/7134 completed (loss: 0.1021270751953125, acc: 0.9685792326927185)
[2025-02-13 02:31:49,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:49,955][root][INFO] - Training Epoch: 1/2, step 1051/7134 completed (loss: 0.08010999858379364, acc: 0.9794520735740662)
[2025-02-13 02:31:50,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:50,369][root][INFO] - Training Epoch: 1/2, step 1052/7134 completed (loss: 0.15705405175685883, acc: 0.9612277746200562)
[2025-02-13 02:31:50,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:50,785][root][INFO] - Training Epoch: 1/2, step 1053/7134 completed (loss: 0.08447408676147461, acc: 0.9760563373565674)
[2025-02-13 02:31:50,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:51,217][root][INFO] - Training Epoch: 1/2, step 1054/7134 completed (loss: 0.0999903604388237, acc: 0.9682119488716125)
[2025-02-13 02:31:51,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:51,624][root][INFO] - Training Epoch: 1/2, step 1055/7134 completed (loss: 0.15220651030540466, acc: 0.9578713774681091)
[2025-02-13 02:31:51,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:52,048][root][INFO] - Training Epoch: 1/2, step 1056/7134 completed (loss: 0.16250668466091156, acc: 0.9597989916801453)
[2025-02-13 02:31:52,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:52,473][root][INFO] - Training Epoch: 1/2, step 1057/7134 completed (loss: 0.17110422253608704, acc: 0.9552023410797119)
[2025-02-13 02:31:52,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:52,927][root][INFO] - Training Epoch: 1/2, step 1058/7134 completed (loss: 0.13024239242076874, acc: 0.9666666388511658)
[2025-02-13 02:31:53,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:53,389][root][INFO] - Training Epoch: 1/2, step 1059/7134 completed (loss: 0.11968853324651718, acc: 0.9690322875976562)
[2025-02-13 02:31:53,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:53,813][root][INFO] - Training Epoch: 1/2, step 1060/7134 completed (loss: 0.1330859661102295, acc: 0.9668769836425781)
[2025-02-13 02:31:53,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:54,280][root][INFO] - Training Epoch: 1/2, step 1061/7134 completed (loss: 0.17102722823619843, acc: 0.9622092843055725)
[2025-02-13 02:31:54,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:54,688][root][INFO] - Training Epoch: 1/2, step 1062/7134 completed (loss: 0.13070079684257507, acc: 0.9559321999549866)
[2025-02-13 02:31:54,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:55,113][root][INFO] - Training Epoch: 1/2, step 1063/7134 completed (loss: 0.08033964037895203, acc: 0.9789302945137024)
[2025-02-13 02:31:55,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:55,571][root][INFO] - Training Epoch: 1/2, step 1064/7134 completed (loss: 0.12915697693824768, acc: 0.9629629850387573)
[2025-02-13 02:31:55,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:56,009][root][INFO] - Training Epoch: 1/2, step 1065/7134 completed (loss: 0.0713687539100647, acc: 0.9783197641372681)
[2025-02-13 02:31:56,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:56,430][root][INFO] - Training Epoch: 1/2, step 1066/7134 completed (loss: 0.10604646801948547, acc: 0.9748322367668152)
[2025-02-13 02:31:56,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:56,847][root][INFO] - Training Epoch: 1/2, step 1067/7134 completed (loss: 0.06835195422172546, acc: 0.9753466844558716)
[2025-02-13 02:31:56,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:57,261][root][INFO] - Training Epoch: 1/2, step 1068/7134 completed (loss: 0.09651612490415573, acc: 0.9699812531471252)
[2025-02-13 02:31:57,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:57,686][root][INFO] - Training Epoch: 1/2, step 1069/7134 completed (loss: 0.12210965901613235, acc: 0.9666110277175903)
[2025-02-13 02:31:57,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:58,078][root][INFO] - Training Epoch: 1/2, step 1070/7134 completed (loss: 0.12469838559627533, acc: 0.9737274050712585)
[2025-02-13 02:31:58,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:58,537][root][INFO] - Training Epoch: 1/2, step 1071/7134 completed (loss: 0.06720099598169327, acc: 0.9852744340896606)
[2025-02-13 02:31:58,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:58,977][root][INFO] - Training Epoch: 1/2, step 1072/7134 completed (loss: 0.040396708995103836, acc: 0.9878214001655579)
[2025-02-13 02:31:59,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:59,398][root][INFO] - Training Epoch: 1/2, step 1073/7134 completed (loss: 0.04522594437003136, acc: 0.9870316982269287)
[2025-02-13 02:31:59,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:59,800][root][INFO] - Training Epoch: 1/2, step 1074/7134 completed (loss: 0.13367260992527008, acc: 0.9689119458198547)
[2025-02-13 02:31:59,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:00,241][root][INFO] - Training Epoch: 1/2, step 1075/7134 completed (loss: 0.22009789943695068, acc: 0.9558011293411255)
[2025-02-13 02:32:00,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:00,683][root][INFO] - Training Epoch: 1/2, step 1076/7134 completed (loss: 0.23376278579235077, acc: 0.9428191781044006)
[2025-02-13 02:32:00,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:01,078][root][INFO] - Training Epoch: 1/2, step 1077/7134 completed (loss: 0.2237807810306549, acc: 0.9436936974525452)
[2025-02-13 02:32:01,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:01,521][root][INFO] - Training Epoch: 1/2, step 1078/7134 completed (loss: 0.2815975546836853, acc: 0.9369951486587524)
[2025-02-13 02:32:01,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:01,965][root][INFO] - Training Epoch: 1/2, step 1079/7134 completed (loss: 0.12132541090250015, acc: 0.967704713344574)
[2025-02-13 02:32:02,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:02,367][root][INFO] - Training Epoch: 1/2, step 1080/7134 completed (loss: 0.12188491225242615, acc: 0.9718044996261597)
[2025-02-13 02:32:02,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:02,800][root][INFO] - Training Epoch: 1/2, step 1081/7134 completed (loss: 0.10883935540914536, acc: 0.9719495177268982)
[2025-02-13 02:32:02,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:03,304][root][INFO] - Training Epoch: 1/2, step 1082/7134 completed (loss: 0.07139506191015244, acc: 0.9773895144462585)
[2025-02-13 02:32:03,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:03,767][root][INFO] - Training Epoch: 1/2, step 1083/7134 completed (loss: 0.08475882560014725, acc: 0.9751412272453308)
[2025-02-13 02:32:03,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:04,222][root][INFO] - Training Epoch: 1/2, step 1084/7134 completed (loss: 0.09020631015300751, acc: 0.9776951670646667)
[2025-02-13 02:32:04,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:04,654][root][INFO] - Training Epoch: 1/2, step 1085/7134 completed (loss: 0.07793385535478592, acc: 0.9785810112953186)
[2025-02-13 02:32:04,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:05,056][root][INFO] - Training Epoch: 1/2, step 1086/7134 completed (loss: 0.13939762115478516, acc: 0.964083194732666)
[2025-02-13 02:32:05,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:05,511][root][INFO] - Training Epoch: 1/2, step 1087/7134 completed (loss: 0.09257391840219498, acc: 0.9743902683258057)
[2025-02-13 02:32:05,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:05,933][root][INFO] - Training Epoch: 1/2, step 1088/7134 completed (loss: 0.07374988496303558, acc: 0.9776536226272583)
[2025-02-13 02:32:06,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:06,352][root][INFO] - Training Epoch: 1/2, step 1089/7134 completed (loss: 0.09520865976810455, acc: 0.9701937437057495)
[2025-02-13 02:32:06,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:06,786][root][INFO] - Training Epoch: 1/2, step 1090/7134 completed (loss: 0.10616294294595718, acc: 0.9687987565994263)
[2025-02-13 02:32:06,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:07,198][root][INFO] - Training Epoch: 1/2, step 1091/7134 completed (loss: 0.0928250178694725, acc: 0.9710806608200073)
[2025-02-13 02:32:07,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:07,646][root][INFO] - Training Epoch: 1/2, step 1092/7134 completed (loss: 0.057352110743522644, acc: 0.9812138676643372)
[2025-02-13 02:32:07,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:08,070][root][INFO] - Training Epoch: 1/2, step 1093/7134 completed (loss: 0.1318172663450241, acc: 0.9717868566513062)
[2025-02-13 02:32:08,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:08,494][root][INFO] - Training Epoch: 1/2, step 1094/7134 completed (loss: 0.14494043588638306, acc: 0.9609665274620056)
[2025-02-13 02:32:08,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:08,997][root][INFO] - Training Epoch: 1/2, step 1095/7134 completed (loss: 0.1210700273513794, acc: 0.9634591937065125)
[2025-02-13 02:32:09,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:09,468][root][INFO] - Training Epoch: 1/2, step 1096/7134 completed (loss: 0.1106351912021637, acc: 0.9683544039726257)
[2025-02-13 02:32:09,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:09,917][root][INFO] - Training Epoch: 1/2, step 1097/7134 completed (loss: 0.14060157537460327, acc: 0.959269642829895)
[2025-02-13 02:32:10,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:10,346][root][INFO] - Training Epoch: 1/2, step 1098/7134 completed (loss: 0.08513010293245316, acc: 0.9752781391143799)
[2025-02-13 02:32:10,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:10,792][root][INFO] - Training Epoch: 1/2, step 1099/7134 completed (loss: 0.100015789270401, acc: 0.9756410121917725)
[2025-02-13 02:32:10,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:11,254][root][INFO] - Training Epoch: 1/2, step 1100/7134 completed (loss: 0.10967656970024109, acc: 0.967391312122345)
[2025-02-13 02:32:11,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:11,706][root][INFO] - Training Epoch: 1/2, step 1101/7134 completed (loss: 0.0949607640504837, acc: 0.9701298475265503)
[2025-02-13 02:32:11,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:12,120][root][INFO] - Training Epoch: 1/2, step 1102/7134 completed (loss: 0.13010773062705994, acc: 0.9608610272407532)
[2025-02-13 02:32:12,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:12,578][root][INFO] - Training Epoch: 1/2, step 1103/7134 completed (loss: 0.13280099630355835, acc: 0.9666666388511658)
[2025-02-13 02:32:12,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:12,993][root][INFO] - Training Epoch: 1/2, step 1104/7134 completed (loss: 0.10471991449594498, acc: 0.9783861637115479)
[2025-02-13 02:32:13,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:13,416][root][INFO] - Training Epoch: 1/2, step 1105/7134 completed (loss: 0.12193088233470917, acc: 0.9663072824478149)
[2025-02-13 02:32:13,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:13,839][root][INFO] - Training Epoch: 1/2, step 1106/7134 completed (loss: 0.15167169272899628, acc: 0.9614325165748596)
[2025-02-13 02:32:13,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:14,310][root][INFO] - Training Epoch: 1/2, step 1107/7134 completed (loss: 0.09704088419675827, acc: 0.969519317150116)
[2025-02-13 02:32:14,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:14,752][root][INFO] - Training Epoch: 1/2, step 1108/7134 completed (loss: 0.07793648540973663, acc: 0.9768041372299194)
[2025-02-13 02:32:14,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:15,209][root][INFO] - Training Epoch: 1/2, step 1109/7134 completed (loss: 0.1541372537612915, acc: 0.9497206807136536)
[2025-02-13 02:32:15,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:15,634][root][INFO] - Training Epoch: 1/2, step 1110/7134 completed (loss: 0.14806057512760162, acc: 0.9580602645874023)
[2025-02-13 02:32:15,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:16,058][root][INFO] - Training Epoch: 1/2, step 1111/7134 completed (loss: 0.09113430976867676, acc: 0.980663001537323)
[2025-02-13 02:32:16,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:16,454][root][INFO] - Training Epoch: 1/2, step 1112/7134 completed (loss: 0.14308872818946838, acc: 0.9558823704719543)
[2025-02-13 02:32:16,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:16,882][root][INFO] - Training Epoch: 1/2, step 1113/7134 completed (loss: 0.0980084016919136, acc: 0.9678249955177307)
[2025-02-13 02:32:17,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:17,335][root][INFO] - Training Epoch: 1/2, step 1114/7134 completed (loss: 0.10573960840702057, acc: 0.9756986498832703)
[2025-02-13 02:32:17,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:17,760][root][INFO] - Training Epoch: 1/2, step 1115/7134 completed (loss: 0.1285342574119568, acc: 0.9622641801834106)
[2025-02-13 02:32:17,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:18,161][root][INFO] - Training Epoch: 1/2, step 1116/7134 completed (loss: 0.059037432074546814, acc: 0.983208954334259)
[2025-02-13 02:32:18,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:18,571][root][INFO] - Training Epoch: 1/2, step 1117/7134 completed (loss: 0.09574568271636963, acc: 0.9708588719367981)
[2025-02-13 02:32:18,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:18,967][root][INFO] - Training Epoch: 1/2, step 1118/7134 completed (loss: 0.11364976316690445, acc: 0.9655172228813171)
[2025-02-13 02:32:19,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:19,382][root][INFO] - Training Epoch: 1/2, step 1119/7134 completed (loss: 0.11961212009191513, acc: 0.9636650681495667)
[2025-02-13 02:32:19,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:19,807][root][INFO] - Training Epoch: 1/2, step 1120/7134 completed (loss: 0.1600501388311386, acc: 0.955974817276001)
[2025-02-13 02:32:19,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:20,221][root][INFO] - Training Epoch: 1/2, step 1121/7134 completed (loss: 0.14618751406669617, acc: 0.9586410522460938)
[2025-02-13 02:32:20,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:20,592][root][INFO] - Training Epoch: 1/2, step 1122/7134 completed (loss: 0.07755716145038605, acc: 0.9825242757797241)
[2025-02-13 02:32:20,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:21,006][root][INFO] - Training Epoch: 1/2, step 1123/7134 completed (loss: 0.07890767604112625, acc: 0.9791332483291626)
[2025-02-13 02:32:21,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:21,436][root][INFO] - Training Epoch: 1/2, step 1124/7134 completed (loss: 0.06902550905942917, acc: 0.9829721450805664)
[2025-02-13 02:32:21,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:21,866][root][INFO] - Training Epoch: 1/2, step 1125/7134 completed (loss: 0.058594271540641785, acc: 0.9824086427688599)
[2025-02-13 02:32:22,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:22,288][root][INFO] - Training Epoch: 1/2, step 1126/7134 completed (loss: 0.126235231757164, acc: 0.969613254070282)
[2025-02-13 02:32:22,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:22,705][root][INFO] - Training Epoch: 1/2, step 1127/7134 completed (loss: 0.09942415356636047, acc: 0.9648000001907349)
[2025-02-13 02:32:22,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:23,121][root][INFO] - Training Epoch: 1/2, step 1128/7134 completed (loss: 0.08601026237010956, acc: 0.96875)
[2025-02-13 02:32:23,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:23,501][root][INFO] - Training Epoch: 1/2, step 1129/7134 completed (loss: 0.09406592696905136, acc: 0.9730941653251648)
[2025-02-13 02:32:23,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:23,946][root][INFO] - Training Epoch: 1/2, step 1130/7134 completed (loss: 0.08955217152833939, acc: 0.9773828983306885)
[2025-02-13 02:32:24,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:24,422][root][INFO] - Training Epoch: 1/2, step 1131/7134 completed (loss: 0.13011892139911652, acc: 0.963443398475647)
[2025-02-13 02:32:24,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:24,833][root][INFO] - Training Epoch: 1/2, step 1132/7134 completed (loss: 0.2149815857410431, acc: 0.9540581703186035)
[2025-02-13 02:32:24,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:25,297][root][INFO] - Training Epoch: 1/2, step 1133/7134 completed (loss: 0.17944540083408356, acc: 0.9543509483337402)
[2025-02-13 02:32:25,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:25,732][root][INFO] - Training Epoch: 1/2, step 1134/7134 completed (loss: 0.12326968461275101, acc: 0.971321702003479)
[2025-02-13 02:32:25,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:26,144][root][INFO] - Training Epoch: 1/2, step 1135/7134 completed (loss: 0.11448980867862701, acc: 0.9760000109672546)
[2025-02-13 02:32:26,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:26,600][root][INFO] - Training Epoch: 1/2, step 1136/7134 completed (loss: 0.17411591112613678, acc: 0.9551020264625549)
[2025-02-13 02:32:26,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:26,915][root][INFO] - Training Epoch: 1/2, step 1137/7134 completed (loss: 0.1280898153781891, acc: 0.9636363387107849)
[2025-02-13 02:32:27,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:27,403][root][INFO] - Training Epoch: 1/2, step 1138/7134 completed (loss: 0.13225668668746948, acc: 0.9576502442359924)
[2025-02-13 02:32:27,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:27,819][root][INFO] - Training Epoch: 1/2, step 1139/7134 completed (loss: 0.1709594428539276, acc: 0.9494097828865051)
[2025-02-13 02:32:27,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:28,265][root][INFO] - Training Epoch: 1/2, step 1140/7134 completed (loss: 0.1079457625746727, acc: 0.9774436354637146)
[2025-02-13 02:32:28,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:28,695][root][INFO] - Training Epoch: 1/2, step 1141/7134 completed (loss: 0.09188124537467957, acc: 0.9672130942344666)
[2025-02-13 02:32:28,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:29,138][root][INFO] - Training Epoch: 1/2, step 1142/7134 completed (loss: 0.1060541495680809, acc: 0.9778645634651184)
[2025-02-13 02:32:29,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:29,563][root][INFO] - Training Epoch: 1/2, step 1143/7134 completed (loss: 0.15845713019371033, acc: 0.9554937481880188)
[2025-02-13 02:32:29,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:30,006][root][INFO] - Training Epoch: 1/2, step 1144/7134 completed (loss: 0.12913087010383606, acc: 0.9674999713897705)
[2025-02-13 02:32:30,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:30,453][root][INFO] - Training Epoch: 1/2, step 1145/7134 completed (loss: 0.0787515789270401, acc: 0.9808153510093689)
[2025-02-13 02:32:30,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:30,866][root][INFO] - Training Epoch: 1/2, step 1146/7134 completed (loss: 0.09209388494491577, acc: 0.9800724387168884)
[2025-02-13 02:32:30,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:31,292][root][INFO] - Training Epoch: 1/2, step 1147/7134 completed (loss: 0.0805794894695282, acc: 0.971321702003479)
[2025-02-13 02:32:31,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:31,719][root][INFO] - Training Epoch: 1/2, step 1148/7134 completed (loss: 0.10806199908256531, acc: 0.9726962447166443)
[2025-02-13 02:32:31,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:32,126][root][INFO] - Training Epoch: 1/2, step 1149/7134 completed (loss: 0.21937468647956848, acc: 0.9384615421295166)
[2025-02-13 02:32:32,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:32,574][root][INFO] - Training Epoch: 1/2, step 1150/7134 completed (loss: 0.09367309510707855, acc: 0.9782945513725281)
[2025-02-13 02:32:32,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:33,002][root][INFO] - Training Epoch: 1/2, step 1151/7134 completed (loss: 0.04325070232152939, acc: 0.9918864369392395)
[2025-02-13 02:32:33,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:33,426][root][INFO] - Training Epoch: 1/2, step 1152/7134 completed (loss: 0.06021071970462799, acc: 0.9811965823173523)
[2025-02-13 02:32:33,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:33,872][root][INFO] - Training Epoch: 1/2, step 1153/7134 completed (loss: 0.13720905780792236, acc: 0.9673024415969849)
[2025-02-13 02:32:34,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:34,297][root][INFO] - Training Epoch: 1/2, step 1154/7134 completed (loss: 0.13888293504714966, acc: 0.969911515712738)
[2025-02-13 02:32:34,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:34,723][root][INFO] - Training Epoch: 1/2, step 1155/7134 completed (loss: 0.1311265081167221, acc: 0.9627193212509155)
[2025-02-13 02:32:34,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:35,177][root][INFO] - Training Epoch: 1/2, step 1156/7134 completed (loss: 0.04919380322098732, acc: 0.9846389889717102)
[2025-02-13 02:32:35,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:35,594][root][INFO] - Training Epoch: 1/2, step 1157/7134 completed (loss: 0.11412164568901062, acc: 0.9762309193611145)
[2025-02-13 02:32:35,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:36,006][root][INFO] - Training Epoch: 1/2, step 1158/7134 completed (loss: 0.12679968774318695, acc: 0.9684418439865112)
[2025-02-13 02:32:36,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:36,454][root][INFO] - Training Epoch: 1/2, step 1159/7134 completed (loss: 0.11932067573070526, acc: 0.9673758745193481)
[2025-02-13 02:32:36,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:36,908][root][INFO] - Training Epoch: 1/2, step 1160/7134 completed (loss: 0.07975698262453079, acc: 0.976190447807312)
[2025-02-13 02:32:37,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:37,333][root][INFO] - Training Epoch: 1/2, step 1161/7134 completed (loss: 0.15424826741218567, acc: 0.9706361889839172)
[2025-02-13 02:32:37,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:37,742][root][INFO] - Training Epoch: 1/2, step 1162/7134 completed (loss: 0.07499480992555618, acc: 0.9768595099449158)
[2025-02-13 02:32:37,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:38,143][root][INFO] - Training Epoch: 1/2, step 1163/7134 completed (loss: 0.07999706268310547, acc: 0.9812286496162415)
[2025-02-13 02:32:38,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:38,565][root][INFO] - Training Epoch: 1/2, step 1164/7134 completed (loss: 0.11118001490831375, acc: 0.9735743999481201)
[2025-02-13 02:32:38,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:39,034][root][INFO] - Training Epoch: 1/2, step 1165/7134 completed (loss: 0.060686808079481125, acc: 0.9837177991867065)
[2025-02-13 02:32:39,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:39,454][root][INFO] - Training Epoch: 1/2, step 1166/7134 completed (loss: 0.06688611954450607, acc: 0.9835391044616699)
[2025-02-13 02:32:39,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:39,872][root][INFO] - Training Epoch: 1/2, step 1167/7134 completed (loss: 0.05054186284542084, acc: 0.9882121682167053)
[2025-02-13 02:32:40,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:40,345][root][INFO] - Training Epoch: 1/2, step 1168/7134 completed (loss: 0.04559580981731415, acc: 0.9868420958518982)
[2025-02-13 02:32:40,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:40,798][root][INFO] - Training Epoch: 1/2, step 1169/7134 completed (loss: 0.09609896689653397, acc: 0.9743243455886841)
[2025-02-13 02:32:40,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:41,228][root][INFO] - Training Epoch: 1/2, step 1170/7134 completed (loss: 0.05405386909842491, acc: 0.9873595237731934)
[2025-02-13 02:32:41,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:41,596][root][INFO] - Training Epoch: 1/2, step 1171/7134 completed (loss: 0.0512666329741478, acc: 0.9866443872451782)
[2025-02-13 02:32:41,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:42,013][root][INFO] - Training Epoch: 1/2, step 1172/7134 completed (loss: 0.09327548742294312, acc: 0.9773070812225342)
[2025-02-13 02:32:42,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:42,444][root][INFO] - Training Epoch: 1/2, step 1173/7134 completed (loss: 0.09142237901687622, acc: 0.9768115878105164)
[2025-02-13 02:32:42,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:42,878][root][INFO] - Training Epoch: 1/2, step 1174/7134 completed (loss: 0.04872608557343483, acc: 0.9879879951477051)
[2025-02-13 02:32:43,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:43,329][root][INFO] - Training Epoch: 1/2, step 1175/7134 completed (loss: 0.07915420830249786, acc: 0.9739130139350891)
[2025-02-13 02:32:43,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:43,755][root][INFO] - Training Epoch: 1/2, step 1176/7134 completed (loss: 0.03417641296982765, acc: 0.9880136847496033)
[2025-02-13 02:32:43,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:44,151][root][INFO] - Training Epoch: 1/2, step 1177/7134 completed (loss: 0.03178715333342552, acc: 0.9882352948188782)
[2025-02-13 02:32:44,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:44,526][root][INFO] - Training Epoch: 1/2, step 1178/7134 completed (loss: 0.05101226270198822, acc: 0.9836448431015015)
[2025-02-13 02:32:44,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:44,953][root][INFO] - Training Epoch: 1/2, step 1179/7134 completed (loss: 0.05935181304812431, acc: 0.980879545211792)
[2025-02-13 02:32:45,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:45,412][root][INFO] - Training Epoch: 1/2, step 1180/7134 completed (loss: 0.06268524378538132, acc: 0.985401451587677)
[2025-02-13 02:32:45,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:45,837][root][INFO] - Training Epoch: 1/2, step 1181/7134 completed (loss: 0.0767962634563446, acc: 0.9827337861061096)
[2025-02-13 02:32:45,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:46,235][root][INFO] - Training Epoch: 1/2, step 1182/7134 completed (loss: 0.045182134956121445, acc: 0.9863247871398926)
[2025-02-13 02:32:46,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:46,682][root][INFO] - Training Epoch: 1/2, step 1183/7134 completed (loss: 0.04513395577669144, acc: 0.98531574010849)
[2025-02-13 02:32:46,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:47,106][root][INFO] - Training Epoch: 1/2, step 1184/7134 completed (loss: 0.1007508784532547, acc: 0.9785407781600952)
[2025-02-13 02:32:47,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:47,538][root][INFO] - Training Epoch: 1/2, step 1185/7134 completed (loss: 0.030292149633169174, acc: 0.9894737005233765)
[2025-02-13 02:32:47,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:47,963][root][INFO] - Training Epoch: 1/2, step 1186/7134 completed (loss: 0.042032916098833084, acc: 0.993514895439148)
[2025-02-13 02:32:48,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:48,394][root][INFO] - Training Epoch: 1/2, step 1187/7134 completed (loss: 0.06747884303331375, acc: 0.9806094169616699)
[2025-02-13 02:32:48,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:48,839][root][INFO] - Training Epoch: 1/2, step 1188/7134 completed (loss: 0.02472830004990101, acc: 0.9920844435691833)
[2025-02-13 02:32:48,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:49,265][root][INFO] - Training Epoch: 1/2, step 1189/7134 completed (loss: 0.05736825615167618, acc: 0.986994206905365)
[2025-02-13 02:32:49,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:49,674][root][INFO] - Training Epoch: 1/2, step 1190/7134 completed (loss: 0.04575251787900925, acc: 0.9872773289680481)
[2025-02-13 02:32:49,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:50,102][root][INFO] - Training Epoch: 1/2, step 1191/7134 completed (loss: 0.10387091338634491, acc: 0.9689807891845703)
[2025-02-13 02:32:50,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:50,546][root][INFO] - Training Epoch: 1/2, step 1192/7134 completed (loss: 0.06913569569587708, acc: 0.9737876653671265)
[2025-02-13 02:32:50,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:50,976][root][INFO] - Training Epoch: 1/2, step 1193/7134 completed (loss: 0.11427845060825348, acc: 0.9696969985961914)
[2025-02-13 02:32:51,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:51,401][root][INFO] - Training Epoch: 1/2, step 1194/7134 completed (loss: 0.08156891912221909, acc: 0.9821428656578064)
[2025-02-13 02:32:51,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:51,834][root][INFO] - Training Epoch: 1/2, step 1195/7134 completed (loss: 0.10076556354761124, acc: 0.9754902124404907)
[2025-02-13 02:32:51,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:52,277][root][INFO] - Training Epoch: 1/2, step 1196/7134 completed (loss: 0.11654524505138397, acc: 0.9704918265342712)
[2025-02-13 02:32:52,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:52,686][root][INFO] - Training Epoch: 1/2, step 1197/7134 completed (loss: 0.08063548058271408, acc: 0.9809384346008301)
[2025-02-13 02:32:52,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:53,109][root][INFO] - Training Epoch: 1/2, step 1198/7134 completed (loss: 0.0678652822971344, acc: 0.9769784212112427)
[2025-02-13 02:32:53,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:53,580][root][INFO] - Training Epoch: 1/2, step 1199/7134 completed (loss: 0.05095489323139191, acc: 0.9867452383041382)
[2025-02-13 02:32:53,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:54,014][root][INFO] - Training Epoch: 1/2, step 1200/7134 completed (loss: 0.12002868205308914, acc: 0.975359320640564)
[2025-02-13 02:32:54,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:54,473][root][INFO] - Training Epoch: 1/2, step 1201/7134 completed (loss: 0.0814189612865448, acc: 0.9793814420700073)
[2025-02-13 02:32:54,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:54,941][root][INFO] - Training Epoch: 1/2, step 1202/7134 completed (loss: 0.08061631768941879, acc: 0.9771198034286499)
[2025-02-13 02:32:55,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:55,346][root][INFO] - Training Epoch: 1/2, step 1203/7134 completed (loss: 0.03051431104540825, acc: 0.9924127459526062)
[2025-02-13 02:32:55,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:55,749][root][INFO] - Training Epoch: 1/2, step 1204/7134 completed (loss: 0.07761123776435852, acc: 0.97947758436203)
[2025-02-13 02:32:55,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:56,187][root][INFO] - Training Epoch: 1/2, step 1205/7134 completed (loss: 0.10325039178133011, acc: 0.9704225063323975)
[2025-02-13 02:32:56,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:56,657][root][INFO] - Training Epoch: 1/2, step 1206/7134 completed (loss: 0.08212539553642273, acc: 0.9789081811904907)
[2025-02-13 02:32:56,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:57,112][root][INFO] - Training Epoch: 1/2, step 1207/7134 completed (loss: 0.07623588293790817, acc: 0.9861809015274048)
[2025-02-13 02:32:57,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:57,533][root][INFO] - Training Epoch: 1/2, step 1208/7134 completed (loss: 0.0912327989935875, acc: 0.9757462739944458)
[2025-02-13 02:32:57,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:57,951][root][INFO] - Training Epoch: 1/2, step 1209/7134 completed (loss: 0.15371209383010864, acc: 0.9616087675094604)
[2025-02-13 02:32:58,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:58,373][root][INFO] - Training Epoch: 1/2, step 1210/7134 completed (loss: 0.10307841002941132, acc: 0.9744816422462463)
[2025-02-13 02:32:58,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:58,787][root][INFO] - Training Epoch: 1/2, step 1211/7134 completed (loss: 0.18882378935813904, acc: 0.9518304467201233)
[2025-02-13 02:32:58,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:59,195][root][INFO] - Training Epoch: 1/2, step 1212/7134 completed (loss: 0.13781332969665527, acc: 0.9710366129875183)
[2025-02-13 02:32:59,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:59,639][root][INFO] - Training Epoch: 1/2, step 1213/7134 completed (loss: 0.08295831829309464, acc: 0.9688826203346252)
[2025-02-13 02:32:59,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:00,054][root][INFO] - Training Epoch: 1/2, step 1214/7134 completed (loss: 0.1496112048625946, acc: 0.9615384340286255)
[2025-02-13 02:33:00,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:00,503][root][INFO] - Training Epoch: 1/2, step 1215/7134 completed (loss: 0.1368524432182312, acc: 0.9641379117965698)
[2025-02-13 02:33:00,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:00,945][root][INFO] - Training Epoch: 1/2, step 1216/7134 completed (loss: 0.07652075588703156, acc: 0.9751824736595154)
[2025-02-13 02:33:01,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:01,401][root][INFO] - Training Epoch: 1/2, step 1217/7134 completed (loss: 0.0996190756559372, acc: 0.975649356842041)
[2025-02-13 02:33:01,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:01,869][root][INFO] - Training Epoch: 1/2, step 1218/7134 completed (loss: 0.18133802711963654, acc: 0.9611940383911133)
[2025-02-13 02:33:02,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:02,329][root][INFO] - Training Epoch: 1/2, step 1219/7134 completed (loss: 0.22330273687839508, acc: 0.9507908821105957)
[2025-02-13 02:33:02,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:02,792][root][INFO] - Training Epoch: 1/2, step 1220/7134 completed (loss: 0.17006108164787292, acc: 0.9581218361854553)
[2025-02-13 02:33:02,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:03,289][root][INFO] - Training Epoch: 1/2, step 1221/7134 completed (loss: 0.1706673949956894, acc: 0.9550438523292542)
[2025-02-13 02:33:03,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:03,745][root][INFO] - Training Epoch: 1/2, step 1222/7134 completed (loss: 0.13782504200935364, acc: 0.9642431735992432)
[2025-02-13 02:33:03,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:04,197][root][INFO] - Training Epoch: 1/2, step 1223/7134 completed (loss: 0.2505141794681549, acc: 0.9451302886009216)
[2025-02-13 02:33:04,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:04,665][root][INFO] - Training Epoch: 1/2, step 1224/7134 completed (loss: 0.1273220330476761, acc: 0.9713024497032166)
[2025-02-13 02:33:04,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:05,130][root][INFO] - Training Epoch: 1/2, step 1225/7134 completed (loss: 0.1509818136692047, acc: 0.9545970559120178)
[2025-02-13 02:33:05,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:05,577][root][INFO] - Training Epoch: 1/2, step 1226/7134 completed (loss: 0.21807126700878143, acc: 0.9456740617752075)
[2025-02-13 02:33:05,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:06,018][root][INFO] - Training Epoch: 1/2, step 1227/7134 completed (loss: 0.1893523931503296, acc: 0.9436997175216675)
[2025-02-13 02:33:06,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:06,456][root][INFO] - Training Epoch: 1/2, step 1228/7134 completed (loss: 0.16167078912258148, acc: 0.9599427580833435)
[2025-02-13 02:33:06,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:06,905][root][INFO] - Training Epoch: 1/2, step 1229/7134 completed (loss: 0.1261899173259735, acc: 0.9639856219291687)
[2025-02-13 02:33:07,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:07,375][root][INFO] - Training Epoch: 1/2, step 1230/7134 completed (loss: 0.14604218304157257, acc: 0.9604519605636597)
[2025-02-13 02:33:07,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:07,880][root][INFO] - Training Epoch: 1/2, step 1231/7134 completed (loss: 0.11968906223773956, acc: 0.9648396968841553)
[2025-02-13 02:33:08,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:08,385][root][INFO] - Training Epoch: 1/2, step 1232/7134 completed (loss: 0.12309218943119049, acc: 0.9701810479164124)
[2025-02-13 02:33:08,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:08,844][root][INFO] - Training Epoch: 1/2, step 1233/7134 completed (loss: 0.19677011668682098, acc: 0.9468207955360413)
[2025-02-13 02:33:08,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:09,319][root][INFO] - Training Epoch: 1/2, step 1234/7134 completed (loss: 0.10048073530197144, acc: 0.9767080545425415)
[2025-02-13 02:33:09,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:09,784][root][INFO] - Training Epoch: 1/2, step 1235/7134 completed (loss: 0.14436781406402588, acc: 0.9688249230384827)
[2025-02-13 02:33:09,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:10,244][root][INFO] - Training Epoch: 1/2, step 1236/7134 completed (loss: 0.24987688660621643, acc: 0.9311859607696533)
[2025-02-13 02:33:10,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:10,703][root][INFO] - Training Epoch: 1/2, step 1237/7134 completed (loss: 0.16132232546806335, acc: 0.9627329111099243)
[2025-02-13 02:33:10,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:11,152][root][INFO] - Training Epoch: 1/2, step 1238/7134 completed (loss: 0.25321564078330994, acc: 0.9170305728912354)
[2025-02-13 02:33:11,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:11,589][root][INFO] - Training Epoch: 1/2, step 1239/7134 completed (loss: 0.20628225803375244, acc: 0.9370529055595398)
[2025-02-13 02:33:11,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:12,053][root][INFO] - Training Epoch: 1/2, step 1240/7134 completed (loss: 0.18379773199558258, acc: 0.9391534328460693)
[2025-02-13 02:33:12,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:12,302][root][INFO] - Training Epoch: 1/2, step 1241/7134 completed (loss: 0.1822226494550705, acc: 0.9558233022689819)
[2025-02-13 02:33:12,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:12,768][root][INFO] - Training Epoch: 1/2, step 1242/7134 completed (loss: 0.1515377014875412, acc: 0.9543269276618958)
[2025-02-13 02:33:12,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:13,241][root][INFO] - Training Epoch: 1/2, step 1243/7134 completed (loss: 0.13064083456993103, acc: 0.9665272235870361)
[2025-02-13 02:33:13,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:13,686][root][INFO] - Training Epoch: 1/2, step 1244/7134 completed (loss: 0.14700935781002045, acc: 0.9569536447525024)
[2025-02-13 02:33:13,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:14,161][root][INFO] - Training Epoch: 1/2, step 1245/7134 completed (loss: 0.12382715940475464, acc: 0.9647355079650879)
[2025-02-13 02:33:14,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:14,580][root][INFO] - Training Epoch: 1/2, step 1246/7134 completed (loss: 0.1630374640226364, acc: 0.9550706148147583)
[2025-02-13 02:33:14,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:15,029][root][INFO] - Training Epoch: 1/2, step 1247/7134 completed (loss: 0.10681479424238205, acc: 0.9710564613342285)
[2025-02-13 02:33:15,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:15,456][root][INFO] - Training Epoch: 1/2, step 1248/7134 completed (loss: 0.11156601458787918, acc: 0.970251739025116)
[2025-02-13 02:33:15,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:15,876][root][INFO] - Training Epoch: 1/2, step 1249/7134 completed (loss: 0.09479645639657974, acc: 0.9786477088928223)
[2025-02-13 02:33:16,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:16,288][root][INFO] - Training Epoch: 1/2, step 1250/7134 completed (loss: 0.062451187521219254, acc: 0.9830028414726257)
[2025-02-13 02:33:16,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:16,715][root][INFO] - Training Epoch: 1/2, step 1251/7134 completed (loss: 0.07299906015396118, acc: 0.9807956218719482)
[2025-02-13 02:33:16,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:17,139][root][INFO] - Training Epoch: 1/2, step 1252/7134 completed (loss: 0.0838264673948288, acc: 0.9845678806304932)
[2025-02-13 02:33:17,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:17,566][root][INFO] - Training Epoch: 1/2, step 1253/7134 completed (loss: 0.11183685809373856, acc: 0.9710424542427063)
[2025-02-13 02:33:17,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:17,992][root][INFO] - Training Epoch: 1/2, step 1254/7134 completed (loss: 0.15808185935020447, acc: 0.9560605883598328)
[2025-02-13 02:33:18,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:18,410][root][INFO] - Training Epoch: 1/2, step 1255/7134 completed (loss: 0.10855992138385773, acc: 0.966472327709198)
[2025-02-13 02:33:18,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:18,806][root][INFO] - Training Epoch: 1/2, step 1256/7134 completed (loss: 0.09849613904953003, acc: 0.9758842587471008)
[2025-02-13 02:33:18,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:19,233][root][INFO] - Training Epoch: 1/2, step 1257/7134 completed (loss: 0.06379608809947968, acc: 0.9789983630180359)
[2025-02-13 02:33:19,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:19,659][root][INFO] - Training Epoch: 1/2, step 1258/7134 completed (loss: 0.1225210577249527, acc: 0.9648609161376953)
[2025-02-13 02:33:19,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:20,072][root][INFO] - Training Epoch: 1/2, step 1259/7134 completed (loss: 0.09990391880273819, acc: 0.9806867241859436)
[2025-02-13 02:33:20,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:20,503][root][INFO] - Training Epoch: 1/2, step 1260/7134 completed (loss: 0.08466695249080658, acc: 0.9768785834312439)
[2025-02-13 02:33:20,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:20,955][root][INFO] - Training Epoch: 1/2, step 1261/7134 completed (loss: 0.07693175226449966, acc: 0.9792284965515137)
[2025-02-13 02:33:21,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:21,315][root][INFO] - Training Epoch: 1/2, step 1262/7134 completed (loss: 0.018489621579647064, acc: 0.9981684684753418)
[2025-02-13 02:33:21,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:21,715][root][INFO] - Training Epoch: 1/2, step 1263/7134 completed (loss: 0.07047509402036667, acc: 0.9857142567634583)
[2025-02-13 02:33:21,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:22,151][root][INFO] - Training Epoch: 1/2, step 1264/7134 completed (loss: 0.0840713232755661, acc: 0.9844236969947815)
[2025-02-13 02:33:22,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:22,614][root][INFO] - Training Epoch: 1/2, step 1265/7134 completed (loss: 0.08106668293476105, acc: 0.9786967635154724)
[2025-02-13 02:33:22,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:23,046][root][INFO] - Training Epoch: 1/2, step 1266/7134 completed (loss: 0.06732013821601868, acc: 0.979345977306366)
[2025-02-13 02:33:23,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:23,460][root][INFO] - Training Epoch: 1/2, step 1267/7134 completed (loss: 0.08586306869983673, acc: 0.9806896448135376)
[2025-02-13 02:33:23,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:23,895][root][INFO] - Training Epoch: 1/2, step 1268/7134 completed (loss: 0.09052757918834686, acc: 0.9734120965003967)
[2025-02-13 02:33:24,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:24,322][root][INFO] - Training Epoch: 1/2, step 1269/7134 completed (loss: 0.11536660045385361, acc: 0.9689655303955078)
[2025-02-13 02:33:24,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:24,745][root][INFO] - Training Epoch: 1/2, step 1270/7134 completed (loss: 0.064107246696949, acc: 0.9804741740226746)
[2025-02-13 02:33:24,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:25,117][root][INFO] - Training Epoch: 1/2, step 1271/7134 completed (loss: 0.10880377143621445, acc: 0.9740740656852722)
[2025-02-13 02:33:25,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:25,577][root][INFO] - Training Epoch: 1/2, step 1272/7134 completed (loss: 0.053223151713609695, acc: 0.9867899417877197)
[2025-02-13 02:33:25,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:25,990][root][INFO] - Training Epoch: 1/2, step 1273/7134 completed (loss: 0.04676980525255203, acc: 0.9887096881866455)
[2025-02-13 02:33:26,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:26,412][root][INFO] - Training Epoch: 1/2, step 1274/7134 completed (loss: 0.06334507465362549, acc: 0.9864864945411682)
[2025-02-13 02:33:26,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:26,823][root][INFO] - Training Epoch: 1/2, step 1275/7134 completed (loss: 0.08296963572502136, acc: 0.9766606688499451)
[2025-02-13 02:33:26,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:27,228][root][INFO] - Training Epoch: 1/2, step 1276/7134 completed (loss: 0.03950925171375275, acc: 0.9870967864990234)
[2025-02-13 02:33:27,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:27,627][root][INFO] - Training Epoch: 1/2, step 1277/7134 completed (loss: 0.05405675619840622, acc: 0.9854604005813599)
[2025-02-13 02:33:27,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:28,091][root][INFO] - Training Epoch: 1/2, step 1278/7134 completed (loss: 0.09996471554040909, acc: 0.9673405885696411)
[2025-02-13 02:33:28,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:28,522][root][INFO] - Training Epoch: 1/2, step 1279/7134 completed (loss: 0.13404175639152527, acc: 0.9652605652809143)
[2025-02-13 02:33:28,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:28,944][root][INFO] - Training Epoch: 1/2, step 1280/7134 completed (loss: 0.13434316217899323, acc: 0.9724047183990479)
[2025-02-13 02:33:29,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:29,355][root][INFO] - Training Epoch: 1/2, step 1281/7134 completed (loss: 0.1199185773730278, acc: 0.956970751285553)
[2025-02-13 02:33:29,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:29,805][root][INFO] - Training Epoch: 1/2, step 1282/7134 completed (loss: 0.15258318185806274, acc: 0.9639037251472473)
[2025-02-13 02:33:29,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:30,265][root][INFO] - Training Epoch: 1/2, step 1283/7134 completed (loss: 0.13640454411506653, acc: 0.9718309640884399)
[2025-02-13 02:33:30,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:30,705][root][INFO] - Training Epoch: 1/2, step 1284/7134 completed (loss: 0.10366468876600266, acc: 0.971389651298523)
[2025-02-13 02:33:30,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:31,120][root][INFO] - Training Epoch: 1/2, step 1285/7134 completed (loss: 0.09087608009576797, acc: 0.9783549904823303)
[2025-02-13 02:33:31,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:31,575][root][INFO] - Training Epoch: 1/2, step 1286/7134 completed (loss: 0.11224519461393356, acc: 0.9742489457130432)
[2025-02-13 02:33:31,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:31,979][root][INFO] - Training Epoch: 1/2, step 1287/7134 completed (loss: 0.2112448662519455, acc: 0.9485049843788147)
[2025-02-13 02:33:32,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:32,394][root][INFO] - Training Epoch: 1/2, step 1288/7134 completed (loss: 0.09439975023269653, acc: 0.9746192693710327)
[2025-02-13 02:33:32,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:32,805][root][INFO] - Training Epoch: 1/2, step 1289/7134 completed (loss: 0.09655850380659103, acc: 0.9782244563102722)
[2025-02-13 02:33:32,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:33,237][root][INFO] - Training Epoch: 1/2, step 1290/7134 completed (loss: 0.16664721071720123, acc: 0.9591549038887024)
[2025-02-13 02:33:33,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:33,715][root][INFO] - Training Epoch: 1/2, step 1291/7134 completed (loss: 0.10818012058734894, acc: 0.9762901067733765)
[2025-02-13 02:33:33,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:34,147][root][INFO] - Training Epoch: 1/2, step 1292/7134 completed (loss: 0.11885552108287811, acc: 0.9636363387107849)
[2025-02-13 02:33:34,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:34,603][root][INFO] - Training Epoch: 1/2, step 1293/7134 completed (loss: 0.08497601002454758, acc: 0.9697322249412537)
[2025-02-13 02:33:34,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:35,031][root][INFO] - Training Epoch: 1/2, step 1294/7134 completed (loss: 0.09565119445323944, acc: 0.9778933525085449)
[2025-02-13 02:33:35,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:35,478][root][INFO] - Training Epoch: 1/2, step 1295/7134 completed (loss: 0.10240615904331207, acc: 0.9745330810546875)
[2025-02-13 02:33:35,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:35,902][root][INFO] - Training Epoch: 1/2, step 1296/7134 completed (loss: 0.08299387991428375, acc: 0.982300877571106)
[2025-02-13 02:33:36,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:36,278][root][INFO] - Training Epoch: 1/2, step 1297/7134 completed (loss: 0.09904748946428299, acc: 0.9708939790725708)
[2025-02-13 02:33:36,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:36,698][root][INFO] - Training Epoch: 1/2, step 1298/7134 completed (loss: 0.16791847348213196, acc: 0.9658273458480835)
[2025-02-13 02:33:36,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:37,053][root][INFO] - Training Epoch: 1/2, step 1299/7134 completed (loss: 0.0850517749786377, acc: 0.9713541865348816)
[2025-02-13 02:33:37,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:37,517][root][INFO] - Training Epoch: 1/2, step 1300/7134 completed (loss: 0.10236531496047974, acc: 0.9671132564544678)
[2025-02-13 02:33:37,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:37,937][root][INFO] - Training Epoch: 1/2, step 1301/7134 completed (loss: 0.05453462526202202, acc: 0.9866468906402588)
[2025-02-13 02:33:38,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:38,392][root][INFO] - Training Epoch: 1/2, step 1302/7134 completed (loss: 0.0963534265756607, acc: 0.9743303656578064)
[2025-02-13 02:33:38,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:38,856][root][INFO] - Training Epoch: 1/2, step 1303/7134 completed (loss: 0.12636134028434753, acc: 0.9615384340286255)
[2025-02-13 02:33:38,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:39,280][root][INFO] - Training Epoch: 1/2, step 1304/7134 completed (loss: 0.14699673652648926, acc: 0.9644268751144409)
[2025-02-13 02:33:39,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:39,734][root][INFO] - Training Epoch: 1/2, step 1305/7134 completed (loss: 0.10872256755828857, acc: 0.9784263968467712)
[2025-02-13 02:33:39,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:40,211][root][INFO] - Training Epoch: 1/2, step 1306/7134 completed (loss: 0.1498776078224182, acc: 0.9635812044143677)
[2025-02-13 02:33:40,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:40,606][root][INFO] - Training Epoch: 1/2, step 1307/7134 completed (loss: 0.07529011368751526, acc: 0.9826224446296692)
[2025-02-13 02:33:40,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:41,047][root][INFO] - Training Epoch: 1/2, step 1308/7134 completed (loss: 0.0720706582069397, acc: 0.9774436354637146)
[2025-02-13 02:33:41,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:41,507][root][INFO] - Training Epoch: 1/2, step 1309/7134 completed (loss: 0.08777355402708054, acc: 0.9772727489471436)
[2025-02-13 02:33:41,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:41,973][root][INFO] - Training Epoch: 1/2, step 1310/7134 completed (loss: 0.13281041383743286, acc: 0.9646258354187012)
[2025-02-13 02:33:42,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:42,410][root][INFO] - Training Epoch: 1/2, step 1311/7134 completed (loss: 0.15834978222846985, acc: 0.9547511339187622)
[2025-02-13 02:33:42,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:42,867][root][INFO] - Training Epoch: 1/2, step 1312/7134 completed (loss: 0.1504109650850296, acc: 0.9636118412017822)
[2025-02-13 02:33:43,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:43,320][root][INFO] - Training Epoch: 1/2, step 1313/7134 completed (loss: 0.13703514635562897, acc: 0.9560906291007996)
[2025-02-13 02:33:43,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:43,738][root][INFO] - Training Epoch: 1/2, step 1314/7134 completed (loss: 0.09811475872993469, acc: 0.9719298481941223)
[2025-02-13 02:33:43,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:44,164][root][INFO] - Training Epoch: 1/2, step 1315/7134 completed (loss: 0.23889604210853577, acc: 0.9453333616256714)
[2025-02-13 02:33:44,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:44,612][root][INFO] - Training Epoch: 1/2, step 1316/7134 completed (loss: 0.09232211858034134, acc: 0.9811676144599915)
[2025-02-13 02:33:44,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:44,910][root][INFO] - Training Epoch: 1/2, step 1317/7134 completed (loss: 0.17620618641376495, acc: 0.9512194991111755)
[2025-02-13 02:33:45,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:45,352][root][INFO] - Training Epoch: 1/2, step 1318/7134 completed (loss: 0.1630779504776001, acc: 0.9591280817985535)
[2025-02-13 02:33:45,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:45,660][root][INFO] - Training Epoch: 1/2, step 1319/7134 completed (loss: 0.18133951723575592, acc: 0.9450801014900208)
[2025-02-13 02:33:45,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:46,061][root][INFO] - Training Epoch: 1/2, step 1320/7134 completed (loss: 0.20703867077827454, acc: 0.9527687430381775)
[2025-02-13 02:33:46,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:46,717][root][INFO] - Training Epoch: 1/2, step 1321/7134 completed (loss: 0.09940335899591446, acc: 0.968137264251709)
[2025-02-13 02:33:46,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:47,156][root][INFO] - Training Epoch: 1/2, step 1322/7134 completed (loss: 0.11180248111486435, acc: 0.972176730632782)
[2025-02-13 02:33:47,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:47,590][root][INFO] - Training Epoch: 1/2, step 1323/7134 completed (loss: 0.1416015923023224, acc: 0.9571428298950195)
[2025-02-13 02:33:47,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:48,023][root][INFO] - Training Epoch: 1/2, step 1324/7134 completed (loss: 0.08274990320205688, acc: 0.9753289222717285)
[2025-02-13 02:33:48,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:48,353][root][INFO] - Training Epoch: 1/2, step 1325/7134 completed (loss: 0.1132286936044693, acc: 0.9614512324333191)
[2025-02-13 02:33:48,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:48,818][root][INFO] - Training Epoch: 1/2, step 1326/7134 completed (loss: 0.1399780958890915, acc: 0.9604830145835876)
[2025-02-13 02:33:48,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:49,261][root][INFO] - Training Epoch: 1/2, step 1327/7134 completed (loss: 0.12046664208173752, acc: 0.9700149893760681)
[2025-02-13 02:33:49,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:49,699][root][INFO] - Training Epoch: 1/2, step 1328/7134 completed (loss: 0.13682477176189423, acc: 0.9674418568611145)
[2025-02-13 02:33:49,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:50,138][root][INFO] - Training Epoch: 1/2, step 1329/7134 completed (loss: 0.13069036602973938, acc: 0.9640883803367615)
[2025-02-13 02:33:50,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:50,517][root][INFO] - Training Epoch: 1/2, step 1330/7134 completed (loss: 0.08495518565177917, acc: 0.9794167876243591)
[2025-02-13 02:33:50,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:50,971][root][INFO] - Training Epoch: 1/2, step 1331/7134 completed (loss: 0.04592924565076828, acc: 0.9820846915245056)
[2025-02-13 02:33:51,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:51,305][root][INFO] - Training Epoch: 1/2, step 1332/7134 completed (loss: 0.07516098767518997, acc: 0.9803493618965149)
[2025-02-13 02:33:51,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:51,764][root][INFO] - Training Epoch: 1/2, step 1333/7134 completed (loss: 0.12224748730659485, acc: 0.9676870703697205)
[2025-02-13 02:33:51,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:52,213][root][INFO] - Training Epoch: 1/2, step 1334/7134 completed (loss: 0.15663138031959534, acc: 0.9596273303031921)
[2025-02-13 02:33:52,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:52,622][root][INFO] - Training Epoch: 1/2, step 1335/7134 completed (loss: 0.13472184538841248, acc: 0.9733688235282898)
[2025-02-13 02:33:52,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:53,062][root][INFO] - Training Epoch: 1/2, step 1336/7134 completed (loss: 0.0685448870062828, acc: 0.9824086427688599)
[2025-02-13 02:33:53,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:53,524][root][INFO] - Training Epoch: 1/2, step 1337/7134 completed (loss: 0.13155290484428406, acc: 0.9662576913833618)
[2025-02-13 02:33:53,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:53,782][root][INFO] - Training Epoch: 1/2, step 1338/7134 completed (loss: 0.11606166511774063, acc: 0.9598393440246582)
[2025-02-13 02:33:53,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:54,122][root][INFO] - Training Epoch: 1/2, step 1339/7134 completed (loss: 0.1553344577550888, acc: 0.9634831547737122)
[2025-02-13 02:33:54,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:54,534][root][INFO] - Training Epoch: 1/2, step 1340/7134 completed (loss: 0.1111854612827301, acc: 0.9731743931770325)
[2025-02-13 02:33:54,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:54,893][root][INFO] - Training Epoch: 1/2, step 1341/7134 completed (loss: 0.1330728530883789, acc: 0.9689265489578247)
[2025-02-13 02:33:55,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:55,319][root][INFO] - Training Epoch: 1/2, step 1342/7134 completed (loss: 0.06508311629295349, acc: 0.9806763529777527)
[2025-02-13 02:33:55,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:55,769][root][INFO] - Training Epoch: 1/2, step 1343/7134 completed (loss: 0.1115497499704361, acc: 0.971377432346344)
[2025-02-13 02:33:55,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:56,155][root][INFO] - Training Epoch: 1/2, step 1344/7134 completed (loss: 0.07588529586791992, acc: 0.9841628670692444)
[2025-02-13 02:33:56,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:56,425][root][INFO] - Training Epoch: 1/2, step 1345/7134 completed (loss: 0.10319004207849503, acc: 0.9655172228813171)
[2025-02-13 02:33:56,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:56,908][root][INFO] - Training Epoch: 1/2, step 1346/7134 completed (loss: 0.09889517724514008, acc: 0.9786381721496582)
[2025-02-13 02:33:57,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:57,321][root][INFO] - Training Epoch: 1/2, step 1347/7134 completed (loss: 0.09917855262756348, acc: 0.9808362126350403)
[2025-02-13 02:33:57,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:57,757][root][INFO] - Training Epoch: 1/2, step 1348/7134 completed (loss: 0.06902182102203369, acc: 0.9816513657569885)
[2025-02-13 02:33:57,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:58,195][root][INFO] - Training Epoch: 1/2, step 1349/7134 completed (loss: 0.07963472604751587, acc: 0.9743589758872986)
[2025-02-13 02:33:58,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:58,603][root][INFO] - Training Epoch: 1/2, step 1350/7134 completed (loss: 0.051138993352651596, acc: 0.9855282306671143)
[2025-02-13 02:33:58,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:58,996][root][INFO] - Training Epoch: 1/2, step 1351/7134 completed (loss: 0.04874909296631813, acc: 0.990176796913147)
[2025-02-13 02:33:59,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:59,403][root][INFO] - Training Epoch: 1/2, step 1352/7134 completed (loss: 0.09889630228281021, acc: 0.9732142686843872)
[2025-02-13 02:33:59,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:59,813][root][INFO] - Training Epoch: 1/2, step 1353/7134 completed (loss: 0.08952530473470688, acc: 0.977011501789093)
[2025-02-13 02:33:59,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:00,239][root][INFO] - Training Epoch: 1/2, step 1354/7134 completed (loss: 0.048026274889707565, acc: 0.9854133129119873)
[2025-02-13 02:34:00,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:00,641][root][INFO] - Training Epoch: 1/2, step 1355/7134 completed (loss: 0.03148442506790161, acc: 0.9890965819358826)
[2025-02-13 02:34:00,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:01,093][root][INFO] - Training Epoch: 1/2, step 1356/7134 completed (loss: 0.04687460511922836, acc: 0.9868420958518982)
[2025-02-13 02:34:01,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:01,530][root][INFO] - Training Epoch: 1/2, step 1357/7134 completed (loss: 0.021161818876862526, acc: 0.994955837726593)
[2025-02-13 02:34:01,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:01,989][root][INFO] - Training Epoch: 1/2, step 1358/7134 completed (loss: 0.05599473789334297, acc: 0.9818181991577148)
[2025-02-13 02:34:02,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:02,408][root][INFO] - Training Epoch: 1/2, step 1359/7134 completed (loss: 0.04406527057290077, acc: 0.9878542423248291)
[2025-02-13 02:34:02,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:02,832][root][INFO] - Training Epoch: 1/2, step 1360/7134 completed (loss: 0.048188213258981705, acc: 0.9852150678634644)
[2025-02-13 02:34:03,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:03,304][root][INFO] - Training Epoch: 1/2, step 1361/7134 completed (loss: 0.053023725748062134, acc: 0.9883211851119995)
[2025-02-13 02:34:03,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:03,732][root][INFO] - Training Epoch: 1/2, step 1362/7134 completed (loss: 0.0567546971142292, acc: 0.9808743000030518)
[2025-02-13 02:34:03,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:04,173][root][INFO] - Training Epoch: 1/2, step 1363/7134 completed (loss: 0.04065096005797386, acc: 0.9865951538085938)
[2025-02-13 02:34:04,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:04,603][root][INFO] - Training Epoch: 1/2, step 1364/7134 completed (loss: 0.03809265047311783, acc: 0.9900000095367432)
[2025-02-13 02:34:04,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:05,004][root][INFO] - Training Epoch: 1/2, step 1365/7134 completed (loss: 0.039849959313869476, acc: 0.9871794581413269)
[2025-02-13 02:34:05,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:05,448][root][INFO] - Training Epoch: 1/2, step 1366/7134 completed (loss: 0.04540395364165306, acc: 0.9857142567634583)
[2025-02-13 02:34:05,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:05,861][root][INFO] - Training Epoch: 1/2, step 1367/7134 completed (loss: 0.04710985720157623, acc: 0.9879310131072998)
[2025-02-13 02:34:06,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:06,330][root][INFO] - Training Epoch: 1/2, step 1368/7134 completed (loss: 0.08536726236343384, acc: 0.9725610017776489)
[2025-02-13 02:34:06,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:06,746][root][INFO] - Training Epoch: 1/2, step 1369/7134 completed (loss: 0.07362129539251328, acc: 0.985989511013031)
[2025-02-13 02:34:06,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:07,174][root][INFO] - Training Epoch: 1/2, step 1370/7134 completed (loss: 0.07046903669834137, acc: 0.982300877571106)
[2025-02-13 02:34:07,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:07,623][root][INFO] - Training Epoch: 1/2, step 1371/7134 completed (loss: 0.0473385825753212, acc: 0.983988344669342)
[2025-02-13 02:34:07,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:08,039][root][INFO] - Training Epoch: 1/2, step 1372/7134 completed (loss: 0.16152995824813843, acc: 0.9700374603271484)
[2025-02-13 02:34:08,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:08,484][root][INFO] - Training Epoch: 1/2, step 1373/7134 completed (loss: 0.06579618155956268, acc: 0.9803149700164795)
[2025-02-13 02:34:08,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:08,922][root][INFO] - Training Epoch: 1/2, step 1374/7134 completed (loss: 0.0815993994474411, acc: 0.9742574095726013)
[2025-02-13 02:34:09,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:09,296][root][INFO] - Training Epoch: 1/2, step 1375/7134 completed (loss: 0.05687055364251137, acc: 0.9855769276618958)
[2025-02-13 02:34:09,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:09,707][root][INFO] - Training Epoch: 1/2, step 1376/7134 completed (loss: 0.10987012833356857, acc: 0.9603340029716492)
[2025-02-13 02:34:09,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:10,137][root][INFO] - Training Epoch: 1/2, step 1377/7134 completed (loss: 0.0595376193523407, acc: 0.9858823418617249)
[2025-02-13 02:34:10,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:10,499][root][INFO] - Training Epoch: 1/2, step 1378/7134 completed (loss: 0.12998290359973907, acc: 0.961013674736023)
[2025-02-13 02:34:10,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:10,869][root][INFO] - Training Epoch: 1/2, step 1379/7134 completed (loss: 0.09223896265029907, acc: 0.9655913710594177)
[2025-02-13 02:34:11,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:11,292][root][INFO] - Training Epoch: 1/2, step 1380/7134 completed (loss: 0.10630878806114197, acc: 0.9738317728042603)
[2025-02-13 02:34:11,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:11,720][root][INFO] - Training Epoch: 1/2, step 1381/7134 completed (loss: 0.0920681282877922, acc: 0.9751243591308594)
[2025-02-13 02:34:11,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:12,127][root][INFO] - Training Epoch: 1/2, step 1382/7134 completed (loss: 0.10548147559165955, acc: 0.9666666388511658)
[2025-02-13 02:34:12,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:12,562][root][INFO] - Training Epoch: 1/2, step 1383/7134 completed (loss: 0.054002467542886734, acc: 0.9862306118011475)
[2025-02-13 02:34:12,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:12,984][root][INFO] - Training Epoch: 1/2, step 1384/7134 completed (loss: 0.17587809264659882, acc: 0.9517470598220825)
[2025-02-13 02:34:13,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:13,389][root][INFO] - Training Epoch: 1/2, step 1385/7134 completed (loss: 0.14927546679973602, acc: 0.9669724702835083)
[2025-02-13 02:34:13,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:13,819][root][INFO] - Training Epoch: 1/2, step 1386/7134 completed (loss: 0.07090316712856293, acc: 0.9719763994216919)
[2025-02-13 02:34:13,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:14,238][root][INFO] - Training Epoch: 1/2, step 1387/7134 completed (loss: 0.09992638975381851, acc: 0.9742120504379272)
[2025-02-13 02:34:14,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:14,646][root][INFO] - Training Epoch: 1/2, step 1388/7134 completed (loss: 0.08632174134254456, acc: 0.9795570969581604)
[2025-02-13 02:34:14,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:15,064][root][INFO] - Training Epoch: 1/2, step 1389/7134 completed (loss: 0.0847511887550354, acc: 0.9724473357200623)
[2025-02-13 02:34:15,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:15,491][root][INFO] - Training Epoch: 1/2, step 1390/7134 completed (loss: 0.10487788170576096, acc: 0.9719188809394836)
[2025-02-13 02:34:15,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:15,898][root][INFO] - Training Epoch: 1/2, step 1391/7134 completed (loss: 0.10884248465299606, acc: 0.9736379384994507)
[2025-02-13 02:34:16,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:16,320][root][INFO] - Training Epoch: 1/2, step 1392/7134 completed (loss: 0.07889479398727417, acc: 0.9775086641311646)
[2025-02-13 02:34:16,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:16,722][root][INFO] - Training Epoch: 1/2, step 1393/7134 completed (loss: 0.10108254849910736, acc: 0.9677419066429138)
[2025-02-13 02:34:16,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:17,126][root][INFO] - Training Epoch: 1/2, step 1394/7134 completed (loss: 0.06791964173316956, acc: 0.9801587462425232)
[2025-02-13 02:34:17,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:17,499][root][INFO] - Training Epoch: 1/2, step 1395/7134 completed (loss: 0.07661699503660202, acc: 0.9802761077880859)
[2025-02-13 02:34:17,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:17,899][root][INFO] - Training Epoch: 1/2, step 1396/7134 completed (loss: 0.10783746093511581, acc: 0.962837815284729)
[2025-02-13 02:34:18,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:18,307][root][INFO] - Training Epoch: 1/2, step 1397/7134 completed (loss: 0.13319866359233856, acc: 0.9679595232009888)
[2025-02-13 02:34:18,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:18,700][root][INFO] - Training Epoch: 1/2, step 1398/7134 completed (loss: 0.09305298328399658, acc: 0.9736308455467224)
[2025-02-13 02:34:18,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:19,119][root][INFO] - Training Epoch: 1/2, step 1399/7134 completed (loss: 0.15231621265411377, acc: 0.9692028760910034)
[2025-02-13 02:34:19,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:19,548][root][INFO] - Training Epoch: 1/2, step 1400/7134 completed (loss: 0.06655026227235794, acc: 0.9825479984283447)
[2025-02-13 02:34:19,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:19,971][root][INFO] - Training Epoch: 1/2, step 1401/7134 completed (loss: 0.06564794480800629, acc: 0.9818781018257141)
[2025-02-13 02:34:20,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:20,369][root][INFO] - Training Epoch: 1/2, step 1402/7134 completed (loss: 0.04046947881579399, acc: 0.9948186278343201)
[2025-02-13 02:34:20,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:20,806][root][INFO] - Training Epoch: 1/2, step 1403/7134 completed (loss: 0.07182014733552933, acc: 0.9799196720123291)
[2025-02-13 02:34:20,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:21,256][root][INFO] - Training Epoch: 1/2, step 1404/7134 completed (loss: 0.041431352496147156, acc: 0.9893898963928223)
[2025-02-13 02:34:21,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:21,674][root][INFO] - Training Epoch: 1/2, step 1405/7134 completed (loss: 0.07402192056179047, acc: 0.9800570011138916)
[2025-02-13 02:34:21,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:22,119][root][INFO] - Training Epoch: 1/2, step 1406/7134 completed (loss: 0.023317400366067886, acc: 0.9942528605461121)
[2025-02-13 02:34:22,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:22,543][root][INFO] - Training Epoch: 1/2, step 1407/7134 completed (loss: 0.03916383907198906, acc: 0.9882659912109375)
[2025-02-13 02:34:22,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:22,972][root][INFO] - Training Epoch: 1/2, step 1408/7134 completed (loss: 0.06622745096683502, acc: 0.9775640964508057)
[2025-02-13 02:34:23,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:23,421][root][INFO] - Training Epoch: 1/2, step 1409/7134 completed (loss: 0.03575119003653526, acc: 0.9880775213241577)
[2025-02-13 02:34:23,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:23,866][root][INFO] - Training Epoch: 1/2, step 1410/7134 completed (loss: 0.06036399304866791, acc: 0.9869791865348816)
[2025-02-13 02:34:24,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:24,301][root][INFO] - Training Epoch: 1/2, step 1411/7134 completed (loss: 0.026943784207105637, acc: 0.9893617033958435)
[2025-02-13 02:34:24,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:24,764][root][INFO] - Training Epoch: 1/2, step 1412/7134 completed (loss: 0.05000007897615433, acc: 0.9881235361099243)
[2025-02-13 02:34:24,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:25,209][root][INFO] - Training Epoch: 1/2, step 1413/7134 completed (loss: 0.05008146911859512, acc: 0.9870503544807434)
[2025-02-13 02:34:25,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:25,624][root][INFO] - Training Epoch: 1/2, step 1414/7134 completed (loss: 0.04578826203942299, acc: 0.9875389337539673)
[2025-02-13 02:34:25,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:26,047][root][INFO] - Training Epoch: 1/2, step 1415/7134 completed (loss: 0.10917085409164429, acc: 0.9735099077224731)
[2025-02-13 02:34:26,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:26,508][root][INFO] - Training Epoch: 1/2, step 1416/7134 completed (loss: 0.04520450159907341, acc: 0.9887640476226807)
[2025-02-13 02:34:26,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:26,914][root][INFO] - Training Epoch: 1/2, step 1417/7134 completed (loss: 0.03605664148926735, acc: 0.9914966225624084)
[2025-02-13 02:34:27,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:27,293][root][INFO] - Training Epoch: 1/2, step 1418/7134 completed (loss: 0.025346342474222183, acc: 0.9923664331436157)
[2025-02-13 02:34:27,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:27,749][root][INFO] - Training Epoch: 1/2, step 1419/7134 completed (loss: 0.03911126032471657, acc: 0.9905533194541931)
[2025-02-13 02:34:27,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:28,180][root][INFO] - Training Epoch: 1/2, step 1420/7134 completed (loss: 0.04299630597233772, acc: 0.9892183542251587)
[2025-02-13 02:34:28,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:28,610][root][INFO] - Training Epoch: 1/2, step 1421/7134 completed (loss: 0.04711167514324188, acc: 0.9839743375778198)
[2025-02-13 02:34:28,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:29,040][root][INFO] - Training Epoch: 1/2, step 1422/7134 completed (loss: 0.12089785188436508, acc: 0.9672364592552185)
[2025-02-13 02:34:29,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:29,440][root][INFO] - Training Epoch: 1/2, step 1423/7134 completed (loss: 0.11079372465610504, acc: 0.9755101799964905)
[2025-02-13 02:34:29,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:29,861][root][INFO] - Training Epoch: 1/2, step 1424/7134 completed (loss: 0.14172418415546417, acc: 0.9524714946746826)
[2025-02-13 02:34:30,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:30,289][root][INFO] - Training Epoch: 1/2, step 1425/7134 completed (loss: 0.054876916110515594, acc: 0.9821162223815918)
[2025-02-13 02:34:30,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:30,752][root][INFO] - Training Epoch: 1/2, step 1426/7134 completed (loss: 0.08256598562002182, acc: 0.9768637418746948)
[2025-02-13 02:34:30,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:31,212][root][INFO] - Training Epoch: 1/2, step 1427/7134 completed (loss: 0.08910579979419708, acc: 0.9719029664993286)
[2025-02-13 02:34:31,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:31,623][root][INFO] - Training Epoch: 1/2, step 1428/7134 completed (loss: 0.36748814582824707, acc: 0.9123867154121399)
[2025-02-13 02:34:31,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:32,065][root][INFO] - Training Epoch: 1/2, step 1429/7134 completed (loss: 0.05083182081580162, acc: 0.9854469895362854)
[2025-02-13 02:34:32,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:32,523][root][INFO] - Training Epoch: 1/2, step 1430/7134 completed (loss: 0.11074615269899368, acc: 0.9686346650123596)
[2025-02-13 02:34:32,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:32,952][root][INFO] - Training Epoch: 1/2, step 1431/7134 completed (loss: 0.08591097593307495, acc: 0.9701492786407471)
[2025-02-13 02:34:33,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:33,402][root][INFO] - Training Epoch: 1/2, step 1432/7134 completed (loss: 0.08638685941696167, acc: 0.9768339991569519)
[2025-02-13 02:34:33,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:33,849][root][INFO] - Training Epoch: 1/2, step 1433/7134 completed (loss: 0.11205462366342545, acc: 0.9659686088562012)
[2025-02-13 02:34:33,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:34,278][root][INFO] - Training Epoch: 1/2, step 1434/7134 completed (loss: 0.11493043601512909, acc: 0.9651941061019897)
[2025-02-13 02:34:34,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:34,734][root][INFO] - Training Epoch: 1/2, step 1435/7134 completed (loss: 0.10876839607954025, acc: 0.9662756323814392)
[2025-02-13 02:34:34,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:35,176][root][INFO] - Training Epoch: 1/2, step 1436/7134 completed (loss: 0.14004144072532654, acc: 0.9619289636611938)
[2025-02-13 02:34:35,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:35,597][root][INFO] - Training Epoch: 1/2, step 1437/7134 completed (loss: 0.09716420620679855, acc: 0.976323127746582)
[2025-02-13 02:34:35,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:36,051][root][INFO] - Training Epoch: 1/2, step 1438/7134 completed (loss: 0.14113947749137878, acc: 0.9693430662155151)
[2025-02-13 02:34:36,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:36,505][root][INFO] - Training Epoch: 1/2, step 1439/7134 completed (loss: 0.12015590071678162, acc: 0.9684600830078125)
[2025-02-13 02:34:36,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:36,915][root][INFO] - Training Epoch: 1/2, step 1440/7134 completed (loss: 0.18042075634002686, acc: 0.9535284042358398)
[2025-02-13 02:34:37,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:37,359][root][INFO] - Training Epoch: 1/2, step 1441/7134 completed (loss: 0.08214861899614334, acc: 0.9794721603393555)
[2025-02-13 02:34:37,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:37,711][root][INFO] - Training Epoch: 1/2, step 1442/7134 completed (loss: 0.06758284568786621, acc: 0.980861246585846)
[2025-02-13 02:34:37,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:38,136][root][INFO] - Training Epoch: 1/2, step 1443/7134 completed (loss: 0.12915821373462677, acc: 0.9630931615829468)
[2025-02-13 02:34:38,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:38,558][root][INFO] - Training Epoch: 1/2, step 1444/7134 completed (loss: 0.07894086092710495, acc: 0.9727626442909241)
[2025-02-13 02:34:38,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:39,005][root][INFO] - Training Epoch: 1/2, step 1445/7134 completed (loss: 0.11522084474563599, acc: 0.9667221307754517)
[2025-02-13 02:34:39,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:39,426][root][INFO] - Training Epoch: 1/2, step 1446/7134 completed (loss: 0.08077318966388702, acc: 0.9729729890823364)
[2025-02-13 02:34:39,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:39,849][root][INFO] - Training Epoch: 1/2, step 1447/7134 completed (loss: 0.09851997345685959, acc: 0.9659574627876282)
[2025-02-13 02:34:39,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:40,229][root][INFO] - Training Epoch: 1/2, step 1448/7134 completed (loss: 0.14649857580661774, acc: 0.9512711763381958)
[2025-02-13 02:34:40,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:40,665][root][INFO] - Training Epoch: 1/2, step 1449/7134 completed (loss: 0.1366056203842163, acc: 0.966292142868042)
[2025-02-13 02:34:40,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:41,066][root][INFO] - Training Epoch: 1/2, step 1450/7134 completed (loss: 0.07858465611934662, acc: 0.9782293438911438)
[2025-02-13 02:34:41,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:41,476][root][INFO] - Training Epoch: 1/2, step 1451/7134 completed (loss: 0.07226718217134476, acc: 0.9751098155975342)
[2025-02-13 02:34:41,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:41,874][root][INFO] - Training Epoch: 1/2, step 1452/7134 completed (loss: 0.10832001268863678, acc: 0.9791666865348816)
[2025-02-13 02:34:42,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:42,277][root][INFO] - Training Epoch: 1/2, step 1453/7134 completed (loss: 0.11565843969583511, acc: 0.9683042764663696)
[2025-02-13 02:34:42,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:42,728][root][INFO] - Training Epoch: 1/2, step 1454/7134 completed (loss: 0.13583411276340485, acc: 0.9636608362197876)
[2025-02-13 02:34:42,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:43,132][root][INFO] - Training Epoch: 1/2, step 1455/7134 completed (loss: 0.11102032661437988, acc: 0.9706422090530396)
[2025-02-13 02:34:43,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:43,540][root][INFO] - Training Epoch: 1/2, step 1456/7134 completed (loss: 0.06516686826944351, acc: 0.9868995547294617)
[2025-02-13 02:34:43,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:43,947][root][INFO] - Training Epoch: 1/2, step 1457/7134 completed (loss: 0.06132437661290169, acc: 0.9813953638076782)
[2025-02-13 02:34:44,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:44,366][root][INFO] - Training Epoch: 1/2, step 1458/7134 completed (loss: 0.0624658465385437, acc: 0.9852724671363831)
[2025-02-13 02:34:44,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:44,774][root][INFO] - Training Epoch: 1/2, step 1459/7134 completed (loss: 0.0743027850985527, acc: 0.980461835861206)
[2025-02-13 02:34:44,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:45,210][root][INFO] - Training Epoch: 1/2, step 1460/7134 completed (loss: 0.13018471002578735, acc: 0.9684210419654846)
[2025-02-13 02:34:45,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:45,615][root][INFO] - Training Epoch: 1/2, step 1461/7134 completed (loss: 0.09592199325561523, acc: 0.9712918400764465)
[2025-02-13 02:34:45,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:46,048][root][INFO] - Training Epoch: 1/2, step 1462/7134 completed (loss: 0.15980833768844604, acc: 0.9583333134651184)
[2025-02-13 02:34:46,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:46,476][root][INFO] - Training Epoch: 1/2, step 1463/7134 completed (loss: 0.1461898237466812, acc: 0.9581589698791504)
[2025-02-13 02:34:46,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:46,916][root][INFO] - Training Epoch: 1/2, step 1464/7134 completed (loss: 0.18659113347530365, acc: 0.954954981803894)
[2025-02-13 02:34:47,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:47,346][root][INFO] - Training Epoch: 1/2, step 1465/7134 completed (loss: 0.10152346640825272, acc: 0.9780564308166504)
[2025-02-13 02:34:47,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:47,782][root][INFO] - Training Epoch: 1/2, step 1466/7134 completed (loss: 0.05668921023607254, acc: 0.9886685609817505)
[2025-02-13 02:34:47,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:48,213][root][INFO] - Training Epoch: 1/2, step 1467/7134 completed (loss: 0.046869952231645584, acc: 0.9870316982269287)
[2025-02-13 02:34:48,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:48,658][root][INFO] - Training Epoch: 1/2, step 1468/7134 completed (loss: 0.06803742796182632, acc: 0.9775429368019104)
[2025-02-13 02:34:48,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:49,090][root][INFO] - Training Epoch: 1/2, step 1469/7134 completed (loss: 0.043304529041051865, acc: 0.9892328381538391)
[2025-02-13 02:34:49,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:49,499][root][INFO] - Training Epoch: 1/2, step 1470/7134 completed (loss: 0.09087593108415604, acc: 0.9682080745697021)
[2025-02-13 02:34:49,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:49,915][root][INFO] - Training Epoch: 1/2, step 1471/7134 completed (loss: 0.11281362920999527, acc: 0.9698046445846558)
[2025-02-13 02:34:50,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:50,360][root][INFO] - Training Epoch: 1/2, step 1472/7134 completed (loss: 0.055806100368499756, acc: 0.9861634969711304)
[2025-02-13 02:34:50,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:50,784][root][INFO] - Training Epoch: 1/2, step 1473/7134 completed (loss: 0.08799475431442261, acc: 0.9762611389160156)
[2025-02-13 02:34:50,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:51,189][root][INFO] - Training Epoch: 1/2, step 1474/7134 completed (loss: 0.11824335157871246, acc: 0.968120813369751)
[2025-02-13 02:34:51,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:51,627][root][INFO] - Training Epoch: 1/2, step 1475/7134 completed (loss: 0.12605337798595428, acc: 0.9709762334823608)
[2025-02-13 02:34:51,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:52,056][root][INFO] - Training Epoch: 1/2, step 1476/7134 completed (loss: 0.08286212384700775, acc: 0.9736024737358093)
[2025-02-13 02:34:52,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:52,462][root][INFO] - Training Epoch: 1/2, step 1477/7134 completed (loss: 0.08951842039823532, acc: 0.9720767736434937)
[2025-02-13 02:34:52,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:52,900][root][INFO] - Training Epoch: 1/2, step 1478/7134 completed (loss: 0.06364431232213974, acc: 0.9824817776679993)
[2025-02-13 02:34:53,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:53,330][root][INFO] - Training Epoch: 1/2, step 1479/7134 completed (loss: 0.09956087172031403, acc: 0.9741379022598267)
[2025-02-13 02:34:53,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:53,766][root][INFO] - Training Epoch: 1/2, step 1480/7134 completed (loss: 0.046242740005254745, acc: 0.984415590763092)
[2025-02-13 02:34:53,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:54,224][root][INFO] - Training Epoch: 1/2, step 1481/7134 completed (loss: 0.06031445413827896, acc: 0.9894737005233765)
[2025-02-13 02:34:54,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:54,671][root][INFO] - Training Epoch: 1/2, step 1482/7134 completed (loss: 0.07526577264070511, acc: 0.9815340638160706)
[2025-02-13 02:34:54,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:55,095][root][INFO] - Training Epoch: 1/2, step 1483/7134 completed (loss: 0.04721022769808769, acc: 0.9842022061347961)
[2025-02-13 02:34:55,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:55,530][root][INFO] - Training Epoch: 1/2, step 1484/7134 completed (loss: 0.10552909970283508, acc: 0.9725433588027954)
[2025-02-13 02:34:55,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:55,955][root][INFO] - Training Epoch: 1/2, step 1485/7134 completed (loss: 0.061524152755737305, acc: 0.984829306602478)
[2025-02-13 02:34:56,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:56,373][root][INFO] - Training Epoch: 1/2, step 1486/7134 completed (loss: 0.07181882113218307, acc: 0.9837618470191956)
[2025-02-13 02:34:56,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:56,814][root][INFO] - Training Epoch: 1/2, step 1487/7134 completed (loss: 0.06068607047200203, acc: 0.9790849685668945)
[2025-02-13 02:34:56,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:57,287][root][INFO] - Training Epoch: 1/2, step 1488/7134 completed (loss: 0.08437664806842804, acc: 0.9743315577507019)
[2025-02-13 02:34:57,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:57,720][root][INFO] - Training Epoch: 1/2, step 1489/7134 completed (loss: 0.09106642007827759, acc: 0.9766355156898499)
[2025-02-13 02:34:57,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:58,189][root][INFO] - Training Epoch: 1/2, step 1490/7134 completed (loss: 0.05212206020951271, acc: 0.9894179701805115)
[2025-02-13 02:34:58,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:58,631][root][INFO] - Training Epoch: 1/2, step 1491/7134 completed (loss: 0.05125166475772858, acc: 0.9885057210922241)
[2025-02-13 02:34:58,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:59,071][root][INFO] - Training Epoch: 1/2, step 1492/7134 completed (loss: 0.08509264886379242, acc: 0.9802784323692322)
[2025-02-13 02:34:59,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:59,515][root][INFO] - Training Epoch: 1/2, step 1493/7134 completed (loss: 0.08151793479919434, acc: 0.975806474685669)
[2025-02-13 02:34:59,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:59,977][root][INFO] - Training Epoch: 1/2, step 1494/7134 completed (loss: 0.05705331638455391, acc: 0.985401451587677)
[2025-02-13 02:35:00,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:00,419][root][INFO] - Training Epoch: 1/2, step 1495/7134 completed (loss: 0.05743769183754921, acc: 0.9844311475753784)
[2025-02-13 02:35:00,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:00,867][root][INFO] - Training Epoch: 1/2, step 1496/7134 completed (loss: 0.086618572473526, acc: 0.9786324501037598)
[2025-02-13 02:35:01,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:01,331][root][INFO] - Training Epoch: 1/2, step 1497/7134 completed (loss: 0.05066433548927307, acc: 0.9852941036224365)
[2025-02-13 02:35:01,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:01,731][root][INFO] - Training Epoch: 1/2, step 1498/7134 completed (loss: 0.0519879050552845, acc: 0.9894598126411438)
[2025-02-13 02:35:01,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:02,203][root][INFO] - Training Epoch: 1/2, step 1499/7134 completed (loss: 0.04665766656398773, acc: 0.9869513511657715)
[2025-02-13 02:35:02,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:02,585][root][INFO] - Training Epoch: 1/2, step 1500/7134 completed (loss: 0.0657409057021141, acc: 0.9817880988121033)
[2025-02-13 02:35:02,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:03,040][root][INFO] - Training Epoch: 1/2, step 1501/7134 completed (loss: 0.0771222785115242, acc: 0.9793510437011719)
[2025-02-13 02:35:03,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:03,492][root][INFO] - Training Epoch: 1/2, step 1502/7134 completed (loss: 0.06564945727586746, acc: 0.9864712357521057)
[2025-02-13 02:35:03,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:03,892][root][INFO] - Training Epoch: 1/2, step 1503/7134 completed (loss: 0.11217796802520752, acc: 0.9744318127632141)
[2025-02-13 02:35:04,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:04,310][root][INFO] - Training Epoch: 1/2, step 1504/7134 completed (loss: 0.039833154529333115, acc: 0.9899371266365051)
[2025-02-13 02:35:04,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:04,724][root][INFO] - Training Epoch: 1/2, step 1505/7134 completed (loss: 0.05800265446305275, acc: 0.9781931638717651)
[2025-02-13 02:35:04,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:05,170][root][INFO] - Training Epoch: 1/2, step 1506/7134 completed (loss: 0.1260114461183548, acc: 0.9680851101875305)
[2025-02-13 02:35:05,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:05,592][root][INFO] - Training Epoch: 1/2, step 1507/7134 completed (loss: 0.050639547407627106, acc: 0.9869109988212585)
[2025-02-13 02:35:05,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:06,058][root][INFO] - Training Epoch: 1/2, step 1508/7134 completed (loss: 0.05041135847568512, acc: 0.9824945330619812)
[2025-02-13 02:35:06,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:06,500][root][INFO] - Training Epoch: 1/2, step 1509/7134 completed (loss: 0.036018241196870804, acc: 0.9887955188751221)
[2025-02-13 02:35:06,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:06,955][root][INFO] - Training Epoch: 1/2, step 1510/7134 completed (loss: 0.28791284561157227, acc: 0.9599999785423279)
[2025-02-13 02:35:07,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:07,393][root][INFO] - Training Epoch: 1/2, step 1511/7134 completed (loss: 0.1455758661031723, acc: 0.9602836966514587)
[2025-02-13 02:35:07,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:07,839][root][INFO] - Training Epoch: 1/2, step 1512/7134 completed (loss: 0.07007680088281631, acc: 0.9817351698875427)
[2025-02-13 02:35:07,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:08,270][root][INFO] - Training Epoch: 1/2, step 1513/7134 completed (loss: 0.03846703842282295, acc: 0.9907407164573669)
[2025-02-13 02:35:08,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:08,732][root][INFO] - Training Epoch: 1/2, step 1514/7134 completed (loss: 0.07855076342821121, acc: 0.9813664555549622)
[2025-02-13 02:35:08,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:09,183][root][INFO] - Training Epoch: 1/2, step 1515/7134 completed (loss: 0.12348451465368271, acc: 0.9649532437324524)
[2025-02-13 02:35:09,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:09,463][root][INFO] - Training Epoch: 1/2, step 1516/7134 completed (loss: 0.12681537866592407, acc: 0.9601139426231384)
[2025-02-13 02:35:09,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:09,800][root][INFO] - Training Epoch: 1/2, step 1517/7134 completed (loss: 0.12098609656095505, acc: 0.9600840210914612)
[2025-02-13 02:35:09,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:10,134][root][INFO] - Training Epoch: 1/2, step 1518/7134 completed (loss: 0.08412528038024902, acc: 0.9759036302566528)
[2025-02-13 02:35:10,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:10,544][root][INFO] - Training Epoch: 1/2, step 1519/7134 completed (loss: 0.07770945876836777, acc: 0.9746543765068054)
[2025-02-13 02:35:10,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:11,000][root][INFO] - Training Epoch: 1/2, step 1520/7134 completed (loss: 0.1030198186635971, acc: 0.9734789133071899)
[2025-02-13 02:35:11,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:11,492][root][INFO] - Training Epoch: 1/2, step 1521/7134 completed (loss: 0.1759728044271469, acc: 0.9441340565681458)
[2025-02-13 02:35:11,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:11,905][root][INFO] - Training Epoch: 1/2, step 1522/7134 completed (loss: 0.20671895146369934, acc: 0.9460501074790955)
[2025-02-13 02:35:12,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:12,321][root][INFO] - Training Epoch: 1/2, step 1523/7134 completed (loss: 0.2157672792673111, acc: 0.9355322122573853)
[2025-02-13 02:35:12,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:12,649][root][INFO] - Training Epoch: 1/2, step 1524/7134 completed (loss: 0.0776425153017044, acc: 0.9798657894134521)
[2025-02-13 02:35:12,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:13,086][root][INFO] - Training Epoch: 1/2, step 1525/7134 completed (loss: 0.10364031046628952, acc: 0.9683257937431335)
[2025-02-13 02:35:13,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:13,561][root][INFO] - Training Epoch: 1/2, step 1526/7134 completed (loss: 0.14434830844402313, acc: 0.9535558819770813)
[2025-02-13 02:35:13,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:14,050][root][INFO] - Training Epoch: 1/2, step 1527/7134 completed (loss: 0.22358085215091705, acc: 0.9365609288215637)
[2025-02-13 02:35:14,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:14,541][root][INFO] - Training Epoch: 1/2, step 1528/7134 completed (loss: 0.16841638088226318, acc: 0.9533011317253113)
[2025-02-13 02:35:14,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:15,004][root][INFO] - Training Epoch: 1/2, step 1529/7134 completed (loss: 0.11149933934211731, acc: 0.9619500637054443)
[2025-02-13 02:35:15,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:15,422][root][INFO] - Training Epoch: 1/2, step 1530/7134 completed (loss: 0.10390729457139969, acc: 0.971319317817688)
[2025-02-13 02:35:15,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:15,834][root][INFO] - Training Epoch: 1/2, step 1531/7134 completed (loss: 0.2567809224128723, acc: 0.9414893388748169)
[2025-02-13 02:35:15,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:16,315][root][INFO] - Training Epoch: 1/2, step 1532/7134 completed (loss: 0.06884635239839554, acc: 0.9768785834312439)
[2025-02-13 02:35:16,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:16,751][root][INFO] - Training Epoch: 1/2, step 1533/7134 completed (loss: 0.08970478177070618, acc: 0.9737206101417542)
[2025-02-13 02:35:16,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:17,201][root][INFO] - Training Epoch: 1/2, step 1534/7134 completed (loss: 0.08006501197814941, acc: 0.9764543175697327)
[2025-02-13 02:35:17,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:17,700][root][INFO] - Training Epoch: 1/2, step 1535/7134 completed (loss: 0.07971484959125519, acc: 0.9748502969741821)
[2025-02-13 02:35:17,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:18,145][root][INFO] - Training Epoch: 1/2, step 1536/7134 completed (loss: 0.053324636071920395, acc: 0.9873577952384949)
[2025-02-13 02:35:18,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:18,552][root][INFO] - Training Epoch: 1/2, step 1537/7134 completed (loss: 0.10156793892383575, acc: 0.9738219976425171)
[2025-02-13 02:35:18,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:18,979][root][INFO] - Training Epoch: 1/2, step 1538/7134 completed (loss: 0.10218892991542816, acc: 0.9782330393791199)
[2025-02-13 02:35:19,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:19,421][root][INFO] - Training Epoch: 1/2, step 1539/7134 completed (loss: 0.08402568101882935, acc: 0.9749670624732971)
[2025-02-13 02:35:19,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:19,866][root][INFO] - Training Epoch: 1/2, step 1540/7134 completed (loss: 0.06885416805744171, acc: 0.9772117733955383)
[2025-02-13 02:35:19,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:20,310][root][INFO] - Training Epoch: 1/2, step 1541/7134 completed (loss: 0.10907808691263199, acc: 0.9723270535469055)
[2025-02-13 02:35:20,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:20,790][root][INFO] - Training Epoch: 1/2, step 1542/7134 completed (loss: 0.06362289935350418, acc: 0.9743935465812683)
[2025-02-13 02:35:20,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:21,243][root][INFO] - Training Epoch: 1/2, step 1543/7134 completed (loss: 0.053689248859882355, acc: 0.9836065769195557)
[2025-02-13 02:35:21,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:21,699][root][INFO] - Training Epoch: 1/2, step 1544/7134 completed (loss: 0.05224664509296417, acc: 0.987864077091217)
[2025-02-13 02:35:21,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:22,101][root][INFO] - Training Epoch: 1/2, step 1545/7134 completed (loss: 0.056570347398519516, acc: 0.9856528043746948)
[2025-02-13 02:35:22,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:22,568][root][INFO] - Training Epoch: 1/2, step 1546/7134 completed (loss: 0.1142774373292923, acc: 0.9719495177268982)
[2025-02-13 02:35:22,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:23,003][root][INFO] - Training Epoch: 1/2, step 1547/7134 completed (loss: 0.0751791000366211, acc: 0.9776119589805603)
[2025-02-13 02:35:23,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:23,466][root][INFO] - Training Epoch: 1/2, step 1548/7134 completed (loss: 0.09128591418266296, acc: 0.9761092066764832)
[2025-02-13 02:35:23,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:23,930][root][INFO] - Training Epoch: 1/2, step 1549/7134 completed (loss: 0.057419683784246445, acc: 0.9811738729476929)
[2025-02-13 02:35:24,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:24,385][root][INFO] - Training Epoch: 1/2, step 1550/7134 completed (loss: 0.05650980770587921, acc: 0.9822559952735901)
[2025-02-13 02:35:24,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:24,807][root][INFO] - Training Epoch: 1/2, step 1551/7134 completed (loss: 0.07476866245269775, acc: 0.9753246903419495)
[2025-02-13 02:35:24,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:25,278][root][INFO] - Training Epoch: 1/2, step 1552/7134 completed (loss: 0.07031513005495071, acc: 0.9795258641242981)
[2025-02-13 02:35:25,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:25,691][root][INFO] - Training Epoch: 1/2, step 1553/7134 completed (loss: 0.052741825580596924, acc: 0.9830949306488037)
[2025-02-13 02:35:25,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:26,162][root][INFO] - Training Epoch: 1/2, step 1554/7134 completed (loss: 0.07713449746370316, acc: 0.9650507569313049)
[2025-02-13 02:35:26,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:26,610][root][INFO] - Training Epoch: 1/2, step 1555/7134 completed (loss: 0.06639819592237473, acc: 0.9820828437805176)
[2025-02-13 02:35:26,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:27,063][root][INFO] - Training Epoch: 1/2, step 1556/7134 completed (loss: 0.07044993340969086, acc: 0.9823736548423767)
[2025-02-13 02:35:27,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:27,528][root][INFO] - Training Epoch: 1/2, step 1557/7134 completed (loss: 0.07405383884906769, acc: 0.9742729067802429)
[2025-02-13 02:35:27,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:27,945][root][INFO] - Training Epoch: 1/2, step 1558/7134 completed (loss: 0.07876028120517731, acc: 0.9774965047836304)
[2025-02-13 02:35:28,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:28,378][root][INFO] - Training Epoch: 1/2, step 1559/7134 completed (loss: 0.06058323010802269, acc: 0.9799196720123291)
[2025-02-13 02:35:28,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:28,829][root][INFO] - Training Epoch: 1/2, step 1560/7134 completed (loss: 0.12777939438819885, acc: 0.963151216506958)
[2025-02-13 02:35:28,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:29,285][root][INFO] - Training Epoch: 1/2, step 1561/7134 completed (loss: 0.1455748826265335, acc: 0.9631636142730713)
[2025-02-13 02:35:29,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:29,712][root][INFO] - Training Epoch: 1/2, step 1562/7134 completed (loss: 0.08529495447874069, acc: 0.9773070812225342)
[2025-02-13 02:35:29,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:30,147][root][INFO] - Training Epoch: 1/2, step 1563/7134 completed (loss: 0.09134548157453537, acc: 0.973714292049408)
[2025-02-13 02:35:30,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:30,591][root][INFO] - Training Epoch: 1/2, step 1564/7134 completed (loss: 0.13637788593769073, acc: 0.9641088843345642)
[2025-02-13 02:35:30,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:30,986][root][INFO] - Training Epoch: 1/2, step 1565/7134 completed (loss: 0.09576886892318726, acc: 0.9680672287940979)
[2025-02-13 02:35:31,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:31,403][root][INFO] - Training Epoch: 1/2, step 1566/7134 completed (loss: 0.09488651901483536, acc: 0.9726402163505554)
[2025-02-13 02:35:31,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:31,896][root][INFO] - Training Epoch: 1/2, step 1567/7134 completed (loss: 0.08878348022699356, acc: 0.9763092398643494)
[2025-02-13 02:35:32,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:32,350][root][INFO] - Training Epoch: 1/2, step 1568/7134 completed (loss: 0.0783449187874794, acc: 0.9857988357543945)
[2025-02-13 02:35:32,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:32,796][root][INFO] - Training Epoch: 1/2, step 1569/7134 completed (loss: 0.10667870193719864, acc: 0.9663742780685425)
[2025-02-13 02:35:32,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:33,268][root][INFO] - Training Epoch: 1/2, step 1570/7134 completed (loss: 0.11315619945526123, acc: 0.9713340401649475)
[2025-02-13 02:35:33,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:33,692][root][INFO] - Training Epoch: 1/2, step 1571/7134 completed (loss: 0.10763447731733322, acc: 0.9679715037345886)
[2025-02-13 02:35:33,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:34,160][root][INFO] - Training Epoch: 1/2, step 1572/7134 completed (loss: 0.08246856182813644, acc: 0.971731424331665)
[2025-02-13 02:35:34,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:34,576][root][INFO] - Training Epoch: 1/2, step 1573/7134 completed (loss: 0.06340835243463516, acc: 0.9840348362922668)
[2025-02-13 02:35:34,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:35,041][root][INFO] - Training Epoch: 1/2, step 1574/7134 completed (loss: 0.10670159757137299, acc: 0.9682713150978088)
[2025-02-13 02:35:35,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:35,478][root][INFO] - Training Epoch: 1/2, step 1575/7134 completed (loss: 0.09147007018327713, acc: 0.9767184257507324)
[2025-02-13 02:35:35,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:35,922][root][INFO] - Training Epoch: 1/2, step 1576/7134 completed (loss: 0.09213052690029144, acc: 0.9742990732192993)
[2025-02-13 02:35:36,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:36,376][root][INFO] - Training Epoch: 1/2, step 1577/7134 completed (loss: 0.08591719716787338, acc: 0.9799764156341553)
[2025-02-13 02:35:36,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:36,818][root][INFO] - Training Epoch: 1/2, step 1578/7134 completed (loss: 0.09126674383878708, acc: 0.97428959608078)
[2025-02-13 02:35:36,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:37,295][root][INFO] - Training Epoch: 1/2, step 1579/7134 completed (loss: 0.08621030300855637, acc: 0.9769585132598877)
[2025-02-13 02:35:37,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:37,752][root][INFO] - Training Epoch: 1/2, step 1580/7134 completed (loss: 0.12093289196491241, acc: 0.9669603705406189)
[2025-02-13 02:35:37,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:38,111][root][INFO] - Training Epoch: 1/2, step 1581/7134 completed (loss: 0.17463473975658417, acc: 0.9537037014961243)
[2025-02-13 02:35:38,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:38,554][root][INFO] - Training Epoch: 1/2, step 1582/7134 completed (loss: 0.1601848602294922, acc: 0.949921727180481)
[2025-02-13 02:35:38,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:38,973][root][INFO] - Training Epoch: 1/2, step 1583/7134 completed (loss: 0.08949844539165497, acc: 0.9713466763496399)
[2025-02-13 02:35:39,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:39,456][root][INFO] - Training Epoch: 1/2, step 1584/7134 completed (loss: 0.08802356570959091, acc: 0.9732334017753601)
[2025-02-13 02:35:39,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:39,900][root][INFO] - Training Epoch: 1/2, step 1585/7134 completed (loss: 0.07862772047519684, acc: 0.9740871787071228)
[2025-02-13 02:35:40,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:40,331][root][INFO] - Training Epoch: 1/2, step 1586/7134 completed (loss: 0.10916119813919067, acc: 0.9679334759712219)
[2025-02-13 02:35:40,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:40,776][root][INFO] - Training Epoch: 1/2, step 1587/7134 completed (loss: 0.07195291668176651, acc: 0.9811617136001587)
[2025-02-13 02:35:40,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:41,227][root][INFO] - Training Epoch: 1/2, step 1588/7134 completed (loss: 0.07997780293226242, acc: 0.9853528738021851)
[2025-02-13 02:35:41,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:41,662][root][INFO] - Training Epoch: 1/2, step 1589/7134 completed (loss: 0.05809160694479942, acc: 0.986146092414856)
[2025-02-13 02:35:41,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:42,078][root][INFO] - Training Epoch: 1/2, step 1590/7134 completed (loss: 0.06795790046453476, acc: 0.981873095035553)
[2025-02-13 02:35:42,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:42,496][root][INFO] - Training Epoch: 1/2, step 1591/7134 completed (loss: 0.06754589080810547, acc: 0.9796954393386841)
[2025-02-13 02:35:42,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:42,957][root][INFO] - Training Epoch: 1/2, step 1592/7134 completed (loss: 0.0719749704003334, acc: 0.980028510093689)
[2025-02-13 02:35:43,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:43,436][root][INFO] - Training Epoch: 1/2, step 1593/7134 completed (loss: 0.0769614726305008, acc: 0.9804597496986389)
[2025-02-13 02:35:43,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:43,870][root][INFO] - Training Epoch: 1/2, step 1594/7134 completed (loss: 0.0630694180727005, acc: 0.9818181991577148)
[2025-02-13 02:35:44,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:44,306][root][INFO] - Training Epoch: 1/2, step 1595/7134 completed (loss: 0.05060090497136116, acc: 0.9874551892280579)
[2025-02-13 02:35:44,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:44,736][root][INFO] - Training Epoch: 1/2, step 1596/7134 completed (loss: 0.1165924072265625, acc: 0.9685039520263672)
[2025-02-13 02:35:44,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:45,161][root][INFO] - Training Epoch: 1/2, step 1597/7134 completed (loss: 0.0552542619407177, acc: 0.984240710735321)
[2025-02-13 02:35:45,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:45,561][root][INFO] - Training Epoch: 1/2, step 1598/7134 completed (loss: 0.10390454530715942, acc: 0.9720559120178223)
[2025-02-13 02:35:45,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:45,990][root][INFO] - Training Epoch: 1/2, step 1599/7134 completed (loss: 0.1627344787120819, acc: 0.9586777091026306)
[2025-02-13 02:35:46,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:46,444][root][INFO] - Training Epoch: 1/2, step 1600/7134 completed (loss: 0.09755710512399673, acc: 0.9792147874832153)
[2025-02-13 02:35:46,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:46,864][root][INFO] - Training Epoch: 1/2, step 1601/7134 completed (loss: 0.05278977006673813, acc: 0.9848693013191223)
[2025-02-13 02:35:47,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:47,318][root][INFO] - Training Epoch: 1/2, step 1602/7134 completed (loss: 0.0670228973031044, acc: 0.9924585223197937)
[2025-02-13 02:35:47,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:47,707][root][INFO] - Training Epoch: 1/2, step 1603/7134 completed (loss: 0.10254515707492828, acc: 0.9795396327972412)
[2025-02-13 02:35:47,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:48,148][root][INFO] - Training Epoch: 1/2, step 1604/7134 completed (loss: 0.05726270377635956, acc: 0.9840849041938782)
[2025-02-13 02:35:48,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:48,575][root][INFO] - Training Epoch: 1/2, step 1605/7134 completed (loss: 0.0786152184009552, acc: 0.9778357148170471)
[2025-02-13 02:35:48,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:48,990][root][INFO] - Training Epoch: 1/2, step 1606/7134 completed (loss: 0.030223362147808075, acc: 0.9908116459846497)
[2025-02-13 02:35:49,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:49,397][root][INFO] - Training Epoch: 1/2, step 1607/7134 completed (loss: 0.04553784057497978, acc: 0.9857434034347534)
[2025-02-13 02:35:49,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:49,818][root][INFO] - Training Epoch: 1/2, step 1608/7134 completed (loss: 0.04555385932326317, acc: 0.9869888424873352)
[2025-02-13 02:35:49,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:50,267][root][INFO] - Training Epoch: 1/2, step 1609/7134 completed (loss: 0.0743120089173317, acc: 0.9766355156898499)
[2025-02-13 02:35:50,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:50,686][root][INFO] - Training Epoch: 1/2, step 1610/7134 completed (loss: 0.09383732825517654, acc: 0.9799666404724121)
[2025-02-13 02:35:50,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:51,107][root][INFO] - Training Epoch: 1/2, step 1611/7134 completed (loss: 0.09975079447031021, acc: 0.9775132536888123)
[2025-02-13 02:35:51,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:51,527][root][INFO] - Training Epoch: 1/2, step 1612/7134 completed (loss: 0.10054008662700653, acc: 0.9796437621116638)
[2025-02-13 02:35:51,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:51,950][root][INFO] - Training Epoch: 1/2, step 1613/7134 completed (loss: 0.08930566906929016, acc: 0.9802371263504028)
[2025-02-13 02:35:52,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:52,383][root][INFO] - Training Epoch: 1/2, step 1614/7134 completed (loss: 0.09122305363416672, acc: 0.9790301322937012)
[2025-02-13 02:35:52,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:52,872][root][INFO] - Training Epoch: 1/2, step 1615/7134 completed (loss: 0.07936551421880722, acc: 0.9798657894134521)
[2025-02-13 02:35:53,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:53,311][root][INFO] - Training Epoch: 1/2, step 1616/7134 completed (loss: 0.03292550891637802, acc: 0.9942280054092407)
[2025-02-13 02:35:53,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:53,733][root][INFO] - Training Epoch: 1/2, step 1617/7134 completed (loss: 0.09248445928096771, acc: 0.9781591296195984)
[2025-02-13 02:35:53,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:54,153][root][INFO] - Training Epoch: 1/2, step 1618/7134 completed (loss: 0.12731212377548218, acc: 0.961403489112854)
[2025-02-13 02:35:54,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:54,583][root][INFO] - Training Epoch: 1/2, step 1619/7134 completed (loss: 0.08820756524801254, acc: 0.968664824962616)
[2025-02-13 02:35:54,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:55,020][root][INFO] - Training Epoch: 1/2, step 1620/7134 completed (loss: 0.13235192000865936, acc: 0.9652042388916016)
[2025-02-13 02:35:55,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:55,452][root][INFO] - Training Epoch: 1/2, step 1621/7134 completed (loss: 0.1129293292760849, acc: 0.9729729890823364)
[2025-02-13 02:35:55,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:55,873][root][INFO] - Training Epoch: 1/2, step 1622/7134 completed (loss: 0.09639842808246613, acc: 0.9791086316108704)
[2025-02-13 02:35:56,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:56,325][root][INFO] - Training Epoch: 1/2, step 1623/7134 completed (loss: 0.09463983029127121, acc: 0.9789325594902039)
[2025-02-13 02:35:56,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:56,766][root][INFO] - Training Epoch: 1/2, step 1624/7134 completed (loss: 0.10926449298858643, acc: 0.9691630005836487)
[2025-02-13 02:35:56,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:57,199][root][INFO] - Training Epoch: 1/2, step 1625/7134 completed (loss: 0.06887950003147125, acc: 0.9857549667358398)
[2025-02-13 02:35:57,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:57,619][root][INFO] - Training Epoch: 1/2, step 1626/7134 completed (loss: 0.12534233927726746, acc: 0.9699074029922485)
[2025-02-13 02:35:57,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:58,019][root][INFO] - Training Epoch: 1/2, step 1627/7134 completed (loss: 0.05423378571867943, acc: 0.985401451587677)
[2025-02-13 02:35:58,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:58,441][root][INFO] - Training Epoch: 1/2, step 1628/7134 completed (loss: 0.027826540172100067, acc: 0.9941691160202026)
[2025-02-13 02:35:58,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:58,846][root][INFO] - Training Epoch: 1/2, step 1629/7134 completed (loss: 0.062250081449747086, acc: 0.9800000190734863)
[2025-02-13 02:35:58,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:59,261][root][INFO] - Training Epoch: 1/2, step 1630/7134 completed (loss: 0.06511662900447845, acc: 0.9792592525482178)
[2025-02-13 02:35:59,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:59,687][root][INFO] - Training Epoch: 1/2, step 1631/7134 completed (loss: 0.08661488443613052, acc: 0.9742765426635742)
[2025-02-13 02:35:59,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:00,088][root][INFO] - Training Epoch: 1/2, step 1632/7134 completed (loss: 0.05321968346834183, acc: 0.9817578792572021)
[2025-02-13 02:36:00,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:00,504][root][INFO] - Training Epoch: 1/2, step 1633/7134 completed (loss: 0.04851188510656357, acc: 0.9874411225318909)
[2025-02-13 02:36:00,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:00,921][root][INFO] - Training Epoch: 1/2, step 1634/7134 completed (loss: 0.0600096732378006, acc: 0.9831546545028687)
[2025-02-13 02:36:01,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:01,322][root][INFO] - Training Epoch: 1/2, step 1635/7134 completed (loss: 0.07634443044662476, acc: 0.9775967597961426)
[2025-02-13 02:36:01,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:01,736][root][INFO] - Training Epoch: 1/2, step 1636/7134 completed (loss: 0.08162060379981995, acc: 0.9790475964546204)
[2025-02-13 02:36:01,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:02,162][root][INFO] - Training Epoch: 1/2, step 1637/7134 completed (loss: 0.04181554168462753, acc: 0.9858757257461548)
[2025-02-13 02:36:02,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:02,589][root][INFO] - Training Epoch: 1/2, step 1638/7134 completed (loss: 0.12083014100790024, acc: 0.9677419066429138)
[2025-02-13 02:36:02,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:03,025][root][INFO] - Training Epoch: 1/2, step 1639/7134 completed (loss: 0.1777188628911972, acc: 0.95716392993927)
[2025-02-13 02:36:03,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:03,476][root][INFO] - Training Epoch: 1/2, step 1640/7134 completed (loss: 0.10069115459918976, acc: 0.9649389982223511)
[2025-02-13 02:36:03,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:03,897][root][INFO] - Training Epoch: 1/2, step 1641/7134 completed (loss: 0.06797727197408676, acc: 0.9841954112052917)
[2025-02-13 02:36:04,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:04,340][root][INFO] - Training Epoch: 1/2, step 1642/7134 completed (loss: 0.11511635780334473, acc: 0.9697368144989014)
[2025-02-13 02:36:04,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:04,753][root][INFO] - Training Epoch: 1/2, step 1643/7134 completed (loss: 0.1526901125907898, acc: 0.9618717432022095)
[2025-02-13 02:36:04,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:05,203][root][INFO] - Training Epoch: 1/2, step 1644/7134 completed (loss: 0.11597897857427597, acc: 0.9726206064224243)
[2025-02-13 02:36:05,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:05,633][root][INFO] - Training Epoch: 1/2, step 1645/7134 completed (loss: 0.10756123065948486, acc: 0.977011501789093)
[2025-02-13 02:36:05,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:06,104][root][INFO] - Training Epoch: 1/2, step 1646/7134 completed (loss: 0.12661218643188477, acc: 0.9728434681892395)
[2025-02-13 02:36:06,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:06,551][root][INFO] - Training Epoch: 1/2, step 1647/7134 completed (loss: 0.07031452655792236, acc: 0.978723406791687)
[2025-02-13 02:36:06,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:06,996][root][INFO] - Training Epoch: 1/2, step 1648/7134 completed (loss: 0.10839103162288666, acc: 0.977393627166748)
[2025-02-13 02:36:07,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:07,444][root][INFO] - Training Epoch: 1/2, step 1649/7134 completed (loss: 0.06503259390592575, acc: 0.9833518266677856)
[2025-02-13 02:36:07,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:07,909][root][INFO] - Training Epoch: 1/2, step 1650/7134 completed (loss: 0.09206240624189377, acc: 0.969072163105011)
[2025-02-13 02:36:08,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:08,371][root][INFO] - Training Epoch: 1/2, step 1651/7134 completed (loss: 0.13421370089054108, acc: 0.9646017551422119)
[2025-02-13 02:36:08,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:08,857][root][INFO] - Training Epoch: 1/2, step 1652/7134 completed (loss: 0.05866850167512894, acc: 0.9836423397064209)
[2025-02-13 02:36:08,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:09,311][root][INFO] - Training Epoch: 1/2, step 1653/7134 completed (loss: 0.07481177896261215, acc: 0.9775840640068054)
[2025-02-13 02:36:09,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:09,813][root][INFO] - Training Epoch: 1/2, step 1654/7134 completed (loss: 0.10851188749074936, acc: 0.9745157957077026)
[2025-02-13 02:36:09,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:10,281][root][INFO] - Training Epoch: 1/2, step 1655/7134 completed (loss: 0.08894234895706177, acc: 0.9764559864997864)
[2025-02-13 02:36:10,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:10,741][root][INFO] - Training Epoch: 1/2, step 1656/7134 completed (loss: 0.101343534886837, acc: 0.9729729890823364)
[2025-02-13 02:36:10,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:11,237][root][INFO] - Training Epoch: 1/2, step 1657/7134 completed (loss: 0.0924689993262291, acc: 0.9698629975318909)
[2025-02-13 02:36:11,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:11,713][root][INFO] - Training Epoch: 1/2, step 1658/7134 completed (loss: 0.07525880634784698, acc: 0.9755101799964905)
[2025-02-13 02:36:11,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:12,178][root][INFO] - Training Epoch: 1/2, step 1659/7134 completed (loss: 0.10027870535850525, acc: 0.9725118279457092)
[2025-02-13 02:36:12,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:12,658][root][INFO] - Training Epoch: 1/2, step 1660/7134 completed (loss: 0.08905858546495438, acc: 0.979468584060669)
[2025-02-13 02:36:12,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:13,129][root][INFO] - Training Epoch: 1/2, step 1661/7134 completed (loss: 0.10497333854436874, acc: 0.9715536236763)
[2025-02-13 02:36:13,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:13,587][root][INFO] - Training Epoch: 1/2, step 1662/7134 completed (loss: 0.09055083990097046, acc: 0.9766702055931091)
[2025-02-13 02:36:13,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:14,072][root][INFO] - Training Epoch: 1/2, step 1663/7134 completed (loss: 0.06673810631036758, acc: 0.9825282692909241)
[2025-02-13 02:36:14,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:14,513][root][INFO] - Training Epoch: 1/2, step 1664/7134 completed (loss: 0.06659799069166183, acc: 0.9809296727180481)
[2025-02-13 02:36:14,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:14,970][root][INFO] - Training Epoch: 1/2, step 1665/7134 completed (loss: 0.08340855687856674, acc: 0.9827337861061096)
[2025-02-13 02:36:15,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:15,490][root][INFO] - Training Epoch: 1/2, step 1666/7134 completed (loss: 0.07804583013057709, acc: 0.9750499129295349)
[2025-02-13 02:36:15,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:15,984][root][INFO] - Training Epoch: 1/2, step 1667/7134 completed (loss: 0.16536031663417816, acc: 0.9501187801361084)
[2025-02-13 02:36:16,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:16,456][root][INFO] - Training Epoch: 1/2, step 1668/7134 completed (loss: 0.06458035856485367, acc: 0.9821428656578064)
[2025-02-13 02:36:16,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:16,981][root][INFO] - Training Epoch: 1/2, step 1669/7134 completed (loss: 0.07514791935682297, acc: 0.9783427715301514)
[2025-02-13 02:36:17,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:17,491][root][INFO] - Training Epoch: 1/2, step 1670/7134 completed (loss: 0.09514964371919632, acc: 0.9751908183097839)
[2025-02-13 02:36:17,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:17,973][root][INFO] - Training Epoch: 1/2, step 1671/7134 completed (loss: 0.09208764135837555, acc: 0.9701313972473145)
[2025-02-13 02:36:18,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:18,418][root][INFO] - Training Epoch: 1/2, step 1672/7134 completed (loss: 0.1473245471715927, acc: 0.9521912336349487)
[2025-02-13 02:36:18,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:18,871][root][INFO] - Training Epoch: 1/2, step 1673/7134 completed (loss: 0.11773623526096344, acc: 0.9690860509872437)
[2025-02-13 02:36:18,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:19,279][root][INFO] - Training Epoch: 1/2, step 1674/7134 completed (loss: 0.10247055441141129, acc: 0.9720670580863953)
[2025-02-13 02:36:19,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:19,737][root][INFO] - Training Epoch: 1/2, step 1675/7134 completed (loss: 0.16492165625095367, acc: 0.9578231573104858)
[2025-02-13 02:36:19,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:20,163][root][INFO] - Training Epoch: 1/2, step 1676/7134 completed (loss: 0.08557986468076706, acc: 0.9737569093704224)
[2025-02-13 02:36:20,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:20,581][root][INFO] - Training Epoch: 1/2, step 1677/7134 completed (loss: 0.1391785889863968, acc: 0.9607201218605042)
[2025-02-13 02:36:20,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:21,035][root][INFO] - Training Epoch: 1/2, step 1678/7134 completed (loss: 0.058072615414857864, acc: 0.983561635017395)
[2025-02-13 02:36:21,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:21,457][root][INFO] - Training Epoch: 1/2, step 1679/7134 completed (loss: 0.13002459704875946, acc: 0.9707174301147461)
[2025-02-13 02:36:21,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:21,912][root][INFO] - Training Epoch: 1/2, step 1680/7134 completed (loss: 0.09310658276081085, acc: 0.974530816078186)
[2025-02-13 02:36:22,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:22,328][root][INFO] - Training Epoch: 1/2, step 1681/7134 completed (loss: 0.0823550894856453, acc: 0.984375)
[2025-02-13 02:36:22,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:22,770][root][INFO] - Training Epoch: 1/2, step 1682/7134 completed (loss: 0.15245942771434784, acc: 0.9610950946807861)
[2025-02-13 02:36:22,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:23,200][root][INFO] - Training Epoch: 1/2, step 1683/7134 completed (loss: 0.09579343348741531, acc: 0.9724919199943542)
[2025-02-13 02:36:23,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:23,627][root][INFO] - Training Epoch: 1/2, step 1684/7134 completed (loss: 0.08643898367881775, acc: 0.9777397513389587)
[2025-02-13 02:36:23,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:24,061][root][INFO] - Training Epoch: 1/2, step 1685/7134 completed (loss: 0.10215871781110764, acc: 0.9694019556045532)
[2025-02-13 02:36:24,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:24,471][root][INFO] - Training Epoch: 1/2, step 1686/7134 completed (loss: 0.07123962789773941, acc: 0.974662184715271)
[2025-02-13 02:36:24,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:24,906][root][INFO] - Training Epoch: 1/2, step 1687/7134 completed (loss: 0.07498003542423248, acc: 0.980988621711731)
[2025-02-13 02:36:25,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:25,311][root][INFO] - Training Epoch: 1/2, step 1688/7134 completed (loss: 0.07556275278329849, acc: 0.9799072742462158)
[2025-02-13 02:36:25,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:25,722][root][INFO] - Training Epoch: 1/2, step 1689/7134 completed (loss: 0.0714515894651413, acc: 0.9841269850730896)
[2025-02-13 02:36:25,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:26,119][root][INFO] - Training Epoch: 1/2, step 1690/7134 completed (loss: 0.04579188674688339, acc: 0.9898374080657959)
[2025-02-13 02:36:26,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:26,523][root][INFO] - Training Epoch: 1/2, step 1691/7134 completed (loss: 0.04877173528075218, acc: 0.9900662302970886)
[2025-02-13 02:36:26,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:26,935][root][INFO] - Training Epoch: 1/2, step 1692/7134 completed (loss: 0.08932261914014816, acc: 0.9679595232009888)
[2025-02-13 02:36:27,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:27,360][root][INFO] - Training Epoch: 1/2, step 1693/7134 completed (loss: 0.05198940262198448, acc: 0.9865269660949707)
[2025-02-13 02:36:27,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:27,783][root][INFO] - Training Epoch: 1/2, step 1694/7134 completed (loss: 0.06011116877198219, acc: 0.9843505620956421)
[2025-02-13 02:36:27,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:28,216][root][INFO] - Training Epoch: 1/2, step 1695/7134 completed (loss: 0.041973184794187546, acc: 0.9891892075538635)
[2025-02-13 02:36:28,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:28,613][root][INFO] - Training Epoch: 1/2, step 1696/7134 completed (loss: 0.08044079691171646, acc: 0.9748743772506714)
[2025-02-13 02:36:28,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:29,016][root][INFO] - Training Epoch: 1/2, step 1697/7134 completed (loss: 0.08267192542552948, acc: 0.9827855825424194)
[2025-02-13 02:36:29,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:29,432][root][INFO] - Training Epoch: 1/2, step 1698/7134 completed (loss: 0.04636729136109352, acc: 0.9905956387519836)
[2025-02-13 02:36:29,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:29,875][root][INFO] - Training Epoch: 1/2, step 1699/7134 completed (loss: 0.05885394290089607, acc: 0.9844290614128113)
[2025-02-13 02:36:30,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:30,334][root][INFO] - Training Epoch: 1/2, step 1700/7134 completed (loss: 0.1096593588590622, acc: 0.9731663465499878)
[2025-02-13 02:36:30,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:30,756][root][INFO] - Training Epoch: 1/2, step 1701/7134 completed (loss: 0.10191988945007324, acc: 0.9680111408233643)
[2025-02-13 02:36:30,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:31,188][root][INFO] - Training Epoch: 1/2, step 1702/7134 completed (loss: 0.1050390899181366, acc: 0.9701863527297974)
[2025-02-13 02:36:31,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:31,608][root][INFO] - Training Epoch: 1/2, step 1703/7134 completed (loss: 0.11594220250844955, acc: 0.9709944725036621)
[2025-02-13 02:36:31,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:32,046][root][INFO] - Training Epoch: 1/2, step 1704/7134 completed (loss: 0.1764964610338211, acc: 0.95652174949646)
[2025-02-13 02:36:32,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:32,494][root][INFO] - Training Epoch: 1/2, step 1705/7134 completed (loss: 0.09724564105272293, acc: 0.9723865985870361)
[2025-02-13 02:36:32,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:32,760][root][INFO] - Training Epoch: 1/2, step 1706/7134 completed (loss: 0.07484585046768188, acc: 0.9848484992980957)
[2025-02-13 02:36:32,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:33,152][root][INFO] - Training Epoch: 1/2, step 1707/7134 completed (loss: 0.07985176146030426, acc: 0.9809725284576416)
[2025-02-13 02:36:33,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:33,574][root][INFO] - Training Epoch: 1/2, step 1708/7134 completed (loss: 0.06519985944032669, acc: 0.9772357940673828)
[2025-02-13 02:36:33,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:33,991][root][INFO] - Training Epoch: 1/2, step 1709/7134 completed (loss: 0.08410942554473877, acc: 0.9741379022598267)
[2025-02-13 02:36:34,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:34,457][root][INFO] - Training Epoch: 1/2, step 1710/7134 completed (loss: 0.13438363373279572, acc: 0.9626436829566956)
[2025-02-13 02:36:34,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:34,859][root][INFO] - Training Epoch: 1/2, step 1711/7134 completed (loss: 0.08425981551408768, acc: 0.9827288389205933)
[2025-02-13 02:36:34,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:35,265][root][INFO] - Training Epoch: 1/2, step 1712/7134 completed (loss: 0.14780479669570923, acc: 0.9596491456031799)
[2025-02-13 02:36:35,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:35,668][root][INFO] - Training Epoch: 1/2, step 1713/7134 completed (loss: 0.07023824006319046, acc: 0.9840319156646729)
[2025-02-13 02:36:35,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:36,101][root][INFO] - Training Epoch: 1/2, step 1714/7134 completed (loss: 0.1385376900434494, acc: 0.9572368264198303)
[2025-02-13 02:36:36,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:36,539][root][INFO] - Training Epoch: 1/2, step 1715/7134 completed (loss: 0.11743089556694031, acc: 0.9675456285476685)
[2025-02-13 02:36:36,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:36,942][root][INFO] - Training Epoch: 1/2, step 1716/7134 completed (loss: 0.13820023834705353, acc: 0.9566666483879089)
[2025-02-13 02:36:37,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:37,333][root][INFO] - Training Epoch: 1/2, step 1717/7134 completed (loss: 0.06749285012483597, acc: 0.9727626442909241)
[2025-02-13 02:36:37,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:37,750][root][INFO] - Training Epoch: 1/2, step 1718/7134 completed (loss: 0.10237745195627213, acc: 0.9643652439117432)
[2025-02-13 02:36:37,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:38,193][root][INFO] - Training Epoch: 1/2, step 1719/7134 completed (loss: 0.1130572035908699, acc: 0.9739726185798645)
[2025-02-13 02:36:38,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:38,557][root][INFO] - Training Epoch: 1/2, step 1720/7134 completed (loss: 0.15867094695568085, acc: 0.9482401609420776)
[2025-02-13 02:36:38,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:38,916][root][INFO] - Training Epoch: 1/2, step 1721/7134 completed (loss: 0.09066988527774811, acc: 0.9730185270309448)
[2025-02-13 02:36:39,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:39,268][root][INFO] - Training Epoch: 1/2, step 1722/7134 completed (loss: 0.07134957611560822, acc: 0.9763157963752747)
[2025-02-13 02:36:39,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:39,683][root][INFO] - Training Epoch: 1/2, step 1723/7134 completed (loss: 0.14189597964286804, acc: 0.9597615599632263)
[2025-02-13 02:36:39,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:40,106][root][INFO] - Training Epoch: 1/2, step 1724/7134 completed (loss: 0.1507122665643692, acc: 0.9623233675956726)
[2025-02-13 02:36:40,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:40,433][root][INFO] - Training Epoch: 1/2, step 1725/7134 completed (loss: 0.08427729457616806, acc: 0.971061110496521)
[2025-02-13 02:36:40,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:40,790][root][INFO] - Training Epoch: 1/2, step 1726/7134 completed (loss: 0.11567030102014542, acc: 0.9641255736351013)
[2025-02-13 02:36:40,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:41,203][root][INFO] - Training Epoch: 1/2, step 1727/7134 completed (loss: 0.08014289289712906, acc: 0.9799270033836365)
[2025-02-13 02:36:41,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:41,630][root][INFO] - Training Epoch: 1/2, step 1728/7134 completed (loss: 0.07687969505786896, acc: 0.9834437370300293)
[2025-02-13 02:36:41,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:42,021][root][INFO] - Training Epoch: 1/2, step 1729/7134 completed (loss: 0.10913554579019547, acc: 0.9653767943382263)
[2025-02-13 02:36:42,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:42,442][root][INFO] - Training Epoch: 1/2, step 1730/7134 completed (loss: 0.0658811703324318, acc: 0.9850106835365295)
[2025-02-13 02:36:42,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:42,726][root][INFO] - Training Epoch: 1/2, step 1731/7134 completed (loss: 0.14079459011554718, acc: 0.9537572264671326)
[2025-02-13 02:36:42,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:43,102][root][INFO] - Training Epoch: 1/2, step 1732/7134 completed (loss: 0.09039907902479172, acc: 0.9712525606155396)
[2025-02-13 02:36:43,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:43,440][root][INFO] - Training Epoch: 1/2, step 1733/7134 completed (loss: 0.12115778774023056, acc: 0.9670782089233398)
[2025-02-13 02:36:43,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:43,910][root][INFO] - Training Epoch: 1/2, step 1734/7134 completed (loss: 0.12409896403551102, acc: 0.961916446685791)
[2025-02-13 02:36:44,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:44,349][root][INFO] - Training Epoch: 1/2, step 1735/7134 completed (loss: 0.04430762305855751, acc: 0.9845722317695618)
[2025-02-13 02:36:44,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:44,803][root][INFO] - Training Epoch: 1/2, step 1736/7134 completed (loss: 0.08994115889072418, acc: 0.9769697189331055)
[2025-02-13 02:36:44,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:45,248][root][INFO] - Training Epoch: 1/2, step 1737/7134 completed (loss: 0.08119289577007294, acc: 0.9789325594902039)
[2025-02-13 02:36:45,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:45,699][root][INFO] - Training Epoch: 1/2, step 1738/7134 completed (loss: 0.058952055871486664, acc: 0.9855247139930725)
[2025-02-13 02:36:45,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:46,148][root][INFO] - Training Epoch: 1/2, step 1739/7134 completed (loss: 0.07768534123897552, acc: 0.9730538725852966)
[2025-02-13 02:36:46,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:46,596][root][INFO] - Training Epoch: 1/2, step 1740/7134 completed (loss: 0.04861990734934807, acc: 0.9839181303977966)
[2025-02-13 02:36:46,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:47,072][root][INFO] - Training Epoch: 1/2, step 1741/7134 completed (loss: 0.08298400789499283, acc: 0.9814814925193787)
[2025-02-13 02:36:47,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:47,495][root][INFO] - Training Epoch: 1/2, step 1742/7134 completed (loss: 0.08482806384563446, acc: 0.9758865237236023)
[2025-02-13 02:36:47,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:47,954][root][INFO] - Training Epoch: 1/2, step 1743/7134 completed (loss: 0.1144706979393959, acc: 0.9719887971878052)
[2025-02-13 02:36:48,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:48,383][root][INFO] - Training Epoch: 1/2, step 1744/7134 completed (loss: 0.06512261927127838, acc: 0.9820846915245056)
[2025-02-13 02:36:48,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:48,832][root][INFO] - Training Epoch: 1/2, step 1745/7134 completed (loss: 0.06830140203237534, acc: 0.9837092757225037)
[2025-02-13 02:36:48,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:49,287][root][INFO] - Training Epoch: 1/2, step 1746/7134 completed (loss: 0.04369296506047249, acc: 0.9811046719551086)
[2025-02-13 02:36:49,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:49,757][root][INFO] - Training Epoch: 1/2, step 1747/7134 completed (loss: 0.060617364943027496, acc: 0.9824780821800232)
[2025-02-13 02:36:49,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:50,217][root][INFO] - Training Epoch: 1/2, step 1748/7134 completed (loss: 0.09686576575040817, acc: 0.9689826369285583)
[2025-02-13 02:36:50,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:50,687][root][INFO] - Training Epoch: 1/2, step 1749/7134 completed (loss: 0.06075378134846687, acc: 0.9829738736152649)
[2025-02-13 02:36:50,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:51,162][root][INFO] - Training Epoch: 1/2, step 1750/7134 completed (loss: 0.07135534286499023, acc: 0.9760192036628723)
[2025-02-13 02:36:51,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:51,610][root][INFO] - Training Epoch: 1/2, step 1751/7134 completed (loss: 0.06590928882360458, acc: 0.9818181991577148)
[2025-02-13 02:36:51,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:52,057][root][INFO] - Training Epoch: 1/2, step 1752/7134 completed (loss: 0.09966158121824265, acc: 0.9716494679450989)
[2025-02-13 02:36:52,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:52,489][root][INFO] - Training Epoch: 1/2, step 1753/7134 completed (loss: 0.09708517789840698, acc: 0.9778933525085449)
[2025-02-13 02:36:52,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:52,912][root][INFO] - Training Epoch: 1/2, step 1754/7134 completed (loss: 0.06870339065790176, acc: 0.9812679886817932)
[2025-02-13 02:36:53,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:53,341][root][INFO] - Training Epoch: 1/2, step 1755/7134 completed (loss: 0.06812762469053268, acc: 0.976190447807312)
[2025-02-13 02:36:53,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:53,780][root][INFO] - Training Epoch: 1/2, step 1756/7134 completed (loss: 0.08214221894741058, acc: 0.9765929579734802)
[2025-02-13 02:36:53,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:54,195][root][INFO] - Training Epoch: 1/2, step 1757/7134 completed (loss: 0.08296548575162888, acc: 0.981697142124176)
[2025-02-13 02:36:54,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:54,656][root][INFO] - Training Epoch: 1/2, step 1758/7134 completed (loss: 0.05483477935194969, acc: 0.9823232293128967)
[2025-02-13 02:36:54,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:55,091][root][INFO] - Training Epoch: 1/2, step 1759/7134 completed (loss: 0.061952587217092514, acc: 0.9819193482398987)
[2025-02-13 02:36:55,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:55,507][root][INFO] - Training Epoch: 1/2, step 1760/7134 completed (loss: 0.07182938605546951, acc: 0.9798319339752197)
[2025-02-13 02:36:55,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:55,893][root][INFO] - Training Epoch: 1/2, step 1761/7134 completed (loss: 0.037274330854415894, acc: 0.9900568127632141)
[2025-02-13 02:36:56,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:56,311][root][INFO] - Training Epoch: 1/2, step 1762/7134 completed (loss: 0.06097990274429321, acc: 0.9731958508491516)
[2025-02-13 02:36:56,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:56,713][root][INFO] - Training Epoch: 1/2, step 1763/7134 completed (loss: 0.08630349487066269, acc: 0.9743589758872986)
[2025-02-13 02:36:56,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:57,125][root][INFO] - Training Epoch: 1/2, step 1764/7134 completed (loss: 0.08996870368719101, acc: 0.9851301312446594)
[2025-02-13 02:36:57,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:57,558][root][INFO] - Training Epoch: 1/2, step 1765/7134 completed (loss: 0.06239454075694084, acc: 0.9861111044883728)
[2025-02-13 02:36:57,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:57,968][root][INFO] - Training Epoch: 1/2, step 1766/7134 completed (loss: 0.07067476958036423, acc: 0.9824047088623047)
[2025-02-13 02:36:58,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:58,401][root][INFO] - Training Epoch: 1/2, step 1767/7134 completed (loss: 0.1204092875123024, acc: 0.9660537242889404)
[2025-02-13 02:36:58,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:58,821][root][INFO] - Training Epoch: 1/2, step 1768/7134 completed (loss: 0.05474289506673813, acc: 0.9845857620239258)
[2025-02-13 02:36:58,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:59,259][root][INFO] - Training Epoch: 1/2, step 1769/7134 completed (loss: 0.05664779618382454, acc: 0.9808823466300964)
[2025-02-13 02:36:59,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:59,663][root][INFO] - Training Epoch: 1/2, step 1770/7134 completed (loss: 0.0497538223862648, acc: 0.987860381603241)
[2025-02-13 02:36:59,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:00,087][root][INFO] - Training Epoch: 1/2, step 1771/7134 completed (loss: 0.044671643525362015, acc: 0.9902234673500061)
[2025-02-13 02:37:00,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:00,489][root][INFO] - Training Epoch: 1/2, step 1772/7134 completed (loss: 0.05069073289632797, acc: 0.9849246144294739)
[2025-02-13 02:37:00,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:00,917][root][INFO] - Training Epoch: 1/2, step 1773/7134 completed (loss: 0.040415916591882706, acc: 0.9898256063461304)
[2025-02-13 02:37:01,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:01,315][root][INFO] - Training Epoch: 1/2, step 1774/7134 completed (loss: 0.045341312885284424, acc: 0.9833887219429016)
[2025-02-13 02:37:01,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:01,707][root][INFO] - Training Epoch: 1/2, step 1775/7134 completed (loss: 0.0658586323261261, acc: 0.984375)
[2025-02-13 02:37:01,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:02,112][root][INFO] - Training Epoch: 1/2, step 1776/7134 completed (loss: 0.06013965606689453, acc: 0.9835329055786133)
[2025-02-13 02:37:02,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:02,519][root][INFO] - Training Epoch: 1/2, step 1777/7134 completed (loss: 0.04006959870457649, acc: 0.9911242723464966)
[2025-02-13 02:37:02,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:02,936][root][INFO] - Training Epoch: 1/2, step 1778/7134 completed (loss: 0.08716431260108948, acc: 0.9807692170143127)
[2025-02-13 02:37:03,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:03,339][root][INFO] - Training Epoch: 1/2, step 1779/7134 completed (loss: 0.05499262735247612, acc: 0.9777777791023254)
[2025-02-13 02:37:03,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:03,822][root][INFO] - Training Epoch: 1/2, step 1780/7134 completed (loss: 0.05382787063717842, acc: 0.987270176410675)
[2025-02-13 02:37:03,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:04,227][root][INFO] - Training Epoch: 1/2, step 1781/7134 completed (loss: 0.04409083351492882, acc: 0.9832060933113098)
[2025-02-13 02:37:04,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:04,632][root][INFO] - Training Epoch: 1/2, step 1782/7134 completed (loss: 0.04615529254078865, acc: 0.990867555141449)
[2025-02-13 02:37:05,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:05,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:06,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:06,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:06,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:07,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:07,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:08,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:08,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:08,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:09,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:09,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:09,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:10,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:10,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:10,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:11,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:11,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:11,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:12,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:12,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:13,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:13,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:13,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:14,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:14,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:14,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:15,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:15,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:16,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:16,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:16,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:17,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:17,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:17,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:18,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:18,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:18,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:19,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:19,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:20,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:20,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:20,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:21,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:21,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:22,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:22,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:22,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:23,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:23,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:24,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:24,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:24,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:25,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:25,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:25,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:26,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:26,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:26,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:27,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:27,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:27,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:28,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:28,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:28,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:29,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:29,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:29,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:30,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:30,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:30,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:31,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:31,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:32,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:32,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:32,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:33,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:33,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:33,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:34,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:34,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:34,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:35,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:35,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:35,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:36,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:36,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:37,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:37,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:37,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:38,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:38,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:39,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:39,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:39,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:40,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:40,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:40,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:41,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:41,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:41,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:42,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:42,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:42,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:43,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:43,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:43,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:44,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:44,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:44,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:45,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:45,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:45,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:46,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:46,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:47,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:47,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:47,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:48,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:48,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:48,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:49,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:49,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:49,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:50,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:50,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:50,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:51,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:51,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:51,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:52,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:52,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:52,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:53,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:53,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:54,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:54,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:54,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:55,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:55,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:55,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:56,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:56,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:56,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:57,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:57,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:58,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:58,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:58,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:59,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:59,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:59,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:00,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:00,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:00,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:01,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:01,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:02,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:02,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:02,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:03,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:03,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:03,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:04,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:04,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:04,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:05,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:05,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:06,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:06,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:07,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:07,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:07,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:08,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:08,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:08,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:09,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:09,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:10,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:10,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:10,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:11,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:11,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:11,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:12,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:12,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:12,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:13,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:13,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:14,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:14,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:14,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:15,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:15,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:15,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:16,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:16,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:16,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:17,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:17,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:17,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:18,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:18,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:18,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:19,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:19,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:20,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:20,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:20,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:21,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:21,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:21,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:22,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:22,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:22,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:23,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:23,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:24,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:24,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:24,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:25,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:25,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:25,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:26,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:26,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:27,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:27,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:27,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:28,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:28,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:28,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:29,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:29,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:29,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:30,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:30,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:31,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:31,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:31,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:32,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:32,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:32,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:33,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:33,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:33,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:34,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:34,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:35,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:35,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:36,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:36,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:36,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:37,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:37,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:37,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:38,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:38,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:39,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:39,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:39,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:40,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:40,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:40,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:41,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:41,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:41,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:42,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:42,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:42,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:43,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:43,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:44,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:44,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:45,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:45,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:45,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:46,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:46,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:46,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:47,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:47,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:48,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:48,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:49,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:49,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:49,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:50,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:50,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:51,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:51,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:51,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:52,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:52,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:53,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:53,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:54,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:54,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:54,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:55,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:55,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:55,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:56,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:56,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:56,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:57,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:57,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:57,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:58,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:58,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:58,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:59,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:59,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:59,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:00,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:00,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:00,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:01,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:01,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:02,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:02,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:02,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:03,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:03,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:03,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:04,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:04,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:05,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:05,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:05,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:06,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:06,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:06,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:07,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:07,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:08,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:08,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:08,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:09,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:09,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:09,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:10,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:10,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:11,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:11,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:11,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:12,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:12,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:13,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:13,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:13,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:14,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:14,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:15,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:15,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:15,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:16,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:16,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:16,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:17,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:17,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:18,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:18,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:19,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:20,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:20,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:21,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:21,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:21,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:22,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:22,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:22,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:23,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:23,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:23,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:24,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:24,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:24,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:25,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:25,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:25,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:26,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:26,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:27,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:27,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:27,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:28,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:28,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:28,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:29,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:29,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:29,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:30,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:30,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:30,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:31,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:31,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:31,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:32,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:32,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:32,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:33,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:33,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:33,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:34,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:34,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:34,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:35,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:35,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:35,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:36,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:36,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:36,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:37,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:37,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:38,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:38,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:38,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:39,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:39,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:39,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:40,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:40,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:40,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:41,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:41,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:41,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:42,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:42,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:42,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:43,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:43,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:43,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:44,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:44,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:44,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:45,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:45,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:46,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:46,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:46,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:47,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:47,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:47,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:48,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:48,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:48,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:49,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:49,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:49,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:50,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:50,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:50,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:51,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:51,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:51,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:52,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:52,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:52,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:53,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:53,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:54,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:54,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:54,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:55,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:55,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:55,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:56,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:56,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:56,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:57,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:57,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:57,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:58,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:58,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:58,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:59,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:59,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:59,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:00,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:00,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:01,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:01,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:01,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:02,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:02,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:02,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:03,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:03,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:03,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:04,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:04,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:05,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:05,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:05,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:06,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:06,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:06,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:07,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:07,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:08,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:08,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:08,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:09,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:09,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:10,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:10,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:10,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:11,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:11,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:11,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:12,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:12,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:12,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:13,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:13,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:13,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:14,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:14,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:15,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:15,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:15,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:16,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:16,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:16,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:17,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:17,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:17,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:18,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:18,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:18,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:19,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:19,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:19,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:20,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:20,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:20,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:21,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:21,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:22,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:22,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:22,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:23,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:23,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:23,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:24,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:24,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:24,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:25,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:25,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:25,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:26,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:26,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:27,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:27,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:27,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:28,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:28,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:28,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:29,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:29,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:29,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:30,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:30,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:30,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:31,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:31,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:31,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:32,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:32,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:33,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:33,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:33,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:34,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:34,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:34,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:35,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:35,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:35,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:36,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:36,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:37,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:37,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:38,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:38,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:38,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:39,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:39,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:40,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:40,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:40,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:41,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:41,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:41,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:42,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:42,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:42,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:43,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:43,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:43,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:44,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:44,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:44,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:45,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:45,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:45,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:46,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:46,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:46,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:47,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:47,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:47,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:48,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:48,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:49,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:49,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:49,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:50,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:50,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:50,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:51,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:51,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:51,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:52,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:52,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:53,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:53,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:54,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:55,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:55,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:55,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:56,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:56,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:57,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:57,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:58,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:58,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:58,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:59,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:59,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:59,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:00,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:00,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:00,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:01,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:01,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:01,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:02,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:02,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:03,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:03,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:04,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:04,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:04,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:05,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:05,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:05,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:06,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:06,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:07,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:07,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:07,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:08,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:08,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:08,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:09,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:09,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:09,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:10,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:10,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:11,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:11,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:11,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:12,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:12,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:13,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:13,656][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.1078, device='cuda:0') eval_epoch_loss=tensor(0.1024, device='cuda:0') eval_epoch_acc=tensor(0.9736, device='cuda:0')
[2025-02-13 02:41:13,658][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 02:41:13,658][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 02:41:14,000][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_1783_loss_0.10236252099275589/model.pt
[2025-02-13 02:41:14,004][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 02:41:14,005][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.10236252099275589
[2025-02-13 02:41:14,005][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9736115336418152
[2025-02-13 02:41:14,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:14,438][root][INFO] - Training Epoch: 1/2, step 1783/7134 completed (loss: 0.06162058934569359, acc: 0.9810810685157776)
[2025-02-13 02:41:14,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:14,870][root][INFO] - Training Epoch: 1/2, step 1784/7134 completed (loss: 0.06468158215284348, acc: 0.9868035316467285)
[2025-02-13 02:41:15,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:15,309][root][INFO] - Training Epoch: 1/2, step 1785/7134 completed (loss: 0.042545389384031296, acc: 0.9821183085441589)
[2025-02-13 02:41:15,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:15,706][root][INFO] - Training Epoch: 1/2, step 1786/7134 completed (loss: 0.04852744936943054, acc: 0.9859594106674194)
[2025-02-13 02:41:15,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:16,128][root][INFO] - Training Epoch: 1/2, step 1787/7134 completed (loss: 0.04811589792370796, acc: 0.9821428656578064)
[2025-02-13 02:41:16,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:16,519][root][INFO] - Training Epoch: 1/2, step 1788/7134 completed (loss: 0.04067447409033775, acc: 0.9901800155639648)
[2025-02-13 02:41:16,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:16,935][root][INFO] - Training Epoch: 1/2, step 1789/7134 completed (loss: 0.04392261058092117, acc: 0.9861751198768616)
[2025-02-13 02:41:17,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:17,370][root][INFO] - Training Epoch: 1/2, step 1790/7134 completed (loss: 0.15568560361862183, acc: 0.9710144996643066)
[2025-02-13 02:41:17,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:17,770][root][INFO] - Training Epoch: 1/2, step 1791/7134 completed (loss: 0.17068777978420258, acc: 0.9657632112503052)
[2025-02-13 02:41:17,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:18,214][root][INFO] - Training Epoch: 1/2, step 1792/7134 completed (loss: 0.22615380585193634, acc: 0.9579945802688599)
[2025-02-13 02:41:18,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:18,654][root][INFO] - Training Epoch: 1/2, step 1793/7134 completed (loss: 0.20197056233882904, acc: 0.954054057598114)
[2025-02-13 02:41:18,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:19,101][root][INFO] - Training Epoch: 1/2, step 1794/7134 completed (loss: 0.1358654648065567, acc: 0.9606918096542358)
[2025-02-13 02:41:19,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:19,515][root][INFO] - Training Epoch: 1/2, step 1795/7134 completed (loss: 0.17503796517848969, acc: 0.9609929323196411)
[2025-02-13 02:41:19,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:19,925][root][INFO] - Training Epoch: 1/2, step 1796/7134 completed (loss: 0.16497789323329926, acc: 0.9576802253723145)
[2025-02-13 02:41:20,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:20,341][root][INFO] - Training Epoch: 1/2, step 1797/7134 completed (loss: 0.11595756560564041, acc: 0.9677870869636536)
[2025-02-13 02:41:20,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:20,782][root][INFO] - Training Epoch: 1/2, step 1798/7134 completed (loss: 0.09224940091371536, acc: 0.9749034643173218)
[2025-02-13 02:41:20,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:21,223][root][INFO] - Training Epoch: 1/2, step 1799/7134 completed (loss: 0.15740688145160675, acc: 0.9623115658760071)
[2025-02-13 02:41:21,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:21,668][root][INFO] - Training Epoch: 1/2, step 1800/7134 completed (loss: 0.16901734471321106, acc: 0.9591121673583984)
[2025-02-13 02:41:21,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:22,108][root][INFO] - Training Epoch: 1/2, step 1801/7134 completed (loss: 0.1390424519777298, acc: 0.9682203531265259)
[2025-02-13 02:41:22,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:22,542][root][INFO] - Training Epoch: 1/2, step 1802/7134 completed (loss: 0.09163151681423187, acc: 0.9737171530723572)
[2025-02-13 02:41:22,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:22,954][root][INFO] - Training Epoch: 1/2, step 1803/7134 completed (loss: 0.09476964920759201, acc: 0.973805844783783)
[2025-02-13 02:41:23,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:23,376][root][INFO] - Training Epoch: 1/2, step 1804/7134 completed (loss: 0.04793009161949158, acc: 0.9889435172080994)
[2025-02-13 02:41:23,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:23,775][root][INFO] - Training Epoch: 1/2, step 1805/7134 completed (loss: 0.11829874664545059, acc: 0.966810941696167)
[2025-02-13 02:41:23,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:24,183][root][INFO] - Training Epoch: 1/2, step 1806/7134 completed (loss: 0.07804740220308304, acc: 0.9815789461135864)
[2025-02-13 02:41:24,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:24,588][root][INFO] - Training Epoch: 1/2, step 1807/7134 completed (loss: 0.06025197356939316, acc: 0.9801980257034302)
[2025-02-13 02:41:24,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:24,988][root][INFO] - Training Epoch: 1/2, step 1808/7134 completed (loss: 0.06549960374832153, acc: 0.9791937470436096)
[2025-02-13 02:41:25,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:25,433][root][INFO] - Training Epoch: 1/2, step 1809/7134 completed (loss: 0.10021666437387466, acc: 0.9729729890823364)
[2025-02-13 02:41:25,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:25,883][root][INFO] - Training Epoch: 1/2, step 1810/7134 completed (loss: 0.04473481327295303, acc: 0.9896907210350037)
[2025-02-13 02:41:26,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:26,304][root][INFO] - Training Epoch: 1/2, step 1811/7134 completed (loss: 0.16050556302070618, acc: 0.9565807580947876)
[2025-02-13 02:41:26,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:26,736][root][INFO] - Training Epoch: 1/2, step 1812/7134 completed (loss: 0.0771486833691597, acc: 0.9821428656578064)
[2025-02-13 02:41:26,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:27,183][root][INFO] - Training Epoch: 1/2, step 1813/7134 completed (loss: 0.09503336250782013, acc: 0.9748502969741821)
[2025-02-13 02:41:27,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:27,637][root][INFO] - Training Epoch: 1/2, step 1814/7134 completed (loss: 0.05279838293790817, acc: 0.9836829900741577)
[2025-02-13 02:41:27,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:28,063][root][INFO] - Training Epoch: 1/2, step 1815/7134 completed (loss: 0.08372093737125397, acc: 0.9755747318267822)
[2025-02-13 02:41:28,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:28,466][root][INFO] - Training Epoch: 1/2, step 1816/7134 completed (loss: 0.10215200483798981, acc: 0.9749608635902405)
[2025-02-13 02:41:28,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:28,908][root][INFO] - Training Epoch: 1/2, step 1817/7134 completed (loss: 0.06179141625761986, acc: 0.9832935333251953)
[2025-02-13 02:41:29,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:29,353][root][INFO] - Training Epoch: 1/2, step 1818/7134 completed (loss: 0.08414340764284134, acc: 0.9801734685897827)
[2025-02-13 02:41:29,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:29,789][root][INFO] - Training Epoch: 1/2, step 1819/7134 completed (loss: 0.05394047871232033, acc: 0.9788029789924622)
[2025-02-13 02:41:29,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:30,279][root][INFO] - Training Epoch: 1/2, step 1820/7134 completed (loss: 0.06903218477964401, acc: 0.9814410209655762)
[2025-02-13 02:41:30,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:30,711][root][INFO] - Training Epoch: 1/2, step 1821/7134 completed (loss: 0.07272331416606903, acc: 0.9857767820358276)
[2025-02-13 02:41:30,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:31,113][root][INFO] - Training Epoch: 1/2, step 1822/7134 completed (loss: 0.02800985798239708, acc: 0.9912280440330505)
[2025-02-13 02:41:31,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:31,546][root][INFO] - Training Epoch: 1/2, step 1823/7134 completed (loss: 0.045145463198423386, acc: 0.9890859723091125)
[2025-02-13 02:41:31,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:31,981][root][INFO] - Training Epoch: 1/2, step 1824/7134 completed (loss: 0.051047325134277344, acc: 0.9851668477058411)
[2025-02-13 02:41:32,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:32,430][root][INFO] - Training Epoch: 1/2, step 1825/7134 completed (loss: 0.04524977505207062, acc: 0.9865471124649048)
[2025-02-13 02:41:32,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:32,869][root][INFO] - Training Epoch: 1/2, step 1826/7134 completed (loss: 0.04633399099111557, acc: 0.9878869652748108)
[2025-02-13 02:41:33,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:33,313][root][INFO] - Training Epoch: 1/2, step 1827/7134 completed (loss: 0.0892157033085823, acc: 0.9801543354988098)
[2025-02-13 02:41:33,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:33,747][root][INFO] - Training Epoch: 1/2, step 1828/7134 completed (loss: 0.014704427681863308, acc: 0.9974259734153748)
[2025-02-13 02:41:33,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:34,178][root][INFO] - Training Epoch: 1/2, step 1829/7134 completed (loss: 0.058917269110679626, acc: 0.9813664555549622)
[2025-02-13 02:41:34,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:34,618][root][INFO] - Training Epoch: 1/2, step 1830/7134 completed (loss: 0.05964454263448715, acc: 0.981840193271637)
[2025-02-13 02:41:34,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:35,009][root][INFO] - Training Epoch: 1/2, step 1831/7134 completed (loss: 0.03357556834816933, acc: 0.987500011920929)
[2025-02-13 02:41:35,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:35,410][root][INFO] - Training Epoch: 1/2, step 1832/7134 completed (loss: 0.11407670378684998, acc: 0.9734982252120972)
[2025-02-13 02:41:35,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:35,841][root][INFO] - Training Epoch: 1/2, step 1833/7134 completed (loss: 0.2600329518318176, acc: 0.9492600560188293)
[2025-02-13 02:41:35,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:36,287][root][INFO] - Training Epoch: 1/2, step 1834/7134 completed (loss: 0.093546561896801, acc: 0.9788732528686523)
[2025-02-13 02:41:36,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:36,703][root][INFO] - Training Epoch: 1/2, step 1835/7134 completed (loss: 0.1569998860359192, acc: 0.9738371968269348)
[2025-02-13 02:41:36,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:37,139][root][INFO] - Training Epoch: 1/2, step 1836/7134 completed (loss: 0.08689770102500916, acc: 0.9669030904769897)
[2025-02-13 02:41:37,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:37,573][root][INFO] - Training Epoch: 1/2, step 1837/7134 completed (loss: 0.060070328414440155, acc: 0.9828009605407715)
[2025-02-13 02:41:37,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:37,902][root][INFO] - Training Epoch: 1/2, step 1838/7134 completed (loss: 0.03795278072357178, acc: 0.9857142567634583)
[2025-02-13 02:41:38,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:38,348][root][INFO] - Training Epoch: 1/2, step 1839/7134 completed (loss: 0.05817554518580437, acc: 0.9870466589927673)
[2025-02-13 02:41:38,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:38,790][root][INFO] - Training Epoch: 1/2, step 1840/7134 completed (loss: 0.09998999536037445, acc: 0.9721059799194336)
[2025-02-13 02:41:38,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:39,196][root][INFO] - Training Epoch: 1/2, step 1841/7134 completed (loss: 0.08388538658618927, acc: 0.9744681119918823)
[2025-02-13 02:41:39,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:39,640][root][INFO] - Training Epoch: 1/2, step 1842/7134 completed (loss: 0.06710179150104523, acc: 0.9836734533309937)
[2025-02-13 02:41:39,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:40,056][root][INFO] - Training Epoch: 1/2, step 1843/7134 completed (loss: 0.10653465986251831, acc: 0.9662500023841858)
[2025-02-13 02:41:40,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:40,502][root][INFO] - Training Epoch: 1/2, step 1844/7134 completed (loss: 0.08061635494232178, acc: 0.9788618087768555)
[2025-02-13 02:41:40,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:40,956][root][INFO] - Training Epoch: 1/2, step 1845/7134 completed (loss: 0.1007595807313919, acc: 0.968684732913971)
[2025-02-13 02:41:41,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:41,374][root][INFO] - Training Epoch: 1/2, step 1846/7134 completed (loss: 0.05334996059536934, acc: 0.9914529919624329)
[2025-02-13 02:41:41,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:41,832][root][INFO] - Training Epoch: 1/2, step 1847/7134 completed (loss: 0.06488347798585892, acc: 0.981566846370697)
[2025-02-13 02:41:41,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:42,274][root][INFO] - Training Epoch: 1/2, step 1848/7134 completed (loss: 0.0800391137599945, acc: 0.9788135886192322)
[2025-02-13 02:41:42,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:42,704][root][INFO] - Training Epoch: 1/2, step 1849/7134 completed (loss: 0.08260981738567352, acc: 0.9841827750205994)
[2025-02-13 02:41:42,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:43,154][root][INFO] - Training Epoch: 1/2, step 1850/7134 completed (loss: 0.0424320325255394, acc: 0.9889975786209106)
[2025-02-13 02:41:43,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:43,599][root][INFO] - Training Epoch: 1/2, step 1851/7134 completed (loss: 0.07107584178447723, acc: 0.9792429804801941)
[2025-02-13 02:41:43,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:44,006][root][INFO] - Training Epoch: 1/2, step 1852/7134 completed (loss: 0.047902461141347885, acc: 0.9829192757606506)
[2025-02-13 02:41:44,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:44,440][root][INFO] - Training Epoch: 1/2, step 1853/7134 completed (loss: 0.059519775211811066, acc: 0.9844497442245483)
[2025-02-13 02:41:44,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:44,876][root][INFO] - Training Epoch: 1/2, step 1854/7134 completed (loss: 0.0923941507935524, acc: 0.9738406538963318)
[2025-02-13 02:41:45,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:45,317][root][INFO] - Training Epoch: 1/2, step 1855/7134 completed (loss: 0.06961512565612793, acc: 0.9849711060523987)
[2025-02-13 02:41:45,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:45,729][root][INFO] - Training Epoch: 1/2, step 1856/7134 completed (loss: 0.05548938736319542, acc: 0.9815497994422913)
[2025-02-13 02:41:45,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:46,201][root][INFO] - Training Epoch: 1/2, step 1857/7134 completed (loss: 0.1103324443101883, acc: 0.9778324961662292)
[2025-02-13 02:41:46,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:46,642][root][INFO] - Training Epoch: 1/2, step 1858/7134 completed (loss: 0.03892164304852486, acc: 0.9880383014678955)
[2025-02-13 02:41:46,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:47,100][root][INFO] - Training Epoch: 1/2, step 1859/7134 completed (loss: 0.033214956521987915, acc: 0.9908987283706665)
[2025-02-13 02:41:47,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:47,540][root][INFO] - Training Epoch: 1/2, step 1860/7134 completed (loss: 0.03823331743478775, acc: 0.9911392331123352)
[2025-02-13 02:41:47,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:47,972][root][INFO] - Training Epoch: 1/2, step 1861/7134 completed (loss: 0.07417028397321701, acc: 0.9844961166381836)
[2025-02-13 02:41:48,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:48,399][root][INFO] - Training Epoch: 1/2, step 1862/7134 completed (loss: 0.04811489209532738, acc: 0.9864314794540405)
[2025-02-13 02:41:48,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:48,833][root][INFO] - Training Epoch: 1/2, step 1863/7134 completed (loss: 0.08295633643865585, acc: 0.9796162843704224)
[2025-02-13 02:41:48,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:49,244][root][INFO] - Training Epoch: 1/2, step 1864/7134 completed (loss: 0.16723409295082092, acc: 0.9646258354187012)
[2025-02-13 02:41:49,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:49,630][root][INFO] - Training Epoch: 1/2, step 1865/7134 completed (loss: 0.08745283633470535, acc: 0.9746192693710327)
[2025-02-13 02:41:49,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:50,052][root][INFO] - Training Epoch: 1/2, step 1866/7134 completed (loss: 0.03355460241436958, acc: 0.9901960492134094)
[2025-02-13 02:41:50,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:50,506][root][INFO] - Training Epoch: 1/2, step 1867/7134 completed (loss: 0.05154263228178024, acc: 0.9836065769195557)
[2025-02-13 02:41:50,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:50,968][root][INFO] - Training Epoch: 1/2, step 1868/7134 completed (loss: 0.11734180897474289, acc: 0.9653465151786804)
[2025-02-13 02:41:51,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:51,382][root][INFO] - Training Epoch: 1/2, step 1869/7134 completed (loss: 0.0831141546368599, acc: 0.975530207157135)
[2025-02-13 02:41:51,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:51,785][root][INFO] - Training Epoch: 1/2, step 1870/7134 completed (loss: 0.0547659695148468, acc: 0.9856770634651184)
[2025-02-13 02:41:51,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:52,223][root][INFO] - Training Epoch: 1/2, step 1871/7134 completed (loss: 0.10576221346855164, acc: 0.9738988876342773)
[2025-02-13 02:41:52,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:52,462][root][INFO] - Training Epoch: 1/2, step 1872/7134 completed (loss: 0.5851127505302429, acc: 0.889502763748169)
[2025-02-13 02:41:52,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:52,866][root][INFO] - Training Epoch: 1/2, step 1873/7134 completed (loss: 0.20612090826034546, acc: 0.9382422566413879)
[2025-02-13 02:41:53,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:53,314][root][INFO] - Training Epoch: 1/2, step 1874/7134 completed (loss: 0.0907038003206253, acc: 0.9816513657569885)
[2025-02-13 02:41:53,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:53,716][root][INFO] - Training Epoch: 1/2, step 1875/7134 completed (loss: 0.07656264305114746, acc: 0.9825327396392822)
[2025-02-13 02:41:53,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:54,125][root][INFO] - Training Epoch: 1/2, step 1876/7134 completed (loss: 0.1629561483860016, acc: 0.9490662217140198)
[2025-02-13 02:41:54,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:54,559][root][INFO] - Training Epoch: 1/2, step 1877/7134 completed (loss: 0.11344070732593536, acc: 0.9647355079650879)
[2025-02-13 02:41:54,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:54,991][root][INFO] - Training Epoch: 1/2, step 1878/7134 completed (loss: 0.11471405625343323, acc: 0.9753466844558716)
[2025-02-13 02:41:55,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:55,395][root][INFO] - Training Epoch: 1/2, step 1879/7134 completed (loss: 0.1320120245218277, acc: 0.9657422304153442)
[2025-02-13 02:41:55,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:55,768][root][INFO] - Training Epoch: 1/2, step 1880/7134 completed (loss: 0.0856490284204483, acc: 0.9798792600631714)
[2025-02-13 02:41:55,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:56,185][root][INFO] - Training Epoch: 1/2, step 1881/7134 completed (loss: 0.10289875417947769, acc: 0.9754902124404907)
[2025-02-13 02:41:56,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:56,517][root][INFO] - Training Epoch: 1/2, step 1882/7134 completed (loss: 0.15981510281562805, acc: 0.9482758641242981)
[2025-02-13 02:41:56,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:56,928][root][INFO] - Training Epoch: 1/2, step 1883/7134 completed (loss: 0.09687181562185287, acc: 0.9783464670181274)
[2025-02-13 02:41:57,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:57,331][root][INFO] - Training Epoch: 1/2, step 1884/7134 completed (loss: 0.1458214521408081, acc: 0.9605055451393127)
[2025-02-13 02:41:57,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:57,733][root][INFO] - Training Epoch: 1/2, step 1885/7134 completed (loss: 0.10862749069929123, acc: 0.9699812531471252)
[2025-02-13 02:41:57,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:58,144][root][INFO] - Training Epoch: 1/2, step 1886/7134 completed (loss: 0.13719019293785095, acc: 0.9728600978851318)
[2025-02-13 02:41:58,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:58,542][root][INFO] - Training Epoch: 1/2, step 1887/7134 completed (loss: 0.11213788390159607, acc: 0.9715369939804077)
[2025-02-13 02:41:58,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:58,969][root][INFO] - Training Epoch: 1/2, step 1888/7134 completed (loss: 0.12357228994369507, acc: 0.9692898392677307)
[2025-02-13 02:41:59,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:59,412][root][INFO] - Training Epoch: 1/2, step 1889/7134 completed (loss: 0.06618513911962509, acc: 0.9797688126564026)
[2025-02-13 02:41:59,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:59,833][root][INFO] - Training Epoch: 1/2, step 1890/7134 completed (loss: 0.0494811125099659, acc: 0.9876161217689514)
[2025-02-13 02:41:59,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:00,243][root][INFO] - Training Epoch: 1/2, step 1891/7134 completed (loss: 0.12973764538764954, acc: 0.9594155550003052)
[2025-02-13 02:42:00,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:00,656][root][INFO] - Training Epoch: 1/2, step 1892/7134 completed (loss: 0.11148083209991455, acc: 0.9768160581588745)
[2025-02-13 02:42:00,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:01,049][root][INFO] - Training Epoch: 1/2, step 1893/7134 completed (loss: 0.07097089290618896, acc: 0.975039005279541)
[2025-02-13 02:42:01,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:01,453][root][INFO] - Training Epoch: 1/2, step 1894/7134 completed (loss: 0.1521114856004715, acc: 0.96875)
[2025-02-13 02:42:01,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:01,807][root][INFO] - Training Epoch: 1/2, step 1895/7134 completed (loss: 0.327192097902298, acc: 0.9281045794487)
[2025-02-13 02:42:01,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:02,209][root][INFO] - Training Epoch: 1/2, step 1896/7134 completed (loss: 0.11512450128793716, acc: 0.9694117903709412)
[2025-02-13 02:42:02,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:02,630][root][INFO] - Training Epoch: 1/2, step 1897/7134 completed (loss: 0.0471147783100605, acc: 0.9870370626449585)
[2025-02-13 02:42:02,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:03,034][root][INFO] - Training Epoch: 1/2, step 1898/7134 completed (loss: 0.06577220559120178, acc: 0.9800994992256165)
[2025-02-13 02:42:03,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:03,387][root][INFO] - Training Epoch: 1/2, step 1899/7134 completed (loss: 0.09337121993303299, acc: 0.9819168448448181)
[2025-02-13 02:42:03,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:03,794][root][INFO] - Training Epoch: 1/2, step 1900/7134 completed (loss: 0.06577663868665695, acc: 0.9870550036430359)
[2025-02-13 02:42:03,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:04,201][root][INFO] - Training Epoch: 1/2, step 1901/7134 completed (loss: 0.1019907221198082, acc: 0.9780621528625488)
[2025-02-13 02:42:04,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:04,597][root][INFO] - Training Epoch: 1/2, step 1902/7134 completed (loss: 0.05485296994447708, acc: 0.9876977205276489)
[2025-02-13 02:42:04,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:05,006][root][INFO] - Training Epoch: 1/2, step 1903/7134 completed (loss: 0.06594018638134003, acc: 0.9739583134651184)
[2025-02-13 02:42:05,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:05,393][root][INFO] - Training Epoch: 1/2, step 1904/7134 completed (loss: 0.06757748126983643, acc: 0.9809321761131287)
[2025-02-13 02:42:05,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:05,781][root][INFO] - Training Epoch: 1/2, step 1905/7134 completed (loss: 0.08978830277919769, acc: 0.9672801494598389)
[2025-02-13 02:42:05,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:06,184][root][INFO] - Training Epoch: 1/2, step 1906/7134 completed (loss: 0.07512813061475754, acc: 0.9824561476707458)
[2025-02-13 02:42:06,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:06,576][root][INFO] - Training Epoch: 1/2, step 1907/7134 completed (loss: 0.064520925283432, acc: 0.9815126061439514)
[2025-02-13 02:42:06,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:07,009][root][INFO] - Training Epoch: 1/2, step 1908/7134 completed (loss: 0.0732489675283432, acc: 0.9828141927719116)
[2025-02-13 02:42:07,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:07,415][root][INFO] - Training Epoch: 1/2, step 1909/7134 completed (loss: 0.06835026293992996, acc: 0.9801324605941772)
[2025-02-13 02:42:07,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:07,858][root][INFO] - Training Epoch: 1/2, step 1910/7134 completed (loss: 0.10253617912530899, acc: 0.9802631735801697)
[2025-02-13 02:42:07,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:08,294][root][INFO] - Training Epoch: 1/2, step 1911/7134 completed (loss: 0.07356300204992294, acc: 0.9890776872634888)
[2025-02-13 02:42:08,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:08,731][root][INFO] - Training Epoch: 1/2, step 1912/7134 completed (loss: 0.036280836910009384, acc: 0.9866844415664673)
[2025-02-13 02:42:08,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:09,115][root][INFO] - Training Epoch: 1/2, step 1913/7134 completed (loss: 0.09272617101669312, acc: 0.9870129823684692)
[2025-02-13 02:42:09,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:09,541][root][INFO] - Training Epoch: 1/2, step 1914/7134 completed (loss: 0.08136270940303802, acc: 0.9824086427688599)
[2025-02-13 02:42:09,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:09,929][root][INFO] - Training Epoch: 1/2, step 1915/7134 completed (loss: 0.08927936851978302, acc: 0.9787928462028503)
[2025-02-13 02:42:10,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:10,329][root][INFO] - Training Epoch: 1/2, step 1916/7134 completed (loss: 0.07603966444730759, acc: 0.9829476475715637)
[2025-02-13 02:42:10,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:10,762][root][INFO] - Training Epoch: 1/2, step 1917/7134 completed (loss: 0.07796840369701385, acc: 0.9845938086509705)
[2025-02-13 02:42:10,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:11,145][root][INFO] - Training Epoch: 1/2, step 1918/7134 completed (loss: 0.0845232605934143, acc: 0.9811912178993225)
[2025-02-13 02:42:11,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:11,599][root][INFO] - Training Epoch: 1/2, step 1919/7134 completed (loss: 0.0874391421675682, acc: 0.9842180609703064)
[2025-02-13 02:42:11,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:12,022][root][INFO] - Training Epoch: 1/2, step 1920/7134 completed (loss: 0.05156620591878891, acc: 0.9876543283462524)
[2025-02-13 02:42:12,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:12,424][root][INFO] - Training Epoch: 1/2, step 1921/7134 completed (loss: 0.07911542803049088, acc: 0.9806094169616699)
[2025-02-13 02:42:12,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:12,884][root][INFO] - Training Epoch: 1/2, step 1922/7134 completed (loss: 0.05225379765033722, acc: 0.987077534198761)
[2025-02-13 02:42:13,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:13,309][root][INFO] - Training Epoch: 1/2, step 1923/7134 completed (loss: 0.05668526515364647, acc: 0.9865996837615967)
[2025-02-13 02:42:13,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:13,733][root][INFO] - Training Epoch: 1/2, step 1924/7134 completed (loss: 0.08583808690309525, acc: 0.9763948321342468)
[2025-02-13 02:42:13,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:14,191][root][INFO] - Training Epoch: 1/2, step 1925/7134 completed (loss: 0.10366278141736984, acc: 0.9714285731315613)
[2025-02-13 02:42:14,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:14,652][root][INFO] - Training Epoch: 1/2, step 1926/7134 completed (loss: 0.039774224162101746, acc: 0.9877426028251648)
[2025-02-13 02:42:14,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:15,108][root][INFO] - Training Epoch: 1/2, step 1927/7134 completed (loss: 0.0812566801905632, acc: 0.9834254384040833)
[2025-02-13 02:42:15,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:15,568][root][INFO] - Training Epoch: 1/2, step 1928/7134 completed (loss: 0.0468655601143837, acc: 0.9863221645355225)
[2025-02-13 02:42:15,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:15,990][root][INFO] - Training Epoch: 1/2, step 1929/7134 completed (loss: 0.05905218794941902, acc: 0.9858757257461548)
[2025-02-13 02:42:16,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:16,431][root][INFO] - Training Epoch: 1/2, step 1930/7134 completed (loss: 0.10926609486341476, acc: 0.9737532734870911)
[2025-02-13 02:42:16,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:16,884][root][INFO] - Training Epoch: 1/2, step 1931/7134 completed (loss: 0.12515544891357422, acc: 0.9734513163566589)
[2025-02-13 02:42:17,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:17,270][root][INFO] - Training Epoch: 1/2, step 1932/7134 completed (loss: 0.13847343623638153, acc: 0.9611940383911133)
[2025-02-13 02:42:17,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:17,703][root][INFO] - Training Epoch: 1/2, step 1933/7134 completed (loss: 0.19067108631134033, acc: 0.9565826058387756)
[2025-02-13 02:42:17,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:18,109][root][INFO] - Training Epoch: 1/2, step 1934/7134 completed (loss: 0.05887918919324875, acc: 0.9872881174087524)
[2025-02-13 02:42:18,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:18,555][root][INFO] - Training Epoch: 1/2, step 1935/7134 completed (loss: 0.15609849989414215, acc: 0.9650654792785645)
[2025-02-13 02:42:18,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:19,001][root][INFO] - Training Epoch: 1/2, step 1936/7134 completed (loss: 0.036848392337560654, acc: 0.9921011328697205)
[2025-02-13 02:42:19,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:19,389][root][INFO] - Training Epoch: 1/2, step 1937/7134 completed (loss: 0.07260099053382874, acc: 0.9821428656578064)
[2025-02-13 02:42:19,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:19,808][root][INFO] - Training Epoch: 1/2, step 1938/7134 completed (loss: 0.12156292796134949, acc: 0.9700854420661926)
[2025-02-13 02:42:19,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:20,204][root][INFO] - Training Epoch: 1/2, step 1939/7134 completed (loss: 0.1009925976395607, acc: 0.9788135886192322)
[2025-02-13 02:42:20,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:20,636][root][INFO] - Training Epoch: 1/2, step 1940/7134 completed (loss: 0.11767613142728806, acc: 0.9673469662666321)
[2025-02-13 02:42:20,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:21,081][root][INFO] - Training Epoch: 1/2, step 1941/7134 completed (loss: 0.12418978661298752, acc: 0.9625668525695801)
[2025-02-13 02:42:21,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:21,481][root][INFO] - Training Epoch: 1/2, step 1942/7134 completed (loss: 0.06956101208925247, acc: 0.9817073345184326)
[2025-02-13 02:42:21,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:21,936][root][INFO] - Training Epoch: 1/2, step 1943/7134 completed (loss: 0.15197107195854187, acc: 0.959756076335907)
[2025-02-13 02:42:22,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:22,369][root][INFO] - Training Epoch: 1/2, step 1944/7134 completed (loss: 0.11326968669891357, acc: 0.9634369015693665)
[2025-02-13 02:42:22,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:22,793][root][INFO] - Training Epoch: 1/2, step 1945/7134 completed (loss: 0.09942067414522171, acc: 0.9724409580230713)
[2025-02-13 02:42:22,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:23,217][root][INFO] - Training Epoch: 1/2, step 1946/7134 completed (loss: 0.10246841609477997, acc: 0.9677419066429138)
[2025-02-13 02:42:23,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:23,642][root][INFO] - Training Epoch: 1/2, step 1947/7134 completed (loss: 0.11610354483127594, acc: 0.9644736647605896)
[2025-02-13 02:42:23,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:24,052][root][INFO] - Training Epoch: 1/2, step 1948/7134 completed (loss: 0.19269894063472748, acc: 0.9506641626358032)
[2025-02-13 02:42:24,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:24,401][root][INFO] - Training Epoch: 1/2, step 1949/7134 completed (loss: 0.08432666212320328, acc: 0.9732441306114197)
[2025-02-13 02:42:24,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:24,831][root][INFO] - Training Epoch: 1/2, step 1950/7134 completed (loss: 0.14054393768310547, acc: 0.9616788029670715)
[2025-02-13 02:42:24,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:25,291][root][INFO] - Training Epoch: 1/2, step 1951/7134 completed (loss: 0.09218259155750275, acc: 0.9765886068344116)
[2025-02-13 02:42:25,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:25,702][root][INFO] - Training Epoch: 1/2, step 1952/7134 completed (loss: 0.12091758847236633, acc: 0.9733777046203613)
[2025-02-13 02:42:25,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:26,125][root][INFO] - Training Epoch: 1/2, step 1953/7134 completed (loss: 0.07183900475502014, acc: 0.9814241528511047)
[2025-02-13 02:42:26,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:26,487][root][INFO] - Training Epoch: 1/2, step 1954/7134 completed (loss: 0.06562955677509308, acc: 0.9780488014221191)
[2025-02-13 02:42:26,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:26,880][root][INFO] - Training Epoch: 1/2, step 1955/7134 completed (loss: 0.0962735041975975, acc: 0.9789103865623474)
[2025-02-13 02:42:26,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:27,175][root][INFO] - Training Epoch: 1/2, step 1956/7134 completed (loss: 0.03991976007819176, acc: 0.9865591526031494)
[2025-02-13 02:42:27,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:27,561][root][INFO] - Training Epoch: 1/2, step 1957/7134 completed (loss: 0.08471669256687164, acc: 0.9759259223937988)
[2025-02-13 02:42:27,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:27,822][root][INFO] - Training Epoch: 1/2, step 1958/7134 completed (loss: 0.10726620256900787, acc: 0.9770773649215698)
[2025-02-13 02:42:27,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:28,208][root][INFO] - Training Epoch: 1/2, step 1959/7134 completed (loss: 0.047030817717313766, acc: 0.9914712309837341)
[2025-02-13 02:42:28,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:28,612][root][INFO] - Training Epoch: 1/2, step 1960/7134 completed (loss: 0.051783692091703415, acc: 0.9830827116966248)
[2025-02-13 02:42:28,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:28,969][root][INFO] - Training Epoch: 1/2, step 1961/7134 completed (loss: 0.06298092007637024, acc: 0.9765886068344116)
[2025-02-13 02:42:29,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:29,384][root][INFO] - Training Epoch: 1/2, step 1962/7134 completed (loss: 0.08434110134840012, acc: 0.9804469347000122)
[2025-02-13 02:42:29,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:29,781][root][INFO] - Training Epoch: 1/2, step 1963/7134 completed (loss: 0.08479185402393341, acc: 0.9723661541938782)
[2025-02-13 02:42:29,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:30,197][root][INFO] - Training Epoch: 1/2, step 1964/7134 completed (loss: 0.0755910649895668, acc: 0.9853479862213135)
[2025-02-13 02:42:30,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:30,607][root][INFO] - Training Epoch: 1/2, step 1965/7134 completed (loss: 0.06986062228679657, acc: 0.9832636117935181)
[2025-02-13 02:42:30,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:31,017][root][INFO] - Training Epoch: 1/2, step 1966/7134 completed (loss: 0.06801178306341171, acc: 0.9816513657569885)
[2025-02-13 02:42:31,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:31,460][root][INFO] - Training Epoch: 1/2, step 1967/7134 completed (loss: 0.09860185533761978, acc: 0.9745222926139832)
[2025-02-13 02:42:31,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:31,900][root][INFO] - Training Epoch: 1/2, step 1968/7134 completed (loss: 0.043409351259469986, acc: 0.9864197373390198)
[2025-02-13 02:42:32,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:32,311][root][INFO] - Training Epoch: 1/2, step 1969/7134 completed (loss: 0.09853547811508179, acc: 0.9746666550636292)
[2025-02-13 02:42:32,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:32,744][root][INFO] - Training Epoch: 1/2, step 1970/7134 completed (loss: 0.07261078804731369, acc: 0.9785276055335999)
[2025-02-13 02:42:32,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:33,152][root][INFO] - Training Epoch: 1/2, step 1971/7134 completed (loss: 0.0735052227973938, acc: 0.9787798523902893)
[2025-02-13 02:42:33,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:33,494][root][INFO] - Training Epoch: 1/2, step 1972/7134 completed (loss: 0.3068964183330536, acc: 0.9238329529762268)
[2025-02-13 02:42:33,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:33,895][root][INFO] - Training Epoch: 1/2, step 1973/7134 completed (loss: 0.17449164390563965, acc: 0.9523809552192688)
[2025-02-13 02:42:34,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:34,342][root][INFO] - Training Epoch: 1/2, step 1974/7134 completed (loss: 0.06831949204206467, acc: 0.9786432385444641)
[2025-02-13 02:42:34,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:34,748][root][INFO] - Training Epoch: 1/2, step 1975/7134 completed (loss: 0.06232406198978424, acc: 0.9848484992980957)
[2025-02-13 02:42:34,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:35,193][root][INFO] - Training Epoch: 1/2, step 1976/7134 completed (loss: 0.08015524595975876, acc: 0.9750849604606628)
[2025-02-13 02:42:35,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:35,627][root][INFO] - Training Epoch: 1/2, step 1977/7134 completed (loss: 0.09552676230669022, acc: 0.9710144996643066)
[2025-02-13 02:42:35,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:36,047][root][INFO] - Training Epoch: 1/2, step 1978/7134 completed (loss: 0.043886084109544754, acc: 0.9892617464065552)
[2025-02-13 02:42:36,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:36,492][root][INFO] - Training Epoch: 1/2, step 1979/7134 completed (loss: 0.06622646003961563, acc: 0.9752604365348816)
[2025-02-13 02:42:36,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:36,936][root][INFO] - Training Epoch: 1/2, step 1980/7134 completed (loss: 0.16803482174873352, acc: 0.958279013633728)
[2025-02-13 02:42:37,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:37,375][root][INFO] - Training Epoch: 1/2, step 1981/7134 completed (loss: 0.07363443076610565, acc: 0.9762237668037415)
[2025-02-13 02:42:37,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:37,784][root][INFO] - Training Epoch: 1/2, step 1982/7134 completed (loss: 0.047928545624017715, acc: 0.9871299862861633)
[2025-02-13 02:42:37,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:38,194][root][INFO] - Training Epoch: 1/2, step 1983/7134 completed (loss: 0.0583144910633564, acc: 0.9885495901107788)
[2025-02-13 02:42:38,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:38,586][root][INFO] - Training Epoch: 1/2, step 1984/7134 completed (loss: 0.04658478870987892, acc: 0.983849287033081)
[2025-02-13 02:42:38,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:38,993][root][INFO] - Training Epoch: 1/2, step 1985/7134 completed (loss: 0.05688508599996567, acc: 0.9818941354751587)
[2025-02-13 02:42:39,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:39,423][root][INFO] - Training Epoch: 1/2, step 1986/7134 completed (loss: 0.07359912246465683, acc: 0.9806950092315674)
[2025-02-13 02:42:39,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:39,861][root][INFO] - Training Epoch: 1/2, step 1987/7134 completed (loss: 0.0734405443072319, acc: 0.9844192862510681)
[2025-02-13 02:42:39,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:40,289][root][INFO] - Training Epoch: 1/2, step 1988/7134 completed (loss: 0.07834611088037491, acc: 0.980028510093689)
[2025-02-13 02:42:40,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:40,678][root][INFO] - Training Epoch: 1/2, step 1989/7134 completed (loss: 0.12273750454187393, acc: 0.9728867411613464)
[2025-02-13 02:42:40,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:41,118][root][INFO] - Training Epoch: 1/2, step 1990/7134 completed (loss: 0.0771106407046318, acc: 0.981796145439148)
[2025-02-13 02:42:41,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:41,556][root][INFO] - Training Epoch: 1/2, step 1991/7134 completed (loss: 0.05388493090867996, acc: 0.985049843788147)
[2025-02-13 02:42:41,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:41,968][root][INFO] - Training Epoch: 1/2, step 1992/7134 completed (loss: 0.08430106192827225, acc: 0.9689922332763672)
[2025-02-13 02:42:42,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:42,366][root][INFO] - Training Epoch: 1/2, step 1993/7134 completed (loss: 0.03681822866201401, acc: 0.9894551634788513)
[2025-02-13 02:42:42,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:42,776][root][INFO] - Training Epoch: 1/2, step 1994/7134 completed (loss: 0.020638303831219673, acc: 0.9931034445762634)
[2025-02-13 02:42:42,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:43,183][root][INFO] - Training Epoch: 1/2, step 1995/7134 completed (loss: 0.04281969740986824, acc: 0.9860896468162537)
[2025-02-13 02:42:43,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:43,591][root][INFO] - Training Epoch: 1/2, step 1996/7134 completed (loss: 0.05767170339822769, acc: 0.9851239919662476)
[2025-02-13 02:42:43,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:43,996][root][INFO] - Training Epoch: 1/2, step 1997/7134 completed (loss: 0.04290864244103432, acc: 0.9912663698196411)
[2025-02-13 02:42:44,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:44,398][root][INFO] - Training Epoch: 1/2, step 1998/7134 completed (loss: 0.0334036722779274, acc: 0.9905808568000793)
[2025-02-13 02:42:44,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:44,804][root][INFO] - Training Epoch: 1/2, step 1999/7134 completed (loss: 0.0685838833451271, acc: 0.9762309193611145)
[2025-02-13 02:42:44,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:45,207][root][INFO] - Training Epoch: 1/2, step 2000/7134 completed (loss: 0.0523269847035408, acc: 0.988959014415741)
[2025-02-13 02:42:45,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:45,592][root][INFO] - Training Epoch: 1/2, step 2001/7134 completed (loss: 0.04515669122338295, acc: 0.989393949508667)
[2025-02-13 02:42:45,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:45,993][root][INFO] - Training Epoch: 1/2, step 2002/7134 completed (loss: 0.051602933555841446, acc: 0.9843478202819824)
[2025-02-13 02:42:46,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:46,345][root][INFO] - Training Epoch: 1/2, step 2003/7134 completed (loss: 0.07021267712116241, acc: 0.9833333492279053)
[2025-02-13 02:42:46,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:46,703][root][INFO] - Training Epoch: 1/2, step 2004/7134 completed (loss: 0.26962268352508545, acc: 0.947257399559021)
[2025-02-13 02:42:46,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:47,132][root][INFO] - Training Epoch: 1/2, step 2005/7134 completed (loss: 0.2018922120332718, acc: 0.9589040875434875)
[2025-02-13 02:42:47,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:47,482][root][INFO] - Training Epoch: 1/2, step 2006/7134 completed (loss: 0.07792042195796967, acc: 0.9827089309692383)
[2025-02-13 02:42:47,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:47,880][root][INFO] - Training Epoch: 1/2, step 2007/7134 completed (loss: 0.05989371985197067, acc: 0.9859437942504883)
[2025-02-13 02:42:48,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:48,267][root][INFO] - Training Epoch: 1/2, step 2008/7134 completed (loss: 0.06153611093759537, acc: 0.9786477088928223)
[2025-02-13 02:42:48,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:48,630][root][INFO] - Training Epoch: 1/2, step 2009/7134 completed (loss: 0.046134449541568756, acc: 0.9855595827102661)
[2025-02-13 02:42:48,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:49,020][root][INFO] - Training Epoch: 1/2, step 2010/7134 completed (loss: 0.05581769719719887, acc: 0.9879931211471558)
[2025-02-13 02:42:49,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:49,413][root][INFO] - Training Epoch: 1/2, step 2011/7134 completed (loss: 0.06142307445406914, acc: 0.9784736037254333)
[2025-02-13 02:42:49,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:49,806][root][INFO] - Training Epoch: 1/2, step 2012/7134 completed (loss: 0.0719066709280014, acc: 0.9767441749572754)
[2025-02-13 02:42:49,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:50,225][root][INFO] - Training Epoch: 1/2, step 2013/7134 completed (loss: 0.05530672147870064, acc: 0.984000027179718)
[2025-02-13 02:42:50,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:50,636][root][INFO] - Training Epoch: 1/2, step 2014/7134 completed (loss: 0.05042102187871933, acc: 0.9858267903327942)
[2025-02-13 02:42:50,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:51,041][root][INFO] - Training Epoch: 1/2, step 2015/7134 completed (loss: 0.0579390786588192, acc: 0.9786184430122375)
[2025-02-13 02:42:51,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:51,473][root][INFO] - Training Epoch: 1/2, step 2016/7134 completed (loss: 0.10531274974346161, acc: 0.9723076820373535)
[2025-02-13 02:42:51,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:51,891][root][INFO] - Training Epoch: 1/2, step 2017/7134 completed (loss: 0.13086238503456116, acc: 0.9675675630569458)
[2025-02-13 02:42:52,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:52,305][root][INFO] - Training Epoch: 1/2, step 2018/7134 completed (loss: 0.04880882799625397, acc: 0.9794871807098389)
[2025-02-13 02:42:52,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:52,704][root][INFO] - Training Epoch: 1/2, step 2019/7134 completed (loss: 0.08262024819850922, acc: 0.9765517115592957)
[2025-02-13 02:42:52,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:53,120][root][INFO] - Training Epoch: 1/2, step 2020/7134 completed (loss: 0.03873734921216965, acc: 0.989847719669342)
[2025-02-13 02:42:53,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:53,544][root][INFO] - Training Epoch: 1/2, step 2021/7134 completed (loss: 0.08257029950618744, acc: 0.9779220819473267)
[2025-02-13 02:42:53,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:53,935][root][INFO] - Training Epoch: 1/2, step 2022/7134 completed (loss: 0.10249274969100952, acc: 0.9809358716011047)
[2025-02-13 02:42:54,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:54,350][root][INFO] - Training Epoch: 1/2, step 2023/7134 completed (loss: 0.06691692024469376, acc: 0.9810218811035156)
[2025-02-13 02:42:54,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:54,778][root][INFO] - Training Epoch: 1/2, step 2024/7134 completed (loss: 0.10448480397462845, acc: 0.9708333611488342)
[2025-02-13 02:42:54,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:55,205][root][INFO] - Training Epoch: 1/2, step 2025/7134 completed (loss: 0.04953208938241005, acc: 0.9879999756813049)
[2025-02-13 02:42:55,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:55,617][root][INFO] - Training Epoch: 1/2, step 2026/7134 completed (loss: 0.10559798032045364, acc: 0.9789156913757324)
[2025-02-13 02:42:55,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:56,026][root][INFO] - Training Epoch: 1/2, step 2027/7134 completed (loss: 0.04906849190592766, acc: 0.9817276000976562)
[2025-02-13 02:42:56,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:56,444][root][INFO] - Training Epoch: 1/2, step 2028/7134 completed (loss: 0.07106757909059525, acc: 0.9795321822166443)
[2025-02-13 02:42:56,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:56,857][root][INFO] - Training Epoch: 1/2, step 2029/7134 completed (loss: 0.10141091048717499, acc: 0.9758522510528564)
[2025-02-13 02:42:56,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:57,245][root][INFO] - Training Epoch: 1/2, step 2030/7134 completed (loss: 0.10068144649267197, acc: 0.9723926186561584)
[2025-02-13 02:42:57,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:57,688][root][INFO] - Training Epoch: 1/2, step 2031/7134 completed (loss: 0.06841520220041275, acc: 0.979141116142273)
[2025-02-13 02:42:57,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:58,105][root][INFO] - Training Epoch: 1/2, step 2032/7134 completed (loss: 0.06153026595711708, acc: 0.9852150678634644)
[2025-02-13 02:42:58,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:58,540][root][INFO] - Training Epoch: 1/2, step 2033/7134 completed (loss: 0.054241541773080826, acc: 0.989708423614502)
[2025-02-13 02:42:58,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:58,935][root][INFO] - Training Epoch: 1/2, step 2034/7134 completed (loss: 0.08545147627592087, acc: 0.9739663004875183)
[2025-02-13 02:42:59,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:59,373][root][INFO] - Training Epoch: 1/2, step 2035/7134 completed (loss: 0.04046344757080078, acc: 0.9900373816490173)
[2025-02-13 02:42:59,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:59,758][root][INFO] - Training Epoch: 1/2, step 2036/7134 completed (loss: 0.0564749576151371, acc: 0.9826989769935608)
[2025-02-13 02:42:59,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:00,195][root][INFO] - Training Epoch: 1/2, step 2037/7134 completed (loss: 0.050705116242170334, acc: 0.9792284965515137)
[2025-02-13 02:43:00,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:00,640][root][INFO] - Training Epoch: 1/2, step 2038/7134 completed (loss: 0.06002742052078247, acc: 0.9810874462127686)
[2025-02-13 02:43:00,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:01,056][root][INFO] - Training Epoch: 1/2, step 2039/7134 completed (loss: 0.029421532526612282, acc: 0.9909909963607788)
[2025-02-13 02:43:01,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:01,392][root][INFO] - Training Epoch: 1/2, step 2040/7134 completed (loss: 0.05776222050189972, acc: 0.9897119402885437)
[2025-02-13 02:43:01,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:01,819][root][INFO] - Training Epoch: 1/2, step 2041/7134 completed (loss: 0.030496753752231598, acc: 0.9931318759918213)
[2025-02-13 02:43:01,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:02,204][root][INFO] - Training Epoch: 1/2, step 2042/7134 completed (loss: 0.06058549880981445, acc: 0.9818548560142517)
[2025-02-13 02:43:02,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:02,599][root][INFO] - Training Epoch: 1/2, step 2043/7134 completed (loss: 0.0662979856133461, acc: 0.9828125238418579)
[2025-02-13 02:43:02,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:03,024][root][INFO] - Training Epoch: 1/2, step 2044/7134 completed (loss: 0.05614285543560982, acc: 0.989266574382782)
[2025-02-13 02:43:03,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:03,414][root][INFO] - Training Epoch: 1/2, step 2045/7134 completed (loss: 0.055694542825222015, acc: 0.9837251305580139)
[2025-02-13 02:43:03,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:03,758][root][INFO] - Training Epoch: 1/2, step 2046/7134 completed (loss: 0.055524613708257675, acc: 0.9837398529052734)
[2025-02-13 02:43:03,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:04,173][root][INFO] - Training Epoch: 1/2, step 2047/7134 completed (loss: 0.05661813169717789, acc: 0.9833055138587952)
[2025-02-13 02:43:04,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:04,591][root][INFO] - Training Epoch: 1/2, step 2048/7134 completed (loss: 0.07811398059129715, acc: 0.974588930606842)
[2025-02-13 02:43:04,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:04,994][root][INFO] - Training Epoch: 1/2, step 2049/7134 completed (loss: 0.10715097188949585, acc: 0.9694117903709412)
[2025-02-13 02:43:05,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:05,370][root][INFO] - Training Epoch: 1/2, step 2050/7134 completed (loss: 0.10339067876338959, acc: 0.9794871807098389)
[2025-02-13 02:43:05,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:05,765][root][INFO] - Training Epoch: 1/2, step 2051/7134 completed (loss: 0.07944958657026291, acc: 0.978515625)
[2025-02-13 02:43:05,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:06,171][root][INFO] - Training Epoch: 1/2, step 2052/7134 completed (loss: 0.12276873737573624, acc: 0.9692832827568054)
[2025-02-13 02:43:06,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:06,567][root][INFO] - Training Epoch: 1/2, step 2053/7134 completed (loss: 0.0728374570608139, acc: 0.9779999852180481)
[2025-02-13 02:43:06,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:06,990][root][INFO] - Training Epoch: 1/2, step 2054/7134 completed (loss: 0.15309526026248932, acc: 0.9538216590881348)
[2025-02-13 02:43:07,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:07,413][root][INFO] - Training Epoch: 1/2, step 2055/7134 completed (loss: 0.12143374234437943, acc: 0.9716535210609436)
[2025-02-13 02:43:07,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:07,877][root][INFO] - Training Epoch: 1/2, step 2056/7134 completed (loss: 0.10804925858974457, acc: 0.9675745964050293)
[2025-02-13 02:43:08,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:08,295][root][INFO] - Training Epoch: 1/2, step 2057/7134 completed (loss: 0.12208712100982666, acc: 0.9660786986351013)
[2025-02-13 02:43:08,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:08,734][root][INFO] - Training Epoch: 1/2, step 2058/7134 completed (loss: 0.14166828989982605, acc: 0.9670014381408691)
[2025-02-13 02:43:08,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:09,152][root][INFO] - Training Epoch: 1/2, step 2059/7134 completed (loss: 0.14835621416568756, acc: 0.9653379321098328)
[2025-02-13 02:43:09,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:09,574][root][INFO] - Training Epoch: 1/2, step 2060/7134 completed (loss: 0.07786858081817627, acc: 0.9814019799232483)
[2025-02-13 02:43:09,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:10,000][root][INFO] - Training Epoch: 1/2, step 2061/7134 completed (loss: 0.10311432182788849, acc: 0.9716714024543762)
[2025-02-13 02:43:10,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:10,340][root][INFO] - Training Epoch: 1/2, step 2062/7134 completed (loss: 0.13967281579971313, acc: 0.9696394801139832)
[2025-02-13 02:43:10,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:10,724][root][INFO] - Training Epoch: 1/2, step 2063/7134 completed (loss: 0.12031589448451996, acc: 0.9664179086685181)
[2025-02-13 02:43:10,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:11,137][root][INFO] - Training Epoch: 1/2, step 2064/7134 completed (loss: 0.04345455393195152, acc: 0.9859374761581421)
[2025-02-13 02:43:11,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:11,533][root][INFO] - Training Epoch: 1/2, step 2065/7134 completed (loss: 0.10828984528779984, acc: 0.9748031497001648)
[2025-02-13 02:43:11,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:11,891][root][INFO] - Training Epoch: 1/2, step 2066/7134 completed (loss: 0.07993722707033157, acc: 0.9819639325141907)
[2025-02-13 02:43:12,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:12,288][root][INFO] - Training Epoch: 1/2, step 2067/7134 completed (loss: 0.07290273159742355, acc: 0.9791271090507507)
[2025-02-13 02:43:12,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:12,688][root][INFO] - Training Epoch: 1/2, step 2068/7134 completed (loss: 0.10641591995954514, acc: 0.9695550203323364)
[2025-02-13 02:43:12,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:13,124][root][INFO] - Training Epoch: 1/2, step 2069/7134 completed (loss: 0.07280776649713516, acc: 0.9784615635871887)
[2025-02-13 02:43:13,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:13,517][root][INFO] - Training Epoch: 1/2, step 2070/7134 completed (loss: 0.07860364019870758, acc: 0.9771863222122192)
[2025-02-13 02:43:13,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:13,915][root][INFO] - Training Epoch: 1/2, step 2071/7134 completed (loss: 0.050233691930770874, acc: 0.9906322956085205)
[2025-02-13 02:43:14,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:14,300][root][INFO] - Training Epoch: 1/2, step 2072/7134 completed (loss: 0.03699996694922447, acc: 0.9925742745399475)
[2025-02-13 02:43:14,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:14,685][root][INFO] - Training Epoch: 1/2, step 2073/7134 completed (loss: 0.06179337203502655, acc: 0.9865384697914124)
[2025-02-13 02:43:14,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:15,136][root][INFO] - Training Epoch: 1/2, step 2074/7134 completed (loss: 0.07899865508079529, acc: 0.9788732528686523)
[2025-02-13 02:43:15,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:15,530][root][INFO] - Training Epoch: 1/2, step 2075/7134 completed (loss: 0.14755870401859283, acc: 0.9565943479537964)
[2025-02-13 02:43:15,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:15,966][root][INFO] - Training Epoch: 1/2, step 2076/7134 completed (loss: 0.0784996822476387, acc: 0.9830769300460815)
[2025-02-13 02:43:16,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:16,395][root][INFO] - Training Epoch: 1/2, step 2077/7134 completed (loss: 0.05809067189693451, acc: 0.9814077019691467)
[2025-02-13 02:43:16,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:16,844][root][INFO] - Training Epoch: 1/2, step 2078/7134 completed (loss: 0.03821096569299698, acc: 0.9930232763290405)
[2025-02-13 02:43:16,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:17,278][root][INFO] - Training Epoch: 1/2, step 2079/7134 completed (loss: 0.04156043007969856, acc: 0.988095223903656)
[2025-02-13 02:43:17,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:17,683][root][INFO] - Training Epoch: 1/2, step 2080/7134 completed (loss: 0.06848443299531937, acc: 0.9808823466300964)
[2025-02-13 02:43:17,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:18,102][root][INFO] - Training Epoch: 1/2, step 2081/7134 completed (loss: 0.03218783810734749, acc: 0.9912917017936707)
[2025-02-13 02:43:18,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:18,542][root][INFO] - Training Epoch: 1/2, step 2082/7134 completed (loss: 0.1316496580839157, acc: 0.9685534834861755)
[2025-02-13 02:43:18,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:18,918][root][INFO] - Training Epoch: 1/2, step 2083/7134 completed (loss: 0.08208214491605759, acc: 0.9851852059364319)
[2025-02-13 02:43:19,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:19,326][root][INFO] - Training Epoch: 1/2, step 2084/7134 completed (loss: 0.04970519244670868, acc: 0.9875776171684265)
[2025-02-13 02:43:19,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:19,719][root][INFO] - Training Epoch: 1/2, step 2085/7134 completed (loss: 0.035762496292591095, acc: 0.9907578825950623)
[2025-02-13 02:43:19,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:20,085][root][INFO] - Training Epoch: 1/2, step 2086/7134 completed (loss: 0.06062322482466698, acc: 0.9817351698875427)
[2025-02-13 02:43:20,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:20,513][root][INFO] - Training Epoch: 1/2, step 2087/7134 completed (loss: 0.030218224972486496, acc: 0.9900426864624023)
[2025-02-13 02:43:20,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:20,938][root][INFO] - Training Epoch: 1/2, step 2088/7134 completed (loss: 0.07399408519268036, acc: 0.9798271059989929)
[2025-02-13 02:43:21,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:21,365][root][INFO] - Training Epoch: 1/2, step 2089/7134 completed (loss: 0.04743511974811554, acc: 0.9873816967010498)
[2025-02-13 02:43:21,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:21,756][root][INFO] - Training Epoch: 1/2, step 2090/7134 completed (loss: 0.035455089062452316, acc: 0.9936808943748474)
[2025-02-13 02:43:21,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:22,156][root][INFO] - Training Epoch: 1/2, step 2091/7134 completed (loss: 0.04580188915133476, acc: 0.9864864945411682)
[2025-02-13 02:43:22,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:22,600][root][INFO] - Training Epoch: 1/2, step 2092/7134 completed (loss: 0.02811521850526333, acc: 0.988959014415741)
[2025-02-13 02:43:22,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:23,063][root][INFO] - Training Epoch: 1/2, step 2093/7134 completed (loss: 0.0380694717168808, acc: 0.9918588995933533)
[2025-02-13 02:43:23,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:23,420][root][INFO] - Training Epoch: 1/2, step 2094/7134 completed (loss: 0.09722744673490524, acc: 0.9776875972747803)
[2025-02-13 02:43:23,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:23,853][root][INFO] - Training Epoch: 1/2, step 2095/7134 completed (loss: 0.06092366948723793, acc: 0.9881266355514526)
[2025-02-13 02:43:23,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:24,241][root][INFO] - Training Epoch: 1/2, step 2096/7134 completed (loss: 0.060212522745132446, acc: 0.982758641242981)
[2025-02-13 02:43:24,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:24,646][root][INFO] - Training Epoch: 1/2, step 2097/7134 completed (loss: 0.03833223879337311, acc: 0.991584837436676)
[2025-02-13 02:43:24,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:25,051][root][INFO] - Training Epoch: 1/2, step 2098/7134 completed (loss: 0.05691881105303764, acc: 0.9887459874153137)
[2025-02-13 02:43:25,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:25,441][root][INFO] - Training Epoch: 1/2, step 2099/7134 completed (loss: 0.057380590587854385, acc: 0.9798164963722229)
[2025-02-13 02:43:25,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:25,872][root][INFO] - Training Epoch: 1/2, step 2100/7134 completed (loss: 0.048248499631881714, acc: 0.9920318722724915)
[2025-02-13 02:43:26,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:26,290][root][INFO] - Training Epoch: 1/2, step 2101/7134 completed (loss: 0.05733390524983406, acc: 0.982367753982544)
[2025-02-13 02:43:26,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:26,689][root][INFO] - Training Epoch: 1/2, step 2102/7134 completed (loss: 0.07346589863300323, acc: 0.9732142686843872)
[2025-02-13 02:43:26,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:27,091][root][INFO] - Training Epoch: 1/2, step 2103/7134 completed (loss: 0.058068133890628815, acc: 0.9800613522529602)
[2025-02-13 02:43:27,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:27,531][root][INFO] - Training Epoch: 1/2, step 2104/7134 completed (loss: 0.07146835327148438, acc: 0.9780927896499634)
[2025-02-13 02:43:27,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:27,930][root][INFO] - Training Epoch: 1/2, step 2105/7134 completed (loss: 0.12754830718040466, acc: 0.9718309640884399)
[2025-02-13 02:43:28,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:28,337][root][INFO] - Training Epoch: 1/2, step 2106/7134 completed (loss: 0.09308674931526184, acc: 0.9755747318267822)
[2025-02-13 02:43:28,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:28,776][root][INFO] - Training Epoch: 1/2, step 2107/7134 completed (loss: 0.07433295249938965, acc: 0.9838056564331055)
[2025-02-13 02:43:28,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:29,214][root][INFO] - Training Epoch: 1/2, step 2108/7134 completed (loss: 0.05882088094949722, acc: 0.9820627570152283)
[2025-02-13 02:43:29,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:29,612][root][INFO] - Training Epoch: 1/2, step 2109/7134 completed (loss: 0.09963913261890411, acc: 0.9682539701461792)
[2025-02-13 02:43:29,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:30,066][root][INFO] - Training Epoch: 1/2, step 2110/7134 completed (loss: 0.09832781553268433, acc: 0.9752907156944275)
[2025-02-13 02:43:30,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:30,479][root][INFO] - Training Epoch: 1/2, step 2111/7134 completed (loss: 0.10099297761917114, acc: 0.9740596413612366)
[2025-02-13 02:43:30,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:30,850][root][INFO] - Training Epoch: 1/2, step 2112/7134 completed (loss: 0.12577566504478455, acc: 0.9665604829788208)
[2025-02-13 02:43:30,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:31,255][root][INFO] - Training Epoch: 1/2, step 2113/7134 completed (loss: 0.04056428372859955, acc: 0.9880775213241577)
[2025-02-13 02:43:31,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:31,667][root][INFO] - Training Epoch: 1/2, step 2114/7134 completed (loss: 0.03446284681558609, acc: 0.9882352948188782)
[2025-02-13 02:43:31,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:32,108][root][INFO] - Training Epoch: 1/2, step 2115/7134 completed (loss: 0.07440705597400665, acc: 0.9799749851226807)
[2025-02-13 02:43:32,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:32,517][root][INFO] - Training Epoch: 1/2, step 2116/7134 completed (loss: 0.08470410853624344, acc: 0.9831288456916809)
[2025-02-13 02:43:32,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:32,937][root][INFO] - Training Epoch: 1/2, step 2117/7134 completed (loss: 0.06577371060848236, acc: 0.9813432693481445)
[2025-02-13 02:43:33,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:33,349][root][INFO] - Training Epoch: 1/2, step 2118/7134 completed (loss: 0.051039185374975204, acc: 0.9890859723091125)
[2025-02-13 02:43:33,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:33,758][root][INFO] - Training Epoch: 1/2, step 2119/7134 completed (loss: 0.07017521560192108, acc: 0.9737532734870911)
[2025-02-13 02:43:33,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:34,160][root][INFO] - Training Epoch: 1/2, step 2120/7134 completed (loss: 0.07327532768249512, acc: 0.9802131056785583)
[2025-02-13 02:43:34,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:34,532][root][INFO] - Training Epoch: 1/2, step 2121/7134 completed (loss: 0.08429855853319168, acc: 0.9793650507926941)
[2025-02-13 02:43:34,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:34,952][root][INFO] - Training Epoch: 1/2, step 2122/7134 completed (loss: 0.04064815863966942, acc: 0.9908854365348816)
[2025-02-13 02:43:35,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:35,350][root][INFO] - Training Epoch: 1/2, step 2123/7134 completed (loss: 0.0527268722653389, acc: 0.9842767119407654)
[2025-02-13 02:43:35,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:35,741][root][INFO] - Training Epoch: 1/2, step 2124/7134 completed (loss: 0.050685152411460876, acc: 0.9844236969947815)
[2025-02-13 02:43:35,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:36,130][root][INFO] - Training Epoch: 1/2, step 2125/7134 completed (loss: 0.03819590061903, acc: 0.989393949508667)
[2025-02-13 02:43:36,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:36,553][root][INFO] - Training Epoch: 1/2, step 2126/7134 completed (loss: 0.0403900071978569, acc: 0.9848739504814148)
[2025-02-13 02:43:36,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:36,947][root][INFO] - Training Epoch: 1/2, step 2127/7134 completed (loss: 0.05246635153889656, acc: 0.983775794506073)
[2025-02-13 02:43:37,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:37,378][root][INFO] - Training Epoch: 1/2, step 2128/7134 completed (loss: 0.06663788855075836, acc: 0.9790025949478149)
[2025-02-13 02:43:37,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:37,790][root][INFO] - Training Epoch: 1/2, step 2129/7134 completed (loss: 0.07490861415863037, acc: 0.9773755669593811)
[2025-02-13 02:43:37,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:38,186][root][INFO] - Training Epoch: 1/2, step 2130/7134 completed (loss: 0.0646895170211792, acc: 0.9851484894752502)
[2025-02-13 02:43:38,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:38,559][root][INFO] - Training Epoch: 1/2, step 2131/7134 completed (loss: 0.0351191870868206, acc: 0.9876161217689514)
[2025-02-13 02:43:38,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:38,965][root][INFO] - Training Epoch: 1/2, step 2132/7134 completed (loss: 0.03501848131418228, acc: 0.9892183542251587)
[2025-02-13 02:43:39,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:39,377][root][INFO] - Training Epoch: 1/2, step 2133/7134 completed (loss: 0.05829589068889618, acc: 0.9815059304237366)
[2025-02-13 02:43:39,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:39,794][root][INFO] - Training Epoch: 1/2, step 2134/7134 completed (loss: 0.04280771687626839, acc: 0.9892904758453369)
[2025-02-13 02:43:39,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:40,188][root][INFO] - Training Epoch: 1/2, step 2135/7134 completed (loss: 0.06132648140192032, acc: 0.9774096608161926)
[2025-02-13 02:43:40,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:40,607][root][INFO] - Training Epoch: 1/2, step 2136/7134 completed (loss: 0.044881671667099, acc: 0.9879840016365051)
[2025-02-13 02:43:40,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:41,053][root][INFO] - Training Epoch: 1/2, step 2137/7134 completed (loss: 0.02872944436967373, acc: 0.9909677505493164)
[2025-02-13 02:43:41,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:41,444][root][INFO] - Training Epoch: 1/2, step 2138/7134 completed (loss: 0.02386852726340294, acc: 0.9893292784690857)
[2025-02-13 02:43:41,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:41,856][root][INFO] - Training Epoch: 1/2, step 2139/7134 completed (loss: 0.04059857502579689, acc: 0.9916782379150391)
[2025-02-13 02:43:42,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:42,325][root][INFO] - Training Epoch: 1/2, step 2140/7134 completed (loss: 0.07970599830150604, acc: 0.9793388247489929)
[2025-02-13 02:43:42,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:42,627][root][INFO] - Training Epoch: 1/2, step 2141/7134 completed (loss: 0.04962393641471863, acc: 0.9853801131248474)
[2025-02-13 02:43:42,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:43,069][root][INFO] - Training Epoch: 1/2, step 2142/7134 completed (loss: 0.0655631348490715, acc: 0.9830028414726257)
[2025-02-13 02:43:43,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:43,506][root][INFO] - Training Epoch: 1/2, step 2143/7134 completed (loss: 0.07129501551389694, acc: 0.9824561476707458)
[2025-02-13 02:43:43,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:43,954][root][INFO] - Training Epoch: 1/2, step 2144/7134 completed (loss: 0.028865238651633263, acc: 0.9960159659385681)
[2025-02-13 02:43:44,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:44,392][root][INFO] - Training Epoch: 1/2, step 2145/7134 completed (loss: 0.10073493421077728, acc: 0.9771101474761963)
[2025-02-13 02:43:44,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:44,784][root][INFO] - Training Epoch: 1/2, step 2146/7134 completed (loss: 0.07867904752492905, acc: 0.9771126508712769)
[2025-02-13 02:43:44,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:45,245][root][INFO] - Training Epoch: 1/2, step 2147/7134 completed (loss: 0.07043776661157608, acc: 0.9850373864173889)
[2025-02-13 02:43:45,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:45,658][root][INFO] - Training Epoch: 1/2, step 2148/7134 completed (loss: 0.07930674403905869, acc: 0.9825327396392822)
[2025-02-13 02:43:45,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:46,081][root][INFO] - Training Epoch: 1/2, step 2149/7134 completed (loss: 0.03589805215597153, acc: 0.9901408553123474)
[2025-02-13 02:43:46,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:46,485][root][INFO] - Training Epoch: 1/2, step 2150/7134 completed (loss: 0.05438452586531639, acc: 0.981333315372467)
[2025-02-13 02:43:46,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:46,897][root][INFO] - Training Epoch: 1/2, step 2151/7134 completed (loss: 0.053336251527071, acc: 0.9842857122421265)
[2025-02-13 02:43:47,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:47,309][root][INFO] - Training Epoch: 1/2, step 2152/7134 completed (loss: 0.13559693098068237, acc: 0.9632353186607361)
[2025-02-13 02:43:47,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:47,926][root][INFO] - Training Epoch: 1/2, step 2153/7134 completed (loss: 0.06402622163295746, acc: 0.985571563243866)
[2025-02-13 02:43:48,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:48,405][root][INFO] - Training Epoch: 1/2, step 2154/7134 completed (loss: 0.08800759166479111, acc: 0.9772440195083618)
[2025-02-13 02:43:48,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:48,771][root][INFO] - Training Epoch: 1/2, step 2155/7134 completed (loss: 0.12882551550865173, acc: 0.9752747416496277)
[2025-02-13 02:43:48,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:49,166][root][INFO] - Training Epoch: 1/2, step 2156/7134 completed (loss: 0.07675597071647644, acc: 0.9739583134651184)
[2025-02-13 02:43:49,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:49,620][root][INFO] - Training Epoch: 1/2, step 2157/7134 completed (loss: 0.07235032320022583, acc: 0.9743016958236694)
[2025-02-13 02:43:49,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:50,081][root][INFO] - Training Epoch: 1/2, step 2158/7134 completed (loss: 0.0999193862080574, acc: 0.976190447807312)
[2025-02-13 02:43:50,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:50,519][root][INFO] - Training Epoch: 1/2, step 2159/7134 completed (loss: 0.06129726767539978, acc: 0.9885057210922241)
[2025-02-13 02:43:50,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:50,965][root][INFO] - Training Epoch: 1/2, step 2160/7134 completed (loss: 0.08287634700536728, acc: 0.9752407073974609)
[2025-02-13 02:43:51,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:51,411][root][INFO] - Training Epoch: 1/2, step 2161/7134 completed (loss: 0.09967400133609772, acc: 0.9795134663581848)
[2025-02-13 02:43:51,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:51,840][root][INFO] - Training Epoch: 1/2, step 2162/7134 completed (loss: 0.054782070219516754, acc: 0.9878970980644226)
[2025-02-13 02:43:51,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:52,295][root][INFO] - Training Epoch: 1/2, step 2163/7134 completed (loss: 0.07222327589988708, acc: 0.9795275330543518)
[2025-02-13 02:43:52,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:52,696][root][INFO] - Training Epoch: 1/2, step 2164/7134 completed (loss: 0.06408669799566269, acc: 0.9862778782844543)
[2025-02-13 02:43:52,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:53,144][root][INFO] - Training Epoch: 1/2, step 2165/7134 completed (loss: 0.055460136383771896, acc: 0.9842180609703064)
[2025-02-13 02:43:53,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:53,570][root][INFO] - Training Epoch: 1/2, step 2166/7134 completed (loss: 0.07447526603937149, acc: 0.9778645634651184)
[2025-02-13 02:43:53,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:53,960][root][INFO] - Training Epoch: 1/2, step 2167/7134 completed (loss: 0.06017766892910004, acc: 0.9844098091125488)
[2025-02-13 02:43:54,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:54,372][root][INFO] - Training Epoch: 1/2, step 2168/7134 completed (loss: 0.043214842677116394, acc: 0.9904761910438538)
[2025-02-13 02:43:54,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:54,811][root][INFO] - Training Epoch: 1/2, step 2169/7134 completed (loss: 0.06296350061893463, acc: 0.9837586879730225)
[2025-02-13 02:43:54,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:55,231][root][INFO] - Training Epoch: 1/2, step 2170/7134 completed (loss: 0.06031397730112076, acc: 0.9868995547294617)
[2025-02-13 02:43:55,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:55,685][root][INFO] - Training Epoch: 1/2, step 2171/7134 completed (loss: 0.035936158150434494, acc: 0.9909399747848511)
[2025-02-13 02:43:55,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:56,146][root][INFO] - Training Epoch: 1/2, step 2172/7134 completed (loss: 0.04192347079515457, acc: 0.9886363744735718)
[2025-02-13 02:43:56,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:56,575][root][INFO] - Training Epoch: 1/2, step 2173/7134 completed (loss: 0.06653988361358643, acc: 0.9864048361778259)
[2025-02-13 02:43:56,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:56,997][root][INFO] - Training Epoch: 1/2, step 2174/7134 completed (loss: 0.04036494344472885, acc: 0.9900709390640259)
[2025-02-13 02:43:57,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:57,423][root][INFO] - Training Epoch: 1/2, step 2175/7134 completed (loss: 0.10837287455797195, acc: 0.9819944500923157)
[2025-02-13 02:43:57,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:57,901][root][INFO] - Training Epoch: 1/2, step 2176/7134 completed (loss: 0.06211499124765396, acc: 0.9845971465110779)
[2025-02-13 02:43:58,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:58,342][root][INFO] - Training Epoch: 1/2, step 2177/7134 completed (loss: 0.06623717397451401, acc: 0.984415590763092)
[2025-02-13 02:43:58,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:58,772][root][INFO] - Training Epoch: 1/2, step 2178/7134 completed (loss: 0.041232846677303314, acc: 0.9867549538612366)
[2025-02-13 02:43:58,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:59,216][root][INFO] - Training Epoch: 1/2, step 2179/7134 completed (loss: 0.04173710569739342, acc: 0.9873417615890503)
[2025-02-13 02:43:59,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:59,668][root][INFO] - Training Epoch: 1/2, step 2180/7134 completed (loss: 0.07019716501235962, acc: 0.9822695255279541)
[2025-02-13 02:43:59,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:00,132][root][INFO] - Training Epoch: 1/2, step 2181/7134 completed (loss: 0.028759626671671867, acc: 0.9928469061851501)
[2025-02-13 02:44:00,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:00,567][root][INFO] - Training Epoch: 1/2, step 2182/7134 completed (loss: 0.08412669599056244, acc: 0.9780853390693665)
[2025-02-13 02:44:00,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:01,047][root][INFO] - Training Epoch: 1/2, step 2183/7134 completed (loss: 0.05937085673213005, acc: 0.9802095293998718)
[2025-02-13 02:44:01,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:01,492][root][INFO] - Training Epoch: 1/2, step 2184/7134 completed (loss: 0.0468713715672493, acc: 0.9936102032661438)
[2025-02-13 02:44:01,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:01,922][root][INFO] - Training Epoch: 1/2, step 2185/7134 completed (loss: 0.08319951593875885, acc: 0.9830028414726257)
[2025-02-13 02:44:02,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:02,362][root][INFO] - Training Epoch: 1/2, step 2186/7134 completed (loss: 0.1118365153670311, acc: 0.9724473357200623)
[2025-02-13 02:44:02,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:02,784][root][INFO] - Training Epoch: 1/2, step 2187/7134 completed (loss: 0.1376023292541504, acc: 0.9728000164031982)
[2025-02-13 02:44:02,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:03,232][root][INFO] - Training Epoch: 1/2, step 2188/7134 completed (loss: 0.05329349264502525, acc: 0.9822161197662354)
[2025-02-13 02:44:03,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:03,634][root][INFO] - Training Epoch: 1/2, step 2189/7134 completed (loss: 0.0830911174416542, acc: 0.9816513657569885)
[2025-02-13 02:44:03,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:04,075][root][INFO] - Training Epoch: 1/2, step 2190/7134 completed (loss: 0.07796233147382736, acc: 0.9749340415000916)
[2025-02-13 02:44:04,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:04,434][root][INFO] - Training Epoch: 1/2, step 2191/7134 completed (loss: 0.09440615773200989, acc: 0.9647696614265442)
[2025-02-13 02:44:04,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:04,825][root][INFO] - Training Epoch: 1/2, step 2192/7134 completed (loss: 0.03419389948248863, acc: 0.9887005686759949)
[2025-02-13 02:44:04,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:05,249][root][INFO] - Training Epoch: 1/2, step 2193/7134 completed (loss: 0.05581029877066612, acc: 0.9827127456665039)
[2025-02-13 02:44:05,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:05,654][root][INFO] - Training Epoch: 1/2, step 2194/7134 completed (loss: 0.05433408170938492, acc: 0.9849246144294739)
[2025-02-13 02:44:05,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:06,075][root][INFO] - Training Epoch: 1/2, step 2195/7134 completed (loss: 0.06149277836084366, acc: 0.984000027179718)
[2025-02-13 02:44:06,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:06,512][root][INFO] - Training Epoch: 1/2, step 2196/7134 completed (loss: 0.06619994342327118, acc: 0.9792746305465698)
[2025-02-13 02:44:06,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:06,896][root][INFO] - Training Epoch: 1/2, step 2197/7134 completed (loss: 0.06705502420663834, acc: 0.9781420826911926)
[2025-02-13 02:44:07,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:07,312][root][INFO] - Training Epoch: 1/2, step 2198/7134 completed (loss: 0.09885641187429428, acc: 0.9743589758872986)
[2025-02-13 02:44:07,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:07,724][root][INFO] - Training Epoch: 1/2, step 2199/7134 completed (loss: 0.05206593871116638, acc: 0.9803625345230103)
[2025-02-13 02:44:07,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:08,141][root][INFO] - Training Epoch: 1/2, step 2200/7134 completed (loss: 0.053813353180885315, acc: 0.9819168448448181)
[2025-02-13 02:44:08,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:08,561][root][INFO] - Training Epoch: 1/2, step 2201/7134 completed (loss: 0.03945806622505188, acc: 0.9848484992980957)
[2025-02-13 02:44:08,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:08,971][root][INFO] - Training Epoch: 1/2, step 2202/7134 completed (loss: 0.07023901492357254, acc: 0.9782971739768982)
[2025-02-13 02:44:09,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:09,414][root][INFO] - Training Epoch: 1/2, step 2203/7134 completed (loss: 0.039731938391923904, acc: 0.9890410900115967)
[2025-02-13 02:44:09,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:09,812][root][INFO] - Training Epoch: 1/2, step 2204/7134 completed (loss: 0.040964510291814804, acc: 0.9853801131248474)
[2025-02-13 02:44:09,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:10,226][root][INFO] - Training Epoch: 1/2, step 2205/7134 completed (loss: 0.04479724168777466, acc: 0.9859648942947388)
[2025-02-13 02:44:10,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:10,624][root][INFO] - Training Epoch: 1/2, step 2206/7134 completed (loss: 0.10333037376403809, acc: 0.9718804955482483)
[2025-02-13 02:44:10,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:11,028][root][INFO] - Training Epoch: 1/2, step 2207/7134 completed (loss: 0.04397626593708992, acc: 0.9919224381446838)
[2025-02-13 02:44:11,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:11,429][root][INFO] - Training Epoch: 1/2, step 2208/7134 completed (loss: 0.07671336829662323, acc: 0.9801653027534485)
[2025-02-13 02:44:11,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:11,837][root][INFO] - Training Epoch: 1/2, step 2209/7134 completed (loss: 0.06785273551940918, acc: 0.987270176410675)
[2025-02-13 02:44:11,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:12,235][root][INFO] - Training Epoch: 1/2, step 2210/7134 completed (loss: 0.07470902800559998, acc: 0.9772382378578186)
[2025-02-13 02:44:12,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:12,653][root][INFO] - Training Epoch: 1/2, step 2211/7134 completed (loss: 0.05013114586472511, acc: 0.9861496090888977)
[2025-02-13 02:44:12,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:13,073][root][INFO] - Training Epoch: 1/2, step 2212/7134 completed (loss: 0.023819169029593468, acc: 0.9930555820465088)
[2025-02-13 02:44:13,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:13,487][root][INFO] - Training Epoch: 1/2, step 2213/7134 completed (loss: 0.042160917073488235, acc: 0.9830508232116699)
[2025-02-13 02:44:13,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:13,895][root][INFO] - Training Epoch: 1/2, step 2214/7134 completed (loss: 0.06762583553791046, acc: 0.9820895791053772)
[2025-02-13 02:44:14,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:14,298][root][INFO] - Training Epoch: 1/2, step 2215/7134 completed (loss: 0.03468063846230507, acc: 0.9901685118675232)
[2025-02-13 02:44:14,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:14,712][root][INFO] - Training Epoch: 1/2, step 2216/7134 completed (loss: 0.023022376000881195, acc: 0.9946595430374146)
[2025-02-13 02:44:14,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:15,136][root][INFO] - Training Epoch: 1/2, step 2217/7134 completed (loss: 0.04191729053854942, acc: 0.9850560426712036)
[2025-02-13 02:44:15,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:15,532][root][INFO] - Training Epoch: 1/2, step 2218/7134 completed (loss: 0.05253821983933449, acc: 0.9885057210922241)
[2025-02-13 02:44:15,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:15,928][root][INFO] - Training Epoch: 1/2, step 2219/7134 completed (loss: 0.026337070390582085, acc: 0.9907264113426208)
[2025-02-13 02:44:16,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:16,328][root][INFO] - Training Epoch: 1/2, step 2220/7134 completed (loss: 0.032832708209753036, acc: 0.9858490824699402)
[2025-02-13 02:44:16,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:16,763][root][INFO] - Training Epoch: 1/2, step 2221/7134 completed (loss: 0.05271846801042557, acc: 0.9896103739738464)
[2025-02-13 02:44:16,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:17,167][root][INFO] - Training Epoch: 1/2, step 2222/7134 completed (loss: 0.05855535343289375, acc: 0.9826589822769165)
[2025-02-13 02:44:17,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:17,572][root][INFO] - Training Epoch: 1/2, step 2223/7134 completed (loss: 0.05106281861662865, acc: 0.9832636117935181)
[2025-02-13 02:44:17,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:18,000][root][INFO] - Training Epoch: 1/2, step 2224/7134 completed (loss: 0.05412605032324791, acc: 0.9849108457565308)
[2025-02-13 02:44:18,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:18,414][root][INFO] - Training Epoch: 1/2, step 2225/7134 completed (loss: 0.042014941573143005, acc: 0.986975371837616)
[2025-02-13 02:44:18,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:18,828][root][INFO] - Training Epoch: 1/2, step 2226/7134 completed (loss: 0.04177513346076012, acc: 0.9832572340965271)
[2025-02-13 02:44:18,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:19,239][root][INFO] - Training Epoch: 1/2, step 2227/7134 completed (loss: 0.034613557159900665, acc: 0.9926793575286865)
[2025-02-13 02:44:19,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:19,623][root][INFO] - Training Epoch: 1/2, step 2228/7134 completed (loss: 0.03941618278622627, acc: 0.9837398529052734)
[2025-02-13 02:44:19,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:20,030][root][INFO] - Training Epoch: 1/2, step 2229/7134 completed (loss: 0.04645083472132683, acc: 0.9900000095367432)
[2025-02-13 02:44:20,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:20,416][root][INFO] - Training Epoch: 1/2, step 2230/7134 completed (loss: 0.03120037168264389, acc: 0.991909384727478)
[2025-02-13 02:44:20,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:20,803][root][INFO] - Training Epoch: 1/2, step 2231/7134 completed (loss: 0.06063256785273552, acc: 0.9820716977119446)
[2025-02-13 02:44:20,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:21,220][root][INFO] - Training Epoch: 1/2, step 2232/7134 completed (loss: 0.019352195784449577, acc: 0.9959893226623535)
[2025-02-13 02:44:21,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:21,632][root][INFO] - Training Epoch: 1/2, step 2233/7134 completed (loss: 0.04719172790646553, acc: 0.9886202216148376)
[2025-02-13 02:44:21,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:22,065][root][INFO] - Training Epoch: 1/2, step 2234/7134 completed (loss: 0.023993486538529396, acc: 0.994535505771637)
[2025-02-13 02:44:22,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:22,480][root][INFO] - Training Epoch: 1/2, step 2235/7134 completed (loss: 0.06781645119190216, acc: 0.9890282154083252)
[2025-02-13 02:44:22,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:22,874][root][INFO] - Training Epoch: 1/2, step 2236/7134 completed (loss: 0.034998904913663864, acc: 0.9850075244903564)
[2025-02-13 02:44:23,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:23,303][root][INFO] - Training Epoch: 1/2, step 2237/7134 completed (loss: 0.04920360445976257, acc: 0.9830268621444702)
[2025-02-13 02:44:23,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:23,756][root][INFO] - Training Epoch: 1/2, step 2238/7134 completed (loss: 0.09595649689435959, acc: 0.9739508032798767)
[2025-02-13 02:44:23,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:24,198][root][INFO] - Training Epoch: 1/2, step 2239/7134 completed (loss: 0.10847272723913193, acc: 0.9727272987365723)
[2025-02-13 02:44:24,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:24,633][root][INFO] - Training Epoch: 1/2, step 2240/7134 completed (loss: 0.08387075364589691, acc: 0.9818181991577148)
[2025-02-13 02:44:24,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:25,013][root][INFO] - Training Epoch: 1/2, step 2241/7134 completed (loss: 0.11602950096130371, acc: 0.9692671298980713)
[2025-02-13 02:44:25,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:25,380][root][INFO] - Training Epoch: 1/2, step 2242/7134 completed (loss: 0.15516535937786102, acc: 0.9643705487251282)
[2025-02-13 02:44:25,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:25,824][root][INFO] - Training Epoch: 1/2, step 2243/7134 completed (loss: 0.18277274072170258, acc: 0.9557251930236816)
[2025-02-13 02:44:25,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:26,270][root][INFO] - Training Epoch: 1/2, step 2244/7134 completed (loss: 0.07590775191783905, acc: 0.9685534834861755)
[2025-02-13 02:44:26,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:26,662][root][INFO] - Training Epoch: 1/2, step 2245/7134 completed (loss: 0.1367776244878769, acc: 0.9579287767410278)
[2025-02-13 02:44:26,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:27,107][root][INFO] - Training Epoch: 1/2, step 2246/7134 completed (loss: 0.17920690774917603, acc: 0.9445946216583252)
[2025-02-13 02:44:27,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:27,522][root][INFO] - Training Epoch: 1/2, step 2247/7134 completed (loss: 0.12588588893413544, acc: 0.9655172228813171)
[2025-02-13 02:44:27,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:27,953][root][INFO] - Training Epoch: 1/2, step 2248/7134 completed (loss: 0.10529250651597977, acc: 0.9806362390518188)
[2025-02-13 02:44:28,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:28,388][root][INFO] - Training Epoch: 1/2, step 2249/7134 completed (loss: 0.0800449550151825, acc: 0.9706704020500183)
[2025-02-13 02:44:28,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:28,820][root][INFO] - Training Epoch: 1/2, step 2250/7134 completed (loss: 0.09988632053136826, acc: 0.970588207244873)
[2025-02-13 02:44:28,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:29,203][root][INFO] - Training Epoch: 1/2, step 2251/7134 completed (loss: 0.06877855956554413, acc: 0.9773242473602295)
[2025-02-13 02:44:29,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:29,606][root][INFO] - Training Epoch: 1/2, step 2252/7134 completed (loss: 0.09268501400947571, acc: 0.9789915680885315)
[2025-02-13 02:44:29,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:29,992][root][INFO] - Training Epoch: 1/2, step 2253/7134 completed (loss: 0.07624952495098114, acc: 0.97826087474823)
[2025-02-13 02:44:30,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:30,370][root][INFO] - Training Epoch: 1/2, step 2254/7134 completed (loss: 0.1520271897315979, acc: 0.9482071995735168)
[2025-02-13 02:44:30,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:30,793][root][INFO] - Training Epoch: 1/2, step 2255/7134 completed (loss: 0.0629008412361145, acc: 0.9833080172538757)
[2025-02-13 02:44:30,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:31,131][root][INFO] - Training Epoch: 1/2, step 2256/7134 completed (loss: 0.1855425089597702, acc: 0.9424778819084167)
[2025-02-13 02:44:31,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:31,546][root][INFO] - Training Epoch: 1/2, step 2257/7134 completed (loss: 0.061366114765405655, acc: 0.9833837151527405)
[2025-02-13 02:44:31,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:31,949][root][INFO] - Training Epoch: 1/2, step 2258/7134 completed (loss: 0.07440921664237976, acc: 0.9769230484962463)
[2025-02-13 02:44:32,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:32,330][root][INFO] - Training Epoch: 1/2, step 2259/7134 completed (loss: 0.04905666410923004, acc: 0.9844444394111633)
[2025-02-13 02:44:32,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:32,727][root][INFO] - Training Epoch: 1/2, step 2260/7134 completed (loss: 0.08563117682933807, acc: 0.978787899017334)
[2025-02-13 02:44:32,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:33,089][root][INFO] - Training Epoch: 1/2, step 2261/7134 completed (loss: 0.1271073967218399, acc: 0.9616788029670715)
[2025-02-13 02:44:33,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:33,469][root][INFO] - Training Epoch: 1/2, step 2262/7134 completed (loss: 0.04521191120147705, acc: 0.9815837740898132)
[2025-02-13 02:44:33,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:33,869][root][INFO] - Training Epoch: 1/2, step 2263/7134 completed (loss: 0.08559446781873703, acc: 0.9783783555030823)
[2025-02-13 02:44:33,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:34,264][root][INFO] - Training Epoch: 1/2, step 2264/7134 completed (loss: 0.1040656790137291, acc: 0.9732770919799805)
[2025-02-13 02:44:34,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:34,688][root][INFO] - Training Epoch: 1/2, step 2265/7134 completed (loss: 0.1189773678779602, acc: 0.9673539400100708)
[2025-02-13 02:44:34,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:35,105][root][INFO] - Training Epoch: 1/2, step 2266/7134 completed (loss: 0.07343093305826187, acc: 0.9839141964912415)
[2025-02-13 02:44:35,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:35,504][root][INFO] - Training Epoch: 1/2, step 2267/7134 completed (loss: 0.031151311472058296, acc: 0.9958847761154175)
[2025-02-13 02:44:35,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:35,953][root][INFO] - Training Epoch: 1/2, step 2268/7134 completed (loss: 0.04893533140420914, acc: 0.9856528043746948)
[2025-02-13 02:44:36,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:36,393][root][INFO] - Training Epoch: 1/2, step 2269/7134 completed (loss: 0.031626954674720764, acc: 0.9911949634552002)
[2025-02-13 02:44:36,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:36,800][root][INFO] - Training Epoch: 1/2, step 2270/7134 completed (loss: 0.08593706041574478, acc: 0.9684908986091614)
[2025-02-13 02:44:36,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:37,264][root][INFO] - Training Epoch: 1/2, step 2271/7134 completed (loss: 0.08608914166688919, acc: 0.970080554485321)
[2025-02-13 02:44:37,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:37,695][root][INFO] - Training Epoch: 1/2, step 2272/7134 completed (loss: 0.05858729034662247, acc: 0.9865525960922241)
[2025-02-13 02:44:37,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:38,124][root][INFO] - Training Epoch: 1/2, step 2273/7134 completed (loss: 0.03435450419783592, acc: 0.9880239367485046)
[2025-02-13 02:44:38,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:38,568][root][INFO] - Training Epoch: 1/2, step 2274/7134 completed (loss: 0.14522825181484222, acc: 0.9707006216049194)
[2025-02-13 02:44:38,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:39,004][root][INFO] - Training Epoch: 1/2, step 2275/7134 completed (loss: 0.045057229697704315, acc: 0.98591548204422)
[2025-02-13 02:44:39,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:39,464][root][INFO] - Training Epoch: 1/2, step 2276/7134 completed (loss: 0.05250241979956627, acc: 0.9893364906311035)
[2025-02-13 02:44:39,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:39,917][root][INFO] - Training Epoch: 1/2, step 2277/7134 completed (loss: 0.07015778124332428, acc: 0.980369508266449)
[2025-02-13 02:44:40,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:40,343][root][INFO] - Training Epoch: 1/2, step 2278/7134 completed (loss: 0.15427415072917938, acc: 0.969648540019989)
[2025-02-13 02:44:40,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:40,781][root][INFO] - Training Epoch: 1/2, step 2279/7134 completed (loss: 0.060022395104169846, acc: 0.9811097979545593)
[2025-02-13 02:44:40,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:41,229][root][INFO] - Training Epoch: 1/2, step 2280/7134 completed (loss: 0.09032189100980759, acc: 0.9775679111480713)
[2025-02-13 02:44:41,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:41,634][root][INFO] - Training Epoch: 1/2, step 2281/7134 completed (loss: 0.12955018877983093, acc: 0.9746666550636292)
[2025-02-13 02:44:41,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:42,075][root][INFO] - Training Epoch: 1/2, step 2282/7134 completed (loss: 0.022821327671408653, acc: 0.9962916970252991)
[2025-02-13 02:44:42,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:42,511][root][INFO] - Training Epoch: 1/2, step 2283/7134 completed (loss: 0.06407184898853302, acc: 0.97555011510849)
[2025-02-13 02:44:42,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:42,934][root][INFO] - Training Epoch: 1/2, step 2284/7134 completed (loss: 0.060592904686927795, acc: 0.9807976484298706)
[2025-02-13 02:44:43,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:43,401][root][INFO] - Training Epoch: 1/2, step 2285/7134 completed (loss: 0.058747537434101105, acc: 0.9777034521102905)
[2025-02-13 02:44:43,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:43,837][root][INFO] - Training Epoch: 1/2, step 2286/7134 completed (loss: 0.09523255378007889, acc: 0.9727626442909241)
[2025-02-13 02:44:43,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:44,308][root][INFO] - Training Epoch: 1/2, step 2287/7134 completed (loss: 0.04989015683531761, acc: 0.9812332391738892)
[2025-02-13 02:44:44,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:44,766][root][INFO] - Training Epoch: 1/2, step 2288/7134 completed (loss: 0.04736209660768509, acc: 0.9893993139266968)
[2025-02-13 02:44:44,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:45,175][root][INFO] - Training Epoch: 1/2, step 2289/7134 completed (loss: 0.07079611718654633, acc: 0.9833119511604309)
[2025-02-13 02:44:45,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:45,596][root][INFO] - Training Epoch: 1/2, step 2290/7134 completed (loss: 0.05818162485957146, acc: 0.9834395051002502)
[2025-02-13 02:44:45,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:46,029][root][INFO] - Training Epoch: 1/2, step 2291/7134 completed (loss: 0.09988659620285034, acc: 0.9689608812332153)
[2025-02-13 02:44:46,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:46,457][root][INFO] - Training Epoch: 1/2, step 2292/7134 completed (loss: 0.12163815647363663, acc: 0.9767742156982422)
[2025-02-13 02:44:46,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:46,918][root][INFO] - Training Epoch: 1/2, step 2293/7134 completed (loss: 0.07577493041753769, acc: 0.9755011200904846)
[2025-02-13 02:44:47,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:47,352][root][INFO] - Training Epoch: 1/2, step 2294/7134 completed (loss: 0.06825757771730423, acc: 0.9820022583007812)
[2025-02-13 02:44:47,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:47,853][root][INFO] - Training Epoch: 1/2, step 2295/7134 completed (loss: 0.05938339978456497, acc: 0.9800570011138916)
[2025-02-13 02:44:47,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:48,307][root][INFO] - Training Epoch: 1/2, step 2296/7134 completed (loss: 0.039607059210538864, acc: 0.9893617033958435)
[2025-02-13 02:44:48,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:48,776][root][INFO] - Training Epoch: 1/2, step 2297/7134 completed (loss: 0.048800237476825714, acc: 0.9847561120986938)
[2025-02-13 02:44:48,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:49,218][root][INFO] - Training Epoch: 1/2, step 2298/7134 completed (loss: 0.09145668894052505, acc: 0.9726603627204895)
[2025-02-13 02:44:49,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:49,674][root][INFO] - Training Epoch: 1/2, step 2299/7134 completed (loss: 0.0483667366206646, acc: 0.9817975163459778)
[2025-02-13 02:44:49,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:50,141][root][INFO] - Training Epoch: 1/2, step 2300/7134 completed (loss: 0.023884519934654236, acc: 0.990980863571167)
[2025-02-13 02:44:50,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:50,597][root][INFO] - Training Epoch: 1/2, step 2301/7134 completed (loss: 0.07411409914493561, acc: 0.9761589169502258)
[2025-02-13 02:44:50,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:51,050][root][INFO] - Training Epoch: 1/2, step 2302/7134 completed (loss: 0.07814191281795502, acc: 0.9767156839370728)
[2025-02-13 02:44:51,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:51,473][root][INFO] - Training Epoch: 1/2, step 2303/7134 completed (loss: 0.05730360001325607, acc: 0.980637788772583)
[2025-02-13 02:44:51,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:51,930][root][INFO] - Training Epoch: 1/2, step 2304/7134 completed (loss: 0.08657228946685791, acc: 0.9753340482711792)
[2025-02-13 02:44:52,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:52,398][root][INFO] - Training Epoch: 1/2, step 2305/7134 completed (loss: 0.06281857937574387, acc: 0.9776358008384705)
[2025-02-13 02:44:52,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:52,842][root][INFO] - Training Epoch: 1/2, step 2306/7134 completed (loss: 0.04635602608323097, acc: 0.9855875968933105)
[2025-02-13 02:44:52,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:53,301][root][INFO] - Training Epoch: 1/2, step 2307/7134 completed (loss: 0.05882301926612854, acc: 0.9865702390670776)
[2025-02-13 02:44:53,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:53,759][root][INFO] - Training Epoch: 1/2, step 2308/7134 completed (loss: 0.08668570965528488, acc: 0.9775280952453613)
[2025-02-13 02:44:53,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:54,186][root][INFO] - Training Epoch: 1/2, step 2309/7134 completed (loss: 0.040372323244810104, acc: 0.9904076457023621)
[2025-02-13 02:44:54,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:54,620][root][INFO] - Training Epoch: 1/2, step 2310/7134 completed (loss: 0.04574696347117424, acc: 0.9864406585693359)
[2025-02-13 02:44:54,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:55,055][root][INFO] - Training Epoch: 1/2, step 2311/7134 completed (loss: 0.05594801530241966, acc: 0.9862385392189026)
[2025-02-13 02:44:55,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:55,513][root][INFO] - Training Epoch: 1/2, step 2312/7134 completed (loss: 0.03444042429327965, acc: 0.988120973110199)
[2025-02-13 02:44:55,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:55,945][root][INFO] - Training Epoch: 1/2, step 2313/7134 completed (loss: 0.0849807858467102, acc: 0.9805936217308044)
[2025-02-13 02:44:56,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:56,413][root][INFO] - Training Epoch: 1/2, step 2314/7134 completed (loss: 0.03282054513692856, acc: 0.9922308325767517)
[2025-02-13 02:44:56,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:56,876][root][INFO] - Training Epoch: 1/2, step 2315/7134 completed (loss: 0.05346524715423584, acc: 0.9847058653831482)
[2025-02-13 02:44:57,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:57,318][root][INFO] - Training Epoch: 1/2, step 2316/7134 completed (loss: 0.04988912120461464, acc: 0.9896432757377625)
[2025-02-13 02:44:57,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:57,779][root][INFO] - Training Epoch: 1/2, step 2317/7134 completed (loss: 0.04902170971035957, acc: 0.9894179701805115)
[2025-02-13 02:44:57,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:58,242][root][INFO] - Training Epoch: 1/2, step 2318/7134 completed (loss: 0.03785150498151779, acc: 0.9864016771316528)
[2025-02-13 02:44:58,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:58,695][root][INFO] - Training Epoch: 1/2, step 2319/7134 completed (loss: 0.04712870717048645, acc: 0.9846827387809753)
[2025-02-13 02:44:58,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:59,149][root][INFO] - Training Epoch: 1/2, step 2320/7134 completed (loss: 0.04447834566235542, acc: 0.9852272868156433)
[2025-02-13 02:44:59,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:59,565][root][INFO] - Training Epoch: 1/2, step 2321/7134 completed (loss: 0.05357950180768967, acc: 0.987500011920929)
[2025-02-13 02:44:59,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:59,981][root][INFO] - Training Epoch: 1/2, step 2322/7134 completed (loss: 0.04916693642735481, acc: 0.9837037324905396)
[2025-02-13 02:45:00,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:00,379][root][INFO] - Training Epoch: 1/2, step 2323/7134 completed (loss: 0.08019968122243881, acc: 0.9731861352920532)
[2025-02-13 02:45:00,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:00,804][root][INFO] - Training Epoch: 1/2, step 2324/7134 completed (loss: 0.0902344286441803, acc: 0.9800570011138916)
[2025-02-13 02:45:00,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:01,229][root][INFO] - Training Epoch: 1/2, step 2325/7134 completed (loss: 0.057885460555553436, acc: 0.9777777791023254)
[2025-02-13 02:45:01,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:01,623][root][INFO] - Training Epoch: 1/2, step 2326/7134 completed (loss: 0.0328967459499836, acc: 0.9871323704719543)
[2025-02-13 02:45:01,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:02,017][root][INFO] - Training Epoch: 1/2, step 2327/7134 completed (loss: 0.0328562892973423, acc: 0.9868852496147156)
[2025-02-13 02:45:02,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:02,431][root][INFO] - Training Epoch: 1/2, step 2328/7134 completed (loss: 0.10026513040065765, acc: 0.9784946441650391)
[2025-02-13 02:45:02,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:02,842][root][INFO] - Training Epoch: 1/2, step 2329/7134 completed (loss: 0.03859023377299309, acc: 0.9906976819038391)
[2025-02-13 02:45:02,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:03,236][root][INFO] - Training Epoch: 1/2, step 2330/7134 completed (loss: 0.03129611164331436, acc: 0.989313006401062)
[2025-02-13 02:45:03,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:03,642][root][INFO] - Training Epoch: 1/2, step 2331/7134 completed (loss: 0.03959321230649948, acc: 0.9892638325691223)
[2025-02-13 02:45:03,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:04,059][root][INFO] - Training Epoch: 1/2, step 2332/7134 completed (loss: 0.058596838265657425, acc: 0.9925705790519714)
[2025-02-13 02:45:04,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:04,474][root][INFO] - Training Epoch: 1/2, step 2333/7134 completed (loss: 0.023226866498589516, acc: 0.9910714030265808)
[2025-02-13 02:45:04,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:04,888][root][INFO] - Training Epoch: 1/2, step 2334/7134 completed (loss: 0.037071287631988525, acc: 0.9895522594451904)
[2025-02-13 02:45:05,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:05,315][root][INFO] - Training Epoch: 1/2, step 2335/7134 completed (loss: 0.06874258816242218, acc: 0.9838709831237793)
[2025-02-13 02:45:05,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:05,708][root][INFO] - Training Epoch: 1/2, step 2336/7134 completed (loss: 0.012371938675642014, acc: 0.9984543919563293)
[2025-02-13 02:45:05,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:06,117][root][INFO] - Training Epoch: 1/2, step 2337/7134 completed (loss: 0.008875719271600246, acc: 0.9983525276184082)
[2025-02-13 02:45:06,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:06,532][root][INFO] - Training Epoch: 1/2, step 2338/7134 completed (loss: 0.054682157933712006, acc: 0.984544038772583)
[2025-02-13 02:45:06,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:06,943][root][INFO] - Training Epoch: 1/2, step 2339/7134 completed (loss: 0.04662219434976578, acc: 0.985318124294281)
[2025-02-13 02:45:07,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:07,353][root][INFO] - Training Epoch: 1/2, step 2340/7134 completed (loss: 0.046654269099235535, acc: 0.9834087491035461)
[2025-02-13 02:45:07,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:07,758][root][INFO] - Training Epoch: 1/2, step 2341/7134 completed (loss: 0.02430560253560543, acc: 0.9943609237670898)
[2025-02-13 02:45:07,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:08,157][root][INFO] - Training Epoch: 1/2, step 2342/7134 completed (loss: 0.04380332678556442, acc: 0.9822747707366943)
[2025-02-13 02:45:08,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:08,556][root][INFO] - Training Epoch: 1/2, step 2343/7134 completed (loss: 0.038968708366155624, acc: 0.9904000163078308)
[2025-02-13 02:45:08,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:08,970][root][INFO] - Training Epoch: 1/2, step 2344/7134 completed (loss: 0.0274912491440773, acc: 0.992175281047821)
[2025-02-13 02:45:09,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:09,392][root][INFO] - Training Epoch: 1/2, step 2345/7134 completed (loss: 0.038409523665905, acc: 0.9860140085220337)
[2025-02-13 02:45:09,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:09,814][root][INFO] - Training Epoch: 1/2, step 2346/7134 completed (loss: 0.03146470710635185, acc: 0.9886685609817505)
[2025-02-13 02:45:09,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:10,230][root][INFO] - Training Epoch: 1/2, step 2347/7134 completed (loss: 0.08002131432294846, acc: 0.9802631735801697)
[2025-02-13 02:45:10,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:10,671][root][INFO] - Training Epoch: 1/2, step 2348/7134 completed (loss: 0.09910975396633148, acc: 0.9709962010383606)
[2025-02-13 02:45:10,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:11,091][root][INFO] - Training Epoch: 1/2, step 2349/7134 completed (loss: 0.09823240339756012, acc: 0.9777777791023254)
[2025-02-13 02:45:11,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:11,543][root][INFO] - Training Epoch: 1/2, step 2350/7134 completed (loss: 0.1299685537815094, acc: 0.9649389982223511)
[2025-02-13 02:45:11,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:11,983][root][INFO] - Training Epoch: 1/2, step 2351/7134 completed (loss: 0.08273652195930481, acc: 0.9726775884628296)
[2025-02-13 02:45:12,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:12,386][root][INFO] - Training Epoch: 1/2, step 2352/7134 completed (loss: 0.07681626826524734, acc: 0.9727563858032227)
[2025-02-13 02:45:12,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:12,831][root][INFO] - Training Epoch: 1/2, step 2353/7134 completed (loss: 0.05178897827863693, acc: 0.9829787015914917)
[2025-02-13 02:45:12,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:13,258][root][INFO] - Training Epoch: 1/2, step 2354/7134 completed (loss: 0.056509315967559814, acc: 0.9848101139068604)
[2025-02-13 02:45:13,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:13,689][root][INFO] - Training Epoch: 1/2, step 2355/7134 completed (loss: 0.06585702300071716, acc: 0.9876543283462524)
[2025-02-13 02:45:13,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:14,093][root][INFO] - Training Epoch: 1/2, step 2356/7134 completed (loss: 0.07265644520521164, acc: 0.982332170009613)
[2025-02-13 02:45:14,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:14,480][root][INFO] - Training Epoch: 1/2, step 2357/7134 completed (loss: 0.0971696674823761, acc: 0.9702380895614624)
[2025-02-13 02:45:14,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:14,922][root][INFO] - Training Epoch: 1/2, step 2358/7134 completed (loss: 0.10794425755739212, acc: 0.9759036302566528)
[2025-02-13 02:45:15,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:15,334][root][INFO] - Training Epoch: 1/2, step 2359/7134 completed (loss: 0.09215930849313736, acc: 0.9735682606697083)
[2025-02-13 02:45:15,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:15,789][root][INFO] - Training Epoch: 1/2, step 2360/7134 completed (loss: 0.07599196583032608, acc: 0.9780380725860596)
[2025-02-13 02:45:15,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:16,204][root][INFO] - Training Epoch: 1/2, step 2361/7134 completed (loss: 0.08722491562366486, acc: 0.9778106212615967)
[2025-02-13 02:45:16,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:16,522][root][INFO] - Training Epoch: 1/2, step 2362/7134 completed (loss: 0.07573173195123672, acc: 0.9753363132476807)
[2025-02-13 02:45:16,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:16,985][root][INFO] - Training Epoch: 1/2, step 2363/7134 completed (loss: 0.07492478936910629, acc: 0.9779837727546692)
[2025-02-13 02:45:17,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:17,390][root][INFO] - Training Epoch: 1/2, step 2364/7134 completed (loss: 0.06917892396450043, acc: 0.9802131056785583)
[2025-02-13 02:45:17,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:17,794][root][INFO] - Training Epoch: 1/2, step 2365/7134 completed (loss: 0.05099163204431534, acc: 0.9820936918258667)
[2025-02-13 02:45:17,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:18,228][root][INFO] - Training Epoch: 1/2, step 2366/7134 completed (loss: 0.044033437967300415, acc: 0.9907407164573669)
[2025-02-13 02:45:18,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:18,686][root][INFO] - Training Epoch: 1/2, step 2367/7134 completed (loss: 0.09329953044652939, acc: 0.9849246144294739)
[2025-02-13 02:45:18,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:19,136][root][INFO] - Training Epoch: 1/2, step 2368/7134 completed (loss: 0.045394450426101685, acc: 0.9915151596069336)
[2025-02-13 02:45:19,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:19,580][root][INFO] - Training Epoch: 1/2, step 2369/7134 completed (loss: 0.045020461082458496, acc: 0.9847058653831482)
[2025-02-13 02:45:19,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:19,943][root][INFO] - Training Epoch: 1/2, step 2370/7134 completed (loss: 0.06923262029886246, acc: 0.9814814925193787)
[2025-02-13 02:45:20,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:20,396][root][INFO] - Training Epoch: 1/2, step 2371/7134 completed (loss: 0.035029999911785126, acc: 0.9884726405143738)
[2025-02-13 02:45:20,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:20,831][root][INFO] - Training Epoch: 1/2, step 2372/7134 completed (loss: 0.0426335372030735, acc: 0.9863945841789246)
[2025-02-13 02:45:20,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:21,242][root][INFO] - Training Epoch: 1/2, step 2373/7134 completed (loss: 0.03925188630819321, acc: 0.9925558567047119)
[2025-02-13 02:45:21,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:21,665][root][INFO] - Training Epoch: 1/2, step 2374/7134 completed (loss: 0.04989724978804588, acc: 0.9892904758453369)
[2025-02-13 02:45:21,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:22,108][root][INFO] - Training Epoch: 1/2, step 2375/7134 completed (loss: 0.09059037268161774, acc: 0.9818181991577148)
[2025-02-13 02:45:22,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:22,525][root][INFO] - Training Epoch: 1/2, step 2376/7134 completed (loss: 0.1321137398481369, acc: 0.9727427363395691)
[2025-02-13 02:45:22,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:22,964][root][INFO] - Training Epoch: 1/2, step 2377/7134 completed (loss: 0.10055896639823914, acc: 0.9737156629562378)
[2025-02-13 02:45:23,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:23,400][root][INFO] - Training Epoch: 1/2, step 2378/7134 completed (loss: 0.09943676739931107, acc: 0.9741784334182739)
[2025-02-13 02:45:23,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:23,840][root][INFO] - Training Epoch: 1/2, step 2379/7134 completed (loss: 0.12736739218235016, acc: 0.973793089389801)
[2025-02-13 02:45:23,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:24,279][root][INFO] - Training Epoch: 1/2, step 2380/7134 completed (loss: 0.058355025947093964, acc: 0.9818621277809143)
[2025-02-13 02:45:24,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:24,656][root][INFO] - Training Epoch: 1/2, step 2381/7134 completed (loss: 0.1729007065296173, acc: 0.9593908786773682)
[2025-02-13 02:45:24,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:25,052][root][INFO] - Training Epoch: 1/2, step 2382/7134 completed (loss: 0.07430163770914078, acc: 0.9785932898521423)
[2025-02-13 02:45:25,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:25,452][root][INFO] - Training Epoch: 1/2, step 2383/7134 completed (loss: 0.11652422696352005, acc: 0.9698340892791748)
[2025-02-13 02:45:25,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:25,865][root][INFO] - Training Epoch: 1/2, step 2384/7134 completed (loss: 0.0890730693936348, acc: 0.9803921580314636)
[2025-02-13 02:45:26,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:26,314][root][INFO] - Training Epoch: 1/2, step 2385/7134 completed (loss: 0.06986568868160248, acc: 0.9820442199707031)
[2025-02-13 02:45:26,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:26,726][root][INFO] - Training Epoch: 1/2, step 2386/7134 completed (loss: 0.08163554966449738, acc: 0.9803664684295654)
[2025-02-13 02:45:26,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:27,187][root][INFO] - Training Epoch: 1/2, step 2387/7134 completed (loss: 0.07062245160341263, acc: 0.9837586879730225)
[2025-02-13 02:45:27,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:27,623][root][INFO] - Training Epoch: 1/2, step 2388/7134 completed (loss: 0.09566035121679306, acc: 0.9785459041595459)
[2025-02-13 02:45:27,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:28,099][root][INFO] - Training Epoch: 1/2, step 2389/7134 completed (loss: 0.07042401283979416, acc: 0.9827784299850464)
[2025-02-13 02:45:28,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:28,539][root][INFO] - Training Epoch: 1/2, step 2390/7134 completed (loss: 0.07879377156496048, acc: 0.9784946441650391)
[2025-02-13 02:45:28,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:28,964][root][INFO] - Training Epoch: 1/2, step 2391/7134 completed (loss: 0.10806921124458313, acc: 0.975857675075531)
[2025-02-13 02:45:29,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:29,396][root][INFO] - Training Epoch: 1/2, step 2392/7134 completed (loss: 0.07205332815647125, acc: 0.9853137731552124)
[2025-02-13 02:45:29,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:29,824][root][INFO] - Training Epoch: 1/2, step 2393/7134 completed (loss: 0.12585437297821045, acc: 0.9740596413612366)
[2025-02-13 02:45:29,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:30,274][root][INFO] - Training Epoch: 1/2, step 2394/7134 completed (loss: 0.05320160090923309, acc: 0.9842424392700195)
[2025-02-13 02:45:30,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:30,688][root][INFO] - Training Epoch: 1/2, step 2395/7134 completed (loss: 0.18121130764484406, acc: 0.9592834115028381)
[2025-02-13 02:45:30,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:31,143][root][INFO] - Training Epoch: 1/2, step 2396/7134 completed (loss: 0.11820799112319946, acc: 0.9600551128387451)
[2025-02-13 02:45:31,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:31,524][root][INFO] - Training Epoch: 1/2, step 2397/7134 completed (loss: 0.11310034245252609, acc: 0.9723435044288635)
[2025-02-13 02:45:31,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:31,950][root][INFO] - Training Epoch: 1/2, step 2398/7134 completed (loss: 0.056136876344680786, acc: 0.9793672561645508)
[2025-02-13 02:45:32,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:32,378][root][INFO] - Training Epoch: 1/2, step 2399/7134 completed (loss: 0.093553826212883, acc: 0.9771101474761963)
[2025-02-13 02:45:32,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:32,829][root][INFO] - Training Epoch: 1/2, step 2400/7134 completed (loss: 0.06691795587539673, acc: 0.9751724004745483)
[2025-02-13 02:45:32,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:33,239][root][INFO] - Training Epoch: 1/2, step 2401/7134 completed (loss: 0.1388232558965683, acc: 0.9651898741722107)
[2025-02-13 02:45:33,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:33,657][root][INFO] - Training Epoch: 1/2, step 2402/7134 completed (loss: 0.10183737426996231, acc: 0.9654714465141296)
[2025-02-13 02:45:33,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:34,076][root][INFO] - Training Epoch: 1/2, step 2403/7134 completed (loss: 0.062114082276821136, acc: 0.978723406791687)
[2025-02-13 02:45:34,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:34,485][root][INFO] - Training Epoch: 1/2, step 2404/7134 completed (loss: 0.05796452984213829, acc: 0.9795082211494446)
[2025-02-13 02:45:34,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:34,861][root][INFO] - Training Epoch: 1/2, step 2405/7134 completed (loss: 0.09217400848865509, acc: 0.9732441306114197)
[2025-02-13 02:45:34,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:35,275][root][INFO] - Training Epoch: 1/2, step 2406/7134 completed (loss: 0.03341259807348251, acc: 0.9920212626457214)
[2025-02-13 02:45:35,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:35,710][root][INFO] - Training Epoch: 1/2, step 2407/7134 completed (loss: 0.09231190383434296, acc: 0.9726443886756897)
[2025-02-13 02:45:35,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:36,134][root][INFO] - Training Epoch: 1/2, step 2408/7134 completed (loss: 0.14971959590911865, acc: 0.9671052694320679)
[2025-02-13 02:45:36,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:36,558][root][INFO] - Training Epoch: 1/2, step 2409/7134 completed (loss: 0.04613383859395981, acc: 0.9877049326896667)
[2025-02-13 02:45:36,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:36,961][root][INFO] - Training Epoch: 1/2, step 2410/7134 completed (loss: 0.054008107632398605, acc: 0.9856528043746948)
[2025-02-13 02:45:37,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:37,388][root][INFO] - Training Epoch: 1/2, step 2411/7134 completed (loss: 0.06753972917795181, acc: 0.9818887710571289)
[2025-02-13 02:45:37,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:37,799][root][INFO] - Training Epoch: 1/2, step 2412/7134 completed (loss: 0.06888145208358765, acc: 0.9807692170143127)
[2025-02-13 02:45:37,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:38,232][root][INFO] - Training Epoch: 1/2, step 2413/7134 completed (loss: 0.05051133781671524, acc: 0.9822109341621399)
[2025-02-13 02:45:38,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:38,683][root][INFO] - Training Epoch: 1/2, step 2414/7134 completed (loss: 0.05834534019231796, acc: 0.9813432693481445)
[2025-02-13 02:45:38,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:39,138][root][INFO] - Training Epoch: 1/2, step 2415/7134 completed (loss: 0.08914104104042053, acc: 0.973936915397644)
[2025-02-13 02:45:39,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:39,560][root][INFO] - Training Epoch: 1/2, step 2416/7134 completed (loss: 0.06765822321176529, acc: 0.9820193648338318)
[2025-02-13 02:45:39,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:40,030][root][INFO] - Training Epoch: 1/2, step 2417/7134 completed (loss: 0.08923213183879852, acc: 0.9799330830574036)
[2025-02-13 02:45:40,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:40,419][root][INFO] - Training Epoch: 1/2, step 2418/7134 completed (loss: 0.07159227877855301, acc: 0.9789029359817505)
[2025-02-13 02:45:40,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:40,822][root][INFO] - Training Epoch: 1/2, step 2419/7134 completed (loss: 0.04242078214883804, acc: 0.9871244430541992)
[2025-02-13 02:45:40,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:41,232][root][INFO] - Training Epoch: 1/2, step 2420/7134 completed (loss: 0.05154185742139816, acc: 0.9834162592887878)
[2025-02-13 02:45:41,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:41,665][root][INFO] - Training Epoch: 1/2, step 2421/7134 completed (loss: 0.10500461608171463, acc: 0.9738805890083313)
[2025-02-13 02:45:41,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:42,146][root][INFO] - Training Epoch: 1/2, step 2422/7134 completed (loss: 0.052939143031835556, acc: 0.986997663974762)
[2025-02-13 02:45:42,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:42,546][root][INFO] - Training Epoch: 1/2, step 2423/7134 completed (loss: 0.054484836757183075, acc: 0.9832636117935181)
[2025-02-13 02:45:42,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:42,957][root][INFO] - Training Epoch: 1/2, step 2424/7134 completed (loss: 0.05152285471558571, acc: 0.983146071434021)
[2025-02-13 02:45:43,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:43,368][root][INFO] - Training Epoch: 1/2, step 2425/7134 completed (loss: 0.05638112500309944, acc: 0.9800000190734863)
[2025-02-13 02:45:43,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:43,821][root][INFO] - Training Epoch: 1/2, step 2426/7134 completed (loss: 0.04823111370205879, acc: 0.9869203567504883)
[2025-02-13 02:45:43,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:44,265][root][INFO] - Training Epoch: 1/2, step 2427/7134 completed (loss: 0.04452880099415779, acc: 0.9802784323692322)
[2025-02-13 02:45:44,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:44,710][root][INFO] - Training Epoch: 1/2, step 2428/7134 completed (loss: 0.05553225800395012, acc: 0.9847133755683899)
[2025-02-13 02:45:44,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:45,135][root][INFO] - Training Epoch: 1/2, step 2429/7134 completed (loss: 0.07455674558877945, acc: 0.9813084006309509)
[2025-02-13 02:45:45,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:45,533][root][INFO] - Training Epoch: 1/2, step 2430/7134 completed (loss: 0.05700891837477684, acc: 0.9850968718528748)
[2025-02-13 02:45:45,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:45,960][root][INFO] - Training Epoch: 1/2, step 2431/7134 completed (loss: 0.09738150984048843, acc: 0.9750733375549316)
[2025-02-13 02:45:46,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:46,393][root][INFO] - Training Epoch: 1/2, step 2432/7134 completed (loss: 0.14145441353321075, acc: 0.9677419066429138)
[2025-02-13 02:45:46,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:46,826][root][INFO] - Training Epoch: 1/2, step 2433/7134 completed (loss: 0.08612006157636642, acc: 0.9758522510528564)
[2025-02-13 02:45:46,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:47,220][root][INFO] - Training Epoch: 1/2, step 2434/7134 completed (loss: 0.09511640667915344, acc: 0.9767080545425415)
[2025-02-13 02:45:47,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:47,671][root][INFO] - Training Epoch: 1/2, step 2435/7134 completed (loss: 0.057966198772192, acc: 0.9868247509002686)
[2025-02-13 02:45:47,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:48,101][root][INFO] - Training Epoch: 1/2, step 2436/7134 completed (loss: 0.06465546041727066, acc: 0.9778812527656555)
[2025-02-13 02:45:48,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:48,510][root][INFO] - Training Epoch: 1/2, step 2437/7134 completed (loss: 0.06686603277921677, acc: 0.9803921580314636)
[2025-02-13 02:45:48,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:48,960][root][INFO] - Training Epoch: 1/2, step 2438/7134 completed (loss: 0.08180549740791321, acc: 0.980861246585846)
[2025-02-13 02:45:49,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:49,408][root][INFO] - Training Epoch: 1/2, step 2439/7134 completed (loss: 0.07468089461326599, acc: 0.973089337348938)
[2025-02-13 02:45:49,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:49,794][root][INFO] - Training Epoch: 1/2, step 2440/7134 completed (loss: 0.08042261004447937, acc: 0.9798206090927124)
[2025-02-13 02:45:49,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:50,197][root][INFO] - Training Epoch: 1/2, step 2441/7134 completed (loss: 0.15667372941970825, acc: 0.9577735066413879)
[2025-02-13 02:45:50,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:50,623][root][INFO] - Training Epoch: 1/2, step 2442/7134 completed (loss: 0.09938371926546097, acc: 0.9650349617004395)
[2025-02-13 02:45:50,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:51,068][root][INFO] - Training Epoch: 1/2, step 2443/7134 completed (loss: 0.08519962430000305, acc: 0.9847009778022766)
[2025-02-13 02:45:51,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:51,477][root][INFO] - Training Epoch: 1/2, step 2444/7134 completed (loss: 0.08285796642303467, acc: 0.9696969985961914)
[2025-02-13 02:45:51,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:51,912][root][INFO] - Training Epoch: 1/2, step 2445/7134 completed (loss: 0.10297758877277374, acc: 0.9733502268791199)
[2025-02-13 02:45:52,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:52,375][root][INFO] - Training Epoch: 1/2, step 2446/7134 completed (loss: 0.12273933738470078, acc: 0.9666666388511658)
[2025-02-13 02:45:52,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:52,839][root][INFO] - Training Epoch: 1/2, step 2447/7134 completed (loss: 0.0678771585226059, acc: 0.9800221920013428)
[2025-02-13 02:45:52,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:53,261][root][INFO] - Training Epoch: 1/2, step 2448/7134 completed (loss: 0.07664892822504044, acc: 0.9745989441871643)
[2025-02-13 02:45:53,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:53,659][root][INFO] - Training Epoch: 1/2, step 2449/7134 completed (loss: 0.08781083673238754, acc: 0.9774535894393921)
[2025-02-13 02:45:53,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:54,080][root][INFO] - Training Epoch: 1/2, step 2450/7134 completed (loss: 0.09278187155723572, acc: 0.9707750678062439)
[2025-02-13 02:45:54,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:54,489][root][INFO] - Training Epoch: 1/2, step 2451/7134 completed (loss: 0.04370945692062378, acc: 0.9885350465774536)
[2025-02-13 02:45:54,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:54,924][root][INFO] - Training Epoch: 1/2, step 2452/7134 completed (loss: 0.06385237723588943, acc: 0.9804941415786743)
[2025-02-13 02:45:55,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:55,380][root][INFO] - Training Epoch: 1/2, step 2453/7134 completed (loss: 0.06329629570245743, acc: 0.9819355010986328)
[2025-02-13 02:45:55,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:55,784][root][INFO] - Training Epoch: 1/2, step 2454/7134 completed (loss: 0.0783040001988411, acc: 0.9698996543884277)
[2025-02-13 02:45:55,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:56,241][root][INFO] - Training Epoch: 1/2, step 2455/7134 completed (loss: 0.06300844997167587, acc: 0.9815863966941833)
[2025-02-13 02:45:56,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:56,665][root][INFO] - Training Epoch: 1/2, step 2456/7134 completed (loss: 0.07410205155611038, acc: 0.9753566980361938)
[2025-02-13 02:45:56,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:57,125][root][INFO] - Training Epoch: 1/2, step 2457/7134 completed (loss: 0.0818769708275795, acc: 0.9767054915428162)
[2025-02-13 02:45:57,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:57,530][root][INFO] - Training Epoch: 1/2, step 2458/7134 completed (loss: 0.1194070428609848, acc: 0.9732704162597656)
[2025-02-13 02:45:57,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:57,984][root][INFO] - Training Epoch: 1/2, step 2459/7134 completed (loss: 0.03580963984131813, acc: 0.9896449446678162)
[2025-02-13 02:45:58,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:58,412][root][INFO] - Training Epoch: 1/2, step 2460/7134 completed (loss: 0.04335285723209381, acc: 0.9903181195259094)
[2025-02-13 02:45:58,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:58,832][root][INFO] - Training Epoch: 1/2, step 2461/7134 completed (loss: 0.04493299871683121, acc: 0.9869565367698669)
[2025-02-13 02:45:58,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:59,245][root][INFO] - Training Epoch: 1/2, step 2462/7134 completed (loss: 0.07198057323694229, acc: 0.9793103337287903)
[2025-02-13 02:45:59,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:59,682][root][INFO] - Training Epoch: 1/2, step 2463/7134 completed (loss: 0.040455881506204605, acc: 0.9863842725753784)
[2025-02-13 02:45:59,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:00,040][root][INFO] - Training Epoch: 1/2, step 2464/7134 completed (loss: 0.08544377237558365, acc: 0.9794007539749146)
[2025-02-13 02:46:00,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:00,462][root][INFO] - Training Epoch: 1/2, step 2465/7134 completed (loss: 0.06141289696097374, acc: 0.9827814698219299)
[2025-02-13 02:46:00,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:00,901][root][INFO] - Training Epoch: 1/2, step 2466/7134 completed (loss: 0.04713650792837143, acc: 0.9814569354057312)
[2025-02-13 02:46:01,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:01,325][root][INFO] - Training Epoch: 1/2, step 2467/7134 completed (loss: 0.05548122152686119, acc: 0.9884318709373474)
[2025-02-13 02:46:01,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:01,765][root][INFO] - Training Epoch: 1/2, step 2468/7134 completed (loss: 0.03865901008248329, acc: 0.988811194896698)
[2025-02-13 02:46:01,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:02,207][root][INFO] - Training Epoch: 1/2, step 2469/7134 completed (loss: 0.03346603363752365, acc: 0.9890859723091125)
[2025-02-13 02:46:02,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:02,614][root][INFO] - Training Epoch: 1/2, step 2470/7134 completed (loss: 0.07133618742227554, acc: 0.9831649661064148)
[2025-02-13 02:46:02,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:03,021][root][INFO] - Training Epoch: 1/2, step 2471/7134 completed (loss: 0.041223883628845215, acc: 0.9849498271942139)
[2025-02-13 02:46:03,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:03,417][root][INFO] - Training Epoch: 1/2, step 2472/7134 completed (loss: 0.02097008004784584, acc: 0.9939393997192383)
[2025-02-13 02:46:03,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:03,835][root][INFO] - Training Epoch: 1/2, step 2473/7134 completed (loss: 0.05722624063491821, acc: 0.9815573692321777)
[2025-02-13 02:46:03,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:04,251][root][INFO] - Training Epoch: 1/2, step 2474/7134 completed (loss: 0.028237128630280495, acc: 0.9929577708244324)
[2025-02-13 02:46:04,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:04,639][root][INFO] - Training Epoch: 1/2, step 2475/7134 completed (loss: 0.03212297335267067, acc: 0.9879931211471558)
[2025-02-13 02:46:04,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:05,056][root][INFO] - Training Epoch: 1/2, step 2476/7134 completed (loss: 0.08133348077535629, acc: 0.984679639339447)
[2025-02-13 02:46:05,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:05,455][root][INFO] - Training Epoch: 1/2, step 2477/7134 completed (loss: 0.05257907882332802, acc: 0.988950252532959)
[2025-02-13 02:46:05,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:05,858][root][INFO] - Training Epoch: 1/2, step 2478/7134 completed (loss: 0.023554516956210136, acc: 0.9906542301177979)
[2025-02-13 02:46:05,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:06,253][root][INFO] - Training Epoch: 1/2, step 2479/7134 completed (loss: 0.10954387485980988, acc: 0.9777424335479736)
[2025-02-13 02:46:06,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:06,648][root][INFO] - Training Epoch: 1/2, step 2480/7134 completed (loss: 0.020555764436721802, acc: 0.995555579662323)
[2025-02-13 02:46:06,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:07,043][root][INFO] - Training Epoch: 1/2, step 2481/7134 completed (loss: 0.05678209289908409, acc: 0.9776119589805603)
[2025-02-13 02:46:07,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:07,462][root][INFO] - Training Epoch: 1/2, step 2482/7134 completed (loss: 0.0713474228978157, acc: 0.9775280952453613)
[2025-02-13 02:46:07,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:07,889][root][INFO] - Training Epoch: 1/2, step 2483/7134 completed (loss: 0.023899385705590248, acc: 0.9902912378311157)
[2025-02-13 02:46:08,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:08,310][root][INFO] - Training Epoch: 1/2, step 2484/7134 completed (loss: 0.040995024144649506, acc: 0.987500011920929)
[2025-02-13 02:46:08,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:08,722][root][INFO] - Training Epoch: 1/2, step 2485/7134 completed (loss: 0.04562827944755554, acc: 0.9952380657196045)
[2025-02-13 02:46:08,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:09,141][root][INFO] - Training Epoch: 1/2, step 2486/7134 completed (loss: 0.04281822219491005, acc: 0.9913793206214905)
[2025-02-13 02:46:09,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:09,540][root][INFO] - Training Epoch: 1/2, step 2487/7134 completed (loss: 0.10749964416027069, acc: 0.9795275330543518)
[2025-02-13 02:46:09,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:09,956][root][INFO] - Training Epoch: 1/2, step 2488/7134 completed (loss: 0.049696799367666245, acc: 0.9861878156661987)
[2025-02-13 02:46:10,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:10,353][root][INFO] - Training Epoch: 1/2, step 2489/7134 completed (loss: 0.04374902695417404, acc: 0.9902507066726685)
[2025-02-13 02:46:10,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:10,767][root][INFO] - Training Epoch: 1/2, step 2490/7134 completed (loss: 0.031845007091760635, acc: 0.9885877370834351)
[2025-02-13 02:46:10,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:11,166][root][INFO] - Training Epoch: 1/2, step 2491/7134 completed (loss: 0.16340158879756927, acc: 0.9669312238693237)
[2025-02-13 02:46:11,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:11,582][root][INFO] - Training Epoch: 1/2, step 2492/7134 completed (loss: 0.06434150040149689, acc: 0.981389582157135)
[2025-02-13 02:46:11,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:12,020][root][INFO] - Training Epoch: 1/2, step 2493/7134 completed (loss: 0.0813862681388855, acc: 0.9694835543632507)
[2025-02-13 02:46:12,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:12,430][root][INFO] - Training Epoch: 1/2, step 2494/7134 completed (loss: 0.06538883596658707, acc: 0.9849812388420105)
[2025-02-13 02:46:12,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:12,855][root][INFO] - Training Epoch: 1/2, step 2495/7134 completed (loss: 0.0639699324965477, acc: 0.9782903790473938)
[2025-02-13 02:46:12,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:13,289][root][INFO] - Training Epoch: 1/2, step 2496/7134 completed (loss: 0.03721249848604202, acc: 0.9873595237731934)
[2025-02-13 02:46:13,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:13,729][root][INFO] - Training Epoch: 1/2, step 2497/7134 completed (loss: 0.055698417127132416, acc: 0.9829683899879456)
[2025-02-13 02:46:13,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:14,174][root][INFO] - Training Epoch: 1/2, step 2498/7134 completed (loss: 0.05666924640536308, acc: 0.9874857664108276)
[2025-02-13 02:46:14,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:14,622][root][INFO] - Training Epoch: 1/2, step 2499/7134 completed (loss: 0.05058615654706955, acc: 0.9842995405197144)
[2025-02-13 02:46:14,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:15,054][root][INFO] - Training Epoch: 1/2, step 2500/7134 completed (loss: 0.048082947731018066, acc: 0.9858430027961731)
[2025-02-13 02:46:15,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:15,484][root][INFO] - Training Epoch: 1/2, step 2501/7134 completed (loss: 0.06389295309782028, acc: 0.9805492162704468)
[2025-02-13 02:46:15,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:15,920][root][INFO] - Training Epoch: 1/2, step 2502/7134 completed (loss: 0.0258872602134943, acc: 0.9929328560829163)
[2025-02-13 02:46:16,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:16,307][root][INFO] - Training Epoch: 1/2, step 2503/7134 completed (loss: 0.05323193222284317, acc: 0.9829059839248657)
[2025-02-13 02:46:16,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:16,726][root][INFO] - Training Epoch: 1/2, step 2504/7134 completed (loss: 0.0361737385392189, acc: 0.9906666874885559)
[2025-02-13 02:46:16,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:17,157][root][INFO] - Training Epoch: 1/2, step 2505/7134 completed (loss: 0.03773903474211693, acc: 0.9897611141204834)
[2025-02-13 02:46:17,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:17,623][root][INFO] - Training Epoch: 1/2, step 2506/7134 completed (loss: 0.04683239758014679, acc: 0.9839743375778198)
[2025-02-13 02:46:17,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:18,061][root][INFO] - Training Epoch: 1/2, step 2507/7134 completed (loss: 0.020073652267456055, acc: 0.99303138256073)
[2025-02-13 02:46:18,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:18,520][root][INFO] - Training Epoch: 1/2, step 2508/7134 completed (loss: 0.04723937809467316, acc: 0.9884792566299438)
[2025-02-13 02:46:18,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:18,943][root][INFO] - Training Epoch: 1/2, step 2509/7134 completed (loss: 0.03384565934538841, acc: 0.990208089351654)
[2025-02-13 02:46:19,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:19,377][root][INFO] - Training Epoch: 1/2, step 2510/7134 completed (loss: 0.03525438532233238, acc: 0.9893778562545776)
[2025-02-13 02:46:19,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:19,807][root][INFO] - Training Epoch: 1/2, step 2511/7134 completed (loss: 0.06630893796682358, acc: 0.9865471124649048)
[2025-02-13 02:46:19,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:20,217][root][INFO] - Training Epoch: 1/2, step 2512/7134 completed (loss: 0.09677824378013611, acc: 0.9716598987579346)
[2025-02-13 02:46:20,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:20,665][root][INFO] - Training Epoch: 1/2, step 2513/7134 completed (loss: 0.04137960448861122, acc: 0.9905213117599487)
[2025-02-13 02:46:20,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:21,103][root][INFO] - Training Epoch: 1/2, step 2514/7134 completed (loss: 0.027813220396637917, acc: 0.9906651377677917)
[2025-02-13 02:46:21,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:21,537][root][INFO] - Training Epoch: 1/2, step 2515/7134 completed (loss: 0.023593872785568237, acc: 0.9921976327896118)
[2025-02-13 02:46:21,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:21,994][root][INFO] - Training Epoch: 1/2, step 2516/7134 completed (loss: 0.025748591870069504, acc: 0.9931818246841431)
[2025-02-13 02:46:22,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:22,449][root][INFO] - Training Epoch: 1/2, step 2517/7134 completed (loss: 0.038904402405023575, acc: 0.9873708486557007)
[2025-02-13 02:46:22,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:22,852][root][INFO] - Training Epoch: 1/2, step 2518/7134 completed (loss: 0.041012488305568695, acc: 0.9871244430541992)
[2025-02-13 02:46:22,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:23,291][root][INFO] - Training Epoch: 1/2, step 2519/7134 completed (loss: 0.02592659741640091, acc: 0.9917012453079224)
[2025-02-13 02:46:23,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:23,716][root][INFO] - Training Epoch: 1/2, step 2520/7134 completed (loss: 0.028496000915765762, acc: 0.9857549667358398)
[2025-02-13 02:46:23,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:24,160][root][INFO] - Training Epoch: 1/2, step 2521/7134 completed (loss: 0.09412366151809692, acc: 0.9819819927215576)
[2025-02-13 02:46:24,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:24,563][root][INFO] - Training Epoch: 1/2, step 2522/7134 completed (loss: 0.046107180416584015, acc: 0.9856733679771423)
[2025-02-13 02:46:24,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:24,981][root][INFO] - Training Epoch: 1/2, step 2523/7134 completed (loss: 0.06514309346675873, acc: 0.985602080821991)
[2025-02-13 02:46:25,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:25,399][root][INFO] - Training Epoch: 1/2, step 2524/7134 completed (loss: 0.020376306027173996, acc: 0.9958563446998596)
[2025-02-13 02:46:25,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:25,793][root][INFO] - Training Epoch: 1/2, step 2525/7134 completed (loss: 0.04730549082159996, acc: 0.9849749803543091)
[2025-02-13 02:46:25,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:26,122][root][INFO] - Training Epoch: 1/2, step 2526/7134 completed (loss: 0.012090528383851051, acc: 0.9976470470428467)
[2025-02-13 02:46:26,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:26,555][root][INFO] - Training Epoch: 1/2, step 2527/7134 completed (loss: 0.0482071153819561, acc: 0.9890244007110596)
[2025-02-13 02:46:26,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:26,974][root][INFO] - Training Epoch: 1/2, step 2528/7134 completed (loss: 0.020366767421364784, acc: 0.9931600689888)
[2025-02-13 02:46:27,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:27,414][root][INFO] - Training Epoch: 1/2, step 2529/7134 completed (loss: 0.02682371251285076, acc: 0.9906914830207825)
[2025-02-13 02:46:27,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:27,862][root][INFO] - Training Epoch: 1/2, step 2530/7134 completed (loss: 0.03681672737002373, acc: 0.9922077655792236)
[2025-02-13 02:46:27,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:28,268][root][INFO] - Training Epoch: 1/2, step 2531/7134 completed (loss: 0.03413848951458931, acc: 0.9875690340995789)
[2025-02-13 02:46:28,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:28,676][root][INFO] - Training Epoch: 1/2, step 2532/7134 completed (loss: 0.03257676959037781, acc: 0.9861325025558472)
[2025-02-13 02:46:28,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:29,082][root][INFO] - Training Epoch: 1/2, step 2533/7134 completed (loss: 0.04772641882300377, acc: 0.9879336357116699)
[2025-02-13 02:46:29,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:29,490][root][INFO] - Training Epoch: 1/2, step 2534/7134 completed (loss: 0.04757208377122879, acc: 0.9910827875137329)
[2025-02-13 02:46:29,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:29,904][root][INFO] - Training Epoch: 1/2, step 2535/7134 completed (loss: 0.05370974913239479, acc: 0.9826589822769165)
[2025-02-13 02:46:30,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:30,345][root][INFO] - Training Epoch: 1/2, step 2536/7134 completed (loss: 0.049143966287374496, acc: 0.984009861946106)
[2025-02-13 02:46:30,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:30,744][root][INFO] - Training Epoch: 1/2, step 2537/7134 completed (loss: 0.08797068893909454, acc: 0.9785932898521423)
[2025-02-13 02:46:30,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:31,153][root][INFO] - Training Epoch: 1/2, step 2538/7134 completed (loss: 0.10093770921230316, acc: 0.9745042324066162)
[2025-02-13 02:46:31,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:31,521][root][INFO] - Training Epoch: 1/2, step 2539/7134 completed (loss: 0.029410213232040405, acc: 0.9928571581840515)
[2025-02-13 02:46:31,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:31,934][root][INFO] - Training Epoch: 1/2, step 2540/7134 completed (loss: 0.10466749221086502, acc: 0.9785932898521423)
[2025-02-13 02:46:32,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:32,335][root][INFO] - Training Epoch: 1/2, step 2541/7134 completed (loss: 0.03473243862390518, acc: 0.9884105920791626)
[2025-02-13 02:46:32,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:32,734][root][INFO] - Training Epoch: 1/2, step 2542/7134 completed (loss: 0.10739221423864365, acc: 0.9792899489402771)
[2025-02-13 02:46:32,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:33,165][root][INFO] - Training Epoch: 1/2, step 2543/7134 completed (loss: 0.11380577832460403, acc: 0.9754098653793335)
[2025-02-13 02:46:33,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:33,604][root][INFO] - Training Epoch: 1/2, step 2544/7134 completed (loss: 0.05007214844226837, acc: 0.9909793734550476)
[2025-02-13 02:46:33,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:34,025][root][INFO] - Training Epoch: 1/2, step 2545/7134 completed (loss: 0.07632250338792801, acc: 0.9808027744293213)
[2025-02-13 02:46:34,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:34,490][root][INFO] - Training Epoch: 1/2, step 2546/7134 completed (loss: 0.18011249601840973, acc: 0.9555822610855103)
[2025-02-13 02:46:34,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:34,897][root][INFO] - Training Epoch: 1/2, step 2547/7134 completed (loss: 0.08375207334756851, acc: 0.9710144996643066)
[2025-02-13 02:46:35,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:35,284][root][INFO] - Training Epoch: 1/2, step 2548/7134 completed (loss: 0.061952296644449234, acc: 0.9862204790115356)
[2025-02-13 02:46:35,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:35,695][root][INFO] - Training Epoch: 1/2, step 2549/7134 completed (loss: 0.07266224175691605, acc: 0.9815837740898132)
[2025-02-13 02:46:35,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:36,097][root][INFO] - Training Epoch: 1/2, step 2550/7134 completed (loss: 0.043681565672159195, acc: 0.9869494438171387)
[2025-02-13 02:46:36,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:36,496][root][INFO] - Training Epoch: 1/2, step 2551/7134 completed (loss: 0.06906382739543915, acc: 0.9834983348846436)
[2025-02-13 02:46:36,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:36,860][root][INFO] - Training Epoch: 1/2, step 2552/7134 completed (loss: 0.03993799164891243, acc: 0.9873816967010498)
[2025-02-13 02:46:36,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:37,169][root][INFO] - Training Epoch: 1/2, step 2553/7134 completed (loss: 0.05606386065483093, acc: 0.981873095035553)
[2025-02-13 02:46:37,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:37,594][root][INFO] - Training Epoch: 1/2, step 2554/7134 completed (loss: 0.10208238661289215, acc: 0.9715242981910706)
[2025-02-13 02:46:37,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:37,982][root][INFO] - Training Epoch: 1/2, step 2555/7134 completed (loss: 0.07886064797639847, acc: 0.9814049601554871)
[2025-02-13 02:46:38,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:38,378][root][INFO] - Training Epoch: 1/2, step 2556/7134 completed (loss: 0.08198510110378265, acc: 0.9832285046577454)
[2025-02-13 02:46:38,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:38,802][root][INFO] - Training Epoch: 1/2, step 2557/7134 completed (loss: 0.0770169198513031, acc: 0.9786885380744934)
[2025-02-13 02:46:38,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:39,204][root][INFO] - Training Epoch: 1/2, step 2558/7134 completed (loss: 0.024584809318184853, acc: 0.9935275316238403)
[2025-02-13 02:46:39,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:39,616][root][INFO] - Training Epoch: 1/2, step 2559/7134 completed (loss: 0.021810302510857582, acc: 0.9945873022079468)
[2025-02-13 02:46:39,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:40,062][root][INFO] - Training Epoch: 1/2, step 2560/7134 completed (loss: 0.07924826443195343, acc: 0.9788618087768555)
[2025-02-13 02:46:40,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:40,472][root][INFO] - Training Epoch: 1/2, step 2561/7134 completed (loss: 0.08437597006559372, acc: 0.9782134890556335)
[2025-02-13 02:46:40,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:40,865][root][INFO] - Training Epoch: 1/2, step 2562/7134 completed (loss: 0.08357375860214233, acc: 0.9794871807098389)
[2025-02-13 02:46:41,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:41,285][root][INFO] - Training Epoch: 1/2, step 2563/7134 completed (loss: 0.04831251502037048, acc: 0.9912023544311523)
[2025-02-13 02:46:41,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:41,719][root][INFO] - Training Epoch: 1/2, step 2564/7134 completed (loss: 0.1423823982477188, acc: 0.9647650718688965)
[2025-02-13 02:46:41,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:42,119][root][INFO] - Training Epoch: 1/2, step 2565/7134 completed (loss: 0.03849862888455391, acc: 0.9894259572029114)
[2025-02-13 02:46:42,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:42,562][root][INFO] - Training Epoch: 1/2, step 2566/7134 completed (loss: 0.07106431573629379, acc: 0.9817415475845337)
[2025-02-13 02:46:42,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:42,890][root][INFO] - Training Epoch: 1/2, step 2567/7134 completed (loss: 0.060143742710351944, acc: 0.9805996417999268)
[2025-02-13 02:46:43,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:43,297][root][INFO] - Training Epoch: 1/2, step 2568/7134 completed (loss: 0.06561800837516785, acc: 0.9812138676643372)
[2025-02-13 02:46:43,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:43,719][root][INFO] - Training Epoch: 1/2, step 2569/7134 completed (loss: 0.08231955021619797, acc: 0.9774096608161926)
[2025-02-13 02:46:43,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:44,132][root][INFO] - Training Epoch: 1/2, step 2570/7134 completed (loss: 0.06559699773788452, acc: 0.9865067601203918)
[2025-02-13 02:46:44,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:44,515][root][INFO] - Training Epoch: 1/2, step 2571/7134 completed (loss: 0.0315830260515213, acc: 0.9912126660346985)
[2025-02-13 02:46:44,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:44,930][root][INFO] - Training Epoch: 1/2, step 2572/7134 completed (loss: 0.06232401356101036, acc: 0.9826498627662659)
[2025-02-13 02:46:45,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:45,334][root][INFO] - Training Epoch: 1/2, step 2573/7134 completed (loss: 0.10094748437404633, acc: 0.9770491719245911)
[2025-02-13 02:46:45,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:45,714][root][INFO] - Training Epoch: 1/2, step 2574/7134 completed (loss: 0.06318672746419907, acc: 0.9847161769866943)
[2025-02-13 02:46:45,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:46,106][root][INFO] - Training Epoch: 1/2, step 2575/7134 completed (loss: 0.04947628453373909, acc: 0.9814126491546631)
[2025-02-13 02:46:46,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:46,504][root][INFO] - Training Epoch: 1/2, step 2576/7134 completed (loss: 0.06342046707868576, acc: 0.982758641242981)
[2025-02-13 02:46:46,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:46,909][root][INFO] - Training Epoch: 1/2, step 2577/7134 completed (loss: 0.09884529560804367, acc: 0.9777777791023254)
[2025-02-13 02:46:47,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:47,312][root][INFO] - Training Epoch: 1/2, step 2578/7134 completed (loss: 0.08346101641654968, acc: 0.9770290851593018)
[2025-02-13 02:46:47,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:47,716][root][INFO] - Training Epoch: 1/2, step 2579/7134 completed (loss: 0.11750378459692001, acc: 0.9715189933776855)
[2025-02-13 02:46:47,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:48,156][root][INFO] - Training Epoch: 1/2, step 2580/7134 completed (loss: 0.08243626356124878, acc: 0.9738903641700745)
[2025-02-13 02:46:48,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:48,579][root][INFO] - Training Epoch: 1/2, step 2581/7134 completed (loss: 0.0631699487566948, acc: 0.983849287033081)
[2025-02-13 02:46:48,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:49,012][root][INFO] - Training Epoch: 1/2, step 2582/7134 completed (loss: 0.055377230048179626, acc: 0.9810298085212708)
[2025-02-13 02:46:49,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:49,415][root][INFO] - Training Epoch: 1/2, step 2583/7134 completed (loss: 0.06727415323257446, acc: 0.9815078377723694)
[2025-02-13 02:46:49,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:49,790][root][INFO] - Training Epoch: 1/2, step 2584/7134 completed (loss: 0.10320910811424255, acc: 0.9738219976425171)
[2025-02-13 02:46:49,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:50,238][root][INFO] - Training Epoch: 1/2, step 2585/7134 completed (loss: 0.07489141821861267, acc: 0.9827337861061096)
[2025-02-13 02:46:50,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:50,668][root][INFO] - Training Epoch: 1/2, step 2586/7134 completed (loss: 0.10372777283191681, acc: 0.9753521084785461)
[2025-02-13 02:46:50,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:51,107][root][INFO] - Training Epoch: 1/2, step 2587/7134 completed (loss: 0.08002051711082458, acc: 0.9766584634780884)
[2025-02-13 02:46:51,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:51,544][root][INFO] - Training Epoch: 1/2, step 2588/7134 completed (loss: 0.03377654403448105, acc: 0.9849931597709656)
[2025-02-13 02:46:51,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:51,943][root][INFO] - Training Epoch: 1/2, step 2589/7134 completed (loss: 0.07467380166053772, acc: 0.982332170009613)
[2025-02-13 02:46:52,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:52,324][root][INFO] - Training Epoch: 1/2, step 2590/7134 completed (loss: 0.05784669145941734, acc: 0.9803921580314636)
[2025-02-13 02:46:52,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:52,751][root][INFO] - Training Epoch: 1/2, step 2591/7134 completed (loss: 0.09803752601146698, acc: 0.970588207244873)
[2025-02-13 02:46:52,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:53,197][root][INFO] - Training Epoch: 1/2, step 2592/7134 completed (loss: 0.08099224418401718, acc: 0.9729397296905518)
[2025-02-13 02:46:53,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:53,637][root][INFO] - Training Epoch: 1/2, step 2593/7134 completed (loss: 0.06459911912679672, acc: 0.9842932224273682)
[2025-02-13 02:46:53,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:54,046][root][INFO] - Training Epoch: 1/2, step 2594/7134 completed (loss: 0.05586869269609451, acc: 0.9877675771713257)
[2025-02-13 02:46:54,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:54,480][root][INFO] - Training Epoch: 1/2, step 2595/7134 completed (loss: 0.04315607622265816, acc: 0.9865591526031494)
[2025-02-13 02:46:54,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:54,943][root][INFO] - Training Epoch: 1/2, step 2596/7134 completed (loss: 0.07246056944131851, acc: 0.9849624037742615)
[2025-02-13 02:46:55,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:55,377][root][INFO] - Training Epoch: 1/2, step 2597/7134 completed (loss: 0.06651616841554642, acc: 0.9810810685157776)
[2025-02-13 02:46:55,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:55,787][root][INFO] - Training Epoch: 1/2, step 2598/7134 completed (loss: 0.10002060979604721, acc: 0.9783281683921814)
[2025-02-13 02:46:55,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:56,186][root][INFO] - Training Epoch: 1/2, step 2599/7134 completed (loss: 0.017001021653413773, acc: 0.9947229623794556)
[2025-02-13 02:46:56,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:56,642][root][INFO] - Training Epoch: 1/2, step 2600/7134 completed (loss: 0.059758834540843964, acc: 0.9820742607116699)
[2025-02-13 02:46:56,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:57,079][root][INFO] - Training Epoch: 1/2, step 2601/7134 completed (loss: 0.08008553087711334, acc: 0.9797022938728333)
[2025-02-13 02:46:57,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:57,509][root][INFO] - Training Epoch: 1/2, step 2602/7134 completed (loss: 0.0726800486445427, acc: 0.981840193271637)
[2025-02-13 02:46:57,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:57,960][root][INFO] - Training Epoch: 1/2, step 2603/7134 completed (loss: 0.08640225976705551, acc: 0.9767441749572754)
[2025-02-13 02:46:58,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:58,413][root][INFO] - Training Epoch: 1/2, step 2604/7134 completed (loss: 0.06851337105035782, acc: 0.9780701994895935)
[2025-02-13 02:46:58,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:58,841][root][INFO] - Training Epoch: 1/2, step 2605/7134 completed (loss: 0.1641482710838318, acc: 0.9476534128189087)
[2025-02-13 02:46:58,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:59,276][root][INFO] - Training Epoch: 1/2, step 2606/7134 completed (loss: 0.08054936677217484, acc: 0.9812949895858765)
[2025-02-13 02:46:59,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:59,712][root][INFO] - Training Epoch: 1/2, step 2607/7134 completed (loss: 0.09243277460336685, acc: 0.9694117903709412)
[2025-02-13 02:46:59,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:00,145][root][INFO] - Training Epoch: 1/2, step 2608/7134 completed (loss: 0.10393340140581131, acc: 0.9653893709182739)
[2025-02-13 02:47:00,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:00,575][root][INFO] - Training Epoch: 1/2, step 2609/7134 completed (loss: 0.10187403857707977, acc: 0.9721448421478271)
[2025-02-13 02:47:00,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:00,916][root][INFO] - Training Epoch: 1/2, step 2610/7134 completed (loss: 0.11282828450202942, acc: 0.9725190997123718)
[2025-02-13 02:47:01,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:01,379][root][INFO] - Training Epoch: 1/2, step 2611/7134 completed (loss: 0.11063863337039948, acc: 0.9690011739730835)
[2025-02-13 02:47:01,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:01,835][root][INFO] - Training Epoch: 1/2, step 2612/7134 completed (loss: 0.10130510479211807, acc: 0.9681093096733093)
[2025-02-13 02:47:01,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:02,308][root][INFO] - Training Epoch: 1/2, step 2613/7134 completed (loss: 0.05059395357966423, acc: 0.9848101139068604)
[2025-02-13 02:47:02,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:02,762][root][INFO] - Training Epoch: 1/2, step 2614/7134 completed (loss: 0.12493017315864563, acc: 0.9692482948303223)
[2025-02-13 02:47:02,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:03,199][root][INFO] - Training Epoch: 1/2, step 2615/7134 completed (loss: 0.12981757521629333, acc: 0.9640102982521057)
[2025-02-13 02:47:03,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:03,607][root][INFO] - Training Epoch: 1/2, step 2616/7134 completed (loss: 0.12304608523845673, acc: 0.9773662686347961)
[2025-02-13 02:47:03,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:04,049][root][INFO] - Training Epoch: 1/2, step 2617/7134 completed (loss: 0.12588059902191162, acc: 0.9649122953414917)
[2025-02-13 02:47:04,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:04,407][root][INFO] - Training Epoch: 1/2, step 2618/7134 completed (loss: 0.08896172791719437, acc: 0.9733059406280518)
[2025-02-13 02:47:04,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:04,823][root][INFO] - Training Epoch: 1/2, step 2619/7134 completed (loss: 0.0924542248249054, acc: 0.9731543660163879)
[2025-02-13 02:47:04,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:05,261][root][INFO] - Training Epoch: 1/2, step 2620/7134 completed (loss: 0.07740335911512375, acc: 0.9789343476295471)
[2025-02-13 02:47:05,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:05,700][root][INFO] - Training Epoch: 1/2, step 2621/7134 completed (loss: 0.0659494623541832, acc: 0.9810426831245422)
[2025-02-13 02:47:05,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:06,154][root][INFO] - Training Epoch: 1/2, step 2622/7134 completed (loss: 0.08353810757398605, acc: 0.9738134145736694)
[2025-02-13 02:47:06,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:06,609][root][INFO] - Training Epoch: 1/2, step 2623/7134 completed (loss: 0.10135931521654129, acc: 0.9701149463653564)
[2025-02-13 02:47:06,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:07,050][root][INFO] - Training Epoch: 1/2, step 2624/7134 completed (loss: 0.0715472549200058, acc: 0.9786276817321777)
[2025-02-13 02:47:07,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:07,489][root][INFO] - Training Epoch: 1/2, step 2625/7134 completed (loss: 0.07214982062578201, acc: 0.9736111164093018)
[2025-02-13 02:47:07,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:07,929][root][INFO] - Training Epoch: 1/2, step 2626/7134 completed (loss: 0.06685127317905426, acc: 0.9764851331710815)
[2025-02-13 02:47:08,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:08,376][root][INFO] - Training Epoch: 1/2, step 2627/7134 completed (loss: 0.05452127382159233, acc: 0.9851149916648865)
[2025-02-13 02:47:08,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:08,789][root][INFO] - Training Epoch: 1/2, step 2628/7134 completed (loss: 0.07672674208879471, acc: 0.9797979593276978)
[2025-02-13 02:47:08,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:09,284][root][INFO] - Training Epoch: 1/2, step 2629/7134 completed (loss: 0.047371651977300644, acc: 0.982300877571106)
[2025-02-13 02:47:09,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:09,692][root][INFO] - Training Epoch: 1/2, step 2630/7134 completed (loss: 0.06643980741500854, acc: 0.9728434681892395)
[2025-02-13 02:47:09,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:10,127][root][INFO] - Training Epoch: 1/2, step 2631/7134 completed (loss: 0.044815681874752045, acc: 0.9824561476707458)
[2025-02-13 02:47:10,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:10,562][root][INFO] - Training Epoch: 1/2, step 2632/7134 completed (loss: 0.056327659636735916, acc: 0.9823182821273804)
[2025-02-13 02:47:10,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:10,998][root][INFO] - Training Epoch: 1/2, step 2633/7134 completed (loss: 0.09843097627162933, acc: 0.980169951915741)
[2025-02-13 02:47:11,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:11,396][root][INFO] - Training Epoch: 1/2, step 2634/7134 completed (loss: 0.0318559966981411, acc: 0.9880059957504272)
[2025-02-13 02:47:11,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:11,839][root][INFO] - Training Epoch: 1/2, step 2635/7134 completed (loss: 0.13690558075904846, acc: 0.9708333611488342)
[2025-02-13 02:47:11,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:12,277][root][INFO] - Training Epoch: 1/2, step 2636/7134 completed (loss: 0.0880301222205162, acc: 0.9800853729248047)
[2025-02-13 02:47:12,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:12,688][root][INFO] - Training Epoch: 1/2, step 2637/7134 completed (loss: 0.24940545856952667, acc: 0.9371816515922546)
[2025-02-13 02:47:12,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:13,136][root][INFO] - Training Epoch: 1/2, step 2638/7134 completed (loss: 0.12345060706138611, acc: 0.9674054980278015)
[2025-02-13 02:47:13,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:13,542][root][INFO] - Training Epoch: 1/2, step 2639/7134 completed (loss: 0.15778610110282898, acc: 0.9522821307182312)
[2025-02-13 02:47:13,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:13,972][root][INFO] - Training Epoch: 1/2, step 2640/7134 completed (loss: 0.1340397298336029, acc: 0.9687890410423279)
[2025-02-13 02:47:14,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:14,402][root][INFO] - Training Epoch: 1/2, step 2641/7134 completed (loss: 0.10830003023147583, acc: 0.9749631881713867)
[2025-02-13 02:47:14,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:14,793][root][INFO] - Training Epoch: 1/2, step 2642/7134 completed (loss: 0.09743945300579071, acc: 0.9697508811950684)
[2025-02-13 02:47:14,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:15,222][root][INFO] - Training Epoch: 1/2, step 2643/7134 completed (loss: 0.1046706959605217, acc: 0.9617940187454224)
[2025-02-13 02:47:15,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:15,703][root][INFO] - Training Epoch: 1/2, step 2644/7134 completed (loss: 0.07642603665590286, acc: 0.9793689250946045)
[2025-02-13 02:47:15,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:16,115][root][INFO] - Training Epoch: 1/2, step 2645/7134 completed (loss: 0.10438980162143707, acc: 0.9737609624862671)
[2025-02-13 02:47:16,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:16,511][root][INFO] - Training Epoch: 1/2, step 2646/7134 completed (loss: 0.1894746720790863, acc: 0.9522293210029602)
[2025-02-13 02:47:16,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:16,952][root][INFO] - Training Epoch: 1/2, step 2647/7134 completed (loss: 0.0746116116642952, acc: 0.981794536113739)
[2025-02-13 02:47:17,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:17,374][root][INFO] - Training Epoch: 1/2, step 2648/7134 completed (loss: 0.05276777222752571, acc: 0.9845161437988281)
[2025-02-13 02:47:17,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:17,801][root][INFO] - Training Epoch: 1/2, step 2649/7134 completed (loss: 0.15567423403263092, acc: 0.9657227993011475)
[2025-02-13 02:47:17,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:18,192][root][INFO] - Training Epoch: 1/2, step 2650/7134 completed (loss: 0.13953527808189392, acc: 0.9531013369560242)
[2025-02-13 02:47:18,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:18,627][root][INFO] - Training Epoch: 1/2, step 2651/7134 completed (loss: 0.06768620759248734, acc: 0.9797377586364746)
[2025-02-13 02:47:18,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:19,044][root][INFO] - Training Epoch: 1/2, step 2652/7134 completed (loss: 0.06842373311519623, acc: 0.9836065769195557)
[2025-02-13 02:47:19,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:19,431][root][INFO] - Training Epoch: 1/2, step 2653/7134 completed (loss: 0.09795782715082169, acc: 0.9654605388641357)
[2025-02-13 02:47:19,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:19,846][root][INFO] - Training Epoch: 1/2, step 2654/7134 completed (loss: 0.06101159378886223, acc: 0.9901639223098755)
[2025-02-13 02:47:19,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:20,247][root][INFO] - Training Epoch: 1/2, step 2655/7134 completed (loss: 0.07077501714229584, acc: 0.9848713874816895)
[2025-02-13 02:47:20,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:20,669][root][INFO] - Training Epoch: 1/2, step 2656/7134 completed (loss: 0.07973799109458923, acc: 0.9819193482398987)
[2025-02-13 02:47:20,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:21,085][root][INFO] - Training Epoch: 1/2, step 2657/7134 completed (loss: 0.07877293229103088, acc: 0.977419376373291)
[2025-02-13 02:47:21,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:21,495][root][INFO] - Training Epoch: 1/2, step 2658/7134 completed (loss: 0.12549515068531036, acc: 0.9746328592300415)
[2025-02-13 02:47:21,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:21,871][root][INFO] - Training Epoch: 1/2, step 2659/7134 completed (loss: 0.04954536631703377, acc: 0.9865900278091431)
[2025-02-13 02:47:22,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:22,239][root][INFO] - Training Epoch: 1/2, step 2660/7134 completed (loss: 0.059944409877061844, acc: 0.9818548560142517)
[2025-02-13 02:47:22,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:22,656][root][INFO] - Training Epoch: 1/2, step 2661/7134 completed (loss: 0.05773204192519188, acc: 0.9901823401451111)
[2025-02-13 02:47:22,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:23,104][root][INFO] - Training Epoch: 1/2, step 2662/7134 completed (loss: 0.03252991661429405, acc: 0.991465151309967)
[2025-02-13 02:47:23,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:23,561][root][INFO] - Training Epoch: 1/2, step 2663/7134 completed (loss: 0.06078600138425827, acc: 0.9836257100105286)
[2025-02-13 02:47:23,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:23,997][root][INFO] - Training Epoch: 1/2, step 2664/7134 completed (loss: 0.04321535676717758, acc: 0.9909677505493164)
[2025-02-13 02:47:24,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:24,421][root][INFO] - Training Epoch: 1/2, step 2665/7134 completed (loss: 0.026183223351836205, acc: 0.99609375)
[2025-02-13 02:47:24,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:24,878][root][INFO] - Training Epoch: 1/2, step 2666/7134 completed (loss: 0.04420224204659462, acc: 0.9889841079711914)
[2025-02-13 02:47:25,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:25,294][root][INFO] - Training Epoch: 1/2, step 2667/7134 completed (loss: 0.02666579745709896, acc: 0.9943342804908752)
[2025-02-13 02:47:25,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:25,708][root][INFO] - Training Epoch: 1/2, step 2668/7134 completed (loss: 0.03137403726577759, acc: 0.9932705163955688)
[2025-02-13 02:47:25,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:26,161][root][INFO] - Training Epoch: 1/2, step 2669/7134 completed (loss: 0.029193582013249397, acc: 0.9896432757377625)
[2025-02-13 02:47:26,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:26,582][root][INFO] - Training Epoch: 1/2, step 2670/7134 completed (loss: 0.04284277185797691, acc: 0.9895150661468506)
[2025-02-13 02:47:26,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:26,997][root][INFO] - Training Epoch: 1/2, step 2671/7134 completed (loss: 0.04642116650938988, acc: 0.9866310358047485)
[2025-02-13 02:47:27,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:27,437][root][INFO] - Training Epoch: 1/2, step 2672/7134 completed (loss: 0.027989482507109642, acc: 0.9941691160202026)
[2025-02-13 02:47:27,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:27,891][root][INFO] - Training Epoch: 1/2, step 2673/7134 completed (loss: 0.032473888248205185, acc: 0.9898989796638489)
[2025-02-13 02:47:28,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:28,296][root][INFO] - Training Epoch: 1/2, step 2674/7134 completed (loss: 0.03813879191875458, acc: 0.9902371168136597)
[2025-02-13 02:47:28,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:28,743][root][INFO] - Training Epoch: 1/2, step 2675/7134 completed (loss: 0.01940993033349514, acc: 0.9945205450057983)
[2025-02-13 02:47:28,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:29,167][root][INFO] - Training Epoch: 1/2, step 2676/7134 completed (loss: 0.035364847630262375, acc: 0.9896103739738464)
[2025-02-13 02:47:29,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:29,601][root][INFO] - Training Epoch: 1/2, step 2677/7134 completed (loss: 0.03686118870973587, acc: 0.9895833134651184)
[2025-02-13 02:47:29,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:30,052][root][INFO] - Training Epoch: 1/2, step 2678/7134 completed (loss: 0.009493588469922543, acc: 0.9986807107925415)
[2025-02-13 02:47:30,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:30,507][root][INFO] - Training Epoch: 1/2, step 2679/7134 completed (loss: 0.021458633244037628, acc: 0.9921436309814453)
[2025-02-13 02:47:30,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:30,945][root][INFO] - Training Epoch: 1/2, step 2680/7134 completed (loss: 0.03178840130567551, acc: 0.992601752281189)
[2025-02-13 02:47:31,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:31,341][root][INFO] - Training Epoch: 1/2, step 2681/7134 completed (loss: 0.020057473331689835, acc: 0.9954476356506348)
[2025-02-13 02:47:31,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:31,712][root][INFO] - Training Epoch: 1/2, step 2682/7134 completed (loss: 0.010672427713871002, acc: 0.9968101978302002)
[2025-02-13 02:47:31,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:32,095][root][INFO] - Training Epoch: 1/2, step 2683/7134 completed (loss: 0.01185566559433937, acc: 0.9969651103019714)
[2025-02-13 02:47:32,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:32,475][root][INFO] - Training Epoch: 1/2, step 2684/7134 completed (loss: 0.05664682760834694, acc: 0.9892473220825195)
[2025-02-13 02:47:32,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:32,930][root][INFO] - Training Epoch: 1/2, step 2685/7134 completed (loss: 0.030558820813894272, acc: 0.9921976327896118)
[2025-02-13 02:47:33,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:33,392][root][INFO] - Training Epoch: 1/2, step 2686/7134 completed (loss: 0.06931792199611664, acc: 0.9768518805503845)
[2025-02-13 02:47:33,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:33,815][root][INFO] - Training Epoch: 1/2, step 2687/7134 completed (loss: 0.08753754943609238, acc: 0.9752650260925293)
[2025-02-13 02:47:33,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:34,253][root][INFO] - Training Epoch: 1/2, step 2688/7134 completed (loss: 0.13346053659915924, acc: 0.9669811129570007)
[2025-02-13 02:47:34,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:34,595][root][INFO] - Training Epoch: 1/2, step 2689/7134 completed (loss: 0.10494770109653473, acc: 0.9801084995269775)
[2025-02-13 02:47:34,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:35,024][root][INFO] - Training Epoch: 1/2, step 2690/7134 completed (loss: 0.10126582533121109, acc: 0.9629057049751282)
[2025-02-13 02:47:35,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:35,360][root][INFO] - Training Epoch: 1/2, step 2691/7134 completed (loss: 0.1150054857134819, acc: 0.96875)
[2025-02-13 02:47:35,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:35,787][root][INFO] - Training Epoch: 1/2, step 2692/7134 completed (loss: 0.1619207113981247, acc: 0.9627560377120972)
[2025-02-13 02:47:35,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:36,121][root][INFO] - Training Epoch: 1/2, step 2693/7134 completed (loss: 0.13910792768001556, acc: 0.9634615182876587)
[2025-02-13 02:47:36,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:36,534][root][INFO] - Training Epoch: 1/2, step 2694/7134 completed (loss: 0.1257428377866745, acc: 0.9720588326454163)
[2025-02-13 02:47:36,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:37,006][root][INFO] - Training Epoch: 1/2, step 2695/7134 completed (loss: 0.10109732300043106, acc: 0.9715242981910706)
[2025-02-13 02:47:37,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:37,440][root][INFO] - Training Epoch: 1/2, step 2696/7134 completed (loss: 0.16854040324687958, acc: 0.9570747017860413)
[2025-02-13 02:47:37,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:37,841][root][INFO] - Training Epoch: 1/2, step 2697/7134 completed (loss: 0.1482178419828415, acc: 0.9491525292396545)
[2025-02-13 02:47:37,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:38,279][root][INFO] - Training Epoch: 1/2, step 2698/7134 completed (loss: 0.10652276873588562, acc: 0.9658119678497314)
[2025-02-13 02:47:38,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:38,714][root][INFO] - Training Epoch: 1/2, step 2699/7134 completed (loss: 0.08361395448446274, acc: 0.9690949320793152)
[2025-02-13 02:47:38,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:39,022][root][INFO] - Training Epoch: 1/2, step 2700/7134 completed (loss: 0.13648776710033417, acc: 0.9584569931030273)
[2025-02-13 02:47:39,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:39,385][root][INFO] - Training Epoch: 1/2, step 2701/7134 completed (loss: 0.10632620751857758, acc: 0.9714285731315613)
[2025-02-13 02:47:39,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:39,612][root][INFO] - Training Epoch: 1/2, step 2702/7134 completed (loss: 0.12586398422718048, acc: 0.9635416865348816)
[2025-02-13 02:47:39,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:39,955][root][INFO] - Training Epoch: 1/2, step 2703/7134 completed (loss: 0.0830405130982399, acc: 0.9808917045593262)
[2025-02-13 02:47:40,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:40,404][root][INFO] - Training Epoch: 1/2, step 2704/7134 completed (loss: 0.08448726683855057, acc: 0.979784369468689)
[2025-02-13 02:47:40,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:40,839][root][INFO] - Training Epoch: 1/2, step 2705/7134 completed (loss: 0.13334831595420837, acc: 0.9722222089767456)
[2025-02-13 02:47:40,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:41,282][root][INFO] - Training Epoch: 1/2, step 2706/7134 completed (loss: 0.06492623686790466, acc: 0.9796807169914246)
[2025-02-13 02:47:41,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:41,739][root][INFO] - Training Epoch: 1/2, step 2707/7134 completed (loss: 0.11079993098974228, acc: 0.9706896543502808)
[2025-02-13 02:47:41,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:42,193][root][INFO] - Training Epoch: 1/2, step 2708/7134 completed (loss: 0.11024941504001617, acc: 0.9635343551635742)
[2025-02-13 02:47:42,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:42,649][root][INFO] - Training Epoch: 1/2, step 2709/7134 completed (loss: 0.062243442982435226, acc: 0.9902597665786743)
[2025-02-13 02:47:42,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:43,067][root][INFO] - Training Epoch: 1/2, step 2710/7134 completed (loss: 0.06667324155569077, acc: 0.9860140085220337)
[2025-02-13 02:47:43,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:43,474][root][INFO] - Training Epoch: 1/2, step 2711/7134 completed (loss: 0.06059658154845238, acc: 0.9811320900917053)
[2025-02-13 02:47:43,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:43,908][root][INFO] - Training Epoch: 1/2, step 2712/7134 completed (loss: 0.11589827388525009, acc: 0.9598214030265808)
[2025-02-13 02:47:44,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:44,346][root][INFO] - Training Epoch: 1/2, step 2713/7134 completed (loss: 0.11794435232877731, acc: 0.9693593382835388)
[2025-02-13 02:47:44,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:44,774][root][INFO] - Training Epoch: 1/2, step 2714/7134 completed (loss: 0.06224663555622101, acc: 0.981697142124176)
[2025-02-13 02:47:44,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:45,208][root][INFO] - Training Epoch: 1/2, step 2715/7134 completed (loss: 0.06221575662493706, acc: 0.9834586381912231)
[2025-02-13 02:47:45,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:45,639][root][INFO] - Training Epoch: 1/2, step 2716/7134 completed (loss: 0.07827413082122803, acc: 0.9737876653671265)
[2025-02-13 02:47:45,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:46,063][root][INFO] - Training Epoch: 1/2, step 2717/7134 completed (loss: 0.13828013837337494, acc: 0.9527720808982849)
[2025-02-13 02:47:46,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:46,492][root][INFO] - Training Epoch: 1/2, step 2718/7134 completed (loss: 0.19763541221618652, acc: 0.952464759349823)
[2025-02-13 02:47:46,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:46,931][root][INFO] - Training Epoch: 1/2, step 2719/7134 completed (loss: 0.10311713814735413, acc: 0.962774932384491)
[2025-02-13 02:47:47,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:47,360][root][INFO] - Training Epoch: 1/2, step 2720/7134 completed (loss: 0.08996610343456268, acc: 0.9700315594673157)
[2025-02-13 02:47:47,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:47,715][root][INFO] - Training Epoch: 1/2, step 2721/7134 completed (loss: 0.0903225764632225, acc: 0.9811594486236572)
[2025-02-13 02:47:47,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:47,999][root][INFO] - Training Epoch: 1/2, step 2722/7134 completed (loss: 0.18544906377792358, acc: 0.967391312122345)
[2025-02-13 02:47:48,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:48,421][root][INFO] - Training Epoch: 1/2, step 2723/7134 completed (loss: 0.14449000358581543, acc: 0.9581795930862427)
[2025-02-13 02:47:48,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:48,855][root][INFO] - Training Epoch: 1/2, step 2724/7134 completed (loss: 0.12100765109062195, acc: 0.9659781455993652)
[2025-02-13 02:47:48,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:49,282][root][INFO] - Training Epoch: 1/2, step 2725/7134 completed (loss: 0.07948772609233856, acc: 0.9819375872612)
[2025-02-13 02:47:49,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:49,672][root][INFO] - Training Epoch: 1/2, step 2726/7134 completed (loss: 0.09357663244009018, acc: 0.9778597950935364)
[2025-02-13 02:47:49,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:50,139][root][INFO] - Training Epoch: 1/2, step 2727/7134 completed (loss: 0.07903773337602615, acc: 0.9759259223937988)
[2025-02-13 02:47:50,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:50,548][root][INFO] - Training Epoch: 1/2, step 2728/7134 completed (loss: 0.20049770176410675, acc: 0.9380733966827393)
[2025-02-13 02:47:50,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:50,946][root][INFO] - Training Epoch: 1/2, step 2729/7134 completed (loss: 0.11287634819746017, acc: 0.9634146094322205)
[2025-02-13 02:47:51,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:51,346][root][INFO] - Training Epoch: 1/2, step 2730/7134 completed (loss: 0.10418626666069031, acc: 0.9729166626930237)
[2025-02-13 02:47:51,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:51,721][root][INFO] - Training Epoch: 1/2, step 2731/7134 completed (loss: 0.13162316381931305, acc: 0.9537712931632996)
[2025-02-13 02:47:51,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:52,158][root][INFO] - Training Epoch: 1/2, step 2732/7134 completed (loss: 0.09398076683282852, acc: 0.9670050740242004)
[2025-02-13 02:47:52,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:52,585][root][INFO] - Training Epoch: 1/2, step 2733/7134 completed (loss: 0.10619136691093445, acc: 0.9660786986351013)
[2025-02-13 02:47:52,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:52,969][root][INFO] - Training Epoch: 1/2, step 2734/7134 completed (loss: 0.0879514142870903, acc: 0.9758551120758057)
[2025-02-13 02:47:53,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:53,439][root][INFO] - Training Epoch: 1/2, step 2735/7134 completed (loss: 0.07443489879369736, acc: 0.9728997349739075)
[2025-02-13 02:47:53,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:53,800][root][INFO] - Training Epoch: 1/2, step 2736/7134 completed (loss: 0.1183667778968811, acc: 0.9682835936546326)
[2025-02-13 02:47:53,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:54,206][root][INFO] - Training Epoch: 1/2, step 2737/7134 completed (loss: 0.09714733064174652, acc: 0.971947193145752)
[2025-02-13 02:47:54,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:54,602][root][INFO] - Training Epoch: 1/2, step 2738/7134 completed (loss: 0.07771077007055283, acc: 0.9728571176528931)
[2025-02-13 02:47:54,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:55,008][root][INFO] - Training Epoch: 1/2, step 2739/7134 completed (loss: 0.11693094670772552, acc: 0.9709091186523438)
[2025-02-13 02:47:55,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:55,419][root][INFO] - Training Epoch: 1/2, step 2740/7134 completed (loss: 0.05934596434235573, acc: 0.9811320900917053)
[2025-02-13 02:47:55,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:55,783][root][INFO] - Training Epoch: 1/2, step 2741/7134 completed (loss: 0.10728966444730759, acc: 0.9626168012619019)
[2025-02-13 02:47:55,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:56,221][root][INFO] - Training Epoch: 1/2, step 2742/7134 completed (loss: 0.05733763054013252, acc: 0.9791666865348816)
[2025-02-13 02:47:56,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:56,602][root][INFO] - Training Epoch: 1/2, step 2743/7134 completed (loss: 0.03766877204179764, acc: 0.9842105507850647)
[2025-02-13 02:47:56,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:56,831][root][INFO] - Training Epoch: 1/2, step 2744/7134 completed (loss: 0.037449341267347336, acc: 0.992337167263031)
[2025-02-13 02:47:56,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:57,178][root][INFO] - Training Epoch: 1/2, step 2745/7134 completed (loss: 0.12646718323230743, acc: 0.9686411023139954)
[2025-02-13 02:47:57,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:57,543][root][INFO] - Training Epoch: 1/2, step 2746/7134 completed (loss: 0.06434794515371323, acc: 0.9871794581413269)
[2025-02-13 02:47:57,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:57,927][root][INFO] - Training Epoch: 1/2, step 2747/7134 completed (loss: 0.06018752604722977, acc: 0.9803439974784851)
[2025-02-13 02:47:58,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:58,180][root][INFO] - Training Epoch: 1/2, step 2748/7134 completed (loss: 0.04133542999625206, acc: 0.993630588054657)
[2025-02-13 02:47:58,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:58,482][root][INFO] - Training Epoch: 1/2, step 2749/7134 completed (loss: 0.05808589607477188, acc: 0.9781420826911926)
[2025-02-13 02:47:58,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:58,880][root][INFO] - Training Epoch: 1/2, step 2750/7134 completed (loss: 0.05388304591178894, acc: 0.9849624037742615)
[2025-02-13 02:47:58,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:59,256][root][INFO] - Training Epoch: 1/2, step 2751/7134 completed (loss: 0.06323983520269394, acc: 0.9822784662246704)
[2025-02-13 02:47:59,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:59,652][root][INFO] - Training Epoch: 1/2, step 2752/7134 completed (loss: 0.07233192771673203, acc: 0.9790874719619751)
[2025-02-13 02:47:59,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:00,047][root][INFO] - Training Epoch: 1/2, step 2753/7134 completed (loss: 0.11787916719913483, acc: 0.9624060392379761)
[2025-02-13 02:48:00,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:00,483][root][INFO] - Training Epoch: 1/2, step 2754/7134 completed (loss: 0.0899413675069809, acc: 0.9634888172149658)
[2025-02-13 02:48:00,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:00,884][root][INFO] - Training Epoch: 1/2, step 2755/7134 completed (loss: 0.031624577939510345, acc: 0.9922480583190918)
[2025-02-13 02:48:01,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:01,235][root][INFO] - Training Epoch: 1/2, step 2756/7134 completed (loss: 0.059740617871284485, acc: 0.9790475964546204)
[2025-02-13 02:48:01,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:01,504][root][INFO] - Training Epoch: 1/2, step 2757/7134 completed (loss: 0.08923118561506271, acc: 0.9726775884628296)
[2025-02-13 02:48:01,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:01,888][root][INFO] - Training Epoch: 1/2, step 2758/7134 completed (loss: 0.09607428312301636, acc: 0.9726027250289917)
[2025-02-13 02:48:02,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:02,214][root][INFO] - Training Epoch: 1/2, step 2759/7134 completed (loss: 0.057198796421289444, acc: 0.9832776188850403)
[2025-02-13 02:48:02,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:02,568][root][INFO] - Training Epoch: 1/2, step 2760/7134 completed (loss: 0.08211736381053925, acc: 0.961904764175415)
[2025-02-13 02:48:02,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:02,979][root][INFO] - Training Epoch: 1/2, step 2761/7134 completed (loss: 0.12094111740589142, acc: 0.9671717286109924)
[2025-02-13 02:48:03,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:03,440][root][INFO] - Training Epoch: 1/2, step 2762/7134 completed (loss: 0.08488292992115021, acc: 0.9769850373268127)
[2025-02-13 02:48:03,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:03,873][root][INFO] - Training Epoch: 1/2, step 2763/7134 completed (loss: 0.0824185162782669, acc: 0.9811557531356812)
[2025-02-13 02:48:04,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:04,307][root][INFO] - Training Epoch: 1/2, step 2764/7134 completed (loss: 0.10537943243980408, acc: 0.9657443761825562)
[2025-02-13 02:48:04,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:04,743][root][INFO] - Training Epoch: 1/2, step 2765/7134 completed (loss: 0.06819485872983932, acc: 0.977011501789093)
[2025-02-13 02:48:04,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:05,195][root][INFO] - Training Epoch: 1/2, step 2766/7134 completed (loss: 0.04515961557626724, acc: 0.987075924873352)
[2025-02-13 02:48:05,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:05,632][root][INFO] - Training Epoch: 1/2, step 2767/7134 completed (loss: 0.12734535336494446, acc: 0.9654321074485779)
[2025-02-13 02:48:05,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:06,054][root][INFO] - Training Epoch: 1/2, step 2768/7134 completed (loss: 0.0701797604560852, acc: 0.9801980257034302)
[2025-02-13 02:48:06,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:06,494][root][INFO] - Training Epoch: 1/2, step 2769/7134 completed (loss: 0.0943291187286377, acc: 0.9763513803482056)
[2025-02-13 02:48:06,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:06,946][root][INFO] - Training Epoch: 1/2, step 2770/7134 completed (loss: 0.05554141476750374, acc: 0.9860917925834656)
[2025-02-13 02:48:07,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:07,446][root][INFO] - Training Epoch: 1/2, step 2771/7134 completed (loss: 0.09312877058982849, acc: 0.9742765426635742)
[2025-02-13 02:48:07,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:07,906][root][INFO] - Training Epoch: 1/2, step 2772/7134 completed (loss: 0.05836917459964752, acc: 0.9798657894134521)
[2025-02-13 02:48:08,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:08,366][root][INFO] - Training Epoch: 1/2, step 2773/7134 completed (loss: 0.090673066675663, acc: 0.9760000109672546)
[2025-02-13 02:48:08,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:08,816][root][INFO] - Training Epoch: 1/2, step 2774/7134 completed (loss: 0.08781901001930237, acc: 0.9736841917037964)
[2025-02-13 02:48:08,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:09,247][root][INFO] - Training Epoch: 1/2, step 2775/7134 completed (loss: 0.065469890832901, acc: 0.9852761030197144)
[2025-02-13 02:48:09,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:09,732][root][INFO] - Training Epoch: 1/2, step 2776/7134 completed (loss: 0.060671355575323105, acc: 0.9815950989723206)
[2025-02-13 02:48:09,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:10,167][root][INFO] - Training Epoch: 1/2, step 2777/7134 completed (loss: 0.06307504326105118, acc: 0.9780488014221191)
[2025-02-13 02:48:10,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:10,617][root][INFO] - Training Epoch: 1/2, step 2778/7134 completed (loss: 0.06210770457983017, acc: 0.9849362969398499)
[2025-02-13 02:48:10,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:11,066][root][INFO] - Training Epoch: 1/2, step 2779/7134 completed (loss: 0.10219905525445938, acc: 0.9736286997795105)
[2025-02-13 02:48:11,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:11,517][root][INFO] - Training Epoch: 1/2, step 2780/7134 completed (loss: 0.08040283620357513, acc: 0.9746835231781006)
[2025-02-13 02:48:11,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:11,976][root][INFO] - Training Epoch: 1/2, step 2781/7134 completed (loss: 0.06441625207662582, acc: 0.9831606149673462)
[2025-02-13 02:48:12,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:12,429][root][INFO] - Training Epoch: 1/2, step 2782/7134 completed (loss: 0.061899177730083466, acc: 0.9852941036224365)
[2025-02-13 02:48:12,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:12,862][root][INFO] - Training Epoch: 1/2, step 2783/7134 completed (loss: 0.06136176362633705, acc: 0.9759759902954102)
[2025-02-13 02:48:13,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:13,316][root][INFO] - Training Epoch: 1/2, step 2784/7134 completed (loss: 0.07645855098962784, acc: 0.9789695143699646)
[2025-02-13 02:48:13,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:13,785][root][INFO] - Training Epoch: 1/2, step 2785/7134 completed (loss: 0.035073183476924896, acc: 0.9879912734031677)
[2025-02-13 02:48:13,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:14,242][root][INFO] - Training Epoch: 1/2, step 2786/7134 completed (loss: 0.07126062363386154, acc: 0.9833852648735046)
[2025-02-13 02:48:14,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:14,713][root][INFO] - Training Epoch: 1/2, step 2787/7134 completed (loss: 0.05755063146352768, acc: 0.9840637445449829)
[2025-02-13 02:48:14,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:15,149][root][INFO] - Training Epoch: 1/2, step 2788/7134 completed (loss: 0.07603650540113449, acc: 0.9725130796432495)
[2025-02-13 02:48:15,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:15,520][root][INFO] - Training Epoch: 1/2, step 2789/7134 completed (loss: 0.2114304155111313, acc: 0.9481267929077148)
[2025-02-13 02:48:15,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:15,895][root][INFO] - Training Epoch: 1/2, step 2790/7134 completed (loss: 0.21138830482959747, acc: 0.9580574035644531)
[2025-02-13 02:48:16,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:16,334][root][INFO] - Training Epoch: 1/2, step 2791/7134 completed (loss: 0.05880828946828842, acc: 0.9865471124649048)
[2025-02-13 02:48:16,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:16,792][root][INFO] - Training Epoch: 1/2, step 2792/7134 completed (loss: 0.045567747205495834, acc: 0.9871944189071655)
[2025-02-13 02:48:16,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:17,239][root][INFO] - Training Epoch: 1/2, step 2793/7134 completed (loss: 0.07033555954694748, acc: 0.9757084846496582)
[2025-02-13 02:48:17,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:17,573][root][INFO] - Training Epoch: 1/2, step 2794/7134 completed (loss: 0.09466876834630966, acc: 0.9706840515136719)
[2025-02-13 02:48:17,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:17,984][root][INFO] - Training Epoch: 1/2, step 2795/7134 completed (loss: 0.22642453014850616, acc: 0.9373549818992615)
[2025-02-13 02:48:18,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:18,338][root][INFO] - Training Epoch: 1/2, step 2796/7134 completed (loss: 0.2616545557975769, acc: 0.9377880096435547)
[2025-02-13 02:48:18,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:18,575][root][INFO] - Training Epoch: 1/2, step 2797/7134 completed (loss: 0.20693786442279816, acc: 0.9683544039726257)
[2025-02-13 02:48:18,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:18,970][root][INFO] - Training Epoch: 1/2, step 2798/7134 completed (loss: 0.16972051560878754, acc: 0.9474790096282959)
[2025-02-13 02:48:19,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:19,347][root][INFO] - Training Epoch: 1/2, step 2799/7134 completed (loss: 0.14008434116840363, acc: 0.9597197771072388)
[2025-02-13 02:48:19,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:19,706][root][INFO] - Training Epoch: 1/2, step 2800/7134 completed (loss: 0.12580488622188568, acc: 0.9648506045341492)
[2025-02-13 02:48:19,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:20,152][root][INFO] - Training Epoch: 1/2, step 2801/7134 completed (loss: 0.08116389065980911, acc: 0.9744245409965515)
[2025-02-13 02:48:20,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:20,602][root][INFO] - Training Epoch: 1/2, step 2802/7134 completed (loss: 0.11318041384220123, acc: 0.971285879611969)
[2025-02-13 02:48:20,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:21,040][root][INFO] - Training Epoch: 1/2, step 2803/7134 completed (loss: 0.03916902467608452, acc: 0.985855758190155)
[2025-02-13 02:48:21,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:21,508][root][INFO] - Training Epoch: 1/2, step 2804/7134 completed (loss: 0.0808897614479065, acc: 0.9801980257034302)
[2025-02-13 02:48:21,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:21,924][root][INFO] - Training Epoch: 1/2, step 2805/7134 completed (loss: 0.047602616250514984, acc: 0.9881423115730286)
[2025-02-13 02:48:22,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:22,331][root][INFO] - Training Epoch: 1/2, step 2806/7134 completed (loss: 0.09443306922912598, acc: 0.9721311330795288)
[2025-02-13 02:48:22,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:22,723][root][INFO] - Training Epoch: 1/2, step 2807/7134 completed (loss: 0.1522539258003235, acc: 0.9480249285697937)
[2025-02-13 02:48:22,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:23,162][root][INFO] - Training Epoch: 1/2, step 2808/7134 completed (loss: 0.05739273503422737, acc: 0.980571448802948)
[2025-02-13 02:48:23,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:23,566][root][INFO] - Training Epoch: 1/2, step 2809/7134 completed (loss: 0.08617125451564789, acc: 0.9750445485115051)
[2025-02-13 02:48:23,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:24,057][root][INFO] - Training Epoch: 1/2, step 2810/7134 completed (loss: 0.04368316009640694, acc: 0.9869358539581299)
[2025-02-13 02:48:24,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:24,502][root][INFO] - Training Epoch: 1/2, step 2811/7134 completed (loss: 0.024308687075972557, acc: 0.9964200258255005)
[2025-02-13 02:48:24,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:24,944][root][INFO] - Training Epoch: 1/2, step 2812/7134 completed (loss: 0.037866346538066864, acc: 0.9861111044883728)
[2025-02-13 02:48:25,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:25,394][root][INFO] - Training Epoch: 1/2, step 2813/7134 completed (loss: 0.028334233909845352, acc: 0.988664984703064)
[2025-02-13 02:48:25,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:25,803][root][INFO] - Training Epoch: 1/2, step 2814/7134 completed (loss: 0.048793893307447433, acc: 0.9896103739738464)
[2025-02-13 02:48:25,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:26,238][root][INFO] - Training Epoch: 1/2, step 2815/7134 completed (loss: 0.07522937655448914, acc: 0.982206404209137)
[2025-02-13 02:48:26,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:26,714][root][INFO] - Training Epoch: 1/2, step 2816/7134 completed (loss: 0.08047566562891006, acc: 0.9736495614051819)
[2025-02-13 02:48:26,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:27,195][root][INFO] - Training Epoch: 1/2, step 2817/7134 completed (loss: 0.07226202636957169, acc: 0.9821802973747253)
[2025-02-13 02:48:27,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:27,649][root][INFO] - Training Epoch: 1/2, step 2818/7134 completed (loss: 0.05118832364678383, acc: 0.984649121761322)
[2025-02-13 02:48:27,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:28,044][root][INFO] - Training Epoch: 1/2, step 2819/7134 completed (loss: 0.04830564185976982, acc: 0.9886914491653442)
[2025-02-13 02:48:28,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:28,474][root][INFO] - Training Epoch: 1/2, step 2820/7134 completed (loss: 0.1506357491016388, acc: 0.9631811380386353)
[2025-02-13 02:48:28,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:28,903][root][INFO] - Training Epoch: 1/2, step 2821/7134 completed (loss: 0.09589725732803345, acc: 0.9737156629562378)
[2025-02-13 02:48:29,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:29,340][root][INFO] - Training Epoch: 1/2, step 2822/7134 completed (loss: 0.13258592784404755, acc: 0.9633375406265259)
[2025-02-13 02:48:29,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:29,828][root][INFO] - Training Epoch: 1/2, step 2823/7134 completed (loss: 0.15366990864276886, acc: 0.9531915187835693)
[2025-02-13 02:48:29,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:30,249][root][INFO] - Training Epoch: 1/2, step 2824/7134 completed (loss: 0.1539275199174881, acc: 0.948051929473877)
[2025-02-13 02:48:30,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:30,678][root][INFO] - Training Epoch: 1/2, step 2825/7134 completed (loss: 0.09997677057981491, acc: 0.9812080264091492)
[2025-02-13 02:48:30,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:31,111][root][INFO] - Training Epoch: 1/2, step 2826/7134 completed (loss: 0.13902251422405243, acc: 0.9608433842658997)
[2025-02-13 02:48:31,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:31,547][root][INFO] - Training Epoch: 1/2, step 2827/7134 completed (loss: 0.13649000227451324, acc: 0.961685836315155)
[2025-02-13 02:48:31,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:31,984][root][INFO] - Training Epoch: 1/2, step 2828/7134 completed (loss: 0.07727234810590744, acc: 0.984000027179718)
[2025-02-13 02:48:32,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:32,422][root][INFO] - Training Epoch: 1/2, step 2829/7134 completed (loss: 0.11630535125732422, acc: 0.9604365825653076)
[2025-02-13 02:48:32,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:32,854][root][INFO] - Training Epoch: 1/2, step 2830/7134 completed (loss: 0.035807445645332336, acc: 0.9908925294876099)
[2025-02-13 02:48:32,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:33,268][root][INFO] - Training Epoch: 1/2, step 2831/7134 completed (loss: 0.06391674280166626, acc: 0.9810426831245422)
[2025-02-13 02:48:33,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:33,687][root][INFO] - Training Epoch: 1/2, step 2832/7134 completed (loss: 0.07028891146183014, acc: 0.9903846383094788)
[2025-02-13 02:48:33,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:34,128][root][INFO] - Training Epoch: 1/2, step 2833/7134 completed (loss: 0.07038391381502151, acc: 0.9836257100105286)
[2025-02-13 02:48:34,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:34,566][root][INFO] - Training Epoch: 1/2, step 2834/7134 completed (loss: 0.10320018976926804, acc: 0.9746192693710327)
[2025-02-13 02:48:34,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:34,999][root][INFO] - Training Epoch: 1/2, step 2835/7134 completed (loss: 0.10053552687168121, acc: 0.9757673740386963)
[2025-02-13 02:48:35,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:35,387][root][INFO] - Training Epoch: 1/2, step 2836/7134 completed (loss: 0.11817017942667007, acc: 0.971563994884491)
[2025-02-13 02:48:35,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:35,826][root][INFO] - Training Epoch: 1/2, step 2837/7134 completed (loss: 0.0763377696275711, acc: 0.9770269989967346)
[2025-02-13 02:48:35,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:36,300][root][INFO] - Training Epoch: 1/2, step 2838/7134 completed (loss: 0.12534219026565552, acc: 0.965481162071228)
[2025-02-13 02:48:36,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:36,723][root][INFO] - Training Epoch: 1/2, step 2839/7134 completed (loss: 0.08317571878433228, acc: 0.9772382378578186)
[2025-02-13 02:48:36,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:37,185][root][INFO] - Training Epoch: 1/2, step 2840/7134 completed (loss: 0.045146748423576355, acc: 0.9875690340995789)
[2025-02-13 02:48:37,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:37,613][root][INFO] - Training Epoch: 1/2, step 2841/7134 completed (loss: 0.10253603756427765, acc: 0.9723502397537231)
[2025-02-13 02:48:37,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:38,065][root][INFO] - Training Epoch: 1/2, step 2842/7134 completed (loss: 0.11835864931344986, acc: 0.9623376727104187)
[2025-02-13 02:48:38,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:38,474][root][INFO] - Training Epoch: 1/2, step 2843/7134 completed (loss: 0.04939357936382294, acc: 0.9808823466300964)
[2025-02-13 02:48:38,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:38,919][root][INFO] - Training Epoch: 1/2, step 2844/7134 completed (loss: 0.16637185215950012, acc: 0.9575923681259155)
[2025-02-13 02:48:39,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:39,321][root][INFO] - Training Epoch: 1/2, step 2845/7134 completed (loss: 0.05587654933333397, acc: 0.9826689958572388)
[2025-02-13 02:48:39,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:39,733][root][INFO] - Training Epoch: 1/2, step 2846/7134 completed (loss: 0.15254880487918854, acc: 0.9571663737297058)
[2025-02-13 02:48:39,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:40,175][root][INFO] - Training Epoch: 1/2, step 2847/7134 completed (loss: 0.10533461719751358, acc: 0.9677419066429138)
[2025-02-13 02:48:40,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:40,491][root][INFO] - Training Epoch: 1/2, step 2848/7134 completed (loss: 0.09731823951005936, acc: 0.9673659801483154)
[2025-02-13 02:48:40,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:40,919][root][INFO] - Training Epoch: 1/2, step 2849/7134 completed (loss: 0.06882885843515396, acc: 0.9792531132698059)
[2025-02-13 02:48:41,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:41,320][root][INFO] - Training Epoch: 1/2, step 2850/7134 completed (loss: 0.06705249845981598, acc: 0.9806451797485352)
[2025-02-13 02:48:41,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:41,727][root][INFO] - Training Epoch: 1/2, step 2851/7134 completed (loss: 0.06178374961018562, acc: 0.979742169380188)
[2025-02-13 02:48:41,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:42,114][root][INFO] - Training Epoch: 1/2, step 2852/7134 completed (loss: 0.03504588082432747, acc: 0.9920477271080017)
[2025-02-13 02:48:42,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:42,456][root][INFO] - Training Epoch: 1/2, step 2853/7134 completed (loss: 0.06565356254577637, acc: 0.9822335243225098)
[2025-02-13 02:48:42,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:42,873][root][INFO] - Training Epoch: 1/2, step 2854/7134 completed (loss: 0.07671904563903809, acc: 0.9806835055351257)
[2025-02-13 02:48:43,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:43,270][root][INFO] - Training Epoch: 1/2, step 2855/7134 completed (loss: 0.06738285720348358, acc: 0.9903069734573364)
[2025-02-13 02:48:43,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:43,720][root][INFO] - Training Epoch: 1/2, step 2856/7134 completed (loss: 0.030828239396214485, acc: 0.9887164831161499)
[2025-02-13 02:48:43,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:44,143][root][INFO] - Training Epoch: 1/2, step 2857/7134 completed (loss: 0.08397217839956284, acc: 0.9797688126564026)
[2025-02-13 02:48:44,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:44,529][root][INFO] - Training Epoch: 1/2, step 2858/7134 completed (loss: 0.023042961955070496, acc: 0.9936908483505249)
[2025-02-13 02:48:44,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:44,948][root][INFO] - Training Epoch: 1/2, step 2859/7134 completed (loss: 0.07678532600402832, acc: 0.9776847958564758)
[2025-02-13 02:48:45,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:45,347][root][INFO] - Training Epoch: 1/2, step 2860/7134 completed (loss: 0.055940110236406326, acc: 0.9798164963722229)
[2025-02-13 02:48:45,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:45,747][root][INFO] - Training Epoch: 1/2, step 2861/7134 completed (loss: 0.03154701739549637, acc: 0.9918864369392395)
[2025-02-13 02:48:45,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:46,143][root][INFO] - Training Epoch: 1/2, step 2862/7134 completed (loss: 0.04191764071583748, acc: 0.9879931211471558)
[2025-02-13 02:48:46,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:46,589][root][INFO] - Training Epoch: 1/2, step 2863/7134 completed (loss: 0.024868115782737732, acc: 0.993630588054657)
[2025-02-13 02:48:46,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:46,910][root][INFO] - Training Epoch: 1/2, step 2864/7134 completed (loss: 0.06738810241222382, acc: 0.984415590763092)
[2025-02-13 02:48:47,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:47,324][root][INFO] - Training Epoch: 1/2, step 2865/7134 completed (loss: 0.050949838012456894, acc: 0.9847826361656189)
[2025-02-13 02:48:47,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:47,720][root][INFO] - Training Epoch: 1/2, step 2866/7134 completed (loss: 0.0831986591219902, acc: 0.9760836958885193)
[2025-02-13 02:48:47,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:48,058][root][INFO] - Training Epoch: 1/2, step 2867/7134 completed (loss: 0.039618607610464096, acc: 0.9907975196838379)
[2025-02-13 02:48:48,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:48,451][root][INFO] - Training Epoch: 1/2, step 2868/7134 completed (loss: 0.023962076753377914, acc: 0.9943820238113403)
[2025-02-13 02:48:48,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:48,836][root][INFO] - Training Epoch: 1/2, step 2869/7134 completed (loss: 0.06796078383922577, acc: 0.9912152290344238)
[2025-02-13 02:48:48,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:49,248][root][INFO] - Training Epoch: 1/2, step 2870/7134 completed (loss: 0.0969700813293457, acc: 0.9790356159210205)
[2025-02-13 02:48:49,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:49,639][root][INFO] - Training Epoch: 1/2, step 2871/7134 completed (loss: 0.07808828353881836, acc: 0.9859594106674194)
[2025-02-13 02:48:49,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:50,051][root][INFO] - Training Epoch: 1/2, step 2872/7134 completed (loss: 0.023151149973273277, acc: 0.9921875)
[2025-02-13 02:48:50,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:50,451][root][INFO] - Training Epoch: 1/2, step 2873/7134 completed (loss: 0.03821418806910515, acc: 0.985401451587677)
[2025-02-13 02:48:50,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:50,865][root][INFO] - Training Epoch: 1/2, step 2874/7134 completed (loss: 0.044334352016448975, acc: 0.9859594106674194)
[2025-02-13 02:48:51,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:51,289][root][INFO] - Training Epoch: 1/2, step 2875/7134 completed (loss: 0.03786740079522133, acc: 0.9843527674674988)
[2025-02-13 02:48:51,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:51,683][root][INFO] - Training Epoch: 1/2, step 2876/7134 completed (loss: 0.16320595145225525, acc: 0.9723502397537231)
[2025-02-13 02:48:51,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:52,086][root][INFO] - Training Epoch: 1/2, step 2877/7134 completed (loss: 0.028596652671694756, acc: 0.9912663698196411)
[2025-02-13 02:48:52,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:52,472][root][INFO] - Training Epoch: 1/2, step 2878/7134 completed (loss: 0.04248370602726936, acc: 0.9957627058029175)
[2025-02-13 02:48:52,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:52,864][root][INFO] - Training Epoch: 1/2, step 2879/7134 completed (loss: 0.019977951422333717, acc: 0.9952977895736694)
[2025-02-13 02:48:53,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:53,276][root][INFO] - Training Epoch: 1/2, step 2880/7134 completed (loss: 0.03286002203822136, acc: 0.9921875)
[2025-02-13 02:48:53,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:53,649][root][INFO] - Training Epoch: 1/2, step 2881/7134 completed (loss: 0.06892910599708557, acc: 0.980322003364563)
[2025-02-13 02:48:53,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:54,082][root][INFO] - Training Epoch: 1/2, step 2882/7134 completed (loss: 0.09825997054576874, acc: 0.9728729724884033)
[2025-02-13 02:48:54,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:54,495][root][INFO] - Training Epoch: 1/2, step 2883/7134 completed (loss: 0.06303750723600388, acc: 0.9814814925193787)
[2025-02-13 02:48:54,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:54,890][root][INFO] - Training Epoch: 1/2, step 2884/7134 completed (loss: 0.12412692606449127, acc: 0.9698340892791748)
[2025-02-13 02:48:55,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:55,281][root][INFO] - Training Epoch: 1/2, step 2885/7134 completed (loss: 0.07365154474973679, acc: 0.9802731275558472)
[2025-02-13 02:48:55,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:55,673][root][INFO] - Training Epoch: 1/2, step 2886/7134 completed (loss: 0.13509327173233032, acc: 0.9686567187309265)
[2025-02-13 02:48:55,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:56,123][root][INFO] - Training Epoch: 1/2, step 2887/7134 completed (loss: 0.08361051976680756, acc: 0.9798816442489624)
[2025-02-13 02:48:56,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:56,565][root][INFO] - Training Epoch: 1/2, step 2888/7134 completed (loss: 0.08781543374061584, acc: 0.9778085947036743)
[2025-02-13 02:48:56,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:56,977][root][INFO] - Training Epoch: 1/2, step 2889/7134 completed (loss: 0.05887099727988243, acc: 0.9833794832229614)
[2025-02-13 02:48:57,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:57,412][root][INFO] - Training Epoch: 1/2, step 2890/7134 completed (loss: 0.052509695291519165, acc: 0.9826897382736206)
[2025-02-13 02:48:57,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:57,847][root][INFO] - Training Epoch: 1/2, step 2891/7134 completed (loss: 0.10478372126817703, acc: 0.9707692265510559)
[2025-02-13 02:48:57,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:58,287][root][INFO] - Training Epoch: 1/2, step 2892/7134 completed (loss: 0.08694005757570267, acc: 0.9722921848297119)
[2025-02-13 02:48:58,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:58,725][root][INFO] - Training Epoch: 1/2, step 2893/7134 completed (loss: 0.10895869880914688, acc: 0.9646910429000854)
[2025-02-13 02:48:58,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:59,140][root][INFO] - Training Epoch: 1/2, step 2894/7134 completed (loss: 0.06120289862155914, acc: 0.9764705896377563)
[2025-02-13 02:48:59,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:59,516][root][INFO] - Training Epoch: 1/2, step 2895/7134 completed (loss: 0.04911333695054054, acc: 0.9867674708366394)
[2025-02-13 02:48:59,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:59,951][root][INFO] - Training Epoch: 1/2, step 2896/7134 completed (loss: 0.04304690659046173, acc: 0.9885495901107788)
[2025-02-13 02:49:00,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:00,345][root][INFO] - Training Epoch: 1/2, step 2897/7134 completed (loss: 0.09055212140083313, acc: 0.9698046445846558)
[2025-02-13 02:49:00,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:00,760][root][INFO] - Training Epoch: 1/2, step 2898/7134 completed (loss: 0.05231244117021561, acc: 0.9869281053543091)
[2025-02-13 02:49:00,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:01,165][root][INFO] - Training Epoch: 1/2, step 2899/7134 completed (loss: 0.057020120322704315, acc: 0.9883720874786377)
[2025-02-13 02:49:01,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:01,611][root][INFO] - Training Epoch: 1/2, step 2900/7134 completed (loss: 0.06587263941764832, acc: 0.9864197373390198)
[2025-02-13 02:49:01,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:02,024][root][INFO] - Training Epoch: 1/2, step 2901/7134 completed (loss: 0.06262579560279846, acc: 0.977707028388977)
[2025-02-13 02:49:02,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:02,382][root][INFO] - Training Epoch: 1/2, step 2902/7134 completed (loss: 0.073072150349617, acc: 0.981203019618988)
[2025-02-13 02:49:02,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:02,816][root][INFO] - Training Epoch: 1/2, step 2903/7134 completed (loss: 0.09173420071601868, acc: 0.9743589758872986)
[2025-02-13 02:49:02,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:03,252][root][INFO] - Training Epoch: 1/2, step 2904/7134 completed (loss: 0.04270809888839722, acc: 0.9875156283378601)
[2025-02-13 02:49:03,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:03,686][root][INFO] - Training Epoch: 1/2, step 2905/7134 completed (loss: 0.08370975404977798, acc: 0.9762202501296997)
[2025-02-13 02:49:03,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:04,126][root][INFO] - Training Epoch: 1/2, step 2906/7134 completed (loss: 0.04621085152029991, acc: 0.9880239367485046)
[2025-02-13 02:49:04,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:04,546][root][INFO] - Training Epoch: 1/2, step 2907/7134 completed (loss: 0.0778098776936531, acc: 0.9757738709449768)
[2025-02-13 02:49:04,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:04,977][root][INFO] - Training Epoch: 1/2, step 2908/7134 completed (loss: 0.051414258778095245, acc: 0.9857697486877441)
[2025-02-13 02:49:05,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:05,420][root][INFO] - Training Epoch: 1/2, step 2909/7134 completed (loss: 0.055948514491319656, acc: 0.9856114983558655)
[2025-02-13 02:49:05,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:05,868][root][INFO] - Training Epoch: 1/2, step 2910/7134 completed (loss: 0.1040862426161766, acc: 0.9721627235412598)
[2025-02-13 02:49:06,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:06,335][root][INFO] - Training Epoch: 1/2, step 2911/7134 completed (loss: 0.053912900388240814, acc: 0.982550323009491)
[2025-02-13 02:49:06,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:06,803][root][INFO] - Training Epoch: 1/2, step 2912/7134 completed (loss: 0.05986235663294792, acc: 0.9820689558982849)
[2025-02-13 02:49:06,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:07,252][root][INFO] - Training Epoch: 1/2, step 2913/7134 completed (loss: 0.07062757760286331, acc: 0.9814814925193787)
[2025-02-13 02:49:07,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:07,714][root][INFO] - Training Epoch: 1/2, step 2914/7134 completed (loss: 0.056634970009326935, acc: 0.9819059371948242)
[2025-02-13 02:49:07,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:08,138][root][INFO] - Training Epoch: 1/2, step 2915/7134 completed (loss: 0.105211041867733, acc: 0.9769452214241028)
[2025-02-13 02:49:08,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:08,536][root][INFO] - Training Epoch: 1/2, step 2916/7134 completed (loss: 0.08505905419588089, acc: 0.9725086092948914)
[2025-02-13 02:49:08,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:08,968][root][INFO] - Training Epoch: 1/2, step 2917/7134 completed (loss: 0.06371670216321945, acc: 0.9782244563102722)
[2025-02-13 02:49:09,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:09,401][root][INFO] - Training Epoch: 1/2, step 2918/7134 completed (loss: 0.041901446878910065, acc: 0.9895104765892029)
[2025-02-13 02:49:09,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:09,808][root][INFO] - Training Epoch: 1/2, step 2919/7134 completed (loss: 0.0916796624660492, acc: 0.9764982461929321)
[2025-02-13 02:49:09,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:10,289][root][INFO] - Training Epoch: 1/2, step 2920/7134 completed (loss: 0.051665835082530975, acc: 0.9827814698219299)
[2025-02-13 02:49:10,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:10,723][root][INFO] - Training Epoch: 1/2, step 2921/7134 completed (loss: 0.042340945452451706, acc: 0.9885057210922241)
[2025-02-13 02:49:10,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:11,131][root][INFO] - Training Epoch: 1/2, step 2922/7134 completed (loss: 0.09960300475358963, acc: 0.9723618030548096)
[2025-02-13 02:49:11,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:11,567][root][INFO] - Training Epoch: 1/2, step 2923/7134 completed (loss: 0.1932213008403778, acc: 0.9585253596305847)
[2025-02-13 02:49:11,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:11,975][root][INFO] - Training Epoch: 1/2, step 2924/7134 completed (loss: 0.05788489431142807, acc: 0.9799528121948242)
[2025-02-13 02:49:12,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:12,460][root][INFO] - Training Epoch: 1/2, step 2925/7134 completed (loss: 0.0661623552441597, acc: 0.9864864945411682)
[2025-02-13 02:49:12,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:12,925][root][INFO] - Training Epoch: 1/2, step 2926/7134 completed (loss: 0.14072738587856293, acc: 0.979567289352417)
[2025-02-13 02:49:13,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:13,364][root][INFO] - Training Epoch: 1/2, step 2927/7134 completed (loss: 0.029631931334733963, acc: 0.990326464176178)
[2025-02-13 02:49:13,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:13,797][root][INFO] - Training Epoch: 1/2, step 2928/7134 completed (loss: 0.058714382350444794, acc: 0.9817073345184326)
[2025-02-13 02:49:13,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:14,263][root][INFO] - Training Epoch: 1/2, step 2929/7134 completed (loss: 0.04881411790847778, acc: 0.9921671152114868)
[2025-02-13 02:49:14,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:14,712][root][INFO] - Training Epoch: 1/2, step 2930/7134 completed (loss: 0.05219605565071106, acc: 0.9809104204177856)
[2025-02-13 02:49:14,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:15,124][root][INFO] - Training Epoch: 1/2, step 2931/7134 completed (loss: 0.08165224641561508, acc: 0.974588930606842)
[2025-02-13 02:49:15,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:15,555][root][INFO] - Training Epoch: 1/2, step 2932/7134 completed (loss: 0.0701727569103241, acc: 0.9803012609481812)
[2025-02-13 02:49:15,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:15,994][root][INFO] - Training Epoch: 1/2, step 2933/7134 completed (loss: 0.04307544231414795, acc: 0.9841269850730896)
[2025-02-13 02:49:16,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:16,447][root][INFO] - Training Epoch: 1/2, step 2934/7134 completed (loss: 0.038668468594551086, acc: 0.9858956336975098)
[2025-02-13 02:49:16,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:16,882][root][INFO] - Training Epoch: 1/2, step 2935/7134 completed (loss: 0.04466968774795532, acc: 0.9889867901802063)
[2025-02-13 02:49:17,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:17,351][root][INFO] - Training Epoch: 1/2, step 2936/7134 completed (loss: 0.06397642940282822, acc: 0.9837232828140259)
[2025-02-13 02:49:17,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:17,755][root][INFO] - Training Epoch: 1/2, step 2937/7134 completed (loss: 0.09901094436645508, acc: 0.9756097793579102)
[2025-02-13 02:49:17,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:18,177][root][INFO] - Training Epoch: 1/2, step 2938/7134 completed (loss: 0.04200087487697601, acc: 0.9910614490509033)
[2025-02-13 02:49:18,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:18,524][root][INFO] - Training Epoch: 1/2, step 2939/7134 completed (loss: 0.0400862954556942, acc: 0.9887820482254028)
[2025-02-13 02:49:18,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:18,940][root][INFO] - Training Epoch: 1/2, step 2940/7134 completed (loss: 0.0890134647488594, acc: 0.9822161197662354)
[2025-02-13 02:49:19,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:19,332][root][INFO] - Training Epoch: 1/2, step 2941/7134 completed (loss: 0.03850066289305687, acc: 0.9869375824928284)
[2025-02-13 02:49:19,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:19,711][root][INFO] - Training Epoch: 1/2, step 2942/7134 completed (loss: 0.04464525729417801, acc: 0.9910846948623657)
[2025-02-13 02:49:19,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:20,114][root][INFO] - Training Epoch: 1/2, step 2943/7134 completed (loss: 0.02817261591553688, acc: 0.9915682673454285)
[2025-02-13 02:49:20,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:20,524][root][INFO] - Training Epoch: 1/2, step 2944/7134 completed (loss: 0.04778377339243889, acc: 0.988252580165863)
[2025-02-13 02:49:20,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:20,942][root][INFO] - Training Epoch: 1/2, step 2945/7134 completed (loss: 0.04312299191951752, acc: 0.9844478964805603)
[2025-02-13 02:49:21,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:21,321][root][INFO] - Training Epoch: 1/2, step 2946/7134 completed (loss: 0.05589735880494118, acc: 0.9854651093482971)
[2025-02-13 02:49:21,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:21,722][root][INFO] - Training Epoch: 1/2, step 2947/7134 completed (loss: 0.07234115153551102, acc: 0.9779874086380005)
[2025-02-13 02:49:21,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:22,127][root][INFO] - Training Epoch: 1/2, step 2948/7134 completed (loss: 0.07780655473470688, acc: 0.9785832166671753)
[2025-02-13 02:49:22,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:22,514][root][INFO] - Training Epoch: 1/2, step 2949/7134 completed (loss: 0.017298193648457527, acc: 0.9938837885856628)
[2025-02-13 02:49:22,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:22,920][root][INFO] - Training Epoch: 1/2, step 2950/7134 completed (loss: 0.03520976006984711, acc: 0.9875862002372742)
[2025-02-13 02:49:23,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:23,326][root][INFO] - Training Epoch: 1/2, step 2951/7134 completed (loss: 0.01625112257897854, acc: 0.9927536249160767)
[2025-02-13 02:49:23,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:23,739][root][INFO] - Training Epoch: 1/2, step 2952/7134 completed (loss: 0.021638574078679085, acc: 0.9955489635467529)
[2025-02-13 02:49:23,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:24,134][root][INFO] - Training Epoch: 1/2, step 2953/7134 completed (loss: 0.04322713613510132, acc: 0.9847972989082336)
[2025-02-13 02:49:24,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:24,563][root][INFO] - Training Epoch: 1/2, step 2954/7134 completed (loss: 0.04071129113435745, acc: 0.9884169697761536)
[2025-02-13 02:49:24,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:24,939][root][INFO] - Training Epoch: 1/2, step 2955/7134 completed (loss: 0.046571698039770126, acc: 0.9872159361839294)
[2025-02-13 02:49:25,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:25,363][root][INFO] - Training Epoch: 1/2, step 2956/7134 completed (loss: 0.04350907355546951, acc: 0.9868228435516357)
[2025-02-13 02:49:25,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:25,790][root][INFO] - Training Epoch: 1/2, step 2957/7134 completed (loss: 0.02597196027636528, acc: 0.9946523904800415)
[2025-02-13 02:49:25,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:26,189][root][INFO] - Training Epoch: 1/2, step 2958/7134 completed (loss: 0.0283347200602293, acc: 0.9914529919624329)
[2025-02-13 02:49:26,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:26,588][root][INFO] - Training Epoch: 1/2, step 2959/7134 completed (loss: 0.030855970457196236, acc: 0.9897660613059998)
[2025-02-13 02:49:26,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:27,006][root][INFO] - Training Epoch: 1/2, step 2960/7134 completed (loss: 0.02861919067800045, acc: 0.9903581142425537)
[2025-02-13 02:49:27,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:27,450][root][INFO] - Training Epoch: 1/2, step 2961/7134 completed (loss: 0.03471580147743225, acc: 0.9863945841789246)
[2025-02-13 02:49:27,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:27,857][root][INFO] - Training Epoch: 1/2, step 2962/7134 completed (loss: 0.028264909982681274, acc: 0.9909443855285645)
[2025-02-13 02:49:28,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:28,305][root][INFO] - Training Epoch: 1/2, step 2963/7134 completed (loss: 0.036672722548246384, acc: 0.9869109988212585)
[2025-02-13 02:49:28,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:28,718][root][INFO] - Training Epoch: 1/2, step 2964/7134 completed (loss: 0.029900113120675087, acc: 0.9882869720458984)
[2025-02-13 02:49:28,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:29,131][root][INFO] - Training Epoch: 1/2, step 2965/7134 completed (loss: 0.07945048809051514, acc: 0.9746268391609192)
[2025-02-13 02:49:29,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:29,547][root][INFO] - Training Epoch: 1/2, step 2966/7134 completed (loss: 0.013188916258513927, acc: 0.9957355856895447)
[2025-02-13 02:49:29,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:29,962][root][INFO] - Training Epoch: 1/2, step 2967/7134 completed (loss: 0.04684417322278023, acc: 0.9860334992408752)
[2025-02-13 02:49:30,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:30,417][root][INFO] - Training Epoch: 1/2, step 2968/7134 completed (loss: 0.15118426084518433, acc: 0.9678456783294678)
[2025-02-13 02:49:30,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:30,823][root][INFO] - Training Epoch: 1/2, step 2969/7134 completed (loss: 0.2811494767665863, acc: 0.942307710647583)
[2025-02-13 02:49:30,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:31,238][root][INFO] - Training Epoch: 1/2, step 2970/7134 completed (loss: 0.3564087450504303, acc: 0.9281045794487)
[2025-02-13 02:49:31,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:31,670][root][INFO] - Training Epoch: 1/2, step 2971/7134 completed (loss: 0.07270176708698273, acc: 0.980289101600647)
[2025-02-13 02:49:31,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:32,084][root][INFO] - Training Epoch: 1/2, step 2972/7134 completed (loss: 0.05581648647785187, acc: 0.9792284965515137)
[2025-02-13 02:49:32,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:32,514][root][INFO] - Training Epoch: 1/2, step 2973/7134 completed (loss: 0.09570300579071045, acc: 0.9721518754959106)
[2025-02-13 02:49:32,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:32,950][root][INFO] - Training Epoch: 1/2, step 2974/7134 completed (loss: 0.07176979631185532, acc: 0.9805447459220886)
[2025-02-13 02:49:33,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:33,384][root][INFO] - Training Epoch: 1/2, step 2975/7134 completed (loss: 0.0704331248998642, acc: 0.9820442199707031)
[2025-02-13 02:49:33,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:33,850][root][INFO] - Training Epoch: 1/2, step 2976/7134 completed (loss: 0.059305787086486816, acc: 0.98617023229599)
[2025-02-13 02:49:33,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:34,276][root][INFO] - Training Epoch: 1/2, step 2977/7134 completed (loss: 0.06428039073944092, acc: 0.9855491518974304)
[2025-02-13 02:49:34,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:34,703][root][INFO] - Training Epoch: 1/2, step 2978/7134 completed (loss: 0.05429598689079285, acc: 0.9900709390640259)
[2025-02-13 02:49:34,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:35,100][root][INFO] - Training Epoch: 1/2, step 2979/7134 completed (loss: 0.028803406283259392, acc: 0.9922118186950684)
[2025-02-13 02:49:35,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:35,542][root][INFO] - Training Epoch: 1/2, step 2980/7134 completed (loss: 0.07027410715818405, acc: 0.9821656346321106)
[2025-02-13 02:49:35,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:35,971][root][INFO] - Training Epoch: 1/2, step 2981/7134 completed (loss: 0.09570157527923584, acc: 0.9823608994483948)
[2025-02-13 02:49:36,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:36,380][root][INFO] - Training Epoch: 1/2, step 2982/7134 completed (loss: 0.07263875752687454, acc: 0.9790576100349426)
[2025-02-13 02:49:36,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:36,805][root][INFO] - Training Epoch: 1/2, step 2983/7134 completed (loss: 0.11969330161809921, acc: 0.9775132536888123)
[2025-02-13 02:49:36,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:37,262][root][INFO] - Training Epoch: 1/2, step 2984/7134 completed (loss: 0.06044716015458107, acc: 0.9823899269104004)
[2025-02-13 02:49:37,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:37,706][root][INFO] - Training Epoch: 1/2, step 2985/7134 completed (loss: 0.06979063898324966, acc: 0.9809296727180481)
[2025-02-13 02:49:37,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:38,169][root][INFO] - Training Epoch: 1/2, step 2986/7134 completed (loss: 0.040145620703697205, acc: 0.9881796836853027)
[2025-02-13 02:49:38,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:38,600][root][INFO] - Training Epoch: 1/2, step 2987/7134 completed (loss: 0.04484378546476364, acc: 0.9858657121658325)
[2025-02-13 02:49:38,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:39,035][root][INFO] - Training Epoch: 1/2, step 2988/7134 completed (loss: 0.06011766940355301, acc: 0.985401451587677)
[2025-02-13 02:49:39,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:39,471][root][INFO] - Training Epoch: 1/2, step 2989/7134 completed (loss: 0.05253003165125847, acc: 0.9869281053543091)
[2025-02-13 02:49:39,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:39,856][root][INFO] - Training Epoch: 1/2, step 2990/7134 completed (loss: 0.18703609704971313, acc: 0.9597902297973633)
[2025-02-13 02:49:39,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:40,258][root][INFO] - Training Epoch: 1/2, step 2991/7134 completed (loss: 0.2035103142261505, acc: 0.9514563083648682)
[2025-02-13 02:49:40,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:40,653][root][INFO] - Training Epoch: 1/2, step 2992/7134 completed (loss: 0.1047588512301445, acc: 0.9803921580314636)
[2025-02-13 02:49:40,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:41,106][root][INFO] - Training Epoch: 1/2, step 2993/7134 completed (loss: 0.05330916866660118, acc: 0.9889975786209106)
[2025-02-13 02:49:41,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:41,523][root][INFO] - Training Epoch: 1/2, step 2994/7134 completed (loss: 0.05438336357474327, acc: 0.9892086386680603)
[2025-02-13 02:49:41,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:41,994][root][INFO] - Training Epoch: 1/2, step 2995/7134 completed (loss: 0.042256470769643784, acc: 0.989830493927002)
[2025-02-13 02:49:42,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:42,419][root][INFO] - Training Epoch: 1/2, step 2996/7134 completed (loss: 0.06799330562353134, acc: 0.9789473414421082)
[2025-02-13 02:49:42,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:42,874][root][INFO] - Training Epoch: 1/2, step 2997/7134 completed (loss: 0.03776940330862999, acc: 0.9906445145606995)
[2025-02-13 02:49:42,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:43,266][root][INFO] - Training Epoch: 1/2, step 2998/7134 completed (loss: 0.0554412305355072, acc: 0.9854545593261719)
[2025-02-13 02:49:43,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:43,743][root][INFO] - Training Epoch: 1/2, step 2999/7134 completed (loss: 0.033445365726947784, acc: 0.9927641153335571)
[2025-02-13 02:49:43,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:44,207][root][INFO] - Training Epoch: 1/2, step 3000/7134 completed (loss: 0.038050275295972824, acc: 0.9903640151023865)
[2025-02-13 02:49:44,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:44,660][root][INFO] - Training Epoch: 1/2, step 3001/7134 completed (loss: 0.04814504459500313, acc: 0.9834123253822327)
[2025-02-13 02:49:44,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:45,102][root][INFO] - Training Epoch: 1/2, step 3002/7134 completed (loss: 0.057722557336091995, acc: 0.9829931855201721)
[2025-02-13 02:49:45,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:45,559][root][INFO] - Training Epoch: 1/2, step 3003/7134 completed (loss: 0.039169903844594955, acc: 0.9872390031814575)
[2025-02-13 02:49:45,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:46,006][root][INFO] - Training Epoch: 1/2, step 3004/7134 completed (loss: 0.028202170506119728, acc: 0.9910614490509033)
[2025-02-13 02:49:46,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:46,469][root][INFO] - Training Epoch: 1/2, step 3005/7134 completed (loss: 0.024139368906617165, acc: 0.9920182228088379)
[2025-02-13 02:49:46,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:46,927][root][INFO] - Training Epoch: 1/2, step 3006/7134 completed (loss: 0.029122784733772278, acc: 0.992337167263031)
[2025-02-13 02:49:47,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:47,357][root][INFO] - Training Epoch: 1/2, step 3007/7134 completed (loss: 0.06275517493486404, acc: 0.9824355840682983)
[2025-02-13 02:49:47,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:47,763][root][INFO] - Training Epoch: 1/2, step 3008/7134 completed (loss: 0.09546219557523727, acc: 0.9748252034187317)
[2025-02-13 02:49:47,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:48,161][root][INFO] - Training Epoch: 1/2, step 3009/7134 completed (loss: 0.12039288878440857, acc: 0.9721518754959106)
[2025-02-13 02:49:48,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:48,603][root][INFO] - Training Epoch: 1/2, step 3010/7134 completed (loss: 0.07595647126436234, acc: 0.9790055155754089)
[2025-02-13 02:49:48,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:49,036][root][INFO] - Training Epoch: 1/2, step 3011/7134 completed (loss: 0.11900563538074493, acc: 0.9671897292137146)
[2025-02-13 02:49:49,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:49,465][root][INFO] - Training Epoch: 1/2, step 3012/7134 completed (loss: 0.09961362928152084, acc: 0.9714738726615906)
[2025-02-13 02:49:49,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:49,928][root][INFO] - Training Epoch: 1/2, step 3013/7134 completed (loss: 0.035179730504751205, acc: 0.985023021697998)
[2025-02-13 02:49:50,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:50,359][root][INFO] - Training Epoch: 1/2, step 3014/7134 completed (loss: 0.0530804842710495, acc: 0.9835873246192932)
[2025-02-13 02:49:50,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:50,752][root][INFO] - Training Epoch: 1/2, step 3015/7134 completed (loss: 0.04192691296339035, acc: 0.9917355179786682)
[2025-02-13 02:49:50,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:51,189][root][INFO] - Training Epoch: 1/2, step 3016/7134 completed (loss: 0.038590531796216965, acc: 0.9887482523918152)
[2025-02-13 02:49:51,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:51,609][root][INFO] - Training Epoch: 1/2, step 3017/7134 completed (loss: 0.13512907922267914, acc: 0.9559321999549866)
[2025-02-13 02:49:51,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:51,992][root][INFO] - Training Epoch: 1/2, step 3018/7134 completed (loss: 0.0627746507525444, acc: 0.9867768883705139)
[2025-02-13 02:49:52,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:52,421][root][INFO] - Training Epoch: 1/2, step 3019/7134 completed (loss: 0.05324168503284454, acc: 0.9852941036224365)
[2025-02-13 02:49:52,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:52,828][root][INFO] - Training Epoch: 1/2, step 3020/7134 completed (loss: 0.08618726581335068, acc: 0.9786096215248108)
[2025-02-13 02:49:52,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:53,213][root][INFO] - Training Epoch: 1/2, step 3021/7134 completed (loss: 0.02816953882575035, acc: 0.9927272796630859)
[2025-02-13 02:49:53,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:53,630][root][INFO] - Training Epoch: 1/2, step 3022/7134 completed (loss: 0.06470485776662827, acc: 0.9871382713317871)
[2025-02-13 02:49:53,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:53,988][root][INFO] - Training Epoch: 1/2, step 3023/7134 completed (loss: 0.07327862083911896, acc: 0.9761431217193604)
[2025-02-13 02:49:54,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:54,414][root][INFO] - Training Epoch: 1/2, step 3024/7134 completed (loss: 0.03462626412510872, acc: 0.992337167263031)
[2025-02-13 02:49:54,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:54,849][root][INFO] - Training Epoch: 1/2, step 3025/7134 completed (loss: 0.09821932762861252, acc: 0.9793103337287903)
[2025-02-13 02:49:54,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:55,250][root][INFO] - Training Epoch: 1/2, step 3026/7134 completed (loss: 0.041743699461221695, acc: 0.9910233616828918)
[2025-02-13 02:49:55,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:55,649][root][INFO] - Training Epoch: 1/2, step 3027/7134 completed (loss: 0.0910908505320549, acc: 0.9858585596084595)
[2025-02-13 02:49:55,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:55,963][root][INFO] - Training Epoch: 1/2, step 3028/7134 completed (loss: 0.04095238819718361, acc: 0.9931972622871399)
[2025-02-13 02:49:56,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:56,353][root][INFO] - Training Epoch: 1/2, step 3029/7134 completed (loss: 0.054521139711141586, acc: 0.989180862903595)
[2025-02-13 02:49:56,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:56,805][root][INFO] - Training Epoch: 1/2, step 3030/7134 completed (loss: 0.05380451679229736, acc: 0.9862595200538635)
[2025-02-13 02:49:56,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:57,192][root][INFO] - Training Epoch: 1/2, step 3031/7134 completed (loss: 0.02336866408586502, acc: 0.9894067645072937)
[2025-02-13 02:49:57,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:57,604][root][INFO] - Training Epoch: 1/2, step 3032/7134 completed (loss: 0.06145688518881798, acc: 0.9872958064079285)
[2025-02-13 02:49:57,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:57,996][root][INFO] - Training Epoch: 1/2, step 3033/7134 completed (loss: 0.051761649549007416, acc: 0.9841269850730896)
[2025-02-13 02:49:58,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:58,400][root][INFO] - Training Epoch: 1/2, step 3034/7134 completed (loss: 0.06073285639286041, acc: 0.9851064085960388)
[2025-02-13 02:49:58,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:58,786][root][INFO] - Training Epoch: 1/2, step 3035/7134 completed (loss: 0.03335979953408241, acc: 0.9937008023262024)
[2025-02-13 02:49:58,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:59,188][root][INFO] - Training Epoch: 1/2, step 3036/7134 completed (loss: 0.07599400728940964, acc: 0.9799330830574036)
[2025-02-13 02:49:59,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:59,629][root][INFO] - Training Epoch: 1/2, step 3037/7134 completed (loss: 0.061306439340114594, acc: 0.9799692034721375)
[2025-02-13 02:49:59,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:00,045][root][INFO] - Training Epoch: 1/2, step 3038/7134 completed (loss: 0.02593580260872841, acc: 0.9930434823036194)
[2025-02-13 02:50:00,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:00,450][root][INFO] - Training Epoch: 1/2, step 3039/7134 completed (loss: 0.05494775250554085, acc: 0.9892802238464355)
[2025-02-13 02:50:00,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:00,839][root][INFO] - Training Epoch: 1/2, step 3040/7134 completed (loss: 0.0739886611700058, acc: 0.9777365326881409)
[2025-02-13 02:50:00,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:01,244][root][INFO] - Training Epoch: 1/2, step 3041/7134 completed (loss: 0.08901496231555939, acc: 0.979266345500946)
[2025-02-13 02:50:01,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:01,648][root][INFO] - Training Epoch: 1/2, step 3042/7134 completed (loss: 0.04511919245123863, acc: 0.9867060780525208)
[2025-02-13 02:50:01,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:02,033][root][INFO] - Training Epoch: 1/2, step 3043/7134 completed (loss: 0.052934430539608, acc: 0.980988621711731)
[2025-02-13 02:50:02,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:02,416][root][INFO] - Training Epoch: 1/2, step 3044/7134 completed (loss: 0.031367331743240356, acc: 0.9913644194602966)
[2025-02-13 02:50:02,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:02,795][root][INFO] - Training Epoch: 1/2, step 3045/7134 completed (loss: 0.021597977727651596, acc: 0.994915246963501)
[2025-02-13 02:50:02,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:03,239][root][INFO] - Training Epoch: 1/2, step 3046/7134 completed (loss: 0.0480547770857811, acc: 0.9877049326896667)
[2025-02-13 02:50:03,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:03,665][root][INFO] - Training Epoch: 1/2, step 3047/7134 completed (loss: 0.02867068722844124, acc: 0.9886792302131653)
[2025-02-13 02:50:03,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:04,143][root][INFO] - Training Epoch: 1/2, step 3048/7134 completed (loss: 0.017883403226733208, acc: 0.996221661567688)
[2025-02-13 02:50:04,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:04,551][root][INFO] - Training Epoch: 1/2, step 3049/7134 completed (loss: 0.03785666078329086, acc: 0.9883720874786377)
[2025-02-13 02:50:04,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:04,994][root][INFO] - Training Epoch: 1/2, step 3050/7134 completed (loss: 0.050886377692222595, acc: 0.980997622013092)
[2025-02-13 02:50:05,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:05,401][root][INFO] - Training Epoch: 1/2, step 3051/7134 completed (loss: 0.06916862726211548, acc: 0.9789473414421082)
[2025-02-13 02:50:05,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:05,834][root][INFO] - Training Epoch: 1/2, step 3052/7134 completed (loss: 0.04655073583126068, acc: 0.9896907210350037)
[2025-02-13 02:50:05,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:06,262][root][INFO] - Training Epoch: 1/2, step 3053/7134 completed (loss: 0.042100805789232254, acc: 0.989130437374115)
[2025-02-13 02:50:06,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:06,681][root][INFO] - Training Epoch: 1/2, step 3054/7134 completed (loss: 0.06445170193910599, acc: 0.9818181991577148)
[2025-02-13 02:50:06,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:07,094][root][INFO] - Training Epoch: 1/2, step 3055/7134 completed (loss: 0.04496138542890549, acc: 0.988034188747406)
[2025-02-13 02:50:07,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:07,529][root][INFO] - Training Epoch: 1/2, step 3056/7134 completed (loss: 0.03171231970191002, acc: 0.9922879338264465)
[2025-02-13 02:50:07,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:07,989][root][INFO] - Training Epoch: 1/2, step 3057/7134 completed (loss: 0.028119632974267006, acc: 0.9916782379150391)
[2025-02-13 02:50:08,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:08,416][root][INFO] - Training Epoch: 1/2, step 3058/7134 completed (loss: 0.10188226401805878, acc: 0.977979302406311)
[2025-02-13 02:50:08,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:08,844][root][INFO] - Training Epoch: 1/2, step 3059/7134 completed (loss: 0.04368109628558159, acc: 0.9876712560653687)
[2025-02-13 02:50:08,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:09,283][root][INFO] - Training Epoch: 1/2, step 3060/7134 completed (loss: 0.04128577560186386, acc: 0.9868735074996948)
[2025-02-13 02:50:09,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:09,680][root][INFO] - Training Epoch: 1/2, step 3061/7134 completed (loss: 0.03480317443609238, acc: 0.9939393997192383)
[2025-02-13 02:50:09,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:10,124][root][INFO] - Training Epoch: 1/2, step 3062/7134 completed (loss: 0.01937512867152691, acc: 0.9946091771125793)
[2025-02-13 02:50:10,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:10,549][root][INFO] - Training Epoch: 1/2, step 3063/7134 completed (loss: 0.04318973794579506, acc: 0.9858793616294861)
[2025-02-13 02:50:10,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:10,962][root][INFO] - Training Epoch: 1/2, step 3064/7134 completed (loss: 0.02188006602227688, acc: 0.9957746267318726)
[2025-02-13 02:50:11,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:11,399][root][INFO] - Training Epoch: 1/2, step 3065/7134 completed (loss: 0.036675285547971725, acc: 0.9919354915618896)
[2025-02-13 02:50:11,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:11,818][root][INFO] - Training Epoch: 1/2, step 3066/7134 completed (loss: 0.025771034881472588, acc: 0.9935483932495117)
[2025-02-13 02:50:11,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:12,203][root][INFO] - Training Epoch: 1/2, step 3067/7134 completed (loss: 0.04065828025341034, acc: 0.9854545593261719)
[2025-02-13 02:50:12,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:12,641][root][INFO] - Training Epoch: 1/2, step 3068/7134 completed (loss: 0.05378852039575577, acc: 0.9864681959152222)
[2025-02-13 02:50:12,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:13,083][root][INFO] - Training Epoch: 1/2, step 3069/7134 completed (loss: 0.031418245285749435, acc: 0.9873272180557251)
[2025-02-13 02:50:13,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:13,546][root][INFO] - Training Epoch: 1/2, step 3070/7134 completed (loss: 0.022185813635587692, acc: 0.9925768971443176)
[2025-02-13 02:50:13,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:13,985][root][INFO] - Training Epoch: 1/2, step 3071/7134 completed (loss: 0.036526910960674286, acc: 0.9897040128707886)
[2025-02-13 02:50:14,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:14,425][root][INFO] - Training Epoch: 1/2, step 3072/7134 completed (loss: 0.03289273381233215, acc: 0.9894982576370239)
[2025-02-13 02:50:14,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:14,829][root][INFO] - Training Epoch: 1/2, step 3073/7134 completed (loss: 0.02142731472849846, acc: 0.9923664331436157)
[2025-02-13 02:50:14,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:15,238][root][INFO] - Training Epoch: 1/2, step 3074/7134 completed (loss: 0.03279414772987366, acc: 0.9933444261550903)
[2025-02-13 02:50:15,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:15,657][root][INFO] - Training Epoch: 1/2, step 3075/7134 completed (loss: 0.02902207151055336, acc: 0.9916666746139526)
[2025-02-13 02:50:15,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:16,101][root][INFO] - Training Epoch: 1/2, step 3076/7134 completed (loss: 0.054079052060842514, acc: 0.9866814613342285)
[2025-02-13 02:50:16,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:16,584][root][INFO] - Training Epoch: 1/2, step 3077/7134 completed (loss: 0.09384164214134216, acc: 0.9748634099960327)
[2025-02-13 02:50:16,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:17,034][root][INFO] - Training Epoch: 1/2, step 3078/7134 completed (loss: 0.08276176452636719, acc: 0.9748634099960327)
[2025-02-13 02:50:17,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:17,472][root][INFO] - Training Epoch: 1/2, step 3079/7134 completed (loss: 0.03452474623918533, acc: 0.9913580417633057)
[2025-02-13 02:50:17,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:17,928][root][INFO] - Training Epoch: 1/2, step 3080/7134 completed (loss: 0.07171569019556046, acc: 0.9749181866645813)
[2025-02-13 02:50:18,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:18,241][root][INFO] - Training Epoch: 1/2, step 3081/7134 completed (loss: 0.03128276765346527, acc: 0.9870689511299133)
[2025-02-13 02:50:18,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:18,674][root][INFO] - Training Epoch: 1/2, step 3082/7134 completed (loss: 0.10302101075649261, acc: 0.973525881767273)
[2025-02-13 02:50:18,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:19,116][root][INFO] - Training Epoch: 1/2, step 3083/7134 completed (loss: 0.0870368629693985, acc: 0.9751309156417847)
[2025-02-13 02:50:19,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:19,551][root][INFO] - Training Epoch: 1/2, step 3084/7134 completed (loss: 0.09170883148908615, acc: 0.976190447807312)
[2025-02-13 02:50:19,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:19,993][root][INFO] - Training Epoch: 1/2, step 3085/7134 completed (loss: 0.08833938837051392, acc: 0.970802903175354)
[2025-02-13 02:50:20,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:20,478][root][INFO] - Training Epoch: 1/2, step 3086/7134 completed (loss: 0.087478868663311, acc: 0.9753086566925049)
[2025-02-13 02:50:20,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:20,929][root][INFO] - Training Epoch: 1/2, step 3087/7134 completed (loss: 0.06779951602220535, acc: 0.9799138903617859)
[2025-02-13 02:50:21,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:21,385][root][INFO] - Training Epoch: 1/2, step 3088/7134 completed (loss: 0.05370344594120979, acc: 0.9830890893936157)
[2025-02-13 02:50:21,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:21,860][root][INFO] - Training Epoch: 1/2, step 3089/7134 completed (loss: 0.0514712929725647, acc: 0.9859762787818909)
[2025-02-13 02:50:22,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:22,316][root][INFO] - Training Epoch: 1/2, step 3090/7134 completed (loss: 0.10303656756877899, acc: 0.9741848111152649)
[2025-02-13 02:50:22,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:22,781][root][INFO] - Training Epoch: 1/2, step 3091/7134 completed (loss: 0.06373463571071625, acc: 0.9781106114387512)
[2025-02-13 02:50:22,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:23,178][root][INFO] - Training Epoch: 1/2, step 3092/7134 completed (loss: 0.07235445827245712, acc: 0.9852125644683838)
[2025-02-13 02:50:23,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:23,621][root][INFO] - Training Epoch: 1/2, step 3093/7134 completed (loss: 0.06951071321964264, acc: 0.9801324605941772)
[2025-02-13 02:50:23,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:24,078][root][INFO] - Training Epoch: 1/2, step 3094/7134 completed (loss: 0.028369206935167313, acc: 0.9904631972312927)
[2025-02-13 02:50:24,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:24,496][root][INFO] - Training Epoch: 1/2, step 3095/7134 completed (loss: 0.07568120211362839, acc: 0.980555534362793)
[2025-02-13 02:50:24,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:24,929][root][INFO] - Training Epoch: 1/2, step 3096/7134 completed (loss: 0.09387365728616714, acc: 0.9756097793579102)
[2025-02-13 02:50:25,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:25,383][root][INFO] - Training Epoch: 1/2, step 3097/7134 completed (loss: 0.0725945457816124, acc: 0.9786324501037598)
[2025-02-13 02:50:25,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:25,825][root][INFO] - Training Epoch: 1/2, step 3098/7134 completed (loss: 0.045020755380392075, acc: 0.9889937043190002)
[2025-02-13 02:50:25,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:26,226][root][INFO] - Training Epoch: 1/2, step 3099/7134 completed (loss: 0.05375480279326439, acc: 0.9854133129119873)
[2025-02-13 02:50:26,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:26,701][root][INFO] - Training Epoch: 1/2, step 3100/7134 completed (loss: 0.046439364552497864, acc: 0.9882978796958923)
[2025-02-13 02:50:26,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:27,143][root][INFO] - Training Epoch: 1/2, step 3101/7134 completed (loss: 0.051253776997327805, acc: 0.9817517995834351)
[2025-02-13 02:50:27,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:27,605][root][INFO] - Training Epoch: 1/2, step 3102/7134 completed (loss: 0.05450621247291565, acc: 0.9822221994400024)
[2025-02-13 02:50:27,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:28,076][root][INFO] - Training Epoch: 1/2, step 3103/7134 completed (loss: 0.06490189582109451, acc: 0.98296058177948)
[2025-02-13 02:50:28,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:28,416][root][INFO] - Training Epoch: 1/2, step 3104/7134 completed (loss: 0.031132129952311516, acc: 0.9923664331436157)
[2025-02-13 02:50:28,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:28,846][root][INFO] - Training Epoch: 1/2, step 3105/7134 completed (loss: 0.0681525245308876, acc: 0.9835164546966553)
[2025-02-13 02:50:28,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:29,275][root][INFO] - Training Epoch: 1/2, step 3106/7134 completed (loss: 0.040954332798719406, acc: 0.9849498271942139)
[2025-02-13 02:50:29,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:29,684][root][INFO] - Training Epoch: 1/2, step 3107/7134 completed (loss: 0.02859492041170597, acc: 0.9882352948188782)
[2025-02-13 02:50:29,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:30,121][root][INFO] - Training Epoch: 1/2, step 3108/7134 completed (loss: 0.08154552429914474, acc: 0.9830769300460815)
[2025-02-13 02:50:30,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:30,539][root][INFO] - Training Epoch: 1/2, step 3109/7134 completed (loss: 0.0512651726603508, acc: 0.9815497994422913)
[2025-02-13 02:50:30,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:30,959][root][INFO] - Training Epoch: 1/2, step 3110/7134 completed (loss: 0.039924319833517075, acc: 0.9853658676147461)
[2025-02-13 02:50:31,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:31,367][root][INFO] - Training Epoch: 1/2, step 3111/7134 completed (loss: 0.1543227732181549, acc: 0.9639175534248352)
[2025-02-13 02:50:31,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:31,755][root][INFO] - Training Epoch: 1/2, step 3112/7134 completed (loss: 0.05277210846543312, acc: 0.9877192974090576)
[2025-02-13 02:50:31,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:32,207][root][INFO] - Training Epoch: 1/2, step 3113/7134 completed (loss: 0.04318845272064209, acc: 0.9887640476226807)
[2025-02-13 02:50:32,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:32,623][root][INFO] - Training Epoch: 1/2, step 3114/7134 completed (loss: 0.0607185885310173, acc: 0.9844827651977539)
[2025-02-13 02:50:32,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:33,026][root][INFO] - Training Epoch: 1/2, step 3115/7134 completed (loss: 0.07708294689655304, acc: 0.9876543283462524)
[2025-02-13 02:50:33,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:33,465][root][INFO] - Training Epoch: 1/2, step 3116/7134 completed (loss: 0.047913119196891785, acc: 0.9871794581413269)
[2025-02-13 02:50:33,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:33,900][root][INFO] - Training Epoch: 1/2, step 3117/7134 completed (loss: 0.10476875305175781, acc: 0.979619562625885)
[2025-02-13 02:50:34,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:34,331][root][INFO] - Training Epoch: 1/2, step 3118/7134 completed (loss: 0.06274521350860596, acc: 0.9841897487640381)
[2025-02-13 02:50:34,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:34,767][root][INFO] - Training Epoch: 1/2, step 3119/7134 completed (loss: 0.0795479491353035, acc: 0.9779506921768188)
[2025-02-13 02:50:34,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:35,162][root][INFO] - Training Epoch: 1/2, step 3120/7134 completed (loss: 0.07233750820159912, acc: 0.9873060584068298)
[2025-02-13 02:50:35,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:35,580][root][INFO] - Training Epoch: 1/2, step 3121/7134 completed (loss: 0.05713042616844177, acc: 0.980719804763794)
[2025-02-13 02:50:35,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:36,031][root][INFO] - Training Epoch: 1/2, step 3122/7134 completed (loss: 0.050454333424568176, acc: 0.9883720874786377)
[2025-02-13 02:50:36,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:36,470][root][INFO] - Training Epoch: 1/2, step 3123/7134 completed (loss: 0.07226444035768509, acc: 0.9788557291030884)
[2025-02-13 02:50:36,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:36,892][root][INFO] - Training Epoch: 1/2, step 3124/7134 completed (loss: 0.06179297715425491, acc: 0.9888613820075989)
[2025-02-13 02:50:37,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:37,303][root][INFO] - Training Epoch: 1/2, step 3125/7134 completed (loss: 0.02559502050280571, acc: 0.9924356937408447)
[2025-02-13 02:50:37,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:37,770][root][INFO] - Training Epoch: 1/2, step 3126/7134 completed (loss: 0.06083093583583832, acc: 0.9809402823448181)
[2025-02-13 02:50:37,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:38,204][root][INFO] - Training Epoch: 1/2, step 3127/7134 completed (loss: 0.04316425323486328, acc: 0.9872093200683594)
[2025-02-13 02:50:38,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:38,629][root][INFO] - Training Epoch: 1/2, step 3128/7134 completed (loss: 0.03623034432530403, acc: 0.9909677505493164)
[2025-02-13 02:50:38,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:39,086][root][INFO] - Training Epoch: 1/2, step 3129/7134 completed (loss: 0.03336842730641365, acc: 0.9925834536552429)
[2025-02-13 02:50:39,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:39,564][root][INFO] - Training Epoch: 1/2, step 3130/7134 completed (loss: 0.04542739689350128, acc: 0.9885321259498596)
[2025-02-13 02:50:39,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:39,992][root][INFO] - Training Epoch: 1/2, step 3131/7134 completed (loss: 0.06005968898534775, acc: 0.9811594486236572)
[2025-02-13 02:50:40,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:40,354][root][INFO] - Training Epoch: 1/2, step 3132/7134 completed (loss: 0.02675151266157627, acc: 0.9913232326507568)
[2025-02-13 02:50:40,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:40,804][root][INFO] - Training Epoch: 1/2, step 3133/7134 completed (loss: 0.1211988553404808, acc: 0.9752747416496277)
[2025-02-13 02:50:40,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:41,246][root][INFO] - Training Epoch: 1/2, step 3134/7134 completed (loss: 0.046137213706970215, acc: 0.9835164546966553)
[2025-02-13 02:50:41,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:41,590][root][INFO] - Training Epoch: 1/2, step 3135/7134 completed (loss: 0.09993899613618851, acc: 0.9699769020080566)
[2025-02-13 02:50:41,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:42,010][root][INFO] - Training Epoch: 1/2, step 3136/7134 completed (loss: 0.04342573881149292, acc: 0.9878261089324951)
[2025-02-13 02:50:42,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:42,422][root][INFO] - Training Epoch: 1/2, step 3137/7134 completed (loss: 0.1007070541381836, acc: 0.9642857313156128)
[2025-02-13 02:50:42,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:42,810][root][INFO] - Training Epoch: 1/2, step 3138/7134 completed (loss: 0.0756877064704895, acc: 0.9852941036224365)
[2025-02-13 02:50:42,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:43,154][root][INFO] - Training Epoch: 1/2, step 3139/7134 completed (loss: 0.13649190962314606, acc: 0.9613733887672424)
[2025-02-13 02:50:43,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:43,536][root][INFO] - Training Epoch: 1/2, step 3140/7134 completed (loss: 0.04597003012895584, acc: 0.9844357967376709)
[2025-02-13 02:50:43,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:43,935][root][INFO] - Training Epoch: 1/2, step 3141/7134 completed (loss: 0.07527323067188263, acc: 0.9772727489471436)
[2025-02-13 02:50:44,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:44,344][root][INFO] - Training Epoch: 1/2, step 3142/7134 completed (loss: 0.035519443452358246, acc: 0.9910314083099365)
[2025-02-13 02:50:44,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:44,759][root][INFO] - Training Epoch: 1/2, step 3143/7134 completed (loss: 0.039411917328834534, acc: 0.9887459874153137)
[2025-02-13 02:50:44,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:45,180][root][INFO] - Training Epoch: 1/2, step 3144/7134 completed (loss: 0.03195885568857193, acc: 0.9921383857727051)
[2025-02-13 02:50:45,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:45,591][root][INFO] - Training Epoch: 1/2, step 3145/7134 completed (loss: 0.07424086332321167, acc: 0.9824945330619812)
[2025-02-13 02:50:45,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:46,007][root][INFO] - Training Epoch: 1/2, step 3146/7134 completed (loss: 0.059984151273965836, acc: 0.9830729365348816)
[2025-02-13 02:50:46,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:46,411][root][INFO] - Training Epoch: 1/2, step 3147/7134 completed (loss: 0.05167340487241745, acc: 0.9855538010597229)
[2025-02-13 02:50:46,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:46,851][root][INFO] - Training Epoch: 1/2, step 3148/7134 completed (loss: 0.07818514108657837, acc: 0.9770408272743225)
[2025-02-13 02:50:46,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:47,272][root][INFO] - Training Epoch: 1/2, step 3149/7134 completed (loss: 0.03047315590083599, acc: 0.9912152290344238)
[2025-02-13 02:50:47,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:47,689][root][INFO] - Training Epoch: 1/2, step 3150/7134 completed (loss: 0.22289669513702393, acc: 0.9388560056686401)
[2025-02-13 02:50:47,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:48,080][root][INFO] - Training Epoch: 1/2, step 3151/7134 completed (loss: 0.14600029587745667, acc: 0.961685836315155)
[2025-02-13 02:50:48,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:48,482][root][INFO] - Training Epoch: 1/2, step 3152/7134 completed (loss: 0.0812898799777031, acc: 0.9742424488067627)
[2025-02-13 02:50:48,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:48,905][root][INFO] - Training Epoch: 1/2, step 3153/7134 completed (loss: 0.0633108839392662, acc: 0.9824561476707458)
[2025-02-13 02:50:49,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:49,310][root][INFO] - Training Epoch: 1/2, step 3154/7134 completed (loss: 0.0530182346701622, acc: 0.980461835861206)
[2025-02-13 02:50:49,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:49,701][root][INFO] - Training Epoch: 1/2, step 3155/7134 completed (loss: 0.11372166126966476, acc: 0.9756592512130737)
[2025-02-13 02:50:49,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:50,126][root][INFO] - Training Epoch: 1/2, step 3156/7134 completed (loss: 0.1105366051197052, acc: 0.9732540845870972)
[2025-02-13 02:50:50,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:50,520][root][INFO] - Training Epoch: 1/2, step 3157/7134 completed (loss: 0.032135430723428726, acc: 0.9913978576660156)
[2025-02-13 02:50:50,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:50,914][root][INFO] - Training Epoch: 1/2, step 3158/7134 completed (loss: 0.060874782502651215, acc: 0.9852941036224365)
[2025-02-13 02:50:51,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:51,313][root][INFO] - Training Epoch: 1/2, step 3159/7134 completed (loss: 0.0769151896238327, acc: 0.9779411554336548)
[2025-02-13 02:50:51,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:51,737][root][INFO] - Training Epoch: 1/2, step 3160/7134 completed (loss: 0.026213325560092926, acc: 0.9930843710899353)
[2025-02-13 02:50:51,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:52,174][root][INFO] - Training Epoch: 1/2, step 3161/7134 completed (loss: 0.050277046859264374, acc: 0.9845938086509705)
[2025-02-13 02:50:52,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:52,584][root][INFO] - Training Epoch: 1/2, step 3162/7134 completed (loss: 0.02995612844824791, acc: 0.9883381724357605)
[2025-02-13 02:50:52,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:52,974][root][INFO] - Training Epoch: 1/2, step 3163/7134 completed (loss: 0.07875184714794159, acc: 0.9816360473632812)
[2025-02-13 02:50:53,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:53,386][root][INFO] - Training Epoch: 1/2, step 3164/7134 completed (loss: 0.09521260112524033, acc: 0.9748743772506714)
[2025-02-13 02:50:53,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:53,797][root][INFO] - Training Epoch: 1/2, step 3165/7134 completed (loss: 0.044477056711912155, acc: 0.9866443872451782)
[2025-02-13 02:50:53,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:54,210][root][INFO] - Training Epoch: 1/2, step 3166/7134 completed (loss: 0.1143251359462738, acc: 0.9684210419654846)
[2025-02-13 02:50:54,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:54,636][root][INFO] - Training Epoch: 1/2, step 3167/7134 completed (loss: 0.13320443034172058, acc: 0.9593750238418579)
[2025-02-13 02:50:54,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:55,040][root][INFO] - Training Epoch: 1/2, step 3168/7134 completed (loss: 0.1272735595703125, acc: 0.9710366129875183)
[2025-02-13 02:50:55,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:55,432][root][INFO] - Training Epoch: 1/2, step 3169/7134 completed (loss: 0.09118787944316864, acc: 0.9774590134620667)
[2025-02-13 02:50:55,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:55,842][root][INFO] - Training Epoch: 1/2, step 3170/7134 completed (loss: 0.04107668995857239, acc: 0.9818181991577148)
[2025-02-13 02:50:55,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:56,263][root][INFO] - Training Epoch: 1/2, step 3171/7134 completed (loss: 0.08499694615602493, acc: 0.9829620122909546)
[2025-02-13 02:50:56,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:56,655][root][INFO] - Training Epoch: 1/2, step 3172/7134 completed (loss: 0.03219272196292877, acc: 0.984674334526062)
[2025-02-13 02:50:56,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:57,011][root][INFO] - Training Epoch: 1/2, step 3173/7134 completed (loss: 0.04910384491086006, acc: 0.9846938848495483)
[2025-02-13 02:50:57,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:57,431][root][INFO] - Training Epoch: 1/2, step 3174/7134 completed (loss: 0.09300647675991058, acc: 0.9689348936080933)
[2025-02-13 02:50:57,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:57,849][root][INFO] - Training Epoch: 1/2, step 3175/7134 completed (loss: 0.06643017381429672, acc: 0.9845070242881775)
[2025-02-13 02:50:57,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:58,256][root][INFO] - Training Epoch: 1/2, step 3176/7134 completed (loss: 0.09544873982667923, acc: 0.9729323387145996)
[2025-02-13 02:50:58,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:58,644][root][INFO] - Training Epoch: 1/2, step 3177/7134 completed (loss: 0.08697804063558578, acc: 0.9809027910232544)
[2025-02-13 02:50:58,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:59,045][root][INFO] - Training Epoch: 1/2, step 3178/7134 completed (loss: 0.03915329650044441, acc: 0.9901477694511414)
[2025-02-13 02:50:59,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:59,475][root][INFO] - Training Epoch: 1/2, step 3179/7134 completed (loss: 0.10114047676324844, acc: 0.9734659790992737)
[2025-02-13 02:50:59,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:59,872][root][INFO] - Training Epoch: 1/2, step 3180/7134 completed (loss: 0.05227513983845711, acc: 0.9857142567634583)
[2025-02-13 02:51:00,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:00,283][root][INFO] - Training Epoch: 1/2, step 3181/7134 completed (loss: 0.07872821390628815, acc: 0.9781591296195984)
[2025-02-13 02:51:00,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:00,705][root][INFO] - Training Epoch: 1/2, step 3182/7134 completed (loss: 0.03972196206450462, acc: 0.992548406124115)
[2025-02-13 02:51:00,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:01,091][root][INFO] - Training Epoch: 1/2, step 3183/7134 completed (loss: 0.06620568037033081, acc: 0.9830795526504517)
[2025-02-13 02:51:01,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:01,511][root][INFO] - Training Epoch: 1/2, step 3184/7134 completed (loss: 0.07052048295736313, acc: 0.9836309552192688)
[2025-02-13 02:51:01,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:01,918][root][INFO] - Training Epoch: 1/2, step 3185/7134 completed (loss: 0.014355923980474472, acc: 0.9971510171890259)
[2025-02-13 02:51:02,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:02,339][root][INFO] - Training Epoch: 1/2, step 3186/7134 completed (loss: 0.047300953418016434, acc: 0.9837037324905396)
[2025-02-13 02:51:02,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:02,721][root][INFO] - Training Epoch: 1/2, step 3187/7134 completed (loss: 0.054049715399742126, acc: 0.9857594966888428)
[2025-02-13 02:51:02,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:03,134][root][INFO] - Training Epoch: 1/2, step 3188/7134 completed (loss: 0.045993197709321976, acc: 0.9870634078979492)
[2025-02-13 02:51:03,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:03,538][root][INFO] - Training Epoch: 1/2, step 3189/7134 completed (loss: 0.07970146834850311, acc: 0.9797394871711731)
[2025-02-13 02:51:03,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:03,975][root][INFO] - Training Epoch: 1/2, step 3190/7134 completed (loss: 0.058382101356983185, acc: 0.9860759377479553)
[2025-02-13 02:51:04,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:04,363][root][INFO] - Training Epoch: 1/2, step 3191/7134 completed (loss: 0.05502113327383995, acc: 0.9857142567634583)
[2025-02-13 02:51:04,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:04,754][root][INFO] - Training Epoch: 1/2, step 3192/7134 completed (loss: 0.06013472005724907, acc: 0.9810963869094849)
[2025-02-13 02:51:04,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:05,155][root][INFO] - Training Epoch: 1/2, step 3193/7134 completed (loss: 0.11378617584705353, acc: 0.9677891731262207)
[2025-02-13 02:51:05,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:05,550][root][INFO] - Training Epoch: 1/2, step 3194/7134 completed (loss: 0.06325767189264297, acc: 0.9803921580314636)
[2025-02-13 02:51:05,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:05,946][root][INFO] - Training Epoch: 1/2, step 3195/7134 completed (loss: 0.07040486484766006, acc: 0.9828473329544067)
[2025-02-13 02:51:06,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:06,340][root][INFO] - Training Epoch: 1/2, step 3196/7134 completed (loss: 0.02542666159570217, acc: 0.9925233721733093)
[2025-02-13 02:51:06,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:06,727][root][INFO] - Training Epoch: 1/2, step 3197/7134 completed (loss: 0.06287124752998352, acc: 0.9887323975563049)
[2025-02-13 02:51:06,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:07,135][root][INFO] - Training Epoch: 1/2, step 3198/7134 completed (loss: 0.05723900720477104, acc: 0.9863842725753784)
[2025-02-13 02:51:07,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:07,526][root][INFO] - Training Epoch: 1/2, step 3199/7134 completed (loss: 0.04698461294174194, acc: 0.9867841601371765)
[2025-02-13 02:51:07,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:07,889][root][INFO] - Training Epoch: 1/2, step 3200/7134 completed (loss: 0.0598195344209671, acc: 0.982425332069397)
[2025-02-13 02:51:08,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:08,310][root][INFO] - Training Epoch: 1/2, step 3201/7134 completed (loss: 0.14094063639640808, acc: 0.9674620628356934)
[2025-02-13 02:51:08,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:08,729][root][INFO] - Training Epoch: 1/2, step 3202/7134 completed (loss: 0.050910040736198425, acc: 0.9875518679618835)
[2025-02-13 02:51:08,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:09,128][root][INFO] - Training Epoch: 1/2, step 3203/7134 completed (loss: 0.054433390498161316, acc: 0.9842022061347961)
[2025-02-13 02:51:09,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:09,494][root][INFO] - Training Epoch: 1/2, step 3204/7134 completed (loss: 0.10607073456048965, acc: 0.9755638837814331)
[2025-02-13 02:51:09,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:09,896][root][INFO] - Training Epoch: 1/2, step 3205/7134 completed (loss: 0.020489206537604332, acc: 0.9929328560829163)
[2025-02-13 02:51:10,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:10,339][root][INFO] - Training Epoch: 1/2, step 3206/7134 completed (loss: 0.04161296784877777, acc: 0.9898697733879089)
[2025-02-13 02:51:10,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:10,745][root][INFO] - Training Epoch: 1/2, step 3207/7134 completed (loss: 0.059916093945503235, acc: 0.9785605072975159)
[2025-02-13 02:51:10,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:11,168][root][INFO] - Training Epoch: 1/2, step 3208/7134 completed (loss: 0.09059017896652222, acc: 0.9820261597633362)
[2025-02-13 02:51:11,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:11,570][root][INFO] - Training Epoch: 1/2, step 3209/7134 completed (loss: 0.0466860830783844, acc: 0.988095223903656)
[2025-02-13 02:51:11,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:11,983][root][INFO] - Training Epoch: 1/2, step 3210/7134 completed (loss: 0.020718643441796303, acc: 0.9924699068069458)
[2025-02-13 02:51:12,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:12,367][root][INFO] - Training Epoch: 1/2, step 3211/7134 completed (loss: 0.054773204028606415, acc: 0.9916840195655823)
[2025-02-13 02:51:12,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:12,783][root][INFO] - Training Epoch: 1/2, step 3212/7134 completed (loss: 0.0161698330193758, acc: 0.9944055676460266)
[2025-02-13 02:51:12,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:13,197][root][INFO] - Training Epoch: 1/2, step 3213/7134 completed (loss: 0.031391676515340805, acc: 0.9956958293914795)
[2025-02-13 02:51:13,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:13,640][root][INFO] - Training Epoch: 1/2, step 3214/7134 completed (loss: 0.03820713609457016, acc: 0.9866130948066711)
[2025-02-13 02:51:13,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:14,072][root][INFO] - Training Epoch: 1/2, step 3215/7134 completed (loss: 0.041003238409757614, acc: 0.9915966391563416)
[2025-02-13 02:51:14,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:14,499][root][INFO] - Training Epoch: 1/2, step 3216/7134 completed (loss: 0.024190539494156837, acc: 0.9928366541862488)
[2025-02-13 02:51:14,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:14,943][root][INFO] - Training Epoch: 1/2, step 3217/7134 completed (loss: 0.02645748481154442, acc: 0.9896774291992188)
[2025-02-13 02:51:15,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:15,337][root][INFO] - Training Epoch: 1/2, step 3218/7134 completed (loss: 0.07448667287826538, acc: 0.9796556830406189)
[2025-02-13 02:51:15,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:15,749][root][INFO] - Training Epoch: 1/2, step 3219/7134 completed (loss: 0.06921742111444473, acc: 0.9778434038162231)
[2025-02-13 02:51:15,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:16,174][root][INFO] - Training Epoch: 1/2, step 3220/7134 completed (loss: 0.0612347237765789, acc: 0.979973316192627)
[2025-02-13 02:51:16,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:16,609][root][INFO] - Training Epoch: 1/2, step 3221/7134 completed (loss: 0.046287573873996735, acc: 0.9904109835624695)
[2025-02-13 02:51:16,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:17,018][root][INFO] - Training Epoch: 1/2, step 3222/7134 completed (loss: 0.08030740916728973, acc: 0.9837296605110168)
[2025-02-13 02:51:17,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:17,431][root][INFO] - Training Epoch: 1/2, step 3223/7134 completed (loss: 0.07660041749477386, acc: 0.9857142567634583)
[2025-02-13 02:51:17,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:17,882][root][INFO] - Training Epoch: 1/2, step 3224/7134 completed (loss: 0.09870954602956772, acc: 0.9739508032798767)
[2025-02-13 02:51:18,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:18,321][root][INFO] - Training Epoch: 1/2, step 3225/7134 completed (loss: 0.05605188012123108, acc: 0.982300877571106)
[2025-02-13 02:51:18,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:18,710][root][INFO] - Training Epoch: 1/2, step 3226/7134 completed (loss: 0.05648680776357651, acc: 0.9748653769493103)
[2025-02-13 02:51:18,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:19,178][root][INFO] - Training Epoch: 1/2, step 3227/7134 completed (loss: 0.10714586824178696, acc: 0.9703296422958374)
[2025-02-13 02:51:19,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:19,626][root][INFO] - Training Epoch: 1/2, step 3228/7134 completed (loss: 0.04002729058265686, acc: 0.9881734848022461)
[2025-02-13 02:51:19,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:20,055][root][INFO] - Training Epoch: 1/2, step 3229/7134 completed (loss: 0.05661078542470932, acc: 0.9804161787033081)
[2025-02-13 02:51:20,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:20,464][root][INFO] - Training Epoch: 1/2, step 3230/7134 completed (loss: 0.07723993808031082, acc: 0.9748520851135254)
[2025-02-13 02:51:20,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:20,818][root][INFO] - Training Epoch: 1/2, step 3231/7134 completed (loss: 0.0783492922782898, acc: 0.978723406791687)
[2025-02-13 02:51:20,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:21,253][root][INFO] - Training Epoch: 1/2, step 3232/7134 completed (loss: 0.03937834873795509, acc: 0.9898989796638489)
[2025-02-13 02:51:21,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:21,650][root][INFO] - Training Epoch: 1/2, step 3233/7134 completed (loss: 0.09272659569978714, acc: 0.9822485446929932)
[2025-02-13 02:51:21,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:22,040][root][INFO] - Training Epoch: 1/2, step 3234/7134 completed (loss: 0.03491116687655449, acc: 0.9921011328697205)
[2025-02-13 02:51:22,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:22,453][root][INFO] - Training Epoch: 1/2, step 3235/7134 completed (loss: 0.0372503288090229, acc: 0.9847618937492371)
[2025-02-13 02:51:22,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:22,841][root][INFO] - Training Epoch: 1/2, step 3236/7134 completed (loss: 0.13956226408481598, acc: 0.9720176458358765)
[2025-02-13 02:51:22,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:23,252][root][INFO] - Training Epoch: 1/2, step 3237/7134 completed (loss: 0.034694984555244446, acc: 0.9919484853744507)
[2025-02-13 02:51:23,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:23,660][root][INFO] - Training Epoch: 1/2, step 3238/7134 completed (loss: 0.09068156778812408, acc: 0.9723926186561584)
[2025-02-13 02:51:23,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:24,069][root][INFO] - Training Epoch: 1/2, step 3239/7134 completed (loss: 0.039362650364637375, acc: 0.9873816967010498)
[2025-02-13 02:51:24,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:24,473][root][INFO] - Training Epoch: 1/2, step 3240/7134 completed (loss: 0.03778544440865517, acc: 0.9895366430282593)
[2025-02-13 02:51:24,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:24,876][root][INFO] - Training Epoch: 1/2, step 3241/7134 completed (loss: 0.03385346755385399, acc: 0.9896142482757568)
[2025-02-13 02:51:25,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:25,281][root][INFO] - Training Epoch: 1/2, step 3242/7134 completed (loss: 0.03632485494017601, acc: 0.991830050945282)
[2025-02-13 02:51:25,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:25,705][root][INFO] - Training Epoch: 1/2, step 3243/7134 completed (loss: 0.042767804116010666, acc: 0.9850136041641235)
[2025-02-13 02:51:25,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:26,104][root][INFO] - Training Epoch: 1/2, step 3244/7134 completed (loss: 0.030897771939635277, acc: 0.991391658782959)
[2025-02-13 02:51:26,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:26,541][root][INFO] - Training Epoch: 1/2, step 3245/7134 completed (loss: 0.03389393910765648, acc: 0.9922680258750916)
[2025-02-13 02:51:26,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:26,976][root][INFO] - Training Epoch: 1/2, step 3246/7134 completed (loss: 0.06252171099185944, acc: 0.9807692170143127)
[2025-02-13 02:51:27,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:27,393][root][INFO] - Training Epoch: 1/2, step 3247/7134 completed (loss: 0.04182249307632446, acc: 0.9837925434112549)
[2025-02-13 02:51:27,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:27,795][root][INFO] - Training Epoch: 1/2, step 3248/7134 completed (loss: 0.033993225544691086, acc: 0.9885057210922241)
[2025-02-13 02:51:27,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:28,208][root][INFO] - Training Epoch: 1/2, step 3249/7134 completed (loss: 0.03244401142001152, acc: 0.9916782379150391)
[2025-02-13 02:51:28,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:28,617][root][INFO] - Training Epoch: 1/2, step 3250/7134 completed (loss: 0.054100025445222855, acc: 0.9841726422309875)
[2025-02-13 02:51:28,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:29,023][root][INFO] - Training Epoch: 1/2, step 3251/7134 completed (loss: 0.0221809484064579, acc: 0.9914407730102539)
[2025-02-13 02:51:29,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:29,446][root][INFO] - Training Epoch: 1/2, step 3252/7134 completed (loss: 0.011093872599303722, acc: 0.9957447052001953)
[2025-02-13 02:51:29,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:29,856][root][INFO] - Training Epoch: 1/2, step 3253/7134 completed (loss: 0.028330696746706963, acc: 0.9935794472694397)
[2025-02-13 02:51:30,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:30,279][root][INFO] - Training Epoch: 1/2, step 3254/7134 completed (loss: 0.019564004614949226, acc: 0.9932546615600586)
[2025-02-13 02:51:30,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:30,685][root][INFO] - Training Epoch: 1/2, step 3255/7134 completed (loss: 0.018665144219994545, acc: 0.989347517490387)
[2025-02-13 02:51:30,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:31,089][root][INFO] - Training Epoch: 1/2, step 3256/7134 completed (loss: 0.02560385875403881, acc: 0.9940740466117859)
[2025-02-13 02:51:31,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:31,494][root][INFO] - Training Epoch: 1/2, step 3257/7134 completed (loss: 0.02714863233268261, acc: 0.994452178478241)
[2025-02-13 02:51:31,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:31,911][root][INFO] - Training Epoch: 1/2, step 3258/7134 completed (loss: 0.04616211727261543, acc: 0.9846153855323792)
[2025-02-13 02:51:32,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:32,319][root][INFO] - Training Epoch: 1/2, step 3259/7134 completed (loss: 0.04482176899909973, acc: 0.9911764860153198)
[2025-02-13 02:51:32,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:32,713][root][INFO] - Training Epoch: 1/2, step 3260/7134 completed (loss: 0.11153688281774521, acc: 0.9777448177337646)
[2025-02-13 02:51:32,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:33,117][root][INFO] - Training Epoch: 1/2, step 3261/7134 completed (loss: 0.0782281905412674, acc: 0.9748322367668152)
[2025-02-13 02:51:33,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:33,535][root][INFO] - Training Epoch: 1/2, step 3262/7134 completed (loss: 0.045122601091861725, acc: 0.9855282306671143)
[2025-02-13 02:51:33,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:33,940][root][INFO] - Training Epoch: 1/2, step 3263/7134 completed (loss: 0.07424512505531311, acc: 0.9838472604751587)
[2025-02-13 02:51:34,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:34,340][root][INFO] - Training Epoch: 1/2, step 3264/7134 completed (loss: 0.03258388489484787, acc: 0.9888888597488403)
[2025-02-13 02:51:34,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:34,766][root][INFO] - Training Epoch: 1/2, step 3265/7134 completed (loss: 0.1016487404704094, acc: 0.9786324501037598)
[2025-02-13 02:51:34,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:35,186][root][INFO] - Training Epoch: 1/2, step 3266/7134 completed (loss: 0.059480220079422, acc: 0.9849849939346313)
[2025-02-13 02:51:35,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:35,604][root][INFO] - Training Epoch: 1/2, step 3267/7134 completed (loss: 0.0870145633816719, acc: 0.9742120504379272)
[2025-02-13 02:51:35,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:35,992][root][INFO] - Training Epoch: 1/2, step 3268/7134 completed (loss: 0.062126971781253815, acc: 0.9861325025558472)
[2025-02-13 02:51:36,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:36,425][root][INFO] - Training Epoch: 1/2, step 3269/7134 completed (loss: 0.045395173132419586, acc: 0.9873737096786499)
[2025-02-13 02:51:36,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:36,861][root][INFO] - Training Epoch: 1/2, step 3270/7134 completed (loss: 0.021607598289847374, acc: 0.9964115023612976)
[2025-02-13 02:51:36,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:37,131][root][INFO] - Training Epoch: 1/2, step 3271/7134 completed (loss: 0.008853198029100895, acc: 1.0)
[2025-02-13 02:51:37,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:37,543][root][INFO] - Training Epoch: 1/2, step 3272/7134 completed (loss: 0.03427458927035332, acc: 0.9969465732574463)
[2025-02-13 02:51:37,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:37,981][root][INFO] - Training Epoch: 1/2, step 3273/7134 completed (loss: 0.03726586326956749, acc: 0.9872286319732666)
[2025-02-13 02:51:38,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:38,416][root][INFO] - Training Epoch: 1/2, step 3274/7134 completed (loss: 0.06240957975387573, acc: 0.98531574010849)
[2025-02-13 02:51:38,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:38,841][root][INFO] - Training Epoch: 1/2, step 3275/7134 completed (loss: 0.1653832495212555, acc: 0.957602322101593)
[2025-02-13 02:51:38,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:39,255][root][INFO] - Training Epoch: 1/2, step 3276/7134 completed (loss: 0.08333807438611984, acc: 0.9738292098045349)
[2025-02-13 02:51:39,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:39,674][root][INFO] - Training Epoch: 1/2, step 3277/7134 completed (loss: 0.07151763886213303, acc: 0.9756097793579102)
[2025-02-13 02:51:39,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:40,021][root][INFO] - Training Epoch: 1/2, step 3278/7134 completed (loss: 0.054200679063797, acc: 0.9827916026115417)
[2025-02-13 02:51:40,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:40,459][root][INFO] - Training Epoch: 1/2, step 3279/7134 completed (loss: 0.066375732421875, acc: 0.9776397347450256)
[2025-02-13 02:51:40,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:40,843][root][INFO] - Training Epoch: 1/2, step 3280/7134 completed (loss: 0.20332154631614685, acc: 0.9585062265396118)
[2025-02-13 02:51:40,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:41,264][root][INFO] - Training Epoch: 1/2, step 3281/7134 completed (loss: 0.06315647065639496, acc: 0.9855072498321533)
[2025-02-13 02:51:41,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:41,670][root][INFO] - Training Epoch: 1/2, step 3282/7134 completed (loss: 0.039274588227272034, acc: 0.9873417615890503)
[2025-02-13 02:51:41,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:42,099][root][INFO] - Training Epoch: 1/2, step 3283/7134 completed (loss: 0.06313595175743103, acc: 0.9793814420700073)
[2025-02-13 02:51:42,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:42,504][root][INFO] - Training Epoch: 1/2, step 3284/7134 completed (loss: 0.10110288113355637, acc: 0.9726027250289917)
[2025-02-13 02:51:42,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:42,906][root][INFO] - Training Epoch: 1/2, step 3285/7134 completed (loss: 0.05510735139250755, acc: 0.9783281683921814)
[2025-02-13 02:51:43,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:43,319][root][INFO] - Training Epoch: 1/2, step 3286/7134 completed (loss: 0.04777628183364868, acc: 0.9909774661064148)
[2025-02-13 02:51:43,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:43,721][root][INFO] - Training Epoch: 1/2, step 3287/7134 completed (loss: 0.12070974707603455, acc: 0.9736841917037964)
[2025-02-13 02:51:43,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:44,142][root][INFO] - Training Epoch: 1/2, step 3288/7134 completed (loss: 0.06990937888622284, acc: 0.976047933101654)
[2025-02-13 02:51:44,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:44,490][root][INFO] - Training Epoch: 1/2, step 3289/7134 completed (loss: 0.1127791553735733, acc: 0.9719298481941223)
[2025-02-13 02:51:44,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:44,875][root][INFO] - Training Epoch: 1/2, step 3290/7134 completed (loss: 0.09563228487968445, acc: 0.9791666865348816)
[2025-02-13 02:51:45,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:45,268][root][INFO] - Training Epoch: 1/2, step 3291/7134 completed (loss: 0.08203326910734177, acc: 0.9755011200904846)
[2025-02-13 02:51:45,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:45,664][root][INFO] - Training Epoch: 1/2, step 3292/7134 completed (loss: 0.05565173551440239, acc: 0.9876543283462524)
[2025-02-13 02:51:45,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:46,111][root][INFO] - Training Epoch: 1/2, step 3293/7134 completed (loss: 0.049087148159742355, acc: 0.982300877571106)
[2025-02-13 02:51:46,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:46,521][root][INFO] - Training Epoch: 1/2, step 3294/7134 completed (loss: 0.10888057202100754, acc: 0.9709035158157349)
[2025-02-13 02:51:46,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:46,892][root][INFO] - Training Epoch: 1/2, step 3295/7134 completed (loss: 0.032662179321050644, acc: 0.9897611141204834)
[2025-02-13 02:51:47,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:47,281][root][INFO] - Training Epoch: 1/2, step 3296/7134 completed (loss: 0.07444953918457031, acc: 0.9780621528625488)
[2025-02-13 02:51:47,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:47,630][root][INFO] - Training Epoch: 1/2, step 3297/7134 completed (loss: 0.06098296493291855, acc: 0.9812206625938416)
[2025-02-13 02:51:47,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:48,026][root][INFO] - Training Epoch: 1/2, step 3298/7134 completed (loss: 0.11527428030967712, acc: 0.97444087266922)
[2025-02-13 02:51:48,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:48,422][root][INFO] - Training Epoch: 1/2, step 3299/7134 completed (loss: 0.055327050387859344, acc: 0.9848993420600891)
[2025-02-13 02:51:48,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:48,811][root][INFO] - Training Epoch: 1/2, step 3300/7134 completed (loss: 0.08156032115221024, acc: 0.9732620120048523)
[2025-02-13 02:51:48,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:49,157][root][INFO] - Training Epoch: 1/2, step 3301/7134 completed (loss: 0.1075192540884018, acc: 0.9730290174484253)
[2025-02-13 02:51:49,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:49,545][root][INFO] - Training Epoch: 1/2, step 3302/7134 completed (loss: 0.06998640298843384, acc: 0.9848484992980957)
[2025-02-13 02:51:49,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:49,870][root][INFO] - Training Epoch: 1/2, step 3303/7134 completed (loss: 0.06518405675888062, acc: 0.9867841601371765)
[2025-02-13 02:51:50,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:50,304][root][INFO] - Training Epoch: 1/2, step 3304/7134 completed (loss: 0.03719234839081764, acc: 0.9892601370811462)
[2025-02-13 02:51:50,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:50,702][root][INFO] - Training Epoch: 1/2, step 3305/7134 completed (loss: 0.05692116171121597, acc: 0.9879336357116699)
[2025-02-13 02:51:50,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:51,168][root][INFO] - Training Epoch: 1/2, step 3306/7134 completed (loss: 0.053272545337677, acc: 0.9844357967376709)
[2025-02-13 02:51:51,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:51,564][root][INFO] - Training Epoch: 1/2, step 3307/7134 completed (loss: 0.07904108613729477, acc: 0.9813874959945679)
[2025-02-13 02:51:51,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:51,966][root][INFO] - Training Epoch: 1/2, step 3308/7134 completed (loss: 0.05945845693349838, acc: 0.9826589822769165)
[2025-02-13 02:51:52,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:52,357][root][INFO] - Training Epoch: 1/2, step 3309/7134 completed (loss: 0.10208597779273987, acc: 0.9734513163566589)
[2025-02-13 02:51:52,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:52,797][root][INFO] - Training Epoch: 1/2, step 3310/7134 completed (loss: 0.07224518060684204, acc: 0.9829424023628235)
[2025-02-13 02:51:52,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:53,089][root][INFO] - Training Epoch: 1/2, step 3311/7134 completed (loss: 0.05692223832011223, acc: 0.9784946441650391)
[2025-02-13 02:51:53,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:53,476][root][INFO] - Training Epoch: 1/2, step 3312/7134 completed (loss: 0.07591760158538818, acc: 0.9745454788208008)
[2025-02-13 02:51:53,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:53,883][root][INFO] - Training Epoch: 1/2, step 3313/7134 completed (loss: 0.03133293613791466, acc: 0.9952267408370972)
[2025-02-13 02:51:54,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:54,285][root][INFO] - Training Epoch: 1/2, step 3314/7134 completed (loss: 0.052120503038167953, acc: 0.9909420013427734)
[2025-02-13 02:51:54,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:54,728][root][INFO] - Training Epoch: 1/2, step 3315/7134 completed (loss: 0.10480587184429169, acc: 0.972862958908081)
[2025-02-13 02:51:54,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:55,164][root][INFO] - Training Epoch: 1/2, step 3316/7134 completed (loss: 0.03370155021548271, acc: 0.9899159669876099)
[2025-02-13 02:51:55,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:55,578][root][INFO] - Training Epoch: 1/2, step 3317/7134 completed (loss: 0.13830238580703735, acc: 0.9634615182876587)
[2025-02-13 02:51:55,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:55,839][root][INFO] - Training Epoch: 1/2, step 3318/7134 completed (loss: 0.08774642646312714, acc: 0.9783393740653992)
[2025-02-13 02:51:55,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:56,280][root][INFO] - Training Epoch: 1/2, step 3319/7134 completed (loss: 0.05736122280359268, acc: 0.9820846915245056)
[2025-02-13 02:51:56,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:56,719][root][INFO] - Training Epoch: 1/2, step 3320/7134 completed (loss: 0.08528248965740204, acc: 0.9764397740364075)
[2025-02-13 02:51:56,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:57,122][root][INFO] - Training Epoch: 1/2, step 3321/7134 completed (loss: 0.12622925639152527, acc: 0.9743119478225708)
[2025-02-13 02:51:57,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:57,575][root][INFO] - Training Epoch: 1/2, step 3322/7134 completed (loss: 0.06642577797174454, acc: 0.9804804921150208)
[2025-02-13 02:51:57,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:58,006][root][INFO] - Training Epoch: 1/2, step 3323/7134 completed (loss: 0.057929299771785736, acc: 0.9765517115592957)
[2025-02-13 02:51:58,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:58,464][root][INFO] - Training Epoch: 1/2, step 3324/7134 completed (loss: 0.09520923346281052, acc: 0.9740871787071228)
[2025-02-13 02:51:58,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:58,901][root][INFO] - Training Epoch: 1/2, step 3325/7134 completed (loss: 0.06535019725561142, acc: 0.9717742204666138)
[2025-02-13 02:51:59,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:59,325][root][INFO] - Training Epoch: 1/2, step 3326/7134 completed (loss: 0.07358334958553314, acc: 0.9858267903327942)
[2025-02-13 02:51:59,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:59,732][root][INFO] - Training Epoch: 1/2, step 3327/7134 completed (loss: 0.0571221187710762, acc: 0.9797022938728333)
[2025-02-13 02:51:59,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:00,181][root][INFO] - Training Epoch: 1/2, step 3328/7134 completed (loss: 0.06405014544725418, acc: 0.9862155318260193)
[2025-02-13 02:52:00,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:00,608][root][INFO] - Training Epoch: 1/2, step 3329/7134 completed (loss: 0.03517099469900131, acc: 0.9914737939834595)
[2025-02-13 02:52:00,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:01,060][root][INFO] - Training Epoch: 1/2, step 3330/7134 completed (loss: 0.07915133982896805, acc: 0.981566846370697)
[2025-02-13 02:52:01,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:01,468][root][INFO] - Training Epoch: 1/2, step 3331/7134 completed (loss: 0.12117014080286026, acc: 0.9665697813034058)
[2025-02-13 02:52:01,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:01,904][root][INFO] - Training Epoch: 1/2, step 3332/7134 completed (loss: 0.16181580722332, acc: 0.9498270153999329)
[2025-02-13 02:52:02,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:02,281][root][INFO] - Training Epoch: 1/2, step 3333/7134 completed (loss: 0.12522906064987183, acc: 0.9733606576919556)
[2025-02-13 02:52:02,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:02,691][root][INFO] - Training Epoch: 1/2, step 3334/7134 completed (loss: 0.12058838456869125, acc: 0.9635134935379028)
[2025-02-13 02:52:02,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:03,100][root][INFO] - Training Epoch: 1/2, step 3335/7134 completed (loss: 0.06105237081646919, acc: 0.9857594966888428)
[2025-02-13 02:52:03,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:03,510][root][INFO] - Training Epoch: 1/2, step 3336/7134 completed (loss: 0.037755195051431656, acc: 0.991253674030304)
[2025-02-13 02:52:03,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:03,918][root][INFO] - Training Epoch: 1/2, step 3337/7134 completed (loss: 0.02443217672407627, acc: 0.990138053894043)
[2025-02-13 02:52:04,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:04,346][root][INFO] - Training Epoch: 1/2, step 3338/7134 completed (loss: 0.03373149037361145, acc: 0.991304337978363)
[2025-02-13 02:52:04,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:04,786][root][INFO] - Training Epoch: 1/2, step 3339/7134 completed (loss: 0.08245424181222916, acc: 0.9809069037437439)
[2025-02-13 02:52:04,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:05,192][root][INFO] - Training Epoch: 1/2, step 3340/7134 completed (loss: 0.13969111442565918, acc: 0.9638009071350098)
[2025-02-13 02:52:05,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:05,625][root][INFO] - Training Epoch: 1/2, step 3341/7134 completed (loss: 0.07406839728355408, acc: 0.9792592525482178)
[2025-02-13 02:52:05,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:06,037][root][INFO] - Training Epoch: 1/2, step 3342/7134 completed (loss: 0.056796472519636154, acc: 0.9831081032752991)
[2025-02-13 02:52:06,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:06,454][root][INFO] - Training Epoch: 1/2, step 3343/7134 completed (loss: 0.04461053013801575, acc: 0.9879032373428345)
[2025-02-13 02:52:06,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:06,873][root][INFO] - Training Epoch: 1/2, step 3344/7134 completed (loss: 0.04115397110581398, acc: 0.9914236664772034)
[2025-02-13 02:52:07,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:07,292][root][INFO] - Training Epoch: 1/2, step 3345/7134 completed (loss: 0.0860162302851677, acc: 0.987034022808075)
[2025-02-13 02:52:07,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:07,716][root][INFO] - Training Epoch: 1/2, step 3346/7134 completed (loss: 0.045130569487810135, acc: 0.9868420958518982)
[2025-02-13 02:52:07,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:08,110][root][INFO] - Training Epoch: 1/2, step 3347/7134 completed (loss: 0.09632175415754318, acc: 0.9739413857460022)
[2025-02-13 02:52:08,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:08,512][root][INFO] - Training Epoch: 1/2, step 3348/7134 completed (loss: 0.0698128268122673, acc: 0.9807407259941101)
[2025-02-13 02:52:08,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:08,875][root][INFO] - Training Epoch: 1/2, step 3349/7134 completed (loss: 0.032453618943691254, acc: 0.9911816716194153)
[2025-02-13 02:52:09,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:09,282][root][INFO] - Training Epoch: 1/2, step 3350/7134 completed (loss: 0.043968938291072845, acc: 0.9857723712921143)
[2025-02-13 02:52:09,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:09,678][root][INFO] - Training Epoch: 1/2, step 3351/7134 completed (loss: 0.10149271041154861, acc: 0.9734748005867004)
[2025-02-13 02:52:09,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:10,069][root][INFO] - Training Epoch: 1/2, step 3352/7134 completed (loss: 0.027867399156093597, acc: 0.9895651936531067)
[2025-02-13 02:52:10,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:10,459][root][INFO] - Training Epoch: 1/2, step 3353/7134 completed (loss: 0.05383554846048355, acc: 0.9881656765937805)
[2025-02-13 02:52:10,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:10,857][root][INFO] - Training Epoch: 1/2, step 3354/7134 completed (loss: 0.0495205782353878, acc: 0.9891473054885864)
[2025-02-13 02:52:10,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:11,268][root][INFO] - Training Epoch: 1/2, step 3355/7134 completed (loss: 0.024666298180818558, acc: 0.9929478168487549)
[2025-02-13 02:52:11,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:11,674][root][INFO] - Training Epoch: 1/2, step 3356/7134 completed (loss: 0.019010411575436592, acc: 0.9930555820465088)
[2025-02-13 02:52:11,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:12,087][root][INFO] - Training Epoch: 1/2, step 3357/7134 completed (loss: 0.04083910584449768, acc: 0.9913420081138611)
[2025-02-13 02:52:12,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:12,476][root][INFO] - Training Epoch: 1/2, step 3358/7134 completed (loss: 0.07663088291883469, acc: 0.9826989769935608)
[2025-02-13 02:52:12,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:12,870][root][INFO] - Training Epoch: 1/2, step 3359/7134 completed (loss: 0.04546106979250908, acc: 0.991909384727478)
[2025-02-13 02:52:12,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:13,266][root][INFO] - Training Epoch: 1/2, step 3360/7134 completed (loss: 0.1095646545290947, acc: 0.9721029996871948)
[2025-02-13 02:52:13,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:13,659][root][INFO] - Training Epoch: 1/2, step 3361/7134 completed (loss: 0.08944954723119736, acc: 0.9731543660163879)
[2025-02-13 02:52:13,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:14,052][root][INFO] - Training Epoch: 1/2, step 3362/7134 completed (loss: 0.0705702155828476, acc: 0.9773242473602295)
[2025-02-13 02:52:14,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:14,401][root][INFO] - Training Epoch: 1/2, step 3363/7134 completed (loss: 0.018807152286171913, acc: 0.9953051805496216)
[2025-02-13 02:52:14,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:14,819][root][INFO] - Training Epoch: 1/2, step 3364/7134 completed (loss: 0.10385002940893173, acc: 0.9754224419593811)
[2025-02-13 02:52:14,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:15,222][root][INFO] - Training Epoch: 1/2, step 3365/7134 completed (loss: 0.09649312496185303, acc: 0.9807229042053223)
[2025-02-13 02:52:15,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:15,633][root][INFO] - Training Epoch: 1/2, step 3366/7134 completed (loss: 0.03590104356408119, acc: 0.9887892603874207)
[2025-02-13 02:52:15,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:16,034][root][INFO] - Training Epoch: 1/2, step 3367/7134 completed (loss: 0.04244095832109451, acc: 0.9903581142425537)
[2025-02-13 02:52:16,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:16,453][root][INFO] - Training Epoch: 1/2, step 3368/7134 completed (loss: 0.07030218094587326, acc: 0.9828392863273621)
[2025-02-13 02:52:16,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:16,902][root][INFO] - Training Epoch: 1/2, step 3369/7134 completed (loss: 0.07979758083820343, acc: 0.9812949895858765)
[2025-02-13 02:52:17,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:17,316][root][INFO] - Training Epoch: 1/2, step 3370/7134 completed (loss: 0.04090864956378937, acc: 0.9899857044219971)
[2025-02-13 02:52:17,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:17,715][root][INFO] - Training Epoch: 1/2, step 3371/7134 completed (loss: 0.038210429251194, acc: 0.9884868264198303)
[2025-02-13 02:52:17,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:18,129][root][INFO] - Training Epoch: 1/2, step 3372/7134 completed (loss: 0.04723739996552467, acc: 0.9890282154083252)
[2025-02-13 02:52:18,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:18,530][root][INFO] - Training Epoch: 1/2, step 3373/7134 completed (loss: 0.04187435656785965, acc: 0.9907407164573669)
[2025-02-13 02:52:18,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:18,972][root][INFO] - Training Epoch: 1/2, step 3374/7134 completed (loss: 0.04233952611684799, acc: 0.9902507066726685)
[2025-02-13 02:52:19,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:19,413][root][INFO] - Training Epoch: 1/2, step 3375/7134 completed (loss: 0.06026361510157585, acc: 0.9855282306671143)
[2025-02-13 02:52:19,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:19,845][root][INFO] - Training Epoch: 1/2, step 3376/7134 completed (loss: 0.036713022738695145, acc: 0.9885057210922241)
[2025-02-13 02:52:19,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:20,311][root][INFO] - Training Epoch: 1/2, step 3377/7134 completed (loss: 0.019146448001265526, acc: 0.9940405488014221)
[2025-02-13 02:52:20,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:20,738][root][INFO] - Training Epoch: 1/2, step 3378/7134 completed (loss: 0.024418942630290985, acc: 0.9934895634651184)
[2025-02-13 02:52:20,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:21,143][root][INFO] - Training Epoch: 1/2, step 3379/7134 completed (loss: 0.03551414608955383, acc: 0.9894737005233765)
[2025-02-13 02:52:21,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:21,547][root][INFO] - Training Epoch: 1/2, step 3380/7134 completed (loss: 0.06615924835205078, acc: 0.9861538410186768)
[2025-02-13 02:52:21,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:21,986][root][INFO] - Training Epoch: 1/2, step 3381/7134 completed (loss: 0.059367839246988297, acc: 0.9806201457977295)
[2025-02-13 02:52:22,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:22,396][root][INFO] - Training Epoch: 1/2, step 3382/7134 completed (loss: 0.02476818487048149, acc: 0.991349458694458)
[2025-02-13 02:52:22,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:22,836][root][INFO] - Training Epoch: 1/2, step 3383/7134 completed (loss: 0.0559057779610157, acc: 0.9870874881744385)
[2025-02-13 02:52:22,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:23,224][root][INFO] - Training Epoch: 1/2, step 3384/7134 completed (loss: 0.05258970707654953, acc: 0.9877675771713257)
[2025-02-13 02:52:23,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:23,636][root][INFO] - Training Epoch: 1/2, step 3385/7134 completed (loss: 0.03799370303750038, acc: 0.9884892106056213)
[2025-02-13 02:52:23,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:24,049][root][INFO] - Training Epoch: 1/2, step 3386/7134 completed (loss: 0.030219323933124542, acc: 0.991525411605835)
[2025-02-13 02:52:24,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:24,462][root][INFO] - Training Epoch: 1/2, step 3387/7134 completed (loss: 0.06180219724774361, acc: 0.9863945841789246)
[2025-02-13 02:52:24,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:24,874][root][INFO] - Training Epoch: 1/2, step 3388/7134 completed (loss: 0.060257717967033386, acc: 0.9819168448448181)
[2025-02-13 02:52:25,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:25,285][root][INFO] - Training Epoch: 1/2, step 3389/7134 completed (loss: 0.03467588871717453, acc: 0.990777313709259)
[2025-02-13 02:52:25,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:25,710][root][INFO] - Training Epoch: 1/2, step 3390/7134 completed (loss: 0.03844977170228958, acc: 0.9896373152732849)
[2025-02-13 02:52:25,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:26,095][root][INFO] - Training Epoch: 1/2, step 3391/7134 completed (loss: 0.021845459938049316, acc: 0.9958791136741638)
[2025-02-13 02:52:26,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:26,477][root][INFO] - Training Epoch: 1/2, step 3392/7134 completed (loss: 0.03825175762176514, acc: 0.9827044010162354)
[2025-02-13 02:52:26,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:26,869][root][INFO] - Training Epoch: 1/2, step 3393/7134 completed (loss: 0.06697845458984375, acc: 0.979938268661499)
[2025-02-13 02:52:27,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:27,288][root][INFO] - Training Epoch: 1/2, step 3394/7134 completed (loss: 0.04680405557155609, acc: 0.9857594966888428)
[2025-02-13 02:52:27,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:27,666][root][INFO] - Training Epoch: 1/2, step 3395/7134 completed (loss: 0.09235308319330215, acc: 0.975649356842041)
[2025-02-13 02:52:27,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:28,103][root][INFO] - Training Epoch: 1/2, step 3396/7134 completed (loss: 0.026590809226036072, acc: 0.9940119981765747)
[2025-02-13 02:52:28,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:28,545][root][INFO] - Training Epoch: 1/2, step 3397/7134 completed (loss: 0.04827937111258507, acc: 0.9914425611495972)
[2025-02-13 02:52:28,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:28,962][root][INFO] - Training Epoch: 1/2, step 3398/7134 completed (loss: 0.05180090665817261, acc: 0.9892473220825195)
[2025-02-13 02:52:29,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:29,365][root][INFO] - Training Epoch: 1/2, step 3399/7134 completed (loss: 0.13330936431884766, acc: 0.9724264740943909)
[2025-02-13 02:52:29,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:29,779][root][INFO] - Training Epoch: 1/2, step 3400/7134 completed (loss: 0.10576120764017105, acc: 0.9756592512130737)
[2025-02-13 02:52:29,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:30,183][root][INFO] - Training Epoch: 1/2, step 3401/7134 completed (loss: 0.08791552484035492, acc: 0.9790209531784058)
[2025-02-13 02:52:30,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:30,668][root][INFO] - Training Epoch: 1/2, step 3402/7134 completed (loss: 0.08537822216749191, acc: 0.9723320007324219)
[2025-02-13 02:52:30,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:31,128][root][INFO] - Training Epoch: 1/2, step 3403/7134 completed (loss: 0.10787669569253922, acc: 0.96484375)
[2025-02-13 02:52:31,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:31,535][root][INFO] - Training Epoch: 1/2, step 3404/7134 completed (loss: 0.025951862335205078, acc: 0.9925261735916138)
[2025-02-13 02:52:31,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:31,950][root][INFO] - Training Epoch: 1/2, step 3405/7134 completed (loss: 0.09119529277086258, acc: 0.9838129281997681)
[2025-02-13 02:52:32,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:32,384][root][INFO] - Training Epoch: 1/2, step 3406/7134 completed (loss: 0.05184956267476082, acc: 0.9832473993301392)
[2025-02-13 02:52:32,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:32,756][root][INFO] - Training Epoch: 1/2, step 3407/7134 completed (loss: 0.06467820703983307, acc: 0.9828392863273621)
[2025-02-13 02:52:32,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:33,145][root][INFO] - Training Epoch: 1/2, step 3408/7134 completed (loss: 0.07624659687280655, acc: 0.9769585132598877)
[2025-02-13 02:52:33,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:33,583][root][INFO] - Training Epoch: 1/2, step 3409/7134 completed (loss: 0.07659567147493362, acc: 0.981055498123169)
[2025-02-13 02:52:33,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:33,993][root][INFO] - Training Epoch: 1/2, step 3410/7134 completed (loss: 0.06846463680267334, acc: 0.9848484992980957)
[2025-02-13 02:52:34,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:34,425][root][INFO] - Training Epoch: 1/2, step 3411/7134 completed (loss: 0.05919405817985535, acc: 0.9857697486877441)
[2025-02-13 02:52:34,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:34,812][root][INFO] - Training Epoch: 1/2, step 3412/7134 completed (loss: 0.06455070525407791, acc: 0.9735350012779236)
[2025-02-13 02:52:34,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:35,217][root][INFO] - Training Epoch: 1/2, step 3413/7134 completed (loss: 0.07499147206544876, acc: 0.9760000109672546)
[2025-02-13 02:52:35,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:35,619][root][INFO] - Training Epoch: 1/2, step 3414/7134 completed (loss: 0.06261016428470612, acc: 0.9866443872451782)
[2025-02-13 02:52:35,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:36,057][root][INFO] - Training Epoch: 1/2, step 3415/7134 completed (loss: 0.07259152829647064, acc: 0.9839743375778198)
[2025-02-13 02:52:36,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:36,512][root][INFO] - Training Epoch: 1/2, step 3416/7134 completed (loss: 0.061480529606342316, acc: 0.9807923436164856)
[2025-02-13 02:52:36,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:36,942][root][INFO] - Training Epoch: 1/2, step 3417/7134 completed (loss: 0.04045938700437546, acc: 0.9905533194541931)
[2025-02-13 02:52:37,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:37,357][root][INFO] - Training Epoch: 1/2, step 3418/7134 completed (loss: 0.045528609305620193, acc: 0.9896907210350037)
[2025-02-13 02:52:37,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:37,752][root][INFO] - Training Epoch: 1/2, step 3419/7134 completed (loss: 0.02230105921626091, acc: 0.9899135231971741)
[2025-02-13 02:52:37,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:38,166][root][INFO] - Training Epoch: 1/2, step 3420/7134 completed (loss: 0.017572524026036263, acc: 0.9952606558799744)
[2025-02-13 02:52:38,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:38,586][root][INFO] - Training Epoch: 1/2, step 3421/7134 completed (loss: 0.03836100921034813, acc: 0.993779182434082)
[2025-02-13 02:52:38,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:38,968][root][INFO] - Training Epoch: 1/2, step 3422/7134 completed (loss: 0.015268013812601566, acc: 0.9947460889816284)
[2025-02-13 02:52:39,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:39,390][root][INFO] - Training Epoch: 1/2, step 3423/7134 completed (loss: 0.015930896624922752, acc: 0.9954545497894287)
[2025-02-13 02:52:39,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:39,800][root][INFO] - Training Epoch: 1/2, step 3424/7134 completed (loss: 0.10303545743227005, acc: 0.974397599697113)
[2025-02-13 02:52:39,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:40,201][root][INFO] - Training Epoch: 1/2, step 3425/7134 completed (loss: 0.05690060555934906, acc: 0.9820512533187866)
[2025-02-13 02:52:40,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:40,632][root][INFO] - Training Epoch: 1/2, step 3426/7134 completed (loss: 0.05842124670743942, acc: 0.9903448224067688)
[2025-02-13 02:52:40,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:41,017][root][INFO] - Training Epoch: 1/2, step 3427/7134 completed (loss: 0.04255533963441849, acc: 0.9936808943748474)
[2025-02-13 02:52:41,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:41,448][root][INFO] - Training Epoch: 1/2, step 3428/7134 completed (loss: 0.03160357102751732, acc: 0.9928366541862488)
[2025-02-13 02:52:41,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:41,891][root][INFO] - Training Epoch: 1/2, step 3429/7134 completed (loss: 0.05646931380033493, acc: 0.9822006225585938)
[2025-02-13 02:52:42,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:42,301][root][INFO] - Training Epoch: 1/2, step 3430/7134 completed (loss: 0.09021386504173279, acc: 0.9789302945137024)
[2025-02-13 02:52:42,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:42,723][root][INFO] - Training Epoch: 1/2, step 3431/7134 completed (loss: 0.059052031487226486, acc: 0.9847198724746704)
[2025-02-13 02:52:42,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:43,163][root][INFO] - Training Epoch: 1/2, step 3432/7134 completed (loss: 0.05389287695288658, acc: 0.9876543283462524)
[2025-02-13 02:52:43,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:43,610][root][INFO] - Training Epoch: 1/2, step 3433/7134 completed (loss: 0.1049424558877945, acc: 0.9738562107086182)
[2025-02-13 02:52:43,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:44,014][root][INFO] - Training Epoch: 1/2, step 3434/7134 completed (loss: 0.05484180897474289, acc: 0.9841017723083496)
[2025-02-13 02:52:44,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:44,425][root][INFO] - Training Epoch: 1/2, step 3435/7134 completed (loss: 0.059565044939517975, acc: 0.9871382713317871)
[2025-02-13 02:52:44,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:44,844][root][INFO] - Training Epoch: 1/2, step 3436/7134 completed (loss: 0.054544974118471146, acc: 0.9756097793579102)
[2025-02-13 02:52:44,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:45,211][root][INFO] - Training Epoch: 1/2, step 3437/7134 completed (loss: 0.04030180722475052, acc: 0.9921875)
[2025-02-13 02:52:45,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:45,618][root][INFO] - Training Epoch: 1/2, step 3438/7134 completed (loss: 0.04617907851934433, acc: 0.991919219493866)
[2025-02-13 02:52:45,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:45,997][root][INFO] - Training Epoch: 1/2, step 3439/7134 completed (loss: 0.05153696611523628, acc: 0.9819168448448181)
[2025-02-13 02:52:46,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:46,398][root][INFO] - Training Epoch: 1/2, step 3440/7134 completed (loss: 0.11717455089092255, acc: 0.9629629850387573)
[2025-02-13 02:52:46,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:46,797][root][INFO] - Training Epoch: 1/2, step 3441/7134 completed (loss: 0.11532764881849289, acc: 0.97398841381073)
[2025-02-13 02:52:46,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:47,200][root][INFO] - Training Epoch: 1/2, step 3442/7134 completed (loss: 0.031625889241695404, acc: 0.989051103591919)
[2025-02-13 02:52:47,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:47,555][root][INFO] - Training Epoch: 1/2, step 3443/7134 completed (loss: 0.08312931656837463, acc: 0.9803370833396912)
[2025-02-13 02:52:47,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:47,961][root][INFO] - Training Epoch: 1/2, step 3444/7134 completed (loss: 0.06961595267057419, acc: 0.9841772317886353)
[2025-02-13 02:52:48,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:48,305][root][INFO] - Training Epoch: 1/2, step 3445/7134 completed (loss: 0.15800601243972778, acc: 0.9591397643089294)
[2025-02-13 02:52:48,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:48,726][root][INFO] - Training Epoch: 1/2, step 3446/7134 completed (loss: 0.10987913608551025, acc: 0.9648829698562622)
[2025-02-13 02:52:48,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:49,142][root][INFO] - Training Epoch: 1/2, step 3447/7134 completed (loss: 0.08700519800186157, acc: 0.9706840515136719)
[2025-02-13 02:52:49,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:49,547][root][INFO] - Training Epoch: 1/2, step 3448/7134 completed (loss: 0.04135292023420334, acc: 0.9864864945411682)
[2025-02-13 02:52:49,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:49,954][root][INFO] - Training Epoch: 1/2, step 3449/7134 completed (loss: 0.06689862161874771, acc: 0.9846938848495483)
[2025-02-13 02:52:50,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:50,368][root][INFO] - Training Epoch: 1/2, step 3450/7134 completed (loss: 0.0786467120051384, acc: 0.9822784662246704)
[2025-02-13 02:52:50,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:50,769][root][INFO] - Training Epoch: 1/2, step 3451/7134 completed (loss: 0.06411489844322205, acc: 0.982332170009613)
[2025-02-13 02:52:50,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:51,170][root][INFO] - Training Epoch: 1/2, step 3452/7134 completed (loss: 0.06155698746442795, acc: 0.9848155975341797)
[2025-02-13 02:52:51,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:51,562][root][INFO] - Training Epoch: 1/2, step 3453/7134 completed (loss: 0.07387903332710266, acc: 0.9779286980628967)
[2025-02-13 02:52:51,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:51,989][root][INFO] - Training Epoch: 1/2, step 3454/7134 completed (loss: 0.06001739576458931, acc: 0.9858956336975098)
[2025-02-13 02:52:52,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:52,422][root][INFO] - Training Epoch: 1/2, step 3455/7134 completed (loss: 0.029088299721479416, acc: 0.9927536249160767)
[2025-02-13 02:52:52,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:52,860][root][INFO] - Training Epoch: 1/2, step 3456/7134 completed (loss: 0.04823901504278183, acc: 0.9917647242546082)
[2025-02-13 02:52:52,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:53,289][root][INFO] - Training Epoch: 1/2, step 3457/7134 completed (loss: 0.038888365030288696, acc: 0.988135576248169)
[2025-02-13 02:52:53,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:53,731][root][INFO] - Training Epoch: 1/2, step 3458/7134 completed (loss: 0.07638239115476608, acc: 0.9837618470191956)
[2025-02-13 02:52:53,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:54,133][root][INFO] - Training Epoch: 1/2, step 3459/7134 completed (loss: 0.05169881507754326, acc: 0.9878234267234802)
[2025-02-13 02:52:54,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:54,477][root][INFO] - Training Epoch: 1/2, step 3460/7134 completed (loss: 0.18525683879852295, acc: 0.9615384340286255)
[2025-02-13 02:52:54,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:54,774][root][INFO] - Training Epoch: 1/2, step 3461/7134 completed (loss: 0.23801231384277344, acc: 0.951724112033844)
[2025-02-13 02:52:54,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:55,170][root][INFO] - Training Epoch: 1/2, step 3462/7134 completed (loss: 0.10485837608575821, acc: 0.9748822450637817)
[2025-02-13 02:52:55,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:55,618][root][INFO] - Training Epoch: 1/2, step 3463/7134 completed (loss: 0.029652876779437065, acc: 0.9909502267837524)
[2025-02-13 02:52:55,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:56,044][root][INFO] - Training Epoch: 1/2, step 3464/7134 completed (loss: 0.06063991039991379, acc: 0.9772117733955383)
[2025-02-13 02:52:56,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:56,430][root][INFO] - Training Epoch: 1/2, step 3465/7134 completed (loss: 0.09413054585456848, acc: 0.9755600690841675)
[2025-02-13 02:52:56,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:56,826][root][INFO] - Training Epoch: 1/2, step 3466/7134 completed (loss: 0.027210356667637825, acc: 0.9904912710189819)
[2025-02-13 02:52:56,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:57,227][root][INFO] - Training Epoch: 1/2, step 3467/7134 completed (loss: 0.06826667487621307, acc: 0.987261176109314)
[2025-02-13 02:52:57,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:57,635][root][INFO] - Training Epoch: 1/2, step 3468/7134 completed (loss: 0.01601741649210453, acc: 0.9971056580543518)
[2025-02-13 02:52:57,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:58,040][root][INFO] - Training Epoch: 1/2, step 3469/7134 completed (loss: 0.06589747220277786, acc: 0.9754716753959656)
[2025-02-13 02:52:58,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:58,425][root][INFO] - Training Epoch: 1/2, step 3470/7134 completed (loss: 0.025018665939569473, acc: 0.9930875301361084)
[2025-02-13 02:52:58,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:58,818][root][INFO] - Training Epoch: 1/2, step 3471/7134 completed (loss: 0.08444678783416748, acc: 0.9861878156661987)
[2025-02-13 02:52:58,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:59,240][root][INFO] - Training Epoch: 1/2, step 3472/7134 completed (loss: 0.029562991112470627, acc: 0.992548406124115)
[2025-02-13 02:52:59,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:59,714][root][INFO] - Training Epoch: 1/2, step 3473/7134 completed (loss: 0.0466805174946785, acc: 0.9867549538612366)
[2025-02-13 02:52:59,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:00,093][root][INFO] - Training Epoch: 1/2, step 3474/7134 completed (loss: 0.058413516730070114, acc: 0.9858657121658325)
[2025-02-13 02:53:00,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:00,496][root][INFO] - Training Epoch: 1/2, step 3475/7134 completed (loss: 0.037619587033987045, acc: 0.9893428087234497)
[2025-02-13 02:53:00,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:00,912][root][INFO] - Training Epoch: 1/2, step 3476/7134 completed (loss: 0.07249592989683151, acc: 0.9794520735740662)
[2025-02-13 02:53:01,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:01,313][root][INFO] - Training Epoch: 1/2, step 3477/7134 completed (loss: 0.054757531732320786, acc: 0.9917762875556946)
[2025-02-13 02:53:01,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:01,715][root][INFO] - Training Epoch: 1/2, step 3478/7134 completed (loss: 0.0194509606808424, acc: 0.9955089688301086)
[2025-02-13 02:53:01,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:02,120][root][INFO] - Training Epoch: 1/2, step 3479/7134 completed (loss: 0.011073325760662556, acc: 0.998487114906311)
[2025-02-13 02:53:02,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:02,556][root][INFO] - Training Epoch: 1/2, step 3480/7134 completed (loss: 0.04206903278827667, acc: 0.9891008138656616)
[2025-02-13 02:53:02,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:02,960][root][INFO] - Training Epoch: 1/2, step 3481/7134 completed (loss: 0.027181630954146385, acc: 0.9946808218955994)
[2025-02-13 02:53:03,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:03,352][root][INFO] - Training Epoch: 1/2, step 3482/7134 completed (loss: 0.026811707764863968, acc: 0.9910072088241577)
[2025-02-13 02:53:03,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:03,774][root][INFO] - Training Epoch: 1/2, step 3483/7134 completed (loss: 0.01042621023952961, acc: 0.9969183206558228)
[2025-02-13 02:53:03,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:04,160][root][INFO] - Training Epoch: 1/2, step 3484/7134 completed (loss: 0.03246404603123665, acc: 0.991150438785553)
[2025-02-13 02:53:04,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:04,537][root][INFO] - Training Epoch: 1/2, step 3485/7134 completed (loss: 0.05275481194257736, acc: 0.9826839566230774)
[2025-02-13 02:53:04,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:04,945][root][INFO] - Training Epoch: 1/2, step 3486/7134 completed (loss: 0.02730075642466545, acc: 0.9910072088241577)
[2025-02-13 02:53:05,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:05,357][root][INFO] - Training Epoch: 1/2, step 3487/7134 completed (loss: 0.05200433358550072, acc: 0.9779086709022522)
[2025-02-13 02:53:05,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:05,773][root][INFO] - Training Epoch: 1/2, step 3488/7134 completed (loss: 0.038708947598934174, acc: 0.9901315569877625)
[2025-02-13 02:53:05,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:06,155][root][INFO] - Training Epoch: 1/2, step 3489/7134 completed (loss: 0.10362135618925095, acc: 0.9723756909370422)
[2025-02-13 02:53:06,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:06,603][root][INFO] - Training Epoch: 1/2, step 3490/7134 completed (loss: 0.09949275106191635, acc: 0.9793205261230469)
[2025-02-13 02:53:06,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:07,015][root][INFO] - Training Epoch: 1/2, step 3491/7134 completed (loss: 0.060381025075912476, acc: 0.9858155846595764)
[2025-02-13 02:53:07,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:07,364][root][INFO] - Training Epoch: 1/2, step 3492/7134 completed (loss: 0.03787795826792717, acc: 0.9897330403327942)
[2025-02-13 02:53:07,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:07,790][root][INFO] - Training Epoch: 1/2, step 3493/7134 completed (loss: 0.032155439257621765, acc: 0.9901960492134094)
[2025-02-13 02:53:07,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:08,170][root][INFO] - Training Epoch: 1/2, step 3494/7134 completed (loss: 0.040924180299043655, acc: 0.9913793206214905)
[2025-02-13 02:53:08,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:08,566][root][INFO] - Training Epoch: 1/2, step 3495/7134 completed (loss: 0.024468228220939636, acc: 0.9880095720291138)
[2025-02-13 02:53:08,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:08,945][root][INFO] - Training Epoch: 1/2, step 3496/7134 completed (loss: 0.03905148431658745, acc: 0.9887387156486511)
[2025-02-13 02:53:09,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:09,341][root][INFO] - Training Epoch: 1/2, step 3497/7134 completed (loss: 0.04520973190665245, acc: 0.9821958541870117)
[2025-02-13 02:53:09,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:09,762][root][INFO] - Training Epoch: 1/2, step 3498/7134 completed (loss: 0.04036881402134895, acc: 0.9862805008888245)
[2025-02-13 02:53:09,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:10,172][root][INFO] - Training Epoch: 1/2, step 3499/7134 completed (loss: 0.01862630806863308, acc: 0.9967585206031799)
[2025-02-13 02:53:10,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:10,583][root][INFO] - Training Epoch: 1/2, step 3500/7134 completed (loss: 0.0832945853471756, acc: 0.9765051603317261)
[2025-02-13 02:53:10,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:10,996][root][INFO] - Training Epoch: 1/2, step 3501/7134 completed (loss: 0.10650470852851868, acc: 0.9637826681137085)
[2025-02-13 02:53:11,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:11,383][root][INFO] - Training Epoch: 1/2, step 3502/7134 completed (loss: 0.11733101308345795, acc: 0.9684210419654846)
[2025-02-13 02:53:11,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:11,797][root][INFO] - Training Epoch: 1/2, step 3503/7134 completed (loss: 0.06895142793655396, acc: 0.984402060508728)
[2025-02-13 02:53:11,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:12,208][root][INFO] - Training Epoch: 1/2, step 3504/7134 completed (loss: 0.02201869525015354, acc: 0.9954751133918762)
[2025-02-13 02:53:12,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:12,626][root][INFO] - Training Epoch: 1/2, step 3505/7134 completed (loss: 0.03284181281924248, acc: 0.9946332573890686)
[2025-02-13 02:53:12,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:12,991][root][INFO] - Training Epoch: 1/2, step 3506/7134 completed (loss: 0.04751824587583542, acc: 0.9848771095275879)
[2025-02-13 02:53:13,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:13,414][root][INFO] - Training Epoch: 1/2, step 3507/7134 completed (loss: 0.0160084031522274, acc: 0.9933775067329407)
[2025-02-13 02:53:13,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:13,829][root][INFO] - Training Epoch: 1/2, step 3508/7134 completed (loss: 0.029530957341194153, acc: 0.9926900863647461)
[2025-02-13 02:53:13,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:14,252][root][INFO] - Training Epoch: 1/2, step 3509/7134 completed (loss: 0.03226916119456291, acc: 0.988950252532959)
[2025-02-13 02:53:14,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:14,628][root][INFO] - Training Epoch: 1/2, step 3510/7134 completed (loss: 0.05830981209874153, acc: 0.9845626354217529)
[2025-02-13 02:53:14,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:15,027][root][INFO] - Training Epoch: 1/2, step 3511/7134 completed (loss: 0.014442658983170986, acc: 0.9963964223861694)
[2025-02-13 02:53:15,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:15,426][root][INFO] - Training Epoch: 1/2, step 3512/7134 completed (loss: 0.04598354920744896, acc: 0.9841521382331848)
[2025-02-13 02:53:15,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:15,837][root][INFO] - Training Epoch: 1/2, step 3513/7134 completed (loss: 0.044137321412563324, acc: 0.9893805384635925)
[2025-02-13 02:53:15,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:16,284][root][INFO] - Training Epoch: 1/2, step 3514/7134 completed (loss: 0.04182438924908638, acc: 0.9930875301361084)
[2025-02-13 02:53:16,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:16,690][root][INFO] - Training Epoch: 1/2, step 3515/7134 completed (loss: 0.027105553075671196, acc: 0.9916201233863831)
[2025-02-13 02:53:16,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:17,147][root][INFO] - Training Epoch: 1/2, step 3516/7134 completed (loss: 0.02319939434528351, acc: 0.9945828914642334)
[2025-02-13 02:53:17,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:17,590][root][INFO] - Training Epoch: 1/2, step 3517/7134 completed (loss: 0.037446774542331696, acc: 0.9916567206382751)
[2025-02-13 02:53:17,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:18,032][root][INFO] - Training Epoch: 1/2, step 3518/7134 completed (loss: 0.03981838375329971, acc: 0.9870610237121582)
[2025-02-13 02:53:18,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:18,474][root][INFO] - Training Epoch: 1/2, step 3519/7134 completed (loss: 0.03417579457163811, acc: 0.9921348094940186)
[2025-02-13 02:53:18,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:18,924][root][INFO] - Training Epoch: 1/2, step 3520/7134 completed (loss: 0.09302235394716263, acc: 0.981176495552063)
[2025-02-13 02:53:19,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:19,361][root][INFO] - Training Epoch: 1/2, step 3521/7134 completed (loss: 0.028282681480050087, acc: 0.9900426864624023)
[2025-02-13 02:53:19,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:19,744][root][INFO] - Training Epoch: 1/2, step 3522/7134 completed (loss: 0.04914878308773041, acc: 0.9866310358047485)
[2025-02-13 02:53:19,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:20,181][root][INFO] - Training Epoch: 1/2, step 3523/7134 completed (loss: 0.01686832308769226, acc: 0.9969183206558228)
[2025-02-13 02:53:20,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:20,620][root][INFO] - Training Epoch: 1/2, step 3524/7134 completed (loss: 0.06670469790697098, acc: 0.9830729365348816)
[2025-02-13 02:53:20,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:21,052][root][INFO] - Training Epoch: 1/2, step 3525/7134 completed (loss: 0.02433675341308117, acc: 0.9934102296829224)
[2025-02-13 02:53:21,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:21,486][root][INFO] - Training Epoch: 1/2, step 3526/7134 completed (loss: 0.052377697080373764, acc: 0.9873257279396057)
[2025-02-13 02:53:21,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:21,944][root][INFO] - Training Epoch: 1/2, step 3527/7134 completed (loss: 0.04461197182536125, acc: 0.98591548204422)
[2025-02-13 02:53:22,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:22,390][root][INFO] - Training Epoch: 1/2, step 3528/7134 completed (loss: 0.016980130225419998, acc: 0.9965397715568542)
[2025-02-13 02:53:22,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:22,811][root][INFO] - Training Epoch: 1/2, step 3529/7134 completed (loss: 0.019468188285827637, acc: 0.9952940940856934)
[2025-02-13 02:53:22,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:23,277][root][INFO] - Training Epoch: 1/2, step 3530/7134 completed (loss: 0.028477856889367104, acc: 0.9885321259498596)
[2025-02-13 02:53:23,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:23,711][root][INFO] - Training Epoch: 1/2, step 3531/7134 completed (loss: 0.03679652512073517, acc: 0.9849397540092468)
[2025-02-13 02:53:23,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:24,152][root][INFO] - Training Epoch: 1/2, step 3532/7134 completed (loss: 0.014218193478882313, acc: 0.9949495196342468)
[2025-02-13 02:53:24,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:24,588][root][INFO] - Training Epoch: 1/2, step 3533/7134 completed (loss: 0.02448258362710476, acc: 0.9953434467315674)
[2025-02-13 02:53:24,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:25,012][root][INFO] - Training Epoch: 1/2, step 3534/7134 completed (loss: 0.03959580138325691, acc: 0.9888734221458435)
[2025-02-13 02:53:25,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:25,431][root][INFO] - Training Epoch: 1/2, step 3535/7134 completed (loss: 0.035977818071842194, acc: 0.9892473220825195)
[2025-02-13 02:53:25,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:25,855][root][INFO] - Training Epoch: 1/2, step 3536/7134 completed (loss: 0.06717849522829056, acc: 0.9836065769195557)
[2025-02-13 02:53:25,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:26,247][root][INFO] - Training Epoch: 1/2, step 3537/7134 completed (loss: 0.04012472182512283, acc: 0.9899328947067261)
[2025-02-13 02:53:26,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:26,634][root][INFO] - Training Epoch: 1/2, step 3538/7134 completed (loss: 0.05328286439180374, acc: 0.9923954606056213)
[2025-02-13 02:53:26,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:26,955][root][INFO] - Training Epoch: 1/2, step 3539/7134 completed (loss: 0.05628732964396477, acc: 0.9805825352668762)
[2025-02-13 02:53:27,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:27,341][root][INFO] - Training Epoch: 1/2, step 3540/7134 completed (loss: 0.06380140036344528, acc: 0.9772382378578186)
[2025-02-13 02:53:27,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:27,757][root][INFO] - Training Epoch: 1/2, step 3541/7134 completed (loss: 0.0605417899787426, acc: 0.9809104204177856)
[2025-02-13 02:53:27,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:28,167][root][INFO] - Training Epoch: 1/2, step 3542/7134 completed (loss: 0.04430296644568443, acc: 0.9830769300460815)
[2025-02-13 02:53:28,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:28,550][root][INFO] - Training Epoch: 1/2, step 3543/7134 completed (loss: 0.07150966674089432, acc: 0.9771126508712769)
[2025-02-13 02:53:28,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:28,984][root][INFO] - Training Epoch: 1/2, step 3544/7134 completed (loss: 0.04853329807519913, acc: 0.9846153855323792)
[2025-02-13 02:53:29,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:29,429][root][INFO] - Training Epoch: 1/2, step 3545/7134 completed (loss: 0.03723033517599106, acc: 0.9906832575798035)
[2025-02-13 02:53:29,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:29,842][root][INFO] - Training Epoch: 1/2, step 3546/7134 completed (loss: 0.05508837103843689, acc: 0.9851484894752502)
[2025-02-13 02:53:29,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:30,290][root][INFO] - Training Epoch: 1/2, step 3547/7134 completed (loss: 0.07152824848890305, acc: 0.9790209531784058)
[2025-02-13 02:53:30,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:30,716][root][INFO] - Training Epoch: 1/2, step 3548/7134 completed (loss: 0.05243685469031334, acc: 0.980088472366333)
[2025-02-13 02:53:30,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:31,119][root][INFO] - Training Epoch: 1/2, step 3549/7134 completed (loss: 0.07160061597824097, acc: 0.971781313419342)
[2025-02-13 02:53:31,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:31,520][root][INFO] - Training Epoch: 1/2, step 3550/7134 completed (loss: 0.043397143483161926, acc: 0.9824868440628052)
[2025-02-13 02:53:31,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:31,921][root][INFO] - Training Epoch: 1/2, step 3551/7134 completed (loss: 0.03807375580072403, acc: 0.9865319728851318)
[2025-02-13 02:53:32,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:32,349][root][INFO] - Training Epoch: 1/2, step 3552/7134 completed (loss: 0.1470404863357544, acc: 0.9569288492202759)
[2025-02-13 02:53:32,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:32,748][root][INFO] - Training Epoch: 1/2, step 3553/7134 completed (loss: 0.08546306937932968, acc: 0.9819079041481018)
[2025-02-13 02:53:32,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:33,155][root][INFO] - Training Epoch: 1/2, step 3554/7134 completed (loss: 0.06472164392471313, acc: 0.9864457845687866)
[2025-02-13 02:53:33,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:33,547][root][INFO] - Training Epoch: 1/2, step 3555/7134 completed (loss: 0.04184548184275627, acc: 0.9858712553977966)
[2025-02-13 02:53:33,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:33,922][root][INFO] - Training Epoch: 1/2, step 3556/7134 completed (loss: 0.047817427664995193, acc: 0.9865642786026001)
[2025-02-13 02:53:34,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:34,359][root][INFO] - Training Epoch: 1/2, step 3557/7134 completed (loss: 0.06108615919947624, acc: 0.9854809641838074)
[2025-02-13 02:53:34,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:34,760][root][INFO] - Training Epoch: 1/2, step 3558/7134 completed (loss: 0.09618311375379562, acc: 0.9793814420700073)
[2025-02-13 02:53:34,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:35,169][root][INFO] - Training Epoch: 1/2, step 3559/7134 completed (loss: 0.031521543860435486, acc: 0.9883720874786377)
[2025-02-13 02:53:35,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:35,580][root][INFO] - Training Epoch: 1/2, step 3560/7134 completed (loss: 0.02684570476412773, acc: 0.9931507110595703)
[2025-02-13 02:53:35,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:36,006][root][INFO] - Training Epoch: 1/2, step 3561/7134 completed (loss: 0.04989643767476082, acc: 0.9859943985939026)
[2025-02-13 02:53:36,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:36,401][root][INFO] - Training Epoch: 1/2, step 3562/7134 completed (loss: 0.0478706881403923, acc: 0.9821428656578064)
[2025-02-13 02:53:36,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:36,804][root][INFO] - Training Epoch: 1/2, step 3563/7134 completed (loss: 0.05384189262986183, acc: 0.9872813820838928)
[2025-02-13 02:53:36,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:37,208][root][INFO] - Training Epoch: 1/2, step 3564/7134 completed (loss: 0.0768338143825531, acc: 0.9774436354637146)
[2025-02-13 02:53:37,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:37,611][root][INFO] - Training Epoch: 1/2, step 3565/7134 completed (loss: 0.09972067177295685, acc: 0.9661654233932495)
[2025-02-13 02:53:38,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:38,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:38,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:39,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:39,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:39,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:40,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:40,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:40,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:41,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:41,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:41,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:42,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:42,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:42,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:43,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:43,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:43,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:44,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:44,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:44,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:45,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:45,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:45,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:46,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:46,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:46,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:47,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:47,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:48,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:48,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:48,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:49,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:49,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:49,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:50,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:50,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:50,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:51,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:51,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:51,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:52,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:52,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:52,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:53,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:53,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:54,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:54,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:54,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:55,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:55,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:56,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:56,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:56,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:57,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:57,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:57,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:58,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:58,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:58,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:59,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:59,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:59,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:00,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:00,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:00,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:01,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:01,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:01,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:02,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:02,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:02,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:03,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:03,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:03,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:04,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:04,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:04,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:05,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:05,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:05,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:06,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:06,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:06,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:07,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:07,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:07,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:08,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:08,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:08,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:09,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:09,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:09,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:10,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:10,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:10,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:11,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:11,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:11,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:12,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:12,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:12,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:13,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:13,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:13,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:14,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:14,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:14,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:15,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:15,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:16,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:16,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:16,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:17,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:17,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:17,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:18,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:18,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:18,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:19,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:19,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:19,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:20,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:20,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:20,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:21,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:21,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:21,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:22,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:22,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:22,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:23,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:23,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:23,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:24,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:24,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:24,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:25,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:25,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:25,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:26,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:26,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:26,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:27,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:27,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:28,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:28,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:28,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:29,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:29,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:29,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:30,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:30,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:30,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:31,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:31,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:31,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:32,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:32,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:32,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:33,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:33,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:33,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:34,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:34,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:34,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:35,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:35,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:35,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:36,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:36,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:36,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:37,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:37,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:38,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:38,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:38,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:40,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:40,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:41,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:41,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:41,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:41,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:42,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:42,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:42,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:43,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:43,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:44,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:44,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:44,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:45,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:45,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:45,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:45,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:46,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:46,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:46,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:47,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:47,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:47,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:48,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:48,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:48,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:49,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:49,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:49,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:50,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:50,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:51,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:51,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:51,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:52,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:52,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:52,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:53,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:53,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:53,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:54,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:54,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:54,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:55,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:55,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:55,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:56,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:56,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:56,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:57,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:57,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:57,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:58,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:58,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:58,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:59,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:59,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:59,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:00,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:00,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:00,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:01,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:01,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:01,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:02,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:02,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:03,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:03,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:03,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:04,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:04,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:04,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:05,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:05,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:06,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:06,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:06,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:07,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:07,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:07,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:08,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:08,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:08,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:09,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:09,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:09,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:10,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:10,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:10,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:11,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:11,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:12,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:12,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:12,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:13,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:13,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:14,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:14,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:14,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:15,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:15,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:16,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:16,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:16,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:17,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:17,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:17,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:18,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:18,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:19,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:19,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:19,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:20,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:20,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:21,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:21,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:21,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:22,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:22,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:22,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:23,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:23,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:23,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:24,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:24,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:24,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:25,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:25,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:25,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:26,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:26,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:26,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:27,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:27,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:27,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:28,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:28,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:29,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:29,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:29,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:30,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:30,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:30,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:31,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:31,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:32,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:32,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:32,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:33,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:33,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:33,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:34,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:34,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:34,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:35,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:35,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:36,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:36,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:36,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:37,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:37,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:38,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:38,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:38,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:39,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:39,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:40,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:40,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:40,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:40,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:41,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:41,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:41,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:42,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:42,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:42,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:43,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:43,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:44,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:44,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:45,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:46,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:46,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:47,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:47,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:47,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:48,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:48,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:48,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:49,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:49,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:49,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:49,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:50,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:50,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:50,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:51,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:51,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:51,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:52,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:52,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:52,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:53,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:53,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:53,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:54,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:54,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:54,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:55,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:55,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:55,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:56,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:56,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:56,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:57,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:57,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:57,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:58,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:58,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:58,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:59,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:59,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:59,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:00,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:00,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:00,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:00,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:01,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:01,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:01,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:02,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:02,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:02,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:03,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:03,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:03,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:04,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:04,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:04,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:05,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:05,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:05,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:06,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:06,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:06,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:07,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:07,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:07,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:08,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:08,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:08,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:09,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:09,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:09,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:10,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:10,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:10,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:11,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:11,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:11,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:12,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:12,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:12,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:13,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:13,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:13,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:14,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:14,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:14,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:15,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:15,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:15,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:16,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:16,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:16,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:17,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:17,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:17,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:18,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:18,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:18,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:19,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:19,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:19,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:20,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:20,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:20,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:21,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:21,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:21,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:22,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:22,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:22,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:23,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:23,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:23,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:23,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:24,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:24,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:25,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:25,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:25,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:26,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:26,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:26,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:27,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:27,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:27,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:28,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:28,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:29,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:29,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:29,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:30,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:30,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:30,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:31,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:31,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:31,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:32,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:32,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:33,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:33,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:33,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:34,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:34,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:34,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:35,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:35,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:35,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:36,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:36,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:36,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:37,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:37,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:37,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:38,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:38,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:38,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:39,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:39,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:39,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:40,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:40,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:40,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:41,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:41,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:41,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:42,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:42,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:42,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:43,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:43,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:43,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:44,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:44,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:44,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:45,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:45,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:45,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:46,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:46,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:46,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:47,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:47,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:47,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:48,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:48,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:48,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:49,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:49,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:50,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:50,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:50,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:51,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:51,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:51,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:52,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:52,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:52,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:53,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:53,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:53,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:54,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:54,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:54,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:55,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:55,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:55,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:56,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:56,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:56,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:57,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:57,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:57,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:58,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:58,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:59,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:59,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:00,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:00,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:00,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:01,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:01,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:01,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:02,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:02,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:02,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:03,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:03,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:03,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:04,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:04,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:04,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:05,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:05,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:05,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:06,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:06,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:06,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:07,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:07,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:07,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:08,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:08,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:08,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:09,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:09,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:09,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:10,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:10,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:10,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:11,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:11,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:11,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:12,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:12,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:12,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:13,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:13,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:14,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:14,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:14,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:15,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:16,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:17,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:17,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:17,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:18,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:18,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:19,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:19,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:19,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:20,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:20,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:21,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:21,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:21,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:22,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:22,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:23,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:23,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:23,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:24,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:24,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:24,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:25,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:25,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:26,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:26,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:26,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:27,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:27,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:27,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:28,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:28,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:28,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:29,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:29,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:29,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:30,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:30,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:30,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:31,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:31,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:31,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:32,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:32,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:32,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:33,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:33,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:34,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:34,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:35,076][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0706, device='cuda:0') eval_epoch_loss=tensor(0.0682, device='cuda:0') eval_epoch_acc=tensor(0.9810, device='cuda:0')
[2025-02-13 02:57:35,077][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 02:57:35,078][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 02:57:35,323][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_3566_loss_0.06821627169847488/model.pt
[2025-02-13 02:57:35,330][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 02:57:35,331][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.06821627169847488
[2025-02-13 02:57:35,332][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.980992317199707
[2025-02-13 02:57:35,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:35,745][root][INFO] - Training Epoch: 1/2, step 3566/7134 completed (loss: 0.08537968248128891, acc: 0.9681274890899658)
[2025-02-13 02:57:35,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:36,152][root][INFO] - Training Epoch: 1/2, step 3567/7134 completed (loss: 0.10528469830751419, acc: 0.9805951118469238)
[2025-02-13 02:57:36,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:36,566][root][INFO] - Training Epoch: 1/2, step 3568/7134 completed (loss: 0.10556302219629288, acc: 0.9803600907325745)
[2025-02-13 02:57:36,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:36,912][root][INFO] - Training Epoch: 1/2, step 3569/7134 completed (loss: 0.07967160642147064, acc: 0.9820788502693176)
[2025-02-13 02:57:37,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:37,337][root][INFO] - Training Epoch: 1/2, step 3570/7134 completed (loss: 0.08984451740980148, acc: 0.9785124063491821)
[2025-02-13 02:57:37,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:37,750][root][INFO] - Training Epoch: 1/2, step 3571/7134 completed (loss: 0.0783318355679512, acc: 0.9854111671447754)
[2025-02-13 02:57:37,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:38,199][root][INFO] - Training Epoch: 1/2, step 3572/7134 completed (loss: 0.044288262724876404, acc: 0.9881423115730286)
[2025-02-13 02:57:38,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:38,628][root][INFO] - Training Epoch: 1/2, step 3573/7134 completed (loss: 0.07635103166103363, acc: 0.9777777791023254)
[2025-02-13 02:57:38,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:39,022][root][INFO] - Training Epoch: 1/2, step 3574/7134 completed (loss: 0.11173631250858307, acc: 0.9578543901443481)
[2025-02-13 02:57:39,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:39,426][root][INFO] - Training Epoch: 1/2, step 3575/7134 completed (loss: 0.1379968374967575, acc: 0.9639794230461121)
[2025-02-13 02:57:39,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:39,828][root][INFO] - Training Epoch: 1/2, step 3576/7134 completed (loss: 0.1344069540500641, acc: 0.9650092124938965)
[2025-02-13 02:57:39,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:40,300][root][INFO] - Training Epoch: 1/2, step 3577/7134 completed (loss: 0.08016601949930191, acc: 0.9734513163566589)
[2025-02-13 02:57:40,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:40,733][root][INFO] - Training Epoch: 1/2, step 3578/7134 completed (loss: 0.054435890167951584, acc: 0.976710319519043)
[2025-02-13 02:57:40,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:41,151][root][INFO] - Training Epoch: 1/2, step 3579/7134 completed (loss: 0.0631067082285881, acc: 0.9826666712760925)
[2025-02-13 02:57:41,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:41,562][root][INFO] - Training Epoch: 1/2, step 3580/7134 completed (loss: 0.11031627655029297, acc: 0.9559054970741272)
[2025-02-13 02:57:41,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:41,980][root][INFO] - Training Epoch: 1/2, step 3581/7134 completed (loss: 0.10121535509824753, acc: 0.9675993919372559)
[2025-02-13 02:57:42,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:42,338][root][INFO] - Training Epoch: 1/2, step 3582/7134 completed (loss: 0.06563553959131241, acc: 0.9810426831245422)
[2025-02-13 02:57:42,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:42,772][root][INFO] - Training Epoch: 1/2, step 3583/7134 completed (loss: 0.08393928408622742, acc: 0.9772036671638489)
[2025-02-13 02:57:42,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:43,125][root][INFO] - Training Epoch: 1/2, step 3584/7134 completed (loss: 0.08757317066192627, acc: 0.9894179701805115)
[2025-02-13 02:57:43,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:43,544][root][INFO] - Training Epoch: 1/2, step 3585/7134 completed (loss: 0.07544617354869843, acc: 0.9722222089767456)
[2025-02-13 02:57:43,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:43,945][root][INFO] - Training Epoch: 1/2, step 3586/7134 completed (loss: 0.08033118396997452, acc: 0.9743177890777588)
[2025-02-13 02:57:44,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:44,351][root][INFO] - Training Epoch: 1/2, step 3587/7134 completed (loss: 0.06037154048681259, acc: 0.9830795526504517)
[2025-02-13 02:57:44,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:44,797][root][INFO] - Training Epoch: 1/2, step 3588/7134 completed (loss: 0.10496637225151062, acc: 0.976331353187561)
[2025-02-13 02:57:44,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:45,219][root][INFO] - Training Epoch: 1/2, step 3589/7134 completed (loss: 0.07653158158063889, acc: 0.9848713874816895)
[2025-02-13 02:57:45,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:45,654][root][INFO] - Training Epoch: 1/2, step 3590/7134 completed (loss: 0.047598015516996384, acc: 0.983146071434021)
[2025-02-13 02:57:45,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:46,038][root][INFO] - Training Epoch: 1/2, step 3591/7134 completed (loss: 0.04957710579037666, acc: 0.9879879951477051)
[2025-02-13 02:57:46,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:46,442][root][INFO] - Training Epoch: 1/2, step 3592/7134 completed (loss: 0.028292739763855934, acc: 0.9933884143829346)
[2025-02-13 02:57:46,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:46,875][root][INFO] - Training Epoch: 1/2, step 3593/7134 completed (loss: 0.07418064773082733, acc: 0.9842382073402405)
[2025-02-13 02:57:47,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:47,284][root][INFO] - Training Epoch: 1/2, step 3594/7134 completed (loss: 0.04144833981990814, acc: 0.9886363744735718)
[2025-02-13 02:57:47,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:47,711][root][INFO] - Training Epoch: 1/2, step 3595/7134 completed (loss: 0.06382565945386887, acc: 0.9785605072975159)
[2025-02-13 02:57:47,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:48,124][root][INFO] - Training Epoch: 1/2, step 3596/7134 completed (loss: 0.05993245542049408, acc: 0.9834938049316406)
[2025-02-13 02:57:48,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:48,567][root][INFO] - Training Epoch: 1/2, step 3597/7134 completed (loss: 0.048990242183208466, acc: 0.9870503544807434)
[2025-02-13 02:57:48,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:49,046][root][INFO] - Training Epoch: 1/2, step 3598/7134 completed (loss: 0.06080111488699913, acc: 0.9794050455093384)
[2025-02-13 02:57:49,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:49,485][root][INFO] - Training Epoch: 1/2, step 3599/7134 completed (loss: 0.09265528619289398, acc: 0.9760403633117676)
[2025-02-13 02:57:49,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:49,940][root][INFO] - Training Epoch: 1/2, step 3600/7134 completed (loss: 0.06421954929828644, acc: 0.9876543283462524)
[2025-02-13 02:57:50,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:50,376][root][INFO] - Training Epoch: 1/2, step 3601/7134 completed (loss: 0.156468003988266, acc: 0.9613792896270752)
[2025-02-13 02:57:50,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:50,829][root][INFO] - Training Epoch: 1/2, step 3602/7134 completed (loss: 0.12593737244606018, acc: 0.9708284735679626)
[2025-02-13 02:57:50,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:51,229][root][INFO] - Training Epoch: 1/2, step 3603/7134 completed (loss: 0.0426853783428669, acc: 0.987500011920929)
[2025-02-13 02:57:51,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:51,616][root][INFO] - Training Epoch: 1/2, step 3604/7134 completed (loss: 0.05845976248383522, acc: 0.9812382459640503)
[2025-02-13 02:57:51,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:52,073][root][INFO] - Training Epoch: 1/2, step 3605/7134 completed (loss: 0.08683203160762787, acc: 0.9741200804710388)
[2025-02-13 02:57:52,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:52,507][root][INFO] - Training Epoch: 1/2, step 3606/7134 completed (loss: 0.07510124146938324, acc: 0.9822379946708679)
[2025-02-13 02:57:52,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:52,939][root][INFO] - Training Epoch: 1/2, step 3607/7134 completed (loss: 0.04983721300959587, acc: 0.9798657894134521)
[2025-02-13 02:57:53,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:53,392][root][INFO] - Training Epoch: 1/2, step 3608/7134 completed (loss: 0.11759304255247116, acc: 0.9745575189590454)
[2025-02-13 02:57:53,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:53,847][root][INFO] - Training Epoch: 1/2, step 3609/7134 completed (loss: 0.0560680590569973, acc: 0.9847715497016907)
[2025-02-13 02:57:53,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:54,292][root][INFO] - Training Epoch: 1/2, step 3610/7134 completed (loss: 0.11823046952486038, acc: 0.9610389471054077)
[2025-02-13 02:57:54,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:54,734][root][INFO] - Training Epoch: 1/2, step 3611/7134 completed (loss: 0.09195351600646973, acc: 0.975806474685669)
[2025-02-13 02:57:54,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:55,205][root][INFO] - Training Epoch: 1/2, step 3612/7134 completed (loss: 0.13727524876594543, acc: 0.9688796401023865)
[2025-02-13 02:57:55,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:55,659][root][INFO] - Training Epoch: 1/2, step 3613/7134 completed (loss: 0.08864881098270416, acc: 0.9720101952552795)
[2025-02-13 02:57:55,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:56,072][root][INFO] - Training Epoch: 1/2, step 3614/7134 completed (loss: 0.07000015676021576, acc: 0.9718875288963318)
[2025-02-13 02:57:56,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:56,507][root][INFO] - Training Epoch: 1/2, step 3615/7134 completed (loss: 0.036339085549116135, acc: 0.9877675771713257)
[2025-02-13 02:57:56,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:56,927][root][INFO] - Training Epoch: 1/2, step 3616/7134 completed (loss: 0.06435280293226242, acc: 0.9795918464660645)
[2025-02-13 02:57:57,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:57,380][root][INFO] - Training Epoch: 1/2, step 3617/7134 completed (loss: 0.07804417610168457, acc: 0.9764851331710815)
[2025-02-13 02:57:57,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:57,782][root][INFO] - Training Epoch: 1/2, step 3618/7134 completed (loss: 0.1564219444990158, acc: 0.9493464231491089)
[2025-02-13 02:57:57,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:58,227][root][INFO] - Training Epoch: 1/2, step 3619/7134 completed (loss: 0.1143123060464859, acc: 0.961904764175415)
[2025-02-13 02:57:58,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:58,689][root][INFO] - Training Epoch: 1/2, step 3620/7134 completed (loss: 0.0826425552368164, acc: 0.9690576791763306)
[2025-02-13 02:57:58,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:59,100][root][INFO] - Training Epoch: 1/2, step 3621/7134 completed (loss: 0.05968885496258736, acc: 0.9819694757461548)
[2025-02-13 02:57:59,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:59,549][root][INFO] - Training Epoch: 1/2, step 3622/7134 completed (loss: 0.04970121383666992, acc: 0.9819819927215576)
[2025-02-13 02:57:59,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:59,890][root][INFO] - Training Epoch: 1/2, step 3623/7134 completed (loss: 0.04486257955431938, acc: 0.9851301312446594)
[2025-02-13 02:58:00,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:00,310][root][INFO] - Training Epoch: 1/2, step 3624/7134 completed (loss: 0.07253853231668472, acc: 0.9783333539962769)
[2025-02-13 02:58:00,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:00,674][root][INFO] - Training Epoch: 1/2, step 3625/7134 completed (loss: 0.07395526766777039, acc: 0.9778671860694885)
[2025-02-13 02:58:00,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:01,044][root][INFO] - Training Epoch: 1/2, step 3626/7134 completed (loss: 0.2268841713666916, acc: 0.9466666579246521)
[2025-02-13 02:58:01,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:01,441][root][INFO] - Training Epoch: 1/2, step 3627/7134 completed (loss: 0.13577406108379364, acc: 0.9692671298980713)
[2025-02-13 02:58:01,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:01,808][root][INFO] - Training Epoch: 1/2, step 3628/7134 completed (loss: 0.16050422191619873, acc: 0.9659090638160706)
[2025-02-13 02:58:01,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:02,147][root][INFO] - Training Epoch: 1/2, step 3629/7134 completed (loss: 0.15526588261127472, acc: 0.957317054271698)
[2025-02-13 02:58:02,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:02,492][root][INFO] - Training Epoch: 1/2, step 3630/7134 completed (loss: 0.1259458363056183, acc: 0.9638242721557617)
[2025-02-13 02:58:02,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:02,903][root][INFO] - Training Epoch: 1/2, step 3631/7134 completed (loss: 0.12253441661596298, acc: 0.9672130942344666)
[2025-02-13 02:58:03,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:03,311][root][INFO] - Training Epoch: 1/2, step 3632/7134 completed (loss: 0.10078484565019608, acc: 0.9758551120758057)
[2025-02-13 02:58:03,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:03,714][root][INFO] - Training Epoch: 1/2, step 3633/7134 completed (loss: 0.10836393386125565, acc: 0.9784792065620422)
[2025-02-13 02:58:03,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:04,104][root][INFO] - Training Epoch: 1/2, step 3634/7134 completed (loss: 0.12105415761470795, acc: 0.97773277759552)
[2025-02-13 02:58:04,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:04,505][root][INFO] - Training Epoch: 1/2, step 3635/7134 completed (loss: 0.049485787749290466, acc: 0.9856630563735962)
[2025-02-13 02:58:04,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:04,895][root][INFO] - Training Epoch: 1/2, step 3636/7134 completed (loss: 0.05231757462024689, acc: 0.9862204790115356)
[2025-02-13 02:58:05,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:05,311][root][INFO] - Training Epoch: 1/2, step 3637/7134 completed (loss: 0.058816805481910706, acc: 0.98591548204422)
[2025-02-13 02:58:05,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:05,698][root][INFO] - Training Epoch: 1/2, step 3638/7134 completed (loss: 0.05308759957551956, acc: 0.9788235425949097)
[2025-02-13 02:58:05,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:06,095][root][INFO] - Training Epoch: 1/2, step 3639/7134 completed (loss: 0.04236428067088127, acc: 0.9864864945411682)
[2025-02-13 02:58:06,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:06,513][root][INFO] - Training Epoch: 1/2, step 3640/7134 completed (loss: 0.054972946643829346, acc: 0.982824444770813)
[2025-02-13 02:58:06,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:06,865][root][INFO] - Training Epoch: 1/2, step 3641/7134 completed (loss: 0.0456508994102478, acc: 0.9883268475532532)
[2025-02-13 02:58:07,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:07,281][root][INFO] - Training Epoch: 1/2, step 3642/7134 completed (loss: 0.04303893819451332, acc: 0.9907264113426208)
[2025-02-13 02:58:07,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:07,687][root][INFO] - Training Epoch: 1/2, step 3643/7134 completed (loss: 0.04599545896053314, acc: 0.9896907210350037)
[2025-02-13 02:58:07,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:08,095][root][INFO] - Training Epoch: 1/2, step 3644/7134 completed (loss: 0.06866943836212158, acc: 0.9821138381958008)
[2025-02-13 02:58:08,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:08,520][root][INFO] - Training Epoch: 1/2, step 3645/7134 completed (loss: 0.09290286153554916, acc: 0.9823943376541138)
[2025-02-13 02:58:08,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:08,911][root][INFO] - Training Epoch: 1/2, step 3646/7134 completed (loss: 0.14831538498401642, acc: 0.9652777910232544)
[2025-02-13 02:58:09,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:09,306][root][INFO] - Training Epoch: 1/2, step 3647/7134 completed (loss: 0.029061218723654747, acc: 0.9922480583190918)
[2025-02-13 02:58:09,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:09,751][root][INFO] - Training Epoch: 1/2, step 3648/7134 completed (loss: 0.10367745161056519, acc: 0.9711934328079224)
[2025-02-13 02:58:09,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:10,179][root][INFO] - Training Epoch: 1/2, step 3649/7134 completed (loss: 0.12411519140005112, acc: 0.970588207244873)
[2025-02-13 02:58:10,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:10,622][root][INFO] - Training Epoch: 1/2, step 3650/7134 completed (loss: 0.09173690527677536, acc: 0.97124183177948)
[2025-02-13 02:58:10,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:11,060][root][INFO] - Training Epoch: 1/2, step 3651/7134 completed (loss: 0.09029704332351685, acc: 0.9772117733955383)
[2025-02-13 02:58:11,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:11,467][root][INFO] - Training Epoch: 1/2, step 3652/7134 completed (loss: 0.08849649876356125, acc: 0.9758672714233398)
[2025-02-13 02:58:11,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:11,888][root][INFO] - Training Epoch: 1/2, step 3653/7134 completed (loss: 0.11226597428321838, acc: 0.9698340892791748)
[2025-02-13 02:58:12,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:12,303][root][INFO] - Training Epoch: 1/2, step 3654/7134 completed (loss: 0.0356975682079792, acc: 0.9930915236473083)
[2025-02-13 02:58:12,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:12,721][root][INFO] - Training Epoch: 1/2, step 3655/7134 completed (loss: 0.07055700570344925, acc: 0.9821428656578064)
[2025-02-13 02:58:12,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:13,073][root][INFO] - Training Epoch: 1/2, step 3656/7134 completed (loss: 0.10441315919160843, acc: 0.9709091186523438)
[2025-02-13 02:58:13,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:13,486][root][INFO] - Training Epoch: 1/2, step 3657/7134 completed (loss: 0.025786494836211205, acc: 0.9919871687889099)
[2025-02-13 02:58:13,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:13,902][root][INFO] - Training Epoch: 1/2, step 3658/7134 completed (loss: 0.06262364983558655, acc: 0.9844236969947815)
[2025-02-13 02:58:14,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:14,316][root][INFO] - Training Epoch: 1/2, step 3659/7134 completed (loss: 0.04431238770484924, acc: 0.9850522875785828)
[2025-02-13 02:58:14,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:14,573][root][INFO] - Training Epoch: 1/2, step 3660/7134 completed (loss: 0.08040158450603485, acc: 0.980327844619751)
[2025-02-13 02:58:14,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:14,831][root][INFO] - Training Epoch: 1/2, step 3661/7134 completed (loss: 0.05278356745839119, acc: 0.9857142567634583)
[2025-02-13 02:58:14,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:15,278][root][INFO] - Training Epoch: 1/2, step 3662/7134 completed (loss: 0.10318339616060257, acc: 0.9756097793579102)
[2025-02-13 02:58:15,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:15,686][root][INFO] - Training Epoch: 1/2, step 3663/7134 completed (loss: 0.06764334440231323, acc: 0.9835164546966553)
[2025-02-13 02:58:15,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:16,102][root][INFO] - Training Epoch: 1/2, step 3664/7134 completed (loss: 0.07788874953985214, acc: 0.9908814430236816)
[2025-02-13 02:58:16,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:16,511][root][INFO] - Training Epoch: 1/2, step 3665/7134 completed (loss: 0.049994081258773804, acc: 0.9868637323379517)
[2025-02-13 02:58:16,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:16,912][root][INFO] - Training Epoch: 1/2, step 3666/7134 completed (loss: 0.07016441971063614, acc: 0.9762773513793945)
[2025-02-13 02:58:17,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:17,278][root][INFO] - Training Epoch: 1/2, step 3667/7134 completed (loss: 0.1953091323375702, acc: 0.9544419050216675)
[2025-02-13 02:58:17,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:17,675][root][INFO] - Training Epoch: 1/2, step 3668/7134 completed (loss: 0.05445992201566696, acc: 0.9912739992141724)
[2025-02-13 02:58:17,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:18,075][root][INFO] - Training Epoch: 1/2, step 3669/7134 completed (loss: 0.031152667477726936, acc: 0.9886578321456909)
[2025-02-13 02:58:18,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:18,482][root][INFO] - Training Epoch: 1/2, step 3670/7134 completed (loss: 0.03233049064874649, acc: 0.9945054650306702)
[2025-02-13 02:58:18,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:18,873][root][INFO] - Training Epoch: 1/2, step 3671/7134 completed (loss: 0.0467655248939991, acc: 0.9841040372848511)
[2025-02-13 02:58:19,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:19,320][root][INFO] - Training Epoch: 1/2, step 3672/7134 completed (loss: 0.033228617161512375, acc: 0.993686854839325)
[2025-02-13 02:58:19,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:19,722][root][INFO] - Training Epoch: 1/2, step 3673/7134 completed (loss: 0.07336578518152237, acc: 0.9832776188850403)
[2025-02-13 02:58:19,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:20,137][root][INFO] - Training Epoch: 1/2, step 3674/7134 completed (loss: 0.033066511154174805, acc: 0.9905511736869812)
[2025-02-13 02:58:20,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:20,567][root][INFO] - Training Epoch: 1/2, step 3675/7134 completed (loss: 0.03535868972539902, acc: 0.9923469424247742)
[2025-02-13 02:58:20,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:20,981][root][INFO] - Training Epoch: 1/2, step 3676/7134 completed (loss: 0.03684959560632706, acc: 0.9883381724357605)
[2025-02-13 02:58:21,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:21,422][root][INFO] - Training Epoch: 1/2, step 3677/7134 completed (loss: 0.04051906242966652, acc: 0.9876712560653687)
[2025-02-13 02:58:21,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:21,842][root][INFO] - Training Epoch: 1/2, step 3678/7134 completed (loss: 0.05337075516581535, acc: 0.9838056564331055)
[2025-02-13 02:58:21,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:22,293][root][INFO] - Training Epoch: 1/2, step 3679/7134 completed (loss: 0.03858770430088043, acc: 0.9950413107872009)
[2025-02-13 02:58:22,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:22,727][root][INFO] - Training Epoch: 1/2, step 3680/7134 completed (loss: 0.07094091922044754, acc: 0.9821428656578064)
[2025-02-13 02:58:22,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:23,131][root][INFO] - Training Epoch: 1/2, step 3681/7134 completed (loss: 0.043662797659635544, acc: 0.9846368432044983)
[2025-02-13 02:58:23,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:23,553][root][INFO] - Training Epoch: 1/2, step 3682/7134 completed (loss: 0.037579990923404694, acc: 0.990777313709259)
[2025-02-13 02:58:23,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:24,006][root][INFO] - Training Epoch: 1/2, step 3683/7134 completed (loss: 0.035946279764175415, acc: 0.9905511736869812)
[2025-02-13 02:58:24,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:24,404][root][INFO] - Training Epoch: 1/2, step 3684/7134 completed (loss: 0.050420790910720825, acc: 0.9884560108184814)
[2025-02-13 02:58:24,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:24,794][root][INFO] - Training Epoch: 1/2, step 3685/7134 completed (loss: 0.02960021421313286, acc: 0.9865125417709351)
[2025-02-13 02:58:24,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:25,199][root][INFO] - Training Epoch: 1/2, step 3686/7134 completed (loss: 0.023638401180505753, acc: 0.995502233505249)
[2025-02-13 02:58:25,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:25,612][root][INFO] - Training Epoch: 1/2, step 3687/7134 completed (loss: 0.026597002521157265, acc: 0.9923076629638672)
[2025-02-13 02:58:25,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:26,014][root][INFO] - Training Epoch: 1/2, step 3688/7134 completed (loss: 0.014612731523811817, acc: 0.9979715943336487)
[2025-02-13 02:58:26,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:26,436][root][INFO] - Training Epoch: 1/2, step 3689/7134 completed (loss: 0.02445107139647007, acc: 0.9901960492134094)
[2025-02-13 02:58:26,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:26,878][root][INFO] - Training Epoch: 1/2, step 3690/7134 completed (loss: 0.02993319183588028, acc: 0.9885550737380981)
[2025-02-13 02:58:27,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:27,269][root][INFO] - Training Epoch: 1/2, step 3691/7134 completed (loss: 0.04597479850053787, acc: 0.985111653804779)
[2025-02-13 02:58:27,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:27,684][root][INFO] - Training Epoch: 1/2, step 3692/7134 completed (loss: 0.05738535523414612, acc: 0.978691041469574)
[2025-02-13 02:58:27,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:28,105][root][INFO] - Training Epoch: 1/2, step 3693/7134 completed (loss: 0.04065852612257004, acc: 0.9810725450515747)
[2025-02-13 02:58:28,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:28,520][root][INFO] - Training Epoch: 1/2, step 3694/7134 completed (loss: 0.03475603088736534, acc: 0.9928876161575317)
[2025-02-13 02:58:28,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:28,946][root][INFO] - Training Epoch: 1/2, step 3695/7134 completed (loss: 0.04973920062184334, acc: 0.9858356714248657)
[2025-02-13 02:58:29,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:29,392][root][INFO] - Training Epoch: 1/2, step 3696/7134 completed (loss: 0.03905165567994118, acc: 0.9884318709373474)
[2025-02-13 02:58:29,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:29,784][root][INFO] - Training Epoch: 1/2, step 3697/7134 completed (loss: 0.019143851473927498, acc: 0.9927113652229309)
[2025-02-13 02:58:29,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:30,177][root][INFO] - Training Epoch: 1/2, step 3698/7134 completed (loss: 0.04565422609448433, acc: 0.9855538010597229)
[2025-02-13 02:58:30,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:30,577][root][INFO] - Training Epoch: 1/2, step 3699/7134 completed (loss: 0.01082685124129057, acc: 0.9966216087341309)
[2025-02-13 02:58:30,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:31,000][root][INFO] - Training Epoch: 1/2, step 3700/7134 completed (loss: 0.040950365364551544, acc: 0.9883381724357605)
[2025-02-13 02:58:31,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:31,447][root][INFO] - Training Epoch: 1/2, step 3701/7134 completed (loss: 0.05675385147333145, acc: 0.9900662302970886)
[2025-02-13 02:58:31,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:31,856][root][INFO] - Training Epoch: 1/2, step 3702/7134 completed (loss: 0.05183079466223717, acc: 0.9862259030342102)
[2025-02-13 02:58:31,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:32,299][root][INFO] - Training Epoch: 1/2, step 3703/7134 completed (loss: 0.0372745618224144, acc: 0.9910600185394287)
[2025-02-13 02:58:32,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:32,701][root][INFO] - Training Epoch: 1/2, step 3704/7134 completed (loss: 0.021691132336854935, acc: 0.991631805896759)
[2025-02-13 02:58:32,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:33,101][root][INFO] - Training Epoch: 1/2, step 3705/7134 completed (loss: 0.07737758755683899, acc: 0.9824841022491455)
[2025-02-13 02:58:33,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:33,511][root][INFO] - Training Epoch: 1/2, step 3706/7134 completed (loss: 0.11758523434400558, acc: 0.9766297936439514)
[2025-02-13 02:58:33,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:33,936][root][INFO] - Training Epoch: 1/2, step 3707/7134 completed (loss: 0.02423524297773838, acc: 0.9873772859573364)
[2025-02-13 02:58:34,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:34,379][root][INFO] - Training Epoch: 1/2, step 3708/7134 completed (loss: 0.06845994293689728, acc: 0.9809523820877075)
[2025-02-13 02:58:34,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:34,852][root][INFO] - Training Epoch: 1/2, step 3709/7134 completed (loss: 0.03809528797864914, acc: 0.9868153929710388)
[2025-02-13 02:58:34,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:35,328][root][INFO] - Training Epoch: 1/2, step 3710/7134 completed (loss: 0.06138192117214203, acc: 0.9784836173057556)
[2025-02-13 02:58:35,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:35,743][root][INFO] - Training Epoch: 1/2, step 3711/7134 completed (loss: 0.05600902438163757, acc: 0.9826302528381348)
[2025-02-13 02:58:35,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:36,203][root][INFO] - Training Epoch: 1/2, step 3712/7134 completed (loss: 0.05809580162167549, acc: 0.9844683408737183)
[2025-02-13 02:58:36,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:36,628][root][INFO] - Training Epoch: 1/2, step 3713/7134 completed (loss: 0.05192684009671211, acc: 0.9868637323379517)
[2025-02-13 02:58:36,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:37,057][root][INFO] - Training Epoch: 1/2, step 3714/7134 completed (loss: 0.05475025624036789, acc: 0.988950252532959)
[2025-02-13 02:58:37,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:37,501][root][INFO] - Training Epoch: 1/2, step 3715/7134 completed (loss: 0.03635847568511963, acc: 0.987908124923706)
[2025-02-13 02:58:37,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:37,960][root][INFO] - Training Epoch: 1/2, step 3716/7134 completed (loss: 0.05794883519411087, acc: 0.9790382385253906)
[2025-02-13 02:58:38,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:38,395][root][INFO] - Training Epoch: 1/2, step 3717/7134 completed (loss: 0.04084880277514458, acc: 0.9890710115432739)
[2025-02-13 02:58:38,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:38,854][root][INFO] - Training Epoch: 1/2, step 3718/7134 completed (loss: 0.06320925056934357, acc: 0.9824150204658508)
[2025-02-13 02:58:39,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:39,302][root][INFO] - Training Epoch: 1/2, step 3719/7134 completed (loss: 0.07171257585287094, acc: 0.9838235378265381)
[2025-02-13 02:58:39,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:39,702][root][INFO] - Training Epoch: 1/2, step 3720/7134 completed (loss: 0.02601785399019718, acc: 0.992668628692627)
[2025-02-13 02:58:39,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:40,156][root][INFO] - Training Epoch: 1/2, step 3721/7134 completed (loss: 0.049973443150520325, acc: 0.9853528738021851)
[2025-02-13 02:58:40,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:40,594][root][INFO] - Training Epoch: 1/2, step 3722/7134 completed (loss: 0.03591827675700188, acc: 0.988916277885437)
[2025-02-13 02:58:40,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:41,007][root][INFO] - Training Epoch: 1/2, step 3723/7134 completed (loss: 0.06214792653918266, acc: 0.9827337861061096)
[2025-02-13 02:58:41,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:41,439][root][INFO] - Training Epoch: 1/2, step 3724/7134 completed (loss: 0.05859211087226868, acc: 0.977011501789093)
[2025-02-13 02:58:41,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:41,884][root][INFO] - Training Epoch: 1/2, step 3725/7134 completed (loss: 0.11128008365631104, acc: 0.9719763994216919)
[2025-02-13 02:58:42,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:42,343][root][INFO] - Training Epoch: 1/2, step 3726/7134 completed (loss: 0.04937324300408363, acc: 0.9904357194900513)
[2025-02-13 02:58:42,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:42,770][root][INFO] - Training Epoch: 1/2, step 3727/7134 completed (loss: 0.03237605839967728, acc: 0.9933244585990906)
[2025-02-13 02:58:42,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:43,213][root][INFO] - Training Epoch: 1/2, step 3728/7134 completed (loss: 0.017535489052534103, acc: 0.9954904317855835)
[2025-02-13 02:58:43,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:43,659][root][INFO] - Training Epoch: 1/2, step 3729/7134 completed (loss: 0.03908228129148483, acc: 0.9897727370262146)
[2025-02-13 02:58:43,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:44,097][root][INFO] - Training Epoch: 1/2, step 3730/7134 completed (loss: 0.043769508600234985, acc: 0.9891451597213745)
[2025-02-13 02:58:44,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:44,533][root][INFO] - Training Epoch: 1/2, step 3731/7134 completed (loss: 0.027115333825349808, acc: 0.993773341178894)
[2025-02-13 02:58:44,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:44,973][root][INFO] - Training Epoch: 1/2, step 3732/7134 completed (loss: 0.02214295230805874, acc: 0.9920544624328613)
[2025-02-13 02:58:45,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:45,397][root][INFO] - Training Epoch: 1/2, step 3733/7134 completed (loss: 0.07218144834041595, acc: 0.9775596261024475)
[2025-02-13 02:58:45,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:45,799][root][INFO] - Training Epoch: 1/2, step 3734/7134 completed (loss: 0.1154264435172081, acc: 0.9644013047218323)
[2025-02-13 02:58:45,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:46,201][root][INFO] - Training Epoch: 1/2, step 3735/7134 completed (loss: 0.062344443053007126, acc: 0.9789156913757324)
[2025-02-13 02:58:46,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:46,634][root][INFO] - Training Epoch: 1/2, step 3736/7134 completed (loss: 0.1246088296175003, acc: 0.9634782671928406)
[2025-02-13 02:58:46,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:47,046][root][INFO] - Training Epoch: 1/2, step 3737/7134 completed (loss: 0.07377078384160995, acc: 0.9758672714233398)
[2025-02-13 02:58:47,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:47,441][root][INFO] - Training Epoch: 1/2, step 3738/7134 completed (loss: 0.050244465470314026, acc: 0.9884488582611084)
[2025-02-13 02:58:47,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:47,783][root][INFO] - Training Epoch: 1/2, step 3739/7134 completed (loss: 0.15355852246284485, acc: 0.9606741666793823)
[2025-02-13 02:58:47,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:48,178][root][INFO] - Training Epoch: 1/2, step 3740/7134 completed (loss: 0.07265427708625793, acc: 0.9788960814476013)
[2025-02-13 02:58:48,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:48,587][root][INFO] - Training Epoch: 1/2, step 3741/7134 completed (loss: 0.0893857330083847, acc: 0.9786019921302795)
[2025-02-13 02:58:48,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:48,945][root][INFO] - Training Epoch: 1/2, step 3742/7134 completed (loss: 0.06224449351429939, acc: 0.9829059839248657)
[2025-02-13 02:58:49,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:49,358][root][INFO] - Training Epoch: 1/2, step 3743/7134 completed (loss: 0.08766255527734756, acc: 0.9719763994216919)
[2025-02-13 02:58:49,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:49,788][root][INFO] - Training Epoch: 1/2, step 3744/7134 completed (loss: 0.11431422084569931, acc: 0.9657443761825562)
[2025-02-13 02:58:49,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:50,162][root][INFO] - Training Epoch: 1/2, step 3745/7134 completed (loss: 0.1100250855088234, acc: 0.9605568647384644)
[2025-02-13 02:58:50,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:50,596][root][INFO] - Training Epoch: 1/2, step 3746/7134 completed (loss: 0.09462746232748032, acc: 0.9698953032493591)
[2025-02-13 02:58:50,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:51,027][root][INFO] - Training Epoch: 1/2, step 3747/7134 completed (loss: 0.06368552893400192, acc: 0.9854227304458618)
[2025-02-13 02:58:51,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:51,374][root][INFO] - Training Epoch: 1/2, step 3748/7134 completed (loss: 0.0849418044090271, acc: 0.9788732528686523)
[2025-02-13 02:58:51,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:51,772][root][INFO] - Training Epoch: 1/2, step 3749/7134 completed (loss: 0.10203338414430618, acc: 0.9745127558708191)
[2025-02-13 02:58:51,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:52,188][root][INFO] - Training Epoch: 1/2, step 3750/7134 completed (loss: 0.04142684489488602, acc: 0.9832636117935181)
[2025-02-13 02:58:52,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:52,608][root][INFO] - Training Epoch: 1/2, step 3751/7134 completed (loss: 0.08431702107191086, acc: 0.9701279997825623)
[2025-02-13 02:58:52,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:53,031][root][INFO] - Training Epoch: 1/2, step 3752/7134 completed (loss: 0.07754135131835938, acc: 0.9791044592857361)
[2025-02-13 02:58:53,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:53,449][root][INFO] - Training Epoch: 1/2, step 3753/7134 completed (loss: 0.11110219359397888, acc: 0.9626308083534241)
[2025-02-13 02:58:53,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:53,846][root][INFO] - Training Epoch: 1/2, step 3754/7134 completed (loss: 0.04611583054065704, acc: 0.9848254919052124)
[2025-02-13 02:58:53,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:54,246][root][INFO] - Training Epoch: 1/2, step 3755/7134 completed (loss: 0.05543949827551842, acc: 0.9870129823684692)
[2025-02-13 02:58:54,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:54,659][root][INFO] - Training Epoch: 1/2, step 3756/7134 completed (loss: 0.07264377921819687, acc: 0.9833794832229614)
[2025-02-13 02:58:54,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:55,076][root][INFO] - Training Epoch: 1/2, step 3757/7134 completed (loss: 0.11026203632354736, acc: 0.9638752341270447)
[2025-02-13 02:58:55,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:55,450][root][INFO] - Training Epoch: 1/2, step 3758/7134 completed (loss: 0.059025414288043976, acc: 0.9855855703353882)
[2025-02-13 02:58:55,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:55,852][root][INFO] - Training Epoch: 1/2, step 3759/7134 completed (loss: 0.10901005566120148, acc: 0.9679054021835327)
[2025-02-13 02:58:55,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:56,300][root][INFO] - Training Epoch: 1/2, step 3760/7134 completed (loss: 0.086147241294384, acc: 0.9774011373519897)
[2025-02-13 02:58:56,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:56,714][root][INFO] - Training Epoch: 1/2, step 3761/7134 completed (loss: 0.05763380229473114, acc: 0.984544038772583)
[2025-02-13 02:58:56,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:57,145][root][INFO] - Training Epoch: 1/2, step 3762/7134 completed (loss: 0.051399245858192444, acc: 0.9862448573112488)
[2025-02-13 02:58:57,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:57,550][root][INFO] - Training Epoch: 1/2, step 3763/7134 completed (loss: 0.04223832115530968, acc: 0.9876543283462524)
[2025-02-13 02:58:57,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:57,969][root][INFO] - Training Epoch: 1/2, step 3764/7134 completed (loss: 0.07243961095809937, acc: 0.9822161197662354)
[2025-02-13 02:58:58,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:58,454][root][INFO] - Training Epoch: 1/2, step 3765/7134 completed (loss: 0.047556668519973755, acc: 0.9843937754631042)
[2025-02-13 02:58:58,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:58,839][root][INFO] - Training Epoch: 1/2, step 3766/7134 completed (loss: 0.09487704932689667, acc: 0.9800570011138916)
[2025-02-13 02:58:58,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:59,274][root][INFO] - Training Epoch: 1/2, step 3767/7134 completed (loss: 0.029340451583266258, acc: 0.9900249242782593)
[2025-02-13 02:58:59,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:59,685][root][INFO] - Training Epoch: 1/2, step 3768/7134 completed (loss: 0.041840676218271255, acc: 0.9875518679618835)
[2025-02-13 02:58:59,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:00,126][root][INFO] - Training Epoch: 1/2, step 3769/7134 completed (loss: 0.042008254677057266, acc: 0.984886646270752)
[2025-02-13 02:59:00,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:00,534][root][INFO] - Training Epoch: 1/2, step 3770/7134 completed (loss: 0.053862858563661575, acc: 0.9844632744789124)
[2025-02-13 02:59:00,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:00,970][root][INFO] - Training Epoch: 1/2, step 3771/7134 completed (loss: 0.040919579565525055, acc: 0.9883117079734802)
[2025-02-13 02:59:01,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:01,394][root][INFO] - Training Epoch: 1/2, step 3772/7134 completed (loss: 0.07877088338136673, acc: 0.9730941653251648)
[2025-02-13 02:59:01,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:01,846][root][INFO] - Training Epoch: 1/2, step 3773/7134 completed (loss: 0.050101201981306076, acc: 0.9888392686843872)
[2025-02-13 02:59:01,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:02,290][root][INFO] - Training Epoch: 1/2, step 3774/7134 completed (loss: 0.05976022034883499, acc: 0.9802784323692322)
[2025-02-13 02:59:02,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:02,715][root][INFO] - Training Epoch: 1/2, step 3775/7134 completed (loss: 0.03616444393992424, acc: 0.9914039969444275)
[2025-02-13 02:59:02,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:03,150][root][INFO] - Training Epoch: 1/2, step 3776/7134 completed (loss: 0.06409450620412827, acc: 0.9822221994400024)
[2025-02-13 02:59:03,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:03,584][root][INFO] - Training Epoch: 1/2, step 3777/7134 completed (loss: 0.043598998337984085, acc: 0.9874686598777771)
[2025-02-13 02:59:03,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:04,001][root][INFO] - Training Epoch: 1/2, step 3778/7134 completed (loss: 0.08401001989841461, acc: 0.9800853729248047)
[2025-02-13 02:59:04,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:04,402][root][INFO] - Training Epoch: 1/2, step 3779/7134 completed (loss: 0.036248985677957535, acc: 0.9886040091514587)
[2025-02-13 02:59:04,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:04,822][root][INFO] - Training Epoch: 1/2, step 3780/7134 completed (loss: 0.037242528051137924, acc: 0.9855907559394836)
[2025-02-13 02:59:04,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:05,231][root][INFO] - Training Epoch: 1/2, step 3781/7134 completed (loss: 0.06503051519393921, acc: 0.9835391044616699)
[2025-02-13 02:59:05,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:05,619][root][INFO] - Training Epoch: 1/2, step 3782/7134 completed (loss: 0.06049628555774689, acc: 0.9800570011138916)
[2025-02-13 02:59:05,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:06,044][root][INFO] - Training Epoch: 1/2, step 3783/7134 completed (loss: 0.05963291600346565, acc: 0.9819355010986328)
[2025-02-13 02:59:06,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:06,446][root][INFO] - Training Epoch: 1/2, step 3784/7134 completed (loss: 0.06268659234046936, acc: 0.989180862903595)
[2025-02-13 02:59:06,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:06,856][root][INFO] - Training Epoch: 1/2, step 3785/7134 completed (loss: 0.01989010162651539, acc: 0.9921996593475342)
[2025-02-13 02:59:06,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:07,260][root][INFO] - Training Epoch: 1/2, step 3786/7134 completed (loss: 0.06829292327165604, acc: 0.9818887710571289)
[2025-02-13 02:59:07,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:07,643][root][INFO] - Training Epoch: 1/2, step 3787/7134 completed (loss: 0.057744100689888, acc: 0.9872408509254456)
[2025-02-13 02:59:07,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:08,097][root][INFO] - Training Epoch: 1/2, step 3788/7134 completed (loss: 0.052341435104608536, acc: 0.9851552248001099)
[2025-02-13 02:59:08,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:08,529][root][INFO] - Training Epoch: 1/2, step 3789/7134 completed (loss: 0.06944704055786133, acc: 0.9746835231781006)
[2025-02-13 02:59:08,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:08,964][root][INFO] - Training Epoch: 1/2, step 3790/7134 completed (loss: 0.11265504360198975, acc: 0.9733840227127075)
[2025-02-13 02:59:09,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:09,369][root][INFO] - Training Epoch: 1/2, step 3791/7134 completed (loss: 0.048539478331804276, acc: 0.9883871078491211)
[2025-02-13 02:59:09,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:09,777][root][INFO] - Training Epoch: 1/2, step 3792/7134 completed (loss: 0.045880213379859924, acc: 0.9802955389022827)
[2025-02-13 02:59:09,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:10,201][root][INFO] - Training Epoch: 1/2, step 3793/7134 completed (loss: 0.059518154710531235, acc: 0.9851351380348206)
[2025-02-13 02:59:10,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:10,612][root][INFO] - Training Epoch: 1/2, step 3794/7134 completed (loss: 0.052815813571214676, acc: 0.9857327938079834)
[2025-02-13 02:59:10,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:11,062][root][INFO] - Training Epoch: 1/2, step 3795/7134 completed (loss: 0.05665110424160957, acc: 0.983132541179657)
[2025-02-13 02:59:11,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:11,496][root][INFO] - Training Epoch: 1/2, step 3796/7134 completed (loss: 0.04964751750230789, acc: 0.983668327331543)
[2025-02-13 02:59:11,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:11,905][root][INFO] - Training Epoch: 1/2, step 3797/7134 completed (loss: 0.03216593340039253, acc: 0.9895697236061096)
[2025-02-13 02:59:12,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:12,317][root][INFO] - Training Epoch: 1/2, step 3798/7134 completed (loss: 0.07285585254430771, acc: 0.9841954112052917)
[2025-02-13 02:59:12,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:12,735][root][INFO] - Training Epoch: 1/2, step 3799/7134 completed (loss: 0.06734766811132431, acc: 0.9755469560623169)
[2025-02-13 02:59:12,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:13,151][root][INFO] - Training Epoch: 1/2, step 3800/7134 completed (loss: 0.08127834647893906, acc: 0.9785932898521423)
[2025-02-13 02:59:13,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:13,599][root][INFO] - Training Epoch: 1/2, step 3801/7134 completed (loss: 0.03803747519850731, acc: 0.9876543283462524)
[2025-02-13 02:59:13,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:14,039][root][INFO] - Training Epoch: 1/2, step 3802/7134 completed (loss: 0.04335545003414154, acc: 0.9881796836853027)
[2025-02-13 02:59:14,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:14,448][root][INFO] - Training Epoch: 1/2, step 3803/7134 completed (loss: 0.06708645075559616, acc: 0.9789915680885315)
[2025-02-13 02:59:14,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:14,893][root][INFO] - Training Epoch: 1/2, step 3804/7134 completed (loss: 0.04279168322682381, acc: 0.9860031008720398)
[2025-02-13 02:59:15,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:15,330][root][INFO] - Training Epoch: 1/2, step 3805/7134 completed (loss: 0.028814101591706276, acc: 0.9895287752151489)
[2025-02-13 02:59:15,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:15,710][root][INFO] - Training Epoch: 1/2, step 3806/7134 completed (loss: 0.061691321432590485, acc: 0.9802761077880859)
[2025-02-13 02:59:15,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:16,132][root][INFO] - Training Epoch: 1/2, step 3807/7134 completed (loss: 0.019849035888910294, acc: 0.9970760345458984)
[2025-02-13 02:59:16,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:16,584][root][INFO] - Training Epoch: 1/2, step 3808/7134 completed (loss: 0.04411899670958519, acc: 0.9873417615890503)
[2025-02-13 02:59:16,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:17,000][root][INFO] - Training Epoch: 1/2, step 3809/7134 completed (loss: 0.08060409128665924, acc: 0.9771573543548584)
[2025-02-13 02:59:17,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:17,407][root][INFO] - Training Epoch: 1/2, step 3810/7134 completed (loss: 0.04685826599597931, acc: 0.9821428656578064)
[2025-02-13 02:59:17,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:17,839][root][INFO] - Training Epoch: 1/2, step 3811/7134 completed (loss: 0.06543909758329391, acc: 0.9806896448135376)
[2025-02-13 02:59:17,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:18,265][root][INFO] - Training Epoch: 1/2, step 3812/7134 completed (loss: 0.07825677841901779, acc: 0.9811320900917053)
[2025-02-13 02:59:18,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:18,705][root][INFO] - Training Epoch: 1/2, step 3813/7134 completed (loss: 0.07882381975650787, acc: 0.9762532711029053)
[2025-02-13 02:59:18,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:19,138][root][INFO] - Training Epoch: 1/2, step 3814/7134 completed (loss: 0.031263552606105804, acc: 0.9906166195869446)
[2025-02-13 02:59:19,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:19,542][root][INFO] - Training Epoch: 1/2, step 3815/7134 completed (loss: 0.055849604308605194, acc: 0.9807407259941101)
[2025-02-13 02:59:19,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:19,992][root][INFO] - Training Epoch: 1/2, step 3816/7134 completed (loss: 0.04194406792521477, acc: 0.9873816967010498)
[2025-02-13 02:59:20,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:20,389][root][INFO] - Training Epoch: 1/2, step 3817/7134 completed (loss: 0.08215484023094177, acc: 0.9784172773361206)
[2025-02-13 02:59:20,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:20,733][root][INFO] - Training Epoch: 1/2, step 3818/7134 completed (loss: 0.10533298552036285, acc: 0.9818181991577148)
[2025-02-13 02:59:20,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:21,086][root][INFO] - Training Epoch: 1/2, step 3819/7134 completed (loss: 0.07241108268499374, acc: 0.9759036302566528)
[2025-02-13 02:59:21,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:21,479][root][INFO] - Training Epoch: 1/2, step 3820/7134 completed (loss: 0.07341429591178894, acc: 0.9691211581230164)
[2025-02-13 02:59:21,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:21,869][root][INFO] - Training Epoch: 1/2, step 3821/7134 completed (loss: 0.050010137259960175, acc: 0.9813874959945679)
[2025-02-13 02:59:21,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:22,263][root][INFO] - Training Epoch: 1/2, step 3822/7134 completed (loss: 0.06736333668231964, acc: 0.9783890247344971)
[2025-02-13 02:59:22,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:22,691][root][INFO] - Training Epoch: 1/2, step 3823/7134 completed (loss: 0.04263549670577049, acc: 0.9820193648338318)
[2025-02-13 02:59:22,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:23,088][root][INFO] - Training Epoch: 1/2, step 3824/7134 completed (loss: 0.09353847056627274, acc: 0.9730185270309448)
[2025-02-13 02:59:23,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:23,503][root][INFO] - Training Epoch: 1/2, step 3825/7134 completed (loss: 0.04689982533454895, acc: 0.9820359349250793)
[2025-02-13 02:59:23,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:23,933][root][INFO] - Training Epoch: 1/2, step 3826/7134 completed (loss: 0.05592501536011696, acc: 0.9808823466300964)
[2025-02-13 02:59:24,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:24,370][root][INFO] - Training Epoch: 1/2, step 3827/7134 completed (loss: 0.0518646240234375, acc: 0.9887955188751221)
[2025-02-13 02:59:24,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:24,714][root][INFO] - Training Epoch: 1/2, step 3828/7134 completed (loss: 0.07012426853179932, acc: 0.9761273264884949)
[2025-02-13 02:59:24,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:25,162][root][INFO] - Training Epoch: 1/2, step 3829/7134 completed (loss: 0.040166810154914856, acc: 0.9909502267837524)
[2025-02-13 02:59:25,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:25,579][root][INFO] - Training Epoch: 1/2, step 3830/7134 completed (loss: 0.06955690681934357, acc: 0.979899525642395)
[2025-02-13 02:59:25,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:26,010][root][INFO] - Training Epoch: 1/2, step 3831/7134 completed (loss: 0.04996654763817787, acc: 0.9879879951477051)
[2025-02-13 02:59:26,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:26,395][root][INFO] - Training Epoch: 1/2, step 3832/7134 completed (loss: 0.051558490842580795, acc: 0.990176796913147)
[2025-02-13 02:59:26,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:26,821][root][INFO] - Training Epoch: 1/2, step 3833/7134 completed (loss: 0.032015204429626465, acc: 0.9913366436958313)
[2025-02-13 02:59:26,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:27,233][root][INFO] - Training Epoch: 1/2, step 3834/7134 completed (loss: 0.012059648521244526, acc: 0.998701274394989)
[2025-02-13 02:59:27,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:27,637][root][INFO] - Training Epoch: 1/2, step 3835/7134 completed (loss: 0.015388806350529194, acc: 0.9939320683479309)
[2025-02-13 02:59:27,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:27,940][root][INFO] - Training Epoch: 1/2, step 3836/7134 completed (loss: 0.07779832184314728, acc: 0.9826086759567261)
[2025-02-13 02:59:28,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:28,335][root][INFO] - Training Epoch: 1/2, step 3837/7134 completed (loss: 0.036193761974573135, acc: 0.9942528605461121)
[2025-02-13 02:59:28,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:28,761][root][INFO] - Training Epoch: 1/2, step 3838/7134 completed (loss: 0.031198972836136818, acc: 0.9880478382110596)
[2025-02-13 02:59:28,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:29,213][root][INFO] - Training Epoch: 1/2, step 3839/7134 completed (loss: 0.03426605463027954, acc: 0.9917126893997192)
[2025-02-13 02:59:29,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:29,604][root][INFO] - Training Epoch: 1/2, step 3840/7134 completed (loss: 0.03301693871617317, acc: 0.9892904758453369)
[2025-02-13 02:59:29,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:30,057][root][INFO] - Training Epoch: 1/2, step 3841/7134 completed (loss: 0.0582684688270092, acc: 0.9789122939109802)
[2025-02-13 02:59:30,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:30,367][root][INFO] - Training Epoch: 1/2, step 3842/7134 completed (loss: 0.016219042241573334, acc: 0.9926470518112183)
[2025-02-13 02:59:30,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:30,846][root][INFO] - Training Epoch: 1/2, step 3843/7134 completed (loss: 0.04319492354989052, acc: 0.9874869585037231)
[2025-02-13 02:59:30,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:31,286][root][INFO] - Training Epoch: 1/2, step 3844/7134 completed (loss: 0.04214965179562569, acc: 0.9885641932487488)
[2025-02-13 02:59:31,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:31,681][root][INFO] - Training Epoch: 1/2, step 3845/7134 completed (loss: 0.05510702729225159, acc: 0.9882155060768127)
[2025-02-13 02:59:31,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:32,056][root][INFO] - Training Epoch: 1/2, step 3846/7134 completed (loss: 0.028087906539440155, acc: 0.9932088255882263)
[2025-02-13 02:59:32,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:32,444][root][INFO] - Training Epoch: 1/2, step 3847/7134 completed (loss: 0.03498926758766174, acc: 0.9904000163078308)
[2025-02-13 02:59:32,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:32,898][root][INFO] - Training Epoch: 1/2, step 3848/7134 completed (loss: 0.038323767483234406, acc: 0.991094172000885)
[2025-02-13 02:59:33,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:33,293][root][INFO] - Training Epoch: 1/2, step 3849/7134 completed (loss: 0.07315316051244736, acc: 0.9803921580314636)
[2025-02-13 02:59:33,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:33,708][root][INFO] - Training Epoch: 1/2, step 3850/7134 completed (loss: 0.08740708976984024, acc: 0.980440080165863)
[2025-02-13 02:59:33,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:34,163][root][INFO] - Training Epoch: 1/2, step 3851/7134 completed (loss: 0.06547769159078598, acc: 0.9776847958564758)
[2025-02-13 02:59:34,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:34,577][root][INFO] - Training Epoch: 1/2, step 3852/7134 completed (loss: 0.035992883145809174, acc: 0.9903537034988403)
[2025-02-13 02:59:34,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:34,990][root][INFO] - Training Epoch: 1/2, step 3853/7134 completed (loss: 0.06925762444734573, acc: 0.9746835231781006)
[2025-02-13 02:59:35,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:35,407][root][INFO] - Training Epoch: 1/2, step 3854/7134 completed (loss: 0.02574753761291504, acc: 0.9915730357170105)
[2025-02-13 02:59:35,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:35,846][root][INFO] - Training Epoch: 1/2, step 3855/7134 completed (loss: 0.0700722485780716, acc: 0.9825436472892761)
[2025-02-13 02:59:35,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:36,252][root][INFO] - Training Epoch: 1/2, step 3856/7134 completed (loss: 0.07445996254682541, acc: 0.9790794849395752)
[2025-02-13 02:59:36,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:36,618][root][INFO] - Training Epoch: 1/2, step 3857/7134 completed (loss: 0.06162692606449127, acc: 0.9807074069976807)
[2025-02-13 02:59:36,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:37,045][root][INFO] - Training Epoch: 1/2, step 3858/7134 completed (loss: 0.08269961178302765, acc: 0.977337121963501)
[2025-02-13 02:59:37,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:37,480][root][INFO] - Training Epoch: 1/2, step 3859/7134 completed (loss: 0.0516609326004982, acc: 0.9872773289680481)
[2025-02-13 02:59:37,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:37,927][root][INFO] - Training Epoch: 1/2, step 3860/7134 completed (loss: 0.03932691365480423, acc: 0.9910614490509033)
[2025-02-13 02:59:38,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:38,355][root][INFO] - Training Epoch: 1/2, step 3861/7134 completed (loss: 0.06272495537996292, acc: 0.9768707752227783)
[2025-02-13 02:59:38,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:38,729][root][INFO] - Training Epoch: 1/2, step 3862/7134 completed (loss: 0.10236213356256485, acc: 0.9720853567123413)
[2025-02-13 02:59:38,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:39,198][root][INFO] - Training Epoch: 1/2, step 3863/7134 completed (loss: 0.0504881925880909, acc: 0.9864197373390198)
[2025-02-13 02:59:39,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:39,637][root][INFO] - Training Epoch: 1/2, step 3864/7134 completed (loss: 0.05145932361483574, acc: 0.9882506728172302)
[2025-02-13 02:59:39,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:40,067][root][INFO] - Training Epoch: 1/2, step 3865/7134 completed (loss: 0.041308678686618805, acc: 0.9900398254394531)
[2025-02-13 02:59:40,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:40,482][root][INFO] - Training Epoch: 1/2, step 3866/7134 completed (loss: 0.07327152788639069, acc: 0.980463981628418)
[2025-02-13 02:59:40,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:40,921][root][INFO] - Training Epoch: 1/2, step 3867/7134 completed (loss: 0.05108550190925598, acc: 0.9867424368858337)
[2025-02-13 02:59:41,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:41,320][root][INFO] - Training Epoch: 1/2, step 3868/7134 completed (loss: 0.04119047522544861, acc: 0.9881556630134583)
[2025-02-13 02:59:41,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:41,732][root][INFO] - Training Epoch: 1/2, step 3869/7134 completed (loss: 0.05642315372824669, acc: 0.9878970980644226)
[2025-02-13 02:59:41,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:42,142][root][INFO] - Training Epoch: 1/2, step 3870/7134 completed (loss: 0.08237773925065994, acc: 0.9791666865348816)
[2025-02-13 02:59:42,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:42,552][root][INFO] - Training Epoch: 1/2, step 3871/7134 completed (loss: 0.06027395650744438, acc: 0.9800498485565186)
[2025-02-13 02:59:42,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:43,020][root][INFO] - Training Epoch: 1/2, step 3872/7134 completed (loss: 0.08449956029653549, acc: 0.9753747582435608)
[2025-02-13 02:59:43,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:43,456][root][INFO] - Training Epoch: 1/2, step 3873/7134 completed (loss: 0.04165000468492508, acc: 0.9872123003005981)
[2025-02-13 02:59:43,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:43,872][root][INFO] - Training Epoch: 1/2, step 3874/7134 completed (loss: 0.08131557703018188, acc: 0.9792531132698059)
[2025-02-13 02:59:44,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:44,324][root][INFO] - Training Epoch: 1/2, step 3875/7134 completed (loss: 0.08536113798618317, acc: 0.978723406791687)
[2025-02-13 02:59:44,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:44,731][root][INFO] - Training Epoch: 1/2, step 3876/7134 completed (loss: 0.05609104037284851, acc: 0.9849905967712402)
[2025-02-13 02:59:44,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:45,145][root][INFO] - Training Epoch: 1/2, step 3877/7134 completed (loss: 0.09794881939888, acc: 0.977806806564331)
[2025-02-13 02:59:45,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:45,553][root][INFO] - Training Epoch: 1/2, step 3878/7134 completed (loss: 0.09950277209281921, acc: 0.9757412672042847)
[2025-02-13 02:59:45,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:45,987][root][INFO] - Training Epoch: 1/2, step 3879/7134 completed (loss: 0.031034398823976517, acc: 0.9926062822341919)
[2025-02-13 02:59:46,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:46,423][root][INFO] - Training Epoch: 1/2, step 3880/7134 completed (loss: 0.14222218096256256, acc: 0.9676945805549622)
[2025-02-13 02:59:46,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:46,838][root][INFO] - Training Epoch: 1/2, step 3881/7134 completed (loss: 0.11933250725269318, acc: 0.9693333506584167)
[2025-02-13 02:59:46,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:47,261][root][INFO] - Training Epoch: 1/2, step 3882/7134 completed (loss: 0.0793592780828476, acc: 0.9787499904632568)
[2025-02-13 02:59:47,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:47,691][root][INFO] - Training Epoch: 1/2, step 3883/7134 completed (loss: 0.05056744068861008, acc: 0.9818181991577148)
[2025-02-13 02:59:47,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:48,108][root][INFO] - Training Epoch: 1/2, step 3884/7134 completed (loss: 0.03844139352440834, acc: 0.9904761910438538)
[2025-02-13 02:59:48,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:48,512][root][INFO] - Training Epoch: 1/2, step 3885/7134 completed (loss: 0.05016527697443962, acc: 0.9869451522827148)
[2025-02-13 02:59:48,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:48,950][root][INFO] - Training Epoch: 1/2, step 3886/7134 completed (loss: 0.05050121247768402, acc: 0.9818840622901917)
[2025-02-13 02:59:49,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:49,359][root][INFO] - Training Epoch: 1/2, step 3887/7134 completed (loss: 0.05069180205464363, acc: 0.9864253401756287)
[2025-02-13 02:59:49,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:49,791][root][INFO] - Training Epoch: 1/2, step 3888/7134 completed (loss: 0.06168462708592415, acc: 0.981249988079071)
[2025-02-13 02:59:49,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:50,253][root][INFO] - Training Epoch: 1/2, step 3889/7134 completed (loss: 0.033035434782505035, acc: 0.9910600185394287)
[2025-02-13 02:59:50,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:50,695][root][INFO] - Training Epoch: 1/2, step 3890/7134 completed (loss: 0.03346896916627884, acc: 0.9903730154037476)
[2025-02-13 02:59:50,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:51,138][root][INFO] - Training Epoch: 1/2, step 3891/7134 completed (loss: 0.0390196219086647, acc: 0.9869281053543091)
[2025-02-13 02:59:51,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:51,569][root][INFO] - Training Epoch: 1/2, step 3892/7134 completed (loss: 0.05306015536189079, acc: 0.9830769300460815)
[2025-02-13 02:59:51,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:51,985][root][INFO] - Training Epoch: 1/2, step 3893/7134 completed (loss: 0.095330148935318, acc: 0.9703608155250549)
[2025-02-13 02:59:52,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:52,386][root][INFO] - Training Epoch: 1/2, step 3894/7134 completed (loss: 0.030309302732348442, acc: 0.9915730357170105)
[2025-02-13 02:59:52,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:52,818][root][INFO] - Training Epoch: 1/2, step 3895/7134 completed (loss: 0.052042584866285324, acc: 0.9845161437988281)
[2025-02-13 02:59:52,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:53,268][root][INFO] - Training Epoch: 1/2, step 3896/7134 completed (loss: 0.01930464804172516, acc: 0.9954853057861328)
[2025-02-13 02:59:53,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:53,714][root][INFO] - Training Epoch: 1/2, step 3897/7134 completed (loss: 0.07915183156728745, acc: 0.9802817106246948)
[2025-02-13 02:59:53,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:54,137][root][INFO] - Training Epoch: 1/2, step 3898/7134 completed (loss: 0.07412916421890259, acc: 0.9809160232543945)
[2025-02-13 02:59:54,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:54,573][root][INFO] - Training Epoch: 1/2, step 3899/7134 completed (loss: 0.03956142067909241, acc: 0.9909443855285645)
[2025-02-13 02:59:54,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:55,029][root][INFO] - Training Epoch: 1/2, step 3900/7134 completed (loss: 0.055019717663526535, acc: 0.9837775230407715)
[2025-02-13 02:59:55,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:55,433][root][INFO] - Training Epoch: 1/2, step 3901/7134 completed (loss: 0.09672636538743973, acc: 0.9743935465812683)
[2025-02-13 02:59:55,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:55,851][root][INFO] - Training Epoch: 1/2, step 3902/7134 completed (loss: 0.11545126140117645, acc: 0.9721854329109192)
[2025-02-13 02:59:56,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:56,304][root][INFO] - Training Epoch: 1/2, step 3903/7134 completed (loss: 0.1254776418209076, acc: 0.9638069868087769)
[2025-02-13 02:59:56,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:56,695][root][INFO] - Training Epoch: 1/2, step 3904/7134 completed (loss: 0.0767739787697792, acc: 0.9817351698875427)
[2025-02-13 02:59:56,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:57,133][root][INFO] - Training Epoch: 1/2, step 3905/7134 completed (loss: 0.050217315554618835, acc: 0.9844852089881897)
[2025-02-13 02:59:57,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:57,572][root][INFO] - Training Epoch: 1/2, step 3906/7134 completed (loss: 0.04463570937514305, acc: 0.982758641242981)
[2025-02-13 02:59:57,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:57,980][root][INFO] - Training Epoch: 1/2, step 3907/7134 completed (loss: 0.036579180508852005, acc: 0.9871465563774109)
[2025-02-13 02:59:58,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:58,410][root][INFO] - Training Epoch: 1/2, step 3908/7134 completed (loss: 0.08946322649717331, acc: 0.9693721532821655)
[2025-02-13 02:59:58,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:58,824][root][INFO] - Training Epoch: 1/2, step 3909/7134 completed (loss: 0.05029044300317764, acc: 0.9806138873100281)
[2025-02-13 02:59:58,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:59,229][root][INFO] - Training Epoch: 1/2, step 3910/7134 completed (loss: 0.06334603577852249, acc: 0.9791377186775208)
[2025-02-13 02:59:59,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:59,667][root][INFO] - Training Epoch: 1/2, step 3911/7134 completed (loss: 0.04476119577884674, acc: 0.9886731505393982)
[2025-02-13 02:59:59,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:00,089][root][INFO] - Training Epoch: 1/2, step 3912/7134 completed (loss: 0.016416264697909355, acc: 0.9955357313156128)
[2025-02-13 03:00:00,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:00,512][root][INFO] - Training Epoch: 1/2, step 3913/7134 completed (loss: 0.05606553331017494, acc: 0.9884615540504456)
[2025-02-13 03:00:00,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:00,922][root][INFO] - Training Epoch: 1/2, step 3914/7134 completed (loss: 0.046880677342414856, acc: 0.9865319728851318)
[2025-02-13 03:00:01,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:01,366][root][INFO] - Training Epoch: 1/2, step 3915/7134 completed (loss: 0.03040170669555664, acc: 0.9929478168487549)
[2025-02-13 03:00:01,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:01,805][root][INFO] - Training Epoch: 1/2, step 3916/7134 completed (loss: 0.03793978691101074, acc: 0.9910827875137329)
[2025-02-13 03:00:01,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:02,246][root][INFO] - Training Epoch: 1/2, step 3917/7134 completed (loss: 0.029869774356484413, acc: 0.9879662990570068)
[2025-02-13 03:00:02,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:02,673][root][INFO] - Training Epoch: 1/2, step 3918/7134 completed (loss: 0.055061016231775284, acc: 0.9778645634651184)
[2025-02-13 03:00:02,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:03,108][root][INFO] - Training Epoch: 1/2, step 3919/7134 completed (loss: 0.09867081791162491, acc: 0.9723126888275146)
[2025-02-13 03:00:03,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:03,543][root][INFO] - Training Epoch: 1/2, step 3920/7134 completed (loss: 0.07845646142959595, acc: 0.9781931638717651)
[2025-02-13 03:00:03,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:03,953][root][INFO] - Training Epoch: 1/2, step 3921/7134 completed (loss: 0.07224913686513901, acc: 0.9739999771118164)
[2025-02-13 03:00:04,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:04,340][root][INFO] - Training Epoch: 1/2, step 3922/7134 completed (loss: 0.13861441612243652, acc: 0.9580712914466858)
[2025-02-13 03:00:04,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:04,725][root][INFO] - Training Epoch: 1/2, step 3923/7134 completed (loss: 0.1527145355939865, acc: 0.9640411138534546)
[2025-02-13 03:00:04,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:05,162][root][INFO] - Training Epoch: 1/2, step 3924/7134 completed (loss: 0.07376966625452042, acc: 0.9741379022598267)
[2025-02-13 03:00:05,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:05,580][root][INFO] - Training Epoch: 1/2, step 3925/7134 completed (loss: 0.07846737653017044, acc: 0.9795022010803223)
[2025-02-13 03:00:05,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:05,975][root][INFO] - Training Epoch: 1/2, step 3926/7134 completed (loss: 0.0689552053809166, acc: 0.9856114983558655)
[2025-02-13 03:00:06,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:06,394][root][INFO] - Training Epoch: 1/2, step 3927/7134 completed (loss: 0.04504364728927612, acc: 0.9862448573112488)
[2025-02-13 03:00:06,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:06,819][root][INFO] - Training Epoch: 1/2, step 3928/7134 completed (loss: 0.08756953477859497, acc: 0.9741379022598267)
[2025-02-13 03:00:06,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:07,207][root][INFO] - Training Epoch: 1/2, step 3929/7134 completed (loss: 0.04581762105226517, acc: 0.9856915473937988)
[2025-02-13 03:00:07,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:07,622][root][INFO] - Training Epoch: 1/2, step 3930/7134 completed (loss: 0.09754418581724167, acc: 0.9663865566253662)
[2025-02-13 03:00:07,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:08,027][root][INFO] - Training Epoch: 1/2, step 3931/7134 completed (loss: 0.051423151046037674, acc: 0.9775784611701965)
[2025-02-13 03:00:08,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:08,446][root][INFO] - Training Epoch: 1/2, step 3932/7134 completed (loss: 0.06307591497898102, acc: 0.9822134375572205)
[2025-02-13 03:00:08,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:08,808][root][INFO] - Training Epoch: 1/2, step 3933/7134 completed (loss: 0.03104155883193016, acc: 0.9934210777282715)
[2025-02-13 03:00:08,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:09,202][root][INFO] - Training Epoch: 1/2, step 3934/7134 completed (loss: 0.045124322175979614, acc: 0.9812646508216858)
[2025-02-13 03:00:09,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:09,588][root][INFO] - Training Epoch: 1/2, step 3935/7134 completed (loss: 0.012472599744796753, acc: 0.9964664578437805)
[2025-02-13 03:00:09,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:09,986][root][INFO] - Training Epoch: 1/2, step 3936/7134 completed (loss: 0.05664411559700966, acc: 0.9849056601524353)
[2025-02-13 03:00:10,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:10,390][root][INFO] - Training Epoch: 1/2, step 3937/7134 completed (loss: 0.06848885864019394, acc: 0.9795918464660645)
[2025-02-13 03:00:10,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:10,764][root][INFO] - Training Epoch: 1/2, step 3938/7134 completed (loss: 0.04123639315366745, acc: 0.9886877536773682)
[2025-02-13 03:00:10,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:11,151][root][INFO] - Training Epoch: 1/2, step 3939/7134 completed (loss: 0.09765361249446869, acc: 0.9695431590080261)
[2025-02-13 03:00:11,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:11,477][root][INFO] - Training Epoch: 1/2, step 3940/7134 completed (loss: 0.07969480752944946, acc: 0.9849246144294739)
[2025-02-13 03:00:11,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:11,798][root][INFO] - Training Epoch: 1/2, step 3941/7134 completed (loss: 0.039872556924819946, acc: 0.9847561120986938)
[2025-02-13 03:00:11,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:12,185][root][INFO] - Training Epoch: 1/2, step 3942/7134 completed (loss: 0.04703109711408615, acc: 0.9877049326896667)
[2025-02-13 03:00:12,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:12,437][root][INFO] - Training Epoch: 1/2, step 3943/7134 completed (loss: 0.05629928037524223, acc: 0.9768339991569519)
[2025-02-13 03:00:12,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:12,835][root][INFO] - Training Epoch: 1/2, step 3944/7134 completed (loss: 0.046519502997398376, acc: 0.9870848655700684)
[2025-02-13 03:00:12,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:13,232][root][INFO] - Training Epoch: 1/2, step 3945/7134 completed (loss: 0.052702851593494415, acc: 0.9817073345184326)
[2025-02-13 03:00:13,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:13,587][root][INFO] - Training Epoch: 1/2, step 3946/7134 completed (loss: 0.052502941340208054, acc: 0.9874739050865173)
[2025-02-13 03:00:13,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:13,958][root][INFO] - Training Epoch: 1/2, step 3947/7134 completed (loss: 0.06514734774827957, acc: 0.9870466589927673)
[2025-02-13 03:00:14,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:14,344][root][INFO] - Training Epoch: 1/2, step 3948/7134 completed (loss: 0.02497986890375614, acc: 0.9974874258041382)
[2025-02-13 03:00:14,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:14,740][root][INFO] - Training Epoch: 1/2, step 3949/7134 completed (loss: 0.048665594309568405, acc: 0.9813432693481445)
[2025-02-13 03:00:14,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:15,075][root][INFO] - Training Epoch: 1/2, step 3950/7134 completed (loss: 0.06732393801212311, acc: 0.9776785969734192)
[2025-02-13 03:00:15,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:15,436][root][INFO] - Training Epoch: 1/2, step 3951/7134 completed (loss: 0.059706296771764755, acc: 0.9849624037742615)
[2025-02-13 03:00:15,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:15,825][root][INFO] - Training Epoch: 1/2, step 3952/7134 completed (loss: 0.04552195966243744, acc: 0.9871794581413269)
[2025-02-13 03:00:15,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:16,192][root][INFO] - Training Epoch: 1/2, step 3953/7134 completed (loss: 0.05258164927363396, acc: 0.9829221963882446)
[2025-02-13 03:00:16,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:16,546][root][INFO] - Training Epoch: 1/2, step 3954/7134 completed (loss: 0.04351966455578804, acc: 0.9845361113548279)
[2025-02-13 03:00:16,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:16,933][root][INFO] - Training Epoch: 1/2, step 3955/7134 completed (loss: 0.041571542620658875, acc: 0.9865092635154724)
[2025-02-13 03:00:17,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:17,334][root][INFO] - Training Epoch: 1/2, step 3956/7134 completed (loss: 0.03361790254712105, acc: 0.9919999837875366)
[2025-02-13 03:00:17,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:17,726][root][INFO] - Training Epoch: 1/2, step 3957/7134 completed (loss: 0.04619000479578972, acc: 0.9864864945411682)
[2025-02-13 03:00:17,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:18,075][root][INFO] - Training Epoch: 1/2, step 3958/7134 completed (loss: 0.11116168648004532, acc: 0.9615384340286255)
[2025-02-13 03:00:18,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:18,478][root][INFO] - Training Epoch: 1/2, step 3959/7134 completed (loss: 0.03631899878382683, acc: 0.9912087917327881)
[2025-02-13 03:00:18,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:18,911][root][INFO] - Training Epoch: 1/2, step 3960/7134 completed (loss: 0.04118311405181885, acc: 0.9871794581413269)
[2025-02-13 03:00:19,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:19,324][root][INFO] - Training Epoch: 1/2, step 3961/7134 completed (loss: 0.018594585359096527, acc: 0.9940029978752136)
[2025-02-13 03:00:19,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:19,725][root][INFO] - Training Epoch: 1/2, step 3962/7134 completed (loss: 0.030715368688106537, acc: 0.9886731505393982)
[2025-02-13 03:00:19,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:20,142][root][INFO] - Training Epoch: 1/2, step 3963/7134 completed (loss: 0.12047497183084488, acc: 0.9794721603393555)
[2025-02-13 03:00:20,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:20,535][root][INFO] - Training Epoch: 1/2, step 3964/7134 completed (loss: 0.09109478443861008, acc: 0.979629635810852)
[2025-02-13 03:00:20,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:20,942][root][INFO] - Training Epoch: 1/2, step 3965/7134 completed (loss: 0.04381361976265907, acc: 0.9885550737380981)
[2025-02-13 03:00:21,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:21,340][root][INFO] - Training Epoch: 1/2, step 3966/7134 completed (loss: 0.027246898040175438, acc: 0.9909747242927551)
[2025-02-13 03:00:21,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:21,776][root][INFO] - Training Epoch: 1/2, step 3967/7134 completed (loss: 0.09688418358564377, acc: 0.9705372452735901)
[2025-02-13 03:00:21,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:22,217][root][INFO] - Training Epoch: 1/2, step 3968/7134 completed (loss: 0.03237106278538704, acc: 0.9882965087890625)
[2025-02-13 03:00:22,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:22,545][root][INFO] - Training Epoch: 1/2, step 3969/7134 completed (loss: 0.10359303653240204, acc: 0.9685863852500916)
[2025-02-13 03:00:22,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:22,948][root][INFO] - Training Epoch: 1/2, step 3970/7134 completed (loss: 0.10757610201835632, acc: 0.9784172773361206)
[2025-02-13 03:00:23,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:23,388][root][INFO] - Training Epoch: 1/2, step 3971/7134 completed (loss: 0.13972169160842896, acc: 0.9693094491958618)
[2025-02-13 03:00:23,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:23,791][root][INFO] - Training Epoch: 1/2, step 3972/7134 completed (loss: 0.19588607549667358, acc: 0.9432989954948425)
[2025-02-13 03:00:23,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:24,208][root][INFO] - Training Epoch: 1/2, step 3973/7134 completed (loss: 0.05512892082333565, acc: 0.9792746305465698)
[2025-02-13 03:00:24,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:24,642][root][INFO] - Training Epoch: 1/2, step 3974/7134 completed (loss: 0.12423627823591232, acc: 0.9661246538162231)
[2025-02-13 03:00:24,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:25,048][root][INFO] - Training Epoch: 1/2, step 3975/7134 completed (loss: 0.06171474978327751, acc: 0.9758307933807373)
[2025-02-13 03:00:25,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:25,490][root][INFO] - Training Epoch: 1/2, step 3976/7134 completed (loss: 0.11418947577476501, acc: 0.9734513163566589)
[2025-02-13 03:00:25,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:25,917][root][INFO] - Training Epoch: 1/2, step 3977/7134 completed (loss: 0.04050887003540993, acc: 0.9895651936531067)
[2025-02-13 03:00:26,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:26,327][root][INFO] - Training Epoch: 1/2, step 3978/7134 completed (loss: 0.02877197414636612, acc: 0.9948253631591797)
[2025-02-13 03:00:26,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:26,763][root][INFO] - Training Epoch: 1/2, step 3979/7134 completed (loss: 0.09016838669776917, acc: 0.9744991064071655)
[2025-02-13 03:00:26,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:27,175][root][INFO] - Training Epoch: 1/2, step 3980/7134 completed (loss: 0.03020077757537365, acc: 0.9939024448394775)
[2025-02-13 03:00:27,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:27,580][root][INFO] - Training Epoch: 1/2, step 3981/7134 completed (loss: 0.06218278035521507, acc: 0.9847095012664795)
[2025-02-13 03:00:27,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:27,996][root][INFO] - Training Epoch: 1/2, step 3982/7134 completed (loss: 0.11150777339935303, acc: 0.9713056087493896)
[2025-02-13 03:00:28,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:28,398][root][INFO] - Training Epoch: 1/2, step 3983/7134 completed (loss: 0.32325974106788635, acc: 0.936170220375061)
[2025-02-13 03:00:28,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:28,829][root][INFO] - Training Epoch: 1/2, step 3984/7134 completed (loss: 0.09236349165439606, acc: 0.9779220819473267)
[2025-02-13 03:00:28,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:29,225][root][INFO] - Training Epoch: 1/2, step 3985/7134 completed (loss: 0.0326758474111557, acc: 0.9893048405647278)
[2025-02-13 03:00:29,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:29,651][root][INFO] - Training Epoch: 1/2, step 3986/7134 completed (loss: 0.20175020396709442, acc: 0.9575757384300232)
[2025-02-13 03:00:29,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:30,074][root][INFO] - Training Epoch: 1/2, step 3987/7134 completed (loss: 0.08234516531229019, acc: 0.9808061122894287)
[2025-02-13 03:00:30,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:30,509][root][INFO] - Training Epoch: 1/2, step 3988/7134 completed (loss: 0.1139543280005455, acc: 0.9646017551422119)
[2025-02-13 03:00:30,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:30,823][root][INFO] - Training Epoch: 1/2, step 3989/7134 completed (loss: 0.2295597791671753, acc: 0.9530916810035706)
[2025-02-13 03:00:30,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:31,240][root][INFO] - Training Epoch: 1/2, step 3990/7134 completed (loss: 0.05127489194273949, acc: 0.9883913993835449)
[2025-02-13 03:00:31,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:31,565][root][INFO] - Training Epoch: 1/2, step 3991/7134 completed (loss: 0.06684441864490509, acc: 0.9834983348846436)
[2025-02-13 03:00:31,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:31,949][root][INFO] - Training Epoch: 1/2, step 3992/7134 completed (loss: 0.09186078608036041, acc: 0.9756757020950317)
[2025-02-13 03:00:32,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:32,316][root][INFO] - Training Epoch: 1/2, step 3993/7134 completed (loss: 0.07931357622146606, acc: 0.9747706651687622)
[2025-02-13 03:00:32,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:32,673][root][INFO] - Training Epoch: 1/2, step 3994/7134 completed (loss: 0.08430436998605728, acc: 0.9833333492279053)
[2025-02-13 03:00:32,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:33,039][root][INFO] - Training Epoch: 1/2, step 3995/7134 completed (loss: 0.14833322167396545, acc: 0.9651162624359131)
[2025-02-13 03:00:33,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:33,401][root][INFO] - Training Epoch: 1/2, step 3996/7134 completed (loss: 0.045814577490091324, acc: 0.9897330403327942)
[2025-02-13 03:00:33,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:33,772][root][INFO] - Training Epoch: 1/2, step 3997/7134 completed (loss: 0.1899225413799286, acc: 0.9582417607307434)
[2025-02-13 03:00:33,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:34,110][root][INFO] - Training Epoch: 1/2, step 3998/7134 completed (loss: 0.1050586923956871, acc: 0.9693654179573059)
[2025-02-13 03:00:34,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:34,453][root][INFO] - Training Epoch: 1/2, step 3999/7134 completed (loss: 0.05894836038351059, acc: 0.9840319156646729)
[2025-02-13 03:00:34,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:34,849][root][INFO] - Training Epoch: 1/2, step 4000/7134 completed (loss: 0.09256835281848907, acc: 0.9806950092315674)
[2025-02-13 03:00:34,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:35,240][root][INFO] - Training Epoch: 1/2, step 4001/7134 completed (loss: 0.08637748658657074, acc: 0.9761431217193604)
[2025-02-13 03:00:35,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:35,632][root][INFO] - Training Epoch: 1/2, step 4002/7134 completed (loss: 0.12958331406116486, acc: 0.9734939932823181)
[2025-02-13 03:00:35,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:36,048][root][INFO] - Training Epoch: 1/2, step 4003/7134 completed (loss: 0.10543995350599289, acc: 0.9760147333145142)
[2025-02-13 03:00:36,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:36,396][root][INFO] - Training Epoch: 1/2, step 4004/7134 completed (loss: 0.0846341997385025, acc: 0.9696969985961914)
[2025-02-13 03:00:36,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:36,791][root][INFO] - Training Epoch: 1/2, step 4005/7134 completed (loss: 0.044493645429611206, acc: 0.9858712553977966)
[2025-02-13 03:00:36,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:37,247][root][INFO] - Training Epoch: 1/2, step 4006/7134 completed (loss: 0.031174173578619957, acc: 0.9927158951759338)
[2025-02-13 03:00:37,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:37,672][root][INFO] - Training Epoch: 1/2, step 4007/7134 completed (loss: 0.05153069645166397, acc: 0.9835082292556763)
[2025-02-13 03:00:37,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:38,127][root][INFO] - Training Epoch: 1/2, step 4008/7134 completed (loss: 0.03232442960143089, acc: 0.9926004409790039)
[2025-02-13 03:00:38,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:38,560][root][INFO] - Training Epoch: 1/2, step 4009/7134 completed (loss: 0.0241027120500803, acc: 0.9933422207832336)
[2025-02-13 03:00:38,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:39,012][root][INFO] - Training Epoch: 1/2, step 4010/7134 completed (loss: 0.03586515039205551, acc: 0.991051435470581)
[2025-02-13 03:00:39,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:39,453][root][INFO] - Training Epoch: 1/2, step 4011/7134 completed (loss: 0.03832605108618736, acc: 0.9880478382110596)
[2025-02-13 03:00:39,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:39,846][root][INFO] - Training Epoch: 1/2, step 4012/7134 completed (loss: 0.02502833679318428, acc: 0.991525411605835)
[2025-02-13 03:00:39,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:40,274][root][INFO] - Training Epoch: 1/2, step 4013/7134 completed (loss: 0.013613261282444, acc: 0.9967845678329468)
[2025-02-13 03:00:40,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:40,718][root][INFO] - Training Epoch: 1/2, step 4014/7134 completed (loss: 0.021281834691762924, acc: 0.9921630024909973)
[2025-02-13 03:00:40,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:41,132][root][INFO] - Training Epoch: 1/2, step 4015/7134 completed (loss: 0.03038198873400688, acc: 0.9892473220825195)
[2025-02-13 03:00:41,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:41,595][root][INFO] - Training Epoch: 1/2, step 4016/7134 completed (loss: 0.054845184087753296, acc: 0.9885203838348389)
[2025-02-13 03:00:41,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:42,042][root][INFO] - Training Epoch: 1/2, step 4017/7134 completed (loss: 0.0469590462744236, acc: 0.9872978925704956)
[2025-02-13 03:00:42,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:42,481][root][INFO] - Training Epoch: 1/2, step 4018/7134 completed (loss: 0.05028169974684715, acc: 0.9890909194946289)
[2025-02-13 03:00:42,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:42,905][root][INFO] - Training Epoch: 1/2, step 4019/7134 completed (loss: 0.05528701841831207, acc: 0.9847715497016907)
[2025-02-13 03:00:43,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:43,303][root][INFO] - Training Epoch: 1/2, step 4020/7134 completed (loss: 0.0478900782763958, acc: 0.9836065769195557)
[2025-02-13 03:00:43,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:43,695][root][INFO] - Training Epoch: 1/2, step 4021/7134 completed (loss: 0.03807755932211876, acc: 0.9893048405647278)
[2025-02-13 03:00:43,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:44,127][root][INFO] - Training Epoch: 1/2, step 4022/7134 completed (loss: 0.060099370777606964, acc: 0.9889570474624634)
[2025-02-13 03:00:44,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:44,584][root][INFO] - Training Epoch: 1/2, step 4023/7134 completed (loss: 0.049424219876527786, acc: 0.9886792302131653)
[2025-02-13 03:00:44,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:45,012][root][INFO] - Training Epoch: 1/2, step 4024/7134 completed (loss: 0.04851139336824417, acc: 0.9868766665458679)
[2025-02-13 03:00:45,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:45,446][root][INFO] - Training Epoch: 1/2, step 4025/7134 completed (loss: 0.07499013096094131, acc: 0.9807474613189697)
[2025-02-13 03:00:45,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:45,839][root][INFO] - Training Epoch: 1/2, step 4026/7134 completed (loss: 0.10631468892097473, acc: 0.976401150226593)
[2025-02-13 03:00:45,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:46,268][root][INFO] - Training Epoch: 1/2, step 4027/7134 completed (loss: 0.07119631767272949, acc: 0.9808153510093689)
[2025-02-13 03:00:46,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:46,678][root][INFO] - Training Epoch: 1/2, step 4028/7134 completed (loss: 0.06784145534038544, acc: 0.979345977306366)
[2025-02-13 03:00:46,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:47,121][root][INFO] - Training Epoch: 1/2, step 4029/7134 completed (loss: 0.035445328801870346, acc: 0.9889298677444458)
[2025-02-13 03:00:47,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:47,550][root][INFO] - Training Epoch: 1/2, step 4030/7134 completed (loss: 0.04024605080485344, acc: 0.987679660320282)
[2025-02-13 03:00:47,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:47,956][root][INFO] - Training Epoch: 1/2, step 4031/7134 completed (loss: 0.06501225382089615, acc: 0.9882698059082031)
[2025-02-13 03:00:48,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:48,383][root][INFO] - Training Epoch: 1/2, step 4032/7134 completed (loss: 0.1056801900267601, acc: 0.9759036302566528)
[2025-02-13 03:00:48,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:48,764][root][INFO] - Training Epoch: 1/2, step 4033/7134 completed (loss: 0.03449302166700363, acc: 0.9861634969711304)
[2025-02-13 03:00:48,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:49,193][root][INFO] - Training Epoch: 1/2, step 4034/7134 completed (loss: 0.013828838244080544, acc: 0.9939098954200745)
[2025-02-13 03:00:49,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:49,678][root][INFO] - Training Epoch: 1/2, step 4035/7134 completed (loss: 0.06041887030005455, acc: 0.9834710955619812)
[2025-02-13 03:00:49,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:50,075][root][INFO] - Training Epoch: 1/2, step 4036/7134 completed (loss: 0.039895713329315186, acc: 0.9835164546966553)
[2025-02-13 03:00:50,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:50,524][root][INFO] - Training Epoch: 1/2, step 4037/7134 completed (loss: 0.02833711914718151, acc: 0.9903730154037476)
[2025-02-13 03:00:50,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:50,935][root][INFO] - Training Epoch: 1/2, step 4038/7134 completed (loss: 0.06566393375396729, acc: 0.9834938049316406)
[2025-02-13 03:00:51,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:51,326][root][INFO] - Training Epoch: 1/2, step 4039/7134 completed (loss: 0.05628045275807381, acc: 0.978723406791687)
[2025-02-13 03:00:51,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:51,744][root][INFO] - Training Epoch: 1/2, step 4040/7134 completed (loss: 0.03517017140984535, acc: 0.9901477694511414)
[2025-02-13 03:00:51,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:52,187][root][INFO] - Training Epoch: 1/2, step 4041/7134 completed (loss: 0.04087797552347183, acc: 0.9919354915618896)
[2025-02-13 03:00:52,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:52,602][root][INFO] - Training Epoch: 1/2, step 4042/7134 completed (loss: 0.02530978061258793, acc: 0.9955882430076599)
[2025-02-13 03:00:52,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:52,982][root][INFO] - Training Epoch: 1/2, step 4043/7134 completed (loss: 0.03199838101863861, acc: 0.9867841601371765)
[2025-02-13 03:00:53,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:53,404][root][INFO] - Training Epoch: 1/2, step 4044/7134 completed (loss: 0.015961507335305214, acc: 0.9949495196342468)
[2025-02-13 03:00:53,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:53,839][root][INFO] - Training Epoch: 1/2, step 4045/7134 completed (loss: 0.07862616330385208, acc: 0.9833333492279053)
[2025-02-13 03:00:53,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:54,263][root][INFO] - Training Epoch: 1/2, step 4046/7134 completed (loss: 0.054252028465270996, acc: 0.9848901033401489)
[2025-02-13 03:00:54,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:54,709][root][INFO] - Training Epoch: 1/2, step 4047/7134 completed (loss: 0.050072863698005676, acc: 0.9868735074996948)
[2025-02-13 03:00:54,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:55,160][root][INFO] - Training Epoch: 1/2, step 4048/7134 completed (loss: 0.03617233410477638, acc: 0.9871794581413269)
[2025-02-13 03:00:55,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:55,638][root][INFO] - Training Epoch: 1/2, step 4049/7134 completed (loss: 0.045183151960372925, acc: 0.9885057210922241)
[2025-02-13 03:00:55,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:56,050][root][INFO] - Training Epoch: 1/2, step 4050/7134 completed (loss: 0.07195759564638138, acc: 0.9815602898597717)
[2025-02-13 03:00:56,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:56,492][root][INFO] - Training Epoch: 1/2, step 4051/7134 completed (loss: 0.08677524328231812, acc: 0.9844497442245483)
[2025-02-13 03:00:56,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:56,881][root][INFO] - Training Epoch: 1/2, step 4052/7134 completed (loss: 0.03215145319700241, acc: 0.9896640777587891)
[2025-02-13 03:00:57,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:57,308][root][INFO] - Training Epoch: 1/2, step 4053/7134 completed (loss: 0.05123361945152283, acc: 0.9840849041938782)
[2025-02-13 03:00:57,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:57,743][root][INFO] - Training Epoch: 1/2, step 4054/7134 completed (loss: 0.04716994985938072, acc: 0.9900000095367432)
[2025-02-13 03:00:57,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:58,193][root][INFO] - Training Epoch: 1/2, step 4055/7134 completed (loss: 0.04585317149758339, acc: 0.981796145439148)
[2025-02-13 03:00:58,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:58,639][root][INFO] - Training Epoch: 1/2, step 4056/7134 completed (loss: 0.04342474415898323, acc: 0.9878787994384766)
[2025-02-13 03:00:58,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:59,079][root][INFO] - Training Epoch: 1/2, step 4057/7134 completed (loss: 0.08152464032173157, acc: 0.9833101630210876)
[2025-02-13 03:00:59,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:59,439][root][INFO] - Training Epoch: 1/2, step 4058/7134 completed (loss: 0.1125737652182579, acc: 0.9751434326171875)
[2025-02-13 03:00:59,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:59,835][root][INFO] - Training Epoch: 1/2, step 4059/7134 completed (loss: 0.06930842995643616, acc: 0.9817517995834351)
[2025-02-13 03:00:59,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:00,225][root][INFO] - Training Epoch: 1/2, step 4060/7134 completed (loss: 0.040934085845947266, acc: 0.9860896468162537)
[2025-02-13 03:01:00,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:00,644][root][INFO] - Training Epoch: 1/2, step 4061/7134 completed (loss: 0.035710953176021576, acc: 0.9886040091514587)
[2025-02-13 03:01:00,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:01,047][root][INFO] - Training Epoch: 1/2, step 4062/7134 completed (loss: 0.02762693539261818, acc: 0.9937008023262024)
[2025-02-13 03:01:01,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:01,409][root][INFO] - Training Epoch: 1/2, step 4063/7134 completed (loss: 0.03543464094400406, acc: 0.9858657121658325)
[2025-02-13 03:01:01,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:01,832][root][INFO] - Training Epoch: 1/2, step 4064/7134 completed (loss: 0.02710001915693283, acc: 0.9954614043235779)
[2025-02-13 03:01:01,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:02,257][root][INFO] - Training Epoch: 1/2, step 4065/7134 completed (loss: 0.03867180645465851, acc: 0.9875862002372742)
[2025-02-13 03:01:02,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:02,694][root][INFO] - Training Epoch: 1/2, step 4066/7134 completed (loss: 0.03768385574221611, acc: 0.9902777671813965)
[2025-02-13 03:01:02,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:03,094][root][INFO] - Training Epoch: 1/2, step 4067/7134 completed (loss: 0.039429839700460434, acc: 0.9885621070861816)
[2025-02-13 03:01:03,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:03,492][root][INFO] - Training Epoch: 1/2, step 4068/7134 completed (loss: 0.03796442598104477, acc: 0.9851852059364319)
[2025-02-13 03:01:03,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:03,895][root][INFO] - Training Epoch: 1/2, step 4069/7134 completed (loss: 0.035538699477910995, acc: 0.9906103014945984)
[2025-02-13 03:01:04,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:04,303][root][INFO] - Training Epoch: 1/2, step 4070/7134 completed (loss: 0.04408784583210945, acc: 0.9878234267234802)
[2025-02-13 03:01:04,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:04,698][root][INFO] - Training Epoch: 1/2, step 4071/7134 completed (loss: 0.0489220954477787, acc: 0.9851729869842529)
[2025-02-13 03:01:04,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:05,117][root][INFO] - Training Epoch: 1/2, step 4072/7134 completed (loss: 0.057099197059869766, acc: 0.9870129823684692)
[2025-02-13 03:01:05,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:05,539][root][INFO] - Training Epoch: 1/2, step 4073/7134 completed (loss: 0.029200896620750427, acc: 0.9899425506591797)
[2025-02-13 03:01:05,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:05,960][root][INFO] - Training Epoch: 1/2, step 4074/7134 completed (loss: 0.04068648815155029, acc: 0.9899857044219971)
[2025-02-13 03:01:06,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:06,383][root][INFO] - Training Epoch: 1/2, step 4075/7134 completed (loss: 0.01971801184117794, acc: 0.9925037622451782)
[2025-02-13 03:01:06,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:06,822][root][INFO] - Training Epoch: 1/2, step 4076/7134 completed (loss: 0.017378075048327446, acc: 0.9971056580543518)
[2025-02-13 03:01:06,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:07,224][root][INFO] - Training Epoch: 1/2, step 4077/7134 completed (loss: 0.05814392492175102, acc: 0.9796954393386841)
[2025-02-13 03:01:07,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:07,635][root][INFO] - Training Epoch: 1/2, step 4078/7134 completed (loss: 0.03174036368727684, acc: 0.9871794581413269)
[2025-02-13 03:01:07,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:07,974][root][INFO] - Training Epoch: 1/2, step 4079/7134 completed (loss: 0.03581591695547104, acc: 0.9928698539733887)
[2025-02-13 03:01:08,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:08,359][root][INFO] - Training Epoch: 1/2, step 4080/7134 completed (loss: 0.025914911180734634, acc: 0.9889240264892578)
[2025-02-13 03:01:08,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:08,742][root][INFO] - Training Epoch: 1/2, step 4081/7134 completed (loss: 0.012879876419901848, acc: 0.9983498454093933)
[2025-02-13 03:01:08,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:09,130][root][INFO] - Training Epoch: 1/2, step 4082/7134 completed (loss: 0.09682638943195343, acc: 0.9704251289367676)
[2025-02-13 03:01:09,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:09,532][root][INFO] - Training Epoch: 1/2, step 4083/7134 completed (loss: 0.04824412241578102, acc: 0.9880239367485046)
[2025-02-13 03:01:09,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:09,931][root][INFO] - Training Epoch: 1/2, step 4084/7134 completed (loss: 0.10278045386075974, acc: 0.95686274766922)
[2025-02-13 03:01:10,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:10,345][root][INFO] - Training Epoch: 1/2, step 4085/7134 completed (loss: 0.05945199355483055, acc: 0.979784369468689)
[2025-02-13 03:01:10,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:10,747][root][INFO] - Training Epoch: 1/2, step 4086/7134 completed (loss: 0.09545760601758957, acc: 0.9770867228507996)
[2025-02-13 03:01:10,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:11,182][root][INFO] - Training Epoch: 1/2, step 4087/7134 completed (loss: 0.029156159609556198, acc: 0.9961240291595459)
[2025-02-13 03:01:11,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:11,610][root][INFO] - Training Epoch: 1/2, step 4088/7134 completed (loss: 0.10155394673347473, acc: 0.9701492786407471)
[2025-02-13 03:01:11,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:12,013][root][INFO] - Training Epoch: 1/2, step 4089/7134 completed (loss: 0.09622666239738464, acc: 0.9769093990325928)
[2025-02-13 03:01:12,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:12,424][root][INFO] - Training Epoch: 1/2, step 4090/7134 completed (loss: 0.14519430696964264, acc: 0.9670184850692749)
[2025-02-13 03:01:12,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:12,823][root][INFO] - Training Epoch: 1/2, step 4091/7134 completed (loss: 0.12028997391462326, acc: 0.9730185270309448)
[2025-02-13 03:01:12,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:13,265][root][INFO] - Training Epoch: 1/2, step 4092/7134 completed (loss: 0.07658155262470245, acc: 0.9768339991569519)
[2025-02-13 03:01:13,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:13,646][root][INFO] - Training Epoch: 1/2, step 4093/7134 completed (loss: 0.04869712516665459, acc: 0.9817850589752197)
[2025-02-13 03:01:13,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:14,055][root][INFO] - Training Epoch: 1/2, step 4094/7134 completed (loss: 0.09905066341161728, acc: 0.9729729890823364)
[2025-02-13 03:01:14,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:14,499][root][INFO] - Training Epoch: 1/2, step 4095/7134 completed (loss: 0.04850244149565697, acc: 0.9826338887214661)
[2025-02-13 03:01:14,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:14,859][root][INFO] - Training Epoch: 1/2, step 4096/7134 completed (loss: 0.1067223846912384, acc: 0.9791044592857361)
[2025-02-13 03:01:15,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:15,312][root][INFO] - Training Epoch: 1/2, step 4097/7134 completed (loss: 0.08670787513256073, acc: 0.9838709831237793)
[2025-02-13 03:01:15,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:15,726][root][INFO] - Training Epoch: 1/2, step 4098/7134 completed (loss: 0.04538683220744133, acc: 0.989159882068634)
[2025-02-13 03:01:15,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:16,128][root][INFO] - Training Epoch: 1/2, step 4099/7134 completed (loss: 0.05851441249251366, acc: 0.98591548204422)
[2025-02-13 03:01:16,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:16,578][root][INFO] - Training Epoch: 1/2, step 4100/7134 completed (loss: 0.008317586965858936, acc: 1.0)
[2025-02-13 03:01:16,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:16,972][root][INFO] - Training Epoch: 1/2, step 4101/7134 completed (loss: 0.019133517518639565, acc: 0.9961977005004883)
[2025-02-13 03:01:17,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:17,376][root][INFO] - Training Epoch: 1/2, step 4102/7134 completed (loss: 0.05284043028950691, acc: 0.9817184805870056)
[2025-02-13 03:01:17,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:17,790][root][INFO] - Training Epoch: 1/2, step 4103/7134 completed (loss: 0.011094165965914726, acc: 0.9955157041549683)
[2025-02-13 03:01:17,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:18,197][root][INFO] - Training Epoch: 1/2, step 4104/7134 completed (loss: 0.02205689065158367, acc: 0.9932318329811096)
[2025-02-13 03:01:18,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:18,611][root][INFO] - Training Epoch: 1/2, step 4105/7134 completed (loss: 0.02292529121041298, acc: 0.9924471378326416)
[2025-02-13 03:01:18,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:19,016][root][INFO] - Training Epoch: 1/2, step 4106/7134 completed (loss: 0.039433859288692474, acc: 0.9861963391304016)
[2025-02-13 03:01:19,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:19,399][root][INFO] - Training Epoch: 1/2, step 4107/7134 completed (loss: 0.035672254860401154, acc: 0.9915966391563416)
[2025-02-13 03:01:19,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:19,781][root][INFO] - Training Epoch: 1/2, step 4108/7134 completed (loss: 0.04945174604654312, acc: 0.9870550036430359)
[2025-02-13 03:01:19,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:20,183][root][INFO] - Training Epoch: 1/2, step 4109/7134 completed (loss: 0.02120617963373661, acc: 0.9919742941856384)
[2025-02-13 03:01:20,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:20,580][root][INFO] - Training Epoch: 1/2, step 4110/7134 completed (loss: 0.012676323764026165, acc: 0.9982993006706238)
[2025-02-13 03:01:20,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:21,003][root][INFO] - Training Epoch: 1/2, step 4111/7134 completed (loss: 0.021029924973845482, acc: 0.9921135902404785)
[2025-02-13 03:01:21,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:21,427][root][INFO] - Training Epoch: 1/2, step 4112/7134 completed (loss: 0.04149477183818817, acc: 0.9856630563735962)
[2025-02-13 03:01:21,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:21,848][root][INFO] - Training Epoch: 1/2, step 4113/7134 completed (loss: 0.05573062598705292, acc: 0.9844961166381836)
[2025-02-13 03:01:21,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:22,240][root][INFO] - Training Epoch: 1/2, step 4114/7134 completed (loss: 0.11007090657949448, acc: 0.9674796462059021)
[2025-02-13 03:01:22,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:22,636][root][INFO] - Training Epoch: 1/2, step 4115/7134 completed (loss: 0.0895395576953888, acc: 0.9776119589805603)
[2025-02-13 03:01:22,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:23,066][root][INFO] - Training Epoch: 1/2, step 4116/7134 completed (loss: 0.013040147721767426, acc: 0.9983713626861572)
[2025-02-13 03:01:23,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:23,471][root][INFO] - Training Epoch: 1/2, step 4117/7134 completed (loss: 0.04933224245905876, acc: 0.988959014415741)
[2025-02-13 03:01:23,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:23,815][root][INFO] - Training Epoch: 1/2, step 4118/7134 completed (loss: 0.023542338982224464, acc: 0.9926289916038513)
[2025-02-13 03:01:23,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:24,216][root][INFO] - Training Epoch: 1/2, step 4119/7134 completed (loss: 0.06447931379079819, acc: 0.9838449358940125)
[2025-02-13 03:01:24,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:24,576][root][INFO] - Training Epoch: 1/2, step 4120/7134 completed (loss: 0.041728027164936066, acc: 0.9917627573013306)
[2025-02-13 03:01:24,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:24,988][root][INFO] - Training Epoch: 1/2, step 4121/7134 completed (loss: 0.06286437064409256, acc: 0.9857954382896423)
[2025-02-13 03:01:25,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:25,390][root][INFO] - Training Epoch: 1/2, step 4122/7134 completed (loss: 0.023992210626602173, acc: 0.9904580116271973)
[2025-02-13 03:01:25,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:25,792][root][INFO] - Training Epoch: 1/2, step 4123/7134 completed (loss: 0.030776333063840866, acc: 0.9909502267837524)
[2025-02-13 03:01:25,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:26,214][root][INFO] - Training Epoch: 1/2, step 4124/7134 completed (loss: 0.040726758539676666, acc: 0.9886845946311951)
[2025-02-13 03:01:26,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:26,616][root][INFO] - Training Epoch: 1/2, step 4125/7134 completed (loss: 0.027296466752886772, acc: 0.9926470518112183)
[2025-02-13 03:01:26,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:27,025][root][INFO] - Training Epoch: 1/2, step 4126/7134 completed (loss: 0.024075694382190704, acc: 0.9943342804908752)
[2025-02-13 03:01:27,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:27,419][root][INFO] - Training Epoch: 1/2, step 4127/7134 completed (loss: 0.043919824063777924, acc: 0.989062488079071)
[2025-02-13 03:01:27,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:27,810][root][INFO] - Training Epoch: 1/2, step 4128/7134 completed (loss: 0.02508183754980564, acc: 0.992175281047821)
[2025-02-13 03:01:27,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:28,253][root][INFO] - Training Epoch: 1/2, step 4129/7134 completed (loss: 0.028904074802994728, acc: 0.9940652847290039)
[2025-02-13 03:01:28,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:28,645][root][INFO] - Training Epoch: 1/2, step 4130/7134 completed (loss: 0.04913530498743057, acc: 0.9903661012649536)
[2025-02-13 03:01:28,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:29,039][root][INFO] - Training Epoch: 1/2, step 4131/7134 completed (loss: 0.01656055822968483, acc: 0.996303141117096)
[2025-02-13 03:01:29,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:29,431][root][INFO] - Training Epoch: 1/2, step 4132/7134 completed (loss: 0.024420902132987976, acc: 0.9910714030265808)
[2025-02-13 03:01:29,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:29,797][root][INFO] - Training Epoch: 1/2, step 4133/7134 completed (loss: 0.05876535177230835, acc: 0.9814814925193787)
[2025-02-13 03:01:29,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:30,161][root][INFO] - Training Epoch: 1/2, step 4134/7134 completed (loss: 0.08969661593437195, acc: 0.971563994884491)
[2025-02-13 03:01:30,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:30,563][root][INFO] - Training Epoch: 1/2, step 4135/7134 completed (loss: 0.02955467253923416, acc: 0.9893428087234497)
[2025-02-13 03:01:30,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:30,993][root][INFO] - Training Epoch: 1/2, step 4136/7134 completed (loss: 0.020841805264353752, acc: 0.9946902394294739)
[2025-02-13 03:01:31,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:31,401][root][INFO] - Training Epoch: 1/2, step 4137/7134 completed (loss: 0.05399439111351967, acc: 0.984375)
[2025-02-13 03:01:31,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:31,802][root][INFO] - Training Epoch: 1/2, step 4138/7134 completed (loss: 0.04910273477435112, acc: 0.982758641242981)
[2025-02-13 03:01:31,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:32,182][root][INFO] - Training Epoch: 1/2, step 4139/7134 completed (loss: 0.04223030433058739, acc: 0.991416335105896)
[2025-02-13 03:01:32,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:32,584][root][INFO] - Training Epoch: 1/2, step 4140/7134 completed (loss: 0.04648946225643158, acc: 0.9875862002372742)
[2025-02-13 03:01:32,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:33,050][root][INFO] - Training Epoch: 1/2, step 4141/7134 completed (loss: 0.12114432454109192, acc: 0.9785100221633911)
[2025-02-13 03:01:33,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:33,496][root][INFO] - Training Epoch: 1/2, step 4142/7134 completed (loss: 0.1295461654663086, acc: 0.9656750559806824)
[2025-02-13 03:01:33,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:33,967][root][INFO] - Training Epoch: 1/2, step 4143/7134 completed (loss: 0.06005246192216873, acc: 0.9867346882820129)
[2025-02-13 03:01:34,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:34,439][root][INFO] - Training Epoch: 1/2, step 4144/7134 completed (loss: 0.102351114153862, acc: 0.9703872203826904)
[2025-02-13 03:01:34,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:34,870][root][INFO] - Training Epoch: 1/2, step 4145/7134 completed (loss: 0.06765174865722656, acc: 0.9765343070030212)
[2025-02-13 03:01:34,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:35,219][root][INFO] - Training Epoch: 1/2, step 4146/7134 completed (loss: 0.14472848176956177, acc: 0.9676767587661743)
[2025-02-13 03:01:35,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:35,629][root][INFO] - Training Epoch: 1/2, step 4147/7134 completed (loss: 0.10741299390792847, acc: 0.969348669052124)
[2025-02-13 03:01:35,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:36,028][root][INFO] - Training Epoch: 1/2, step 4148/7134 completed (loss: 0.21384868025779724, acc: 0.9440154433250427)
[2025-02-13 03:01:36,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:36,330][root][INFO] - Training Epoch: 1/2, step 4149/7134 completed (loss: 0.16827267408370972, acc: 0.9599999785423279)
[2025-02-13 03:01:36,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:36,731][root][INFO] - Training Epoch: 1/2, step 4150/7134 completed (loss: 0.19433365762233734, acc: 0.9416499137878418)
[2025-02-13 03:01:36,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:37,191][root][INFO] - Training Epoch: 1/2, step 4151/7134 completed (loss: 0.21266911923885345, acc: 0.945652186870575)
[2025-02-13 03:01:37,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:37,616][root][INFO] - Training Epoch: 1/2, step 4152/7134 completed (loss: 0.08000664412975311, acc: 0.973724901676178)
[2025-02-13 03:01:37,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:38,049][root][INFO] - Training Epoch: 1/2, step 4153/7134 completed (loss: 0.12100213021039963, acc: 0.9662576913833618)
[2025-02-13 03:01:38,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:38,511][root][INFO] - Training Epoch: 1/2, step 4154/7134 completed (loss: 0.10085337609052658, acc: 0.9722222089767456)
[2025-02-13 03:01:38,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:38,909][root][INFO] - Training Epoch: 1/2, step 4155/7134 completed (loss: 0.09842199087142944, acc: 0.9691252112388611)
[2025-02-13 03:01:39,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:39,348][root][INFO] - Training Epoch: 1/2, step 4156/7134 completed (loss: 0.09291639924049377, acc: 0.9638386368751526)
[2025-02-13 03:01:39,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:39,745][root][INFO] - Training Epoch: 1/2, step 4157/7134 completed (loss: 0.07640653103590012, acc: 0.9740518927574158)
[2025-02-13 03:01:39,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:40,116][root][INFO] - Training Epoch: 1/2, step 4158/7134 completed (loss: 0.23567557334899902, acc: 0.9434447288513184)
[2025-02-13 03:01:40,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:40,550][root][INFO] - Training Epoch: 1/2, step 4159/7134 completed (loss: 0.08713412284851074, acc: 0.9694397449493408)
[2025-02-13 03:01:40,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:40,995][root][INFO] - Training Epoch: 1/2, step 4160/7134 completed (loss: 0.1544312983751297, acc: 0.9641509652137756)
[2025-02-13 03:01:41,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:41,449][root][INFO] - Training Epoch: 1/2, step 4161/7134 completed (loss: 0.08877916634082794, acc: 0.977911651134491)
[2025-02-13 03:01:41,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:41,861][root][INFO] - Training Epoch: 1/2, step 4162/7134 completed (loss: 0.1029646024107933, acc: 0.9761549830436707)
[2025-02-13 03:01:41,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:42,288][root][INFO] - Training Epoch: 1/2, step 4163/7134 completed (loss: 0.13325735926628113, acc: 0.9667171239852905)
[2025-02-13 03:01:42,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:42,701][root][INFO] - Training Epoch: 1/2, step 4164/7134 completed (loss: 0.0869215726852417, acc: 0.9825834631919861)
[2025-02-13 03:01:42,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:43,157][root][INFO] - Training Epoch: 1/2, step 4165/7134 completed (loss: 0.08699031919240952, acc: 0.9787765145301819)
[2025-02-13 03:01:43,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:43,591][root][INFO] - Training Epoch: 1/2, step 4166/7134 completed (loss: 0.07234950363636017, acc: 0.9823788404464722)
[2025-02-13 03:01:43,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:44,007][root][INFO] - Training Epoch: 1/2, step 4167/7134 completed (loss: 0.04852014407515526, acc: 0.984375)
[2025-02-13 03:01:44,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:44,445][root][INFO] - Training Epoch: 1/2, step 4168/7134 completed (loss: 0.059833161532878876, acc: 0.984375)
[2025-02-13 03:01:44,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:44,919][root][INFO] - Training Epoch: 1/2, step 4169/7134 completed (loss: 0.08797768503427505, acc: 0.9831045269966125)
[2025-02-13 03:01:45,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:45,397][root][INFO] - Training Epoch: 1/2, step 4170/7134 completed (loss: 0.14176134765148163, acc: 0.9554139971733093)
[2025-02-13 03:01:45,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:45,881][root][INFO] - Training Epoch: 1/2, step 4171/7134 completed (loss: 0.048919159919023514, acc: 0.979238748550415)
[2025-02-13 03:01:46,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:46,309][root][INFO] - Training Epoch: 1/2, step 4172/7134 completed (loss: 0.07833784073591232, acc: 0.9812967777252197)
[2025-02-13 03:01:46,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:46,704][root][INFO] - Training Epoch: 1/2, step 4173/7134 completed (loss: 0.09378174692392349, acc: 0.9806867241859436)
[2025-02-13 03:01:46,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:47,098][root][INFO] - Training Epoch: 1/2, step 4174/7134 completed (loss: 0.024591343477368355, acc: 0.99042147397995)
[2025-02-13 03:01:47,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:47,507][root][INFO] - Training Epoch: 1/2, step 4175/7134 completed (loss: 0.0352872833609581, acc: 0.993127167224884)
[2025-02-13 03:01:47,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:47,943][root][INFO] - Training Epoch: 1/2, step 4176/7134 completed (loss: 0.030866147950291634, acc: 0.9910581111907959)
[2025-02-13 03:01:48,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:48,409][root][INFO] - Training Epoch: 1/2, step 4177/7134 completed (loss: 0.029056517407298088, acc: 0.9934297204017639)
[2025-02-13 03:01:48,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:48,850][root][INFO] - Training Epoch: 1/2, step 4178/7134 completed (loss: 0.041464004665613174, acc: 0.9879153966903687)
[2025-02-13 03:01:49,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:49,323][root][INFO] - Training Epoch: 1/2, step 4179/7134 completed (loss: 0.031726591289043427, acc: 0.9895012974739075)
[2025-02-13 03:01:49,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:49,765][root][INFO] - Training Epoch: 1/2, step 4180/7134 completed (loss: 0.02570929005742073, acc: 0.991525411605835)
[2025-02-13 03:01:49,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:50,188][root][INFO] - Training Epoch: 1/2, step 4181/7134 completed (loss: 0.021134093403816223, acc: 0.9967373609542847)
[2025-02-13 03:01:50,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:50,616][root][INFO] - Training Epoch: 1/2, step 4182/7134 completed (loss: 0.015820251777768135, acc: 0.9958275556564331)
[2025-02-13 03:01:50,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:51,075][root][INFO] - Training Epoch: 1/2, step 4183/7134 completed (loss: 0.02730812132358551, acc: 0.9902642369270325)
[2025-02-13 03:01:51,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:51,485][root][INFO] - Training Epoch: 1/2, step 4184/7134 completed (loss: 0.054869238287210464, acc: 0.9840849041938782)
[2025-02-13 03:01:51,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:51,923][root][INFO] - Training Epoch: 1/2, step 4185/7134 completed (loss: 0.024048607796430588, acc: 0.9931507110595703)
[2025-02-13 03:01:52,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:52,384][root][INFO] - Training Epoch: 1/2, step 4186/7134 completed (loss: 0.08843453973531723, acc: 0.9831528067588806)
[2025-02-13 03:01:52,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:52,785][root][INFO] - Training Epoch: 1/2, step 4187/7134 completed (loss: 0.011636457405984402, acc: 0.99609375)
[2025-02-13 03:01:52,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:53,223][root][INFO] - Training Epoch: 1/2, step 4188/7134 completed (loss: 0.014107867144048214, acc: 0.9950000047683716)
[2025-02-13 03:01:53,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:53,666][root][INFO] - Training Epoch: 1/2, step 4189/7134 completed (loss: 0.02978353761136532, acc: 0.9916083812713623)
[2025-02-13 03:01:53,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:54,095][root][INFO] - Training Epoch: 1/2, step 4190/7134 completed (loss: 0.030344940721988678, acc: 0.9943181872367859)
[2025-02-13 03:01:54,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:54,416][root][INFO] - Training Epoch: 1/2, step 4191/7134 completed (loss: 0.05735376849770546, acc: 0.9861751198768616)
[2025-02-13 03:01:54,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:54,842][root][INFO] - Training Epoch: 1/2, step 4192/7134 completed (loss: 0.022594578564167023, acc: 0.9920127987861633)
[2025-02-13 03:01:54,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:55,284][root][INFO] - Training Epoch: 1/2, step 4193/7134 completed (loss: 0.04756541550159454, acc: 0.9866310358047485)
[2025-02-13 03:01:55,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:55,724][root][INFO] - Training Epoch: 1/2, step 4194/7134 completed (loss: 0.010486072860658169, acc: 0.9954614043235779)
[2025-02-13 03:01:55,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:56,131][root][INFO] - Training Epoch: 1/2, step 4195/7134 completed (loss: 0.04868068918585777, acc: 0.989159882068634)
[2025-02-13 03:01:56,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:56,520][root][INFO] - Training Epoch: 1/2, step 4196/7134 completed (loss: 0.03148331865668297, acc: 0.9896193742752075)
[2025-02-13 03:01:56,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:56,972][root][INFO] - Training Epoch: 1/2, step 4197/7134 completed (loss: 0.04782147333025932, acc: 0.9870634078979492)
[2025-02-13 03:01:57,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:57,408][root][INFO] - Training Epoch: 1/2, step 4198/7134 completed (loss: 0.02752143330872059, acc: 0.9972222447395325)
[2025-02-13 03:01:57,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:57,795][root][INFO] - Training Epoch: 1/2, step 4199/7134 completed (loss: 0.05641058832406998, acc: 0.9835766553878784)
[2025-02-13 03:01:57,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:58,193][root][INFO] - Training Epoch: 1/2, step 4200/7134 completed (loss: 0.04254641756415367, acc: 0.9892473220825195)
[2025-02-13 03:01:58,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:58,609][root][INFO] - Training Epoch: 1/2, step 4201/7134 completed (loss: 0.033031679689884186, acc: 0.9931507110595703)
[2025-02-13 03:01:58,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:59,014][root][INFO] - Training Epoch: 1/2, step 4202/7134 completed (loss: 0.06877080351114273, acc: 0.9844961166381836)
[2025-02-13 03:01:59,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:59,468][root][INFO] - Training Epoch: 1/2, step 4203/7134 completed (loss: 0.05627838522195816, acc: 0.9828392863273621)
[2025-02-13 03:01:59,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:59,874][root][INFO] - Training Epoch: 1/2, step 4204/7134 completed (loss: 0.11137948930263519, acc: 0.9663742780685425)
[2025-02-13 03:02:00,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:00,268][root][INFO] - Training Epoch: 1/2, step 4205/7134 completed (loss: 0.07376623153686523, acc: 0.9798561334609985)
[2025-02-13 03:02:00,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:00,711][root][INFO] - Training Epoch: 1/2, step 4206/7134 completed (loss: 0.08937615901231766, acc: 0.9788434505462646)
[2025-02-13 03:02:00,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:01,126][root][INFO] - Training Epoch: 1/2, step 4207/7134 completed (loss: 0.09620381891727448, acc: 0.9840764403343201)
[2025-02-13 03:02:01,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:01,519][root][INFO] - Training Epoch: 1/2, step 4208/7134 completed (loss: 0.0989360511302948, acc: 0.9776714444160461)
[2025-02-13 03:02:01,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:01,931][root][INFO] - Training Epoch: 1/2, step 4209/7134 completed (loss: 0.10140355676412582, acc: 0.980169951915741)
[2025-02-13 03:02:02,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:02,358][root][INFO] - Training Epoch: 1/2, step 4210/7134 completed (loss: 0.012267989106476307, acc: 0.9979550242424011)
[2025-02-13 03:02:02,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:02,608][root][INFO] - Training Epoch: 1/2, step 4211/7134 completed (loss: 0.04667958617210388, acc: 0.9907833933830261)
[2025-02-13 03:02:02,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:02,951][root][INFO] - Training Epoch: 1/2, step 4212/7134 completed (loss: 0.06777417659759521, acc: 0.978723406791687)
[2025-02-13 03:02:03,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:03,392][root][INFO] - Training Epoch: 1/2, step 4213/7134 completed (loss: 0.03904328867793083, acc: 0.9871244430541992)
[2025-02-13 03:02:03,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:03,840][root][INFO] - Training Epoch: 1/2, step 4214/7134 completed (loss: 0.08615124970674515, acc: 0.9791044592857361)
[2025-02-13 03:02:03,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:04,221][root][INFO] - Training Epoch: 1/2, step 4215/7134 completed (loss: 0.05859052762389183, acc: 0.9868131875991821)
[2025-02-13 03:02:04,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:04,623][root][INFO] - Training Epoch: 1/2, step 4216/7134 completed (loss: 0.06504829972982407, acc: 0.9817184805870056)
[2025-02-13 03:02:04,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:05,010][root][INFO] - Training Epoch: 1/2, step 4217/7134 completed (loss: 0.028984781354665756, acc: 0.988811194896698)
[2025-02-13 03:02:05,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:05,399][root][INFO] - Training Epoch: 1/2, step 4218/7134 completed (loss: 0.06272564083337784, acc: 0.9876288771629333)
[2025-02-13 03:02:05,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:05,688][root][INFO] - Training Epoch: 1/2, step 4219/7134 completed (loss: 0.054426275193691254, acc: 0.9879518151283264)
[2025-02-13 03:02:05,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:06,125][root][INFO] - Training Epoch: 1/2, step 4220/7134 completed (loss: 0.04606940224766731, acc: 0.9899280667304993)
[2025-02-13 03:02:06,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:06,562][root][INFO] - Training Epoch: 1/2, step 4221/7134 completed (loss: 0.036787085235118866, acc: 0.991150438785553)
[2025-02-13 03:02:06,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:06,938][root][INFO] - Training Epoch: 1/2, step 4222/7134 completed (loss: 0.17072108387947083, acc: 0.9733333587646484)
[2025-02-13 03:02:07,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:07,339][root][INFO] - Training Epoch: 1/2, step 4223/7134 completed (loss: 0.07290636748075485, acc: 0.9801324605941772)
[2025-02-13 03:02:07,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:07,734][root][INFO] - Training Epoch: 1/2, step 4224/7134 completed (loss: 0.0815969705581665, acc: 0.9762418866157532)
[2025-02-13 03:02:07,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:08,160][root][INFO] - Training Epoch: 1/2, step 4225/7134 completed (loss: 0.1270361691713333, acc: 0.9779286980628967)
[2025-02-13 03:02:08,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:08,598][root][INFO] - Training Epoch: 1/2, step 4226/7134 completed (loss: 0.07682664692401886, acc: 0.9795657992362976)
[2025-02-13 03:02:08,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:08,957][root][INFO] - Training Epoch: 1/2, step 4227/7134 completed (loss: 0.12526080012321472, acc: 0.9705159664154053)
[2025-02-13 03:02:09,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:09,361][root][INFO] - Training Epoch: 1/2, step 4228/7134 completed (loss: 0.17014309763908386, acc: 0.9620689749717712)
[2025-02-13 03:02:09,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:09,761][root][INFO] - Training Epoch: 1/2, step 4229/7134 completed (loss: 0.1115124300122261, acc: 0.9790356159210205)
[2025-02-13 03:02:09,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:10,064][root][INFO] - Training Epoch: 1/2, step 4230/7134 completed (loss: 0.08798816055059433, acc: 0.9735293984413147)
[2025-02-13 03:02:10,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:10,487][root][INFO] - Training Epoch: 1/2, step 4231/7134 completed (loss: 0.19061043858528137, acc: 0.9558011293411255)
[2025-02-13 03:02:10,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:10,902][root][INFO] - Training Epoch: 1/2, step 4232/7134 completed (loss: 0.12031377851963043, acc: 0.9648854732513428)
[2025-02-13 03:02:11,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:11,314][root][INFO] - Training Epoch: 1/2, step 4233/7134 completed (loss: 0.12095588445663452, acc: 0.9716193675994873)
[2025-02-13 03:02:11,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:11,719][root][INFO] - Training Epoch: 1/2, step 4234/7134 completed (loss: 0.07934112846851349, acc: 0.9825119376182556)
[2025-02-13 03:02:11,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:12,154][root][INFO] - Training Epoch: 1/2, step 4235/7134 completed (loss: 0.05319846794009209, acc: 0.9842022061347961)
[2025-02-13 03:02:12,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:12,541][root][INFO] - Training Epoch: 1/2, step 4236/7134 completed (loss: 0.02932715229690075, acc: 0.997474730014801)
[2025-02-13 03:02:12,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:12,943][root][INFO] - Training Epoch: 1/2, step 4237/7134 completed (loss: 0.06872045993804932, acc: 0.9814432859420776)
[2025-02-13 03:02:13,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:13,362][root][INFO] - Training Epoch: 1/2, step 4238/7134 completed (loss: 0.07668844610452652, acc: 0.9803600907325745)
[2025-02-13 03:02:13,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:13,750][root][INFO] - Training Epoch: 1/2, step 4239/7134 completed (loss: 0.04057253897190094, acc: 0.9872000217437744)
[2025-02-13 03:02:13,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:14,146][root][INFO] - Training Epoch: 1/2, step 4240/7134 completed (loss: 0.04998043179512024, acc: 0.9886914491653442)
[2025-02-13 03:02:14,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:14,535][root][INFO] - Training Epoch: 1/2, step 4241/7134 completed (loss: 0.019851578399538994, acc: 0.9956616163253784)
[2025-02-13 03:02:14,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:14,893][root][INFO] - Training Epoch: 1/2, step 4242/7134 completed (loss: 0.06709857285022736, acc: 0.9799554347991943)
[2025-02-13 03:02:15,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:15,316][root][INFO] - Training Epoch: 1/2, step 4243/7134 completed (loss: 0.08764436095952988, acc: 0.9819168448448181)
[2025-02-13 03:02:15,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:15,660][root][INFO] - Training Epoch: 1/2, step 4244/7134 completed (loss: 0.05782921612262726, acc: 0.9822616577148438)
[2025-02-13 03:02:15,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:16,001][root][INFO] - Training Epoch: 1/2, step 4245/7134 completed (loss: 0.052569180727005005, acc: 0.9902200698852539)
[2025-02-13 03:02:16,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:16,414][root][INFO] - Training Epoch: 1/2, step 4246/7134 completed (loss: 0.057443633675575256, acc: 0.9857369065284729)
[2025-02-13 03:02:16,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:16,809][root][INFO] - Training Epoch: 1/2, step 4247/7134 completed (loss: 0.053577519953250885, acc: 0.9821746945381165)
[2025-02-13 03:02:16,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:17,221][root][INFO] - Training Epoch: 1/2, step 4248/7134 completed (loss: 0.07929842174053192, acc: 0.9775640964508057)
[2025-02-13 03:02:17,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:17,678][root][INFO] - Training Epoch: 1/2, step 4249/7134 completed (loss: 0.0884961411356926, acc: 0.9825396537780762)
[2025-02-13 03:02:17,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:18,010][root][INFO] - Training Epoch: 1/2, step 4250/7134 completed (loss: 0.09461652487516403, acc: 0.9830148816108704)
[2025-02-13 03:02:18,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:18,414][root][INFO] - Training Epoch: 1/2, step 4251/7134 completed (loss: 0.03931783139705658, acc: 0.9901315569877625)
[2025-02-13 03:02:18,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:18,820][root][INFO] - Training Epoch: 1/2, step 4252/7134 completed (loss: 0.054532960057258606, acc: 0.9841017723083496)
[2025-02-13 03:02:18,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:19,230][root][INFO] - Training Epoch: 1/2, step 4253/7134 completed (loss: 0.08025220781564713, acc: 0.9804772138595581)
[2025-02-13 03:02:19,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:19,607][root][INFO] - Training Epoch: 1/2, step 4254/7134 completed (loss: 0.03708591312170029, acc: 0.9925187230110168)
[2025-02-13 03:02:19,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:20,041][root][INFO] - Training Epoch: 1/2, step 4255/7134 completed (loss: 0.06099383533000946, acc: 0.9811643958091736)
[2025-02-13 03:02:20,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:20,438][root][INFO] - Training Epoch: 1/2, step 4256/7134 completed (loss: 0.05295968055725098, acc: 0.9841628670692444)
[2025-02-13 03:02:20,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:20,846][root][INFO] - Training Epoch: 1/2, step 4257/7134 completed (loss: 0.03079075738787651, acc: 0.9928698539733887)
[2025-02-13 03:02:20,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:21,256][root][INFO] - Training Epoch: 1/2, step 4258/7134 completed (loss: 0.022044984623789787, acc: 0.9942747950553894)
[2025-02-13 03:02:21,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:21,691][root][INFO] - Training Epoch: 1/2, step 4259/7134 completed (loss: 0.09057672321796417, acc: 0.9742120504379272)
[2025-02-13 03:02:21,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:22,112][root][INFO] - Training Epoch: 1/2, step 4260/7134 completed (loss: 0.03822070732712746, acc: 0.9855072498321533)
[2025-02-13 03:02:22,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:22,506][root][INFO] - Training Epoch: 1/2, step 4261/7134 completed (loss: 0.04523714631795883, acc: 0.9847036600112915)
[2025-02-13 03:02:22,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:22,928][root][INFO] - Training Epoch: 1/2, step 4262/7134 completed (loss: 0.04420832544565201, acc: 0.9874804615974426)
[2025-02-13 03:02:23,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:23,339][root][INFO] - Training Epoch: 1/2, step 4263/7134 completed (loss: 0.08937152475118637, acc: 0.9847972989082336)
[2025-02-13 03:02:23,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:23,745][root][INFO] - Training Epoch: 1/2, step 4264/7134 completed (loss: 0.07222887873649597, acc: 0.9762202501296997)
[2025-02-13 03:02:23,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:24,173][root][INFO] - Training Epoch: 1/2, step 4265/7134 completed (loss: 0.04401366040110588, acc: 0.9861111044883728)
[2025-02-13 03:02:24,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:24,629][root][INFO] - Training Epoch: 1/2, step 4266/7134 completed (loss: 0.037747565656900406, acc: 0.9916167855262756)
[2025-02-13 03:02:24,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:25,089][root][INFO] - Training Epoch: 1/2, step 4267/7134 completed (loss: 0.04916933551430702, acc: 0.9866666793823242)
[2025-02-13 03:02:25,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:25,513][root][INFO] - Training Epoch: 1/2, step 4268/7134 completed (loss: 0.13754266500473022, acc: 0.9639278650283813)
[2025-02-13 03:02:25,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:25,963][root][INFO] - Training Epoch: 1/2, step 4269/7134 completed (loss: 0.1758345514535904, acc: 0.9553956985473633)
[2025-02-13 03:02:26,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:26,395][root][INFO] - Training Epoch: 1/2, step 4270/7134 completed (loss: 0.07875265926122665, acc: 0.9765625)
[2025-02-13 03:02:26,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:26,836][root][INFO] - Training Epoch: 1/2, step 4271/7134 completed (loss: 0.05202551186084747, acc: 0.9825654029846191)
[2025-02-13 03:02:26,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:27,299][root][INFO] - Training Epoch: 1/2, step 4272/7134 completed (loss: 0.04873013496398926, acc: 0.9847418069839478)
[2025-02-13 03:02:27,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:27,744][root][INFO] - Training Epoch: 1/2, step 4273/7134 completed (loss: 0.05512956157326698, acc: 0.9849435091018677)
[2025-02-13 03:02:27,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:28,206][root][INFO] - Training Epoch: 1/2, step 4274/7134 completed (loss: 0.10377593338489532, acc: 0.9719626307487488)
[2025-02-13 03:02:28,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:28,637][root][INFO] - Training Epoch: 1/2, step 4275/7134 completed (loss: 0.041805706918239594, acc: 0.9873737096786499)
[2025-02-13 03:02:28,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:29,076][root][INFO] - Training Epoch: 1/2, step 4276/7134 completed (loss: 0.07254400104284286, acc: 0.9793814420700073)
[2025-02-13 03:02:29,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:29,477][root][INFO] - Training Epoch: 1/2, step 4277/7134 completed (loss: 0.03595510125160217, acc: 0.9904761910438538)
[2025-02-13 03:02:29,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:29,912][root][INFO] - Training Epoch: 1/2, step 4278/7134 completed (loss: 0.06946293264627457, acc: 0.9814814925193787)
[2025-02-13 03:02:30,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:30,328][root][INFO] - Training Epoch: 1/2, step 4279/7134 completed (loss: 0.05456411838531494, acc: 0.9851190447807312)
[2025-02-13 03:02:30,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:30,775][root][INFO] - Training Epoch: 1/2, step 4280/7134 completed (loss: 0.027304688468575478, acc: 0.9873015880584717)
[2025-02-13 03:02:30,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:31,239][root][INFO] - Training Epoch: 1/2, step 4281/7134 completed (loss: 0.07321923971176147, acc: 0.9814356565475464)
[2025-02-13 03:02:31,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:31,671][root][INFO] - Training Epoch: 1/2, step 4282/7134 completed (loss: 0.025756031274795532, acc: 0.9925925731658936)
[2025-02-13 03:02:31,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:32,129][root][INFO] - Training Epoch: 1/2, step 4283/7134 completed (loss: 0.03749583661556244, acc: 0.9896789193153381)
[2025-02-13 03:02:32,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:32,500][root][INFO] - Training Epoch: 1/2, step 4284/7134 completed (loss: 0.06502556800842285, acc: 0.9818840622901917)
[2025-02-13 03:02:32,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:32,932][root][INFO] - Training Epoch: 1/2, step 4285/7134 completed (loss: 0.054294150322675705, acc: 0.982876718044281)
[2025-02-13 03:02:33,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:33,326][root][INFO] - Training Epoch: 1/2, step 4286/7134 completed (loss: 0.05376520752906799, acc: 0.9766187071800232)
[2025-02-13 03:02:33,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:33,756][root][INFO] - Training Epoch: 1/2, step 4287/7134 completed (loss: 0.07182604819536209, acc: 0.9806700944900513)
[2025-02-13 03:02:33,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:34,200][root][INFO] - Training Epoch: 1/2, step 4288/7134 completed (loss: 0.07240170240402222, acc: 0.9815602898597717)
[2025-02-13 03:02:34,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:34,659][root][INFO] - Training Epoch: 1/2, step 4289/7134 completed (loss: 0.08396419137716293, acc: 0.9759398698806763)
[2025-02-13 03:02:34,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:35,099][root][INFO] - Training Epoch: 1/2, step 4290/7134 completed (loss: 0.04531767591834068, acc: 0.9851149916648865)
[2025-02-13 03:02:35,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:35,535][root][INFO] - Training Epoch: 1/2, step 4291/7134 completed (loss: 0.036891963332891464, acc: 0.9880810379981995)
[2025-02-13 03:02:35,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:35,954][root][INFO] - Training Epoch: 1/2, step 4292/7134 completed (loss: 0.03569861128926277, acc: 0.990963876247406)
[2025-02-13 03:02:36,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:36,359][root][INFO] - Training Epoch: 1/2, step 4293/7134 completed (loss: 0.034235741943120956, acc: 0.9879032373428345)
[2025-02-13 03:02:36,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:36,768][root][INFO] - Training Epoch: 1/2, step 4294/7134 completed (loss: 0.05323725938796997, acc: 0.9883551597595215)
[2025-02-13 03:02:36,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:37,197][root][INFO] - Training Epoch: 1/2, step 4295/7134 completed (loss: 0.03488798066973686, acc: 0.989051103591919)
[2025-02-13 03:02:37,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:37,605][root][INFO] - Training Epoch: 1/2, step 4296/7134 completed (loss: 0.02545844204723835, acc: 0.9951338171958923)
[2025-02-13 03:02:37,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:38,015][root][INFO] - Training Epoch: 1/2, step 4297/7134 completed (loss: 0.024380099028348923, acc: 0.9976525902748108)
[2025-02-13 03:02:38,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:38,436][root][INFO] - Training Epoch: 1/2, step 4298/7134 completed (loss: 0.042454566806554794, acc: 0.9876203536987305)
[2025-02-13 03:02:38,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:38,832][root][INFO] - Training Epoch: 1/2, step 4299/7134 completed (loss: 0.042472533881664276, acc: 0.9871060252189636)
[2025-02-13 03:02:38,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:39,229][root][INFO] - Training Epoch: 1/2, step 4300/7134 completed (loss: 0.024049686267971992, acc: 0.9912280440330505)
[2025-02-13 03:02:39,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:39,639][root][INFO] - Training Epoch: 1/2, step 4301/7134 completed (loss: 0.07038301229476929, acc: 0.9836065769195557)
[2025-02-13 03:02:39,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:40,055][root][INFO] - Training Epoch: 1/2, step 4302/7134 completed (loss: 0.03404577076435089, acc: 0.9865671396255493)
[2025-02-13 03:02:40,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:40,491][root][INFO] - Training Epoch: 1/2, step 4303/7134 completed (loss: 0.04601277783513069, acc: 0.9856801629066467)
[2025-02-13 03:02:40,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:40,932][root][INFO] - Training Epoch: 1/2, step 4304/7134 completed (loss: 0.04519791156053543, acc: 0.9881129264831543)
[2025-02-13 03:02:41,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:41,392][root][INFO] - Training Epoch: 1/2, step 4305/7134 completed (loss: 0.05188006907701492, acc: 0.9882628917694092)
[2025-02-13 03:02:41,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:41,837][root][INFO] - Training Epoch: 1/2, step 4306/7134 completed (loss: 0.08930420875549316, acc: 0.9816031455993652)
[2025-02-13 03:02:41,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:42,268][root][INFO] - Training Epoch: 1/2, step 4307/7134 completed (loss: 0.08605636656284332, acc: 0.9759229421615601)
[2025-02-13 03:02:42,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:42,721][root][INFO] - Training Epoch: 1/2, step 4308/7134 completed (loss: 0.04868929088115692, acc: 0.9849187731742859)
[2025-02-13 03:02:42,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:43,166][root][INFO] - Training Epoch: 1/2, step 4309/7134 completed (loss: 0.03928327560424805, acc: 0.9901356101036072)
[2025-02-13 03:02:43,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:43,603][root][INFO] - Training Epoch: 1/2, step 4310/7134 completed (loss: 0.03165077045559883, acc: 0.9941245317459106)
[2025-02-13 03:02:43,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:44,012][root][INFO] - Training Epoch: 1/2, step 4311/7134 completed (loss: 0.02311769314110279, acc: 0.9942029118537903)
[2025-02-13 03:02:44,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:44,418][root][INFO] - Training Epoch: 1/2, step 4312/7134 completed (loss: 0.024105129763484, acc: 0.9931880235671997)
[2025-02-13 03:02:44,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:44,832][root][INFO] - Training Epoch: 1/2, step 4313/7134 completed (loss: 0.03940580412745476, acc: 0.9922077655792236)
[2025-02-13 03:02:44,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:45,248][root][INFO] - Training Epoch: 1/2, step 4314/7134 completed (loss: 0.043800003826618195, acc: 0.9864048361778259)
[2025-02-13 03:02:45,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:45,680][root][INFO] - Training Epoch: 1/2, step 4315/7134 completed (loss: 0.058156080543994904, acc: 0.9803921580314636)
[2025-02-13 03:02:45,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:46,115][root][INFO] - Training Epoch: 1/2, step 4316/7134 completed (loss: 0.04434603080153465, acc: 0.9872449040412903)
[2025-02-13 03:02:46,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:46,549][root][INFO] - Training Epoch: 1/2, step 4317/7134 completed (loss: 0.046429168432950974, acc: 0.986369252204895)
[2025-02-13 03:02:46,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:46,980][root][INFO] - Training Epoch: 1/2, step 4318/7134 completed (loss: 0.03450918570160866, acc: 0.9899103045463562)
[2025-02-13 03:02:47,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:47,444][root][INFO] - Training Epoch: 1/2, step 4319/7134 completed (loss: 0.02163904905319214, acc: 0.9910813570022583)
[2025-02-13 03:02:47,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:47,849][root][INFO] - Training Epoch: 1/2, step 4320/7134 completed (loss: 0.035084716975688934, acc: 0.9878378510475159)
[2025-02-13 03:02:47,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:48,204][root][INFO] - Training Epoch: 1/2, step 4321/7134 completed (loss: 0.05859041213989258, acc: 0.9838337302207947)
[2025-02-13 03:02:48,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:48,641][root][INFO] - Training Epoch: 1/2, step 4322/7134 completed (loss: 0.07261795550584793, acc: 0.9874213933944702)
[2025-02-13 03:02:48,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:49,035][root][INFO] - Training Epoch: 1/2, step 4323/7134 completed (loss: 0.06308211386203766, acc: 0.9807322025299072)
[2025-02-13 03:02:49,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:49,419][root][INFO] - Training Epoch: 1/2, step 4324/7134 completed (loss: 0.0216510072350502, acc: 0.9922480583190918)
[2025-02-13 03:02:49,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:49,831][root][INFO] - Training Epoch: 1/2, step 4325/7134 completed (loss: 0.03156513720750809, acc: 0.9920254945755005)
[2025-02-13 03:02:49,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:50,245][root][INFO] - Training Epoch: 1/2, step 4326/7134 completed (loss: 0.056854233145713806, acc: 0.9823943376541138)
[2025-02-13 03:02:50,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:50,633][root][INFO] - Training Epoch: 1/2, step 4327/7134 completed (loss: 0.037314273416996, acc: 0.9860140085220337)
[2025-02-13 03:02:50,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:51,027][root][INFO] - Training Epoch: 1/2, step 4328/7134 completed (loss: 0.06915774941444397, acc: 0.981203019618988)
[2025-02-13 03:02:51,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:51,471][root][INFO] - Training Epoch: 1/2, step 4329/7134 completed (loss: 0.04607038199901581, acc: 0.9921362996101379)
[2025-02-13 03:02:51,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:51,800][root][INFO] - Training Epoch: 1/2, step 4330/7134 completed (loss: 0.08087698370218277, acc: 0.9808102250099182)
[2025-02-13 03:02:51,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:52,172][root][INFO] - Training Epoch: 1/2, step 4331/7134 completed (loss: 0.02542133443057537, acc: 0.9915013909339905)
[2025-02-13 03:02:52,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:52,579][root][INFO] - Training Epoch: 1/2, step 4332/7134 completed (loss: 0.06876865774393082, acc: 0.9790874719619751)
[2025-02-13 03:02:52,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:52,988][root][INFO] - Training Epoch: 1/2, step 4333/7134 completed (loss: 0.04385019838809967, acc: 0.9848254919052124)
[2025-02-13 03:02:53,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:53,428][root][INFO] - Training Epoch: 1/2, step 4334/7134 completed (loss: 0.06756501644849777, acc: 0.9805389046669006)
[2025-02-13 03:02:53,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:53,828][root][INFO] - Training Epoch: 1/2, step 4335/7134 completed (loss: 0.06403237581253052, acc: 0.9838709831237793)
[2025-02-13 03:02:53,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:54,239][root][INFO] - Training Epoch: 1/2, step 4336/7134 completed (loss: 0.024330873042345047, acc: 0.9886731505393982)
[2025-02-13 03:02:54,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:54,645][root][INFO] - Training Epoch: 1/2, step 4337/7134 completed (loss: 0.03302309662103653, acc: 0.9889240264892578)
[2025-02-13 03:02:54,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:55,048][root][INFO] - Training Epoch: 1/2, step 4338/7134 completed (loss: 0.022725122049450874, acc: 0.9952606558799744)
[2025-02-13 03:02:55,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:55,458][root][INFO] - Training Epoch: 1/2, step 4339/7134 completed (loss: 0.06192612648010254, acc: 0.9839650392532349)
[2025-02-13 03:02:55,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:55,907][root][INFO] - Training Epoch: 1/2, step 4340/7134 completed (loss: 0.029967082664370537, acc: 0.9915540814399719)
[2025-02-13 03:02:56,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:56,321][root][INFO] - Training Epoch: 1/2, step 4341/7134 completed (loss: 0.043960507959127426, acc: 0.9866270422935486)
[2025-02-13 03:02:56,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:56,771][root][INFO] - Training Epoch: 1/2, step 4342/7134 completed (loss: 0.047098785638809204, acc: 0.9839743375778198)
[2025-02-13 03:02:56,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:57,191][root][INFO] - Training Epoch: 1/2, step 4343/7134 completed (loss: 0.030066492035984993, acc: 0.9933444261550903)
[2025-02-13 03:02:57,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:57,549][root][INFO] - Training Epoch: 1/2, step 4344/7134 completed (loss: 0.01861075684428215, acc: 0.9979209899902344)
[2025-02-13 03:02:57,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:57,909][root][INFO] - Training Epoch: 1/2, step 4345/7134 completed (loss: 0.03562312573194504, acc: 0.9875776171684265)
[2025-02-13 03:02:58,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:58,274][root][INFO] - Training Epoch: 1/2, step 4346/7134 completed (loss: 0.020075827836990356, acc: 0.9937888383865356)
[2025-02-13 03:02:58,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:58,683][root][INFO] - Training Epoch: 1/2, step 4347/7134 completed (loss: 0.07880668342113495, acc: 0.9801980257034302)
[2025-02-13 03:02:58,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:59,068][root][INFO] - Training Epoch: 1/2, step 4348/7134 completed (loss: 0.02954866550862789, acc: 0.9919999837875366)
[2025-02-13 03:02:59,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:59,410][root][INFO] - Training Epoch: 1/2, step 4349/7134 completed (loss: 0.01831357553601265, acc: 0.9930716156959534)
[2025-02-13 03:02:59,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:59,843][root][INFO] - Training Epoch: 1/2, step 4350/7134 completed (loss: 0.03758615627884865, acc: 0.9908376932144165)
[2025-02-13 03:02:59,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:00,256][root][INFO] - Training Epoch: 1/2, step 4351/7134 completed (loss: 0.09010186046361923, acc: 0.9794238805770874)
[2025-02-13 03:03:00,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:00,691][root][INFO] - Training Epoch: 1/2, step 4352/7134 completed (loss: 0.06598355621099472, acc: 0.9759188890457153)
[2025-02-13 03:03:00,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:01,131][root][INFO] - Training Epoch: 1/2, step 4353/7134 completed (loss: 0.05951413884758949, acc: 0.9801324605941772)
[2025-02-13 03:03:01,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:01,557][root][INFO] - Training Epoch: 1/2, step 4354/7134 completed (loss: 0.039393506944179535, acc: 0.9853333234786987)
[2025-02-13 03:03:01,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:01,968][root][INFO] - Training Epoch: 1/2, step 4355/7134 completed (loss: 0.03776835650205612, acc: 0.991239070892334)
[2025-02-13 03:03:02,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:02,410][root][INFO] - Training Epoch: 1/2, step 4356/7134 completed (loss: 0.0466778539121151, acc: 0.9844683408737183)
[2025-02-13 03:03:02,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:02,843][root][INFO] - Training Epoch: 1/2, step 4357/7134 completed (loss: 0.07224535942077637, acc: 0.9795275330543518)
[2025-02-13 03:03:02,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:03,293][root][INFO] - Training Epoch: 1/2, step 4358/7134 completed (loss: 0.05662797763943672, acc: 0.9796407222747803)
[2025-02-13 03:03:03,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:03,710][root][INFO] - Training Epoch: 1/2, step 4359/7134 completed (loss: 0.07020162791013718, acc: 0.9778130054473877)
[2025-02-13 03:03:03,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:04,101][root][INFO] - Training Epoch: 1/2, step 4360/7134 completed (loss: 0.07467364519834518, acc: 0.9776536226272583)
[2025-02-13 03:03:04,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:04,527][root][INFO] - Training Epoch: 1/2, step 4361/7134 completed (loss: 0.03898446261882782, acc: 0.986994206905365)
[2025-02-13 03:03:04,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:04,967][root][INFO] - Training Epoch: 1/2, step 4362/7134 completed (loss: 0.04613867402076721, acc: 0.9839704036712646)
[2025-02-13 03:03:05,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:05,402][root][INFO] - Training Epoch: 1/2, step 4363/7134 completed (loss: 0.05014681816101074, acc: 0.9851239919662476)
[2025-02-13 03:03:05,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:05,795][root][INFO] - Training Epoch: 1/2, step 4364/7134 completed (loss: 0.029446681961417198, acc: 0.989062488079071)
[2025-02-13 03:03:05,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:06,213][root][INFO] - Training Epoch: 1/2, step 4365/7134 completed (loss: 0.04054871201515198, acc: 0.9874213933944702)
[2025-02-13 03:03:06,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:06,658][root][INFO] - Training Epoch: 1/2, step 4366/7134 completed (loss: 0.07173724472522736, acc: 0.9742268323898315)
[2025-02-13 03:03:06,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:07,089][root][INFO] - Training Epoch: 1/2, step 4367/7134 completed (loss: 0.044869303703308105, acc: 0.984240710735321)
[2025-02-13 03:03:07,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:07,536][root][INFO] - Training Epoch: 1/2, step 4368/7134 completed (loss: 0.025614002719521523, acc: 0.99370276927948)
[2025-02-13 03:03:07,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:07,953][root][INFO] - Training Epoch: 1/2, step 4369/7134 completed (loss: 0.032409995794296265, acc: 0.9866666793823242)
[2025-02-13 03:03:08,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:08,407][root][INFO] - Training Epoch: 1/2, step 4370/7134 completed (loss: 0.052056826651096344, acc: 0.9886506795883179)
[2025-02-13 03:03:08,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:08,848][root][INFO] - Training Epoch: 1/2, step 4371/7134 completed (loss: 0.05317091569304466, acc: 0.9828495979309082)
[2025-02-13 03:03:08,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:09,259][root][INFO] - Training Epoch: 1/2, step 4372/7134 completed (loss: 0.04446778818964958, acc: 0.9879518151283264)
[2025-02-13 03:03:09,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:09,685][root][INFO] - Training Epoch: 1/2, step 4373/7134 completed (loss: 0.029119832441210747, acc: 0.9881094098091125)
[2025-02-13 03:03:09,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:10,106][root][INFO] - Training Epoch: 1/2, step 4374/7134 completed (loss: 0.048356831073760986, acc: 0.9856114983558655)
[2025-02-13 03:03:10,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:10,542][root][INFO] - Training Epoch: 1/2, step 4375/7134 completed (loss: 0.02960095927119255, acc: 0.9897040128707886)
[2025-02-13 03:03:10,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:10,934][root][INFO] - Training Epoch: 1/2, step 4376/7134 completed (loss: 0.04933098331093788, acc: 0.9886877536773682)
[2025-02-13 03:03:11,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:11,371][root][INFO] - Training Epoch: 1/2, step 4377/7134 completed (loss: 0.06721169501543045, acc: 0.988875150680542)
[2025-02-13 03:03:11,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:11,796][root][INFO] - Training Epoch: 1/2, step 4378/7134 completed (loss: 0.06215411424636841, acc: 0.9852941036224365)
[2025-02-13 03:03:11,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:12,251][root][INFO] - Training Epoch: 1/2, step 4379/7134 completed (loss: 0.10614745318889618, acc: 0.9678030014038086)
[2025-02-13 03:03:12,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:12,641][root][INFO] - Training Epoch: 1/2, step 4380/7134 completed (loss: 0.060516390949487686, acc: 0.985989511013031)
[2025-02-13 03:03:12,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:13,086][root][INFO] - Training Epoch: 1/2, step 4381/7134 completed (loss: 0.029450982809066772, acc: 0.9889655113220215)
[2025-02-13 03:03:13,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:13,479][root][INFO] - Training Epoch: 1/2, step 4382/7134 completed (loss: 0.02920255996286869, acc: 0.9920508861541748)
[2025-02-13 03:03:13,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:13,910][root][INFO] - Training Epoch: 1/2, step 4383/7134 completed (loss: 0.0653877854347229, acc: 0.9807427525520325)
[2025-02-13 03:03:14,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:14,318][root][INFO] - Training Epoch: 1/2, step 4384/7134 completed (loss: 0.03161996975541115, acc: 0.9912060499191284)
[2025-02-13 03:03:14,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:14,746][root][INFO] - Training Epoch: 1/2, step 4385/7134 completed (loss: 0.07891152054071426, acc: 0.9739243984222412)
[2025-02-13 03:03:14,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:15,182][root][INFO] - Training Epoch: 1/2, step 4386/7134 completed (loss: 0.061654459685087204, acc: 0.9810426831245422)
[2025-02-13 03:03:15,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:15,623][root][INFO] - Training Epoch: 1/2, step 4387/7134 completed (loss: 0.06417697668075562, acc: 0.9838969111442566)
[2025-02-13 03:03:15,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:16,031][root][INFO] - Training Epoch: 1/2, step 4388/7134 completed (loss: 0.050456516444683075, acc: 0.9813664555549622)
[2025-02-13 03:03:16,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:16,437][root][INFO] - Training Epoch: 1/2, step 4389/7134 completed (loss: 0.0912180095911026, acc: 0.9771615266799927)
[2025-02-13 03:03:16,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:16,882][root][INFO] - Training Epoch: 1/2, step 4390/7134 completed (loss: 0.09485353529453278, acc: 0.9761570692062378)
[2025-02-13 03:03:17,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:17,289][root][INFO] - Training Epoch: 1/2, step 4391/7134 completed (loss: 0.10034910589456558, acc: 0.9641693830490112)
[2025-02-13 03:03:17,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:17,720][root][INFO] - Training Epoch: 1/2, step 4392/7134 completed (loss: 0.038182202726602554, acc: 0.9914215803146362)
[2025-02-13 03:03:17,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:18,163][root][INFO] - Training Epoch: 1/2, step 4393/7134 completed (loss: 0.03881922364234924, acc: 0.9887482523918152)
[2025-02-13 03:03:18,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:18,610][root][INFO] - Training Epoch: 1/2, step 4394/7134 completed (loss: 0.043413881212472916, acc: 0.9873060584068298)
[2025-02-13 03:03:18,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:19,020][root][INFO] - Training Epoch: 1/2, step 4395/7134 completed (loss: 0.07300885766744614, acc: 0.971742570400238)
[2025-02-13 03:03:19,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:19,418][root][INFO] - Training Epoch: 1/2, step 4396/7134 completed (loss: 0.12262777239084244, acc: 0.9607046246528625)
[2025-02-13 03:03:19,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:19,814][root][INFO] - Training Epoch: 1/2, step 4397/7134 completed (loss: 0.06386398524045944, acc: 0.9788235425949097)
[2025-02-13 03:03:19,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:20,236][root][INFO] - Training Epoch: 1/2, step 4398/7134 completed (loss: 0.024808615446090698, acc: 0.9924242496490479)
[2025-02-13 03:03:20,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:20,646][root][INFO] - Training Epoch: 1/2, step 4399/7134 completed (loss: 0.07889090478420258, acc: 0.9807074069976807)
[2025-02-13 03:03:20,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:21,101][root][INFO] - Training Epoch: 1/2, step 4400/7134 completed (loss: 0.09806086122989655, acc: 0.9764559864997864)
[2025-02-13 03:03:21,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:21,492][root][INFO] - Training Epoch: 1/2, step 4401/7134 completed (loss: 0.10770546644926071, acc: 0.97826087474823)
[2025-02-13 03:03:21,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:21,913][root][INFO] - Training Epoch: 1/2, step 4402/7134 completed (loss: 0.16169659793376923, acc: 0.9563953280448914)
[2025-02-13 03:03:22,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:22,344][root][INFO] - Training Epoch: 1/2, step 4403/7134 completed (loss: 0.22174951434135437, acc: 0.9455958604812622)
[2025-02-13 03:03:22,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:22,758][root][INFO] - Training Epoch: 1/2, step 4404/7134 completed (loss: 0.17522525787353516, acc: 0.9507978558540344)
[2025-02-13 03:03:22,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:23,224][root][INFO] - Training Epoch: 1/2, step 4405/7134 completed (loss: 0.04628541320562363, acc: 0.988120973110199)
[2025-02-13 03:03:23,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:23,706][root][INFO] - Training Epoch: 1/2, step 4406/7134 completed (loss: 0.05121300369501114, acc: 0.9844098091125488)
[2025-02-13 03:03:23,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:24,161][root][INFO] - Training Epoch: 1/2, step 4407/7134 completed (loss: 0.07828184217214584, acc: 0.981502890586853)
[2025-02-13 03:03:24,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:24,600][root][INFO] - Training Epoch: 1/2, step 4408/7134 completed (loss: 0.11875498294830322, acc: 0.9662398099899292)
[2025-02-13 03:03:24,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:25,037][root][INFO] - Training Epoch: 1/2, step 4409/7134 completed (loss: 0.09933669120073318, acc: 0.9706601500511169)
[2025-02-13 03:03:25,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:25,425][root][INFO] - Training Epoch: 1/2, step 4410/7134 completed (loss: 0.13425861299037933, acc: 0.9695340394973755)
[2025-02-13 03:03:25,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:25,847][root][INFO] - Training Epoch: 1/2, step 4411/7134 completed (loss: 0.1519332379102707, acc: 0.9582734107971191)
[2025-02-13 03:03:25,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:26,260][root][INFO] - Training Epoch: 1/2, step 4412/7134 completed (loss: 0.2076592743396759, acc: 0.940119743347168)
[2025-02-13 03:03:26,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:26,677][root][INFO] - Training Epoch: 1/2, step 4413/7134 completed (loss: 0.11501890420913696, acc: 0.9727891087532043)
[2025-02-13 03:03:26,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:27,089][root][INFO] - Training Epoch: 1/2, step 4414/7134 completed (loss: 0.05161118134856224, acc: 0.9834123253822327)
[2025-02-13 03:03:27,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:27,522][root][INFO] - Training Epoch: 1/2, step 4415/7134 completed (loss: 0.11232408881187439, acc: 0.9713574051856995)
[2025-02-13 03:03:27,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:27,979][root][INFO] - Training Epoch: 1/2, step 4416/7134 completed (loss: 0.08098859339952469, acc: 0.9789965152740479)
[2025-02-13 03:03:28,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:28,416][root][INFO] - Training Epoch: 1/2, step 4417/7134 completed (loss: 0.09958937764167786, acc: 0.973557710647583)
[2025-02-13 03:03:28,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:28,848][root][INFO] - Training Epoch: 1/2, step 4418/7134 completed (loss: 0.0782356783747673, acc: 0.9778645634651184)
[2025-02-13 03:03:28,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:29,256][root][INFO] - Training Epoch: 1/2, step 4419/7134 completed (loss: 0.0980226993560791, acc: 0.9699519276618958)
[2025-02-13 03:03:29,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:29,695][root][INFO] - Training Epoch: 1/2, step 4420/7134 completed (loss: 0.11981969326734543, acc: 0.9681881070137024)
[2025-02-13 03:03:29,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:30,135][root][INFO] - Training Epoch: 1/2, step 4421/7134 completed (loss: 0.07875373959541321, acc: 0.9820022583007812)
[2025-02-13 03:03:30,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:30,570][root][INFO] - Training Epoch: 1/2, step 4422/7134 completed (loss: 0.13066640496253967, acc: 0.9670329689979553)
[2025-02-13 03:03:30,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:31,015][root][INFO] - Training Epoch: 1/2, step 4423/7134 completed (loss: 0.0861087515950203, acc: 0.9714611768722534)
[2025-02-13 03:03:31,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:31,462][root][INFO] - Training Epoch: 1/2, step 4424/7134 completed (loss: 0.08305086940526962, acc: 0.9759759902954102)
[2025-02-13 03:03:31,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:31,908][root][INFO] - Training Epoch: 1/2, step 4425/7134 completed (loss: 0.05435898154973984, acc: 0.9812865257263184)
[2025-02-13 03:03:32,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:32,378][root][INFO] - Training Epoch: 1/2, step 4426/7134 completed (loss: 0.07526024430990219, acc: 0.9782833456993103)
[2025-02-13 03:03:32,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:32,729][root][INFO] - Training Epoch: 1/2, step 4427/7134 completed (loss: 0.15914183855056763, acc: 0.9614458084106445)
[2025-02-13 03:03:32,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:33,175][root][INFO] - Training Epoch: 1/2, step 4428/7134 completed (loss: 0.03556542843580246, acc: 0.98591548204422)
[2025-02-13 03:03:33,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:33,463][root][INFO] - Training Epoch: 1/2, step 4429/7134 completed (loss: 0.08390901237726212, acc: 0.969072163105011)
[2025-02-13 03:03:33,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:33,874][root][INFO] - Training Epoch: 1/2, step 4430/7134 completed (loss: 0.0827905461192131, acc: 0.9710366129875183)
[2025-02-13 03:03:34,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:34,271][root][INFO] - Training Epoch: 1/2, step 4431/7134 completed (loss: 0.08833363652229309, acc: 0.9657853841781616)
[2025-02-13 03:03:34,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:34,661][root][INFO] - Training Epoch: 1/2, step 4432/7134 completed (loss: 0.03300505504012108, acc: 0.9923195242881775)
[2025-02-13 03:03:34,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:35,118][root][INFO] - Training Epoch: 1/2, step 4433/7134 completed (loss: 0.05455012619495392, acc: 0.9825174808502197)
[2025-02-13 03:03:35,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:35,545][root][INFO] - Training Epoch: 1/2, step 4434/7134 completed (loss: 0.04197283461689949, acc: 0.9864864945411682)
[2025-02-13 03:03:35,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:35,938][root][INFO] - Training Epoch: 1/2, step 4435/7134 completed (loss: 0.041979141533374786, acc: 0.9852125644683838)
[2025-02-13 03:03:36,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:36,400][root][INFO] - Training Epoch: 1/2, step 4436/7134 completed (loss: 0.0656137764453888, acc: 0.9762569665908813)
[2025-02-13 03:03:36,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:36,818][root][INFO] - Training Epoch: 1/2, step 4437/7134 completed (loss: 0.02710968255996704, acc: 0.990813672542572)
[2025-02-13 03:03:36,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:37,230][root][INFO] - Training Epoch: 1/2, step 4438/7134 completed (loss: 0.04972745105624199, acc: 0.9813559055328369)
[2025-02-13 03:03:37,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:37,642][root][INFO] - Training Epoch: 1/2, step 4439/7134 completed (loss: 0.02737530693411827, acc: 0.9902439117431641)
[2025-02-13 03:03:37,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:38,042][root][INFO] - Training Epoch: 1/2, step 4440/7134 completed (loss: 0.037382014095783234, acc: 0.9848066568374634)
[2025-02-13 03:03:38,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:38,511][root][INFO] - Training Epoch: 1/2, step 4441/7134 completed (loss: 0.04769976809620857, acc: 0.9846389889717102)
[2025-02-13 03:03:38,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:38,944][root][INFO] - Training Epoch: 1/2, step 4442/7134 completed (loss: 0.08861701935529709, acc: 0.9794079661369324)
[2025-02-13 03:03:39,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:39,387][root][INFO] - Training Epoch: 1/2, step 4443/7134 completed (loss: 0.07934464514255524, acc: 0.9807692170143127)
[2025-02-13 03:03:39,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:39,768][root][INFO] - Training Epoch: 1/2, step 4444/7134 completed (loss: 0.060589902102947235, acc: 0.980424165725708)
[2025-02-13 03:03:39,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:40,161][root][INFO] - Training Epoch: 1/2, step 4445/7134 completed (loss: 0.0728655681014061, acc: 0.9790502786636353)
[2025-02-13 03:03:40,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:40,567][root][INFO] - Training Epoch: 1/2, step 4446/7134 completed (loss: 0.053263064473867416, acc: 0.9806763529777527)
[2025-02-13 03:03:40,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:41,019][root][INFO] - Training Epoch: 1/2, step 4447/7134 completed (loss: 0.03890063241124153, acc: 0.9905660152435303)
[2025-02-13 03:03:41,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:41,404][root][INFO] - Training Epoch: 1/2, step 4448/7134 completed (loss: 0.04647884890437126, acc: 0.9817184805870056)
[2025-02-13 03:03:41,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:41,788][root][INFO] - Training Epoch: 1/2, step 4449/7134 completed (loss: 0.10888209939002991, acc: 0.9702602028846741)
[2025-02-13 03:03:41,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:42,216][root][INFO] - Training Epoch: 1/2, step 4450/7134 completed (loss: 0.07756026089191437, acc: 0.9688888788223267)
[2025-02-13 03:03:42,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:42,609][root][INFO] - Training Epoch: 1/2, step 4451/7134 completed (loss: 0.12571369111537933, acc: 0.9751243591308594)
[2025-02-13 03:03:42,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:43,037][root][INFO] - Training Epoch: 1/2, step 4452/7134 completed (loss: 0.029105056077241898, acc: 0.9872000217437744)
[2025-02-13 03:03:43,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:43,443][root][INFO] - Training Epoch: 1/2, step 4453/7134 completed (loss: 0.06061476841568947, acc: 0.9836065769195557)
[2025-02-13 03:03:43,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:43,856][root][INFO] - Training Epoch: 1/2, step 4454/7134 completed (loss: 0.05153375491499901, acc: 0.9858356714248657)
[2025-02-13 03:03:43,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:44,224][root][INFO] - Training Epoch: 1/2, step 4455/7134 completed (loss: 0.14318643510341644, acc: 0.9613259434700012)
[2025-02-13 03:03:44,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:44,611][root][INFO] - Training Epoch: 1/2, step 4456/7134 completed (loss: 0.029078202322125435, acc: 0.9886731505393982)
[2025-02-13 03:03:44,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:44,953][root][INFO] - Training Epoch: 1/2, step 4457/7134 completed (loss: 0.08358924090862274, acc: 0.9700374603271484)
[2025-02-13 03:03:45,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:45,363][root][INFO] - Training Epoch: 1/2, step 4458/7134 completed (loss: 0.01753390021622181, acc: 0.9967266917228699)
[2025-02-13 03:03:45,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:45,788][root][INFO] - Training Epoch: 1/2, step 4459/7134 completed (loss: 0.030773570761084557, acc: 0.9903846383094788)
[2025-02-13 03:03:45,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:46,209][root][INFO] - Training Epoch: 1/2, step 4460/7134 completed (loss: 0.03437807038426399, acc: 0.9889975786209106)
[2025-02-13 03:03:46,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:46,637][root][INFO] - Training Epoch: 1/2, step 4461/7134 completed (loss: 0.08397602289915085, acc: 0.984009861946106)
[2025-02-13 03:03:46,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:47,071][root][INFO] - Training Epoch: 1/2, step 4462/7134 completed (loss: 0.01727842353284359, acc: 0.9945504069328308)
[2025-02-13 03:03:47,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:47,504][root][INFO] - Training Epoch: 1/2, step 4463/7134 completed (loss: 0.030556609854102135, acc: 0.9911949634552002)
[2025-02-13 03:03:47,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:47,954][root][INFO] - Training Epoch: 1/2, step 4464/7134 completed (loss: 0.061037901788949966, acc: 0.9831365942955017)
[2025-02-13 03:03:48,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:48,390][root][INFO] - Training Epoch: 1/2, step 4465/7134 completed (loss: 0.032837435603141785, acc: 0.9896907210350037)
[2025-02-13 03:03:48,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:48,833][root][INFO] - Training Epoch: 1/2, step 4466/7134 completed (loss: 0.024887682870030403, acc: 0.9925925731658936)
[2025-02-13 03:03:48,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:49,277][root][INFO] - Training Epoch: 1/2, step 4467/7134 completed (loss: 0.04687916859984398, acc: 0.9861809015274048)
[2025-02-13 03:03:49,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:49,707][root][INFO] - Training Epoch: 1/2, step 4468/7134 completed (loss: 0.01567968726158142, acc: 0.9964328408241272)
[2025-02-13 03:03:49,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:50,150][root][INFO] - Training Epoch: 1/2, step 4469/7134 completed (loss: 0.04912152141332626, acc: 0.9884393215179443)
[2025-02-13 03:03:50,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:50,612][root][INFO] - Training Epoch: 1/2, step 4470/7134 completed (loss: 0.03646228089928627, acc: 0.9878453016281128)
[2025-02-13 03:03:50,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:51,037][root][INFO] - Training Epoch: 1/2, step 4471/7134 completed (loss: 0.024033909663558006, acc: 0.9950860142707825)
[2025-02-13 03:03:51,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:51,443][root][INFO] - Training Epoch: 1/2, step 4472/7134 completed (loss: 0.037641383707523346, acc: 0.992094874382019)
[2025-02-13 03:03:51,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:51,859][root][INFO] - Training Epoch: 1/2, step 4473/7134 completed (loss: 0.081339992582798, acc: 0.976190447807312)
[2025-02-13 03:03:51,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:52,269][root][INFO] - Training Epoch: 1/2, step 4474/7134 completed (loss: 0.08500714600086212, acc: 0.9721059799194336)
[2025-02-13 03:03:52,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:52,707][root][INFO] - Training Epoch: 1/2, step 4475/7134 completed (loss: 0.1085677519440651, acc: 0.9714640378952026)
[2025-02-13 03:03:52,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:53,140][root][INFO] - Training Epoch: 1/2, step 4476/7134 completed (loss: 0.055495087057352066, acc: 0.9876543283462524)
[2025-02-13 03:03:53,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:53,563][root][INFO] - Training Epoch: 1/2, step 4477/7134 completed (loss: 0.012179004959762096, acc: 0.9963325262069702)
[2025-02-13 03:03:53,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:54,001][root][INFO] - Training Epoch: 1/2, step 4478/7134 completed (loss: 0.026311377063393593, acc: 0.995055615901947)
[2025-02-13 03:03:54,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:54,415][root][INFO] - Training Epoch: 1/2, step 4479/7134 completed (loss: 0.026762237772345543, acc: 0.993261456489563)
[2025-02-13 03:03:54,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:54,863][root][INFO] - Training Epoch: 1/2, step 4480/7134 completed (loss: 0.03867131099104881, acc: 0.9906542301177979)
[2025-02-13 03:03:55,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:55,310][root][INFO] - Training Epoch: 1/2, step 4481/7134 completed (loss: 0.04740181565284729, acc: 0.9906542301177979)
[2025-02-13 03:03:55,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:55,750][root][INFO] - Training Epoch: 1/2, step 4482/7134 completed (loss: 0.014507905580103397, acc: 0.9976019263267517)
[2025-02-13 03:03:55,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:56,149][root][INFO] - Training Epoch: 1/2, step 4483/7134 completed (loss: 0.03346889093518257, acc: 0.9940564632415771)
[2025-02-13 03:03:56,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:56,597][root][INFO] - Training Epoch: 1/2, step 4484/7134 completed (loss: 0.024796301499009132, acc: 0.988095223903656)
[2025-02-13 03:03:56,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:57,001][root][INFO] - Training Epoch: 1/2, step 4485/7134 completed (loss: 0.01600920595228672, acc: 0.9938744306564331)
[2025-02-13 03:03:57,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:57,388][root][INFO] - Training Epoch: 1/2, step 4486/7134 completed (loss: 0.01579219102859497, acc: 0.9940298795700073)
[2025-02-13 03:03:57,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:57,803][root][INFO] - Training Epoch: 1/2, step 4487/7134 completed (loss: 0.010830446146428585, acc: 0.9972413778305054)
[2025-02-13 03:03:57,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:58,146][root][INFO] - Training Epoch: 1/2, step 4488/7134 completed (loss: 0.024527741596102715, acc: 0.9910072088241577)
[2025-02-13 03:03:58,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:58,567][root][INFO] - Training Epoch: 1/2, step 4489/7134 completed (loss: 0.023853642866015434, acc: 0.993127167224884)
[2025-02-13 03:03:58,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:58,990][root][INFO] - Training Epoch: 1/2, step 4490/7134 completed (loss: 0.038095057010650635, acc: 0.9888888597488403)
[2025-02-13 03:03:59,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:59,397][root][INFO] - Training Epoch: 1/2, step 4491/7134 completed (loss: 0.007433377206325531, acc: 1.0)
[2025-02-13 03:03:59,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:59,794][root][INFO] - Training Epoch: 1/2, step 4492/7134 completed (loss: 0.029511848464608192, acc: 0.9889705777168274)
[2025-02-13 03:03:59,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:00,199][root][INFO] - Training Epoch: 1/2, step 4493/7134 completed (loss: 0.011767924763262272, acc: 0.9972489476203918)
[2025-02-13 03:04:00,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:00,625][root][INFO] - Training Epoch: 1/2, step 4494/7134 completed (loss: 0.0498298704624176, acc: 0.9855282306671143)
[2025-02-13 03:04:00,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:01,058][root][INFO] - Training Epoch: 1/2, step 4495/7134 completed (loss: 0.02141014114022255, acc: 0.9936143159866333)
[2025-02-13 03:04:01,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:01,467][root][INFO] - Training Epoch: 1/2, step 4496/7134 completed (loss: 0.03422000631690025, acc: 0.9900285005569458)
[2025-02-13 03:04:01,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:01,869][root][INFO] - Training Epoch: 1/2, step 4497/7134 completed (loss: 0.01993642747402191, acc: 0.9947019815444946)
[2025-02-13 03:04:02,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:02,292][root][INFO] - Training Epoch: 1/2, step 4498/7134 completed (loss: 0.06748487055301666, acc: 0.9912917017936707)
[2025-02-13 03:04:02,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:02,639][root][INFO] - Training Epoch: 1/2, step 4499/7134 completed (loss: 0.059311989694833755, acc: 0.9895651936531067)
[2025-02-13 03:04:02,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:03,076][root][INFO] - Training Epoch: 1/2, step 4500/7134 completed (loss: 0.03350725769996643, acc: 0.9902098178863525)
[2025-02-13 03:04:03,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:03,484][root][INFO] - Training Epoch: 1/2, step 4501/7134 completed (loss: 0.02738683670759201, acc: 0.9917920827865601)
[2025-02-13 03:04:03,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:03,866][root][INFO] - Training Epoch: 1/2, step 4502/7134 completed (loss: 0.025486884638667107, acc: 0.9940029978752136)
[2025-02-13 03:04:03,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:04,234][root][INFO] - Training Epoch: 1/2, step 4503/7134 completed (loss: 0.018506713211536407, acc: 0.9935794472694397)
[2025-02-13 03:04:04,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:04,640][root][INFO] - Training Epoch: 1/2, step 4504/7134 completed (loss: 0.046635713428258896, acc: 0.9865471124649048)
[2025-02-13 03:04:04,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:05,084][root][INFO] - Training Epoch: 1/2, step 4505/7134 completed (loss: 0.04579545930027962, acc: 0.9852579832077026)
[2025-02-13 03:04:05,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:05,491][root][INFO] - Training Epoch: 1/2, step 4506/7134 completed (loss: 0.039132583886384964, acc: 0.9909909963607788)
[2025-02-13 03:04:05,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:05,892][root][INFO] - Training Epoch: 1/2, step 4507/7134 completed (loss: 0.03494919091463089, acc: 0.9900426864624023)
[2025-02-13 03:04:06,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:06,300][root][INFO] - Training Epoch: 1/2, step 4508/7134 completed (loss: 0.044107094407081604, acc: 0.9886040091514587)
[2025-02-13 03:04:06,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:06,751][root][INFO] - Training Epoch: 1/2, step 4509/7134 completed (loss: 0.04993801936507225, acc: 0.9865771532058716)
[2025-02-13 03:04:06,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:07,189][root][INFO] - Training Epoch: 1/2, step 4510/7134 completed (loss: 0.048309326171875, acc: 0.9830747246742249)
[2025-02-13 03:04:07,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:07,634][root][INFO] - Training Epoch: 1/2, step 4511/7134 completed (loss: 0.07944121211767197, acc: 0.9789029359817505)
[2025-02-13 03:04:07,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:08,033][root][INFO] - Training Epoch: 1/2, step 4512/7134 completed (loss: 0.07953772693872452, acc: 0.9767025113105774)
[2025-02-13 03:04:08,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:08,475][root][INFO] - Training Epoch: 1/2, step 4513/7134 completed (loss: 0.036342885345220566, acc: 0.9901685118675232)
[2025-02-13 03:04:08,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:08,898][root][INFO] - Training Epoch: 1/2, step 4514/7134 completed (loss: 0.09758376330137253, acc: 0.9766423106193542)
[2025-02-13 03:04:09,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:09,333][root][INFO] - Training Epoch: 1/2, step 4515/7134 completed (loss: 0.07388823479413986, acc: 0.9761193990707397)
[2025-02-13 03:04:09,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:09,757][root][INFO] - Training Epoch: 1/2, step 4516/7134 completed (loss: 0.061220843344926834, acc: 0.9791921377182007)
[2025-02-13 03:04:09,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:10,188][root][INFO] - Training Epoch: 1/2, step 4517/7134 completed (loss: 0.04602528363466263, acc: 0.98591548204422)
[2025-02-13 03:04:10,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:10,632][root][INFO] - Training Epoch: 1/2, step 4518/7134 completed (loss: 0.08633333444595337, acc: 0.980867326259613)
[2025-02-13 03:04:10,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:11,073][root][INFO] - Training Epoch: 1/2, step 4519/7134 completed (loss: 0.04353965446352959, acc: 0.987908124923706)
[2025-02-13 03:04:11,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:11,505][root][INFO] - Training Epoch: 1/2, step 4520/7134 completed (loss: 0.0536949448287487, acc: 0.9858757257461548)
[2025-02-13 03:04:11,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:11,967][root][INFO] - Training Epoch: 1/2, step 4521/7134 completed (loss: 0.061887066811323166, acc: 0.9807460904121399)
[2025-02-13 03:04:12,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:12,407][root][INFO] - Training Epoch: 1/2, step 4522/7134 completed (loss: 0.07283712923526764, acc: 0.9798488616943359)
[2025-02-13 03:04:12,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:12,843][root][INFO] - Training Epoch: 1/2, step 4523/7134 completed (loss: 0.03544878959655762, acc: 0.9869822263717651)
[2025-02-13 03:04:12,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:13,302][root][INFO] - Training Epoch: 1/2, step 4524/7134 completed (loss: 0.06445399671792984, acc: 0.9838056564331055)
[2025-02-13 03:04:13,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:13,734][root][INFO] - Training Epoch: 1/2, step 4525/7134 completed (loss: 0.031568363308906555, acc: 0.9900000095367432)
[2025-02-13 03:04:13,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:14,144][root][INFO] - Training Epoch: 1/2, step 4526/7134 completed (loss: 0.07183372974395752, acc: 0.9780645370483398)
[2025-02-13 03:04:14,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:14,619][root][INFO] - Training Epoch: 1/2, step 4527/7134 completed (loss: 0.0321914367377758, acc: 0.9888517260551453)
[2025-02-13 03:04:14,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:15,060][root][INFO] - Training Epoch: 1/2, step 4528/7134 completed (loss: 0.09287372976541519, acc: 0.9781491160392761)
[2025-02-13 03:04:15,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:15,449][root][INFO] - Training Epoch: 1/2, step 4529/7134 completed (loss: 0.088809534907341, acc: 0.984455943107605)
[2025-02-13 03:04:15,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:15,834][root][INFO] - Training Epoch: 1/2, step 4530/7134 completed (loss: 0.039838455617427826, acc: 0.9901960492134094)
[2025-02-13 03:04:15,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:16,276][root][INFO] - Training Epoch: 1/2, step 4531/7134 completed (loss: 0.04109371826052666, acc: 0.985637366771698)
[2025-02-13 03:04:16,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:16,730][root][INFO] - Training Epoch: 1/2, step 4532/7134 completed (loss: 0.06597134470939636, acc: 0.9850597381591797)
[2025-02-13 03:04:16,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:17,159][root][INFO] - Training Epoch: 1/2, step 4533/7134 completed (loss: 0.043342288583517075, acc: 0.9857650995254517)
[2025-02-13 03:04:17,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:17,591][root][INFO] - Training Epoch: 1/2, step 4534/7134 completed (loss: 0.029371963813900948, acc: 0.9906103014945984)
[2025-02-13 03:04:17,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:18,016][root][INFO] - Training Epoch: 1/2, step 4535/7134 completed (loss: 0.048037491738796234, acc: 0.980637788772583)
[2025-02-13 03:04:18,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:18,490][root][INFO] - Training Epoch: 1/2, step 4536/7134 completed (loss: 0.04938049614429474, acc: 0.9837164878845215)
[2025-02-13 03:04:18,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:18,943][root][INFO] - Training Epoch: 1/2, step 4537/7134 completed (loss: 0.020480258390307426, acc: 0.9921787977218628)
[2025-02-13 03:04:19,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:19,400][root][INFO] - Training Epoch: 1/2, step 4538/7134 completed (loss: 0.06307316571474075, acc: 0.9839080572128296)
[2025-02-13 03:04:19,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:19,857][root][INFO] - Training Epoch: 1/2, step 4539/7134 completed (loss: 0.05685391649603844, acc: 0.9848974943161011)
[2025-02-13 03:04:19,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:20,306][root][INFO] - Training Epoch: 1/2, step 4540/7134 completed (loss: 0.03887207806110382, acc: 0.9898843765258789)
[2025-02-13 03:04:20,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:20,662][root][INFO] - Training Epoch: 1/2, step 4541/7134 completed (loss: 0.08431807160377502, acc: 0.975359320640564)
[2025-02-13 03:04:20,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:21,025][root][INFO] - Training Epoch: 1/2, step 4542/7134 completed (loss: 0.08830282837152481, acc: 0.9783549904823303)
[2025-02-13 03:04:21,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:21,390][root][INFO] - Training Epoch: 1/2, step 4543/7134 completed (loss: 0.09703224152326584, acc: 0.9696969985961914)
[2025-02-13 03:04:21,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:21,810][root][INFO] - Training Epoch: 1/2, step 4544/7134 completed (loss: 0.06294696778059006, acc: 0.9832317233085632)
[2025-02-13 03:04:21,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:22,209][root][INFO] - Training Epoch: 1/2, step 4545/7134 completed (loss: 0.08009220659732819, acc: 0.978805422782898)
[2025-02-13 03:04:22,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:22,605][root][INFO] - Training Epoch: 1/2, step 4546/7134 completed (loss: 0.0733477994799614, acc: 0.9831932783126831)
[2025-02-13 03:04:22,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:23,001][root][INFO] - Training Epoch: 1/2, step 4547/7134 completed (loss: 0.036849088966846466, acc: 0.987889289855957)
[2025-02-13 03:04:23,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:23,430][root][INFO] - Training Epoch: 1/2, step 4548/7134 completed (loss: 0.061142485588788986, acc: 0.9850746393203735)
[2025-02-13 03:04:23,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:23,783][root][INFO] - Training Epoch: 1/2, step 4549/7134 completed (loss: 0.06136101111769676, acc: 0.9871244430541992)
[2025-02-13 03:04:23,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:24,225][root][INFO] - Training Epoch: 1/2, step 4550/7134 completed (loss: 0.0861542671918869, acc: 0.9782244563102722)
[2025-02-13 03:04:24,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:24,627][root][INFO] - Training Epoch: 1/2, step 4551/7134 completed (loss: 0.04020385071635246, acc: 0.9885714054107666)
[2025-02-13 03:04:24,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:25,016][root][INFO] - Training Epoch: 1/2, step 4552/7134 completed (loss: 0.04957966133952141, acc: 0.9866666793823242)
[2025-02-13 03:04:25,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:25,402][root][INFO] - Training Epoch: 1/2, step 4553/7134 completed (loss: 0.040162939578294754, acc: 0.9915397763252258)
[2025-02-13 03:04:25,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:25,784][root][INFO] - Training Epoch: 1/2, step 4554/7134 completed (loss: 0.0646645575761795, acc: 0.9819819927215576)
[2025-02-13 03:04:25,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:26,183][root][INFO] - Training Epoch: 1/2, step 4555/7134 completed (loss: 0.06932257860898972, acc: 0.9806094169616699)
[2025-02-13 03:04:26,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:26,624][root][INFO] - Training Epoch: 1/2, step 4556/7134 completed (loss: 0.07111102342605591, acc: 0.9744361042976379)
[2025-02-13 03:04:26,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:26,961][root][INFO] - Training Epoch: 1/2, step 4557/7134 completed (loss: 0.09884094446897507, acc: 0.9781022071838379)
[2025-02-13 03:04:27,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:27,367][root][INFO] - Training Epoch: 1/2, step 4558/7134 completed (loss: 0.058290570974349976, acc: 0.9882155060768127)
[2025-02-13 03:04:27,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:27,778][root][INFO] - Training Epoch: 1/2, step 4559/7134 completed (loss: 0.0866270437836647, acc: 0.9831932783126831)
[2025-02-13 03:04:27,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:28,173][root][INFO] - Training Epoch: 1/2, step 4560/7134 completed (loss: 0.04539749026298523, acc: 0.989130437374115)
[2025-02-13 03:04:28,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:28,530][root][INFO] - Training Epoch: 1/2, step 4561/7134 completed (loss: 0.0686287060379982, acc: 0.9843137264251709)
[2025-02-13 03:04:28,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:28,923][root][INFO] - Training Epoch: 1/2, step 4562/7134 completed (loss: 0.05176447704434395, acc: 0.9834254384040833)
[2025-02-13 03:04:29,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:29,350][root][INFO] - Training Epoch: 1/2, step 4563/7134 completed (loss: 0.03725460544228554, acc: 0.9908397197723389)
[2025-02-13 03:04:29,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:29,794][root][INFO] - Training Epoch: 1/2, step 4564/7134 completed (loss: 0.06416067481040955, acc: 0.9858956336975098)
[2025-02-13 03:04:29,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:30,174][root][INFO] - Training Epoch: 1/2, step 4565/7134 completed (loss: 0.06680995225906372, acc: 0.9827916026115417)
[2025-02-13 03:04:30,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:30,579][root][INFO] - Training Epoch: 1/2, step 4566/7134 completed (loss: 0.046264417469501495, acc: 0.9856630563735962)
[2025-02-13 03:04:30,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:30,956][root][INFO] - Training Epoch: 1/2, step 4567/7134 completed (loss: 0.03017420694231987, acc: 0.9908088445663452)
[2025-02-13 03:04:31,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:31,324][root][INFO] - Training Epoch: 1/2, step 4568/7134 completed (loss: 0.06772956252098083, acc: 0.9839449524879456)
[2025-02-13 03:04:31,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:31,726][root][INFO] - Training Epoch: 1/2, step 4569/7134 completed (loss: 0.03385855257511139, acc: 0.9921568632125854)
[2025-02-13 03:04:31,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:32,131][root][INFO] - Training Epoch: 1/2, step 4570/7134 completed (loss: 0.03727085888385773, acc: 0.9839650392532349)
[2025-02-13 03:04:32,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:32,557][root][INFO] - Training Epoch: 1/2, step 4571/7134 completed (loss: 0.036130622029304504, acc: 0.9897304177284241)
[2025-02-13 03:04:32,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:32,966][root][INFO] - Training Epoch: 1/2, step 4572/7134 completed (loss: 0.053947288542985916, acc: 0.9890965819358826)
[2025-02-13 03:04:33,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:33,392][root][INFO] - Training Epoch: 1/2, step 4573/7134 completed (loss: 0.047877855598926544, acc: 0.9859943985939026)
[2025-02-13 03:04:33,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:33,805][root][INFO] - Training Epoch: 1/2, step 4574/7134 completed (loss: 0.021945904940366745, acc: 0.9946996569633484)
[2025-02-13 03:04:33,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:34,229][root][INFO] - Training Epoch: 1/2, step 4575/7134 completed (loss: 0.0456765852868557, acc: 0.9840425252914429)
[2025-02-13 03:04:34,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:34,649][root][INFO] - Training Epoch: 1/2, step 4576/7134 completed (loss: 0.024114077910780907, acc: 0.9937264919281006)
[2025-02-13 03:04:34,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:35,065][root][INFO] - Training Epoch: 1/2, step 4577/7134 completed (loss: 0.045370738953351974, acc: 0.9901685118675232)
[2025-02-13 03:04:35,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:35,473][root][INFO] - Training Epoch: 1/2, step 4578/7134 completed (loss: 0.03940611332654953, acc: 0.9883551597595215)
[2025-02-13 03:04:35,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:35,868][root][INFO] - Training Epoch: 1/2, step 4579/7134 completed (loss: 0.06396709382534027, acc: 0.9889065027236938)
[2025-02-13 03:04:36,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:36,277][root][INFO] - Training Epoch: 1/2, step 4580/7134 completed (loss: 0.023876557126641273, acc: 0.9925373196601868)
[2025-02-13 03:04:36,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:36,685][root][INFO] - Training Epoch: 1/2, step 4581/7134 completed (loss: 0.03271518275141716, acc: 0.9911816716194153)
[2025-02-13 03:04:36,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:37,101][root][INFO] - Training Epoch: 1/2, step 4582/7134 completed (loss: 0.03383641690015793, acc: 0.9884868264198303)
[2025-02-13 03:04:37,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:37,485][root][INFO] - Training Epoch: 1/2, step 4583/7134 completed (loss: 0.017832236364483833, acc: 0.9934640526771545)
[2025-02-13 03:04:37,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:37,890][root][INFO] - Training Epoch: 1/2, step 4584/7134 completed (loss: 0.023399079218506813, acc: 0.9919484853744507)
[2025-02-13 03:04:38,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:38,309][root][INFO] - Training Epoch: 1/2, step 4585/7134 completed (loss: 0.03771935775876045, acc: 0.9854369163513184)
[2025-02-13 03:04:38,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:38,755][root][INFO] - Training Epoch: 1/2, step 4586/7134 completed (loss: 0.041503824293613434, acc: 0.9856957197189331)
[2025-02-13 03:04:38,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:39,180][root][INFO] - Training Epoch: 1/2, step 4587/7134 completed (loss: 0.04458075761795044, acc: 0.9863013625144958)
[2025-02-13 03:04:39,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:39,607][root][INFO] - Training Epoch: 1/2, step 4588/7134 completed (loss: 0.05621534213423729, acc: 0.9825581312179565)
[2025-02-13 03:04:39,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:39,999][root][INFO] - Training Epoch: 1/2, step 4589/7134 completed (loss: 0.07490728795528412, acc: 0.976047933101654)
[2025-02-13 03:04:40,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:40,426][root][INFO] - Training Epoch: 1/2, step 4590/7134 completed (loss: 0.04525010287761688, acc: 0.9848066568374634)
[2025-02-13 03:04:40,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:40,852][root][INFO] - Training Epoch: 1/2, step 4591/7134 completed (loss: 0.04537573829293251, acc: 0.9828495979309082)
[2025-02-13 03:04:40,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:41,248][root][INFO] - Training Epoch: 1/2, step 4592/7134 completed (loss: 0.049669601023197174, acc: 0.9854862093925476)
[2025-02-13 03:04:41,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:41,655][root][INFO] - Training Epoch: 1/2, step 4593/7134 completed (loss: 0.05137477070093155, acc: 0.9837037324905396)
[2025-02-13 03:04:41,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:42,101][root][INFO] - Training Epoch: 1/2, step 4594/7134 completed (loss: 0.05171738192439079, acc: 0.9800000190734863)
[2025-02-13 03:04:42,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:42,525][root][INFO] - Training Epoch: 1/2, step 4595/7134 completed (loss: 0.047103818506002426, acc: 0.9836552739143372)
[2025-02-13 03:04:42,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:42,935][root][INFO] - Training Epoch: 1/2, step 4596/7134 completed (loss: 0.052236393094062805, acc: 0.9825396537780762)
[2025-02-13 03:04:43,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:43,371][root][INFO] - Training Epoch: 1/2, step 4597/7134 completed (loss: 0.024310950189828873, acc: 0.9890643954277039)
[2025-02-13 03:04:43,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:43,812][root][INFO] - Training Epoch: 1/2, step 4598/7134 completed (loss: 0.07435090839862823, acc: 0.9840490818023682)
[2025-02-13 03:04:43,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:44,199][root][INFO] - Training Epoch: 1/2, step 4599/7134 completed (loss: 0.01606796309351921, acc: 0.9936407208442688)
[2025-02-13 03:04:44,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:44,616][root][INFO] - Training Epoch: 1/2, step 4600/7134 completed (loss: 0.02791845239698887, acc: 0.9918808937072754)
[2025-02-13 03:04:44,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:45,078][root][INFO] - Training Epoch: 1/2, step 4601/7134 completed (loss: 0.046277426183223724, acc: 0.9843546152114868)
[2025-02-13 03:04:45,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:45,515][root][INFO] - Training Epoch: 1/2, step 4602/7134 completed (loss: 0.04355844855308533, acc: 0.9863760471343994)
[2025-02-13 03:04:45,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:45,944][root][INFO] - Training Epoch: 1/2, step 4603/7134 completed (loss: 0.06348279118537903, acc: 0.9820689558982849)
[2025-02-13 03:04:46,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:46,353][root][INFO] - Training Epoch: 1/2, step 4604/7134 completed (loss: 0.03397279977798462, acc: 0.9855538010597229)
[2025-02-13 03:04:46,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:46,771][root][INFO] - Training Epoch: 1/2, step 4605/7134 completed (loss: 0.06899403035640717, acc: 0.9737991094589233)
[2025-02-13 03:04:46,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:47,196][root][INFO] - Training Epoch: 1/2, step 4606/7134 completed (loss: 0.02073054574429989, acc: 0.9938837885856628)
[2025-02-13 03:04:47,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:47,557][root][INFO] - Training Epoch: 1/2, step 4607/7134 completed (loss: 0.02578791044652462, acc: 0.9831649661064148)
[2025-02-13 03:04:47,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:47,975][root][INFO] - Training Epoch: 1/2, step 4608/7134 completed (loss: 0.02277049794793129, acc: 0.9940740466117859)
[2025-02-13 03:04:48,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:48,389][root][INFO] - Training Epoch: 1/2, step 4609/7134 completed (loss: 0.04320888966321945, acc: 0.9944953918457031)
[2025-02-13 03:04:48,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:48,813][root][INFO] - Training Epoch: 1/2, step 4610/7134 completed (loss: 0.029690710827708244, acc: 0.9867197871208191)
[2025-02-13 03:04:48,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:49,229][root][INFO] - Training Epoch: 1/2, step 4611/7134 completed (loss: 0.013089890591800213, acc: 0.9968701004981995)
[2025-02-13 03:04:49,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:49,592][root][INFO] - Training Epoch: 1/2, step 4612/7134 completed (loss: 0.0270058736205101, acc: 0.9944238066673279)
[2025-02-13 03:04:49,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:49,988][root][INFO] - Training Epoch: 1/2, step 4613/7134 completed (loss: 0.02091568149626255, acc: 0.9930555820465088)
[2025-02-13 03:04:50,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:50,379][root][INFO] - Training Epoch: 1/2, step 4614/7134 completed (loss: 0.05578906461596489, acc: 0.9848484992980957)
[2025-02-13 03:04:50,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:50,821][root][INFO] - Training Epoch: 1/2, step 4615/7134 completed (loss: 0.05388568341732025, acc: 0.9810040593147278)
[2025-02-13 03:04:50,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:51,233][root][INFO] - Training Epoch: 1/2, step 4616/7134 completed (loss: 0.05660165846347809, acc: 0.9837278127670288)
[2025-02-13 03:04:51,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:51,605][root][INFO] - Training Epoch: 1/2, step 4617/7134 completed (loss: 0.03785233944654465, acc: 0.9864864945411682)
[2025-02-13 03:04:51,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:52,008][root][INFO] - Training Epoch: 1/2, step 4618/7134 completed (loss: 0.05068963021039963, acc: 0.9728155136108398)
[2025-02-13 03:04:52,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:52,443][root][INFO] - Training Epoch: 1/2, step 4619/7134 completed (loss: 0.01501941867172718, acc: 0.9954751133918762)
[2025-02-13 03:04:52,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:52,867][root][INFO] - Training Epoch: 1/2, step 4620/7134 completed (loss: 0.016589177772402763, acc: 0.9942029118537903)
[2025-02-13 03:04:53,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:53,264][root][INFO] - Training Epoch: 1/2, step 4621/7134 completed (loss: 0.018031880259513855, acc: 0.9907407164573669)
[2025-02-13 03:04:53,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:53,683][root][INFO] - Training Epoch: 1/2, step 4622/7134 completed (loss: 0.04991687461733818, acc: 0.9875156283378601)
[2025-02-13 03:04:53,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:54,091][root][INFO] - Training Epoch: 1/2, step 4623/7134 completed (loss: 0.061008963733911514, acc: 0.9816993474960327)
[2025-02-13 03:04:54,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:54,580][root][INFO] - Training Epoch: 1/2, step 4624/7134 completed (loss: 0.07808521389961243, acc: 0.9807956218719482)
[2025-02-13 03:04:54,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:54,982][root][INFO] - Training Epoch: 1/2, step 4625/7134 completed (loss: 0.1283523440361023, acc: 0.9655172228813171)
[2025-02-13 03:04:55,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:55,419][root][INFO] - Training Epoch: 1/2, step 4626/7134 completed (loss: 0.07362980395555496, acc: 0.9798657894134521)
[2025-02-13 03:04:55,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:55,828][root][INFO] - Training Epoch: 1/2, step 4627/7134 completed (loss: 0.10794766992330551, acc: 0.97420334815979)
[2025-02-13 03:04:55,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:56,275][root][INFO] - Training Epoch: 1/2, step 4628/7134 completed (loss: 0.0767667219042778, acc: 0.9739130139350891)
[2025-02-13 03:04:56,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:56,702][root][INFO] - Training Epoch: 1/2, step 4629/7134 completed (loss: 0.09884609282016754, acc: 0.9726495742797852)
[2025-02-13 03:04:56,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:57,156][root][INFO] - Training Epoch: 1/2, step 4630/7134 completed (loss: 0.07147597521543503, acc: 0.9801849126815796)
[2025-02-13 03:04:57,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:57,608][root][INFO] - Training Epoch: 1/2, step 4631/7134 completed (loss: 0.05972960591316223, acc: 0.9811946749687195)
[2025-02-13 03:04:57,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:58,085][root][INFO] - Training Epoch: 1/2, step 4632/7134 completed (loss: 0.05942185968160629, acc: 0.9852941036224365)
[2025-02-13 03:04:58,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:58,516][root][INFO] - Training Epoch: 1/2, step 4633/7134 completed (loss: 0.08286351710557938, acc: 0.9780346751213074)
[2025-02-13 03:04:58,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:58,928][root][INFO] - Training Epoch: 1/2, step 4634/7134 completed (loss: 0.035955801606178284, acc: 0.9902912378311157)
[2025-02-13 03:04:59,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:59,297][root][INFO] - Training Epoch: 1/2, step 4635/7134 completed (loss: 0.08509615063667297, acc: 0.9817073345184326)
[2025-02-13 03:04:59,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:59,567][root][INFO] - Training Epoch: 1/2, step 4636/7134 completed (loss: 0.02366240695118904, acc: 0.9922680258750916)
[2025-02-13 03:04:59,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:59,996][root][INFO] - Training Epoch: 1/2, step 4637/7134 completed (loss: 0.04708751663565636, acc: 0.9879032373428345)
[2025-02-13 03:05:00,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:00,419][root][INFO] - Training Epoch: 1/2, step 4638/7134 completed (loss: 0.07958097755908966, acc: 0.9775429368019104)
[2025-02-13 03:05:00,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:00,845][root][INFO] - Training Epoch: 1/2, step 4639/7134 completed (loss: 0.07381074130535126, acc: 0.9817517995834351)
[2025-02-13 03:05:00,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:01,301][root][INFO] - Training Epoch: 1/2, step 4640/7134 completed (loss: 0.07715389877557755, acc: 0.9807692170143127)
[2025-02-13 03:05:01,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:01,723][root][INFO] - Training Epoch: 1/2, step 4641/7134 completed (loss: 0.04590639844536781, acc: 0.984375)
[2025-02-13 03:05:01,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:02,155][root][INFO] - Training Epoch: 1/2, step 4642/7134 completed (loss: 0.020850062370300293, acc: 0.9960317611694336)
[2025-02-13 03:05:02,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:02,608][root][INFO] - Training Epoch: 1/2, step 4643/7134 completed (loss: 0.03311498835682869, acc: 0.9940828680992126)
[2025-02-13 03:05:02,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:03,064][root][INFO] - Training Epoch: 1/2, step 4644/7134 completed (loss: 0.03454212844371796, acc: 0.9917159676551819)
[2025-02-13 03:05:03,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:03,504][root][INFO] - Training Epoch: 1/2, step 4645/7134 completed (loss: 0.028264297172427177, acc: 0.9944367408752441)
[2025-02-13 03:05:03,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:03,938][root][INFO] - Training Epoch: 1/2, step 4646/7134 completed (loss: 0.02650129422545433, acc: 0.9890109896659851)
[2025-02-13 03:05:04,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:04,395][root][INFO] - Training Epoch: 1/2, step 4647/7134 completed (loss: 0.09967508167028427, acc: 0.9830729365348816)
[2025-02-13 03:05:04,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:04,827][root][INFO] - Training Epoch: 1/2, step 4648/7134 completed (loss: 0.05760832875967026, acc: 0.9807956218719482)
[2025-02-13 03:05:04,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:05,265][root][INFO] - Training Epoch: 1/2, step 4649/7134 completed (loss: 0.06211197003722191, acc: 0.9842382073402405)
[2025-02-13 03:05:05,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:05,717][root][INFO] - Training Epoch: 1/2, step 4650/7134 completed (loss: 0.07537221908569336, acc: 0.9747023582458496)
[2025-02-13 03:05:05,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:06,136][root][INFO] - Training Epoch: 1/2, step 4651/7134 completed (loss: 0.1396346539258957, acc: 0.9619771838188171)
[2025-02-13 03:05:06,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:06,563][root][INFO] - Training Epoch: 1/2, step 4652/7134 completed (loss: 0.0436544194817543, acc: 0.9878048896789551)
[2025-02-13 03:05:06,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:07,021][root][INFO] - Training Epoch: 1/2, step 4653/7134 completed (loss: 0.06305874139070511, acc: 0.9838709831237793)
[2025-02-13 03:05:07,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:07,463][root][INFO] - Training Epoch: 1/2, step 4654/7134 completed (loss: 0.10016963630914688, acc: 0.9707673788070679)
[2025-02-13 03:05:07,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:07,895][root][INFO] - Training Epoch: 1/2, step 4655/7134 completed (loss: 0.04719610884785652, acc: 0.9890561103820801)
[2025-02-13 03:05:08,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:08,342][root][INFO] - Training Epoch: 1/2, step 4656/7134 completed (loss: 0.051618605852127075, acc: 0.9848484992980957)
[2025-02-13 03:05:08,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:08,752][root][INFO] - Training Epoch: 1/2, step 4657/7134 completed (loss: 0.06394980102777481, acc: 0.9787535667419434)
[2025-02-13 03:05:08,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:09,162][root][INFO] - Training Epoch: 1/2, step 4658/7134 completed (loss: 0.0732404962182045, acc: 0.9702300429344177)
[2025-02-13 03:05:09,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:09,605][root][INFO] - Training Epoch: 1/2, step 4659/7134 completed (loss: 0.10076193511486053, acc: 0.9755784273147583)
[2025-02-13 03:05:09,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:10,043][root][INFO] - Training Epoch: 1/2, step 4660/7134 completed (loss: 0.10047099739313126, acc: 0.9717868566513062)
[2025-02-13 03:05:10,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:10,427][root][INFO] - Training Epoch: 1/2, step 4661/7134 completed (loss: 0.05834417790174484, acc: 0.9832317233085632)
[2025-02-13 03:05:10,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:10,813][root][INFO] - Training Epoch: 1/2, step 4662/7134 completed (loss: 0.09426315128803253, acc: 0.9744898080825806)
[2025-02-13 03:05:10,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:11,256][root][INFO] - Training Epoch: 1/2, step 4663/7134 completed (loss: 0.08128092437982559, acc: 0.9799465537071228)
[2025-02-13 03:05:11,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:11,697][root][INFO] - Training Epoch: 1/2, step 4664/7134 completed (loss: 0.06362979859113693, acc: 0.9828269481658936)
[2025-02-13 03:05:11,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:12,109][root][INFO] - Training Epoch: 1/2, step 4665/7134 completed (loss: 0.06143694743514061, acc: 0.9822646379470825)
[2025-02-13 03:05:12,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:12,562][root][INFO] - Training Epoch: 1/2, step 4666/7134 completed (loss: 0.07866561412811279, acc: 0.9817470908164978)
[2025-02-13 03:05:12,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:12,946][root][INFO] - Training Epoch: 1/2, step 4667/7134 completed (loss: 0.08107472956180573, acc: 0.9741824269294739)
[2025-02-13 03:05:13,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:13,350][root][INFO] - Training Epoch: 1/2, step 4668/7134 completed (loss: 0.05131412670016289, acc: 0.9820788502693176)
[2025-02-13 03:05:13,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:13,772][root][INFO] - Training Epoch: 1/2, step 4669/7134 completed (loss: 0.03458966687321663, acc: 0.9896449446678162)
[2025-02-13 03:05:13,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:14,164][root][INFO] - Training Epoch: 1/2, step 4670/7134 completed (loss: 0.08003740757703781, acc: 0.9805068373680115)
[2025-02-13 03:05:14,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:14,570][root][INFO] - Training Epoch: 1/2, step 4671/7134 completed (loss: 0.056263238191604614, acc: 0.9858956336975098)
[2025-02-13 03:05:14,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:14,969][root][INFO] - Training Epoch: 1/2, step 4672/7134 completed (loss: 0.07304167002439499, acc: 0.9773519039154053)
[2025-02-13 03:05:15,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:15,370][root][INFO] - Training Epoch: 1/2, step 4673/7134 completed (loss: 0.07230844348669052, acc: 0.9828392863273621)
[2025-02-13 03:05:15,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:15,782][root][INFO] - Training Epoch: 1/2, step 4674/7134 completed (loss: 0.0601990707218647, acc: 0.9825673699378967)
[2025-02-13 03:05:15,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:16,217][root][INFO] - Training Epoch: 1/2, step 4675/7134 completed (loss: 0.05185910314321518, acc: 0.9859872460365295)
[2025-02-13 03:05:16,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:16,622][root][INFO] - Training Epoch: 1/2, step 4676/7134 completed (loss: 0.031296659260988235, acc: 0.9939758777618408)
[2025-02-13 03:05:16,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:17,005][root][INFO] - Training Epoch: 1/2, step 4677/7134 completed (loss: 0.051617905497550964, acc: 0.984649121761322)
[2025-02-13 03:05:17,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:17,415][root][INFO] - Training Epoch: 1/2, step 4678/7134 completed (loss: 0.0796770229935646, acc: 0.97919762134552)
[2025-02-13 03:05:17,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:17,838][root][INFO] - Training Epoch: 1/2, step 4679/7134 completed (loss: 0.06872754544019699, acc: 0.9848275780677795)
[2025-02-13 03:05:17,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:18,226][root][INFO] - Training Epoch: 1/2, step 4680/7134 completed (loss: 0.06854552775621414, acc: 0.9766355156898499)
[2025-02-13 03:05:18,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:18,627][root][INFO] - Training Epoch: 1/2, step 4681/7134 completed (loss: 0.05984823405742645, acc: 0.9876922965049744)
[2025-02-13 03:05:18,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:19,100][root][INFO] - Training Epoch: 1/2, step 4682/7134 completed (loss: 0.0393068790435791, acc: 0.9866179823875427)
[2025-02-13 03:05:19,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:19,501][root][INFO] - Training Epoch: 1/2, step 4683/7134 completed (loss: 0.05122193321585655, acc: 0.9857346415519714)
[2025-02-13 03:05:19,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:19,959][root][INFO] - Training Epoch: 1/2, step 4684/7134 completed (loss: 0.08236267417669296, acc: 0.9800838828086853)
[2025-02-13 03:05:20,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:20,412][root][INFO] - Training Epoch: 1/2, step 4685/7134 completed (loss: 0.04842643812298775, acc: 0.9907786846160889)
[2025-02-13 03:05:20,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:20,882][root][INFO] - Training Epoch: 1/2, step 4686/7134 completed (loss: 0.042981017380952835, acc: 0.9883585572242737)
[2025-02-13 03:05:20,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:21,207][root][INFO] - Training Epoch: 1/2, step 4687/7134 completed (loss: 0.18678069114685059, acc: 0.953987717628479)
[2025-02-13 03:05:21,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:21,646][root][INFO] - Training Epoch: 1/2, step 4688/7134 completed (loss: 0.2289094626903534, acc: 0.9445585012435913)
[2025-02-13 03:05:21,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:22,102][root][INFO] - Training Epoch: 1/2, step 4689/7134 completed (loss: 0.1641816347837448, acc: 0.9513990879058838)
[2025-02-13 03:05:22,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:22,549][root][INFO] - Training Epoch: 1/2, step 4690/7134 completed (loss: 0.1352670043706894, acc: 0.9727463126182556)
[2025-02-13 03:05:22,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:22,972][root][INFO] - Training Epoch: 1/2, step 4691/7134 completed (loss: 0.05415485054254532, acc: 0.9807256460189819)
[2025-02-13 03:05:23,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:23,286][root][INFO] - Training Epoch: 1/2, step 4692/7134 completed (loss: 0.1910410076379776, acc: 0.9575371742248535)
[2025-02-13 03:05:23,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:23,779][root][INFO] - Training Epoch: 1/2, step 4693/7134 completed (loss: 0.18020427227020264, acc: 0.9624060392379761)
[2025-02-13 03:05:23,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:24,182][root][INFO] - Training Epoch: 1/2, step 4694/7134 completed (loss: 0.16685788333415985, acc: 0.9553752541542053)
[2025-02-13 03:05:24,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:24,601][root][INFO] - Training Epoch: 1/2, step 4695/7134 completed (loss: 0.09285756200551987, acc: 0.9737274050712585)
[2025-02-13 03:05:24,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:25,030][root][INFO] - Training Epoch: 1/2, step 4696/7134 completed (loss: 0.04935067519545555, acc: 0.9866270422935486)
[2025-02-13 03:05:25,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:25,355][root][INFO] - Training Epoch: 1/2, step 4697/7134 completed (loss: 0.061039675027132034, acc: 0.9830827116966248)
[2025-02-13 03:05:25,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:25,746][root][INFO] - Training Epoch: 1/2, step 4698/7134 completed (loss: 0.12834812700748444, acc: 0.9675572514533997)
[2025-02-13 03:05:25,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:26,194][root][INFO] - Training Epoch: 1/2, step 4699/7134 completed (loss: 0.07437658309936523, acc: 0.9762340188026428)
[2025-02-13 03:05:26,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:26,627][root][INFO] - Training Epoch: 1/2, step 4700/7134 completed (loss: 0.08505330979824066, acc: 0.9746646881103516)
[2025-02-13 03:05:26,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:26,956][root][INFO] - Training Epoch: 1/2, step 4701/7134 completed (loss: 0.12344987690448761, acc: 0.9648562073707581)
[2025-02-13 03:05:27,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:27,405][root][INFO] - Training Epoch: 1/2, step 4702/7134 completed (loss: 0.046169716864824295, acc: 0.9856687784194946)
[2025-02-13 03:05:27,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:27,842][root][INFO] - Training Epoch: 1/2, step 4703/7134 completed (loss: 0.05769224464893341, acc: 0.9878378510475159)
[2025-02-13 03:05:27,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:28,256][root][INFO] - Training Epoch: 1/2, step 4704/7134 completed (loss: 0.037785232067108154, acc: 0.991793692111969)
[2025-02-13 03:05:28,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:28,697][root][INFO] - Training Epoch: 1/2, step 4705/7134 completed (loss: 0.036169227212667465, acc: 0.9871345162391663)
[2025-02-13 03:05:28,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:29,159][root][INFO] - Training Epoch: 1/2, step 4706/7134 completed (loss: 0.09827370941638947, acc: 0.9766355156898499)
[2025-02-13 03:05:29,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:29,569][root][INFO] - Training Epoch: 1/2, step 4707/7134 completed (loss: 0.2318684309720993, acc: 0.942176878452301)
[2025-02-13 03:05:29,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:29,974][root][INFO] - Training Epoch: 1/2, step 4708/7134 completed (loss: 0.04320431873202324, acc: 0.9800498485565186)
[2025-02-13 03:05:30,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:30,416][root][INFO] - Training Epoch: 1/2, step 4709/7134 completed (loss: 0.03549576550722122, acc: 0.9894598126411438)
[2025-02-13 03:05:30,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:30,850][root][INFO] - Training Epoch: 1/2, step 4710/7134 completed (loss: 0.05704732611775398, acc: 0.9810771346092224)
[2025-02-13 03:05:30,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:31,309][root][INFO] - Training Epoch: 1/2, step 4711/7134 completed (loss: 0.08082439005374908, acc: 0.9745330810546875)
[2025-02-13 03:05:31,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:31,716][root][INFO] - Training Epoch: 1/2, step 4712/7134 completed (loss: 0.10633529722690582, acc: 0.9651514887809753)
[2025-02-13 03:05:31,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:32,125][root][INFO] - Training Epoch: 1/2, step 4713/7134 completed (loss: 0.033170152455568314, acc: 0.987500011920929)
[2025-02-13 03:05:32,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:32,476][root][INFO] - Training Epoch: 1/2, step 4714/7134 completed (loss: 0.08374268561601639, acc: 0.9757009148597717)
[2025-02-13 03:05:32,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:32,874][root][INFO] - Training Epoch: 1/2, step 4715/7134 completed (loss: 0.1279473900794983, acc: 0.9658848643302917)
[2025-02-13 03:05:32,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:33,260][root][INFO] - Training Epoch: 1/2, step 4716/7134 completed (loss: 0.016759952530264854, acc: 0.9923780560493469)
[2025-02-13 03:05:33,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:33,653][root][INFO] - Training Epoch: 1/2, step 4717/7134 completed (loss: 0.047645844519138336, acc: 0.9911110997200012)
[2025-02-13 03:05:33,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:34,052][root][INFO] - Training Epoch: 1/2, step 4718/7134 completed (loss: 0.0476064458489418, acc: 0.9850993156433105)
[2025-02-13 03:05:34,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:34,469][root][INFO] - Training Epoch: 1/2, step 4719/7134 completed (loss: 0.03490709885954857, acc: 0.991150438785553)
[2025-02-13 03:05:34,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:34,891][root][INFO] - Training Epoch: 1/2, step 4720/7134 completed (loss: 0.011201859451830387, acc: 0.9971949458122253)
[2025-02-13 03:05:35,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:35,282][root][INFO] - Training Epoch: 1/2, step 4721/7134 completed (loss: 0.012798802927136421, acc: 0.9942362904548645)
[2025-02-13 03:05:35,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:35,678][root][INFO] - Training Epoch: 1/2, step 4722/7134 completed (loss: 0.027563033625483513, acc: 0.9896449446678162)
[2025-02-13 03:05:35,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:36,073][root][INFO] - Training Epoch: 1/2, step 4723/7134 completed (loss: 0.015693118795752525, acc: 0.9939024448394775)
[2025-02-13 03:05:36,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:36,528][root][INFO] - Training Epoch: 1/2, step 4724/7134 completed (loss: 0.0252826064825058, acc: 0.9911242723464966)
[2025-02-13 03:05:36,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:36,952][root][INFO] - Training Epoch: 1/2, step 4725/7134 completed (loss: 0.028419051319360733, acc: 0.9960421919822693)
[2025-02-13 03:05:37,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:37,361][root][INFO] - Training Epoch: 1/2, step 4726/7134 completed (loss: 0.053562115877866745, acc: 0.980141818523407)
[2025-02-13 03:05:37,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:37,765][root][INFO] - Training Epoch: 1/2, step 4727/7134 completed (loss: 0.046052731573581696, acc: 0.9874607920646667)
[2025-02-13 03:05:37,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:38,169][root][INFO] - Training Epoch: 1/2, step 4728/7134 completed (loss: 0.057042550295591354, acc: 0.9901153445243835)
[2025-02-13 03:05:38,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:38,573][root][INFO] - Training Epoch: 1/2, step 4729/7134 completed (loss: 0.02262914925813675, acc: 0.9957746267318726)
[2025-02-13 03:05:38,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:38,988][root][INFO] - Training Epoch: 1/2, step 4730/7134 completed (loss: 0.03378643840551376, acc: 0.9877488613128662)
[2025-02-13 03:05:39,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:39,389][root][INFO] - Training Epoch: 1/2, step 4731/7134 completed (loss: 0.052598413079977036, acc: 0.989313006401062)
[2025-02-13 03:05:39,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:39,793][root][INFO] - Training Epoch: 1/2, step 4732/7134 completed (loss: 0.06570355594158173, acc: 0.9818511605262756)
[2025-02-13 03:05:39,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:40,203][root][INFO] - Training Epoch: 1/2, step 4733/7134 completed (loss: 0.028147079050540924, acc: 0.9919224381446838)
[2025-02-13 03:05:40,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:40,617][root][INFO] - Training Epoch: 1/2, step 4734/7134 completed (loss: 0.06778741627931595, acc: 0.9825119376182556)
[2025-02-13 03:05:40,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:41,043][root][INFO] - Training Epoch: 1/2, step 4735/7134 completed (loss: 0.04030763730406761, acc: 0.9864864945411682)
[2025-02-13 03:05:41,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:41,404][root][INFO] - Training Epoch: 1/2, step 4736/7134 completed (loss: 0.06411555409431458, acc: 0.9842209219932556)
[2025-02-13 03:05:41,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:41,818][root][INFO] - Training Epoch: 1/2, step 4737/7134 completed (loss: 0.03875328227877617, acc: 0.9890282154083252)
[2025-02-13 03:05:41,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:42,218][root][INFO] - Training Epoch: 1/2, step 4738/7134 completed (loss: 0.03874102234840393, acc: 0.987860381603241)
[2025-02-13 03:05:42,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:42,624][root][INFO] - Training Epoch: 1/2, step 4739/7134 completed (loss: 0.09398359060287476, acc: 0.9728434681892395)
[2025-02-13 03:05:42,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:43,045][root][INFO] - Training Epoch: 1/2, step 4740/7134 completed (loss: 0.08954616636037827, acc: 0.9805389046669006)
[2025-02-13 03:05:43,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:43,391][root][INFO] - Training Epoch: 1/2, step 4741/7134 completed (loss: 0.06460072845220566, acc: 0.9741219878196716)
[2025-02-13 03:05:43,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:43,793][root][INFO] - Training Epoch: 1/2, step 4742/7134 completed (loss: 0.11611182242631912, acc: 0.9663716554641724)
[2025-02-13 03:05:43,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:44,187][root][INFO] - Training Epoch: 1/2, step 4743/7134 completed (loss: 0.07806964963674545, acc: 0.9804270267486572)
[2025-02-13 03:05:44,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:44,513][root][INFO] - Training Epoch: 1/2, step 4744/7134 completed (loss: 0.06138934940099716, acc: 0.9814049601554871)
[2025-02-13 03:05:44,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:44,833][root][INFO] - Training Epoch: 1/2, step 4745/7134 completed (loss: 0.08423440158367157, acc: 0.9799330830574036)
[2025-02-13 03:05:44,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:45,236][root][INFO] - Training Epoch: 1/2, step 4746/7134 completed (loss: 0.07150478661060333, acc: 0.9852941036224365)
[2025-02-13 03:05:45,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:45,667][root][INFO] - Training Epoch: 1/2, step 4747/7134 completed (loss: 0.05419868975877762, acc: 0.9851552248001099)
[2025-02-13 03:05:45,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:46,097][root][INFO] - Training Epoch: 1/2, step 4748/7134 completed (loss: 0.0904470831155777, acc: 0.9734513163566589)
[2025-02-13 03:05:46,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:46,533][root][INFO] - Training Epoch: 1/2, step 4749/7134 completed (loss: 0.07284730672836304, acc: 0.9773333072662354)
[2025-02-13 03:05:46,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:46,958][root][INFO] - Training Epoch: 1/2, step 4750/7134 completed (loss: 0.09847886860370636, acc: 0.9710982441902161)
[2025-02-13 03:05:47,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:47,415][root][INFO] - Training Epoch: 1/2, step 4751/7134 completed (loss: 0.04078076407313347, acc: 0.9886845946311951)
[2025-02-13 03:05:47,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:47,852][root][INFO] - Training Epoch: 1/2, step 4752/7134 completed (loss: 0.09407695382833481, acc: 0.9846389889717102)
[2025-02-13 03:05:47,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:48,258][root][INFO] - Training Epoch: 1/2, step 4753/7134 completed (loss: 0.09023396670818329, acc: 0.9789156913757324)
[2025-02-13 03:05:48,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:48,659][root][INFO] - Training Epoch: 1/2, step 4754/7134 completed (loss: 0.03243110701441765, acc: 0.9941262602806091)
[2025-02-13 03:05:48,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:49,092][root][INFO] - Training Epoch: 1/2, step 4755/7134 completed (loss: 0.06630443036556244, acc: 0.9833564758300781)
[2025-02-13 03:05:49,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:49,480][root][INFO] - Training Epoch: 1/2, step 4756/7134 completed (loss: 0.09733136743307114, acc: 0.9769874215126038)
[2025-02-13 03:05:49,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:49,953][root][INFO] - Training Epoch: 1/2, step 4757/7134 completed (loss: 0.056002192199230194, acc: 0.9862227439880371)
[2025-02-13 03:05:50,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:50,381][root][INFO] - Training Epoch: 1/2, step 4758/7134 completed (loss: 0.04618339613080025, acc: 0.9887955188751221)
[2025-02-13 03:05:50,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:50,816][root][INFO] - Training Epoch: 1/2, step 4759/7134 completed (loss: 0.03181974217295647, acc: 0.9913793206214905)
[2025-02-13 03:05:50,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:51,219][root][INFO] - Training Epoch: 1/2, step 4760/7134 completed (loss: 0.0763629600405693, acc: 0.977225661277771)
[2025-02-13 03:05:51,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:51,624][root][INFO] - Training Epoch: 1/2, step 4761/7134 completed (loss: 0.05639109015464783, acc: 0.9908397197723389)
[2025-02-13 03:05:51,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:52,046][root][INFO] - Training Epoch: 1/2, step 4762/7134 completed (loss: 0.05960577726364136, acc: 0.9891892075538635)
[2025-02-13 03:05:52,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:52,456][root][INFO] - Training Epoch: 1/2, step 4763/7134 completed (loss: 0.06449899822473526, acc: 0.9817629456520081)
[2025-02-13 03:05:52,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:52,894][root][INFO] - Training Epoch: 1/2, step 4764/7134 completed (loss: 0.06121167540550232, acc: 0.9814586043357849)
[2025-02-13 03:05:53,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:53,312][root][INFO] - Training Epoch: 1/2, step 4765/7134 completed (loss: 0.09096556901931763, acc: 0.9827337861061096)
[2025-02-13 03:05:53,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:53,750][root][INFO] - Training Epoch: 1/2, step 4766/7134 completed (loss: 0.06083700805902481, acc: 0.9884169697761536)
[2025-02-13 03:05:53,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:54,072][root][INFO] - Training Epoch: 1/2, step 4767/7134 completed (loss: 0.06540115922689438, acc: 0.9821882843971252)
[2025-02-13 03:05:54,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:54,534][root][INFO] - Training Epoch: 1/2, step 4768/7134 completed (loss: 0.09089236706495285, acc: 0.9756097793579102)
[2025-02-13 03:05:54,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:54,926][root][INFO] - Training Epoch: 1/2, step 4769/7134 completed (loss: 0.0807182714343071, acc: 0.9807383418083191)
[2025-02-13 03:05:55,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:55,318][root][INFO] - Training Epoch: 1/2, step 4770/7134 completed (loss: 0.03659799322485924, acc: 0.9878787994384766)
[2025-02-13 03:05:55,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:55,708][root][INFO] - Training Epoch: 1/2, step 4771/7134 completed (loss: 0.12240784615278244, acc: 0.9680232405662537)
[2025-02-13 03:05:55,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:56,109][root][INFO] - Training Epoch: 1/2, step 4772/7134 completed (loss: 0.06623566150665283, acc: 0.9904305934906006)
[2025-02-13 03:05:56,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:56,503][root][INFO] - Training Epoch: 1/2, step 4773/7134 completed (loss: 0.06599946320056915, acc: 0.9849315285682678)
[2025-02-13 03:05:56,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:56,886][root][INFO] - Training Epoch: 1/2, step 4774/7134 completed (loss: 0.07079442590475082, acc: 0.983146071434021)
[2025-02-13 03:05:57,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:57,286][root][INFO] - Training Epoch: 1/2, step 4775/7134 completed (loss: 0.0744667649269104, acc: 0.9822866320610046)
[2025-02-13 03:05:57,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:57,701][root][INFO] - Training Epoch: 1/2, step 4776/7134 completed (loss: 0.059872787445783615, acc: 0.9855538010597229)
[2025-02-13 03:05:57,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:58,141][root][INFO] - Training Epoch: 1/2, step 4777/7134 completed (loss: 0.03847119212150574, acc: 0.989983320236206)
[2025-02-13 03:05:58,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:58,540][root][INFO] - Training Epoch: 1/2, step 4778/7134 completed (loss: 0.04100906103849411, acc: 0.9912472367286682)
[2025-02-13 03:05:58,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:58,981][root][INFO] - Training Epoch: 1/2, step 4779/7134 completed (loss: 0.05602320656180382, acc: 0.9871086478233337)
[2025-02-13 03:05:59,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:59,403][root][INFO] - Training Epoch: 1/2, step 4780/7134 completed (loss: 0.012705488130450249, acc: 0.9953271150588989)
[2025-02-13 03:05:59,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:59,815][root][INFO] - Training Epoch: 1/2, step 4781/7134 completed (loss: 0.02068391442298889, acc: 0.9941291809082031)
[2025-02-13 03:05:59,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:00,243][root][INFO] - Training Epoch: 1/2, step 4782/7134 completed (loss: 0.045401398092508316, acc: 0.9837925434112549)
[2025-02-13 03:06:00,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:00,651][root][INFO] - Training Epoch: 1/2, step 4783/7134 completed (loss: 0.06790091097354889, acc: 0.9839857816696167)
[2025-02-13 03:06:00,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:01,094][root][INFO] - Training Epoch: 1/2, step 4784/7134 completed (loss: 0.02978350594639778, acc: 0.9918144345283508)
[2025-02-13 03:06:01,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:01,494][root][INFO] - Training Epoch: 1/2, step 4785/7134 completed (loss: 0.07737353444099426, acc: 0.983146071434021)
[2025-02-13 03:06:01,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:01,905][root][INFO] - Training Epoch: 1/2, step 4786/7134 completed (loss: 0.063389353454113, acc: 0.9860248565673828)
[2025-02-13 03:06:02,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:02,315][root][INFO] - Training Epoch: 1/2, step 4787/7134 completed (loss: 0.01248709112405777, acc: 0.996835470199585)
[2025-02-13 03:06:02,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:02,721][root][INFO] - Training Epoch: 1/2, step 4788/7134 completed (loss: 0.020111599937081337, acc: 0.9946808218955994)
[2025-02-13 03:06:02,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:03,120][root][INFO] - Training Epoch: 1/2, step 4789/7134 completed (loss: 0.04192616418004036, acc: 0.993630588054657)
[2025-02-13 03:06:03,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:03,558][root][INFO] - Training Epoch: 1/2, step 4790/7134 completed (loss: 0.03502460569143295, acc: 0.991847813129425)
[2025-02-13 03:06:03,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:03,959][root][INFO] - Training Epoch: 1/2, step 4791/7134 completed (loss: 0.026516200974583626, acc: 0.9925512075424194)
[2025-02-13 03:06:04,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:04,378][root][INFO] - Training Epoch: 1/2, step 4792/7134 completed (loss: 0.02776404283940792, acc: 0.9933110475540161)
[2025-02-13 03:06:04,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:04,728][root][INFO] - Training Epoch: 1/2, step 4793/7134 completed (loss: 0.06255519390106201, acc: 0.9780821800231934)
[2025-02-13 03:06:04,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:05,128][root][INFO] - Training Epoch: 1/2, step 4794/7134 completed (loss: 0.07835842669010162, acc: 0.9752380847930908)
[2025-02-13 03:06:05,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:05,558][root][INFO] - Training Epoch: 1/2, step 4795/7134 completed (loss: 0.030061662197113037, acc: 0.9872881174087524)
[2025-02-13 03:06:05,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:05,952][root][INFO] - Training Epoch: 1/2, step 4796/7134 completed (loss: 0.03257684037089348, acc: 0.9874213933944702)
[2025-02-13 03:06:06,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:06,389][root][INFO] - Training Epoch: 1/2, step 4797/7134 completed (loss: 0.030470633879303932, acc: 0.9900621175765991)
[2025-02-13 03:06:06,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:06,801][root][INFO] - Training Epoch: 1/2, step 4798/7134 completed (loss: 0.044527873396873474, acc: 0.989347517490387)
[2025-02-13 03:06:06,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:07,246][root][INFO] - Training Epoch: 1/2, step 4799/7134 completed (loss: 0.03754888102412224, acc: 0.985855758190155)
[2025-02-13 03:06:07,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:07,651][root][INFO] - Training Epoch: 1/2, step 4800/7134 completed (loss: 0.04536178708076477, acc: 0.9880775213241577)
[2025-02-13 03:06:07,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:08,091][root][INFO] - Training Epoch: 1/2, step 4801/7134 completed (loss: 0.024392375722527504, acc: 0.9920814633369446)
[2025-02-13 03:06:08,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:08,534][root][INFO] - Training Epoch: 1/2, step 4802/7134 completed (loss: 0.06113926321268082, acc: 0.9869109988212585)
[2025-02-13 03:06:08,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:08,924][root][INFO] - Training Epoch: 1/2, step 4803/7134 completed (loss: 0.06439409404993057, acc: 0.9867647290229797)
[2025-02-13 03:06:09,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:09,357][root][INFO] - Training Epoch: 1/2, step 4804/7134 completed (loss: 0.05924694985151291, acc: 0.9815384745597839)
[2025-02-13 03:06:09,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:09,787][root][INFO] - Training Epoch: 1/2, step 4805/7134 completed (loss: 0.05263892188668251, acc: 0.991909384727478)
[2025-02-13 03:06:09,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:10,209][root][INFO] - Training Epoch: 1/2, step 4806/7134 completed (loss: 0.04728153347969055, acc: 0.9933444261550903)
[2025-02-13 03:06:10,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:10,586][root][INFO] - Training Epoch: 1/2, step 4807/7134 completed (loss: 0.10186424106359482, acc: 0.9848024249076843)
[2025-02-13 03:06:10,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:10,968][root][INFO] - Training Epoch: 1/2, step 4808/7134 completed (loss: 0.06690650433301926, acc: 0.9858155846595764)
[2025-02-13 03:06:11,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:11,391][root][INFO] - Training Epoch: 1/2, step 4809/7134 completed (loss: 0.03958404064178467, acc: 0.9841269850730896)
[2025-02-13 03:06:11,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:11,839][root][INFO] - Training Epoch: 1/2, step 4810/7134 completed (loss: 0.06855463981628418, acc: 0.9871612191200256)
[2025-02-13 03:06:11,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:12,231][root][INFO] - Training Epoch: 1/2, step 4811/7134 completed (loss: 0.06574945151805878, acc: 0.9864341020584106)
[2025-02-13 03:06:12,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:12,651][root][INFO] - Training Epoch: 1/2, step 4812/7134 completed (loss: 0.09915582090616226, acc: 0.9819193482398987)
[2025-02-13 03:06:12,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:12,980][root][INFO] - Training Epoch: 1/2, step 4813/7134 completed (loss: 0.013933036476373672, acc: 0.9974026083946228)
[2025-02-13 03:06:13,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:13,397][root][INFO] - Training Epoch: 1/2, step 4814/7134 completed (loss: 0.041970282793045044, acc: 0.9897959232330322)
[2025-02-13 03:06:13,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:13,706][root][INFO] - Training Epoch: 1/2, step 4815/7134 completed (loss: 0.0051893265917897224, acc: 1.0)
[2025-02-13 03:06:13,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:14,120][root][INFO] - Training Epoch: 1/2, step 4816/7134 completed (loss: 0.022266598418354988, acc: 0.9938119053840637)
[2025-02-13 03:06:14,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:14,514][root][INFO] - Training Epoch: 1/2, step 4817/7134 completed (loss: 0.020887622609734535, acc: 0.9927710890769958)
[2025-02-13 03:06:14,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:14,979][root][INFO] - Training Epoch: 1/2, step 4818/7134 completed (loss: 0.028018997982144356, acc: 0.9923076629638672)
[2025-02-13 03:06:15,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:15,381][root][INFO] - Training Epoch: 1/2, step 4819/7134 completed (loss: 0.03666108846664429, acc: 0.9918864369392395)
[2025-02-13 03:06:15,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:15,820][root][INFO] - Training Epoch: 1/2, step 4820/7134 completed (loss: 0.018783433362841606, acc: 0.993565022945404)
[2025-02-13 03:06:15,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:16,256][root][INFO] - Training Epoch: 1/2, step 4821/7134 completed (loss: 0.022839024662971497, acc: 0.994413435459137)
[2025-02-13 03:06:16,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:16,669][root][INFO] - Training Epoch: 1/2, step 4822/7134 completed (loss: 0.028719501569867134, acc: 0.9943289160728455)
[2025-02-13 03:06:16,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:17,100][root][INFO] - Training Epoch: 1/2, step 4823/7134 completed (loss: 0.07062459737062454, acc: 0.9842022061347961)
[2025-02-13 03:06:17,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:17,471][root][INFO] - Training Epoch: 1/2, step 4824/7134 completed (loss: 0.06368143856525421, acc: 0.9771689772605896)
[2025-02-13 03:06:17,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:17,875][root][INFO] - Training Epoch: 1/2, step 4825/7134 completed (loss: 0.01928945630788803, acc: 0.9927007555961609)
[2025-02-13 03:06:18,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:18,323][root][INFO] - Training Epoch: 1/2, step 4826/7134 completed (loss: 0.03318942338228226, acc: 0.9892703890800476)
[2025-02-13 03:06:18,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:18,717][root][INFO] - Training Epoch: 1/2, step 4827/7134 completed (loss: 0.04152815043926239, acc: 0.9896193742752075)
[2025-02-13 03:06:18,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:19,146][root][INFO] - Training Epoch: 1/2, step 4828/7134 completed (loss: 0.024217650294303894, acc: 0.9939246773719788)
[2025-02-13 03:06:19,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:19,597][root][INFO] - Training Epoch: 1/2, step 4829/7134 completed (loss: 0.02531890943646431, acc: 0.9961538314819336)
[2025-02-13 03:06:19,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:20,035][root][INFO] - Training Epoch: 1/2, step 4830/7134 completed (loss: 0.02089468017220497, acc: 0.9955621361732483)
[2025-02-13 03:06:20,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:20,472][root][INFO] - Training Epoch: 1/2, step 4831/7134 completed (loss: 0.013824683614075184, acc: 0.9956140518188477)
[2025-02-13 03:06:20,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:20,887][root][INFO] - Training Epoch: 1/2, step 4832/7134 completed (loss: 0.013716625981032848, acc: 0.9983713626861572)
[2025-02-13 03:06:21,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:21,337][root][INFO] - Training Epoch: 1/2, step 4833/7134 completed (loss: 0.018257584422826767, acc: 0.9949173927307129)
[2025-02-13 03:06:21,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:21,771][root][INFO] - Training Epoch: 1/2, step 4834/7134 completed (loss: 0.02179713174700737, acc: 0.9952380657196045)
[2025-02-13 03:06:21,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:22,192][root][INFO] - Training Epoch: 1/2, step 4835/7134 completed (loss: 0.02118881605565548, acc: 0.995945930480957)
[2025-02-13 03:06:22,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:22,620][root][INFO] - Training Epoch: 1/2, step 4836/7134 completed (loss: 0.014457744546234608, acc: 0.9938931465148926)
[2025-02-13 03:06:22,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:23,012][root][INFO] - Training Epoch: 1/2, step 4837/7134 completed (loss: 0.09146725386381149, acc: 0.9834558963775635)
[2025-02-13 03:06:23,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:23,425][root][INFO] - Training Epoch: 1/2, step 4838/7134 completed (loss: 0.0386158749461174, acc: 0.9919484853744507)
[2025-02-13 03:06:23,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:23,809][root][INFO] - Training Epoch: 1/2, step 4839/7134 completed (loss: 0.08616741746664047, acc: 0.9810671210289001)
[2025-02-13 03:06:23,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:24,229][root][INFO] - Training Epoch: 1/2, step 4840/7134 completed (loss: 0.07942910492420197, acc: 0.9783236980438232)
[2025-02-13 03:06:24,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:24,618][root][INFO] - Training Epoch: 1/2, step 4841/7134 completed (loss: 0.034877460449934006, acc: 0.9940387606620789)
[2025-02-13 03:06:24,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:25,056][root][INFO] - Training Epoch: 1/2, step 4842/7134 completed (loss: 0.05614126846194267, acc: 0.9821428656578064)
[2025-02-13 03:06:25,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:25,496][root][INFO] - Training Epoch: 1/2, step 4843/7134 completed (loss: 0.03960786387324333, acc: 0.9881129264831543)
[2025-02-13 03:06:25,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:25,867][root][INFO] - Training Epoch: 1/2, step 4844/7134 completed (loss: 0.033987198024988174, acc: 0.9854604005813599)
[2025-02-13 03:06:26,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:26,302][root][INFO] - Training Epoch: 1/2, step 4845/7134 completed (loss: 0.03925475850701332, acc: 0.9890710115432739)
[2025-02-13 03:06:26,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:26,713][root][INFO] - Training Epoch: 1/2, step 4846/7134 completed (loss: 0.055718794465065, acc: 0.9837837815284729)
[2025-02-13 03:06:26,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:27,106][root][INFO] - Training Epoch: 1/2, step 4847/7134 completed (loss: 0.038589466363191605, acc: 0.9919137358665466)
[2025-02-13 03:06:27,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:27,514][root][INFO] - Training Epoch: 1/2, step 4848/7134 completed (loss: 0.03549918904900551, acc: 0.9944751262664795)
[2025-02-13 03:06:27,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:27,897][root][INFO] - Training Epoch: 1/2, step 4849/7134 completed (loss: 0.03013860248029232, acc: 0.9925705790519714)
[2025-02-13 03:06:28,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:28,329][root][INFO] - Training Epoch: 1/2, step 4850/7134 completed (loss: 0.04159647598862648, acc: 0.984088122844696)
[2025-02-13 03:06:28,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:28,781][root][INFO] - Training Epoch: 1/2, step 4851/7134 completed (loss: 0.04933162406086922, acc: 0.9848130941390991)
[2025-02-13 03:06:28,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:29,215][root][INFO] - Training Epoch: 1/2, step 4852/7134 completed (loss: 0.044504158198833466, acc: 0.9866844415664673)
[2025-02-13 03:06:29,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:29,606][root][INFO] - Training Epoch: 1/2, step 4853/7134 completed (loss: 0.10263673216104507, acc: 0.9771754741668701)
[2025-02-13 03:06:29,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:29,992][root][INFO] - Training Epoch: 1/2, step 4854/7134 completed (loss: 0.043342433869838715, acc: 0.9853333234786987)
[2025-02-13 03:06:30,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:30,425][root][INFO] - Training Epoch: 1/2, step 4855/7134 completed (loss: 0.04388769716024399, acc: 0.987908124923706)
[2025-02-13 03:06:30,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:30,879][root][INFO] - Training Epoch: 1/2, step 4856/7134 completed (loss: 0.04585477337241173, acc: 0.9885495901107788)
[2025-02-13 03:06:31,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:31,275][root][INFO] - Training Epoch: 1/2, step 4857/7134 completed (loss: 0.016168050467967987, acc: 0.9956834316253662)
[2025-02-13 03:06:31,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:31,701][root][INFO] - Training Epoch: 1/2, step 4858/7134 completed (loss: 0.03312135487794876, acc: 0.9900285005569458)
[2025-02-13 03:06:31,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:32,135][root][INFO] - Training Epoch: 1/2, step 4859/7134 completed (loss: 0.017992453649640083, acc: 0.9948520064353943)
[2025-02-13 03:06:32,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:32,551][root][INFO] - Training Epoch: 1/2, step 4860/7134 completed (loss: 0.03485703468322754, acc: 0.9891008138656616)
[2025-02-13 03:06:32,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:32,962][root][INFO] - Training Epoch: 1/2, step 4861/7134 completed (loss: 0.03736647963523865, acc: 0.986522912979126)
[2025-02-13 03:06:33,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:33,374][root][INFO] - Training Epoch: 1/2, step 4862/7134 completed (loss: 0.03181181475520134, acc: 0.991416335105896)
[2025-02-13 03:06:33,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:33,789][root][INFO] - Training Epoch: 1/2, step 4863/7134 completed (loss: 0.03538290783762932, acc: 0.9908758997917175)
[2025-02-13 03:06:33,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:34,193][root][INFO] - Training Epoch: 1/2, step 4864/7134 completed (loss: 0.04952557012438774, acc: 0.9815384745597839)
[2025-02-13 03:06:34,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:34,621][root][INFO] - Training Epoch: 1/2, step 4865/7134 completed (loss: 0.04234844073653221, acc: 0.9890109896659851)
[2025-02-13 03:06:34,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:35,079][root][INFO] - Training Epoch: 1/2, step 4866/7134 completed (loss: 0.038505446165800095, acc: 0.9870298504829407)
[2025-02-13 03:06:35,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:35,514][root][INFO] - Training Epoch: 1/2, step 4867/7134 completed (loss: 0.0541209913790226, acc: 0.9894179701805115)
[2025-02-13 03:06:35,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:35,906][root][INFO] - Training Epoch: 1/2, step 4868/7134 completed (loss: 0.06390774250030518, acc: 0.9825581312179565)
[2025-02-13 03:06:36,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:36,301][root][INFO] - Training Epoch: 1/2, step 4869/7134 completed (loss: 0.060795001685619354, acc: 0.9822221994400024)
[2025-02-13 03:06:36,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:36,737][root][INFO] - Training Epoch: 1/2, step 4870/7134 completed (loss: 0.027997085824608803, acc: 0.9953051805496216)
[2025-02-13 03:06:36,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:37,123][root][INFO] - Training Epoch: 1/2, step 4871/7134 completed (loss: 0.053336430341005325, acc: 0.9907407164573669)
[2025-02-13 03:06:37,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:37,532][root][INFO] - Training Epoch: 1/2, step 4872/7134 completed (loss: 0.029841233044862747, acc: 0.9916550517082214)
[2025-02-13 03:06:37,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:37,984][root][INFO] - Training Epoch: 1/2, step 4873/7134 completed (loss: 0.03125368431210518, acc: 0.9908854365348816)
[2025-02-13 03:06:38,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:38,429][root][INFO] - Training Epoch: 1/2, step 4874/7134 completed (loss: 0.02366515062749386, acc: 0.99314284324646)
[2025-02-13 03:06:38,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:38,887][root][INFO] - Training Epoch: 1/2, step 4875/7134 completed (loss: 0.047537174075841904, acc: 0.9875173568725586)
[2025-02-13 03:06:39,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:39,239][root][INFO] - Training Epoch: 1/2, step 4876/7134 completed (loss: 0.08781658858060837, acc: 0.9788583517074585)
[2025-02-13 03:06:39,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:39,666][root][INFO] - Training Epoch: 1/2, step 4877/7134 completed (loss: 0.05072226747870445, acc: 0.9832258224487305)
[2025-02-13 03:06:39,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:40,070][root][INFO] - Training Epoch: 1/2, step 4878/7134 completed (loss: 0.04138650372624397, acc: 0.9808542132377625)
[2025-02-13 03:06:40,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:40,512][root][INFO] - Training Epoch: 1/2, step 4879/7134 completed (loss: 0.06732991337776184, acc: 0.983433723449707)
[2025-02-13 03:06:40,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:40,940][root][INFO] - Training Epoch: 1/2, step 4880/7134 completed (loss: 0.02178230695426464, acc: 0.9948119521141052)
[2025-02-13 03:06:41,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:41,381][root][INFO] - Training Epoch: 1/2, step 4881/7134 completed (loss: 0.06570742279291153, acc: 0.9824817776679993)
[2025-02-13 03:06:41,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:41,784][root][INFO] - Training Epoch: 1/2, step 4882/7134 completed (loss: 0.05089528113603592, acc: 0.9851973652839661)
[2025-02-13 03:06:41,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:42,185][root][INFO] - Training Epoch: 1/2, step 4883/7134 completed (loss: 0.04603395611047745, acc: 0.9824817776679993)
[2025-02-13 03:06:42,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:42,617][root][INFO] - Training Epoch: 1/2, step 4884/7134 completed (loss: 0.015142683871090412, acc: 0.996927797794342)
[2025-02-13 03:06:42,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:43,002][root][INFO] - Training Epoch: 1/2, step 4885/7134 completed (loss: 0.013507749885320663, acc: 0.9973614811897278)
[2025-02-13 03:06:43,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:43,399][root][INFO] - Training Epoch: 1/2, step 4886/7134 completed (loss: 0.02808978408575058, acc: 0.9897435903549194)
[2025-02-13 03:06:43,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:43,853][root][INFO] - Training Epoch: 1/2, step 4887/7134 completed (loss: 0.018134841695427895, acc: 0.9940263032913208)
[2025-02-13 03:06:44,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:44,313][root][INFO] - Training Epoch: 1/2, step 4888/7134 completed (loss: 0.05495075508952141, acc: 0.9903846383094788)
[2025-02-13 03:06:44,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:44,754][root][INFO] - Training Epoch: 1/2, step 4889/7134 completed (loss: 0.02122177742421627, acc: 0.9951865077018738)
[2025-02-13 03:06:44,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:45,121][root][INFO] - Training Epoch: 1/2, step 4890/7134 completed (loss: 0.052691105753183365, acc: 0.9928571581840515)
[2025-02-13 03:06:45,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:45,551][root][INFO] - Training Epoch: 1/2, step 4891/7134 completed (loss: 0.05271458998322487, acc: 0.9888059496879578)
[2025-02-13 03:06:45,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:45,953][root][INFO] - Training Epoch: 1/2, step 4892/7134 completed (loss: 0.07360534369945526, acc: 0.9798657894134521)
[2025-02-13 03:06:46,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:46,372][root][INFO] - Training Epoch: 1/2, step 4893/7134 completed (loss: 0.19538557529449463, acc: 0.960422158241272)
[2025-02-13 03:06:46,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:46,803][root][INFO] - Training Epoch: 1/2, step 4894/7134 completed (loss: 0.028179600834846497, acc: 0.9913544654846191)
[2025-02-13 03:06:46,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:47,243][root][INFO] - Training Epoch: 1/2, step 4895/7134 completed (loss: 0.019965605810284615, acc: 0.9925000071525574)
[2025-02-13 03:06:47,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:47,637][root][INFO] - Training Epoch: 1/2, step 4896/7134 completed (loss: 0.01575072482228279, acc: 0.9943820238113403)
[2025-02-13 03:06:47,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:48,081][root][INFO] - Training Epoch: 1/2, step 4897/7134 completed (loss: 0.041865911334753036, acc: 0.9859648942947388)
[2025-02-13 03:06:48,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:48,535][root][INFO] - Training Epoch: 1/2, step 4898/7134 completed (loss: 0.03900172933936119, acc: 0.9899371266365051)
[2025-02-13 03:06:48,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:48,968][root][INFO] - Training Epoch: 1/2, step 4899/7134 completed (loss: 0.031345296651124954, acc: 0.9898107647895813)
[2025-02-13 03:06:49,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:49,396][root][INFO] - Training Epoch: 1/2, step 4900/7134 completed (loss: 0.04250211641192436, acc: 0.9894737005233765)
[2025-02-13 03:06:49,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:49,829][root][INFO] - Training Epoch: 1/2, step 4901/7134 completed (loss: 0.03296959772706032, acc: 0.9909774661064148)
[2025-02-13 03:06:49,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:50,271][root][INFO] - Training Epoch: 1/2, step 4902/7134 completed (loss: 0.030231082811951637, acc: 0.9933686852455139)
[2025-02-13 03:06:50,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:50,686][root][INFO] - Training Epoch: 1/2, step 4903/7134 completed (loss: 0.02767939865589142, acc: 0.9928315281867981)
[2025-02-13 03:06:50,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:51,097][root][INFO] - Training Epoch: 1/2, step 4904/7134 completed (loss: 0.03405436500906944, acc: 0.9925373196601868)
[2025-02-13 03:06:51,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:51,508][root][INFO] - Training Epoch: 1/2, step 4905/7134 completed (loss: 0.04704746976494789, acc: 0.987261176109314)
[2025-02-13 03:06:51,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:51,909][root][INFO] - Training Epoch: 1/2, step 4906/7134 completed (loss: 0.03134351223707199, acc: 0.9932546615600586)
[2025-02-13 03:06:52,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:52,321][root][INFO] - Training Epoch: 1/2, step 4907/7134 completed (loss: 0.031822219491004944, acc: 0.9936708807945251)
[2025-02-13 03:06:52,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:52,740][root][INFO] - Training Epoch: 1/2, step 4908/7134 completed (loss: 0.018743285909295082, acc: 0.9904000163078308)
[2025-02-13 03:06:52,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:53,180][root][INFO] - Training Epoch: 1/2, step 4909/7134 completed (loss: 0.011760630644857883, acc: 0.9967585206031799)
[2025-02-13 03:06:53,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:53,593][root][INFO] - Training Epoch: 1/2, step 4910/7134 completed (loss: 0.02908473089337349, acc: 0.9921630024909973)
[2025-02-13 03:06:53,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:54,027][root][INFO] - Training Epoch: 1/2, step 4911/7134 completed (loss: 0.04045233130455017, acc: 0.9894459247589111)
[2025-02-13 03:06:54,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:54,458][root][INFO] - Training Epoch: 1/2, step 4912/7134 completed (loss: 0.038874004036188126, acc: 0.991525411605835)
[2025-02-13 03:06:54,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:54,873][root][INFO] - Training Epoch: 1/2, step 4913/7134 completed (loss: 0.029918190091848373, acc: 0.9932432174682617)
[2025-02-13 03:06:55,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:55,291][root][INFO] - Training Epoch: 1/2, step 4914/7134 completed (loss: 0.02822388894855976, acc: 0.9908257126808167)
[2025-02-13 03:06:55,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:55,717][root][INFO] - Training Epoch: 1/2, step 4915/7134 completed (loss: 0.005226584151387215, acc: 1.0)
[2025-02-13 03:06:55,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:56,031][root][INFO] - Training Epoch: 1/2, step 4916/7134 completed (loss: 0.08685826510190964, acc: 0.9781553149223328)
[2025-02-13 03:06:56,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:56,469][root][INFO] - Training Epoch: 1/2, step 4917/7134 completed (loss: 0.0564860962331295, acc: 0.9874826073646545)
[2025-02-13 03:06:56,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:56,877][root][INFO] - Training Epoch: 1/2, step 4918/7134 completed (loss: 0.006262057926505804, acc: 1.0)
[2025-02-13 03:06:57,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:57,345][root][INFO] - Training Epoch: 1/2, step 4919/7134 completed (loss: 0.05203823372721672, acc: 0.9868420958518982)
[2025-02-13 03:06:57,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:57,792][root][INFO] - Training Epoch: 1/2, step 4920/7134 completed (loss: 0.08628246188163757, acc: 0.9793281555175781)
[2025-02-13 03:06:57,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:58,201][root][INFO] - Training Epoch: 1/2, step 4921/7134 completed (loss: 0.04299062117934227, acc: 0.991391658782959)
[2025-02-13 03:06:58,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:58,636][root][INFO] - Training Epoch: 1/2, step 4922/7134 completed (loss: 0.03131145238876343, acc: 0.9931972622871399)
[2025-02-13 03:06:58,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:59,076][root][INFO] - Training Epoch: 1/2, step 4923/7134 completed (loss: 0.024423489347100258, acc: 0.9926578402519226)
[2025-02-13 03:06:59,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:59,469][root][INFO] - Training Epoch: 1/2, step 4924/7134 completed (loss: 0.03787682205438614, acc: 0.9879999756813049)
[2025-02-13 03:06:59,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:59,875][root][INFO] - Training Epoch: 1/2, step 4925/7134 completed (loss: 0.03362417593598366, acc: 0.9856938719749451)
[2025-02-13 03:07:00,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:00,276][root][INFO] - Training Epoch: 1/2, step 4926/7134 completed (loss: 0.021821223199367523, acc: 0.9942611455917358)
[2025-02-13 03:07:00,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:00,672][root][INFO] - Training Epoch: 1/2, step 4927/7134 completed (loss: 0.03934550657868385, acc: 0.9887955188751221)
[2025-02-13 03:07:00,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:01,095][root][INFO] - Training Epoch: 1/2, step 4928/7134 completed (loss: 0.025968005880713463, acc: 0.9884124994277954)
[2025-02-13 03:07:01,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:01,481][root][INFO] - Training Epoch: 1/2, step 4929/7134 completed (loss: 0.028439121320843697, acc: 0.9891473054885864)
[2025-02-13 03:07:01,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:01,888][root][INFO] - Training Epoch: 1/2, step 4930/7134 completed (loss: 0.04881108179688454, acc: 0.9909502267837524)
[2025-02-13 03:07:02,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:02,294][root][INFO] - Training Epoch: 1/2, step 4931/7134 completed (loss: 0.07323174923658371, acc: 0.9847792983055115)
[2025-02-13 03:07:02,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:02,728][root][INFO] - Training Epoch: 1/2, step 4932/7134 completed (loss: 0.03148490563035011, acc: 0.9905533194541931)
[2025-02-13 03:07:02,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:03,159][root][INFO] - Training Epoch: 1/2, step 4933/7134 completed (loss: 0.048910755664110184, acc: 0.9864029884338379)
[2025-02-13 03:07:03,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:03,599][root][INFO] - Training Epoch: 1/2, step 4934/7134 completed (loss: 0.01477776188403368, acc: 0.9945873022079468)
[2025-02-13 03:07:03,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:04,038][root][INFO] - Training Epoch: 1/2, step 4935/7134 completed (loss: 0.026943378150463104, acc: 0.9917920827865601)
[2025-02-13 03:07:04,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:04,443][root][INFO] - Training Epoch: 1/2, step 4936/7134 completed (loss: 0.04259340092539787, acc: 0.9907407164573669)
[2025-02-13 03:07:04,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:04,852][root][INFO] - Training Epoch: 1/2, step 4937/7134 completed (loss: 0.03259760141372681, acc: 0.988041877746582)
[2025-02-13 03:07:04,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:05,266][root][INFO] - Training Epoch: 1/2, step 4938/7134 completed (loss: 0.03748833388090134, acc: 0.9878542423248291)
[2025-02-13 03:07:05,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:05,682][root][INFO] - Training Epoch: 1/2, step 4939/7134 completed (loss: 0.02489818073809147, acc: 0.990212082862854)
[2025-02-13 03:07:05,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:06,101][root][INFO] - Training Epoch: 1/2, step 4940/7134 completed (loss: 0.041392166167497635, acc: 0.9863636493682861)
[2025-02-13 03:07:06,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:06,528][root][INFO] - Training Epoch: 1/2, step 4941/7134 completed (loss: 0.03376920521259308, acc: 0.9919246435165405)
[2025-02-13 03:07:06,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:06,959][root][INFO] - Training Epoch: 1/2, step 4942/7134 completed (loss: 0.03173296898603439, acc: 0.9906166195869446)
[2025-02-13 03:07:07,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:07,380][root][INFO] - Training Epoch: 1/2, step 4943/7134 completed (loss: 0.04671965539455414, acc: 0.9863201379776001)
[2025-02-13 03:07:07,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:07,784][root][INFO] - Training Epoch: 1/2, step 4944/7134 completed (loss: 0.03660508990287781, acc: 0.9852631688117981)
[2025-02-13 03:07:07,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:08,151][root][INFO] - Training Epoch: 1/2, step 4945/7134 completed (loss: 0.06512901186943054, acc: 0.9737903475761414)
[2025-02-13 03:07:08,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:08,548][root][INFO] - Training Epoch: 1/2, step 4946/7134 completed (loss: 0.07152879983186722, acc: 0.9714285731315613)
[2025-02-13 03:07:08,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:08,957][root][INFO] - Training Epoch: 1/2, step 4947/7134 completed (loss: 0.04450368881225586, acc: 0.9844789505004883)
[2025-02-13 03:07:09,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:09,321][root][INFO] - Training Epoch: 1/2, step 4948/7134 completed (loss: 0.061062008142471313, acc: 0.9866310358047485)
[2025-02-13 03:07:09,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:09,627][root][INFO] - Training Epoch: 1/2, step 4949/7134 completed (loss: 0.06439220905303955, acc: 0.9777777791023254)
[2025-02-13 03:07:09,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:10,031][root][INFO] - Training Epoch: 1/2, step 4950/7134 completed (loss: 0.046377599239349365, acc: 0.9894921183586121)
[2025-02-13 03:07:10,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:10,447][root][INFO] - Training Epoch: 1/2, step 4951/7134 completed (loss: 0.04873676970601082, acc: 0.9857549667358398)
[2025-02-13 03:07:10,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:10,888][root][INFO] - Training Epoch: 1/2, step 4952/7134 completed (loss: 0.038549527525901794, acc: 0.9933775067329407)
[2025-02-13 03:07:11,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:11,239][root][INFO] - Training Epoch: 1/2, step 4953/7134 completed (loss: 0.02686372585594654, acc: 0.9912853837013245)
[2025-02-13 03:07:11,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:11,636][root][INFO] - Training Epoch: 1/2, step 4954/7134 completed (loss: 0.03105892799794674, acc: 0.9857142567634583)
[2025-02-13 03:07:11,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:11,979][root][INFO] - Training Epoch: 1/2, step 4955/7134 completed (loss: 0.03160353749990463, acc: 0.9882903695106506)
[2025-02-13 03:07:12,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:12,385][root][INFO] - Training Epoch: 1/2, step 4956/7134 completed (loss: 0.02783791534602642, acc: 0.991465151309967)
[2025-02-13 03:07:12,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:12,787][root][INFO] - Training Epoch: 1/2, step 4957/7134 completed (loss: 0.0274769589304924, acc: 0.9894179701805115)
[2025-02-13 03:07:12,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:13,194][root][INFO] - Training Epoch: 1/2, step 4958/7134 completed (loss: 0.012926180846989155, acc: 0.9956834316253662)
[2025-02-13 03:07:13,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:13,607][root][INFO] - Training Epoch: 1/2, step 4959/7134 completed (loss: 0.021700505167245865, acc: 0.9911764860153198)
[2025-02-13 03:07:13,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:14,007][root][INFO] - Training Epoch: 1/2, step 4960/7134 completed (loss: 0.021977907046675682, acc: 0.9931034445762634)
[2025-02-13 03:07:14,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:14,415][root][INFO] - Training Epoch: 1/2, step 4961/7134 completed (loss: 0.008321765810251236, acc: 0.9981784820556641)
[2025-02-13 03:07:14,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:14,818][root][INFO] - Training Epoch: 1/2, step 4962/7134 completed (loss: 0.006492553278803825, acc: 0.9984447956085205)
[2025-02-13 03:07:14,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:15,207][root][INFO] - Training Epoch: 1/2, step 4963/7134 completed (loss: 0.0371323823928833, acc: 0.9902597665786743)
[2025-02-13 03:07:15,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:15,605][root][INFO] - Training Epoch: 1/2, step 4964/7134 completed (loss: 0.033600103110075, acc: 0.9902439117431641)
[2025-02-13 03:07:15,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:16,001][root][INFO] - Training Epoch: 1/2, step 4965/7134 completed (loss: 0.04541727155447006, acc: 0.9852941036224365)
[2025-02-13 03:07:16,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:16,390][root][INFO] - Training Epoch: 1/2, step 4966/7134 completed (loss: 0.07800248265266418, acc: 0.9806094169616699)
[2025-02-13 03:07:16,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:16,793][root][INFO] - Training Epoch: 1/2, step 4967/7134 completed (loss: 0.060665447264909744, acc: 0.986328125)
[2025-02-13 03:07:16,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:17,218][root][INFO] - Training Epoch: 1/2, step 4968/7134 completed (loss: 0.09518173336982727, acc: 0.9715302586555481)
[2025-02-13 03:07:17,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:17,646][root][INFO] - Training Epoch: 1/2, step 4969/7134 completed (loss: 0.09784699976444244, acc: 0.9776632189750671)
[2025-02-13 03:07:17,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:18,038][root][INFO] - Training Epoch: 1/2, step 4970/7134 completed (loss: 0.03321082517504692, acc: 0.9899665713310242)
[2025-02-13 03:07:18,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:18,448][root][INFO] - Training Epoch: 1/2, step 4971/7134 completed (loss: 0.02266988344490528, acc: 0.991525411605835)
[2025-02-13 03:07:18,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:18,868][root][INFO] - Training Epoch: 1/2, step 4972/7134 completed (loss: 0.017837537452578545, acc: 0.995768666267395)
[2025-02-13 03:07:19,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:19,272][root][INFO] - Training Epoch: 1/2, step 4973/7134 completed (loss: 0.044348906725645065, acc: 0.9876543283462524)
[2025-02-13 03:07:19,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:19,636][root][INFO] - Training Epoch: 1/2, step 4974/7134 completed (loss: 0.08865807205438614, acc: 0.9759519100189209)
[2025-02-13 03:07:19,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:20,046][root][INFO] - Training Epoch: 1/2, step 4975/7134 completed (loss: 0.07790759950876236, acc: 0.9760383367538452)
[2025-02-13 03:07:20,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:20,368][root][INFO] - Training Epoch: 1/2, step 4976/7134 completed (loss: 0.05558886379003525, acc: 0.9812646508216858)
[2025-02-13 03:07:20,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:20,790][root][INFO] - Training Epoch: 1/2, step 4977/7134 completed (loss: 0.10722928494215012, acc: 0.9720430374145508)
[2025-02-13 03:07:20,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:21,202][root][INFO] - Training Epoch: 1/2, step 4978/7134 completed (loss: 0.037588976323604584, acc: 0.9875776171684265)
[2025-02-13 03:07:21,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:21,590][root][INFO] - Training Epoch: 1/2, step 4979/7134 completed (loss: 0.1340426802635193, acc: 0.9686985015869141)
[2025-02-13 03:07:21,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:22,031][root][INFO] - Training Epoch: 1/2, step 4980/7134 completed (loss: 0.06376002728939056, acc: 0.9871175289154053)
[2025-02-13 03:07:22,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:22,475][root][INFO] - Training Epoch: 1/2, step 4981/7134 completed (loss: 0.04550256207585335, acc: 0.9840510487556458)
[2025-02-13 03:07:22,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:22,875][root][INFO] - Training Epoch: 1/2, step 4982/7134 completed (loss: 0.09631438553333282, acc: 0.9757084846496582)
[2025-02-13 03:07:23,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:23,310][root][INFO] - Training Epoch: 1/2, step 4983/7134 completed (loss: 0.05529139190912247, acc: 0.985981285572052)
[2025-02-13 03:07:23,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:23,565][root][INFO] - Training Epoch: 1/2, step 4984/7134 completed (loss: 0.08491582423448563, acc: 0.9791666865348816)
[2025-02-13 03:07:23,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:23,905][root][INFO] - Training Epoch: 1/2, step 4985/7134 completed (loss: 0.05319037288427353, acc: 0.9877675771713257)
[2025-02-13 03:07:24,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:24,306][root][INFO] - Training Epoch: 1/2, step 4986/7134 completed (loss: 0.09676750004291534, acc: 0.9777117371559143)
[2025-02-13 03:07:24,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:24,684][root][INFO] - Training Epoch: 1/2, step 4987/7134 completed (loss: 0.04923507571220398, acc: 0.9840954542160034)
[2025-02-13 03:07:24,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:25,101][root][INFO] - Training Epoch: 1/2, step 4988/7134 completed (loss: 0.0361962765455246, acc: 0.9893993139266968)
[2025-02-13 03:07:25,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:25,487][root][INFO] - Training Epoch: 1/2, step 4989/7134 completed (loss: 0.07716315984725952, acc: 0.9801980257034302)
[2025-02-13 03:07:25,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:25,886][root][INFO] - Training Epoch: 1/2, step 4990/7134 completed (loss: 0.04467775300145149, acc: 0.9881756901741028)
[2025-02-13 03:07:26,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:26,297][root][INFO] - Training Epoch: 1/2, step 4991/7134 completed (loss: 0.03925308212637901, acc: 0.9865996837615967)
[2025-02-13 03:07:26,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:26,719][root][INFO] - Training Epoch: 1/2, step 4992/7134 completed (loss: 0.11621799319982529, acc: 0.9759299755096436)
[2025-02-13 03:07:26,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:26,969][root][INFO] - Training Epoch: 1/2, step 4993/7134 completed (loss: 0.03382495045661926, acc: 0.9864253401756287)
[2025-02-13 03:07:27,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:27,418][root][INFO] - Training Epoch: 1/2, step 4994/7134 completed (loss: 0.03352140635251999, acc: 0.9900709390640259)
[2025-02-13 03:07:27,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:27,675][root][INFO] - Training Epoch: 1/2, step 4995/7134 completed (loss: 0.05375226214528084, acc: 0.9836734533309937)
[2025-02-13 03:07:27,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:28,070][root][INFO] - Training Epoch: 1/2, step 4996/7134 completed (loss: 0.04209039732813835, acc: 0.9870848655700684)
[2025-02-13 03:07:28,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:28,524][root][INFO] - Training Epoch: 1/2, step 4997/7134 completed (loss: 0.07336322218179703, acc: 0.9777397513389587)
[2025-02-13 03:07:28,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:28,831][root][INFO] - Training Epoch: 1/2, step 4998/7134 completed (loss: 0.12661908566951752, acc: 0.9699453711509705)
[2025-02-13 03:07:28,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:29,232][root][INFO] - Training Epoch: 1/2, step 4999/7134 completed (loss: 0.028845014050602913, acc: 0.9931153059005737)
[2025-02-13 03:07:29,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:29,590][root][INFO] - Training Epoch: 1/2, step 5000/7134 completed (loss: 0.022367455065250397, acc: 0.9930232763290405)
[2025-02-13 03:07:29,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:30,030][root][INFO] - Training Epoch: 1/2, step 5001/7134 completed (loss: 0.07264561206102371, acc: 0.9883720874786377)
[2025-02-13 03:07:30,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:30,437][root][INFO] - Training Epoch: 1/2, step 5002/7134 completed (loss: 0.05084864795207977, acc: 0.9888476133346558)
[2025-02-13 03:07:30,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:30,830][root][INFO] - Training Epoch: 1/2, step 5003/7134 completed (loss: 0.03777849301695824, acc: 0.9904000163078308)
[2025-02-13 03:07:30,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:31,142][root][INFO] - Training Epoch: 1/2, step 5004/7134 completed (loss: 0.013131135143339634, acc: 0.9955849647521973)
[2025-02-13 03:07:31,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:31,498][root][INFO] - Training Epoch: 1/2, step 5005/7134 completed (loss: 0.06072142347693443, acc: 0.9804878234863281)
[2025-02-13 03:07:31,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:31,930][root][INFO] - Training Epoch: 1/2, step 5006/7134 completed (loss: 0.04369393363595009, acc: 0.98531574010849)
[2025-02-13 03:07:32,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:32,315][root][INFO] - Training Epoch: 1/2, step 5007/7134 completed (loss: 0.030182067304849625, acc: 0.9971751570701599)
[2025-02-13 03:07:32,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:32,770][root][INFO] - Training Epoch: 1/2, step 5008/7134 completed (loss: 0.06419922411441803, acc: 0.9763779640197754)
[2025-02-13 03:07:32,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:33,160][root][INFO] - Training Epoch: 1/2, step 5009/7134 completed (loss: 0.07667943090200424, acc: 0.9925558567047119)
[2025-02-13 03:07:33,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:33,541][root][INFO] - Training Epoch: 1/2, step 5010/7134 completed (loss: 0.05956059694290161, acc: 0.9858299493789673)
[2025-02-13 03:07:33,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:33,999][root][INFO] - Training Epoch: 1/2, step 5011/7134 completed (loss: 0.07492092996835709, acc: 0.9821138381958008)
[2025-02-13 03:07:34,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:34,266][root][INFO] - Training Epoch: 1/2, step 5012/7134 completed (loss: 0.10159412771463394, acc: 0.9741379022598267)
[2025-02-13 03:07:34,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:34,647][root][INFO] - Training Epoch: 1/2, step 5013/7134 completed (loss: 0.026741018518805504, acc: 0.9879759550094604)
[2025-02-13 03:07:34,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:35,110][root][INFO] - Training Epoch: 1/2, step 5014/7134 completed (loss: 0.049216363579034805, acc: 0.9897540807723999)
[2025-02-13 03:07:35,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:35,540][root][INFO] - Training Epoch: 1/2, step 5015/7134 completed (loss: 0.09437780827283859, acc: 0.9793103337287903)
[2025-02-13 03:07:35,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:35,921][root][INFO] - Training Epoch: 1/2, step 5016/7134 completed (loss: 0.02302035316824913, acc: 0.994575023651123)
[2025-02-13 03:07:36,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:36,315][root][INFO] - Training Epoch: 1/2, step 5017/7134 completed (loss: 0.028426621109247208, acc: 0.9963964223861694)
[2025-02-13 03:07:36,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:36,715][root][INFO] - Training Epoch: 1/2, step 5018/7134 completed (loss: 0.03912635147571564, acc: 0.98758864402771)
[2025-02-13 03:07:36,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:37,134][root][INFO] - Training Epoch: 1/2, step 5019/7134 completed (loss: 0.031676217913627625, acc: 0.9900285005569458)
[2025-02-13 03:07:37,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:37,594][root][INFO] - Training Epoch: 1/2, step 5020/7134 completed (loss: 0.04981829226016998, acc: 0.9862499833106995)
[2025-02-13 03:07:37,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:37,998][root][INFO] - Training Epoch: 1/2, step 5021/7134 completed (loss: 0.06003324314951897, acc: 0.9816933870315552)
[2025-02-13 03:07:38,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:38,414][root][INFO] - Training Epoch: 1/2, step 5022/7134 completed (loss: 0.04869934916496277, acc: 0.9891975522041321)
[2025-02-13 03:07:38,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:38,864][root][INFO] - Training Epoch: 1/2, step 5023/7134 completed (loss: 0.05332252383232117, acc: 0.9863221645355225)
[2025-02-13 03:07:39,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:39,304][root][INFO] - Training Epoch: 1/2, step 5024/7134 completed (loss: 0.03149908035993576, acc: 0.993565022945404)
[2025-02-13 03:07:39,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:39,714][root][INFO] - Training Epoch: 1/2, step 5025/7134 completed (loss: 0.030197879299521446, acc: 0.9927536249160767)
[2025-02-13 03:07:39,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:40,100][root][INFO] - Training Epoch: 1/2, step 5026/7134 completed (loss: 0.030583219602704048, acc: 0.9912434220314026)
[2025-02-13 03:07:40,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:40,490][root][INFO] - Training Epoch: 1/2, step 5027/7134 completed (loss: 0.032337628304958344, acc: 0.9908925294876099)
[2025-02-13 03:07:40,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:40,880][root][INFO] - Training Epoch: 1/2, step 5028/7134 completed (loss: 0.03195848688483238, acc: 0.9890282154083252)
[2025-02-13 03:07:41,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:41,265][root][INFO] - Training Epoch: 1/2, step 5029/7134 completed (loss: 0.025221293792128563, acc: 0.9886363744735718)
[2025-02-13 03:07:41,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:41,697][root][INFO] - Training Epoch: 1/2, step 5030/7134 completed (loss: 0.026580406352877617, acc: 0.9945429563522339)
[2025-02-13 03:07:41,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:42,106][root][INFO] - Training Epoch: 1/2, step 5031/7134 completed (loss: 0.07335314154624939, acc: 0.9914966225624084)
[2025-02-13 03:07:42,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:42,507][root][INFO] - Training Epoch: 1/2, step 5032/7134 completed (loss: 0.02870262786746025, acc: 0.9951298832893372)
[2025-02-13 03:07:42,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:42,906][root][INFO] - Training Epoch: 1/2, step 5033/7134 completed (loss: 0.019682474434375763, acc: 0.9935897588729858)
[2025-02-13 03:07:43,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:43,294][root][INFO] - Training Epoch: 1/2, step 5034/7134 completed (loss: 0.022615553811192513, acc: 0.991134762763977)
[2025-02-13 03:07:43,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:43,693][root][INFO] - Training Epoch: 1/2, step 5035/7134 completed (loss: 0.005169674288481474, acc: 1.0)
[2025-02-13 03:07:43,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:44,110][root][INFO] - Training Epoch: 1/2, step 5036/7134 completed (loss: 0.07284574955701828, acc: 0.984375)
[2025-02-13 03:07:44,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:44,534][root][INFO] - Training Epoch: 1/2, step 5037/7134 completed (loss: 0.0693264752626419, acc: 0.989276111125946)
[2025-02-13 03:07:44,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:44,904][root][INFO] - Training Epoch: 1/2, step 5038/7134 completed (loss: 0.05205905809998512, acc: 0.9866369962692261)
[2025-02-13 03:07:45,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:45,281][root][INFO] - Training Epoch: 1/2, step 5039/7134 completed (loss: 0.05014779046177864, acc: 0.9857594966888428)
[2025-02-13 03:07:45,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:45,632][root][INFO] - Training Epoch: 1/2, step 5040/7134 completed (loss: 0.020709063857793808, acc: 0.9923518300056458)
[2025-02-13 03:07:45,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:46,059][root][INFO] - Training Epoch: 1/2, step 5041/7134 completed (loss: 0.01002438087016344, acc: 0.9973474740982056)
[2025-02-13 03:07:46,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:46,415][root][INFO] - Training Epoch: 1/2, step 5042/7134 completed (loss: 0.06213720887899399, acc: 0.9860140085220337)
[2025-02-13 03:07:46,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:46,760][root][INFO] - Training Epoch: 1/2, step 5043/7134 completed (loss: 0.04123824089765549, acc: 0.9881423115730286)
[2025-02-13 03:07:46,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:47,183][root][INFO] - Training Epoch: 1/2, step 5044/7134 completed (loss: 0.021699104458093643, acc: 0.9956140518188477)
[2025-02-13 03:07:47,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:47,621][root][INFO] - Training Epoch: 1/2, step 5045/7134 completed (loss: 0.06041383370757103, acc: 0.983849287033081)
[2025-02-13 03:07:47,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:47,961][root][INFO] - Training Epoch: 1/2, step 5046/7134 completed (loss: 0.09474658966064453, acc: 0.9754385948181152)
[2025-02-13 03:07:48,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:48,301][root][INFO] - Training Epoch: 1/2, step 5047/7134 completed (loss: 0.07646464556455612, acc: 0.9801653027534485)
[2025-02-13 03:07:48,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:48,743][root][INFO] - Training Epoch: 1/2, step 5048/7134 completed (loss: 0.05572211742401123, acc: 0.9832572340965271)
[2025-02-13 03:07:48,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:49,134][root][INFO] - Training Epoch: 1/2, step 5049/7134 completed (loss: 0.06932740658521652, acc: 0.9768392443656921)
[2025-02-13 03:07:49,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:49,551][root][INFO] - Training Epoch: 1/2, step 5050/7134 completed (loss: 0.06380511820316315, acc: 0.9831223487854004)
[2025-02-13 03:07:49,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:49,967][root][INFO] - Training Epoch: 1/2, step 5051/7134 completed (loss: 0.0792786255478859, acc: 0.97826087474823)
[2025-02-13 03:07:50,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:50,367][root][INFO] - Training Epoch: 1/2, step 5052/7134 completed (loss: 0.04172701761126518, acc: 0.9863636493682861)
[2025-02-13 03:07:50,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:50,775][root][INFO] - Training Epoch: 1/2, step 5053/7134 completed (loss: 0.11387301981449127, acc: 0.9716981053352356)
[2025-02-13 03:07:50,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:51,223][root][INFO] - Training Epoch: 1/2, step 5054/7134 completed (loss: 0.0893019512295723, acc: 0.9763407111167908)
[2025-02-13 03:07:51,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:51,548][root][INFO] - Training Epoch: 1/2, step 5055/7134 completed (loss: 0.03165677189826965, acc: 0.9857142567634583)
[2025-02-13 03:07:51,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:51,960][root][INFO] - Training Epoch: 1/2, step 5056/7134 completed (loss: 0.1110544204711914, acc: 0.9777777791023254)
[2025-02-13 03:07:52,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:52,378][root][INFO] - Training Epoch: 1/2, step 5057/7134 completed (loss: 0.09730959683656693, acc: 0.9829303026199341)
[2025-02-13 03:07:52,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:52,788][root][INFO] - Training Epoch: 1/2, step 5058/7134 completed (loss: 0.0841788500547409, acc: 0.9826086759567261)
[2025-02-13 03:07:52,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:53,229][root][INFO] - Training Epoch: 1/2, step 5059/7134 completed (loss: 0.04504449665546417, acc: 0.9842725992202759)
[2025-02-13 03:07:53,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:53,654][root][INFO] - Training Epoch: 1/2, step 5060/7134 completed (loss: 0.036077044904232025, acc: 0.9910846948623657)
[2025-02-13 03:07:53,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:54,032][root][INFO] - Training Epoch: 1/2, step 5061/7134 completed (loss: 0.0667409673333168, acc: 0.9784017205238342)
[2025-02-13 03:07:54,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:54,445][root][INFO] - Training Epoch: 1/2, step 5062/7134 completed (loss: 0.09855833649635315, acc: 0.9742709994316101)
[2025-02-13 03:07:54,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:54,868][root][INFO] - Training Epoch: 1/2, step 5063/7134 completed (loss: 0.0680665597319603, acc: 0.9849749803543091)
[2025-02-13 03:07:55,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:55,304][root][INFO] - Training Epoch: 1/2, step 5064/7134 completed (loss: 0.059214506298303604, acc: 0.9897260069847107)
[2025-02-13 03:07:55,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:55,750][root][INFO] - Training Epoch: 1/2, step 5065/7134 completed (loss: 0.0592992790043354, acc: 0.9873060584068298)
[2025-02-13 03:07:55,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:56,164][root][INFO] - Training Epoch: 1/2, step 5066/7134 completed (loss: 0.07571963220834732, acc: 0.9803328514099121)
[2025-02-13 03:07:56,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:56,537][root][INFO] - Training Epoch: 1/2, step 5067/7134 completed (loss: 0.034901052713394165, acc: 0.9922027587890625)
[2025-02-13 03:07:56,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:56,946][root][INFO] - Training Epoch: 1/2, step 5068/7134 completed (loss: 0.060076143592596054, acc: 0.9873617887496948)
[2025-02-13 03:07:57,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:57,362][root][INFO] - Training Epoch: 1/2, step 5069/7134 completed (loss: 0.03402387350797653, acc: 0.9921875)
[2025-02-13 03:07:57,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:57,704][root][INFO] - Training Epoch: 1/2, step 5070/7134 completed (loss: 0.0345599502325058, acc: 0.9885057210922241)
[2025-02-13 03:07:57,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:58,111][root][INFO] - Training Epoch: 1/2, step 5071/7134 completed (loss: 0.08832073956727982, acc: 0.9761570692062378)
[2025-02-13 03:07:58,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:58,536][root][INFO] - Training Epoch: 1/2, step 5072/7134 completed (loss: 0.0653316080570221, acc: 0.981179416179657)
[2025-02-13 03:07:58,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:58,944][root][INFO] - Training Epoch: 1/2, step 5073/7134 completed (loss: 0.0698469877243042, acc: 0.9729207158088684)
[2025-02-13 03:07:59,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:59,386][root][INFO] - Training Epoch: 1/2, step 5074/7134 completed (loss: 0.07491637766361237, acc: 0.9850746393203735)
[2025-02-13 03:07:59,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:59,788][root][INFO] - Training Epoch: 1/2, step 5075/7134 completed (loss: 0.05577247589826584, acc: 0.9883720874786377)
[2025-02-13 03:07:59,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:00,245][root][INFO] - Training Epoch: 1/2, step 5076/7134 completed (loss: 0.07598130404949188, acc: 0.9844497442245483)
[2025-02-13 03:08:00,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:00,687][root][INFO] - Training Epoch: 1/2, step 5077/7134 completed (loss: 0.06263256818056107, acc: 0.9794303774833679)
[2025-02-13 03:08:00,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:01,101][root][INFO] - Training Epoch: 1/2, step 5078/7134 completed (loss: 0.09377128630876541, acc: 0.9748954176902771)
[2025-02-13 03:08:01,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:01,514][root][INFO] - Training Epoch: 1/2, step 5079/7134 completed (loss: 0.030952494591474533, acc: 0.9904761910438538)
[2025-02-13 03:08:01,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:01,970][root][INFO] - Training Epoch: 1/2, step 5080/7134 completed (loss: 0.028021493926644325, acc: 0.991725742816925)
[2025-02-13 03:08:02,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:02,399][root][INFO] - Training Epoch: 1/2, step 5081/7134 completed (loss: 0.024199698120355606, acc: 0.9945651888847351)
[2025-02-13 03:08:02,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:02,847][root][INFO] - Training Epoch: 1/2, step 5082/7134 completed (loss: 0.032668337225914, acc: 0.9931972622871399)
[2025-02-13 03:08:02,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:03,261][root][INFO] - Training Epoch: 1/2, step 5083/7134 completed (loss: 0.04643533378839493, acc: 0.9887482523918152)
[2025-02-13 03:08:03,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:03,722][root][INFO] - Training Epoch: 1/2, step 5084/7134 completed (loss: 0.01945948787033558, acc: 0.9965277910232544)
[2025-02-13 03:08:03,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:04,182][root][INFO] - Training Epoch: 1/2, step 5085/7134 completed (loss: 0.04403200000524521, acc: 0.9869822263717651)
[2025-02-13 03:08:04,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:04,596][root][INFO] - Training Epoch: 1/2, step 5086/7134 completed (loss: 0.05037233233451843, acc: 0.987293541431427)
[2025-02-13 03:08:04,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:05,030][root][INFO] - Training Epoch: 1/2, step 5087/7134 completed (loss: 0.017330344766378403, acc: 0.9919678568840027)
[2025-02-13 03:08:05,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:05,442][root][INFO] - Training Epoch: 1/2, step 5088/7134 completed (loss: 0.010525534860789776, acc: 0.9956076145172119)
[2025-02-13 03:08:05,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:05,881][root][INFO] - Training Epoch: 1/2, step 5089/7134 completed (loss: 0.03025970235466957, acc: 0.9912717938423157)
[2025-02-13 03:08:06,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:06,319][root][INFO] - Training Epoch: 1/2, step 5090/7134 completed (loss: 0.017450496554374695, acc: 0.9953325390815735)
[2025-02-13 03:08:06,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:06,735][root][INFO] - Training Epoch: 1/2, step 5091/7134 completed (loss: 0.022210288792848587, acc: 0.9934123754501343)
[2025-02-13 03:08:06,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:07,151][root][INFO] - Training Epoch: 1/2, step 5092/7134 completed (loss: 0.05432221293449402, acc: 0.9861687421798706)
[2025-02-13 03:08:07,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:07,560][root][INFO] - Training Epoch: 1/2, step 5093/7134 completed (loss: 0.03581441938877106, acc: 0.9903314709663391)
[2025-02-13 03:08:07,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:07,978][root][INFO] - Training Epoch: 1/2, step 5094/7134 completed (loss: 0.04525431990623474, acc: 0.9909090995788574)
[2025-02-13 03:08:08,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:08,374][root][INFO] - Training Epoch: 1/2, step 5095/7134 completed (loss: 0.13103577494621277, acc: 0.9703587889671326)
[2025-02-13 03:08:08,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:08,815][root][INFO] - Training Epoch: 1/2, step 5096/7134 completed (loss: 0.04472382739186287, acc: 0.9838150143623352)
[2025-02-13 03:08:08,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:09,253][root][INFO] - Training Epoch: 1/2, step 5097/7134 completed (loss: 0.05168162286281586, acc: 0.9865689873695374)
[2025-02-13 03:08:09,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:09,695][root][INFO] - Training Epoch: 1/2, step 5098/7134 completed (loss: 0.04618830978870392, acc: 0.9853768348693848)
[2025-02-13 03:08:09,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:10,150][root][INFO] - Training Epoch: 1/2, step 5099/7134 completed (loss: 0.050797682255506516, acc: 0.982679009437561)
[2025-02-13 03:08:10,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:10,601][root][INFO] - Training Epoch: 1/2, step 5100/7134 completed (loss: 0.04820980131626129, acc: 0.9878854751586914)
[2025-02-13 03:08:10,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:11,034][root][INFO] - Training Epoch: 1/2, step 5101/7134 completed (loss: 0.020616406574845314, acc: 0.9953380227088928)
[2025-02-13 03:08:11,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:11,440][root][INFO] - Training Epoch: 1/2, step 5102/7134 completed (loss: 0.060689836740493774, acc: 0.9836289286613464)
[2025-02-13 03:08:11,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:11,907][root][INFO] - Training Epoch: 1/2, step 5103/7134 completed (loss: 0.040570586919784546, acc: 0.9896907210350037)
[2025-02-13 03:08:12,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:12,370][root][INFO] - Training Epoch: 1/2, step 5104/7134 completed (loss: 0.02043238840997219, acc: 0.9955703020095825)
[2025-02-13 03:08:12,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:12,760][root][INFO] - Training Epoch: 1/2, step 5105/7134 completed (loss: 0.027660537511110306, acc: 0.9895678162574768)
[2025-02-13 03:08:12,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:13,187][root][INFO] - Training Epoch: 1/2, step 5106/7134 completed (loss: 0.03245852142572403, acc: 0.9940740466117859)
[2025-02-13 03:08:13,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:13,606][root][INFO] - Training Epoch: 1/2, step 5107/7134 completed (loss: 0.10025224089622498, acc: 0.9825673699378967)
[2025-02-13 03:08:13,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:14,008][root][INFO] - Training Epoch: 1/2, step 5108/7134 completed (loss: 0.09348156303167343, acc: 0.9721254110336304)
[2025-02-13 03:08:14,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:14,473][root][INFO] - Training Epoch: 1/2, step 5109/7134 completed (loss: 0.08323810249567032, acc: 0.9767699241638184)
[2025-02-13 03:08:14,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:14,904][root][INFO] - Training Epoch: 1/2, step 5110/7134 completed (loss: 0.08493854105472565, acc: 0.9743589758872986)
[2025-02-13 03:08:15,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:15,315][root][INFO] - Training Epoch: 1/2, step 5111/7134 completed (loss: 0.04690629243850708, acc: 0.9863013625144958)
[2025-02-13 03:08:15,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:15,753][root][INFO] - Training Epoch: 1/2, step 5112/7134 completed (loss: 0.0598393939435482, acc: 0.9850560426712036)
[2025-02-13 03:08:15,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:16,146][root][INFO] - Training Epoch: 1/2, step 5113/7134 completed (loss: 0.0602780245244503, acc: 0.9850746393203735)
[2025-02-13 03:08:16,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:16,604][root][INFO] - Training Epoch: 1/2, step 5114/7134 completed (loss: 0.031776461750268936, acc: 0.9944367408752441)
[2025-02-13 03:08:16,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:17,031][root][INFO] - Training Epoch: 1/2, step 5115/7134 completed (loss: 0.0335753858089447, acc: 0.9922580718994141)
[2025-02-13 03:08:17,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:17,483][root][INFO] - Training Epoch: 1/2, step 5116/7134 completed (loss: 0.03570893406867981, acc: 0.9909090995788574)
[2025-02-13 03:08:17,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:17,902][root][INFO] - Training Epoch: 1/2, step 5117/7134 completed (loss: 0.0404859334230423, acc: 0.9888424277305603)
[2025-02-13 03:08:18,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:18,282][root][INFO] - Training Epoch: 1/2, step 5118/7134 completed (loss: 0.07681868225336075, acc: 0.9848254919052124)
[2025-02-13 03:08:18,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:18,707][root][INFO] - Training Epoch: 1/2, step 5119/7134 completed (loss: 0.04564300924539566, acc: 0.993122398853302)
[2025-02-13 03:08:18,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:19,116][root][INFO] - Training Epoch: 1/2, step 5120/7134 completed (loss: 0.08616537600755692, acc: 0.981675386428833)
[2025-02-13 03:08:19,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:19,531][root][INFO] - Training Epoch: 1/2, step 5121/7134 completed (loss: 0.028732018545269966, acc: 0.9916782379150391)
[2025-02-13 03:08:19,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:19,945][root][INFO] - Training Epoch: 1/2, step 5122/7134 completed (loss: 0.05223682150244713, acc: 0.9831223487854004)
[2025-02-13 03:08:20,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:20,389][root][INFO] - Training Epoch: 1/2, step 5123/7134 completed (loss: 0.03189238905906677, acc: 0.9915561079978943)
[2025-02-13 03:08:20,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:20,823][root][INFO] - Training Epoch: 1/2, step 5124/7134 completed (loss: 0.052092019468545914, acc: 0.9801242351531982)
[2025-02-13 03:08:20,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:21,244][root][INFO] - Training Epoch: 1/2, step 5125/7134 completed (loss: 0.03387776389718056, acc: 0.9921011328697205)
[2025-02-13 03:08:21,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:21,677][root][INFO] - Training Epoch: 1/2, step 5126/7134 completed (loss: 0.024417990818619728, acc: 0.9948717951774597)
[2025-02-13 03:08:21,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:22,087][root][INFO] - Training Epoch: 1/2, step 5127/7134 completed (loss: 0.04652936011552811, acc: 0.988811194896698)
[2025-02-13 03:08:22,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:22,523][root][INFO] - Training Epoch: 1/2, step 5128/7134 completed (loss: 0.02298855595290661, acc: 0.9945054650306702)
[2025-02-13 03:08:22,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:22,948][root][INFO] - Training Epoch: 1/2, step 5129/7134 completed (loss: 0.08017094433307648, acc: 0.9725490212440491)
[2025-02-13 03:08:23,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:23,362][root][INFO] - Training Epoch: 1/2, step 5130/7134 completed (loss: 0.02702053263783455, acc: 0.9906396269798279)
[2025-02-13 03:08:23,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:23,812][root][INFO] - Training Epoch: 1/2, step 5131/7134 completed (loss: 0.06837774068117142, acc: 0.9870129823684692)
[2025-02-13 03:08:23,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:24,195][root][INFO] - Training Epoch: 1/2, step 5132/7134 completed (loss: 0.024096792563796043, acc: 0.9881188273429871)
[2025-02-13 03:08:24,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:24,615][root][INFO] - Training Epoch: 1/2, step 5133/7134 completed (loss: 0.06168774142861366, acc: 0.9798136353492737)
[2025-02-13 03:08:24,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:25,048][root][INFO] - Training Epoch: 1/2, step 5134/7134 completed (loss: 0.02498818375170231, acc: 0.9924952983856201)
[2025-02-13 03:08:25,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:25,420][root][INFO] - Training Epoch: 1/2, step 5135/7134 completed (loss: 0.06442933529615402, acc: 0.9871428608894348)
[2025-02-13 03:08:25,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:25,856][root][INFO] - Training Epoch: 1/2, step 5136/7134 completed (loss: 0.03683322295546532, acc: 0.9877049326896667)
[2025-02-13 03:08:25,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:26,289][root][INFO] - Training Epoch: 1/2, step 5137/7134 completed (loss: 0.03319631144404411, acc: 0.9963302612304688)
[2025-02-13 03:08:26,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:26,737][root][INFO] - Training Epoch: 1/2, step 5138/7134 completed (loss: 0.02314729243516922, acc: 0.9937499761581421)
[2025-02-13 03:08:26,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:27,122][root][INFO] - Training Epoch: 1/2, step 5139/7134 completed (loss: 0.041878677904605865, acc: 0.9842632412910461)
[2025-02-13 03:08:27,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:27,567][root][INFO] - Training Epoch: 1/2, step 5140/7134 completed (loss: 0.024113576859235764, acc: 0.9957582354545593)
[2025-02-13 03:08:27,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:27,820][root][INFO] - Training Epoch: 1/2, step 5141/7134 completed (loss: 0.059923842549324036, acc: 0.992277979850769)
[2025-02-13 03:08:27,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:28,256][root][INFO] - Training Epoch: 1/2, step 5142/7134 completed (loss: 0.046271663159132004, acc: 0.9870588183403015)
[2025-02-13 03:08:28,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:28,699][root][INFO] - Training Epoch: 1/2, step 5143/7134 completed (loss: 0.0476611852645874, acc: 0.9835025668144226)
[2025-02-13 03:08:28,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:29,127][root][INFO] - Training Epoch: 1/2, step 5144/7134 completed (loss: 0.02283528447151184, acc: 0.9941657185554504)
[2025-02-13 03:08:29,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:29,586][root][INFO] - Training Epoch: 1/2, step 5145/7134 completed (loss: 0.027790473774075508, acc: 0.9918116927146912)
[2025-02-13 03:08:29,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:30,040][root][INFO] - Training Epoch: 1/2, step 5146/7134 completed (loss: 0.0634305402636528, acc: 0.9832776188850403)
[2025-02-13 03:08:30,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:30,436][root][INFO] - Training Epoch: 1/2, step 5147/7134 completed (loss: 0.026715056970715523, acc: 0.994535505771637)
[2025-02-13 03:08:30,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:30,846][root][INFO] - Training Epoch: 1/2, step 5148/7134 completed (loss: 0.09934290498495102, acc: 0.9800266027450562)
[2025-02-13 03:08:30,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:31,313][root][INFO] - Training Epoch: 1/2, step 5149/7134 completed (loss: 0.06377877295017242, acc: 0.9810945391654968)
[2025-02-13 03:08:31,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:31,736][root][INFO] - Training Epoch: 1/2, step 5150/7134 completed (loss: 0.09859472513198853, acc: 0.9726206064224243)
[2025-02-13 03:08:31,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:32,156][root][INFO] - Training Epoch: 1/2, step 5151/7134 completed (loss: 0.041646670550107956, acc: 0.9888613820075989)
[2025-02-13 03:08:32,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:32,592][root][INFO] - Training Epoch: 1/2, step 5152/7134 completed (loss: 0.03566821292042732, acc: 0.9906166195869446)
[2025-02-13 03:08:32,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:33,032][root][INFO] - Training Epoch: 1/2, step 5153/7134 completed (loss: 0.029493074864149094, acc: 0.991769552230835)
[2025-02-13 03:08:33,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:33,496][root][INFO] - Training Epoch: 1/2, step 5154/7134 completed (loss: 0.04695545509457588, acc: 0.9886234402656555)
[2025-02-13 03:08:33,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:33,919][root][INFO] - Training Epoch: 1/2, step 5155/7134 completed (loss: 0.043630629777908325, acc: 0.9924699068069458)
[2025-02-13 03:08:34,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:34,401][root][INFO] - Training Epoch: 1/2, step 5156/7134 completed (loss: 0.025474417954683304, acc: 0.991062581539154)
[2025-02-13 03:08:34,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:34,840][root][INFO] - Training Epoch: 1/2, step 5157/7134 completed (loss: 0.09356046468019485, acc: 0.9706601500511169)
[2025-02-13 03:08:34,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:35,278][root][INFO] - Training Epoch: 1/2, step 5158/7134 completed (loss: 0.08651795983314514, acc: 0.9777448177337646)
[2025-02-13 03:08:35,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:35,705][root][INFO] - Training Epoch: 1/2, step 5159/7134 completed (loss: 0.01903166063129902, acc: 0.9934980273246765)
[2025-02-13 03:08:35,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:36,123][root][INFO] - Training Epoch: 1/2, step 5160/7134 completed (loss: 0.06599634140729904, acc: 0.9783861637115479)
[2025-02-13 03:08:36,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:36,550][root][INFO] - Training Epoch: 1/2, step 5161/7134 completed (loss: 0.0910334587097168, acc: 0.9706293940544128)
[2025-02-13 03:08:36,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:36,957][root][INFO] - Training Epoch: 1/2, step 5162/7134 completed (loss: 0.04342217370867729, acc: 0.9886547923088074)
[2025-02-13 03:08:37,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:37,361][root][INFO] - Training Epoch: 1/2, step 5163/7134 completed (loss: 0.02797645702958107, acc: 0.9929178357124329)
[2025-02-13 03:08:37,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:37,772][root][INFO] - Training Epoch: 1/2, step 5164/7134 completed (loss: 0.029023874551057816, acc: 0.9932773113250732)
[2025-02-13 03:08:37,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:38,229][root][INFO] - Training Epoch: 1/2, step 5165/7134 completed (loss: 0.050345879048109055, acc: 0.9894578456878662)
[2025-02-13 03:08:38,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:38,644][root][INFO] - Training Epoch: 1/2, step 5166/7134 completed (loss: 0.028475528582930565, acc: 0.990867555141449)
[2025-02-13 03:08:38,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:39,050][root][INFO] - Training Epoch: 1/2, step 5167/7134 completed (loss: 0.07950670272111893, acc: 0.9679487347602844)
[2025-02-13 03:08:39,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:39,454][root][INFO] - Training Epoch: 1/2, step 5168/7134 completed (loss: 0.05320032313466072, acc: 0.9829192757606506)
[2025-02-13 03:08:39,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:39,880][root][INFO] - Training Epoch: 1/2, step 5169/7134 completed (loss: 0.08345454186201096, acc: 0.9750778675079346)
[2025-02-13 03:08:40,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:40,311][root][INFO] - Training Epoch: 1/2, step 5170/7134 completed (loss: 0.051223840564489365, acc: 0.9841059446334839)
[2025-02-13 03:08:40,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:40,652][root][INFO] - Training Epoch: 1/2, step 5171/7134 completed (loss: 0.049465786665678024, acc: 0.9829059839248657)
[2025-02-13 03:08:40,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:41,070][root][INFO] - Training Epoch: 1/2, step 5172/7134 completed (loss: 0.09852622449398041, acc: 0.9761499166488647)
[2025-02-13 03:08:41,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:41,461][root][INFO] - Training Epoch: 1/2, step 5173/7134 completed (loss: 0.048529595136642456, acc: 0.9858267903327942)
[2025-02-13 03:08:41,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:41,896][root][INFO] - Training Epoch: 1/2, step 5174/7134 completed (loss: 0.01925254985690117, acc: 0.9961685538291931)
[2025-02-13 03:08:42,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:42,337][root][INFO] - Training Epoch: 1/2, step 5175/7134 completed (loss: 0.022742966189980507, acc: 0.9944751262664795)
[2025-02-13 03:08:42,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:42,778][root][INFO] - Training Epoch: 1/2, step 5176/7134 completed (loss: 0.019439492374658585, acc: 0.9908952713012695)
[2025-02-13 03:08:42,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:43,214][root][INFO] - Training Epoch: 1/2, step 5177/7134 completed (loss: 0.09278633445501328, acc: 0.9669564962387085)
[2025-02-13 03:08:43,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:43,623][root][INFO] - Training Epoch: 1/2, step 5178/7134 completed (loss: 0.14008717238903046, acc: 0.9686411023139954)
[2025-02-13 03:08:43,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:44,034][root][INFO] - Training Epoch: 1/2, step 5179/7134 completed (loss: 0.03062834031879902, acc: 0.9881656765937805)
[2025-02-13 03:08:44,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:44,408][root][INFO] - Training Epoch: 1/2, step 5180/7134 completed (loss: 0.07017725706100464, acc: 0.9756097793579102)
[2025-02-13 03:08:44,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:44,735][root][INFO] - Training Epoch: 1/2, step 5181/7134 completed (loss: 0.09458782523870468, acc: 0.9760638475418091)
[2025-02-13 03:08:44,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:45,115][root][INFO] - Training Epoch: 1/2, step 5182/7134 completed (loss: 0.16895900666713715, acc: 0.9520833492279053)
[2025-02-13 03:08:45,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:45,527][root][INFO] - Training Epoch: 1/2, step 5183/7134 completed (loss: 0.06524574756622314, acc: 0.9834983348846436)
[2025-02-13 03:08:45,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:45,916][root][INFO] - Training Epoch: 1/2, step 5184/7134 completed (loss: 0.09372851252555847, acc: 0.9779950976371765)
[2025-02-13 03:08:46,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:46,321][root][INFO] - Training Epoch: 1/2, step 5185/7134 completed (loss: 0.051878657191991806, acc: 0.9903614521026611)
[2025-02-13 03:08:46,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:46,728][root][INFO] - Training Epoch: 1/2, step 5186/7134 completed (loss: 0.08869780600070953, acc: 0.981670081615448)
[2025-02-13 03:08:46,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:47,127][root][INFO] - Training Epoch: 1/2, step 5187/7134 completed (loss: 0.021518291905522346, acc: 0.9942857027053833)
[2025-02-13 03:08:47,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:47,559][root][INFO] - Training Epoch: 1/2, step 5188/7134 completed (loss: 0.2060764878988266, acc: 0.9490908980369568)
[2025-02-13 03:08:47,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:47,845][root][INFO] - Training Epoch: 1/2, step 5189/7134 completed (loss: 0.1637423038482666, acc: 0.9591836929321289)
[2025-02-13 03:08:47,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:48,186][root][INFO] - Training Epoch: 1/2, step 5190/7134 completed (loss: 0.15265385806560516, acc: 0.9643705487251282)
[2025-02-13 03:08:48,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:48,639][root][INFO] - Training Epoch: 1/2, step 5191/7134 completed (loss: 0.1343957483768463, acc: 0.9604685306549072)
[2025-02-13 03:08:48,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:49,074][root][INFO] - Training Epoch: 1/2, step 5192/7134 completed (loss: 0.08563463389873505, acc: 0.9881094098091125)
[2025-02-13 03:08:49,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:49,499][root][INFO] - Training Epoch: 1/2, step 5193/7134 completed (loss: 0.06126916781067848, acc: 0.9803149700164795)
[2025-02-13 03:08:49,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:49,909][root][INFO] - Training Epoch: 1/2, step 5194/7134 completed (loss: 0.13283610343933105, acc: 0.9564356207847595)
[2025-02-13 03:08:50,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:50,235][root][INFO] - Training Epoch: 1/2, step 5195/7134 completed (loss: 0.07355091720819473, acc: 0.984649121761322)
[2025-02-13 03:08:50,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:50,566][root][INFO] - Training Epoch: 1/2, step 5196/7134 completed (loss: 0.11064345389604568, acc: 0.9688995480537415)
[2025-02-13 03:08:50,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:50,955][root][INFO] - Training Epoch: 1/2, step 5197/7134 completed (loss: 0.046502240002155304, acc: 0.9855967164039612)
[2025-02-13 03:08:51,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:51,333][root][INFO] - Training Epoch: 1/2, step 5198/7134 completed (loss: 0.06702852994203568, acc: 0.9792099595069885)
[2025-02-13 03:08:51,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:51,793][root][INFO] - Training Epoch: 1/2, step 5199/7134 completed (loss: 0.1531665325164795, acc: 0.9607558250427246)
[2025-02-13 03:08:51,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:52,204][root][INFO] - Training Epoch: 1/2, step 5200/7134 completed (loss: 0.07180201262235641, acc: 0.9764243364334106)
[2025-02-13 03:08:52,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:52,646][root][INFO] - Training Epoch: 1/2, step 5201/7134 completed (loss: 0.08473163098096848, acc: 0.9770808219909668)
[2025-02-13 03:08:52,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:53,051][root][INFO] - Training Epoch: 1/2, step 5202/7134 completed (loss: 0.09330102801322937, acc: 0.9752747416496277)
[2025-02-13 03:08:53,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:53,484][root][INFO] - Training Epoch: 1/2, step 5203/7134 completed (loss: 0.09296637028455734, acc: 0.9768707752227783)
[2025-02-13 03:08:53,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:53,872][root][INFO] - Training Epoch: 1/2, step 5204/7134 completed (loss: 0.052017029374837875, acc: 0.983505129814148)
[2025-02-13 03:08:53,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:54,124][root][INFO] - Training Epoch: 1/2, step 5205/7134 completed (loss: 0.0684298574924469, acc: 0.9737827777862549)
[2025-02-13 03:08:54,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:54,575][root][INFO] - Training Epoch: 1/2, step 5206/7134 completed (loss: 0.07891305536031723, acc: 0.9807383418083191)
[2025-02-13 03:08:54,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:54,953][root][INFO] - Training Epoch: 1/2, step 5207/7134 completed (loss: 0.1206442341208458, acc: 0.9758551120758057)
[2025-02-13 03:08:55,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:55,441][root][INFO] - Training Epoch: 1/2, step 5208/7134 completed (loss: 0.04310743883252144, acc: 0.9881266355514526)
[2025-02-13 03:08:55,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:55,843][root][INFO] - Training Epoch: 1/2, step 5209/7134 completed (loss: 0.05720585212111473, acc: 0.9839357137680054)
[2025-02-13 03:08:55,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:56,303][root][INFO] - Training Epoch: 1/2, step 5210/7134 completed (loss: 0.03990863636136055, acc: 0.9893190860748291)
[2025-02-13 03:08:56,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:56,742][root][INFO] - Training Epoch: 1/2, step 5211/7134 completed (loss: 0.06004977226257324, acc: 0.9791666865348816)
[2025-02-13 03:08:56,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:57,072][root][INFO] - Training Epoch: 1/2, step 5212/7134 completed (loss: 0.09215439856052399, acc: 0.9624573588371277)
[2025-02-13 03:08:57,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:57,481][root][INFO] - Training Epoch: 1/2, step 5213/7134 completed (loss: 0.06568817794322968, acc: 0.9820144176483154)
[2025-02-13 03:08:57,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:57,869][root][INFO] - Training Epoch: 1/2, step 5214/7134 completed (loss: 0.1392015814781189, acc: 0.959227442741394)
[2025-02-13 03:08:58,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:58,328][root][INFO] - Training Epoch: 1/2, step 5215/7134 completed (loss: 0.10247857123613358, acc: 0.9744572043418884)
[2025-02-13 03:08:58,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:58,574][root][INFO] - Training Epoch: 1/2, step 5216/7134 completed (loss: 0.10890233516693115, acc: 0.9695817232131958)
[2025-02-13 03:08:58,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:58,870][root][INFO] - Training Epoch: 1/2, step 5217/7134 completed (loss: 0.0819530189037323, acc: 0.9817073345184326)
[2025-02-13 03:08:58,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:59,229][root][INFO] - Training Epoch: 1/2, step 5218/7134 completed (loss: 0.032507579773664474, acc: 0.9887640476226807)
[2025-02-13 03:08:59,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:59,606][root][INFO] - Training Epoch: 1/2, step 5219/7134 completed (loss: 0.06292469799518585, acc: 0.9878934621810913)
[2025-02-13 03:08:59,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:59,950][root][INFO] - Training Epoch: 1/2, step 5220/7134 completed (loss: 0.10700830817222595, acc: 0.971377432346344)
[2025-02-13 03:09:00,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:00,369][root][INFO] - Training Epoch: 1/2, step 5221/7134 completed (loss: 0.08630296587944031, acc: 0.9750778675079346)
[2025-02-13 03:09:00,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:00,629][root][INFO] - Training Epoch: 1/2, step 5222/7134 completed (loss: 0.07784688472747803, acc: 0.9813084006309509)
[2025-02-13 03:09:00,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:00,999][root][INFO] - Training Epoch: 1/2, step 5223/7134 completed (loss: 0.041387125849723816, acc: 0.9907833933830261)
[2025-02-13 03:09:01,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:01,395][root][INFO] - Training Epoch: 1/2, step 5224/7134 completed (loss: 0.0759059488773346, acc: 0.9789103865623474)
[2025-02-13 03:09:01,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:01,825][root][INFO] - Training Epoch: 1/2, step 5225/7134 completed (loss: 0.07838793098926544, acc: 0.9822294116020203)
[2025-02-13 03:09:01,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:02,247][root][INFO] - Training Epoch: 1/2, step 5226/7134 completed (loss: 0.058302707970142365, acc: 0.9818781018257141)
[2025-02-13 03:09:02,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:02,695][root][INFO] - Training Epoch: 1/2, step 5227/7134 completed (loss: 0.050813887268304825, acc: 0.9896103739738464)
[2025-02-13 03:09:02,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:03,098][root][INFO] - Training Epoch: 1/2, step 5228/7134 completed (loss: 0.04498513042926788, acc: 0.9883720874786377)
[2025-02-13 03:09:03,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:03,517][root][INFO] - Training Epoch: 1/2, step 5229/7134 completed (loss: 0.03787296265363693, acc: 0.9851411581039429)
[2025-02-13 03:09:03,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:03,942][root][INFO] - Training Epoch: 1/2, step 5230/7134 completed (loss: 0.03907718136906624, acc: 0.9892183542251587)
[2025-02-13 03:09:04,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:04,386][root][INFO] - Training Epoch: 1/2, step 5231/7134 completed (loss: 0.04086776450276375, acc: 0.9886075854301453)
[2025-02-13 03:09:04,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:04,806][root][INFO] - Training Epoch: 1/2, step 5232/7134 completed (loss: 0.03823469951748848, acc: 0.9895366430282593)
[2025-02-13 03:09:04,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:05,269][root][INFO] - Training Epoch: 1/2, step 5233/7134 completed (loss: 0.03962622582912445, acc: 0.9909909963607788)
[2025-02-13 03:09:05,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:05,718][root][INFO] - Training Epoch: 1/2, step 5234/7134 completed (loss: 0.04566855728626251, acc: 0.990208089351654)
[2025-02-13 03:09:05,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:06,114][root][INFO] - Training Epoch: 1/2, step 5235/7134 completed (loss: 0.03304144740104675, acc: 0.9882352948188782)
[2025-02-13 03:09:06,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:06,522][root][INFO] - Training Epoch: 1/2, step 5236/7134 completed (loss: 0.02370111271739006, acc: 0.9930555820465088)
[2025-02-13 03:09:06,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:06,917][root][INFO] - Training Epoch: 1/2, step 5237/7134 completed (loss: 0.011194052174687386, acc: 0.9982078671455383)
[2025-02-13 03:09:07,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:07,338][root][INFO] - Training Epoch: 1/2, step 5238/7134 completed (loss: 0.018278542906045914, acc: 0.9929577708244324)
[2025-02-13 03:09:07,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:07,679][root][INFO] - Training Epoch: 1/2, step 5239/7134 completed (loss: 0.03084396943449974, acc: 0.992277979850769)
[2025-02-13 03:09:07,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:08,096][root][INFO] - Training Epoch: 1/2, step 5240/7134 completed (loss: 0.032426755875349045, acc: 0.9903181195259094)
[2025-02-13 03:09:08,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:08,517][root][INFO] - Training Epoch: 1/2, step 5241/7134 completed (loss: 0.06242797523736954, acc: 0.9858934283256531)
[2025-02-13 03:09:08,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:08,965][root][INFO] - Training Epoch: 1/2, step 5242/7134 completed (loss: 0.03037862293422222, acc: 0.9933884143829346)
[2025-02-13 03:09:09,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:09,366][root][INFO] - Training Epoch: 1/2, step 5243/7134 completed (loss: 0.04304252937436104, acc: 0.9913793206214905)
[2025-02-13 03:09:09,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:09,759][root][INFO] - Training Epoch: 1/2, step 5244/7134 completed (loss: 0.03371405228972435, acc: 0.9917241334915161)
[2025-02-13 03:09:09,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:10,239][root][INFO] - Training Epoch: 1/2, step 5245/7134 completed (loss: 0.030381597578525543, acc: 0.9895287752151489)
[2025-02-13 03:09:10,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:10,642][root][INFO] - Training Epoch: 1/2, step 5246/7134 completed (loss: 0.04184461385011673, acc: 0.987730085849762)
[2025-02-13 03:09:10,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:11,042][root][INFO] - Training Epoch: 1/2, step 5247/7134 completed (loss: 0.02428097277879715, acc: 0.9925925731658936)
[2025-02-13 03:09:11,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:11,447][root][INFO] - Training Epoch: 1/2, step 5248/7134 completed (loss: 0.07520163804292679, acc: 0.9794007539749146)
[2025-02-13 03:09:11,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:11,857][root][INFO] - Training Epoch: 1/2, step 5249/7134 completed (loss: 0.025298738852143288, acc: 0.9874213933944702)
[2025-02-13 03:09:12,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:12,300][root][INFO] - Training Epoch: 1/2, step 5250/7134 completed (loss: 0.02841389738023281, acc: 0.9924337863922119)
[2025-02-13 03:09:12,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:12,735][root][INFO] - Training Epoch: 1/2, step 5251/7134 completed (loss: 0.029788464307785034, acc: 0.9890909194946289)
[2025-02-13 03:09:12,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:13,116][root][INFO] - Training Epoch: 1/2, step 5252/7134 completed (loss: 0.059804823249578476, acc: 0.9836448431015015)
[2025-02-13 03:09:13,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:13,539][root][INFO] - Training Epoch: 1/2, step 5253/7134 completed (loss: 0.017133096233010292, acc: 0.9950980544090271)
[2025-02-13 03:09:13,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:13,972][root][INFO] - Training Epoch: 1/2, step 5254/7134 completed (loss: 0.05850980058312416, acc: 0.9813242554664612)
[2025-02-13 03:09:14,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:14,406][root][INFO] - Training Epoch: 1/2, step 5255/7134 completed (loss: 0.05285549908876419, acc: 0.9863013625144958)
[2025-02-13 03:09:14,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:14,843][root][INFO] - Training Epoch: 1/2, step 5256/7134 completed (loss: 0.0791516825556755, acc: 0.984544038772583)
[2025-02-13 03:09:14,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:15,258][root][INFO] - Training Epoch: 1/2, step 5257/7134 completed (loss: 0.06285418570041656, acc: 0.9847792983055115)
[2025-02-13 03:09:15,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:15,695][root][INFO] - Training Epoch: 1/2, step 5258/7134 completed (loss: 0.052312035113573074, acc: 0.9885203838348389)
[2025-02-13 03:09:15,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:16,105][root][INFO] - Training Epoch: 1/2, step 5259/7134 completed (loss: 0.07909497618675232, acc: 0.9841772317886353)
[2025-02-13 03:09:16,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:16,541][root][INFO] - Training Epoch: 1/2, step 5260/7134 completed (loss: 0.03008229099214077, acc: 0.9902439117431641)
[2025-02-13 03:09:16,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:16,943][root][INFO] - Training Epoch: 1/2, step 5261/7134 completed (loss: 0.0790930837392807, acc: 0.9769230484962463)
[2025-02-13 03:09:17,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:17,393][root][INFO] - Training Epoch: 1/2, step 5262/7134 completed (loss: 0.039894457906484604, acc: 0.9875862002372742)
[2025-02-13 03:09:17,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:17,792][root][INFO] - Training Epoch: 1/2, step 5263/7134 completed (loss: 0.054368868470191956, acc: 0.9877192974090576)
[2025-02-13 03:09:17,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:18,247][root][INFO] - Training Epoch: 1/2, step 5264/7134 completed (loss: 0.03294786065816879, acc: 0.9878493547439575)
[2025-02-13 03:09:18,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:18,684][root][INFO] - Training Epoch: 1/2, step 5265/7134 completed (loss: 0.04916146770119667, acc: 0.9885641932487488)
[2025-02-13 03:09:18,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:19,091][root][INFO] - Training Epoch: 1/2, step 5266/7134 completed (loss: 0.04418601095676422, acc: 0.9849315285682678)
[2025-02-13 03:09:19,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:19,479][root][INFO] - Training Epoch: 1/2, step 5267/7134 completed (loss: 0.08683977276086807, acc: 0.9720062017440796)
[2025-02-13 03:09:19,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:19,882][root][INFO] - Training Epoch: 1/2, step 5268/7134 completed (loss: 0.08658981323242188, acc: 0.9692898392677307)
[2025-02-13 03:09:20,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:20,291][root][INFO] - Training Epoch: 1/2, step 5269/7134 completed (loss: 0.09682849794626236, acc: 0.971377432346344)
[2025-02-13 03:09:20,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:20,736][root][INFO] - Training Epoch: 1/2, step 5270/7134 completed (loss: 0.040767163038253784, acc: 0.9866310358047485)
[2025-02-13 03:09:20,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:21,161][root][INFO] - Training Epoch: 1/2, step 5271/7134 completed (loss: 0.09747002273797989, acc: 0.9805996417999268)
[2025-02-13 03:09:21,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:21,567][root][INFO] - Training Epoch: 1/2, step 5272/7134 completed (loss: 0.03528205305337906, acc: 0.9897360801696777)
[2025-02-13 03:09:21,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:21,989][root][INFO] - Training Epoch: 1/2, step 5273/7134 completed (loss: 0.02832620032131672, acc: 0.9949066042900085)
[2025-02-13 03:09:22,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:22,397][root][INFO] - Training Epoch: 1/2, step 5274/7134 completed (loss: 0.07658360153436661, acc: 0.9731343388557434)
[2025-02-13 03:09:22,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:22,823][root][INFO] - Training Epoch: 1/2, step 5275/7134 completed (loss: 0.052686482667922974, acc: 0.9890310764312744)
[2025-02-13 03:09:22,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:23,222][root][INFO] - Training Epoch: 1/2, step 5276/7134 completed (loss: 0.048408765345811844, acc: 0.980567991733551)
[2025-02-13 03:09:23,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:23,611][root][INFO] - Training Epoch: 1/2, step 5277/7134 completed (loss: 0.041486576199531555, acc: 0.9841498732566833)
[2025-02-13 03:09:23,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:24,023][root][INFO] - Training Epoch: 1/2, step 5278/7134 completed (loss: 0.07877462357282639, acc: 0.9801223278045654)
[2025-02-13 03:09:24,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:24,443][root][INFO] - Training Epoch: 1/2, step 5279/7134 completed (loss: 0.03167137876152992, acc: 0.9922178983688354)
[2025-02-13 03:09:24,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:24,856][root][INFO] - Training Epoch: 1/2, step 5280/7134 completed (loss: 0.08827922493219376, acc: 0.9800994992256165)
[2025-02-13 03:09:24,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:25,268][root][INFO] - Training Epoch: 1/2, step 5281/7134 completed (loss: 0.02808503620326519, acc: 0.9916805028915405)
[2025-02-13 03:09:25,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:25,665][root][INFO] - Training Epoch: 1/2, step 5282/7134 completed (loss: 0.04412456601858139, acc: 0.9871612191200256)
[2025-02-13 03:09:25,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:26,075][root][INFO] - Training Epoch: 1/2, step 5283/7134 completed (loss: 0.03928540274500847, acc: 0.9917920827865601)
[2025-02-13 03:09:26,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:26,486][root][INFO] - Training Epoch: 1/2, step 5284/7134 completed (loss: 0.03617784380912781, acc: 0.9930843710899353)
[2025-02-13 03:09:26,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:26,935][root][INFO] - Training Epoch: 1/2, step 5285/7134 completed (loss: 0.04486817121505737, acc: 0.9893491268157959)
[2025-02-13 03:09:27,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:27,375][root][INFO] - Training Epoch: 1/2, step 5286/7134 completed (loss: 0.04923953860998154, acc: 0.9867647290229797)
[2025-02-13 03:09:27,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:27,813][root][INFO] - Training Epoch: 1/2, step 5287/7134 completed (loss: 0.08897189050912857, acc: 0.9756097793579102)
[2025-02-13 03:09:27,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:28,223][root][INFO] - Training Epoch: 1/2, step 5288/7134 completed (loss: 0.03251528739929199, acc: 0.9861963391304016)
[2025-02-13 03:09:28,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:28,663][root][INFO] - Training Epoch: 1/2, step 5289/7134 completed (loss: 0.04296010360121727, acc: 0.9815863966941833)
[2025-02-13 03:09:28,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:29,106][root][INFO] - Training Epoch: 1/2, step 5290/7134 completed (loss: 0.052270274609327316, acc: 0.9797468185424805)
[2025-02-13 03:09:29,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:29,549][root][INFO] - Training Epoch: 1/2, step 5291/7134 completed (loss: 0.03280755877494812, acc: 0.9927272796630859)
[2025-02-13 03:09:29,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:29,979][root][INFO] - Training Epoch: 1/2, step 5292/7134 completed (loss: 0.025894178077578545, acc: 0.9923664331436157)
[2025-02-13 03:09:30,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:30,412][root][INFO] - Training Epoch: 1/2, step 5293/7134 completed (loss: 0.04811243340373039, acc: 0.9839416146278381)
[2025-02-13 03:09:30,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:30,847][root][INFO] - Training Epoch: 1/2, step 5294/7134 completed (loss: 0.040741026401519775, acc: 0.9902371168136597)
[2025-02-13 03:09:30,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:31,235][root][INFO] - Training Epoch: 1/2, step 5295/7134 completed (loss: 0.02933337725698948, acc: 0.9926470518112183)
[2025-02-13 03:09:31,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:31,657][root][INFO] - Training Epoch: 1/2, step 5296/7134 completed (loss: 0.059811823070049286, acc: 0.9819193482398987)
[2025-02-13 03:09:31,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:32,069][root][INFO] - Training Epoch: 1/2, step 5297/7134 completed (loss: 0.041153643280267715, acc: 0.9841954112052917)
[2025-02-13 03:09:32,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:32,541][root][INFO] - Training Epoch: 1/2, step 5298/7134 completed (loss: 0.042575180530548096, acc: 0.9829059839248657)
[2025-02-13 03:09:32,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:32,932][root][INFO] - Training Epoch: 1/2, step 5299/7134 completed (loss: 0.03703804314136505, acc: 0.9887640476226807)
[2025-02-13 03:09:33,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:33,343][root][INFO] - Training Epoch: 1/2, step 5300/7134 completed (loss: 0.047677818685770035, acc: 0.988252580165863)
[2025-02-13 03:09:33,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:33,748][root][INFO] - Training Epoch: 1/2, step 5301/7134 completed (loss: 0.040640030056238174, acc: 0.9838472604751587)
[2025-02-13 03:09:33,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:34,166][root][INFO] - Training Epoch: 1/2, step 5302/7134 completed (loss: 0.07832395285367966, acc: 0.978723406791687)
[2025-02-13 03:09:34,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:34,581][root][INFO] - Training Epoch: 1/2, step 5303/7134 completed (loss: 0.07792491465806961, acc: 0.982300877571106)
[2025-02-13 03:09:34,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:35,001][root][INFO] - Training Epoch: 1/2, step 5304/7134 completed (loss: 0.08979900926351547, acc: 0.9783693552017212)
[2025-02-13 03:09:35,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:35,413][root][INFO] - Training Epoch: 1/2, step 5305/7134 completed (loss: 0.08266837149858475, acc: 0.9751824736595154)
[2025-02-13 03:09:35,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:35,832][root][INFO] - Training Epoch: 1/2, step 5306/7134 completed (loss: 0.04859625920653343, acc: 0.987500011920929)
[2025-02-13 03:09:35,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:36,244][root][INFO] - Training Epoch: 1/2, step 5307/7134 completed (loss: 0.05986476317048073, acc: 0.9876543283462524)
[2025-02-13 03:09:36,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:36,639][root][INFO] - Training Epoch: 1/2, step 5308/7134 completed (loss: 0.06952600926160812, acc: 0.9833837151527405)
[2025-02-13 03:09:36,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:37,035][root][INFO] - Training Epoch: 1/2, step 5309/7134 completed (loss: 0.12639759480953217, acc: 0.969588577747345)
[2025-02-13 03:09:37,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:37,459][root][INFO] - Training Epoch: 1/2, step 5310/7134 completed (loss: 0.08651559054851532, acc: 0.9776847958564758)
[2025-02-13 03:09:37,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:37,878][root][INFO] - Training Epoch: 1/2, step 5311/7134 completed (loss: 0.06989631056785583, acc: 0.9855538010597229)
[2025-02-13 03:09:38,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:38,258][root][INFO] - Training Epoch: 1/2, step 5312/7134 completed (loss: 0.03491605073213577, acc: 0.9887429475784302)
[2025-02-13 03:09:38,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:38,674][root][INFO] - Training Epoch: 1/2, step 5313/7134 completed (loss: 0.04845486208796501, acc: 0.9836065769195557)
[2025-02-13 03:09:38,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:39,074][root][INFO] - Training Epoch: 1/2, step 5314/7134 completed (loss: 0.04551371932029724, acc: 0.9843205809593201)
[2025-02-13 03:09:39,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:39,493][root][INFO] - Training Epoch: 1/2, step 5315/7134 completed (loss: 0.03159841522574425, acc: 0.9918166995048523)
[2025-02-13 03:09:39,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:39,896][root][INFO] - Training Epoch: 1/2, step 5316/7134 completed (loss: 0.05080735683441162, acc: 0.9882352948188782)
[2025-02-13 03:09:40,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:40,259][root][INFO] - Training Epoch: 1/2, step 5317/7134 completed (loss: 0.03209155052900314, acc: 0.9892473220825195)
[2025-02-13 03:09:40,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:40,646][root][INFO] - Training Epoch: 1/2, step 5318/7134 completed (loss: 0.02554086223244667, acc: 0.9879102110862732)
[2025-02-13 03:09:40,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:41,045][root][INFO] - Training Epoch: 1/2, step 5319/7134 completed (loss: 0.07341174781322479, acc: 0.9835766553878784)
[2025-02-13 03:09:41,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:41,464][root][INFO] - Training Epoch: 1/2, step 5320/7134 completed (loss: 0.04128293693065643, acc: 0.989130437374115)
[2025-02-13 03:09:41,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:41,869][root][INFO] - Training Epoch: 1/2, step 5321/7134 completed (loss: 0.037310902029275894, acc: 0.9860464930534363)
[2025-02-13 03:09:42,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:42,280][root][INFO] - Training Epoch: 1/2, step 5322/7134 completed (loss: 0.02464469149708748, acc: 0.9924585223197937)
[2025-02-13 03:09:42,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:42,702][root][INFO] - Training Epoch: 1/2, step 5323/7134 completed (loss: 0.039158593863248825, acc: 0.9909090995788574)
[2025-02-13 03:09:42,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:43,101][root][INFO] - Training Epoch: 1/2, step 5324/7134 completed (loss: 0.02635372057557106, acc: 0.9938176274299622)
[2025-02-13 03:09:43,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:43,513][root][INFO] - Training Epoch: 1/2, step 5325/7134 completed (loss: 0.03062460385262966, acc: 0.991482138633728)
[2025-02-13 03:09:43,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:43,925][root][INFO] - Training Epoch: 1/2, step 5326/7134 completed (loss: 0.038921382278203964, acc: 0.9887096881866455)
[2025-02-13 03:09:44,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:44,330][root][INFO] - Training Epoch: 1/2, step 5327/7134 completed (loss: 0.0452539287507534, acc: 0.9898403286933899)
[2025-02-13 03:09:44,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:44,720][root][INFO] - Training Epoch: 1/2, step 5328/7134 completed (loss: 0.027855023741722107, acc: 0.9926605224609375)
[2025-02-13 03:09:44,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:45,126][root][INFO] - Training Epoch: 1/2, step 5329/7134 completed (loss: 0.058162350207567215, acc: 0.9854133129119873)
[2025-02-13 03:09:45,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:45,532][root][INFO] - Training Epoch: 1/2, step 5330/7134 completed (loss: 0.055573970079422, acc: 0.981249988079071)
[2025-02-13 03:09:45,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:45,929][root][INFO] - Training Epoch: 1/2, step 5331/7134 completed (loss: 0.05490001291036606, acc: 0.9877408146858215)
[2025-02-13 03:09:46,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:46,339][root][INFO] - Training Epoch: 1/2, step 5332/7134 completed (loss: 0.039225246757268906, acc: 0.9902439117431641)
[2025-02-13 03:09:46,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:46,736][root][INFO] - Training Epoch: 1/2, step 5333/7134 completed (loss: 0.023915573954582214, acc: 0.9890965819358826)
[2025-02-13 03:09:46,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:47,136][root][INFO] - Training Epoch: 1/2, step 5334/7134 completed (loss: 0.0909116342663765, acc: 0.9831546545028687)
[2025-02-13 03:09:47,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:47,530][root][INFO] - Training Epoch: 1/2, step 5335/7134 completed (loss: 0.04599366337060928, acc: 0.9888178706169128)
[2025-02-13 03:09:47,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:47,957][root][INFO] - Training Epoch: 1/2, step 5336/7134 completed (loss: 0.042860426008701324, acc: 0.9906250238418579)
[2025-02-13 03:09:48,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:48,408][root][INFO] - Training Epoch: 1/2, step 5337/7134 completed (loss: 0.0545516312122345, acc: 0.9799554347991943)
[2025-02-13 03:09:48,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:48,898][root][INFO] - Training Epoch: 1/2, step 5338/7134 completed (loss: 0.03383174166083336, acc: 0.9897260069847107)
[2025-02-13 03:09:49,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:49,300][root][INFO] - Training Epoch: 1/2, step 5339/7134 completed (loss: 0.060298919677734375, acc: 0.9831606149673462)
[2025-02-13 03:09:49,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:49,734][root][INFO] - Training Epoch: 1/2, step 5340/7134 completed (loss: 0.07786841690540314, acc: 0.9774696826934814)
[2025-02-13 03:09:49,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:50,158][root][INFO] - Training Epoch: 1/2, step 5341/7134 completed (loss: 0.022734778001904488, acc: 0.994350254535675)
[2025-02-13 03:09:50,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:50,603][root][INFO] - Training Epoch: 1/2, step 5342/7134 completed (loss: 0.07193952053785324, acc: 0.9773635268211365)
[2025-02-13 03:09:50,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:51,041][root][INFO] - Training Epoch: 1/2, step 5343/7134 completed (loss: 0.06003258004784584, acc: 0.9828495979309082)
[2025-02-13 03:09:51,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:51,491][root][INFO] - Training Epoch: 1/2, step 5344/7134 completed (loss: 0.049310680478811264, acc: 0.9831578731536865)
[2025-02-13 03:09:51,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:51,924][root][INFO] - Training Epoch: 1/2, step 5345/7134 completed (loss: 0.042640168219804764, acc: 0.9896551966667175)
[2025-02-13 03:09:52,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:52,357][root][INFO] - Training Epoch: 1/2, step 5346/7134 completed (loss: 0.05333290621638298, acc: 0.9874081611633301)
[2025-02-13 03:09:52,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:52,808][root][INFO] - Training Epoch: 1/2, step 5347/7134 completed (loss: 0.03382708877325058, acc: 0.988063633441925)
[2025-02-13 03:09:52,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:53,261][root][INFO] - Training Epoch: 1/2, step 5348/7134 completed (loss: 0.04682745411992073, acc: 0.9847645163536072)
[2025-02-13 03:09:53,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:54,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:54,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:54,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:55,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:55,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:55,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:56,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:56,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:57,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:57,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:57,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:58,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:58,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:58,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:59,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:59,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:59,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:00,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:00,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:00,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:01,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:01,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:01,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:02,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:02,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:02,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:03,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:03,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:03,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:04,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:04,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:05,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:05,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:05,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:06,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:06,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:06,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:07,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:07,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:07,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:08,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:08,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:08,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:09,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:09,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:10,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:10,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:10,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:11,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:11,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:11,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:12,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:12,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:12,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:13,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:13,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:13,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:14,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:14,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:14,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:15,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:15,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:15,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:16,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:16,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:16,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:17,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:17,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:17,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:18,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:18,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:18,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:19,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:19,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:19,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:20,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:20,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:20,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:21,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:21,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:21,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:22,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:22,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:22,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:23,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:23,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:24,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:24,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:24,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:25,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:25,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:25,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:26,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:26,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:26,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:27,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:27,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:27,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:28,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:28,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:28,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:29,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:29,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:29,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:30,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:30,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:30,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:31,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:31,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:31,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:32,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:32,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:32,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:33,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:33,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:34,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:34,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:34,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:35,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:35,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:35,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:36,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:36,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:36,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:37,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:37,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:37,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:38,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:38,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:38,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:39,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:39,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:39,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:40,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:40,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:40,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:41,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:41,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:41,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:42,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:42,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:42,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:43,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:43,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:45,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:45,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:46,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:46,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:46,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:47,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:47,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:47,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:48,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:48,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:48,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:49,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:49,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:49,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:50,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:50,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:50,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:51,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:51,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:52,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:52,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:52,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:53,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:53,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:53,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:54,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:54,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:54,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:55,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:55,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:55,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:56,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:56,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:56,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:57,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:57,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:57,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:58,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:58,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:58,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:59,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:59,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:00,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:00,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:00,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:01,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:01,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:01,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:02,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:02,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:02,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:03,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:03,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:03,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:04,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:04,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:04,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:05,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:05,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:05,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:06,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:06,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:06,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:07,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:07,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:07,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:08,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:08,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:08,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:08,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:09,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:09,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:09,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:10,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:10,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:11,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:11,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:11,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:11,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:12,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:12,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:12,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:13,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:13,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:13,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:14,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:14,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:15,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:15,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:15,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:16,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:16,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:16,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:17,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:17,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:17,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:18,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:18,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:19,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:19,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:20,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:20,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:20,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:21,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:21,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:21,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:22,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:22,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:22,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:23,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:23,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:23,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:24,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:24,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:24,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:25,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:25,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:25,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:26,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:26,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:26,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:27,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:27,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:28,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:28,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:29,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:29,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:29,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:30,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:30,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:30,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:31,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:31,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:32,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:32,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:32,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:33,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:33,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:34,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:34,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:34,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:35,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:35,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:36,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:36,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:36,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:37,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:37,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:38,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:38,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:38,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:39,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:39,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:39,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:40,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:40,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:40,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:41,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:41,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:41,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:42,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:42,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:42,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:43,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:43,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:43,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:44,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:44,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:45,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:45,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:45,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:46,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:46,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:46,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:47,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:47,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:48,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:48,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:48,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:49,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:49,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:49,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:50,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:50,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:51,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:51,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:51,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:51,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:52,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:52,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:53,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:53,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:54,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:54,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:54,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:55,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:55,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:56,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:56,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:56,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:57,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:57,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:57,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:58,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:58,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:58,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:59,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:59,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:59,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:00,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:00,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:01,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:01,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:02,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:02,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:03,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:03,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:04,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:04,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:04,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:05,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:05,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:05,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:06,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:06,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:06,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:07,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:07,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:07,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:08,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:08,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:08,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:09,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:09,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:09,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:10,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:10,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:11,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:11,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:11,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:12,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:12,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:12,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:13,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:13,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:13,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:14,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:14,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:14,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:15,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:15,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:15,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:16,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:16,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:16,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:17,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:17,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:17,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:18,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:18,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:18,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:19,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:19,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:19,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:20,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:20,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:20,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:21,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:21,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:21,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:22,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:22,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:22,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:23,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:23,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:23,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:24,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:24,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:24,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:25,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:25,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:25,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:26,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:26,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:26,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:28,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:28,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:29,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:29,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:29,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:30,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:30,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:30,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:31,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:31,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:31,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:32,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:32,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:32,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:33,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:33,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:33,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:34,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:34,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:34,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:35,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:35,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:35,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:36,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:36,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:37,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:37,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:37,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:38,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:38,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:38,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:39,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:39,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:39,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:40,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:40,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:40,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:41,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:41,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:41,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:41,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:42,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:42,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:43,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:43,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:43,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:44,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:44,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:45,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:45,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:45,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:46,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:46,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:47,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:47,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:47,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:48,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:48,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:48,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:49,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:49,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:49,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:50,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:50,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:51,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:51,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:52,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:52,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:52,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:53,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:53,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:53,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:54,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:54,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:55,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:55,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:55,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:56,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:56,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:56,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:57,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:57,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:57,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:58,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:58,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:58,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:59,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:59,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:59,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:00,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:00,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:00,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:01,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:01,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:01,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:02,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:02,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:02,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:03,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:03,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:03,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:04,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:04,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:05,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:05,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:05,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:06,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:06,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:06,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:07,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:07,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:07,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:08,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:08,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:08,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:09,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:09,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:10,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:10,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:10,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:11,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:11,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:11,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:12,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:12,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:12,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:13,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:13,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:13,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:14,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:14,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:14,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:15,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:15,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:15,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:16,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:16,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:17,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:17,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:17,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:18,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:18,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:19,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:19,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:19,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:20,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:20,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:20,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:21,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:21,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:22,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:22,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:22,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:23,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:23,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:23,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:24,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:24,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:24,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:25,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:25,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:25,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:26,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:26,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:26,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:27,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:27,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:27,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:28,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:28,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:28,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:29,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:29,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:29,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:30,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:30,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:31,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:31,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:31,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:32,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:32,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:32,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:33,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:33,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:33,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:34,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:34,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:35,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:35,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:36,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:36,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:37,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:37,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:37,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:38,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:38,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:39,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:39,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:40,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:40,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:40,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:41,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:41,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:41,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:42,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:42,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:42,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:43,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:43,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:44,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:44,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:44,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:45,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:45,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:45,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:46,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:46,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:46,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:47,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:47,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:47,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:48,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:48,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:49,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:49,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:49,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:50,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:50,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:50,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:51,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:51,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:51,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:52,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:52,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:53,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:53,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:53,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:54,610][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0617, device='cuda:0') eval_epoch_loss=tensor(0.0598, device='cuda:0') eval_epoch_acc=tensor(0.9835, device='cuda:0')
[2025-02-13 03:13:54,612][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:13:54,613][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:13:54,889][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_5349_loss_0.05983130633831024/model.pt
[2025-02-13 03:13:54,893][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:13:54,894][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.05983130633831024
[2025-02-13 03:13:54,894][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9834863543510437
[2025-02-13 03:13:55,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:55,351][root][INFO] - Training Epoch: 1/2, step 5349/7134 completed (loss: 0.06833534687757492, acc: 0.9819121360778809)
[2025-02-13 03:13:55,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:55,790][root][INFO] - Training Epoch: 1/2, step 5350/7134 completed (loss: 0.06135551631450653, acc: 0.9828269481658936)
[2025-02-13 03:13:55,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:56,204][root][INFO] - Training Epoch: 1/2, step 5351/7134 completed (loss: 0.05789721757173538, acc: 0.981389582157135)
[2025-02-13 03:13:56,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:56,610][root][INFO] - Training Epoch: 1/2, step 5352/7134 completed (loss: 0.04182356223464012, acc: 0.9870634078979492)
[2025-02-13 03:13:56,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:57,061][root][INFO] - Training Epoch: 1/2, step 5353/7134 completed (loss: 0.019985675811767578, acc: 0.9954904317855835)
[2025-02-13 03:13:57,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:57,513][root][INFO] - Training Epoch: 1/2, step 5354/7134 completed (loss: 0.059323396533727646, acc: 0.9823788404464722)
[2025-02-13 03:13:57,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:57,950][root][INFO] - Training Epoch: 1/2, step 5355/7134 completed (loss: 0.08187074959278107, acc: 0.9739952683448792)
[2025-02-13 03:13:58,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:58,391][root][INFO] - Training Epoch: 1/2, step 5356/7134 completed (loss: 0.07467152923345566, acc: 0.9788293838500977)
[2025-02-13 03:13:58,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:58,792][root][INFO] - Training Epoch: 1/2, step 5357/7134 completed (loss: 0.04636408016085625, acc: 0.9878048896789551)
[2025-02-13 03:13:58,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:59,231][root][INFO] - Training Epoch: 1/2, step 5358/7134 completed (loss: 0.056774456053972244, acc: 0.9903448224067688)
[2025-02-13 03:13:59,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:59,660][root][INFO] - Training Epoch: 1/2, step 5359/7134 completed (loss: 0.023143816739320755, acc: 0.9941725134849548)
[2025-02-13 03:13:59,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:00,087][root][INFO] - Training Epoch: 1/2, step 5360/7134 completed (loss: 0.012672425247728825, acc: 0.9958791136741638)
[2025-02-13 03:14:00,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:00,538][root][INFO] - Training Epoch: 1/2, step 5361/7134 completed (loss: 0.07407347112894058, acc: 0.9832335114479065)
[2025-02-13 03:14:00,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:00,983][root][INFO] - Training Epoch: 1/2, step 5362/7134 completed (loss: 0.03657710924744606, acc: 0.9850746393203735)
[2025-02-13 03:14:01,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:01,400][root][INFO] - Training Epoch: 1/2, step 5363/7134 completed (loss: 0.07219216227531433, acc: 0.9752907156944275)
[2025-02-13 03:14:01,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:01,826][root][INFO] - Training Epoch: 1/2, step 5364/7134 completed (loss: 0.03246385604143143, acc: 0.9922077655792236)
[2025-02-13 03:14:01,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:02,259][root][INFO] - Training Epoch: 1/2, step 5365/7134 completed (loss: 0.03421295806765556, acc: 0.9876543283462524)
[2025-02-13 03:14:02,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:02,651][root][INFO] - Training Epoch: 1/2, step 5366/7134 completed (loss: 0.07299938052892685, acc: 0.9861111044883728)
[2025-02-13 03:14:02,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:03,097][root][INFO] - Training Epoch: 1/2, step 5367/7134 completed (loss: 0.03981078788638115, acc: 0.9886934757232666)
[2025-02-13 03:14:03,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:03,533][root][INFO] - Training Epoch: 1/2, step 5368/7134 completed (loss: 0.040977876633405685, acc: 0.9816053509712219)
[2025-02-13 03:14:03,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:03,917][root][INFO] - Training Epoch: 1/2, step 5369/7134 completed (loss: 0.029348298907279968, acc: 0.9893842935562134)
[2025-02-13 03:14:04,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:04,347][root][INFO] - Training Epoch: 1/2, step 5370/7134 completed (loss: 0.06670865416526794, acc: 0.9811617136001587)
[2025-02-13 03:14:04,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:04,782][root][INFO] - Training Epoch: 1/2, step 5371/7134 completed (loss: 0.09084978699684143, acc: 0.9717608094215393)
[2025-02-13 03:14:04,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:05,219][root][INFO] - Training Epoch: 1/2, step 5372/7134 completed (loss: 0.016216157004237175, acc: 0.9929873943328857)
[2025-02-13 03:14:05,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:05,618][root][INFO] - Training Epoch: 1/2, step 5373/7134 completed (loss: 0.09579166769981384, acc: 0.9707112908363342)
[2025-02-13 03:14:05,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:05,937][root][INFO] - Training Epoch: 1/2, step 5374/7134 completed (loss: 0.0544964037835598, acc: 0.9807074069976807)
[2025-02-13 03:14:06,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:06,340][root][INFO] - Training Epoch: 1/2, step 5375/7134 completed (loss: 0.06599722057580948, acc: 0.9808428883552551)
[2025-02-13 03:14:06,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:06,734][root][INFO] - Training Epoch: 1/2, step 5376/7134 completed (loss: 0.024283403530716896, acc: 0.9912891983985901)
[2025-02-13 03:14:06,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:07,170][root][INFO] - Training Epoch: 1/2, step 5377/7134 completed (loss: 0.05341729149222374, acc: 0.9908952713012695)
[2025-02-13 03:14:07,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:07,605][root][INFO] - Training Epoch: 1/2, step 5378/7134 completed (loss: 0.02026139199733734, acc: 0.9939320683479309)
[2025-02-13 03:14:07,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:08,036][root][INFO] - Training Epoch: 1/2, step 5379/7134 completed (loss: 0.04591881111264229, acc: 0.9883268475532532)
[2025-02-13 03:14:08,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:08,443][root][INFO] - Training Epoch: 1/2, step 5380/7134 completed (loss: 0.04644300043582916, acc: 0.9876543283462524)
[2025-02-13 03:14:08,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:08,844][root][INFO] - Training Epoch: 1/2, step 5381/7134 completed (loss: 0.03450676053762436, acc: 0.9930264949798584)
[2025-02-13 03:14:08,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:09,243][root][INFO] - Training Epoch: 1/2, step 5382/7134 completed (loss: 0.019138241186738014, acc: 0.9931740760803223)
[2025-02-13 03:14:09,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:09,674][root][INFO] - Training Epoch: 1/2, step 5383/7134 completed (loss: 0.02771126851439476, acc: 0.9886363744735718)
[2025-02-13 03:14:09,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:10,127][root][INFO] - Training Epoch: 1/2, step 5384/7134 completed (loss: 0.03210233896970749, acc: 0.9905213117599487)
[2025-02-13 03:14:10,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:10,548][root][INFO] - Training Epoch: 1/2, step 5385/7134 completed (loss: 0.039983898401260376, acc: 0.9867899417877197)
[2025-02-13 03:14:10,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:10,973][root][INFO] - Training Epoch: 1/2, step 5386/7134 completed (loss: 0.020204419270157814, acc: 0.9943820238113403)
[2025-02-13 03:14:11,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:11,415][root][INFO] - Training Epoch: 1/2, step 5387/7134 completed (loss: 0.016393564641475677, acc: 0.9980952143669128)
[2025-02-13 03:14:11,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:11,853][root][INFO] - Training Epoch: 1/2, step 5388/7134 completed (loss: 0.04849807173013687, acc: 0.9898989796638489)
[2025-02-13 03:14:11,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:12,259][root][INFO] - Training Epoch: 1/2, step 5389/7134 completed (loss: 0.027699416503310204, acc: 0.9936102032661438)
[2025-02-13 03:14:12,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:12,681][root][INFO] - Training Epoch: 1/2, step 5390/7134 completed (loss: 0.04535363242030144, acc: 0.9857369065284729)
[2025-02-13 03:14:12,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:13,050][root][INFO] - Training Epoch: 1/2, step 5391/7134 completed (loss: 0.07569263130426407, acc: 0.9845626354217529)
[2025-02-13 03:14:13,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:13,449][root][INFO] - Training Epoch: 1/2, step 5392/7134 completed (loss: 0.05611489340662956, acc: 0.9832317233085632)
[2025-02-13 03:14:13,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:13,850][root][INFO] - Training Epoch: 1/2, step 5393/7134 completed (loss: 0.04066856577992439, acc: 0.98046875)
[2025-02-13 03:14:13,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:14,217][root][INFO] - Training Epoch: 1/2, step 5394/7134 completed (loss: 0.07944436371326447, acc: 0.97817462682724)
[2025-02-13 03:14:14,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:14,647][root][INFO] - Training Epoch: 1/2, step 5395/7134 completed (loss: 0.044900067150592804, acc: 0.9879336357116699)
[2025-02-13 03:14:14,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:15,067][root][INFO] - Training Epoch: 1/2, step 5396/7134 completed (loss: 0.04799528419971466, acc: 0.9864661693572998)
[2025-02-13 03:14:15,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:15,469][root][INFO] - Training Epoch: 1/2, step 5397/7134 completed (loss: 0.04716888442635536, acc: 0.9864636063575745)
[2025-02-13 03:14:15,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:15,877][root][INFO] - Training Epoch: 1/2, step 5398/7134 completed (loss: 0.027945684269070625, acc: 0.9896755218505859)
[2025-02-13 03:14:16,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:16,289][root][INFO] - Training Epoch: 1/2, step 5399/7134 completed (loss: 0.03426240384578705, acc: 0.9917920827865601)
[2025-02-13 03:14:16,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:16,710][root][INFO] - Training Epoch: 1/2, step 5400/7134 completed (loss: 0.13593949377536774, acc: 0.9686520099639893)
[2025-02-13 03:14:16,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:17,128][root][INFO] - Training Epoch: 1/2, step 5401/7134 completed (loss: 0.032497555017471313, acc: 0.9919742941856384)
[2025-02-13 03:14:17,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:17,546][root][INFO] - Training Epoch: 1/2, step 5402/7134 completed (loss: 0.046266525983810425, acc: 0.9867841601371765)
[2025-02-13 03:14:17,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:17,918][root][INFO] - Training Epoch: 1/2, step 5403/7134 completed (loss: 0.0798405259847641, acc: 0.9787928462028503)
[2025-02-13 03:14:18,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:18,349][root][INFO] - Training Epoch: 1/2, step 5404/7134 completed (loss: 0.12264741212129593, acc: 0.9731543660163879)
[2025-02-13 03:14:18,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:18,798][root][INFO] - Training Epoch: 1/2, step 5405/7134 completed (loss: 0.051951318979263306, acc: 0.9842932224273682)
[2025-02-13 03:14:18,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:19,233][root][INFO] - Training Epoch: 1/2, step 5406/7134 completed (loss: 0.043291762471199036, acc: 0.988304078578949)
[2025-02-13 03:14:19,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:19,627][root][INFO] - Training Epoch: 1/2, step 5407/7134 completed (loss: 0.04421553015708923, acc: 0.9830917716026306)
[2025-02-13 03:14:19,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:20,039][root][INFO] - Training Epoch: 1/2, step 5408/7134 completed (loss: 0.09648928046226501, acc: 0.9812792539596558)
[2025-02-13 03:14:20,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:20,453][root][INFO] - Training Epoch: 1/2, step 5409/7134 completed (loss: 0.10627356171607971, acc: 0.9803328514099121)
[2025-02-13 03:14:20,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:20,869][root][INFO] - Training Epoch: 1/2, step 5410/7134 completed (loss: 0.06832987070083618, acc: 0.9776902794837952)
[2025-02-13 03:14:20,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:21,295][root][INFO] - Training Epoch: 1/2, step 5411/7134 completed (loss: 0.0973084419965744, acc: 0.9709724187850952)
[2025-02-13 03:14:21,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:21,738][root][INFO] - Training Epoch: 1/2, step 5412/7134 completed (loss: 0.09728024899959564, acc: 0.9730769395828247)
[2025-02-13 03:14:21,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:22,198][root][INFO] - Training Epoch: 1/2, step 5413/7134 completed (loss: 0.0940970629453659, acc: 0.9671428799629211)
[2025-02-13 03:14:22,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:22,652][root][INFO] - Training Epoch: 1/2, step 5414/7134 completed (loss: 0.1471567451953888, acc: 0.9601542353630066)
[2025-02-13 03:14:22,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:23,071][root][INFO] - Training Epoch: 1/2, step 5415/7134 completed (loss: 0.11225584149360657, acc: 0.9699140191078186)
[2025-02-13 03:14:23,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:23,526][root][INFO] - Training Epoch: 1/2, step 5416/7134 completed (loss: 0.06969722360372543, acc: 0.9779326319694519)
[2025-02-13 03:14:23,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:23,976][root][INFO] - Training Epoch: 1/2, step 5417/7134 completed (loss: 0.04696778208017349, acc: 0.9912609457969666)
[2025-02-13 03:14:24,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:24,383][root][INFO] - Training Epoch: 1/2, step 5418/7134 completed (loss: 0.054497599601745605, acc: 0.9778434038162231)
[2025-02-13 03:14:24,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:24,795][root][INFO] - Training Epoch: 1/2, step 5419/7134 completed (loss: 0.12517592310905457, acc: 0.9694915413856506)
[2025-02-13 03:14:24,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:25,213][root][INFO] - Training Epoch: 1/2, step 5420/7134 completed (loss: 0.1285346895456314, acc: 0.9685314893722534)
[2025-02-13 03:14:25,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:25,646][root][INFO] - Training Epoch: 1/2, step 5421/7134 completed (loss: 0.10992652177810669, acc: 0.9672386646270752)
[2025-02-13 03:14:25,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:26,076][root][INFO] - Training Epoch: 1/2, step 5422/7134 completed (loss: 0.06924981623888016, acc: 0.9837278127670288)
[2025-02-13 03:14:26,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:26,386][root][INFO] - Training Epoch: 1/2, step 5423/7134 completed (loss: 0.08371312916278839, acc: 0.9824047088623047)
[2025-02-13 03:14:26,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:26,780][root][INFO] - Training Epoch: 1/2, step 5424/7134 completed (loss: 0.0409664586186409, acc: 0.9911971688270569)
[2025-02-13 03:14:26,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:27,137][root][INFO] - Training Epoch: 1/2, step 5425/7134 completed (loss: 0.05679326876997948, acc: 0.9883494973182678)
[2025-02-13 03:14:27,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:27,514][root][INFO] - Training Epoch: 1/2, step 5426/7134 completed (loss: 0.07625525444746017, acc: 0.9879518151283264)
[2025-02-13 03:14:27,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:27,923][root][INFO] - Training Epoch: 1/2, step 5427/7134 completed (loss: 0.0339956134557724, acc: 0.9873617887496948)
[2025-02-13 03:14:28,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:28,321][root][INFO] - Training Epoch: 1/2, step 5428/7134 completed (loss: 0.05349056050181389, acc: 0.9846677780151367)
[2025-02-13 03:14:28,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:28,698][root][INFO] - Training Epoch: 1/2, step 5429/7134 completed (loss: 0.08430367708206177, acc: 0.9801443815231323)
[2025-02-13 03:14:28,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:29,112][root][INFO] - Training Epoch: 1/2, step 5430/7134 completed (loss: 0.03246914967894554, acc: 0.9890282154083252)
[2025-02-13 03:14:29,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:29,475][root][INFO] - Training Epoch: 1/2, step 5431/7134 completed (loss: 0.0339280404150486, acc: 0.992438554763794)
[2025-02-13 03:14:29,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:29,870][root][INFO] - Training Epoch: 1/2, step 5432/7134 completed (loss: 0.031801071017980576, acc: 0.9905837774276733)
[2025-02-13 03:14:29,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:30,217][root][INFO] - Training Epoch: 1/2, step 5433/7134 completed (loss: 0.048019926995038986, acc: 0.991919219493866)
[2025-02-13 03:14:30,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:30,606][root][INFO] - Training Epoch: 1/2, step 5434/7134 completed (loss: 0.0417078360915184, acc: 0.9899396300315857)
[2025-02-13 03:14:30,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:30,970][root][INFO] - Training Epoch: 1/2, step 5435/7134 completed (loss: 0.031966954469680786, acc: 0.991631805896759)
[2025-02-13 03:14:31,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:31,377][root][INFO] - Training Epoch: 1/2, step 5436/7134 completed (loss: 0.034993670880794525, acc: 0.9925816059112549)
[2025-02-13 03:14:31,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:31,783][root][INFO] - Training Epoch: 1/2, step 5437/7134 completed (loss: 0.013461358845233917, acc: 0.9965096116065979)
[2025-02-13 03:14:31,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:32,228][root][INFO] - Training Epoch: 1/2, step 5438/7134 completed (loss: 0.02869165875017643, acc: 0.9909560680389404)
[2025-02-13 03:14:32,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:32,648][root][INFO] - Training Epoch: 1/2, step 5439/7134 completed (loss: 0.05657519772648811, acc: 0.9825581312179565)
[2025-02-13 03:14:32,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:33,085][root][INFO] - Training Epoch: 1/2, step 5440/7134 completed (loss: 0.06670626252889633, acc: 0.9821200370788574)
[2025-02-13 03:14:33,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:33,517][root][INFO] - Training Epoch: 1/2, step 5441/7134 completed (loss: 0.04296652227640152, acc: 0.990208089351654)
[2025-02-13 03:14:33,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:33,933][root][INFO] - Training Epoch: 1/2, step 5442/7134 completed (loss: 0.025064684450626373, acc: 0.9945873022079468)
[2025-02-13 03:14:34,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:34,327][root][INFO] - Training Epoch: 1/2, step 5443/7134 completed (loss: 0.042578015476465225, acc: 0.9857954382896423)
[2025-02-13 03:14:34,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:34,744][root][INFO] - Training Epoch: 1/2, step 5444/7134 completed (loss: 0.039172232151031494, acc: 0.985401451587677)
[2025-02-13 03:14:34,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:35,163][root][INFO] - Training Epoch: 1/2, step 5445/7134 completed (loss: 0.05817578360438347, acc: 0.9853747487068176)
[2025-02-13 03:14:35,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:35,580][root][INFO] - Training Epoch: 1/2, step 5446/7134 completed (loss: 0.027557896450161934, acc: 0.994397759437561)
[2025-02-13 03:14:35,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:35,976][root][INFO] - Training Epoch: 1/2, step 5447/7134 completed (loss: 0.0318492092192173, acc: 0.990338146686554)
[2025-02-13 03:14:36,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:36,435][root][INFO] - Training Epoch: 1/2, step 5448/7134 completed (loss: 0.03945550322532654, acc: 0.9860334992408752)
[2025-02-13 03:14:36,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:36,841][root][INFO] - Training Epoch: 1/2, step 5449/7134 completed (loss: 0.018372971564531326, acc: 0.9929824471473694)
[2025-02-13 03:14:36,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:37,260][root][INFO] - Training Epoch: 1/2, step 5450/7134 completed (loss: 0.023069174960255623, acc: 0.992277979850769)
[2025-02-13 03:14:37,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:37,672][root][INFO] - Training Epoch: 1/2, step 5451/7134 completed (loss: 0.046478744596242905, acc: 0.9882044792175293)
[2025-02-13 03:14:37,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:38,089][root][INFO] - Training Epoch: 1/2, step 5452/7134 completed (loss: 0.06243627518415451, acc: 0.9844961166381836)
[2025-02-13 03:14:38,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:38,530][root][INFO] - Training Epoch: 1/2, step 5453/7134 completed (loss: 0.05556776374578476, acc: 0.9865030646324158)
[2025-02-13 03:14:38,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:38,975][root][INFO] - Training Epoch: 1/2, step 5454/7134 completed (loss: 0.037705082446336746, acc: 0.9882659912109375)
[2025-02-13 03:14:39,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:39,415][root][INFO] - Training Epoch: 1/2, step 5455/7134 completed (loss: 0.10104551911354065, acc: 0.9760100841522217)
[2025-02-13 03:14:39,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:39,854][root][INFO] - Training Epoch: 1/2, step 5456/7134 completed (loss: 0.04895509034395218, acc: 0.986146092414856)
[2025-02-13 03:14:40,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:40,304][root][INFO] - Training Epoch: 1/2, step 5457/7134 completed (loss: 0.06958891451358795, acc: 0.9798657894134521)
[2025-02-13 03:14:40,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:40,740][root][INFO] - Training Epoch: 1/2, step 5458/7134 completed (loss: 0.04655904695391655, acc: 0.987261176109314)
[2025-02-13 03:14:40,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:41,157][root][INFO] - Training Epoch: 1/2, step 5459/7134 completed (loss: 0.021051274612545967, acc: 0.9910581111907959)
[2025-02-13 03:14:41,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:41,563][root][INFO] - Training Epoch: 1/2, step 5460/7134 completed (loss: 0.038569651544094086, acc: 0.9944367408752441)
[2025-02-13 03:14:41,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:41,968][root][INFO] - Training Epoch: 1/2, step 5461/7134 completed (loss: 0.081375852227211, acc: 0.9690949320793152)
[2025-02-13 03:14:42,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:42,407][root][INFO] - Training Epoch: 1/2, step 5462/7134 completed (loss: 0.01971255987882614, acc: 0.9928469061851501)
[2025-02-13 03:14:42,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:42,819][root][INFO] - Training Epoch: 1/2, step 5463/7134 completed (loss: 0.029087185859680176, acc: 0.9923664331436157)
[2025-02-13 03:14:42,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:43,247][root][INFO] - Training Epoch: 1/2, step 5464/7134 completed (loss: 0.033940404653549194, acc: 0.9881109595298767)
[2025-02-13 03:14:43,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:43,683][root][INFO] - Training Epoch: 1/2, step 5465/7134 completed (loss: 0.06933515518903732, acc: 0.9865471124649048)
[2025-02-13 03:14:43,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:44,126][root][INFO] - Training Epoch: 1/2, step 5466/7134 completed (loss: 0.024573028087615967, acc: 0.9930955171585083)
[2025-02-13 03:14:44,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:44,592][root][INFO] - Training Epoch: 1/2, step 5467/7134 completed (loss: 0.03995279222726822, acc: 0.98591548204422)
[2025-02-13 03:14:44,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:45,034][root][INFO] - Training Epoch: 1/2, step 5468/7134 completed (loss: 0.041931264102458954, acc: 0.9895591735839844)
[2025-02-13 03:14:45,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:45,471][root][INFO] - Training Epoch: 1/2, step 5469/7134 completed (loss: 0.029909683391451836, acc: 0.9926380515098572)
[2025-02-13 03:14:45,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:45,915][root][INFO] - Training Epoch: 1/2, step 5470/7134 completed (loss: 0.02691982500255108, acc: 0.9927710890769958)
[2025-02-13 03:14:46,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:46,342][root][INFO] - Training Epoch: 1/2, step 5471/7134 completed (loss: 0.025062888860702515, acc: 0.9932432174682617)
[2025-02-13 03:14:46,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:46,778][root][INFO] - Training Epoch: 1/2, step 5472/7134 completed (loss: 0.031201021745800972, acc: 0.9950124621391296)
[2025-02-13 03:14:46,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:47,231][root][INFO] - Training Epoch: 1/2, step 5473/7134 completed (loss: 0.024500641971826553, acc: 0.991411030292511)
[2025-02-13 03:14:47,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:47,649][root][INFO] - Training Epoch: 1/2, step 5474/7134 completed (loss: 0.03986670449376106, acc: 0.9885714054107666)
[2025-02-13 03:14:47,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:48,085][root][INFO] - Training Epoch: 1/2, step 5475/7134 completed (loss: 0.027690382674336433, acc: 0.9943100810050964)
[2025-02-13 03:14:48,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:48,491][root][INFO] - Training Epoch: 1/2, step 5476/7134 completed (loss: 0.019860420376062393, acc: 0.9942113161087036)
[2025-02-13 03:14:48,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:48,931][root][INFO] - Training Epoch: 1/2, step 5477/7134 completed (loss: 0.01688903570175171, acc: 0.9950739145278931)
[2025-02-13 03:14:49,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:49,399][root][INFO] - Training Epoch: 1/2, step 5478/7134 completed (loss: 0.06050023436546326, acc: 0.9845626354217529)
[2025-02-13 03:14:49,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:49,844][root][INFO] - Training Epoch: 1/2, step 5479/7134 completed (loss: 0.05292317271232605, acc: 0.98097825050354)
[2025-02-13 03:14:49,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:50,284][root][INFO] - Training Epoch: 1/2, step 5480/7134 completed (loss: 0.06172274425625801, acc: 0.9828947186470032)
[2025-02-13 03:14:50,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:50,682][root][INFO] - Training Epoch: 1/2, step 5481/7134 completed (loss: 0.03181665763258934, acc: 0.9858585596084595)
[2025-02-13 03:14:50,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:51,398][root][INFO] - Training Epoch: 1/2, step 5482/7134 completed (loss: 0.03699488565325737, acc: 0.990326464176178)
[2025-02-13 03:14:51,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:51,887][root][INFO] - Training Epoch: 1/2, step 5483/7134 completed (loss: 0.05903045833110809, acc: 0.9832869172096252)
[2025-02-13 03:14:52,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:52,323][root][INFO] - Training Epoch: 1/2, step 5484/7134 completed (loss: 0.057204049080610275, acc: 0.9805285334587097)
[2025-02-13 03:14:52,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:52,727][root][INFO] - Training Epoch: 1/2, step 5485/7134 completed (loss: 0.07805377244949341, acc: 0.9727767705917358)
[2025-02-13 03:14:52,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:53,170][root][INFO] - Training Epoch: 1/2, step 5486/7134 completed (loss: 0.08568073064088821, acc: 0.9740484356880188)
[2025-02-13 03:14:53,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:53,567][root][INFO] - Training Epoch: 1/2, step 5487/7134 completed (loss: 0.025871170684695244, acc: 0.9918699264526367)
[2025-02-13 03:14:53,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:54,036][root][INFO] - Training Epoch: 1/2, step 5488/7134 completed (loss: 0.08758504688739777, acc: 0.9671052694320679)
[2025-02-13 03:14:54,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:54,484][root][INFO] - Training Epoch: 1/2, step 5489/7134 completed (loss: 0.05005012825131416, acc: 0.9860681295394897)
[2025-02-13 03:14:54,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:54,791][root][INFO] - Training Epoch: 1/2, step 5490/7134 completed (loss: 0.032173532992601395, acc: 0.9908883571624756)
[2025-02-13 03:14:54,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:55,188][root][INFO] - Training Epoch: 1/2, step 5491/7134 completed (loss: 0.032393403351306915, acc: 0.9912023544311523)
[2025-02-13 03:14:55,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:55,590][root][INFO] - Training Epoch: 1/2, step 5492/7134 completed (loss: 0.05050869658589363, acc: 0.9847715497016907)
[2025-02-13 03:14:55,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:55,992][root][INFO] - Training Epoch: 1/2, step 5493/7134 completed (loss: 0.05954255163669586, acc: 0.98097825050354)
[2025-02-13 03:14:56,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:56,448][root][INFO] - Training Epoch: 1/2, step 5494/7134 completed (loss: 0.017955012619495392, acc: 0.9971550703048706)
[2025-02-13 03:14:56,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:56,827][root][INFO] - Training Epoch: 1/2, step 5495/7134 completed (loss: 0.029961111024022102, acc: 0.9892473220825195)
[2025-02-13 03:14:56,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:57,105][root][INFO] - Training Epoch: 1/2, step 5496/7134 completed (loss: 0.07988264411687851, acc: 0.9727626442909241)
[2025-02-13 03:14:57,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:57,506][root][INFO] - Training Epoch: 1/2, step 5497/7134 completed (loss: 0.08589046448469162, acc: 0.9780380725860596)
[2025-02-13 03:14:57,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:57,936][root][INFO] - Training Epoch: 1/2, step 5498/7134 completed (loss: 0.040187153965234756, acc: 0.982758641242981)
[2025-02-13 03:14:58,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:58,336][root][INFO] - Training Epoch: 1/2, step 5499/7134 completed (loss: 0.0573100745677948, acc: 0.9773869514465332)
[2025-02-13 03:14:58,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:58,751][root][INFO] - Training Epoch: 1/2, step 5500/7134 completed (loss: 0.04199967533349991, acc: 0.9849931597709656)
[2025-02-13 03:14:58,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:59,185][root][INFO] - Training Epoch: 1/2, step 5501/7134 completed (loss: 0.035743243992328644, acc: 0.990123450756073)
[2025-02-13 03:14:59,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:59,625][root][INFO] - Training Epoch: 1/2, step 5502/7134 completed (loss: 0.03787694126367569, acc: 0.9892037510871887)
[2025-02-13 03:14:59,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:00,023][root][INFO] - Training Epoch: 1/2, step 5503/7134 completed (loss: 0.05639348179101944, acc: 0.9840954542160034)
[2025-02-13 03:15:00,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:00,462][root][INFO] - Training Epoch: 1/2, step 5504/7134 completed (loss: 0.04461843892931938, acc: 0.9878934621810913)
[2025-02-13 03:15:00,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:00,875][root][INFO] - Training Epoch: 1/2, step 5505/7134 completed (loss: 0.054093483835458755, acc: 0.980141818523407)
[2025-02-13 03:15:01,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:01,307][root][INFO] - Training Epoch: 1/2, step 5506/7134 completed (loss: 0.05968594178557396, acc: 0.9838523864746094)
[2025-02-13 03:15:01,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:01,731][root][INFO] - Training Epoch: 1/2, step 5507/7134 completed (loss: 0.05566934123635292, acc: 0.9831932783126831)
[2025-02-13 03:15:01,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:02,170][root][INFO] - Training Epoch: 1/2, step 5508/7134 completed (loss: 0.05704896152019501, acc: 0.9848130941390991)
[2025-02-13 03:15:02,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:02,629][root][INFO] - Training Epoch: 1/2, step 5509/7134 completed (loss: 0.04031374305486679, acc: 0.9906432628631592)
[2025-02-13 03:15:02,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:03,070][root][INFO] - Training Epoch: 1/2, step 5510/7134 completed (loss: 0.05555858090519905, acc: 0.9845758080482483)
[2025-02-13 03:15:03,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:03,544][root][INFO] - Training Epoch: 1/2, step 5511/7134 completed (loss: 0.06592514365911484, acc: 0.983031690120697)
[2025-02-13 03:15:03,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:03,973][root][INFO] - Training Epoch: 1/2, step 5512/7134 completed (loss: 0.03479555994272232, acc: 0.9894598126411438)
[2025-02-13 03:15:04,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:04,385][root][INFO] - Training Epoch: 1/2, step 5513/7134 completed (loss: 0.06729581952095032, acc: 0.979619562625885)
[2025-02-13 03:15:04,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:04,827][root][INFO] - Training Epoch: 1/2, step 5514/7134 completed (loss: 0.15131229162216187, acc: 0.9626865386962891)
[2025-02-13 03:15:04,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:05,238][root][INFO] - Training Epoch: 1/2, step 5515/7134 completed (loss: 0.04774726554751396, acc: 0.984308123588562)
[2025-02-13 03:15:05,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:05,632][root][INFO] - Training Epoch: 1/2, step 5516/7134 completed (loss: 0.05037468671798706, acc: 0.9915611743927002)
[2025-02-13 03:15:05,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:06,034][root][INFO] - Training Epoch: 1/2, step 5517/7134 completed (loss: 0.037416763603687286, acc: 0.9867021441459656)
[2025-02-13 03:15:06,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:06,474][root][INFO] - Training Epoch: 1/2, step 5518/7134 completed (loss: 0.058447401970624924, acc: 0.9850543737411499)
[2025-02-13 03:15:06,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:06,912][root][INFO] - Training Epoch: 1/2, step 5519/7134 completed (loss: 0.026143832132220268, acc: 0.9903448224067688)
[2025-02-13 03:15:07,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:07,354][root][INFO] - Training Epoch: 1/2, step 5520/7134 completed (loss: 0.025520620867609978, acc: 0.9943374991416931)
[2025-02-13 03:15:07,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:07,805][root][INFO] - Training Epoch: 1/2, step 5521/7134 completed (loss: 0.057704608887434006, acc: 0.9849246144294739)
[2025-02-13 03:15:07,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:08,238][root][INFO] - Training Epoch: 1/2, step 5522/7134 completed (loss: 0.07182985544204712, acc: 0.9816993474960327)
[2025-02-13 03:15:08,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:08,621][root][INFO] - Training Epoch: 1/2, step 5523/7134 completed (loss: 0.05287300422787666, acc: 0.9859550595283508)
[2025-02-13 03:15:08,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:09,020][root][INFO] - Training Epoch: 1/2, step 5524/7134 completed (loss: 0.06151283159852028, acc: 0.9814241528511047)
[2025-02-13 03:15:09,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:09,456][root][INFO] - Training Epoch: 1/2, step 5525/7134 completed (loss: 0.047304004430770874, acc: 0.9824304580688477)
[2025-02-13 03:15:09,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:09,867][root][INFO] - Training Epoch: 1/2, step 5526/7134 completed (loss: 0.047825887799263, acc: 0.9896907210350037)
[2025-02-13 03:15:10,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:10,280][root][INFO] - Training Epoch: 1/2, step 5527/7134 completed (loss: 0.0517469197511673, acc: 0.9882352948188782)
[2025-02-13 03:15:10,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:10,716][root][INFO] - Training Epoch: 1/2, step 5528/7134 completed (loss: 0.040042515844106674, acc: 0.9857988357543945)
[2025-02-13 03:15:10,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:11,158][root][INFO] - Training Epoch: 1/2, step 5529/7134 completed (loss: 0.06876998394727707, acc: 0.978723406791687)
[2025-02-13 03:15:11,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:11,594][root][INFO] - Training Epoch: 1/2, step 5530/7134 completed (loss: 0.04925193265080452, acc: 0.9910581111907959)
[2025-02-13 03:15:11,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:12,037][root][INFO] - Training Epoch: 1/2, step 5531/7134 completed (loss: 0.05345294252038002, acc: 0.9850560426712036)
[2025-02-13 03:15:12,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:12,495][root][INFO] - Training Epoch: 1/2, step 5532/7134 completed (loss: 0.07537327706813812, acc: 0.9765142202377319)
[2025-02-13 03:15:12,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:12,954][root][INFO] - Training Epoch: 1/2, step 5533/7134 completed (loss: 0.0187364649027586, acc: 0.9943310618400574)
[2025-02-13 03:15:13,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:13,413][root][INFO] - Training Epoch: 1/2, step 5534/7134 completed (loss: 0.06250325590372086, acc: 0.9836448431015015)
[2025-02-13 03:15:13,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:13,861][root][INFO] - Training Epoch: 1/2, step 5535/7134 completed (loss: 0.03693088889122009, acc: 0.9902067184448242)
[2025-02-13 03:15:13,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:14,288][root][INFO] - Training Epoch: 1/2, step 5536/7134 completed (loss: 0.035754721611738205, acc: 0.9883720874786377)
[2025-02-13 03:15:14,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:14,715][root][INFO] - Training Epoch: 1/2, step 5537/7134 completed (loss: 0.060254763811826706, acc: 0.97782963514328)
[2025-02-13 03:15:14,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:15,160][root][INFO] - Training Epoch: 1/2, step 5538/7134 completed (loss: 0.04014922305941582, acc: 0.9852104783058167)
[2025-02-13 03:15:15,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:15,624][root][INFO] - Training Epoch: 1/2, step 5539/7134 completed (loss: 0.045665547251701355, acc: 0.9853603839874268)
[2025-02-13 03:15:15,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:16,069][root][INFO] - Training Epoch: 1/2, step 5540/7134 completed (loss: 0.05266785994172096, acc: 0.9907894730567932)
[2025-02-13 03:15:16,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:16,521][root][INFO] - Training Epoch: 1/2, step 5541/7134 completed (loss: 0.039150603115558624, acc: 0.9893617033958435)
[2025-02-13 03:15:16,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:16,943][root][INFO] - Training Epoch: 1/2, step 5542/7134 completed (loss: 0.029759345576167107, acc: 0.9914407730102539)
[2025-02-13 03:15:17,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:17,399][root][INFO] - Training Epoch: 1/2, step 5543/7134 completed (loss: 0.04004382714629173, acc: 0.9878048896789551)
[2025-02-13 03:15:17,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:17,851][root][INFO] - Training Epoch: 1/2, step 5544/7134 completed (loss: 0.02488691732287407, acc: 0.9924731254577637)
[2025-02-13 03:15:17,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:18,305][root][INFO] - Training Epoch: 1/2, step 5545/7134 completed (loss: 0.04783092066645622, acc: 0.980327844619751)
[2025-02-13 03:15:18,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:18,740][root][INFO] - Training Epoch: 1/2, step 5546/7134 completed (loss: 0.04356606304645538, acc: 0.9856035709381104)
[2025-02-13 03:15:18,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:19,195][root][INFO] - Training Epoch: 1/2, step 5547/7134 completed (loss: 0.035502463579177856, acc: 0.9919354915618896)
[2025-02-13 03:15:19,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:19,622][root][INFO] - Training Epoch: 1/2, step 5548/7134 completed (loss: 0.033598851412534714, acc: 0.9906976819038391)
[2025-02-13 03:15:19,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:20,007][root][INFO] - Training Epoch: 1/2, step 5549/7134 completed (loss: 0.0536048598587513, acc: 0.9871428608894348)
[2025-02-13 03:15:20,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:20,445][root][INFO] - Training Epoch: 1/2, step 5550/7134 completed (loss: 0.017599230632185936, acc: 0.9941245317459106)
[2025-02-13 03:15:20,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:20,845][root][INFO] - Training Epoch: 1/2, step 5551/7134 completed (loss: 0.07058893889188766, acc: 0.981055498123169)
[2025-02-13 03:15:20,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:21,285][root][INFO] - Training Epoch: 1/2, step 5552/7134 completed (loss: 0.0274509247392416, acc: 0.9940546751022339)
[2025-02-13 03:15:21,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:21,716][root][INFO] - Training Epoch: 1/2, step 5553/7134 completed (loss: 0.05004805326461792, acc: 0.9864029884338379)
[2025-02-13 03:15:21,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:22,140][root][INFO] - Training Epoch: 1/2, step 5554/7134 completed (loss: 0.022147057577967644, acc: 0.9926650524139404)
[2025-02-13 03:15:22,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:22,563][root][INFO] - Training Epoch: 1/2, step 5555/7134 completed (loss: 0.04572644829750061, acc: 0.9839572310447693)
[2025-02-13 03:15:22,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:22,958][root][INFO] - Training Epoch: 1/2, step 5556/7134 completed (loss: 0.0660613402724266, acc: 0.9792477488517761)
[2025-02-13 03:15:23,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:23,405][root][INFO] - Training Epoch: 1/2, step 5557/7134 completed (loss: 0.02568679489195347, acc: 0.994397759437561)
[2025-02-13 03:15:23,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:23,845][root][INFO] - Training Epoch: 1/2, step 5558/7134 completed (loss: 0.035502128303050995, acc: 0.9888198971748352)
[2025-02-13 03:15:23,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:24,277][root][INFO] - Training Epoch: 1/2, step 5559/7134 completed (loss: 0.04067851975560188, acc: 0.992546558380127)
[2025-02-13 03:15:24,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:24,698][root][INFO] - Training Epoch: 1/2, step 5560/7134 completed (loss: 0.06352151185274124, acc: 0.9831606149673462)
[2025-02-13 03:15:24,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:25,066][root][INFO] - Training Epoch: 1/2, step 5561/7134 completed (loss: 0.043514467775821686, acc: 0.9943289160728455)
[2025-02-13 03:15:25,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:25,498][root][INFO] - Training Epoch: 1/2, step 5562/7134 completed (loss: 0.04667261987924576, acc: 0.9948805570602417)
[2025-02-13 03:15:25,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:25,931][root][INFO] - Training Epoch: 1/2, step 5563/7134 completed (loss: 0.05166417360305786, acc: 0.9894894957542419)
[2025-02-13 03:15:26,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:26,334][root][INFO] - Training Epoch: 1/2, step 5564/7134 completed (loss: 0.03097505122423172, acc: 0.9944211840629578)
[2025-02-13 03:15:26,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:26,768][root][INFO] - Training Epoch: 1/2, step 5565/7134 completed (loss: 0.016524523496627808, acc: 0.996927797794342)
[2025-02-13 03:15:26,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:27,218][root][INFO] - Training Epoch: 1/2, step 5566/7134 completed (loss: 0.022370127961039543, acc: 0.991963267326355)
[2025-02-13 03:15:27,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:27,686][root][INFO] - Training Epoch: 1/2, step 5567/7134 completed (loss: 0.03758471459150314, acc: 0.990867555141449)
[2025-02-13 03:15:27,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:28,084][root][INFO] - Training Epoch: 1/2, step 5568/7134 completed (loss: 0.03582989424467087, acc: 0.9909793734550476)
[2025-02-13 03:15:28,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:28,548][root][INFO] - Training Epoch: 1/2, step 5569/7134 completed (loss: 0.021755684167146683, acc: 0.9954338073730469)
[2025-02-13 03:15:28,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:28,984][root][INFO] - Training Epoch: 1/2, step 5570/7134 completed (loss: 0.0285347830504179, acc: 0.9937888383865356)
[2025-02-13 03:15:29,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:29,388][root][INFO] - Training Epoch: 1/2, step 5571/7134 completed (loss: 0.047648318111896515, acc: 0.9866270422935486)
[2025-02-13 03:15:29,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:29,803][root][INFO] - Training Epoch: 1/2, step 5572/7134 completed (loss: 0.06901133060455322, acc: 0.9862637519836426)
[2025-02-13 03:15:29,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:30,260][root][INFO] - Training Epoch: 1/2, step 5573/7134 completed (loss: 0.041546814143657684, acc: 0.9868995547294617)
[2025-02-13 03:15:30,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:30,652][root][INFO] - Training Epoch: 1/2, step 5574/7134 completed (loss: 0.01263359747827053, acc: 0.9968253970146179)
[2025-02-13 03:15:30,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:31,084][root][INFO] - Training Epoch: 1/2, step 5575/7134 completed (loss: 0.035075947642326355, acc: 0.9888357520103455)
[2025-02-13 03:15:31,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:31,485][root][INFO] - Training Epoch: 1/2, step 5576/7134 completed (loss: 0.03238435834646225, acc: 0.9813753366470337)
[2025-02-13 03:15:31,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:31,901][root][INFO] - Training Epoch: 1/2, step 5577/7134 completed (loss: 0.06745421886444092, acc: 0.9808027744293213)
[2025-02-13 03:15:32,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:32,333][root][INFO] - Training Epoch: 1/2, step 5578/7134 completed (loss: 0.03228657320141792, acc: 0.991391658782959)
[2025-02-13 03:15:32,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:32,768][root][INFO] - Training Epoch: 1/2, step 5579/7134 completed (loss: 0.043771661818027496, acc: 0.9834024906158447)
[2025-02-13 03:15:32,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:33,171][root][INFO] - Training Epoch: 1/2, step 5580/7134 completed (loss: 0.02131475694477558, acc: 0.9945429563522339)
[2025-02-13 03:15:33,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:33,603][root][INFO] - Training Epoch: 1/2, step 5581/7134 completed (loss: 0.02646680362522602, acc: 0.9880319237709045)
[2025-02-13 03:15:33,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:34,049][root][INFO] - Training Epoch: 1/2, step 5582/7134 completed (loss: 0.026193803176283836, acc: 0.9869621992111206)
[2025-02-13 03:15:34,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:34,496][root][INFO] - Training Epoch: 1/2, step 5583/7134 completed (loss: 0.02539125457406044, acc: 0.99370276927948)
[2025-02-13 03:15:34,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:34,910][root][INFO] - Training Epoch: 1/2, step 5584/7134 completed (loss: 0.027193354442715645, acc: 0.9926035404205322)
[2025-02-13 03:15:35,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:35,362][root][INFO] - Training Epoch: 1/2, step 5585/7134 completed (loss: 0.04520151764154434, acc: 0.9890909194946289)
[2025-02-13 03:15:35,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:35,744][root][INFO] - Training Epoch: 1/2, step 5586/7134 completed (loss: 0.03271278738975525, acc: 0.9893428087234497)
[2025-02-13 03:15:35,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:36,180][root][INFO] - Training Epoch: 1/2, step 5587/7134 completed (loss: 0.028514862060546875, acc: 0.9930651783943176)
[2025-02-13 03:15:36,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:36,620][root][INFO] - Training Epoch: 1/2, step 5588/7134 completed (loss: 0.01653692126274109, acc: 0.9954614043235779)
[2025-02-13 03:15:36,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:37,034][root][INFO] - Training Epoch: 1/2, step 5589/7134 completed (loss: 0.03765686973929405, acc: 0.9936908483505249)
[2025-02-13 03:15:37,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:37,436][root][INFO] - Training Epoch: 1/2, step 5590/7134 completed (loss: 0.051552921533584595, acc: 0.9873239398002625)
[2025-02-13 03:15:37,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:37,838][root][INFO] - Training Epoch: 1/2, step 5591/7134 completed (loss: 0.027501951903104782, acc: 0.9908376932144165)
[2025-02-13 03:15:37,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:38,237][root][INFO] - Training Epoch: 1/2, step 5592/7134 completed (loss: 0.02476510964334011, acc: 0.9931412935256958)
[2025-02-13 03:15:38,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:38,670][root][INFO] - Training Epoch: 1/2, step 5593/7134 completed (loss: 0.03873717784881592, acc: 0.9883419871330261)
[2025-02-13 03:15:38,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:39,098][root][INFO] - Training Epoch: 1/2, step 5594/7134 completed (loss: 0.015600881539285183, acc: 0.9946091771125793)
[2025-02-13 03:15:39,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:39,516][root][INFO] - Training Epoch: 1/2, step 5595/7134 completed (loss: 0.05966505780816078, acc: 0.9839141964912415)
[2025-02-13 03:15:39,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:39,923][root][INFO] - Training Epoch: 1/2, step 5596/7134 completed (loss: 0.019684089347720146, acc: 0.9932795763015747)
[2025-02-13 03:15:40,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:40,345][root][INFO] - Training Epoch: 1/2, step 5597/7134 completed (loss: 0.08476928621530533, acc: 0.9807162284851074)
[2025-02-13 03:15:40,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:40,759][root][INFO] - Training Epoch: 1/2, step 5598/7134 completed (loss: 0.03855103626847267, acc: 0.9874301552772522)
[2025-02-13 03:15:40,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:41,185][root][INFO] - Training Epoch: 1/2, step 5599/7134 completed (loss: 0.07399967312812805, acc: 0.9822404384613037)
[2025-02-13 03:15:41,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:41,626][root][INFO] - Training Epoch: 1/2, step 5600/7134 completed (loss: 0.02311078831553459, acc: 0.9942693114280701)
[2025-02-13 03:15:41,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:42,010][root][INFO] - Training Epoch: 1/2, step 5601/7134 completed (loss: 0.07448031008243561, acc: 0.9826086759567261)
[2025-02-13 03:15:42,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:42,418][root][INFO] - Training Epoch: 1/2, step 5602/7134 completed (loss: 0.07345705479383469, acc: 0.979066014289856)
[2025-02-13 03:15:42,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:42,776][root][INFO] - Training Epoch: 1/2, step 5603/7134 completed (loss: 0.016007399186491966, acc: 0.9934533834457397)
[2025-02-13 03:15:42,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:43,156][root][INFO] - Training Epoch: 1/2, step 5604/7134 completed (loss: 0.08585356175899506, acc: 0.9662379622459412)
[2025-02-13 03:15:43,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:43,546][root][INFO] - Training Epoch: 1/2, step 5605/7134 completed (loss: 0.04025721177458763, acc: 0.9864176511764526)
[2025-02-13 03:15:43,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:43,939][root][INFO] - Training Epoch: 1/2, step 5606/7134 completed (loss: 0.04570229351520538, acc: 0.9871794581413269)
[2025-02-13 03:15:44,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:44,335][root][INFO] - Training Epoch: 1/2, step 5607/7134 completed (loss: 0.029368428513407707, acc: 0.995488703250885)
[2025-02-13 03:15:44,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:44,731][root][INFO] - Training Epoch: 1/2, step 5608/7134 completed (loss: 0.037126634269952774, acc: 0.9885057210922241)
[2025-02-13 03:15:44,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:45,120][root][INFO] - Training Epoch: 1/2, step 5609/7134 completed (loss: 0.04736611992120743, acc: 0.9756521582603455)
[2025-02-13 03:15:45,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:45,480][root][INFO] - Training Epoch: 1/2, step 5610/7134 completed (loss: 0.033216893672943115, acc: 0.9899396300315857)
[2025-02-13 03:15:45,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:45,934][root][INFO] - Training Epoch: 1/2, step 5611/7134 completed (loss: 0.046430740505456924, acc: 0.982594907283783)
[2025-02-13 03:15:46,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:46,360][root][INFO] - Training Epoch: 1/2, step 5612/7134 completed (loss: 0.05322014540433884, acc: 0.9861963391304016)
[2025-02-13 03:15:46,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:46,755][root][INFO] - Training Epoch: 1/2, step 5613/7134 completed (loss: 0.0421728678047657, acc: 0.9874776601791382)
[2025-02-13 03:15:46,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:47,156][root][INFO] - Training Epoch: 1/2, step 5614/7134 completed (loss: 0.06061585620045662, acc: 0.9842857122421265)
[2025-02-13 03:15:47,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:47,580][root][INFO] - Training Epoch: 1/2, step 5615/7134 completed (loss: 0.06328731775283813, acc: 0.975095808506012)
[2025-02-13 03:15:47,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:47,980][root][INFO] - Training Epoch: 1/2, step 5616/7134 completed (loss: 0.03424181789159775, acc: 0.9881129264831543)
[2025-02-13 03:15:48,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:48,372][root][INFO] - Training Epoch: 1/2, step 5617/7134 completed (loss: 0.021078823134303093, acc: 0.9951298832893372)
[2025-02-13 03:15:48,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:48,790][root][INFO] - Training Epoch: 1/2, step 5618/7134 completed (loss: 0.06569557636976242, acc: 0.9788618087768555)
[2025-02-13 03:15:48,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:49,169][root][INFO] - Training Epoch: 1/2, step 5619/7134 completed (loss: 0.05345538631081581, acc: 0.9797160029411316)
[2025-02-13 03:15:49,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:49,564][root][INFO] - Training Epoch: 1/2, step 5620/7134 completed (loss: 0.040013544261455536, acc: 0.9831775426864624)
[2025-02-13 03:15:49,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:50,016][root][INFO] - Training Epoch: 1/2, step 5621/7134 completed (loss: 0.027096685022115707, acc: 0.9923760890960693)
[2025-02-13 03:15:50,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:50,439][root][INFO] - Training Epoch: 1/2, step 5622/7134 completed (loss: 0.06691373884677887, acc: 0.9850136041641235)
[2025-02-13 03:15:50,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:50,812][root][INFO] - Training Epoch: 1/2, step 5623/7134 completed (loss: 0.044958777725696564, acc: 0.9860627055168152)
[2025-02-13 03:15:50,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:51,226][root][INFO] - Training Epoch: 1/2, step 5624/7134 completed (loss: 0.04536012187600136, acc: 0.9885433912277222)
[2025-02-13 03:15:51,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:51,605][root][INFO] - Training Epoch: 1/2, step 5625/7134 completed (loss: 0.013924231752753258, acc: 0.9952940940856934)
[2025-02-13 03:15:51,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:52,003][root][INFO] - Training Epoch: 1/2, step 5626/7134 completed (loss: 0.027408141642808914, acc: 0.989847719669342)
[2025-02-13 03:15:52,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:52,407][root][INFO] - Training Epoch: 1/2, step 5627/7134 completed (loss: 0.13172197341918945, acc: 0.9684600830078125)
[2025-02-13 03:15:52,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:52,848][root][INFO] - Training Epoch: 1/2, step 5628/7134 completed (loss: 0.057168543338775635, acc: 0.9817850589752197)
[2025-02-13 03:15:52,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:53,267][root][INFO] - Training Epoch: 1/2, step 5629/7134 completed (loss: 0.06236174702644348, acc: 0.9789103865623474)
[2025-02-13 03:15:53,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:53,670][root][INFO] - Training Epoch: 1/2, step 5630/7134 completed (loss: 0.09733377397060394, acc: 0.9733059406280518)
[2025-02-13 03:15:53,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:54,069][root][INFO] - Training Epoch: 1/2, step 5631/7134 completed (loss: 0.08662474155426025, acc: 0.9734659790992737)
[2025-02-13 03:15:54,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:54,478][root][INFO] - Training Epoch: 1/2, step 5632/7134 completed (loss: 0.04347128048539162, acc: 0.9872958064079285)
[2025-02-13 03:15:54,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:54,877][root][INFO] - Training Epoch: 1/2, step 5633/7134 completed (loss: 0.0816681757569313, acc: 0.9690553545951843)
[2025-02-13 03:15:55,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:55,281][root][INFO] - Training Epoch: 1/2, step 5634/7134 completed (loss: 0.08755796402692795, acc: 0.9860383868217468)
[2025-02-13 03:15:55,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:55,691][root][INFO] - Training Epoch: 1/2, step 5635/7134 completed (loss: 0.02088673785328865, acc: 0.9966216087341309)
[2025-02-13 03:15:55,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:56,109][root][INFO] - Training Epoch: 1/2, step 5636/7134 completed (loss: 0.048006631433963776, acc: 0.9856230020523071)
[2025-02-13 03:15:56,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:56,507][root][INFO] - Training Epoch: 1/2, step 5637/7134 completed (loss: 0.06011838838458061, acc: 0.9814502596855164)
[2025-02-13 03:15:56,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:56,863][root][INFO] - Training Epoch: 1/2, step 5638/7134 completed (loss: 0.04487059265375137, acc: 0.990234375)
[2025-02-13 03:15:57,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:57,283][root][INFO] - Training Epoch: 1/2, step 5639/7134 completed (loss: 0.052129749208688736, acc: 0.9829059839248657)
[2025-02-13 03:15:57,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:57,672][root][INFO] - Training Epoch: 1/2, step 5640/7134 completed (loss: 0.06561967730522156, acc: 0.9837398529052734)
[2025-02-13 03:15:57,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:58,003][root][INFO] - Training Epoch: 1/2, step 5641/7134 completed (loss: 0.0682549700140953, acc: 0.9873737096786499)
[2025-02-13 03:15:58,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:58,390][root][INFO] - Training Epoch: 1/2, step 5642/7134 completed (loss: 0.0794534906744957, acc: 0.9803030490875244)
[2025-02-13 03:15:58,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:58,774][root][INFO] - Training Epoch: 1/2, step 5643/7134 completed (loss: 0.08157959580421448, acc: 0.9778761267662048)
[2025-02-13 03:15:58,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:59,192][root][INFO] - Training Epoch: 1/2, step 5644/7134 completed (loss: 0.05090692639350891, acc: 0.9857723712921143)
[2025-02-13 03:15:59,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:59,590][root][INFO] - Training Epoch: 1/2, step 5645/7134 completed (loss: 0.03882475569844246, acc: 0.9823269248008728)
[2025-02-13 03:15:59,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:59,986][root][INFO] - Training Epoch: 1/2, step 5646/7134 completed (loss: 0.04998347535729408, acc: 0.9824841022491455)
[2025-02-13 03:16:00,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:00,397][root][INFO] - Training Epoch: 1/2, step 5647/7134 completed (loss: 0.05266878008842468, acc: 0.9844961166381836)
[2025-02-13 03:16:00,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:00,812][root][INFO] - Training Epoch: 1/2, step 5648/7134 completed (loss: 0.108863465487957, acc: 0.9746268391609192)
[2025-02-13 03:16:00,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:01,202][root][INFO] - Training Epoch: 1/2, step 5649/7134 completed (loss: 0.07535839825868607, acc: 0.9816513657569885)
[2025-02-13 03:16:01,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:01,617][root][INFO] - Training Epoch: 1/2, step 5650/7134 completed (loss: 0.03943170607089996, acc: 0.9862778782844543)
[2025-02-13 03:16:01,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:02,036][root][INFO] - Training Epoch: 1/2, step 5651/7134 completed (loss: 0.029031815007328987, acc: 0.9926035404205322)
[2025-02-13 03:16:02,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:02,480][root][INFO] - Training Epoch: 1/2, step 5652/7134 completed (loss: 0.0456540547311306, acc: 0.9855769276618958)
[2025-02-13 03:16:02,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:02,805][root][INFO] - Training Epoch: 1/2, step 5653/7134 completed (loss: 0.08831027150154114, acc: 0.9765625)
[2025-02-13 03:16:02,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:03,206][root][INFO] - Training Epoch: 1/2, step 5654/7134 completed (loss: 0.03627414256334305, acc: 0.9924924969673157)
[2025-02-13 03:16:03,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:03,616][root][INFO] - Training Epoch: 1/2, step 5655/7134 completed (loss: 0.07230450958013535, acc: 0.9795918464660645)
[2025-02-13 03:16:03,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:04,002][root][INFO] - Training Epoch: 1/2, step 5656/7134 completed (loss: 0.026586180552840233, acc: 0.9923809766769409)
[2025-02-13 03:16:04,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:04,401][root][INFO] - Training Epoch: 1/2, step 5657/7134 completed (loss: 0.05356570705771446, acc: 0.9926793575286865)
[2025-02-13 03:16:04,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:04,838][root][INFO] - Training Epoch: 1/2, step 5658/7134 completed (loss: 0.060344357043504715, acc: 0.980663001537323)
[2025-02-13 03:16:04,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:05,246][root][INFO] - Training Epoch: 1/2, step 5659/7134 completed (loss: 0.05963582172989845, acc: 0.9886524677276611)
[2025-02-13 03:16:05,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:05,653][root][INFO] - Training Epoch: 1/2, step 5660/7134 completed (loss: 0.029199881479144096, acc: 0.9917355179786682)
[2025-02-13 03:16:05,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:06,091][root][INFO] - Training Epoch: 1/2, step 5661/7134 completed (loss: 0.015115189366042614, acc: 0.9963503479957581)
[2025-02-13 03:16:06,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:06,513][root][INFO] - Training Epoch: 1/2, step 5662/7134 completed (loss: 0.028929948806762695, acc: 0.9967948794364929)
[2025-02-13 03:16:06,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:06,910][root][INFO] - Training Epoch: 1/2, step 5663/7134 completed (loss: 0.025647014379501343, acc: 0.990755021572113)
[2025-02-13 03:16:07,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:07,309][root][INFO] - Training Epoch: 1/2, step 5664/7134 completed (loss: 0.08936601877212524, acc: 0.9821693897247314)
[2025-02-13 03:16:07,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:07,718][root][INFO] - Training Epoch: 1/2, step 5665/7134 completed (loss: 0.027772244065999985, acc: 0.9900142550468445)
[2025-02-13 03:16:07,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:08,151][root][INFO] - Training Epoch: 1/2, step 5666/7134 completed (loss: 0.015597554855048656, acc: 0.9944827556610107)
[2025-02-13 03:16:08,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:08,533][root][INFO] - Training Epoch: 1/2, step 5667/7134 completed (loss: 0.03130721300840378, acc: 0.996363639831543)
[2025-02-13 03:16:08,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:08,926][root][INFO] - Training Epoch: 1/2, step 5668/7134 completed (loss: 0.06521718204021454, acc: 0.9817276000976562)
[2025-02-13 03:16:09,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:09,329][root][INFO] - Training Epoch: 1/2, step 5669/7134 completed (loss: 0.04216456785798073, acc: 0.9912023544311523)
[2025-02-13 03:16:09,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:09,737][root][INFO] - Training Epoch: 1/2, step 5670/7134 completed (loss: 0.0332488976418972, acc: 0.9866666793823242)
[2025-02-13 03:16:09,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:10,190][root][INFO] - Training Epoch: 1/2, step 5671/7134 completed (loss: 0.017859626561403275, acc: 0.9938499331474304)
[2025-02-13 03:16:10,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:10,633][root][INFO] - Training Epoch: 1/2, step 5672/7134 completed (loss: 0.01961752213537693, acc: 0.9905362725257874)
[2025-02-13 03:16:10,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:11,010][root][INFO] - Training Epoch: 1/2, step 5673/7134 completed (loss: 0.019332120195031166, acc: 0.9940915703773499)
[2025-02-13 03:16:11,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:11,450][root][INFO] - Training Epoch: 1/2, step 5674/7134 completed (loss: 0.021913740783929825, acc: 0.9923547506332397)
[2025-02-13 03:16:11,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:11,846][root][INFO] - Training Epoch: 1/2, step 5675/7134 completed (loss: 0.04634382203221321, acc: 0.9834162592887878)
[2025-02-13 03:16:11,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:12,257][root][INFO] - Training Epoch: 1/2, step 5676/7134 completed (loss: 0.09396666288375854, acc: 0.976331353187561)
[2025-02-13 03:16:12,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:12,670][root][INFO] - Training Epoch: 1/2, step 5677/7134 completed (loss: 0.053510989993810654, acc: 0.9857954382896423)
[2025-02-13 03:16:12,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:13,078][root][INFO] - Training Epoch: 1/2, step 5678/7134 completed (loss: 0.04074101522564888, acc: 0.9897058606147766)
[2025-02-13 03:16:13,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:13,522][root][INFO] - Training Epoch: 1/2, step 5679/7134 completed (loss: 0.09764644503593445, acc: 0.9721815586090088)
[2025-02-13 03:16:13,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:13,935][root][INFO] - Training Epoch: 1/2, step 5680/7134 completed (loss: 0.08729352802038193, acc: 0.9687034487724304)
[2025-02-13 03:16:14,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:14,353][root][INFO] - Training Epoch: 1/2, step 5681/7134 completed (loss: 0.058449722826480865, acc: 0.9758672714233398)
[2025-02-13 03:16:14,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:14,764][root][INFO] - Training Epoch: 1/2, step 5682/7134 completed (loss: 0.03987948223948479, acc: 0.9855700135231018)
[2025-02-13 03:16:14,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:15,185][root][INFO] - Training Epoch: 1/2, step 5683/7134 completed (loss: 0.0715254619717598, acc: 0.9765739440917969)
[2025-02-13 03:16:15,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:15,636][root][INFO] - Training Epoch: 1/2, step 5684/7134 completed (loss: 0.07022014260292053, acc: 0.9831365942955017)
[2025-02-13 03:16:15,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:16,037][root][INFO] - Training Epoch: 1/2, step 5685/7134 completed (loss: 0.07359760999679565, acc: 0.9797794222831726)
[2025-02-13 03:16:16,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:16,455][root][INFO] - Training Epoch: 1/2, step 5686/7134 completed (loss: 0.07500676065683365, acc: 0.9746192693710327)
[2025-02-13 03:16:16,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:16,830][root][INFO] - Training Epoch: 1/2, step 5687/7134 completed (loss: 0.06503962725400925, acc: 0.9798534512519836)
[2025-02-13 03:16:16,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:17,268][root][INFO] - Training Epoch: 1/2, step 5688/7134 completed (loss: 0.09491255134344101, acc: 0.9773030877113342)
[2025-02-13 03:16:17,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:17,699][root][INFO] - Training Epoch: 1/2, step 5689/7134 completed (loss: 0.07814296334981918, acc: 0.9817351698875427)
[2025-02-13 03:16:17,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:18,160][root][INFO] - Training Epoch: 1/2, step 5690/7134 completed (loss: 0.04425618052482605, acc: 0.9905660152435303)
[2025-02-13 03:16:18,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:18,564][root][INFO] - Training Epoch: 1/2, step 5691/7134 completed (loss: 0.09236442297697067, acc: 0.9763975143432617)
[2025-02-13 03:16:18,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:19,015][root][INFO] - Training Epoch: 1/2, step 5692/7134 completed (loss: 0.059691671282052994, acc: 0.9865951538085938)
[2025-02-13 03:16:19,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:19,419][root][INFO] - Training Epoch: 1/2, step 5693/7134 completed (loss: 0.02454272098839283, acc: 0.9895052313804626)
[2025-02-13 03:16:19,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:19,812][root][INFO] - Training Epoch: 1/2, step 5694/7134 completed (loss: 0.05241921916604042, acc: 0.986975371837616)
[2025-02-13 03:16:19,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:20,248][root][INFO] - Training Epoch: 1/2, step 5695/7134 completed (loss: 0.04229829087853432, acc: 0.9885386824607849)
[2025-02-13 03:16:20,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:20,687][root][INFO] - Training Epoch: 1/2, step 5696/7134 completed (loss: 0.039947863668203354, acc: 0.9882352948188782)
[2025-02-13 03:16:20,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:21,118][root][INFO] - Training Epoch: 1/2, step 5697/7134 completed (loss: 0.058601830154657364, acc: 0.9887164831161499)
[2025-02-13 03:16:21,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:21,546][root][INFO] - Training Epoch: 1/2, step 5698/7134 completed (loss: 0.046508025377988815, acc: 0.9904761910438538)
[2025-02-13 03:16:21,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:21,990][root][INFO] - Training Epoch: 1/2, step 5699/7134 completed (loss: 0.0451960414648056, acc: 0.987261176109314)
[2025-02-13 03:16:22,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:22,426][root][INFO] - Training Epoch: 1/2, step 5700/7134 completed (loss: 0.031219355762004852, acc: 0.9872340559959412)
[2025-02-13 03:16:22,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:22,890][root][INFO] - Training Epoch: 1/2, step 5701/7134 completed (loss: 0.058158788830041885, acc: 0.9899874925613403)
[2025-02-13 03:16:23,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:23,320][root][INFO] - Training Epoch: 1/2, step 5702/7134 completed (loss: 0.054920315742492676, acc: 0.9851149916648865)
[2025-02-13 03:16:23,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:23,752][root][INFO] - Training Epoch: 1/2, step 5703/7134 completed (loss: 0.026405174285173416, acc: 0.9887482523918152)
[2025-02-13 03:16:23,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:24,154][root][INFO] - Training Epoch: 1/2, step 5704/7134 completed (loss: 0.06759967654943466, acc: 0.980028510093689)
[2025-02-13 03:16:24,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:24,412][root][INFO] - Training Epoch: 1/2, step 5705/7134 completed (loss: 0.0186091847717762, acc: 0.9928571581840515)
[2025-02-13 03:16:24,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:24,878][root][INFO] - Training Epoch: 1/2, step 5706/7134 completed (loss: 0.02394142374396324, acc: 0.9932885766029358)
[2025-02-13 03:16:25,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:25,278][root][INFO] - Training Epoch: 1/2, step 5707/7134 completed (loss: 0.06128757819533348, acc: 0.9806337952613831)
[2025-02-13 03:16:25,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:25,684][root][INFO] - Training Epoch: 1/2, step 5708/7134 completed (loss: 0.03291155397891998, acc: 0.9911110997200012)
[2025-02-13 03:16:25,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:26,109][root][INFO] - Training Epoch: 1/2, step 5709/7134 completed (loss: 0.06822030991315842, acc: 0.9863201379776001)
[2025-02-13 03:16:26,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:26,549][root][INFO] - Training Epoch: 1/2, step 5710/7134 completed (loss: 0.044149354100227356, acc: 0.9872773289680481)
[2025-02-13 03:16:26,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:26,959][root][INFO] - Training Epoch: 1/2, step 5711/7134 completed (loss: 0.05118197947740555, acc: 0.9842932224273682)
[2025-02-13 03:16:27,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:27,385][root][INFO] - Training Epoch: 1/2, step 5712/7134 completed (loss: 0.057617198675870895, acc: 0.984000027179718)
[2025-02-13 03:16:27,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:27,845][root][INFO] - Training Epoch: 1/2, step 5713/7134 completed (loss: 0.026406824588775635, acc: 0.9936467409133911)
[2025-02-13 03:16:27,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:28,287][root][INFO] - Training Epoch: 1/2, step 5714/7134 completed (loss: 0.028497306630015373, acc: 0.993773341178894)
[2025-02-13 03:16:28,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:28,686][root][INFO] - Training Epoch: 1/2, step 5715/7134 completed (loss: 0.07808144390583038, acc: 0.9832402467727661)
[2025-02-13 03:16:28,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:29,082][root][INFO] - Training Epoch: 1/2, step 5716/7134 completed (loss: 0.13734374940395355, acc: 0.9751332402229309)
[2025-02-13 03:16:29,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:29,483][root][INFO] - Training Epoch: 1/2, step 5717/7134 completed (loss: 0.11453460901975632, acc: 0.9751166701316833)
[2025-02-13 03:16:29,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:29,870][root][INFO] - Training Epoch: 1/2, step 5718/7134 completed (loss: 0.06796237081289291, acc: 0.9826498627662659)
[2025-02-13 03:16:29,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:30,269][root][INFO] - Training Epoch: 1/2, step 5719/7134 completed (loss: 0.03869813680648804, acc: 0.9895052313804626)
[2025-02-13 03:16:30,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:30,668][root][INFO] - Training Epoch: 1/2, step 5720/7134 completed (loss: 0.034441571682691574, acc: 0.9879102110862732)
[2025-02-13 03:16:30,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:31,071][root][INFO] - Training Epoch: 1/2, step 5721/7134 completed (loss: 0.043478645384311676, acc: 0.9855263233184814)
[2025-02-13 03:16:31,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:31,408][root][INFO] - Training Epoch: 1/2, step 5722/7134 completed (loss: 0.04963354766368866, acc: 0.9866962432861328)
[2025-02-13 03:16:31,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:31,830][root][INFO] - Training Epoch: 1/2, step 5723/7134 completed (loss: 0.06489971280097961, acc: 0.97579425573349)
[2025-02-13 03:16:31,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:32,221][root][INFO] - Training Epoch: 1/2, step 5724/7134 completed (loss: 0.03090803511440754, acc: 0.9872000217437744)
[2025-02-13 03:16:32,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:32,650][root][INFO] - Training Epoch: 1/2, step 5725/7134 completed (loss: 0.03590681776404381, acc: 0.9928571581840515)
[2025-02-13 03:16:32,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:33,048][root][INFO] - Training Epoch: 1/2, step 5726/7134 completed (loss: 0.06048860028386116, acc: 0.9847095012664795)
[2025-02-13 03:16:33,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:33,457][root][INFO] - Training Epoch: 1/2, step 5727/7134 completed (loss: 0.020838715136051178, acc: 0.9946523904800415)
[2025-02-13 03:16:33,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:33,832][root][INFO] - Training Epoch: 1/2, step 5728/7134 completed (loss: 0.014778384938836098, acc: 1.0)
[2025-02-13 03:16:33,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:34,217][root][INFO] - Training Epoch: 1/2, step 5729/7134 completed (loss: 0.03375595062971115, acc: 0.9883333444595337)
[2025-02-13 03:16:34,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:34,604][root][INFO] - Training Epoch: 1/2, step 5730/7134 completed (loss: 0.08259841799736023, acc: 0.9797979593276978)
[2025-02-13 03:16:34,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:35,035][root][INFO] - Training Epoch: 1/2, step 5731/7134 completed (loss: 0.056846391409635544, acc: 0.9833887219429016)
[2025-02-13 03:16:35,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:35,427][root][INFO] - Training Epoch: 1/2, step 5732/7134 completed (loss: 0.1456104964017868, acc: 0.9533980488777161)
[2025-02-13 03:16:35,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:35,828][root][INFO] - Training Epoch: 1/2, step 5733/7134 completed (loss: 0.03390178829431534, acc: 0.9890710115432739)
[2025-02-13 03:16:35,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:36,231][root][INFO] - Training Epoch: 1/2, step 5734/7134 completed (loss: 0.01675134338438511, acc: 0.9950000047683716)
[2025-02-13 03:16:36,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:36,627][root][INFO] - Training Epoch: 1/2, step 5735/7134 completed (loss: 0.0648961216211319, acc: 0.9784946441650391)
[2025-02-13 03:16:36,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:37,077][root][INFO] - Training Epoch: 1/2, step 5736/7134 completed (loss: 0.07847356051206589, acc: 0.9774436354637146)
[2025-02-13 03:16:37,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:37,489][root][INFO] - Training Epoch: 1/2, step 5737/7134 completed (loss: 0.05433934926986694, acc: 0.981873095035553)
[2025-02-13 03:16:37,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:37,933][root][INFO] - Training Epoch: 1/2, step 5738/7134 completed (loss: 0.055567216128110886, acc: 0.9794437885284424)
[2025-02-13 03:16:38,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:38,356][root][INFO] - Training Epoch: 1/2, step 5739/7134 completed (loss: 0.08369651436805725, acc: 0.9684210419654846)
[2025-02-13 03:16:38,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:38,769][root][INFO] - Training Epoch: 1/2, step 5740/7134 completed (loss: 0.037069790065288544, acc: 0.9846547245979309)
[2025-02-13 03:16:38,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:39,189][root][INFO] - Training Epoch: 1/2, step 5741/7134 completed (loss: 0.049839507788419724, acc: 0.9884467124938965)
[2025-02-13 03:16:39,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:39,602][root][INFO] - Training Epoch: 1/2, step 5742/7134 completed (loss: 0.05329141020774841, acc: 0.9829171895980835)
[2025-02-13 03:16:39,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:40,047][root][INFO] - Training Epoch: 1/2, step 5743/7134 completed (loss: 0.03701939806342125, acc: 0.9877451062202454)
[2025-02-13 03:16:40,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:40,414][root][INFO] - Training Epoch: 1/2, step 5744/7134 completed (loss: 0.028885092586278915, acc: 0.9895397424697876)
[2025-02-13 03:16:40,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:40,855][root][INFO] - Training Epoch: 1/2, step 5745/7134 completed (loss: 0.035010453313589096, acc: 0.9870466589927673)
[2025-02-13 03:16:40,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:41,248][root][INFO] - Training Epoch: 1/2, step 5746/7134 completed (loss: 0.06543245166540146, acc: 0.9897058606147766)
[2025-02-13 03:16:41,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:41,662][root][INFO] - Training Epoch: 1/2, step 5747/7134 completed (loss: 0.026996657252311707, acc: 0.990304708480835)
[2025-02-13 03:16:41,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:42,104][root][INFO] - Training Epoch: 1/2, step 5748/7134 completed (loss: 0.036013081669807434, acc: 0.9872029423713684)
[2025-02-13 03:16:42,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:42,544][root][INFO] - Training Epoch: 1/2, step 5749/7134 completed (loss: 0.03283362463116646, acc: 0.9908376932144165)
[2025-02-13 03:16:42,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:42,965][root][INFO] - Training Epoch: 1/2, step 5750/7134 completed (loss: 0.06423307955265045, acc: 0.9777227640151978)
[2025-02-13 03:16:43,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:43,382][root][INFO] - Training Epoch: 1/2, step 5751/7134 completed (loss: 0.06135676056146622, acc: 0.9807445406913757)
[2025-02-13 03:16:43,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:43,799][root][INFO] - Training Epoch: 1/2, step 5752/7134 completed (loss: 0.075043685734272, acc: 0.9837586879730225)
[2025-02-13 03:16:43,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:44,199][root][INFO] - Training Epoch: 1/2, step 5753/7134 completed (loss: 0.08984556049108505, acc: 0.971875011920929)
[2025-02-13 03:16:44,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:44,663][root][INFO] - Training Epoch: 1/2, step 5754/7134 completed (loss: 0.06154189258813858, acc: 0.9821656346321106)
[2025-02-13 03:16:44,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:45,077][root][INFO] - Training Epoch: 1/2, step 5755/7134 completed (loss: 0.055811021476984024, acc: 0.985358715057373)
[2025-02-13 03:16:45,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:45,486][root][INFO] - Training Epoch: 1/2, step 5756/7134 completed (loss: 0.05907580628991127, acc: 0.9827127456665039)
[2025-02-13 03:16:45,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:45,925][root][INFO] - Training Epoch: 1/2, step 5757/7134 completed (loss: 0.030821330845355988, acc: 0.9910600185394287)
[2025-02-13 03:16:46,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:46,305][root][INFO] - Training Epoch: 1/2, step 5758/7134 completed (loss: 0.0381610244512558, acc: 0.9867647290229797)
[2025-02-13 03:16:46,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:46,670][root][INFO] - Training Epoch: 1/2, step 5759/7134 completed (loss: 0.05749265104532242, acc: 0.9811320900917053)
[2025-02-13 03:16:46,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:47,097][root][INFO] - Training Epoch: 1/2, step 5760/7134 completed (loss: 0.0359334722161293, acc: 0.9923664331436157)
[2025-02-13 03:16:47,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:47,541][root][INFO] - Training Epoch: 1/2, step 5761/7134 completed (loss: 0.04294514283537865, acc: 0.9816849827766418)
[2025-02-13 03:16:47,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:47,946][root][INFO] - Training Epoch: 1/2, step 5762/7134 completed (loss: 0.0540316142141819, acc: 0.985401451587677)
[2025-02-13 03:16:48,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:48,376][root][INFO] - Training Epoch: 1/2, step 5763/7134 completed (loss: 0.04318348318338394, acc: 0.9887076616287231)
[2025-02-13 03:16:48,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:48,755][root][INFO] - Training Epoch: 1/2, step 5764/7134 completed (loss: 0.037753764539957047, acc: 0.9897435903549194)
[2025-02-13 03:16:48,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:49,174][root][INFO] - Training Epoch: 1/2, step 5765/7134 completed (loss: 0.020440639927983284, acc: 0.9957982897758484)
[2025-02-13 03:16:49,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:49,588][root][INFO] - Training Epoch: 1/2, step 5766/7134 completed (loss: 0.03470991179347038, acc: 0.9955157041549683)
[2025-02-13 03:16:49,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:49,992][root][INFO] - Training Epoch: 1/2, step 5767/7134 completed (loss: 0.01270454004406929, acc: 0.9972183704376221)
[2025-02-13 03:16:50,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:50,416][root][INFO] - Training Epoch: 1/2, step 5768/7134 completed (loss: 0.02325417473912239, acc: 0.9910179376602173)
[2025-02-13 03:16:50,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:50,855][root][INFO] - Training Epoch: 1/2, step 5769/7134 completed (loss: 0.009211462922394276, acc: 0.9967266917228699)
[2025-02-13 03:16:51,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:51,299][root][INFO] - Training Epoch: 1/2, step 5770/7134 completed (loss: 0.04854113236069679, acc: 0.9877601265907288)
[2025-02-13 03:16:51,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:51,716][root][INFO] - Training Epoch: 1/2, step 5771/7134 completed (loss: 0.0514361597597599, acc: 0.98777174949646)
[2025-02-13 03:16:51,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:52,146][root][INFO] - Training Epoch: 1/2, step 5772/7134 completed (loss: 0.02184358984231949, acc: 0.991304337978363)
[2025-02-13 03:16:52,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:52,581][root][INFO] - Training Epoch: 1/2, step 5773/7134 completed (loss: 0.01634409837424755, acc: 0.9933110475540161)
[2025-02-13 03:16:52,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:52,972][root][INFO] - Training Epoch: 1/2, step 5774/7134 completed (loss: 0.036877017468214035, acc: 0.9836795330047607)
[2025-02-13 03:16:53,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:53,369][root][INFO] - Training Epoch: 1/2, step 5775/7134 completed (loss: 0.028423164039850235, acc: 0.9917627573013306)
[2025-02-13 03:16:53,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:53,782][root][INFO] - Training Epoch: 1/2, step 5776/7134 completed (loss: 0.04839107394218445, acc: 0.983146071434021)
[2025-02-13 03:16:53,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:54,222][root][INFO] - Training Epoch: 1/2, step 5777/7134 completed (loss: 0.07679414004087448, acc: 0.9814356565475464)
[2025-02-13 03:16:54,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:54,659][root][INFO] - Training Epoch: 1/2, step 5778/7134 completed (loss: 0.08293698728084564, acc: 0.9759615659713745)
[2025-02-13 03:16:54,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:55,094][root][INFO] - Training Epoch: 1/2, step 5779/7134 completed (loss: 0.058846935629844666, acc: 0.9889958500862122)
[2025-02-13 03:16:55,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:55,546][root][INFO] - Training Epoch: 1/2, step 5780/7134 completed (loss: 0.03263293579220772, acc: 0.9904534816741943)
[2025-02-13 03:16:55,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:55,974][root][INFO] - Training Epoch: 1/2, step 5781/7134 completed (loss: 0.05632070079445839, acc: 0.9820442199707031)
[2025-02-13 03:16:56,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:56,411][root][INFO] - Training Epoch: 1/2, step 5782/7134 completed (loss: 0.015925345942378044, acc: 0.9935232996940613)
[2025-02-13 03:16:56,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:56,849][root][INFO] - Training Epoch: 1/2, step 5783/7134 completed (loss: 0.045185014605522156, acc: 0.9893758296966553)
[2025-02-13 03:16:56,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:57,263][root][INFO] - Training Epoch: 1/2, step 5784/7134 completed (loss: 0.04190885275602341, acc: 0.9878869652748108)
[2025-02-13 03:16:57,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:57,669][root][INFO] - Training Epoch: 1/2, step 5785/7134 completed (loss: 0.05000438913702965, acc: 0.986522912979126)
[2025-02-13 03:16:57,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:58,061][root][INFO] - Training Epoch: 1/2, step 5786/7134 completed (loss: 0.06914598494768143, acc: 0.9754098653793335)
[2025-02-13 03:16:58,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:58,492][root][INFO] - Training Epoch: 1/2, step 5787/7134 completed (loss: 0.03307976573705673, acc: 0.9888059496879578)
[2025-02-13 03:16:58,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:58,884][root][INFO] - Training Epoch: 1/2, step 5788/7134 completed (loss: 0.01758950762450695, acc: 0.9953846335411072)
[2025-02-13 03:16:59,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:59,323][root][INFO] - Training Epoch: 1/2, step 5789/7134 completed (loss: 0.05815199390053749, acc: 0.9826302528381348)
[2025-02-13 03:16:59,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:59,756][root][INFO] - Training Epoch: 1/2, step 5790/7134 completed (loss: 0.040296100080013275, acc: 0.9848901033401489)
[2025-02-13 03:16:59,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:00,180][root][INFO] - Training Epoch: 1/2, step 5791/7134 completed (loss: 0.0643114373087883, acc: 0.9793814420700073)
[2025-02-13 03:17:00,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:00,600][root][INFO] - Training Epoch: 1/2, step 5792/7134 completed (loss: 0.06422362476587296, acc: 0.9813242554664612)
[2025-02-13 03:17:00,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:01,008][root][INFO] - Training Epoch: 1/2, step 5793/7134 completed (loss: 0.06975609064102173, acc: 0.9811320900917053)
[2025-02-13 03:17:01,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:01,410][root][INFO] - Training Epoch: 1/2, step 5794/7134 completed (loss: 0.06239014118909836, acc: 0.9818181991577148)
[2025-02-13 03:17:01,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:01,813][root][INFO] - Training Epoch: 1/2, step 5795/7134 completed (loss: 0.1095464900135994, acc: 0.9726890921592712)
[2025-02-13 03:17:01,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:02,219][root][INFO] - Training Epoch: 1/2, step 5796/7134 completed (loss: 0.04175999015569687, acc: 0.9864636063575745)
[2025-02-13 03:17:02,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:02,626][root][INFO] - Training Epoch: 1/2, step 5797/7134 completed (loss: 0.047442201524972916, acc: 0.9824000000953674)
[2025-02-13 03:17:02,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:03,025][root][INFO] - Training Epoch: 1/2, step 5798/7134 completed (loss: 0.17979350686073303, acc: 0.9617391228675842)
[2025-02-13 03:17:03,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:03,418][root][INFO] - Training Epoch: 1/2, step 5799/7134 completed (loss: 0.0866844654083252, acc: 0.9723865985870361)
[2025-02-13 03:17:03,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:03,820][root][INFO] - Training Epoch: 1/2, step 5800/7134 completed (loss: 0.07994382083415985, acc: 0.9724264740943909)
[2025-02-13 03:17:03,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:04,239][root][INFO] - Training Epoch: 1/2, step 5801/7134 completed (loss: 0.06707712262868881, acc: 0.9817073345184326)
[2025-02-13 03:17:04,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:04,645][root][INFO] - Training Epoch: 1/2, step 5802/7134 completed (loss: 0.042758382856845856, acc: 0.9907578825950623)
[2025-02-13 03:17:04,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:05,089][root][INFO] - Training Epoch: 1/2, step 5803/7134 completed (loss: 0.0277805645018816, acc: 0.9936224222183228)
[2025-02-13 03:17:05,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:05,502][root][INFO] - Training Epoch: 1/2, step 5804/7134 completed (loss: 0.10378985106945038, acc: 0.9675236940383911)
[2025-02-13 03:17:05,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:05,919][root][INFO] - Training Epoch: 1/2, step 5805/7134 completed (loss: 0.05254806578159332, acc: 0.9856114983558655)
[2025-02-13 03:17:06,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:06,346][root][INFO] - Training Epoch: 1/2, step 5806/7134 completed (loss: 0.05395353212952614, acc: 0.9877049326896667)
[2025-02-13 03:17:06,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:06,762][root][INFO] - Training Epoch: 1/2, step 5807/7134 completed (loss: 0.07451792806386948, acc: 0.97579425573349)
[2025-02-13 03:17:06,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:07,208][root][INFO] - Training Epoch: 1/2, step 5808/7134 completed (loss: 0.038019560277462006, acc: 0.9887482523918152)
[2025-02-13 03:17:07,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:07,602][root][INFO] - Training Epoch: 1/2, step 5809/7134 completed (loss: 0.049552179872989655, acc: 0.9912280440330505)
[2025-02-13 03:17:07,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:08,016][root][INFO] - Training Epoch: 1/2, step 5810/7134 completed (loss: 0.04089798033237457, acc: 0.9898580312728882)
[2025-02-13 03:17:08,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:08,322][root][INFO] - Training Epoch: 1/2, step 5811/7134 completed (loss: 0.05174550786614418, acc: 0.983132541179657)
[2025-02-13 03:17:08,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:08,706][root][INFO] - Training Epoch: 1/2, step 5812/7134 completed (loss: 0.014681403525173664, acc: 0.9966996908187866)
[2025-02-13 03:17:08,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:09,099][root][INFO] - Training Epoch: 1/2, step 5813/7134 completed (loss: 0.027491046115756035, acc: 0.99245285987854)
[2025-02-13 03:17:09,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:09,558][root][INFO] - Training Epoch: 1/2, step 5814/7134 completed (loss: 0.028039926663041115, acc: 0.9932318329811096)
[2025-02-13 03:17:09,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:09,967][root][INFO] - Training Epoch: 1/2, step 5815/7134 completed (loss: 0.04710446298122406, acc: 0.9849340915679932)
[2025-02-13 03:17:10,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:10,401][root][INFO] - Training Epoch: 1/2, step 5816/7134 completed (loss: 0.01155104674398899, acc: 0.9950124621391296)
[2025-02-13 03:17:10,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:10,801][root][INFO] - Training Epoch: 1/2, step 5817/7134 completed (loss: 0.0787765234708786, acc: 0.984886646270752)
[2025-02-13 03:17:10,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:11,212][root][INFO] - Training Epoch: 1/2, step 5818/7134 completed (loss: 0.045546483248472214, acc: 0.9851852059364319)
[2025-02-13 03:17:11,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:11,600][root][INFO] - Training Epoch: 1/2, step 5819/7134 completed (loss: 0.04969332367181778, acc: 0.9844827651977539)
[2025-02-13 03:17:11,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:11,970][root][INFO] - Training Epoch: 1/2, step 5820/7134 completed (loss: 0.05822475627064705, acc: 0.9876033067703247)
[2025-02-13 03:17:12,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:12,360][root][INFO] - Training Epoch: 1/2, step 5821/7134 completed (loss: 0.0118101816624403, acc: 0.9969183206558228)
[2025-02-13 03:17:12,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:12,762][root][INFO] - Training Epoch: 1/2, step 5822/7134 completed (loss: 0.035143349319696426, acc: 0.9887096881866455)
[2025-02-13 03:17:12,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:13,136][root][INFO] - Training Epoch: 1/2, step 5823/7134 completed (loss: 0.030087558552622795, acc: 0.9922239780426025)
[2025-02-13 03:17:13,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:13,559][root][INFO] - Training Epoch: 1/2, step 5824/7134 completed (loss: 0.023914175108075142, acc: 0.996889591217041)
[2025-02-13 03:17:13,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:13,975][root][INFO] - Training Epoch: 1/2, step 5825/7134 completed (loss: 0.04748491942882538, acc: 0.9861591458320618)
[2025-02-13 03:17:14,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:14,386][root][INFO] - Training Epoch: 1/2, step 5826/7134 completed (loss: 0.021228469908237457, acc: 0.9946808218955994)
[2025-02-13 03:17:14,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:14,772][root][INFO] - Training Epoch: 1/2, step 5827/7134 completed (loss: 0.025775332003831863, acc: 0.9923076629638672)
[2025-02-13 03:17:14,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:15,183][root][INFO] - Training Epoch: 1/2, step 5828/7134 completed (loss: 0.04810234159231186, acc: 0.9851411581039429)
[2025-02-13 03:17:15,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:15,605][root][INFO] - Training Epoch: 1/2, step 5829/7134 completed (loss: 0.04615858569741249, acc: 0.9894598126411438)
[2025-02-13 03:17:15,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:15,975][root][INFO] - Training Epoch: 1/2, step 5830/7134 completed (loss: 0.03765286132693291, acc: 0.9867674708366394)
[2025-02-13 03:17:16,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:16,357][root][INFO] - Training Epoch: 1/2, step 5831/7134 completed (loss: 0.019000336527824402, acc: 0.9953488111495972)
[2025-02-13 03:17:16,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:16,774][root][INFO] - Training Epoch: 1/2, step 5832/7134 completed (loss: 0.052531324326992035, acc: 0.9881756901741028)
[2025-02-13 03:17:16,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:17,149][root][INFO] - Training Epoch: 1/2, step 5833/7134 completed (loss: 0.04125778004527092, acc: 0.9874551892280579)
[2025-02-13 03:17:17,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:17,563][root][INFO] - Training Epoch: 1/2, step 5834/7134 completed (loss: 0.029246190562844276, acc: 0.9958333373069763)
[2025-02-13 03:17:17,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:17,974][root][INFO] - Training Epoch: 1/2, step 5835/7134 completed (loss: 0.012854553759098053, acc: 0.9956647157669067)
[2025-02-13 03:17:18,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:18,390][root][INFO] - Training Epoch: 1/2, step 5836/7134 completed (loss: 0.040493205189704895, acc: 0.9899135231971741)
[2025-02-13 03:17:18,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:18,795][root][INFO] - Training Epoch: 1/2, step 5837/7134 completed (loss: 0.08402708917856216, acc: 0.9814189076423645)
[2025-02-13 03:17:18,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:19,211][root][INFO] - Training Epoch: 1/2, step 5838/7134 completed (loss: 0.05234117433428764, acc: 0.9917627573013306)
[2025-02-13 03:17:19,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:19,615][root][INFO] - Training Epoch: 1/2, step 5839/7134 completed (loss: 0.02216644398868084, acc: 0.9946428537368774)
[2025-02-13 03:17:19,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:20,012][root][INFO] - Training Epoch: 1/2, step 5840/7134 completed (loss: 0.023638855665922165, acc: 0.9920508861541748)
[2025-02-13 03:17:20,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:20,419][root][INFO] - Training Epoch: 1/2, step 5841/7134 completed (loss: 0.05780065804719925, acc: 0.9872408509254456)
[2025-02-13 03:17:20,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:20,818][root][INFO] - Training Epoch: 1/2, step 5842/7134 completed (loss: 0.044682055711746216, acc: 0.9855072498321533)
[2025-02-13 03:17:20,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:21,248][root][INFO] - Training Epoch: 1/2, step 5843/7134 completed (loss: 0.049118030816316605, acc: 0.9866130948066711)
[2025-02-13 03:17:21,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:21,668][root][INFO] - Training Epoch: 1/2, step 5844/7134 completed (loss: 0.024092890322208405, acc: 0.9943100810050964)
[2025-02-13 03:17:21,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:22,075][root][INFO] - Training Epoch: 1/2, step 5845/7134 completed (loss: 0.030460333451628685, acc: 0.9900285005569458)
[2025-02-13 03:17:22,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:22,488][root][INFO] - Training Epoch: 1/2, step 5846/7134 completed (loss: 0.04186365753412247, acc: 0.9904240965843201)
[2025-02-13 03:17:22,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:22,913][root][INFO] - Training Epoch: 1/2, step 5847/7134 completed (loss: 0.02160804532468319, acc: 0.9931318759918213)
[2025-02-13 03:17:23,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:23,310][root][INFO] - Training Epoch: 1/2, step 5848/7134 completed (loss: 0.04776785522699356, acc: 0.9866071343421936)
[2025-02-13 03:17:23,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:23,700][root][INFO] - Training Epoch: 1/2, step 5849/7134 completed (loss: 0.06064719334244728, acc: 0.9808259606361389)
[2025-02-13 03:17:23,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:24,094][root][INFO] - Training Epoch: 1/2, step 5850/7134 completed (loss: 0.020639345049858093, acc: 0.9938931465148926)
[2025-02-13 03:17:24,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:24,477][root][INFO] - Training Epoch: 1/2, step 5851/7134 completed (loss: 0.024795372039079666, acc: 0.9934210777282715)
[2025-02-13 03:17:24,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:24,883][root][INFO] - Training Epoch: 1/2, step 5852/7134 completed (loss: 0.02208709716796875, acc: 0.9927007555961609)
[2025-02-13 03:17:25,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:25,301][root][INFO] - Training Epoch: 1/2, step 5853/7134 completed (loss: 0.049628809094429016, acc: 0.9878214001655579)
[2025-02-13 03:17:25,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:25,696][root][INFO] - Training Epoch: 1/2, step 5854/7134 completed (loss: 0.026666395366191864, acc: 0.9876106381416321)
[2025-02-13 03:17:25,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:26,078][root][INFO] - Training Epoch: 1/2, step 5855/7134 completed (loss: 0.05190252512693405, acc: 0.9792746305465698)
[2025-02-13 03:17:26,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:26,491][root][INFO] - Training Epoch: 1/2, step 5856/7134 completed (loss: 0.0955071672797203, acc: 0.9773414134979248)
[2025-02-13 03:17:26,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:26,937][root][INFO] - Training Epoch: 1/2, step 5857/7134 completed (loss: 0.036549679934978485, acc: 0.9873617887496948)
[2025-02-13 03:17:27,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:27,323][root][INFO] - Training Epoch: 1/2, step 5858/7134 completed (loss: 0.031685736030340195, acc: 0.9889094233512878)
[2025-02-13 03:17:27,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:27,727][root][INFO] - Training Epoch: 1/2, step 5859/7134 completed (loss: 0.050008200109004974, acc: 0.986328125)
[2025-02-13 03:17:27,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:28,116][root][INFO] - Training Epoch: 1/2, step 5860/7134 completed (loss: 0.04160579666495323, acc: 0.9752066135406494)
[2025-02-13 03:17:28,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:28,496][root][INFO] - Training Epoch: 1/2, step 5861/7134 completed (loss: 0.04993375763297081, acc: 0.9858012199401855)
[2025-02-13 03:17:28,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:28,903][root][INFO] - Training Epoch: 1/2, step 5862/7134 completed (loss: 0.0351986363530159, acc: 0.9871244430541992)
[2025-02-13 03:17:29,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:29,238][root][INFO] - Training Epoch: 1/2, step 5863/7134 completed (loss: 0.05469711869955063, acc: 0.9823232293128967)
[2025-02-13 03:17:29,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:29,653][root][INFO] - Training Epoch: 1/2, step 5864/7134 completed (loss: 0.07688780874013901, acc: 0.9820359349250793)
[2025-02-13 03:17:29,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:30,050][root][INFO] - Training Epoch: 1/2, step 5865/7134 completed (loss: 0.05244216322898865, acc: 0.9873015880584717)
[2025-02-13 03:17:30,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:30,492][root][INFO] - Training Epoch: 1/2, step 5866/7134 completed (loss: 0.03471066430211067, acc: 0.9920634627342224)
[2025-02-13 03:17:30,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:30,900][root][INFO] - Training Epoch: 1/2, step 5867/7134 completed (loss: 0.03050536848604679, acc: 0.9900826215744019)
[2025-02-13 03:17:31,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:31,315][root][INFO] - Training Epoch: 1/2, step 5868/7134 completed (loss: 0.037147458642721176, acc: 0.9896551966667175)
[2025-02-13 03:17:31,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:31,706][root][INFO] - Training Epoch: 1/2, step 5869/7134 completed (loss: 0.05208384990692139, acc: 0.983582079410553)
[2025-02-13 03:17:31,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:32,111][root][INFO] - Training Epoch: 1/2, step 5870/7134 completed (loss: 0.06382488459348679, acc: 0.9790794849395752)
[2025-02-13 03:17:32,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:32,489][root][INFO] - Training Epoch: 1/2, step 5871/7134 completed (loss: 0.025438902899622917, acc: 0.9830148816108704)
[2025-02-13 03:17:32,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:32,889][root][INFO] - Training Epoch: 1/2, step 5872/7134 completed (loss: 0.05509205535054207, acc: 0.9878234267234802)
[2025-02-13 03:17:33,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:33,380][root][INFO] - Training Epoch: 1/2, step 5873/7134 completed (loss: 0.04145682603120804, acc: 0.9906666874885559)
[2025-02-13 03:17:33,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:33,709][root][INFO] - Training Epoch: 1/2, step 5874/7134 completed (loss: 0.01917683146893978, acc: 0.9939758777618408)
[2025-02-13 03:17:33,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:34,042][root][INFO] - Training Epoch: 1/2, step 5875/7134 completed (loss: 0.07192208617925644, acc: 0.9805309772491455)
[2025-02-13 03:17:34,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:34,474][root][INFO] - Training Epoch: 1/2, step 5876/7134 completed (loss: 0.02626822516322136, acc: 0.9927184581756592)
[2025-02-13 03:17:34,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:34,885][root][INFO] - Training Epoch: 1/2, step 5877/7134 completed (loss: 0.06680358201265335, acc: 0.9847715497016907)
[2025-02-13 03:17:35,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:35,305][root][INFO] - Training Epoch: 1/2, step 5878/7134 completed (loss: 0.028370197862386703, acc: 0.9912280440330505)
[2025-02-13 03:17:35,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:35,697][root][INFO] - Training Epoch: 1/2, step 5879/7134 completed (loss: 0.01943807676434517, acc: 1.0)
[2025-02-13 03:17:35,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:35,945][root][INFO] - Training Epoch: 1/2, step 5880/7134 completed (loss: 0.0209701806306839, acc: 0.995555579662323)
[2025-02-13 03:17:36,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:36,347][root][INFO] - Training Epoch: 1/2, step 5881/7134 completed (loss: 0.018812159076333046, acc: 0.9942418336868286)
[2025-02-13 03:17:36,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:36,630][root][INFO] - Training Epoch: 1/2, step 5882/7134 completed (loss: 0.026468483731150627, acc: 0.9905882477760315)
[2025-02-13 03:17:36,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:36,962][root][INFO] - Training Epoch: 1/2, step 5883/7134 completed (loss: 0.040373820811510086, acc: 0.9868913888931274)
[2025-02-13 03:17:37,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:37,369][root][INFO] - Training Epoch: 1/2, step 5884/7134 completed (loss: 0.09931284934282303, acc: 0.9671533107757568)
[2025-02-13 03:17:37,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:37,758][root][INFO] - Training Epoch: 1/2, step 5885/7134 completed (loss: 0.09460872411727905, acc: 0.9755434989929199)
[2025-02-13 03:17:37,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:38,079][root][INFO] - Training Epoch: 1/2, step 5886/7134 completed (loss: 0.02306382916867733, acc: 0.9942857027053833)
[2025-02-13 03:17:38,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:38,471][root][INFO] - Training Epoch: 1/2, step 5887/7134 completed (loss: 0.043592698872089386, acc: 0.9893842935562134)
[2025-02-13 03:17:38,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:38,874][root][INFO] - Training Epoch: 1/2, step 5888/7134 completed (loss: 0.05467570945620537, acc: 0.9831081032752991)
[2025-02-13 03:17:38,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:39,096][root][INFO] - Training Epoch: 1/2, step 5889/7134 completed (loss: 0.013195200823247433, acc: 1.0)
[2025-02-13 03:17:39,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:39,526][root][INFO] - Training Epoch: 1/2, step 5890/7134 completed (loss: 0.019630244001746178, acc: 0.9947552680969238)
[2025-02-13 03:17:39,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:39,941][root][INFO] - Training Epoch: 1/2, step 5891/7134 completed (loss: 0.033624038100242615, acc: 0.988950252532959)
[2025-02-13 03:17:40,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:40,383][root][INFO] - Training Epoch: 1/2, step 5892/7134 completed (loss: 0.0780731812119484, acc: 0.9839650392532349)
[2025-02-13 03:17:40,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:40,793][root][INFO] - Training Epoch: 1/2, step 5893/7134 completed (loss: 0.05596895515918732, acc: 0.9811023473739624)
[2025-02-13 03:17:40,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:41,135][root][INFO] - Training Epoch: 1/2, step 5894/7134 completed (loss: 0.015537142753601074, acc: 0.9967213273048401)
[2025-02-13 03:17:41,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:41,535][root][INFO] - Training Epoch: 1/2, step 5895/7134 completed (loss: 0.015662243589758873, acc: 0.9904761910438538)
[2025-02-13 03:17:41,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:41,944][root][INFO] - Training Epoch: 1/2, step 5896/7134 completed (loss: 0.057347070425748825, acc: 0.9897959232330322)
[2025-02-13 03:17:42,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:42,376][root][INFO] - Training Epoch: 1/2, step 5897/7134 completed (loss: 0.021530980244278908, acc: 0.994490385055542)
[2025-02-13 03:17:42,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:42,776][root][INFO] - Training Epoch: 1/2, step 5898/7134 completed (loss: 0.02603023126721382, acc: 0.9956896305084229)
[2025-02-13 03:17:42,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:43,016][root][INFO] - Training Epoch: 1/2, step 5899/7134 completed (loss: 0.021456679329276085, acc: 0.9940476417541504)
[2025-02-13 03:17:43,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:43,301][root][INFO] - Training Epoch: 1/2, step 5900/7134 completed (loss: 0.027607068419456482, acc: 0.988095223903656)
[2025-02-13 03:17:43,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:43,606][root][INFO] - Training Epoch: 1/2, step 5901/7134 completed (loss: 0.0760471373796463, acc: 0.9850000143051147)
[2025-02-13 03:17:43,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:44,032][root][INFO] - Training Epoch: 1/2, step 5902/7134 completed (loss: 0.10640289634466171, acc: 0.9664948582649231)
[2025-02-13 03:17:44,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:44,452][root][INFO] - Training Epoch: 1/2, step 5903/7134 completed (loss: 0.07222141325473785, acc: 0.9781022071838379)
[2025-02-13 03:17:44,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:44,893][root][INFO] - Training Epoch: 1/2, step 5904/7134 completed (loss: 0.05490509420633316, acc: 0.9808743000030518)
[2025-02-13 03:17:45,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:45,284][root][INFO] - Training Epoch: 1/2, step 5905/7134 completed (loss: 0.05681062489748001, acc: 0.9805285334587097)
[2025-02-13 03:17:45,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:45,733][root][INFO] - Training Epoch: 1/2, step 5906/7134 completed (loss: 0.0812530517578125, acc: 0.977886974811554)
[2025-02-13 03:17:45,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:46,209][root][INFO] - Training Epoch: 1/2, step 5907/7134 completed (loss: 0.054842278361320496, acc: 0.9793014526367188)
[2025-02-13 03:17:46,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:46,682][root][INFO] - Training Epoch: 1/2, step 5908/7134 completed (loss: 0.043439287692308426, acc: 0.9833759665489197)
[2025-02-13 03:17:46,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:47,255][root][INFO] - Training Epoch: 1/2, step 5909/7134 completed (loss: 0.05377711355686188, acc: 0.9855907559394836)
[2025-02-13 03:17:47,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:47,740][root][INFO] - Training Epoch: 1/2, step 5910/7134 completed (loss: 0.05452181026339531, acc: 0.9750346541404724)
[2025-02-13 03:17:47,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:48,155][root][INFO] - Training Epoch: 1/2, step 5911/7134 completed (loss: 0.05411645397543907, acc: 0.9834710955619812)
[2025-02-13 03:17:48,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:48,541][root][INFO] - Training Epoch: 1/2, step 5912/7134 completed (loss: 0.03650058060884476, acc: 0.9817159175872803)
[2025-02-13 03:17:48,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:48,995][root][INFO] - Training Epoch: 1/2, step 5913/7134 completed (loss: 0.11183584481477737, acc: 0.9728260636329651)
[2025-02-13 03:17:49,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:49,376][root][INFO] - Training Epoch: 1/2, step 5914/7134 completed (loss: 0.028208259493112564, acc: 0.9903692007064819)
[2025-02-13 03:17:49,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:49,821][root][INFO] - Training Epoch: 1/2, step 5915/7134 completed (loss: 0.04841659963130951, acc: 0.9813953638076782)
[2025-02-13 03:17:49,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:50,273][root][INFO] - Training Epoch: 1/2, step 5916/7134 completed (loss: 0.050170741975307465, acc: 0.9868938326835632)
[2025-02-13 03:17:50,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:50,646][root][INFO] - Training Epoch: 1/2, step 5917/7134 completed (loss: 0.04022844135761261, acc: 0.9896013736724854)
[2025-02-13 03:17:50,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:51,090][root][INFO] - Training Epoch: 1/2, step 5918/7134 completed (loss: 0.08813855051994324, acc: 0.9811320900917053)
[2025-02-13 03:17:51,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:51,500][root][INFO] - Training Epoch: 1/2, step 5919/7134 completed (loss: 0.041805390268564224, acc: 0.9878419637680054)
[2025-02-13 03:17:51,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:51,900][root][INFO] - Training Epoch: 1/2, step 5920/7134 completed (loss: 0.04871992766857147, acc: 0.9833610653877258)
[2025-02-13 03:17:52,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:52,304][root][INFO] - Training Epoch: 1/2, step 5921/7134 completed (loss: 0.07353387773036957, acc: 0.9749478101730347)
[2025-02-13 03:17:52,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:52,720][root][INFO] - Training Epoch: 1/2, step 5922/7134 completed (loss: 0.19056393206119537, acc: 0.9556737542152405)
[2025-02-13 03:17:52,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:53,167][root][INFO] - Training Epoch: 1/2, step 5923/7134 completed (loss: 0.10916457325220108, acc: 0.966325044631958)
[2025-02-13 03:17:53,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:53,566][root][INFO] - Training Epoch: 1/2, step 5924/7134 completed (loss: 0.05887037143111229, acc: 0.9809069037437439)
[2025-02-13 03:17:53,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:53,902][root][INFO] - Training Epoch: 1/2, step 5925/7134 completed (loss: 0.24058064818382263, acc: 0.9530791640281677)
[2025-02-13 03:17:54,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:54,298][root][INFO] - Training Epoch: 1/2, step 5926/7134 completed (loss: 0.0986877903342247, acc: 0.9733333587646484)
[2025-02-13 03:17:54,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:54,695][root][INFO] - Training Epoch: 1/2, step 5927/7134 completed (loss: 0.10150529444217682, acc: 0.9710467457771301)
[2025-02-13 03:17:54,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:55,010][root][INFO] - Training Epoch: 1/2, step 5928/7134 completed (loss: 0.20998252928256989, acc: 0.9589040875434875)
[2025-02-13 03:17:55,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:55,421][root][INFO] - Training Epoch: 1/2, step 5929/7134 completed (loss: 0.04610249027609825, acc: 0.983208954334259)
[2025-02-13 03:17:55,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:55,787][root][INFO] - Training Epoch: 1/2, step 5930/7134 completed (loss: 0.10243664681911469, acc: 0.9755011200904846)
[2025-02-13 03:17:55,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:56,186][root][INFO] - Training Epoch: 1/2, step 5931/7134 completed (loss: 0.05890501290559769, acc: 0.9827213883399963)
[2025-02-13 03:17:56,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:56,581][root][INFO] - Training Epoch: 1/2, step 5932/7134 completed (loss: 0.11181957274675369, acc: 0.9685184955596924)
[2025-02-13 03:17:56,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:56,926][root][INFO] - Training Epoch: 1/2, step 5933/7134 completed (loss: 0.1155354455113411, acc: 0.9685534834861755)
[2025-02-13 03:17:57,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:57,324][root][INFO] - Training Epoch: 1/2, step 5934/7134 completed (loss: 0.05296330153942108, acc: 0.9822379946708679)
[2025-02-13 03:17:57,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:57,711][root][INFO] - Training Epoch: 1/2, step 5935/7134 completed (loss: 0.04710937663912773, acc: 0.9902912378311157)
[2025-02-13 03:17:57,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:58,094][root][INFO] - Training Epoch: 1/2, step 5936/7134 completed (loss: 0.07703300565481186, acc: 0.9831546545028687)
[2025-02-13 03:17:58,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:58,503][root][INFO] - Training Epoch: 1/2, step 5937/7134 completed (loss: 0.030179036781191826, acc: 0.9920254945755005)
[2025-02-13 03:17:58,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:58,904][root][INFO] - Training Epoch: 1/2, step 5938/7134 completed (loss: 0.0946609377861023, acc: 0.9732313752174377)
[2025-02-13 03:17:59,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:59,266][root][INFO] - Training Epoch: 1/2, step 5939/7134 completed (loss: 0.19571809470653534, acc: 0.9479637742042542)
[2025-02-13 03:17:59,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:59,715][root][INFO] - Training Epoch: 1/2, step 5940/7134 completed (loss: 0.06993896514177322, acc: 0.9737609624862671)
[2025-02-13 03:17:59,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:00,078][root][INFO] - Training Epoch: 1/2, step 5941/7134 completed (loss: 0.10896965116262436, acc: 0.9750000238418579)
[2025-02-13 03:18:00,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:00,474][root][INFO] - Training Epoch: 1/2, step 5942/7134 completed (loss: 0.021552955731749535, acc: 0.9942638874053955)
[2025-02-13 03:18:00,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:00,876][root][INFO] - Training Epoch: 1/2, step 5943/7134 completed (loss: 0.022495200857520103, acc: 0.995708167552948)
[2025-02-13 03:18:01,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:01,319][root][INFO] - Training Epoch: 1/2, step 5944/7134 completed (loss: 0.0727180540561676, acc: 0.9790576100349426)
[2025-02-13 03:18:01,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:01,770][root][INFO] - Training Epoch: 1/2, step 5945/7134 completed (loss: 0.031656015664339066, acc: 0.9894894957542419)
[2025-02-13 03:18:01,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:02,170][root][INFO] - Training Epoch: 1/2, step 5946/7134 completed (loss: 0.04008402302861214, acc: 0.9873188138008118)
[2025-02-13 03:18:02,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:02,610][root][INFO] - Training Epoch: 1/2, step 5947/7134 completed (loss: 0.04533974826335907, acc: 0.9851301312446594)
[2025-02-13 03:18:02,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:03,027][root][INFO] - Training Epoch: 1/2, step 5948/7134 completed (loss: 0.07065517455339432, acc: 0.972000002861023)
[2025-02-13 03:18:03,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:03,457][root][INFO] - Training Epoch: 1/2, step 5949/7134 completed (loss: 0.033948127180337906, acc: 0.9899623394012451)
[2025-02-13 03:18:03,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:03,853][root][INFO] - Training Epoch: 1/2, step 5950/7134 completed (loss: 0.12594252824783325, acc: 0.9667458534240723)
[2025-02-13 03:18:03,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:04,265][root][INFO] - Training Epoch: 1/2, step 5951/7134 completed (loss: 0.07515978068113327, acc: 0.9784792065620422)
[2025-02-13 03:18:04,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:04,713][root][INFO] - Training Epoch: 1/2, step 5952/7134 completed (loss: 0.1975766122341156, acc: 0.9671875238418579)
[2025-02-13 03:18:04,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:05,129][root][INFO] - Training Epoch: 1/2, step 5953/7134 completed (loss: 0.06340589374303818, acc: 0.9779614210128784)
[2025-02-13 03:18:05,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:05,526][root][INFO] - Training Epoch: 1/2, step 5954/7134 completed (loss: 0.04473887011408806, acc: 0.9822335243225098)
[2025-02-13 03:18:05,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:05,764][root][INFO] - Training Epoch: 1/2, step 5955/7134 completed (loss: 0.06552975624799728, acc: 0.9857142567634583)
[2025-02-13 03:18:05,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:06,145][root][INFO] - Training Epoch: 1/2, step 5956/7134 completed (loss: 0.09142211079597473, acc: 0.9745097756385803)
[2025-02-13 03:18:06,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:06,525][root][INFO] - Training Epoch: 1/2, step 5957/7134 completed (loss: 0.05368597432971001, acc: 0.9850075244903564)
[2025-02-13 03:18:06,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:06,944][root][INFO] - Training Epoch: 1/2, step 5958/7134 completed (loss: 0.037716902792453766, acc: 0.9879194498062134)
[2025-02-13 03:18:07,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:07,377][root][INFO] - Training Epoch: 1/2, step 5959/7134 completed (loss: 0.045910995453596115, acc: 0.9879840016365051)
[2025-02-13 03:18:07,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:07,810][root][INFO] - Training Epoch: 1/2, step 5960/7134 completed (loss: 0.010938319377601147, acc: 0.9985954761505127)
[2025-02-13 03:18:07,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:08,161][root][INFO] - Training Epoch: 1/2, step 5961/7134 completed (loss: 0.05487177520990372, acc: 0.9815573692321777)
[2025-02-13 03:18:08,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:08,570][root][INFO] - Training Epoch: 1/2, step 5962/7134 completed (loss: 0.018223712220788002, acc: 0.9934959411621094)
[2025-02-13 03:18:08,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:08,981][root][INFO] - Training Epoch: 1/2, step 5963/7134 completed (loss: 0.026634667068719864, acc: 0.9902234673500061)
[2025-02-13 03:18:09,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:09,377][root][INFO] - Training Epoch: 1/2, step 5964/7134 completed (loss: 0.030433837324380875, acc: 0.9907578825950623)
[2025-02-13 03:18:09,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:09,792][root][INFO] - Training Epoch: 1/2, step 5965/7134 completed (loss: 0.06818373501300812, acc: 0.9865067601203918)
[2025-02-13 03:18:09,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:10,215][root][INFO] - Training Epoch: 1/2, step 5966/7134 completed (loss: 0.04956996813416481, acc: 0.9861303567886353)
[2025-02-13 03:18:10,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:10,640][root][INFO] - Training Epoch: 1/2, step 5967/7134 completed (loss: 0.037979062646627426, acc: 0.9940298795700073)
[2025-02-13 03:18:10,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:11,040][root][INFO] - Training Epoch: 1/2, step 5968/7134 completed (loss: 0.05304441973567009, acc: 0.9840182662010193)
[2025-02-13 03:18:11,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:11,446][root][INFO] - Training Epoch: 1/2, step 5969/7134 completed (loss: 0.05540788173675537, acc: 0.9845201373100281)
[2025-02-13 03:18:11,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:11,852][root][INFO] - Training Epoch: 1/2, step 5970/7134 completed (loss: 0.06311696022748947, acc: 0.9856557250022888)
[2025-02-13 03:18:11,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:12,283][root][INFO] - Training Epoch: 1/2, step 5971/7134 completed (loss: 0.026827072724699974, acc: 0.9924812316894531)
[2025-02-13 03:18:12,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:12,709][root][INFO] - Training Epoch: 1/2, step 5972/7134 completed (loss: 0.0896340012550354, acc: 0.9867841601371765)
[2025-02-13 03:18:12,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:13,173][root][INFO] - Training Epoch: 1/2, step 5973/7134 completed (loss: 0.09177598357200623, acc: 0.971222996711731)
[2025-02-13 03:18:13,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:13,627][root][INFO] - Training Epoch: 1/2, step 5974/7134 completed (loss: 0.01541249267756939, acc: 0.9948006868362427)
[2025-02-13 03:18:13,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:14,007][root][INFO] - Training Epoch: 1/2, step 5975/7134 completed (loss: 0.024643555283546448, acc: 0.9882006049156189)
[2025-02-13 03:18:14,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:14,449][root][INFO] - Training Epoch: 1/2, step 5976/7134 completed (loss: 0.05245649814605713, acc: 0.9861963391304016)
[2025-02-13 03:18:14,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:14,888][root][INFO] - Training Epoch: 1/2, step 5977/7134 completed (loss: 0.019457831978797913, acc: 0.9962825179100037)
[2025-02-13 03:18:15,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:15,317][root][INFO] - Training Epoch: 1/2, step 5978/7134 completed (loss: 0.034223396331071854, acc: 0.9903846383094788)
[2025-02-13 03:18:15,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:15,624][root][INFO] - Training Epoch: 1/2, step 5979/7134 completed (loss: 0.055140525102615356, acc: 0.9851852059364319)
[2025-02-13 03:18:15,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:16,055][root][INFO] - Training Epoch: 1/2, step 5980/7134 completed (loss: 0.03241628035902977, acc: 0.9923175573348999)
[2025-02-13 03:18:16,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:16,504][root][INFO] - Training Epoch: 1/2, step 5981/7134 completed (loss: 0.03345077857375145, acc: 0.989595353603363)
[2025-02-13 03:18:16,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:16,892][root][INFO] - Training Epoch: 1/2, step 5982/7134 completed (loss: 0.052588801831007004, acc: 0.9920477271080017)
[2025-02-13 03:18:17,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:17,314][root][INFO] - Training Epoch: 1/2, step 5983/7134 completed (loss: 0.05466378107666969, acc: 0.9847221970558167)
[2025-02-13 03:18:17,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:17,746][root][INFO] - Training Epoch: 1/2, step 5984/7134 completed (loss: 0.03244517743587494, acc: 0.9914893507957458)
[2025-02-13 03:18:17,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:18,190][root][INFO] - Training Epoch: 1/2, step 5985/7134 completed (loss: 0.04063039273023605, acc: 0.9899874925613403)
[2025-02-13 03:18:18,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:18,648][root][INFO] - Training Epoch: 1/2, step 5986/7134 completed (loss: 0.04893174022436142, acc: 0.987261176109314)
[2025-02-13 03:18:18,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:19,092][root][INFO] - Training Epoch: 1/2, step 5987/7134 completed (loss: 0.05834061652421951, acc: 0.9836829900741577)
[2025-02-13 03:18:19,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:19,513][root][INFO] - Training Epoch: 1/2, step 5988/7134 completed (loss: 0.020834747701883316, acc: 0.9959999918937683)
[2025-02-13 03:18:19,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:19,963][root][INFO] - Training Epoch: 1/2, step 5989/7134 completed (loss: 0.0652213767170906, acc: 0.9870848655700684)
[2025-02-13 03:18:20,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:20,427][root][INFO] - Training Epoch: 1/2, step 5990/7134 completed (loss: 0.04654807597398758, acc: 0.9849315285682678)
[2025-02-13 03:18:20,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:20,690][root][INFO] - Training Epoch: 1/2, step 5991/7134 completed (loss: 0.11476398259401321, acc: 0.9748603105545044)
[2025-02-13 03:18:20,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:21,168][root][INFO] - Training Epoch: 1/2, step 5992/7134 completed (loss: 0.03753437474370003, acc: 0.9907084703445435)
[2025-02-13 03:18:21,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:21,584][root][INFO] - Training Epoch: 1/2, step 5993/7134 completed (loss: 0.03197064623236656, acc: 0.9902912378311157)
[2025-02-13 03:18:21,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:21,964][root][INFO] - Training Epoch: 1/2, step 5994/7134 completed (loss: 0.026490993797779083, acc: 0.9923954606056213)
[2025-02-13 03:18:22,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:22,417][root][INFO] - Training Epoch: 1/2, step 5995/7134 completed (loss: 0.02030065655708313, acc: 0.9929577708244324)
[2025-02-13 03:18:22,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:22,850][root][INFO] - Training Epoch: 1/2, step 5996/7134 completed (loss: 0.06504695117473602, acc: 0.9812206625938416)
[2025-02-13 03:18:22,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:23,280][root][INFO] - Training Epoch: 1/2, step 5997/7134 completed (loss: 0.04462498426437378, acc: 0.9933244585990906)
[2025-02-13 03:18:23,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:23,715][root][INFO] - Training Epoch: 1/2, step 5998/7134 completed (loss: 0.045253753662109375, acc: 0.9862843155860901)
[2025-02-13 03:18:23,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:24,115][root][INFO] - Training Epoch: 1/2, step 5999/7134 completed (loss: 0.028097985312342644, acc: 0.9938042163848877)
[2025-02-13 03:18:24,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:24,504][root][INFO] - Training Epoch: 1/2, step 6000/7134 completed (loss: 0.06104649230837822, acc: 0.981697142124176)
[2025-02-13 03:18:24,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:24,941][root][INFO] - Training Epoch: 1/2, step 6001/7134 completed (loss: 0.026476986706256866, acc: 0.9910485744476318)
[2025-02-13 03:18:25,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:25,353][root][INFO] - Training Epoch: 1/2, step 6002/7134 completed (loss: 0.05522306635975838, acc: 0.9844054579734802)
[2025-02-13 03:18:25,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:25,801][root][INFO] - Training Epoch: 1/2, step 6003/7134 completed (loss: 0.042573247104883194, acc: 0.9838472604751587)
[2025-02-13 03:18:25,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:26,222][root][INFO] - Training Epoch: 1/2, step 6004/7134 completed (loss: 0.03495146706700325, acc: 0.9916550517082214)
[2025-02-13 03:18:26,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:26,619][root][INFO] - Training Epoch: 1/2, step 6005/7134 completed (loss: 0.04498047009110451, acc: 0.9893454909324646)
[2025-02-13 03:18:26,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:27,032][root][INFO] - Training Epoch: 1/2, step 6006/7134 completed (loss: 0.0668354481458664, acc: 0.978723406791687)
[2025-02-13 03:18:27,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:27,471][root][INFO] - Training Epoch: 1/2, step 6007/7134 completed (loss: 0.02411944605410099, acc: 0.9966611266136169)
[2025-02-13 03:18:27,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:27,903][root][INFO] - Training Epoch: 1/2, step 6008/7134 completed (loss: 0.049189139157533646, acc: 0.9836795330047607)
[2025-02-13 03:18:28,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:28,304][root][INFO] - Training Epoch: 1/2, step 6009/7134 completed (loss: 0.07569190114736557, acc: 0.9798136353492737)
[2025-02-13 03:18:28,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:28,731][root][INFO] - Training Epoch: 1/2, step 6010/7134 completed (loss: 0.07734495401382446, acc: 0.9742547273635864)
[2025-02-13 03:18:28,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:29,174][root][INFO] - Training Epoch: 1/2, step 6011/7134 completed (loss: 0.06410753726959229, acc: 0.9805699586868286)
[2025-02-13 03:18:29,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:29,592][root][INFO] - Training Epoch: 1/2, step 6012/7134 completed (loss: 0.05839085951447487, acc: 0.9791044592857361)
[2025-02-13 03:18:29,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:30,006][root][INFO] - Training Epoch: 1/2, step 6013/7134 completed (loss: 0.054086245596408844, acc: 0.98128342628479)
[2025-02-13 03:18:30,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:30,393][root][INFO] - Training Epoch: 1/2, step 6014/7134 completed (loss: 0.03575949743390083, acc: 0.9836660623550415)
[2025-02-13 03:18:30,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:30,822][root][INFO] - Training Epoch: 1/2, step 6015/7134 completed (loss: 0.07457392662763596, acc: 0.9752066135406494)
[2025-02-13 03:18:30,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:31,215][root][INFO] - Training Epoch: 1/2, step 6016/7134 completed (loss: 0.02235545590519905, acc: 0.9892473220825195)
[2025-02-13 03:18:31,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:31,464][root][INFO] - Training Epoch: 1/2, step 6017/7134 completed (loss: 0.10010503977537155, acc: 0.9851484894752502)
[2025-02-13 03:18:31,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:31,711][root][INFO] - Training Epoch: 1/2, step 6018/7134 completed (loss: 0.022185515612363815, acc: 0.9894737005233765)
[2025-02-13 03:18:31,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:31,944][root][INFO] - Training Epoch: 1/2, step 6019/7134 completed (loss: 0.09223760664463043, acc: 0.9741379022598267)
[2025-02-13 03:18:32,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:32,210][root][INFO] - Training Epoch: 1/2, step 6020/7134 completed (loss: 0.006587598938494921, acc: 1.0)
[2025-02-13 03:18:32,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:32,438][root][INFO] - Training Epoch: 1/2, step 6021/7134 completed (loss: 0.04400286078453064, acc: 0.9934640526771545)
[2025-02-13 03:18:32,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:32,734][root][INFO] - Training Epoch: 1/2, step 6022/7134 completed (loss: 0.031621791422367096, acc: 0.9889298677444458)
[2025-02-13 03:18:32,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:33,038][root][INFO] - Training Epoch: 1/2, step 6023/7134 completed (loss: 0.019118521362543106, acc: 0.9945799708366394)
[2025-02-13 03:18:33,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:33,280][root][INFO] - Training Epoch: 1/2, step 6024/7134 completed (loss: 0.010225960984826088, acc: 1.0)
[2025-02-13 03:18:33,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:33,659][root][INFO] - Training Epoch: 1/2, step 6025/7134 completed (loss: 0.025041233748197556, acc: 0.991631805896759)
[2025-02-13 03:18:33,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:33,895][root][INFO] - Training Epoch: 1/2, step 6026/7134 completed (loss: 0.050476085394620895, acc: 0.9942857027053833)
[2025-02-13 03:18:33,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:34,208][root][INFO] - Training Epoch: 1/2, step 6027/7134 completed (loss: 0.029009265825152397, acc: 0.9948586225509644)
[2025-02-13 03:18:34,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:34,442][root][INFO] - Training Epoch: 1/2, step 6028/7134 completed (loss: 0.024604691192507744, acc: 0.9911110997200012)
[2025-02-13 03:18:34,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:34,711][root][INFO] - Training Epoch: 1/2, step 6029/7134 completed (loss: 0.05433639883995056, acc: 0.9819494485855103)
[2025-02-13 03:18:34,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:34,998][root][INFO] - Training Epoch: 1/2, step 6030/7134 completed (loss: 0.043917495757341385, acc: 0.992337167263031)
[2025-02-13 03:18:35,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:35,391][root][INFO] - Training Epoch: 1/2, step 6031/7134 completed (loss: 0.033966224640607834, acc: 0.9912280440330505)
[2025-02-13 03:18:35,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:35,736][root][INFO] - Training Epoch: 1/2, step 6032/7134 completed (loss: 0.038835570216178894, acc: 0.9898734092712402)
[2025-02-13 03:18:35,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:36,043][root][INFO] - Training Epoch: 1/2, step 6033/7134 completed (loss: 0.09376977384090424, acc: 0.9754098653793335)
[2025-02-13 03:18:36,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:36,298][root][INFO] - Training Epoch: 1/2, step 6034/7134 completed (loss: 0.0495988093316555, acc: 0.9918032884597778)
[2025-02-13 03:18:36,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:36,772][root][INFO] - Training Epoch: 1/2, step 6035/7134 completed (loss: 0.10554835200309753, acc: 0.973099410533905)
[2025-02-13 03:18:36,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:37,241][root][INFO] - Training Epoch: 1/2, step 6036/7134 completed (loss: 0.08123096823692322, acc: 0.9748252034187317)
[2025-02-13 03:18:37,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:37,672][root][INFO] - Training Epoch: 1/2, step 6037/7134 completed (loss: 0.040657561272382736, acc: 0.9830508232116699)
[2025-02-13 03:18:37,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:38,127][root][INFO] - Training Epoch: 1/2, step 6038/7134 completed (loss: 0.06269894540309906, acc: 0.9854586124420166)
[2025-02-13 03:18:38,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:38,575][root][INFO] - Training Epoch: 1/2, step 6039/7134 completed (loss: 0.04201897978782654, acc: 0.990510106086731)
[2025-02-13 03:18:38,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:39,026][root][INFO] - Training Epoch: 1/2, step 6040/7134 completed (loss: 0.06733038276433945, acc: 0.9844074845314026)
[2025-02-13 03:18:39,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:39,432][root][INFO] - Training Epoch: 1/2, step 6041/7134 completed (loss: 0.05028560012578964, acc: 0.9882044792175293)
[2025-02-13 03:18:39,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:39,875][root][INFO] - Training Epoch: 1/2, step 6042/7134 completed (loss: 0.028935354202985764, acc: 0.9909999966621399)
[2025-02-13 03:18:40,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:40,309][root][INFO] - Training Epoch: 1/2, step 6043/7134 completed (loss: 0.03986922279000282, acc: 0.9892086386680603)
[2025-02-13 03:18:40,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:40,752][root][INFO] - Training Epoch: 1/2, step 6044/7134 completed (loss: 0.04729853570461273, acc: 0.9851852059364319)
[2025-02-13 03:18:40,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:41,168][root][INFO] - Training Epoch: 1/2, step 6045/7134 completed (loss: 0.0335160531103611, acc: 0.9917159676551819)
[2025-02-13 03:18:41,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:41,605][root][INFO] - Training Epoch: 1/2, step 6046/7134 completed (loss: 0.05488741025328636, acc: 0.9826689958572388)
[2025-02-13 03:18:41,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:42,057][root][INFO] - Training Epoch: 1/2, step 6047/7134 completed (loss: 0.10194078832864761, acc: 0.9713321924209595)
[2025-02-13 03:18:42,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:42,486][root][INFO] - Training Epoch: 1/2, step 6048/7134 completed (loss: 0.09715848416090012, acc: 0.9699398875236511)
[2025-02-13 03:18:42,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:42,876][root][INFO] - Training Epoch: 1/2, step 6049/7134 completed (loss: 0.023908475413918495, acc: 0.9948805570602417)
[2025-02-13 03:18:43,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:43,277][root][INFO] - Training Epoch: 1/2, step 6050/7134 completed (loss: 0.08945970982313156, acc: 0.9740082025527954)
[2025-02-13 03:18:43,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:43,733][root][INFO] - Training Epoch: 1/2, step 6051/7134 completed (loss: 0.10152725875377655, acc: 0.9660266041755676)
[2025-02-13 03:18:43,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:44,188][root][INFO] - Training Epoch: 1/2, step 6052/7134 completed (loss: 0.07957928627729416, acc: 0.9811617136001587)
[2025-02-13 03:18:44,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:44,638][root][INFO] - Training Epoch: 1/2, step 6053/7134 completed (loss: 0.06375957280397415, acc: 0.9802631735801697)
[2025-02-13 03:18:44,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:45,068][root][INFO] - Training Epoch: 1/2, step 6054/7134 completed (loss: 0.048362474888563156, acc: 0.9818840622901917)
[2025-02-13 03:18:45,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:45,475][root][INFO] - Training Epoch: 1/2, step 6055/7134 completed (loss: 0.03482048958539963, acc: 0.9921011328697205)
[2025-02-13 03:18:45,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:45,844][root][INFO] - Training Epoch: 1/2, step 6056/7134 completed (loss: 0.07551681250333786, acc: 0.9769585132598877)
[2025-02-13 03:18:45,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:46,196][root][INFO] - Training Epoch: 1/2, step 6057/7134 completed (loss: 0.06247960776090622, acc: 0.9802631735801697)
[2025-02-13 03:18:46,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:46,545][root][INFO] - Training Epoch: 1/2, step 6058/7134 completed (loss: 0.014019888825714588, acc: 0.9954954981803894)
[2025-02-13 03:18:46,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:46,889][root][INFO] - Training Epoch: 1/2, step 6059/7134 completed (loss: 0.05036759003996849, acc: 0.9787985682487488)
[2025-02-13 03:18:47,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:47,327][root][INFO] - Training Epoch: 1/2, step 6060/7134 completed (loss: 0.03818028047680855, acc: 0.9879699349403381)
[2025-02-13 03:18:47,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:47,725][root][INFO] - Training Epoch: 1/2, step 6061/7134 completed (loss: 0.0797652006149292, acc: 0.9784283638000488)
[2025-02-13 03:18:47,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:48,152][root][INFO] - Training Epoch: 1/2, step 6062/7134 completed (loss: 0.10369221121072769, acc: 0.9702842235565186)
[2025-02-13 03:18:48,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:48,551][root][INFO] - Training Epoch: 1/2, step 6063/7134 completed (loss: 0.11480405181646347, acc: 0.9728752374649048)
[2025-02-13 03:18:48,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:48,988][root][INFO] - Training Epoch: 1/2, step 6064/7134 completed (loss: 0.0754823312163353, acc: 0.9782923460006714)
[2025-02-13 03:18:49,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:49,439][root][INFO] - Training Epoch: 1/2, step 6065/7134 completed (loss: 0.06670405715703964, acc: 0.9769137501716614)
[2025-02-13 03:18:49,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:49,837][root][INFO] - Training Epoch: 1/2, step 6066/7134 completed (loss: 0.09607487916946411, acc: 0.9654036164283752)
[2025-02-13 03:18:49,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:50,239][root][INFO] - Training Epoch: 1/2, step 6067/7134 completed (loss: 0.09034222364425659, acc: 0.9754689931869507)
[2025-02-13 03:18:50,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:50,701][root][INFO] - Training Epoch: 1/2, step 6068/7134 completed (loss: 0.10152621567249298, acc: 0.9687108993530273)
[2025-02-13 03:18:50,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:51,096][root][INFO] - Training Epoch: 1/2, step 6069/7134 completed (loss: 0.09020274132490158, acc: 0.9693430662155151)
[2025-02-13 03:18:51,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:51,547][root][INFO] - Training Epoch: 1/2, step 6070/7134 completed (loss: 0.09309038519859314, acc: 0.9773635268211365)
[2025-02-13 03:18:51,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:51,946][root][INFO] - Training Epoch: 1/2, step 6071/7134 completed (loss: 0.0665372759103775, acc: 0.9784656763076782)
[2025-02-13 03:18:52,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:52,271][root][INFO] - Training Epoch: 1/2, step 6072/7134 completed (loss: 0.11656351387500763, acc: 0.9684873819351196)
[2025-02-13 03:18:52,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:52,673][root][INFO] - Training Epoch: 1/2, step 6073/7134 completed (loss: 0.06972222775220871, acc: 0.9846153855323792)
[2025-02-13 03:18:52,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:53,070][root][INFO] - Training Epoch: 1/2, step 6074/7134 completed (loss: 0.07383111864328384, acc: 0.9823529124259949)
[2025-02-13 03:18:53,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:53,504][root][INFO] - Training Epoch: 1/2, step 6075/7134 completed (loss: 0.06173872947692871, acc: 0.9844124913215637)
[2025-02-13 03:18:53,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:53,916][root][INFO] - Training Epoch: 1/2, step 6076/7134 completed (loss: 0.1000983938574791, acc: 0.9727011322975159)
[2025-02-13 03:18:54,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:54,323][root][INFO] - Training Epoch: 1/2, step 6077/7134 completed (loss: 0.08567433804273605, acc: 0.97444087266922)
[2025-02-13 03:18:54,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:54,739][root][INFO] - Training Epoch: 1/2, step 6078/7134 completed (loss: 0.10053058713674545, acc: 0.9746478796005249)
[2025-02-13 03:18:54,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:55,185][root][INFO] - Training Epoch: 1/2, step 6079/7134 completed (loss: 0.049714069813489914, acc: 0.9817517995834351)
[2025-02-13 03:18:55,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:55,583][root][INFO] - Training Epoch: 1/2, step 6080/7134 completed (loss: 0.04586681351065636, acc: 0.9886845946311951)
[2025-02-13 03:18:55,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:56,000][root][INFO] - Training Epoch: 1/2, step 6081/7134 completed (loss: 0.04369182512164116, acc: 0.9847328066825867)
[2025-02-13 03:18:56,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:56,398][root][INFO] - Training Epoch: 1/2, step 6082/7134 completed (loss: 0.04286998137831688, acc: 0.9920381903648376)
[2025-02-13 03:18:56,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:56,841][root][INFO] - Training Epoch: 1/2, step 6083/7134 completed (loss: 0.020016316324472427, acc: 0.9950860142707825)
[2025-02-13 03:18:56,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:57,237][root][INFO] - Training Epoch: 1/2, step 6084/7134 completed (loss: 0.07338433712720871, acc: 0.9827044010162354)
[2025-02-13 03:18:57,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:57,682][root][INFO] - Training Epoch: 1/2, step 6085/7134 completed (loss: 0.036667127162218094, acc: 0.989266574382782)
[2025-02-13 03:18:57,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:58,159][root][INFO] - Training Epoch: 1/2, step 6086/7134 completed (loss: 0.03743481636047363, acc: 0.9901685118675232)
[2025-02-13 03:18:58,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:58,602][root][INFO] - Training Epoch: 1/2, step 6087/7134 completed (loss: 0.05475161224603653, acc: 0.9798741936683655)
[2025-02-13 03:18:58,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:59,010][root][INFO] - Training Epoch: 1/2, step 6088/7134 completed (loss: 0.039022646844387054, acc: 0.9839141964912415)
[2025-02-13 03:18:59,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:59,470][root][INFO] - Training Epoch: 1/2, step 6089/7134 completed (loss: 0.048827193677425385, acc: 0.9875862002372742)
[2025-02-13 03:18:59,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:59,891][root][INFO] - Training Epoch: 1/2, step 6090/7134 completed (loss: 0.02991883084177971, acc: 0.9903978109359741)
[2025-02-13 03:19:00,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:00,289][root][INFO] - Training Epoch: 1/2, step 6091/7134 completed (loss: 0.026936346665024757, acc: 0.9904458522796631)
[2025-02-13 03:19:00,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:00,682][root][INFO] - Training Epoch: 1/2, step 6092/7134 completed (loss: 0.05609815567731857, acc: 0.9873015880584717)
[2025-02-13 03:19:00,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:01,086][root][INFO] - Training Epoch: 1/2, step 6093/7134 completed (loss: 0.07196380943059921, acc: 0.9814471006393433)
[2025-02-13 03:19:01,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:01,482][root][INFO] - Training Epoch: 1/2, step 6094/7134 completed (loss: 0.03791035711765289, acc: 0.9873949289321899)
[2025-02-13 03:19:01,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:01,857][root][INFO] - Training Epoch: 1/2, step 6095/7134 completed (loss: 0.02981940098106861, acc: 0.9913793206214905)
[2025-02-13 03:19:02,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:02,305][root][INFO] - Training Epoch: 1/2, step 6096/7134 completed (loss: 0.05492468178272247, acc: 0.9892601370811462)
[2025-02-13 03:19:02,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:02,724][root][INFO] - Training Epoch: 1/2, step 6097/7134 completed (loss: 0.06152701377868652, acc: 0.9861496090888977)
[2025-02-13 03:19:02,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:03,142][root][INFO] - Training Epoch: 1/2, step 6098/7134 completed (loss: 0.08745326846837997, acc: 0.9768339991569519)
[2025-02-13 03:19:03,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:03,575][root][INFO] - Training Epoch: 1/2, step 6099/7134 completed (loss: 0.09459362924098969, acc: 0.9847095012664795)
[2025-02-13 03:19:03,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:03,885][root][INFO] - Training Epoch: 1/2, step 6100/7134 completed (loss: 0.11789680272340775, acc: 0.9786585569381714)
[2025-02-13 03:19:04,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:04,265][root][INFO] - Training Epoch: 1/2, step 6101/7134 completed (loss: 0.03847847878932953, acc: 0.9904153347015381)
[2025-02-13 03:19:04,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:04,662][root][INFO] - Training Epoch: 1/2, step 6102/7134 completed (loss: 0.058185677975416183, acc: 0.98591548204422)
[2025-02-13 03:19:04,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:05,085][root][INFO] - Training Epoch: 1/2, step 6103/7134 completed (loss: 0.038057420402765274, acc: 0.9888888597488403)
[2025-02-13 03:19:05,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:05,484][root][INFO] - Training Epoch: 1/2, step 6104/7134 completed (loss: 0.08726708590984344, acc: 0.9733333587646484)
[2025-02-13 03:19:05,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:05,907][root][INFO] - Training Epoch: 1/2, step 6105/7134 completed (loss: 0.06679854542016983, acc: 0.9791666865348816)
[2025-02-13 03:19:06,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:06,257][root][INFO] - Training Epoch: 1/2, step 6106/7134 completed (loss: 0.06071477010846138, acc: 0.9838998317718506)
[2025-02-13 03:19:06,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:06,665][root][INFO] - Training Epoch: 1/2, step 6107/7134 completed (loss: 0.0723431259393692, acc: 0.9831932783126831)
[2025-02-13 03:19:06,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:07,057][root][INFO] - Training Epoch: 1/2, step 6108/7134 completed (loss: 0.05768469721078873, acc: 0.9836065769195557)
[2025-02-13 03:19:07,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:07,434][root][INFO] - Training Epoch: 1/2, step 6109/7134 completed (loss: 0.03432012349367142, acc: 0.9872881174087524)
[2025-02-13 03:19:07,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:07,838][root][INFO] - Training Epoch: 1/2, step 6110/7134 completed (loss: 0.10898831486701965, acc: 0.9636752009391785)
[2025-02-13 03:19:07,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:08,215][root][INFO] - Training Epoch: 1/2, step 6111/7134 completed (loss: 0.09779975563287735, acc: 0.9758713245391846)
[2025-02-13 03:19:08,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:08,628][root][INFO] - Training Epoch: 1/2, step 6112/7134 completed (loss: 0.05487498641014099, acc: 0.97826087474823)
[2025-02-13 03:19:08,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:09,046][root][INFO] - Training Epoch: 1/2, step 6113/7134 completed (loss: 0.022196821868419647, acc: 0.9908952713012695)
[2025-02-13 03:19:09,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:09,432][root][INFO] - Training Epoch: 1/2, step 6114/7134 completed (loss: 0.09015173465013504, acc: 0.9735537171363831)
[2025-02-13 03:19:09,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:09,784][root][INFO] - Training Epoch: 1/2, step 6115/7134 completed (loss: 0.06473679095506668, acc: 0.9885057210922241)
[2025-02-13 03:19:09,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:10,112][root][INFO] - Training Epoch: 1/2, step 6116/7134 completed (loss: 0.09411657601594925, acc: 0.9802371263504028)
[2025-02-13 03:19:10,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:10,532][root][INFO] - Training Epoch: 1/2, step 6117/7134 completed (loss: 0.08697802573442459, acc: 0.9863387942314148)
[2025-02-13 03:19:10,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:11,005][root][INFO] - Training Epoch: 1/2, step 6118/7134 completed (loss: 0.08850263804197311, acc: 0.9742646813392639)
[2025-02-13 03:19:11,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:11,443][root][INFO] - Training Epoch: 1/2, step 6119/7134 completed (loss: 0.04452840983867645, acc: 0.9831932783126831)
[2025-02-13 03:19:11,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:11,850][root][INFO] - Training Epoch: 1/2, step 6120/7134 completed (loss: 0.05107531324028969, acc: 0.9874776601791382)
[2025-02-13 03:19:11,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:12,283][root][INFO] - Training Epoch: 1/2, step 6121/7134 completed (loss: 0.0680139809846878, acc: 0.9820144176483154)
[2025-02-13 03:19:12,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:12,714][root][INFO] - Training Epoch: 1/2, step 6122/7134 completed (loss: 0.05262962728738785, acc: 0.9811320900917053)
[2025-02-13 03:19:12,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:13,124][root][INFO] - Training Epoch: 1/2, step 6123/7134 completed (loss: 0.09892179071903229, acc: 0.9792099595069885)
[2025-02-13 03:19:13,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:13,514][root][INFO] - Training Epoch: 1/2, step 6124/7134 completed (loss: 0.03622458875179291, acc: 0.9865384697914124)
[2025-02-13 03:19:13,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:13,958][root][INFO] - Training Epoch: 1/2, step 6125/7134 completed (loss: 0.13564345240592957, acc: 0.9710843563079834)
[2025-02-13 03:19:14,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:14,361][root][INFO] - Training Epoch: 1/2, step 6126/7134 completed (loss: 0.07276149094104767, acc: 0.9843260049819946)
[2025-02-13 03:19:14,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:14,615][root][INFO] - Training Epoch: 1/2, step 6127/7134 completed (loss: 0.030805028975009918, acc: 0.9904761910438538)
[2025-02-13 03:19:14,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:15,018][root][INFO] - Training Epoch: 1/2, step 6128/7134 completed (loss: 0.04635975509881973, acc: 0.9838056564331055)
[2025-02-13 03:19:15,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:15,435][root][INFO] - Training Epoch: 1/2, step 6129/7134 completed (loss: 0.06499681621789932, acc: 0.9789029359817505)
[2025-02-13 03:19:15,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:15,820][root][INFO] - Training Epoch: 1/2, step 6130/7134 completed (loss: 0.04752109944820404, acc: 0.9874551892280579)
[2025-02-13 03:19:15,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:16,260][root][INFO] - Training Epoch: 1/2, step 6131/7134 completed (loss: 0.06426301598548889, acc: 0.9796407222747803)
[2025-02-13 03:19:16,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:16,659][root][INFO] - Training Epoch: 1/2, step 6132/7134 completed (loss: 0.0660616010427475, acc: 0.981249988079071)
[2025-02-13 03:19:16,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:17,111][root][INFO] - Training Epoch: 1/2, step 6133/7134 completed (loss: 0.05567508935928345, acc: 0.9890776872634888)
[2025-02-13 03:19:17,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:17,489][root][INFO] - Training Epoch: 1/2, step 6134/7134 completed (loss: 0.09544533491134644, acc: 0.9754977226257324)
[2025-02-13 03:19:17,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:17,883][root][INFO] - Training Epoch: 1/2, step 6135/7134 completed (loss: 0.02235548384487629, acc: 0.996515691280365)
[2025-02-13 03:19:18,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:18,291][root][INFO] - Training Epoch: 1/2, step 6136/7134 completed (loss: 0.08148534595966339, acc: 0.981792688369751)
[2025-02-13 03:19:18,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:18,685][root][INFO] - Training Epoch: 1/2, step 6137/7134 completed (loss: 0.0923045203089714, acc: 0.9734789133071899)
[2025-02-13 03:19:18,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:19,083][root][INFO] - Training Epoch: 1/2, step 6138/7134 completed (loss: 0.07243850827217102, acc: 0.9846389889717102)
[2025-02-13 03:19:19,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:19,464][root][INFO] - Training Epoch: 1/2, step 6139/7134 completed (loss: 0.06536936014890671, acc: 0.979629635810852)
[2025-02-13 03:19:19,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:19,922][root][INFO] - Training Epoch: 1/2, step 6140/7134 completed (loss: 0.05818267911672592, acc: 0.9876325130462646)
[2025-02-13 03:19:20,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:20,235][root][INFO] - Training Epoch: 1/2, step 6141/7134 completed (loss: 0.11194021254777908, acc: 0.9877675771713257)
[2025-02-13 03:19:20,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:20,652][root][INFO] - Training Epoch: 1/2, step 6142/7134 completed (loss: 0.045429032295942307, acc: 0.9870503544807434)
[2025-02-13 03:19:20,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:21,080][root][INFO] - Training Epoch: 1/2, step 6143/7134 completed (loss: 0.05491891875863075, acc: 0.9878378510475159)
[2025-02-13 03:19:21,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:21,469][root][INFO] - Training Epoch: 1/2, step 6144/7134 completed (loss: 0.03678370639681816, acc: 0.9915966391563416)
[2025-02-13 03:19:21,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:21,909][root][INFO] - Training Epoch: 1/2, step 6145/7134 completed (loss: 0.07783753424882889, acc: 0.9815059304237366)
[2025-02-13 03:19:22,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:22,300][root][INFO] - Training Epoch: 1/2, step 6146/7134 completed (loss: 0.06921838223934174, acc: 0.9834087491035461)
[2025-02-13 03:19:22,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:22,697][root][INFO] - Training Epoch: 1/2, step 6147/7134 completed (loss: 0.05817068740725517, acc: 0.9821428656578064)
[2025-02-13 03:19:22,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:23,139][root][INFO] - Training Epoch: 1/2, step 6148/7134 completed (loss: 0.06942915171384811, acc: 0.979899525642395)
[2025-02-13 03:19:23,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:23,547][root][INFO] - Training Epoch: 1/2, step 6149/7134 completed (loss: 0.08462017774581909, acc: 0.9742765426635742)
[2025-02-13 03:19:23,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:23,959][root][INFO] - Training Epoch: 1/2, step 6150/7134 completed (loss: 0.08065260201692581, acc: 0.9789473414421082)
[2025-02-13 03:19:24,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:24,323][root][INFO] - Training Epoch: 1/2, step 6151/7134 completed (loss: 0.03944820165634155, acc: 0.9858712553977966)
[2025-02-13 03:19:24,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:24,742][root][INFO] - Training Epoch: 1/2, step 6152/7134 completed (loss: 0.03866082802414894, acc: 0.9879336357116699)
[2025-02-13 03:19:24,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:25,182][root][INFO] - Training Epoch: 1/2, step 6153/7134 completed (loss: 0.03125399351119995, acc: 0.9884225726127625)
[2025-02-13 03:19:25,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:25,622][root][INFO] - Training Epoch: 1/2, step 6154/7134 completed (loss: 0.053576815873384476, acc: 0.9832636117935181)
[2025-02-13 03:19:25,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:26,050][root][INFO] - Training Epoch: 1/2, step 6155/7134 completed (loss: 0.08199375122785568, acc: 0.9807121753692627)
[2025-02-13 03:19:26,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:26,435][root][INFO] - Training Epoch: 1/2, step 6156/7134 completed (loss: 0.03502897545695305, acc: 0.9873217344284058)
[2025-02-13 03:19:26,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:26,858][root][INFO] - Training Epoch: 1/2, step 6157/7134 completed (loss: 0.047391440719366074, acc: 0.9858934283256531)
[2025-02-13 03:19:27,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:27,275][root][INFO] - Training Epoch: 1/2, step 6158/7134 completed (loss: 0.08512981235980988, acc: 0.9837251305580139)
[2025-02-13 03:19:27,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:27,715][root][INFO] - Training Epoch: 1/2, step 6159/7134 completed (loss: 0.0819016546010971, acc: 0.9746682643890381)
[2025-02-13 03:19:27,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:28,128][root][INFO] - Training Epoch: 1/2, step 6160/7134 completed (loss: 0.11993040889501572, acc: 0.9681817889213562)
[2025-02-13 03:19:28,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:28,573][root][INFO] - Training Epoch: 1/2, step 6161/7134 completed (loss: 0.0716472715139389, acc: 0.982425332069397)
[2025-02-13 03:19:28,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:29,007][root][INFO] - Training Epoch: 1/2, step 6162/7134 completed (loss: 0.057885732501745224, acc: 0.9852700233459473)
[2025-02-13 03:19:29,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:29,417][root][INFO] - Training Epoch: 1/2, step 6163/7134 completed (loss: 0.10299640893936157, acc: 0.9767857193946838)
[2025-02-13 03:19:29,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:29,859][root][INFO] - Training Epoch: 1/2, step 6164/7134 completed (loss: 0.11464188992977142, acc: 0.9716714024543762)
[2025-02-13 03:19:29,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:30,235][root][INFO] - Training Epoch: 1/2, step 6165/7134 completed (loss: 0.07657046616077423, acc: 0.9778597950935364)
[2025-02-13 03:19:30,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:30,635][root][INFO] - Training Epoch: 1/2, step 6166/7134 completed (loss: 0.0471525639295578, acc: 0.9929701089859009)
[2025-02-13 03:19:30,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:31,040][root][INFO] - Training Epoch: 1/2, step 6167/7134 completed (loss: 0.05250091105699539, acc: 0.9929278492927551)
[2025-02-13 03:19:31,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:31,475][root][INFO] - Training Epoch: 1/2, step 6168/7134 completed (loss: 0.061188697814941406, acc: 0.9858430027961731)
[2025-02-13 03:19:31,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:31,870][root][INFO] - Training Epoch: 1/2, step 6169/7134 completed (loss: 0.07930953800678253, acc: 0.9839650392532349)
[2025-02-13 03:19:31,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:32,257][root][INFO] - Training Epoch: 1/2, step 6170/7134 completed (loss: 0.04023119807243347, acc: 0.9886578321456909)
[2025-02-13 03:19:32,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:32,676][root][INFO] - Training Epoch: 1/2, step 6171/7134 completed (loss: 0.02370387502014637, acc: 0.9934297204017639)
[2025-02-13 03:19:32,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:33,052][root][INFO] - Training Epoch: 1/2, step 6172/7134 completed (loss: 0.04611414670944214, acc: 0.9839679598808289)
[2025-02-13 03:19:33,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:33,435][root][INFO] - Training Epoch: 1/2, step 6173/7134 completed (loss: 0.019601469859480858, acc: 0.9941089749336243)
[2025-02-13 03:19:33,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:33,846][root][INFO] - Training Epoch: 1/2, step 6174/7134 completed (loss: 0.013250576332211494, acc: 0.9956011772155762)
[2025-02-13 03:19:33,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:34,275][root][INFO] - Training Epoch: 1/2, step 6175/7134 completed (loss: 0.053342305123806, acc: 0.9837278127670288)
[2025-02-13 03:19:34,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:34,665][root][INFO] - Training Epoch: 1/2, step 6176/7134 completed (loss: 0.039387159049510956, acc: 0.986975371837616)
[2025-02-13 03:19:34,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:35,082][root][INFO] - Training Epoch: 1/2, step 6177/7134 completed (loss: 0.03310936316847801, acc: 0.9884910583496094)
[2025-02-13 03:19:35,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:35,481][root][INFO] - Training Epoch: 1/2, step 6178/7134 completed (loss: 0.02379651553928852, acc: 0.9955752491950989)
[2025-02-13 03:19:35,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:35,885][root][INFO] - Training Epoch: 1/2, step 6179/7134 completed (loss: 0.061496149748563766, acc: 0.9875776171684265)
[2025-02-13 03:19:36,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:36,231][root][INFO] - Training Epoch: 1/2, step 6180/7134 completed (loss: 0.16035428643226624, acc: 0.9522293210029602)
[2025-02-13 03:19:36,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:36,612][root][INFO] - Training Epoch: 1/2, step 6181/7134 completed (loss: 0.2818085551261902, acc: 0.9297520518302917)
[2025-02-13 03:19:36,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:37,027][root][INFO] - Training Epoch: 1/2, step 6182/7134 completed (loss: 0.060128845274448395, acc: 0.9777365326881409)
[2025-02-13 03:19:37,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:37,444][root][INFO] - Training Epoch: 1/2, step 6183/7134 completed (loss: 0.0695202499628067, acc: 0.9754977226257324)
[2025-02-13 03:19:37,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:37,839][root][INFO] - Training Epoch: 1/2, step 6184/7134 completed (loss: 0.16864849627017975, acc: 0.9495967626571655)
[2025-02-13 03:19:37,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:38,242][root][INFO] - Training Epoch: 1/2, step 6185/7134 completed (loss: 0.05487678572535515, acc: 0.9816176295280457)
[2025-02-13 03:19:38,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:38,639][root][INFO] - Training Epoch: 1/2, step 6186/7134 completed (loss: 0.045521195977926254, acc: 0.9807692170143127)
[2025-02-13 03:19:38,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:39,084][root][INFO] - Training Epoch: 1/2, step 6187/7134 completed (loss: 0.03846089541912079, acc: 0.9907407164573669)
[2025-02-13 03:19:39,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:39,509][root][INFO] - Training Epoch: 1/2, step 6188/7134 completed (loss: 0.10133003443479538, acc: 0.9732016921043396)
[2025-02-13 03:19:39,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:39,957][root][INFO] - Training Epoch: 1/2, step 6189/7134 completed (loss: 0.119381383061409, acc: 0.9610389471054077)
[2025-02-13 03:19:40,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:40,363][root][INFO] - Training Epoch: 1/2, step 6190/7134 completed (loss: 0.134160578250885, acc: 0.9651162624359131)
[2025-02-13 03:19:40,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:40,802][root][INFO] - Training Epoch: 1/2, step 6191/7134 completed (loss: 0.08845870196819305, acc: 0.9702796936035156)
[2025-02-13 03:19:40,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:41,263][root][INFO] - Training Epoch: 1/2, step 6192/7134 completed (loss: 0.05948266759514809, acc: 0.9796355962753296)
[2025-02-13 03:19:41,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:41,698][root][INFO] - Training Epoch: 1/2, step 6193/7134 completed (loss: 0.08413219451904297, acc: 0.9774078726768494)
[2025-02-13 03:19:41,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:42,129][root][INFO] - Training Epoch: 1/2, step 6194/7134 completed (loss: 0.09487537294626236, acc: 0.966926097869873)
[2025-02-13 03:19:42,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:42,515][root][INFO] - Training Epoch: 1/2, step 6195/7134 completed (loss: 0.1061587929725647, acc: 0.9688995480537415)
[2025-02-13 03:19:42,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:42,943][root][INFO] - Training Epoch: 1/2, step 6196/7134 completed (loss: 0.0667325034737587, acc: 0.9779614210128784)
[2025-02-13 03:19:43,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:43,374][root][INFO] - Training Epoch: 1/2, step 6197/7134 completed (loss: 0.12202004343271255, acc: 0.9710344672203064)
[2025-02-13 03:19:43,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:43,770][root][INFO] - Training Epoch: 1/2, step 6198/7134 completed (loss: 0.046234529465436935, acc: 0.9875518679618835)
[2025-02-13 03:19:43,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:44,206][root][INFO] - Training Epoch: 1/2, step 6199/7134 completed (loss: 0.06756523996591568, acc: 0.982598602771759)
[2025-02-13 03:19:44,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:44,638][root][INFO] - Training Epoch: 1/2, step 6200/7134 completed (loss: 0.060314811766147614, acc: 0.9792683124542236)
[2025-02-13 03:19:44,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:45,087][root][INFO] - Training Epoch: 1/2, step 6201/7134 completed (loss: 0.03621067851781845, acc: 0.9933110475540161)
[2025-02-13 03:19:45,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:45,484][root][INFO] - Training Epoch: 1/2, step 6202/7134 completed (loss: 0.05972318723797798, acc: 0.9848155975341797)
[2025-02-13 03:19:45,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:45,916][root][INFO] - Training Epoch: 1/2, step 6203/7134 completed (loss: 0.03457516059279442, acc: 0.9872449040412903)
[2025-02-13 03:19:46,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:46,333][root][INFO] - Training Epoch: 1/2, step 6204/7134 completed (loss: 0.03871876746416092, acc: 0.9960707426071167)
[2025-02-13 03:19:46,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:46,741][root][INFO] - Training Epoch: 1/2, step 6205/7134 completed (loss: 0.038054775446653366, acc: 0.988054633140564)
[2025-02-13 03:19:46,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:47,126][root][INFO] - Training Epoch: 1/2, step 6206/7134 completed (loss: 0.04671812430024147, acc: 0.9898697733879089)
[2025-02-13 03:19:47,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:47,570][root][INFO] - Training Epoch: 1/2, step 6207/7134 completed (loss: 0.15410982072353363, acc: 0.9666221737861633)
[2025-02-13 03:19:47,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:47,990][root][INFO] - Training Epoch: 1/2, step 6208/7134 completed (loss: 0.05218261107802391, acc: 0.9849537014961243)
[2025-02-13 03:19:48,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:48,431][root][INFO] - Training Epoch: 1/2, step 6209/7134 completed (loss: 0.09855400025844574, acc: 0.9754204154014587)
[2025-02-13 03:19:48,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:48,885][root][INFO] - Training Epoch: 1/2, step 6210/7134 completed (loss: 0.06292535364627838, acc: 0.9791208505630493)
[2025-02-13 03:19:49,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:49,328][root][INFO] - Training Epoch: 1/2, step 6211/7134 completed (loss: 0.061025261878967285, acc: 0.981055498123169)
[2025-02-13 03:19:49,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:49,727][root][INFO] - Training Epoch: 1/2, step 6212/7134 completed (loss: 0.07903482019901276, acc: 0.9754385948181152)
[2025-02-13 03:19:49,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:50,112][root][INFO] - Training Epoch: 1/2, step 6213/7134 completed (loss: 0.031688135117292404, acc: 0.9896050095558167)
[2025-02-13 03:19:50,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:50,540][root][INFO] - Training Epoch: 1/2, step 6214/7134 completed (loss: 0.04546304792165756, acc: 0.9871630072593689)
[2025-02-13 03:19:50,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:50,983][root][INFO] - Training Epoch: 1/2, step 6215/7134 completed (loss: 0.07206286489963531, acc: 0.9746543765068054)
[2025-02-13 03:19:51,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:51,285][root][INFO] - Training Epoch: 1/2, step 6216/7134 completed (loss: 0.05404208227992058, acc: 0.9808510541915894)
[2025-02-13 03:19:51,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:51,719][root][INFO] - Training Epoch: 1/2, step 6217/7134 completed (loss: 0.036299560219049454, acc: 0.9879194498062134)
[2025-02-13 03:19:51,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:52,165][root][INFO] - Training Epoch: 1/2, step 6218/7134 completed (loss: 0.05675689876079559, acc: 0.9832317233085632)
[2025-02-13 03:19:52,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:52,591][root][INFO] - Training Epoch: 1/2, step 6219/7134 completed (loss: 0.03526921570301056, acc: 0.9887482523918152)
[2025-02-13 03:19:52,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:52,964][root][INFO] - Training Epoch: 1/2, step 6220/7134 completed (loss: 0.08354273438453674, acc: 0.9714285731315613)
[2025-02-13 03:19:53,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:53,373][root][INFO] - Training Epoch: 1/2, step 6221/7134 completed (loss: 0.0295892171561718, acc: 0.9894737005233765)
[2025-02-13 03:19:53,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:53,752][root][INFO] - Training Epoch: 1/2, step 6222/7134 completed (loss: 0.06671632081270218, acc: 0.9822404384613037)
[2025-02-13 03:19:53,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:54,181][root][INFO] - Training Epoch: 1/2, step 6223/7134 completed (loss: 0.056142497807741165, acc: 0.9841269850730896)
[2025-02-13 03:19:54,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:54,594][root][INFO] - Training Epoch: 1/2, step 6224/7134 completed (loss: 0.02653229609131813, acc: 0.994194507598877)
[2025-02-13 03:19:54,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:55,038][root][INFO] - Training Epoch: 1/2, step 6225/7134 completed (loss: 0.03420761227607727, acc: 0.9885203838348389)
[2025-02-13 03:19:55,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:55,445][root][INFO] - Training Epoch: 1/2, step 6226/7134 completed (loss: 0.05214591324329376, acc: 0.9829620122909546)
[2025-02-13 03:19:55,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:55,875][root][INFO] - Training Epoch: 1/2, step 6227/7134 completed (loss: 0.023127472028136253, acc: 0.9892904758453369)
[2025-02-13 03:19:56,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:56,310][root][INFO] - Training Epoch: 1/2, step 6228/7134 completed (loss: 0.0346941202878952, acc: 0.9864864945411682)
[2025-02-13 03:19:56,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:56,752][root][INFO] - Training Epoch: 1/2, step 6229/7134 completed (loss: 0.07211777567863464, acc: 0.9814385175704956)
[2025-02-13 03:19:56,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:57,192][root][INFO] - Training Epoch: 1/2, step 6230/7134 completed (loss: 0.04261186718940735, acc: 0.9875311851501465)
[2025-02-13 03:19:57,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:57,582][root][INFO] - Training Epoch: 1/2, step 6231/7134 completed (loss: 0.03863675147294998, acc: 0.985788106918335)
[2025-02-13 03:19:57,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:58,032][root][INFO] - Training Epoch: 1/2, step 6232/7134 completed (loss: 0.026825878769159317, acc: 0.9873737096786499)
[2025-02-13 03:19:58,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:58,464][root][INFO] - Training Epoch: 1/2, step 6233/7134 completed (loss: 0.025939151644706726, acc: 0.9919571280479431)
[2025-02-13 03:19:58,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:58,899][root][INFO] - Training Epoch: 1/2, step 6234/7134 completed (loss: 0.06221694126725197, acc: 0.9826202988624573)
[2025-02-13 03:19:59,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:59,307][root][INFO] - Training Epoch: 1/2, step 6235/7134 completed (loss: 0.03404872119426727, acc: 0.9960886836051941)
[2025-02-13 03:19:59,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:59,738][root][INFO] - Training Epoch: 1/2, step 6236/7134 completed (loss: 0.046154603362083435, acc: 0.9863574504852295)
[2025-02-13 03:19:59,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:00,159][root][INFO] - Training Epoch: 1/2, step 6237/7134 completed (loss: 0.037815771996974945, acc: 0.9862385392189026)
[2025-02-13 03:20:00,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:00,551][root][INFO] - Training Epoch: 1/2, step 6238/7134 completed (loss: 0.015931041911244392, acc: 0.9951534867286682)
[2025-02-13 03:20:00,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:00,996][root][INFO] - Training Epoch: 1/2, step 6239/7134 completed (loss: 0.03949430212378502, acc: 0.9916368126869202)
[2025-02-13 03:20:01,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:01,423][root][INFO] - Training Epoch: 1/2, step 6240/7134 completed (loss: 0.006794403772801161, acc: 1.0)
[2025-02-13 03:20:01,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:01,823][root][INFO] - Training Epoch: 1/2, step 6241/7134 completed (loss: 0.015274539589881897, acc: 0.9936708807945251)
[2025-02-13 03:20:01,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:02,235][root][INFO] - Training Epoch: 1/2, step 6242/7134 completed (loss: 0.023085201159119606, acc: 0.9914966225624084)
[2025-02-13 03:20:02,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:02,640][root][INFO] - Training Epoch: 1/2, step 6243/7134 completed (loss: 0.024640515446662903, acc: 0.9904458522796631)
[2025-02-13 03:20:02,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:03,048][root][INFO] - Training Epoch: 1/2, step 6244/7134 completed (loss: 0.026033639907836914, acc: 0.991769552230835)
[2025-02-13 03:20:03,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:03,459][root][INFO] - Training Epoch: 1/2, step 6245/7134 completed (loss: 0.022767435759305954, acc: 0.9931600689888)
[2025-02-13 03:20:03,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:03,891][root][INFO] - Training Epoch: 1/2, step 6246/7134 completed (loss: 0.0406171977519989, acc: 0.9828721880912781)
[2025-02-13 03:20:04,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:04,333][root][INFO] - Training Epoch: 1/2, step 6247/7134 completed (loss: 0.05062751844525337, acc: 0.9807229042053223)
[2025-02-13 03:20:04,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:04,750][root][INFO] - Training Epoch: 1/2, step 6248/7134 completed (loss: 0.04881823807954788, acc: 0.9844236969947815)
[2025-02-13 03:20:04,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:05,136][root][INFO] - Training Epoch: 1/2, step 6249/7134 completed (loss: 0.007548232562839985, acc: 0.9968553185462952)
[2025-02-13 03:20:05,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:05,533][root][INFO] - Training Epoch: 1/2, step 6250/7134 completed (loss: 0.019420385360717773, acc: 0.9965986609458923)
[2025-02-13 03:20:05,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:05,947][root][INFO] - Training Epoch: 1/2, step 6251/7134 completed (loss: 0.024176672101020813, acc: 0.9923312664031982)
[2025-02-13 03:20:06,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:06,351][root][INFO] - Training Epoch: 1/2, step 6252/7134 completed (loss: 0.027033518999814987, acc: 0.9935275316238403)
[2025-02-13 03:20:06,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:06,752][root][INFO] - Training Epoch: 1/2, step 6253/7134 completed (loss: 0.037889327853918076, acc: 0.9913793206214905)
[2025-02-13 03:20:06,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:07,145][root][INFO] - Training Epoch: 1/2, step 6254/7134 completed (loss: 0.028393445536494255, acc: 0.991349458694458)
[2025-02-13 03:20:07,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:07,560][root][INFO] - Training Epoch: 1/2, step 6255/7134 completed (loss: 0.02730954997241497, acc: 0.9905149340629578)
[2025-02-13 03:20:07,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:07,959][root][INFO] - Training Epoch: 1/2, step 6256/7134 completed (loss: 0.01695621758699417, acc: 0.9900709390640259)
[2025-02-13 03:20:08,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:08,372][root][INFO] - Training Epoch: 1/2, step 6257/7134 completed (loss: 0.025300798937678337, acc: 0.9899280667304993)
[2025-02-13 03:20:08,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:08,754][root][INFO] - Training Epoch: 1/2, step 6258/7134 completed (loss: 0.018076526001095772, acc: 0.995199978351593)
[2025-02-13 03:20:08,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:09,171][root][INFO] - Training Epoch: 1/2, step 6259/7134 completed (loss: 0.019766399636864662, acc: 0.9894894957542419)
[2025-02-13 03:20:09,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:09,562][root][INFO] - Training Epoch: 1/2, step 6260/7134 completed (loss: 0.032894667237997055, acc: 0.9916107654571533)
[2025-02-13 03:20:09,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:09,985][root][INFO] - Training Epoch: 1/2, step 6261/7134 completed (loss: 0.029905682429671288, acc: 0.9908536672592163)
[2025-02-13 03:20:10,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:10,385][root][INFO] - Training Epoch: 1/2, step 6262/7134 completed (loss: 0.007888301275670528, acc: 0.9985652565956116)
[2025-02-13 03:20:10,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:10,781][root][INFO] - Training Epoch: 1/2, step 6263/7134 completed (loss: 0.013601507991552353, acc: 0.9970717430114746)
[2025-02-13 03:20:10,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:11,188][root][INFO] - Training Epoch: 1/2, step 6264/7134 completed (loss: 0.013020969927310944, acc: 0.9967159032821655)
[2025-02-13 03:20:11,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:11,613][root][INFO] - Training Epoch: 1/2, step 6265/7134 completed (loss: 0.017740923911333084, acc: 0.9926035404205322)
[2025-02-13 03:20:11,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:12,009][root][INFO] - Training Epoch: 1/2, step 6266/7134 completed (loss: 0.011551178060472012, acc: 0.996503472328186)
[2025-02-13 03:20:12,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:12,411][root][INFO] - Training Epoch: 1/2, step 6267/7134 completed (loss: 0.02005779929459095, acc: 0.9983713626861572)
[2025-02-13 03:20:12,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:12,835][root][INFO] - Training Epoch: 1/2, step 6268/7134 completed (loss: 0.025428153574466705, acc: 0.9970238208770752)
[2025-02-13 03:20:12,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:13,240][root][INFO] - Training Epoch: 1/2, step 6269/7134 completed (loss: 0.02974855527281761, acc: 0.9957355856895447)
[2025-02-13 03:20:13,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:13,647][root][INFO] - Training Epoch: 1/2, step 6270/7134 completed (loss: 0.020511282607913017, acc: 0.995398759841919)
[2025-02-13 03:20:13,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:14,067][root][INFO] - Training Epoch: 1/2, step 6271/7134 completed (loss: 0.021786917001008987, acc: 0.9959893226623535)
[2025-02-13 03:20:14,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:14,518][root][INFO] - Training Epoch: 1/2, step 6272/7134 completed (loss: 0.0054664914496243, acc: 0.9985315799713135)
[2025-02-13 03:20:14,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:14,959][root][INFO] - Training Epoch: 1/2, step 6273/7134 completed (loss: 0.011281386949121952, acc: 0.99863201379776)
[2025-02-13 03:20:15,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:15,364][root][INFO] - Training Epoch: 1/2, step 6274/7134 completed (loss: 0.029839323833584785, acc: 0.9914407730102539)
[2025-02-13 03:20:15,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:15,774][root][INFO] - Training Epoch: 1/2, step 6275/7134 completed (loss: 0.03514917939901352, acc: 0.9923896789550781)
[2025-02-13 03:20:15,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:16,135][root][INFO] - Training Epoch: 1/2, step 6276/7134 completed (loss: 0.031110549345612526, acc: 0.9877408146858215)
[2025-02-13 03:20:16,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:16,500][root][INFO] - Training Epoch: 1/2, step 6277/7134 completed (loss: 0.10583309084177017, acc: 0.9728506803512573)
[2025-02-13 03:20:16,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:16,894][root][INFO] - Training Epoch: 1/2, step 6278/7134 completed (loss: 0.018535234034061432, acc: 0.9934210777282715)
[2025-02-13 03:20:17,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:17,288][root][INFO] - Training Epoch: 1/2, step 6279/7134 completed (loss: 0.010073427110910416, acc: 0.9982876777648926)
[2025-02-13 03:20:17,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:17,684][root][INFO] - Training Epoch: 1/2, step 6280/7134 completed (loss: 0.03140955790877342, acc: 0.9930192232131958)
[2025-02-13 03:20:17,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:18,082][root][INFO] - Training Epoch: 1/2, step 6281/7134 completed (loss: 0.03377159684896469, acc: 0.9930675625801086)
[2025-02-13 03:20:18,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:18,497][root][INFO] - Training Epoch: 1/2, step 6282/7134 completed (loss: 0.014261601492762566, acc: 0.9937106966972351)
[2025-02-13 03:20:18,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:18,901][root][INFO] - Training Epoch: 1/2, step 6283/7134 completed (loss: 0.0404045470058918, acc: 0.9890829920768738)
[2025-02-13 03:20:19,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:19,291][root][INFO] - Training Epoch: 1/2, step 6284/7134 completed (loss: 0.05287062004208565, acc: 0.9803063273429871)
[2025-02-13 03:20:19,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:19,644][root][INFO] - Training Epoch: 1/2, step 6285/7134 completed (loss: 0.022702930495142937, acc: 0.9947506785392761)
[2025-02-13 03:20:19,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:20,033][root][INFO] - Training Epoch: 1/2, step 6286/7134 completed (loss: 0.020335514098405838, acc: 0.9950658082962036)
[2025-02-13 03:20:20,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:20,437][root][INFO] - Training Epoch: 1/2, step 6287/7134 completed (loss: 0.04405542463064194, acc: 0.9852941036224365)
[2025-02-13 03:20:20,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:20,835][root][INFO] - Training Epoch: 1/2, step 6288/7134 completed (loss: 0.0130022456869483, acc: 0.9967319965362549)
[2025-02-13 03:20:20,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:21,207][root][INFO] - Training Epoch: 1/2, step 6289/7134 completed (loss: 0.029069598764181137, acc: 0.9927927851676941)
[2025-02-13 03:20:21,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:21,616][root][INFO] - Training Epoch: 1/2, step 6290/7134 completed (loss: 0.03251604735851288, acc: 0.9904000163078308)
[2025-02-13 03:20:21,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:21,991][root][INFO] - Training Epoch: 1/2, step 6291/7134 completed (loss: 0.0151021433994174, acc: 0.996363639831543)
[2025-02-13 03:20:22,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:22,338][root][INFO] - Training Epoch: 1/2, step 6292/7134 completed (loss: 0.052238985896110535, acc: 0.984054684638977)
[2025-02-13 03:20:22,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:22,754][root][INFO] - Training Epoch: 1/2, step 6293/7134 completed (loss: 0.06116533651947975, acc: 0.9812286496162415)
[2025-02-13 03:20:22,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:23,174][root][INFO] - Training Epoch: 1/2, step 6294/7134 completed (loss: 0.03156230226159096, acc: 0.990275502204895)
[2025-02-13 03:20:23,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:23,569][root][INFO] - Training Epoch: 1/2, step 6295/7134 completed (loss: 0.029413430020213127, acc: 0.9891501069068909)
[2025-02-13 03:20:23,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:23,929][root][INFO] - Training Epoch: 1/2, step 6296/7134 completed (loss: 0.05623539164662361, acc: 0.9842519760131836)
[2025-02-13 03:20:24,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:24,332][root][INFO] - Training Epoch: 1/2, step 6297/7134 completed (loss: 0.03866797313094139, acc: 0.9819672107696533)
[2025-02-13 03:20:24,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:24,732][root][INFO] - Training Epoch: 1/2, step 6298/7134 completed (loss: 0.04740414023399353, acc: 0.9793814420700073)
[2025-02-13 03:20:24,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:25,143][root][INFO] - Training Epoch: 1/2, step 6299/7134 completed (loss: 0.07097704708576202, acc: 0.9808306694030762)
[2025-02-13 03:20:25,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:25,547][root][INFO] - Training Epoch: 1/2, step 6300/7134 completed (loss: 0.049933768808841705, acc: 0.9832496047019958)
[2025-02-13 03:20:25,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:25,995][root][INFO] - Training Epoch: 1/2, step 6301/7134 completed (loss: 0.011961931362748146, acc: 0.9977011680603027)
[2025-02-13 03:20:26,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:26,422][root][INFO] - Training Epoch: 1/2, step 6302/7134 completed (loss: 0.021525098010897636, acc: 0.9917126893997192)
[2025-02-13 03:20:26,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:26,888][root][INFO] - Training Epoch: 1/2, step 6303/7134 completed (loss: 0.016976431012153625, acc: 0.9957401752471924)
[2025-02-13 03:20:27,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:27,323][root][INFO] - Training Epoch: 1/2, step 6304/7134 completed (loss: 0.023474812507629395, acc: 0.9918919205665588)
[2025-02-13 03:20:27,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:27,749][root][INFO] - Training Epoch: 1/2, step 6305/7134 completed (loss: 0.022929396480321884, acc: 0.9934533834457397)
[2025-02-13 03:20:27,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:28,188][root][INFO] - Training Epoch: 1/2, step 6306/7134 completed (loss: 0.05575481802225113, acc: 0.9879952073097229)
[2025-02-13 03:20:28,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:28,621][root][INFO] - Training Epoch: 1/2, step 6307/7134 completed (loss: 0.07119479775428772, acc: 0.9841269850730896)
[2025-02-13 03:20:28,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:29,052][root][INFO] - Training Epoch: 1/2, step 6308/7134 completed (loss: 0.021861238405108452, acc: 0.9950860142707825)
[2025-02-13 03:20:29,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:29,459][root][INFO] - Training Epoch: 1/2, step 6309/7134 completed (loss: 0.016826853156089783, acc: 0.9936000108718872)
[2025-02-13 03:20:29,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:29,829][root][INFO] - Training Epoch: 1/2, step 6310/7134 completed (loss: 0.0956989973783493, acc: 0.9780219793319702)
[2025-02-13 03:20:29,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:30,263][root][INFO] - Training Epoch: 1/2, step 6311/7134 completed (loss: 0.046927716583013535, acc: 0.9864698648452759)
[2025-02-13 03:20:30,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:30,667][root][INFO] - Training Epoch: 1/2, step 6312/7134 completed (loss: 0.04885612800717354, acc: 0.9842767119407654)
[2025-02-13 03:20:30,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:31,098][root][INFO] - Training Epoch: 1/2, step 6313/7134 completed (loss: 0.016911234706640244, acc: 0.9926380515098572)
[2025-02-13 03:20:31,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:31,546][root][INFO] - Training Epoch: 1/2, step 6314/7134 completed (loss: 0.014752059243619442, acc: 0.996216893196106)
[2025-02-13 03:20:31,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:31,970][root][INFO] - Training Epoch: 1/2, step 6315/7134 completed (loss: 0.039567649364471436, acc: 0.9934425950050354)
[2025-02-13 03:20:32,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:32,395][root][INFO] - Training Epoch: 1/2, step 6316/7134 completed (loss: 0.03440926596522331, acc: 0.990275502204895)
[2025-02-13 03:20:32,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:32,781][root][INFO] - Training Epoch: 1/2, step 6317/7134 completed (loss: 0.018143244087696075, acc: 0.993537962436676)
[2025-02-13 03:20:32,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:33,187][root][INFO] - Training Epoch: 1/2, step 6318/7134 completed (loss: 0.017504174262285233, acc: 0.9920508861541748)
[2025-02-13 03:20:33,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:33,640][root][INFO] - Training Epoch: 1/2, step 6319/7134 completed (loss: 0.013060935772955418, acc: 0.9969135522842407)
[2025-02-13 03:20:33,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:34,105][root][INFO] - Training Epoch: 1/2, step 6320/7134 completed (loss: 0.022063735872507095, acc: 0.9921011328697205)
[2025-02-13 03:20:34,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:34,548][root][INFO] - Training Epoch: 1/2, step 6321/7134 completed (loss: 0.012462952174246311, acc: 0.9933035969734192)
[2025-02-13 03:20:34,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:34,988][root][INFO] - Training Epoch: 1/2, step 6322/7134 completed (loss: 0.03750242665410042, acc: 0.9941860437393188)
[2025-02-13 03:20:35,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:35,435][root][INFO] - Training Epoch: 1/2, step 6323/7134 completed (loss: 0.033157236874103546, acc: 0.9894894957542419)
[2025-02-13 03:20:35,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:35,863][root][INFO] - Training Epoch: 1/2, step 6324/7134 completed (loss: 0.021609097719192505, acc: 0.9946737885475159)
[2025-02-13 03:20:35,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:36,251][root][INFO] - Training Epoch: 1/2, step 6325/7134 completed (loss: 0.04117852821946144, acc: 0.9886792302131653)
[2025-02-13 03:20:36,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:36,664][root][INFO] - Training Epoch: 1/2, step 6326/7134 completed (loss: 0.061494115740060806, acc: 0.9839486479759216)
[2025-02-13 03:20:36,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:37,068][root][INFO] - Training Epoch: 1/2, step 6327/7134 completed (loss: 0.0358537994325161, acc: 0.9894459247589111)
[2025-02-13 03:20:37,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:37,481][root][INFO] - Training Epoch: 1/2, step 6328/7134 completed (loss: 0.026665782555937767, acc: 0.9904305934906006)
[2025-02-13 03:20:37,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:37,890][root][INFO] - Training Epoch: 1/2, step 6329/7134 completed (loss: 0.03952183946967125, acc: 0.9878787994384766)
[2025-02-13 03:20:38,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:38,288][root][INFO] - Training Epoch: 1/2, step 6330/7134 completed (loss: 0.02704259566962719, acc: 0.9925558567047119)
[2025-02-13 03:20:38,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:38,694][root][INFO] - Training Epoch: 1/2, step 6331/7134 completed (loss: 0.017998114228248596, acc: 0.9946996569633484)
[2025-02-13 03:20:38,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:39,116][root][INFO] - Training Epoch: 1/2, step 6332/7134 completed (loss: 0.09530661255121231, acc: 0.9776951670646667)
[2025-02-13 03:20:39,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:39,519][root][INFO] - Training Epoch: 1/2, step 6333/7134 completed (loss: 0.04901667311787605, acc: 0.9809523820877075)
[2025-02-13 03:20:39,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:39,870][root][INFO] - Training Epoch: 1/2, step 6334/7134 completed (loss: 0.03809390589594841, acc: 0.9810606241226196)
[2025-02-13 03:20:40,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:40,268][root][INFO] - Training Epoch: 1/2, step 6335/7134 completed (loss: 0.014907331205904484, acc: 0.9932885766029358)
[2025-02-13 03:20:40,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:40,661][root][INFO] - Training Epoch: 1/2, step 6336/7134 completed (loss: 0.03766947612166405, acc: 0.9885321259498596)
[2025-02-13 03:20:40,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:41,079][root][INFO] - Training Epoch: 1/2, step 6337/7134 completed (loss: 0.015743765980005264, acc: 0.9963503479957581)
[2025-02-13 03:20:41,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:41,496][root][INFO] - Training Epoch: 1/2, step 6338/7134 completed (loss: 0.050488926470279694, acc: 0.9842022061347961)
[2025-02-13 03:20:41,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:41,897][root][INFO] - Training Epoch: 1/2, step 6339/7134 completed (loss: 0.04841439425945282, acc: 0.9844290614128113)
[2025-02-13 03:20:42,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:42,308][root][INFO] - Training Epoch: 1/2, step 6340/7134 completed (loss: 0.016641858965158463, acc: 0.9941349029541016)
[2025-02-13 03:20:42,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:42,701][root][INFO] - Training Epoch: 1/2, step 6341/7134 completed (loss: 0.027851860970258713, acc: 0.992409884929657)
[2025-02-13 03:20:42,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:43,149][root][INFO] - Training Epoch: 1/2, step 6342/7134 completed (loss: 0.025998881086707115, acc: 0.9932705163955688)
[2025-02-13 03:20:43,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:43,575][root][INFO] - Training Epoch: 1/2, step 6343/7134 completed (loss: 0.06458888202905655, acc: 0.9862306118011475)
[2025-02-13 03:20:43,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:44,023][root][INFO] - Training Epoch: 1/2, step 6344/7134 completed (loss: 0.02811416983604431, acc: 0.9890561103820801)
[2025-02-13 03:20:44,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:44,449][root][INFO] - Training Epoch: 1/2, step 6345/7134 completed (loss: 0.04155557602643967, acc: 0.9849726557731628)
[2025-02-13 03:20:44,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:44,857][root][INFO] - Training Epoch: 1/2, step 6346/7134 completed (loss: 0.06945563107728958, acc: 0.9798761606216431)
[2025-02-13 03:20:44,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:45,200][root][INFO] - Training Epoch: 1/2, step 6347/7134 completed (loss: 0.047833893448114395, acc: 0.9848484992980957)
[2025-02-13 03:20:45,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:45,610][root][INFO] - Training Epoch: 1/2, step 6348/7134 completed (loss: 0.09251044690608978, acc: 0.9756944179534912)
[2025-02-13 03:20:45,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:46,008][root][INFO] - Training Epoch: 1/2, step 6349/7134 completed (loss: 0.03965909406542778, acc: 0.9854280352592468)
[2025-02-13 03:20:46,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:46,426][root][INFO] - Training Epoch: 1/2, step 6350/7134 completed (loss: 0.05158056691288948, acc: 0.9809523820877075)
[2025-02-13 03:20:46,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:46,860][root][INFO] - Training Epoch: 1/2, step 6351/7134 completed (loss: 0.061703797429800034, acc: 0.9805970191955566)
[2025-02-13 03:20:47,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:47,281][root][INFO] - Training Epoch: 1/2, step 6352/7134 completed (loss: 0.1270093023777008, acc: 0.9673105478286743)
[2025-02-13 03:20:47,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:47,742][root][INFO] - Training Epoch: 1/2, step 6353/7134 completed (loss: 0.0363713763654232, acc: 0.9912152290344238)
[2025-02-13 03:20:47,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:48,166][root][INFO] - Training Epoch: 1/2, step 6354/7134 completed (loss: 0.03149622678756714, acc: 0.9930747747421265)
[2025-02-13 03:20:48,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:48,571][root][INFO] - Training Epoch: 1/2, step 6355/7134 completed (loss: 0.02956247702240944, acc: 0.9902912378311157)
[2025-02-13 03:20:48,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:48,974][root][INFO] - Training Epoch: 1/2, step 6356/7134 completed (loss: 0.04111455753445625, acc: 0.9910314083099365)
[2025-02-13 03:20:49,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:49,396][root][INFO] - Training Epoch: 1/2, step 6357/7134 completed (loss: 0.014643675647675991, acc: 0.9958333373069763)
[2025-02-13 03:20:49,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:49,847][root][INFO] - Training Epoch: 1/2, step 6358/7134 completed (loss: 0.03684113547205925, acc: 0.9943661689758301)
[2025-02-13 03:20:49,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:50,250][root][INFO] - Training Epoch: 1/2, step 6359/7134 completed (loss: 0.06275931745767593, acc: 0.9845938086509705)
[2025-02-13 03:20:50,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:50,646][root][INFO] - Training Epoch: 1/2, step 6360/7134 completed (loss: 0.033830687403678894, acc: 0.9920381903648376)
[2025-02-13 03:20:50,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:51,050][root][INFO] - Training Epoch: 1/2, step 6361/7134 completed (loss: 0.06116902455687523, acc: 0.9817517995834351)
[2025-02-13 03:20:51,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:51,490][root][INFO] - Training Epoch: 1/2, step 6362/7134 completed (loss: 0.042302455753088, acc: 0.9893292784690857)
[2025-02-13 03:20:51,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:51,907][root][INFO] - Training Epoch: 1/2, step 6363/7134 completed (loss: 0.032668232917785645, acc: 0.9882698059082031)
[2025-02-13 03:20:52,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:52,319][root][INFO] - Training Epoch: 1/2, step 6364/7134 completed (loss: 0.020803608000278473, acc: 0.991416335105896)
[2025-02-13 03:20:52,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:52,725][root][INFO] - Training Epoch: 1/2, step 6365/7134 completed (loss: 0.02918563410639763, acc: 0.9898374080657959)
[2025-02-13 03:20:52,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:53,150][root][INFO] - Training Epoch: 1/2, step 6366/7134 completed (loss: 0.04783470556139946, acc: 0.9841479659080505)
[2025-02-13 03:20:53,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:53,542][root][INFO] - Training Epoch: 1/2, step 6367/7134 completed (loss: 0.06770482659339905, acc: 0.9848771095275879)
[2025-02-13 03:20:53,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:53,963][root][INFO] - Training Epoch: 1/2, step 6368/7134 completed (loss: 0.0653441846370697, acc: 0.9872881174087524)
[2025-02-13 03:20:54,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:54,402][root][INFO] - Training Epoch: 1/2, step 6369/7134 completed (loss: 0.06043703109025955, acc: 0.9855453372001648)
[2025-02-13 03:20:54,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:54,851][root][INFO] - Training Epoch: 1/2, step 6370/7134 completed (loss: 0.13725627958774567, acc: 0.9696570038795471)
[2025-02-13 03:20:54,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:55,263][root][INFO] - Training Epoch: 1/2, step 6371/7134 completed (loss: 0.07189898192882538, acc: 0.9736841917037964)
[2025-02-13 03:20:55,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:55,652][root][INFO] - Training Epoch: 1/2, step 6372/7134 completed (loss: 0.141977459192276, acc: 0.9558541178703308)
[2025-02-13 03:20:55,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:56,025][root][INFO] - Training Epoch: 1/2, step 6373/7134 completed (loss: 0.07822506129741669, acc: 0.9763205647468567)
[2025-02-13 03:20:56,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:56,447][root][INFO] - Training Epoch: 1/2, step 6374/7134 completed (loss: 0.02166343480348587, acc: 0.9958506226539612)
[2025-02-13 03:20:56,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:56,885][root][INFO] - Training Epoch: 1/2, step 6375/7134 completed (loss: 0.05425841361284256, acc: 0.9833119511604309)
[2025-02-13 03:20:57,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:57,274][root][INFO] - Training Epoch: 1/2, step 6376/7134 completed (loss: 0.026218686252832413, acc: 0.9963099360466003)
[2025-02-13 03:20:57,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:57,655][root][INFO] - Training Epoch: 1/2, step 6377/7134 completed (loss: 0.03699469193816185, acc: 0.9853479862213135)
[2025-02-13 03:20:57,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:58,114][root][INFO] - Training Epoch: 1/2, step 6378/7134 completed (loss: 0.06794348359107971, acc: 0.9785637855529785)
[2025-02-13 03:20:58,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:58,509][root][INFO] - Training Epoch: 1/2, step 6379/7134 completed (loss: 0.05592462047934532, acc: 0.9835025668144226)
[2025-02-13 03:20:58,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:58,964][root][INFO] - Training Epoch: 1/2, step 6380/7134 completed (loss: 0.06093517318367958, acc: 0.9865269660949707)
[2025-02-13 03:20:59,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:59,385][root][INFO] - Training Epoch: 1/2, step 6381/7134 completed (loss: 0.03505502641201019, acc: 0.9908397197723389)
[2025-02-13 03:20:59,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:59,828][root][INFO] - Training Epoch: 1/2, step 6382/7134 completed (loss: 0.01879735477268696, acc: 0.9947437644004822)
[2025-02-13 03:20:59,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:00,261][root][INFO] - Training Epoch: 1/2, step 6383/7134 completed (loss: 0.06233728677034378, acc: 0.9891451597213745)
[2025-02-13 03:21:00,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:00,698][root][INFO] - Training Epoch: 1/2, step 6384/7134 completed (loss: 0.05676836147904396, acc: 0.988041877746582)
[2025-02-13 03:21:00,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:01,122][root][INFO] - Training Epoch: 1/2, step 6385/7134 completed (loss: 0.07448849081993103, acc: 0.9892215728759766)
[2025-02-13 03:21:01,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:01,593][root][INFO] - Training Epoch: 1/2, step 6386/7134 completed (loss: 0.024410635232925415, acc: 0.9951159954071045)
[2025-02-13 03:21:01,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:02,022][root][INFO] - Training Epoch: 1/2, step 6387/7134 completed (loss: 0.07169509679079056, acc: 0.9822580814361572)
[2025-02-13 03:21:02,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:02,432][root][INFO] - Training Epoch: 1/2, step 6388/7134 completed (loss: 0.06829185038805008, acc: 0.985029935836792)
[2025-02-13 03:21:02,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:02,887][root][INFO] - Training Epoch: 1/2, step 6389/7134 completed (loss: 0.06409425288438797, acc: 0.9841269850730896)
[2025-02-13 03:21:03,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:03,365][root][INFO] - Training Epoch: 1/2, step 6390/7134 completed (loss: 0.06867679953575134, acc: 0.98886638879776)
[2025-02-13 03:21:03,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:03,791][root][INFO] - Training Epoch: 1/2, step 6391/7134 completed (loss: 0.07317861169576645, acc: 0.984635055065155)
[2025-02-13 03:21:03,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:04,238][root][INFO] - Training Epoch: 1/2, step 6392/7134 completed (loss: 0.07318375259637833, acc: 0.983589768409729)
[2025-02-13 03:21:04,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:04,692][root][INFO] - Training Epoch: 1/2, step 6393/7134 completed (loss: 0.03449565917253494, acc: 0.9886234402656555)
[2025-02-13 03:21:04,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:05,137][root][INFO] - Training Epoch: 1/2, step 6394/7134 completed (loss: 0.07464035600423813, acc: 0.9805285334587097)
[2025-02-13 03:21:05,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:05,605][root][INFO] - Training Epoch: 1/2, step 6395/7134 completed (loss: 0.04199066385626793, acc: 0.9913899302482605)
[2025-02-13 03:21:05,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:06,041][root][INFO] - Training Epoch: 1/2, step 6396/7134 completed (loss: 0.08024221658706665, acc: 0.9873853325843811)
[2025-02-13 03:21:06,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:06,501][root][INFO] - Training Epoch: 1/2, step 6397/7134 completed (loss: 0.032907839864492416, acc: 0.9925558567047119)
[2025-02-13 03:21:06,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:06,946][root][INFO] - Training Epoch: 1/2, step 6398/7134 completed (loss: 0.043590348213911057, acc: 0.9915611743927002)
[2025-02-13 03:21:07,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:07,392][root][INFO] - Training Epoch: 1/2, step 6399/7134 completed (loss: 0.05879124999046326, acc: 0.9862155318260193)
[2025-02-13 03:21:07,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:07,816][root][INFO] - Training Epoch: 1/2, step 6400/7134 completed (loss: 0.027111291885375977, acc: 0.9928774833679199)
[2025-02-13 03:21:07,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:08,277][root][INFO] - Training Epoch: 1/2, step 6401/7134 completed (loss: 0.05194953456521034, acc: 0.988056480884552)
[2025-02-13 03:21:08,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:08,715][root][INFO] - Training Epoch: 1/2, step 6402/7134 completed (loss: 0.04300141707062721, acc: 0.9909194111824036)
[2025-02-13 03:21:08,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:09,110][root][INFO] - Training Epoch: 1/2, step 6403/7134 completed (loss: 0.033251985907554626, acc: 0.9897959232330322)
[2025-02-13 03:21:09,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:09,538][root][INFO] - Training Epoch: 1/2, step 6404/7134 completed (loss: 0.019118666648864746, acc: 0.9944367408752441)
[2025-02-13 03:21:09,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:10,020][root][INFO] - Training Epoch: 1/2, step 6405/7134 completed (loss: 0.024950530380010605, acc: 0.9935483932495117)
[2025-02-13 03:21:10,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:10,480][root][INFO] - Training Epoch: 1/2, step 6406/7134 completed (loss: 0.03725910186767578, acc: 0.9931585192680359)
[2025-02-13 03:21:10,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:10,940][root][INFO] - Training Epoch: 1/2, step 6407/7134 completed (loss: 0.019948197528719902, acc: 0.9947437644004822)
[2025-02-13 03:21:11,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:11,351][root][INFO] - Training Epoch: 1/2, step 6408/7134 completed (loss: 0.05707354471087456, acc: 0.985228955745697)
[2025-02-13 03:21:11,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:11,771][root][INFO] - Training Epoch: 1/2, step 6409/7134 completed (loss: 0.03515855595469475, acc: 0.996052622795105)
[2025-02-13 03:21:11,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:12,180][root][INFO] - Training Epoch: 1/2, step 6410/7134 completed (loss: 0.024148399010300636, acc: 0.9926470518112183)
[2025-02-13 03:21:12,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:12,567][root][INFO] - Training Epoch: 1/2, step 6411/7134 completed (loss: 0.04296620562672615, acc: 0.9899497628211975)
[2025-02-13 03:21:12,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:12,972][root][INFO] - Training Epoch: 1/2, step 6412/7134 completed (loss: 0.012580715119838715, acc: 0.9973333477973938)
[2025-02-13 03:21:13,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:13,415][root][INFO] - Training Epoch: 1/2, step 6413/7134 completed (loss: 0.04192835092544556, acc: 0.9880136847496033)
[2025-02-13 03:21:13,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:13,805][root][INFO] - Training Epoch: 1/2, step 6414/7134 completed (loss: 0.019316181540489197, acc: 0.9939576983451843)
[2025-02-13 03:21:13,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:14,223][root][INFO] - Training Epoch: 1/2, step 6415/7134 completed (loss: 0.028660736978054047, acc: 0.9910141229629517)
[2025-02-13 03:21:14,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:14,629][root][INFO] - Training Epoch: 1/2, step 6416/7134 completed (loss: 0.06451208144426346, acc: 0.9791271090507507)
[2025-02-13 03:21:14,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:15,042][root][INFO] - Training Epoch: 1/2, step 6417/7134 completed (loss: 0.08642090857028961, acc: 0.9841827750205994)
[2025-02-13 03:21:15,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:15,476][root][INFO] - Training Epoch: 1/2, step 6418/7134 completed (loss: 0.1309068500995636, acc: 0.9705055952072144)
[2025-02-13 03:21:15,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:15,859][root][INFO] - Training Epoch: 1/2, step 6419/7134 completed (loss: 0.07527570426464081, acc: 0.9787928462028503)
[2025-02-13 03:21:15,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:16,291][root][INFO] - Training Epoch: 1/2, step 6420/7134 completed (loss: 0.04514420032501221, acc: 0.9884726405143738)
[2025-02-13 03:21:16,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:16,698][root][INFO] - Training Epoch: 1/2, step 6421/7134 completed (loss: 0.057638268917798996, acc: 0.9866270422935486)
[2025-02-13 03:21:16,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:17,126][root][INFO] - Training Epoch: 1/2, step 6422/7134 completed (loss: 0.04843197017908096, acc: 0.9822161197662354)
[2025-02-13 03:21:17,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:17,544][root][INFO] - Training Epoch: 1/2, step 6423/7134 completed (loss: 0.07207944244146347, acc: 0.9809384346008301)
[2025-02-13 03:21:17,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:17,990][root][INFO] - Training Epoch: 1/2, step 6424/7134 completed (loss: 0.057136159390211105, acc: 0.9848675727844238)
[2025-02-13 03:21:18,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:18,439][root][INFO] - Training Epoch: 1/2, step 6425/7134 completed (loss: 0.04608508571982384, acc: 0.9843546152114868)
[2025-02-13 03:21:18,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:18,894][root][INFO] - Training Epoch: 1/2, step 6426/7134 completed (loss: 0.043178658932447433, acc: 0.9868074059486389)
[2025-02-13 03:21:19,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:19,321][root][INFO] - Training Epoch: 1/2, step 6427/7134 completed (loss: 0.05076764523983002, acc: 0.9886363744735718)
[2025-02-13 03:21:19,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:19,763][root][INFO] - Training Epoch: 1/2, step 6428/7134 completed (loss: 0.024664271622896194, acc: 0.9916201233863831)
[2025-02-13 03:21:19,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:20,174][root][INFO] - Training Epoch: 1/2, step 6429/7134 completed (loss: 0.024639522656798363, acc: 0.9909090995788574)
[2025-02-13 03:21:20,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:20,614][root][INFO] - Training Epoch: 1/2, step 6430/7134 completed (loss: 0.05991942435503006, acc: 0.9856114983558655)
[2025-02-13 03:21:20,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:21,018][root][INFO] - Training Epoch: 1/2, step 6431/7134 completed (loss: 0.06579920649528503, acc: 0.9842105507850647)
[2025-02-13 03:21:21,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:21,418][root][INFO] - Training Epoch: 1/2, step 6432/7134 completed (loss: 0.19195912778377533, acc: 0.9545454382896423)
[2025-02-13 03:21:21,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:21,843][root][INFO] - Training Epoch: 1/2, step 6433/7134 completed (loss: 0.05334258824586868, acc: 0.9849726557731628)
[2025-02-13 03:21:21,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:22,244][root][INFO] - Training Epoch: 1/2, step 6434/7134 completed (loss: 0.01918976940214634, acc: 0.9933422207832336)
[2025-02-13 03:21:22,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:22,659][root][INFO] - Training Epoch: 1/2, step 6435/7134 completed (loss: 0.043369632214307785, acc: 0.9878683090209961)
[2025-02-13 03:21:22,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:23,095][root][INFO] - Training Epoch: 1/2, step 6436/7134 completed (loss: 0.017045538872480392, acc: 0.9969087839126587)
[2025-02-13 03:21:23,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:23,527][root][INFO] - Training Epoch: 1/2, step 6437/7134 completed (loss: 0.042049556970596313, acc: 0.9916666746139526)
[2025-02-13 03:21:23,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:23,981][root][INFO] - Training Epoch: 1/2, step 6438/7134 completed (loss: 0.043733835220336914, acc: 0.9876390695571899)
[2025-02-13 03:21:24,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:24,395][root][INFO] - Training Epoch: 1/2, step 6439/7134 completed (loss: 0.05161057040095329, acc: 0.9811023473739624)
[2025-02-13 03:21:24,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:24,861][root][INFO] - Training Epoch: 1/2, step 6440/7134 completed (loss: 0.030744636431336403, acc: 0.9880525469779968)
[2025-02-13 03:21:24,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:25,284][root][INFO] - Training Epoch: 1/2, step 6441/7134 completed (loss: 0.027089044451713562, acc: 0.9911373853683472)
[2025-02-13 03:21:25,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:25,756][root][INFO] - Training Epoch: 1/2, step 6442/7134 completed (loss: 0.036055129021406174, acc: 0.9954596757888794)
[2025-02-13 03:21:25,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:26,198][root][INFO] - Training Epoch: 1/2, step 6443/7134 completed (loss: 0.018321232870221138, acc: 0.9938119053840637)
[2025-02-13 03:21:26,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:26,656][root][INFO] - Training Epoch: 1/2, step 6444/7134 completed (loss: 0.03434792160987854, acc: 0.9902234673500061)
[2025-02-13 03:21:26,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:27,095][root][INFO] - Training Epoch: 1/2, step 6445/7134 completed (loss: 0.05442936718463898, acc: 0.9876695275306702)
[2025-02-13 03:21:27,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:27,542][root][INFO] - Training Epoch: 1/2, step 6446/7134 completed (loss: 0.028448546305298805, acc: 0.9952324032783508)
[2025-02-13 03:21:27,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:28,014][root][INFO] - Training Epoch: 1/2, step 6447/7134 completed (loss: 0.04141214117407799, acc: 0.9884925484657288)
[2025-02-13 03:21:28,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:28,441][root][INFO] - Training Epoch: 1/2, step 6448/7134 completed (loss: 0.09138359874486923, acc: 0.9750778675079346)
[2025-02-13 03:21:28,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:28,879][root][INFO] - Training Epoch: 1/2, step 6449/7134 completed (loss: 0.02547888271510601, acc: 0.9927536249160767)
[2025-02-13 03:21:29,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:29,331][root][INFO] - Training Epoch: 1/2, step 6450/7134 completed (loss: 0.02845328487455845, acc: 0.9910714030265808)
[2025-02-13 03:21:29,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:29,842][root][INFO] - Training Epoch: 1/2, step 6451/7134 completed (loss: 0.06003614887595177, acc: 0.9853085279464722)
[2025-02-13 03:21:29,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:30,274][root][INFO] - Training Epoch: 1/2, step 6452/7134 completed (loss: 0.012773496098816395, acc: 0.994425892829895)
[2025-02-13 03:21:30,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:30,703][root][INFO] - Training Epoch: 1/2, step 6453/7134 completed (loss: 0.0341302789747715, acc: 0.9905914068222046)
[2025-02-13 03:21:30,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:31,181][root][INFO] - Training Epoch: 1/2, step 6454/7134 completed (loss: 0.041785828769207, acc: 0.9917550086975098)
[2025-02-13 03:21:31,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:31,603][root][INFO] - Training Epoch: 1/2, step 6455/7134 completed (loss: 0.014363839291036129, acc: 0.9932088255882263)
[2025-02-13 03:21:31,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:32,006][root][INFO] - Training Epoch: 1/2, step 6456/7134 completed (loss: 0.0391780324280262, acc: 0.9923312664031982)
[2025-02-13 03:21:32,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:32,433][root][INFO] - Training Epoch: 1/2, step 6457/7134 completed (loss: 0.01698223315179348, acc: 0.9955947399139404)
[2025-02-13 03:21:32,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:32,819][root][INFO] - Training Epoch: 1/2, step 6458/7134 completed (loss: 0.04124285280704498, acc: 0.978151261806488)
[2025-02-13 03:21:32,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:33,239][root][INFO] - Training Epoch: 1/2, step 6459/7134 completed (loss: 0.003383180359378457, acc: 1.0)
[2025-02-13 03:21:33,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:33,706][root][INFO] - Training Epoch: 1/2, step 6460/7134 completed (loss: 0.011149896308779716, acc: 0.996458113193512)
[2025-02-13 03:21:33,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:34,130][root][INFO] - Training Epoch: 1/2, step 6461/7134 completed (loss: 0.01897428371012211, acc: 0.9947437644004822)
[2025-02-13 03:21:34,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:34,554][root][INFO] - Training Epoch: 1/2, step 6462/7134 completed (loss: 0.02419506013393402, acc: 0.9901599287986755)
[2025-02-13 03:21:34,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:35,022][root][INFO] - Training Epoch: 1/2, step 6463/7134 completed (loss: 0.029400499537587166, acc: 0.9908883571624756)
[2025-02-13 03:21:35,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:35,448][root][INFO] - Training Epoch: 1/2, step 6464/7134 completed (loss: 0.02403183840215206, acc: 0.9934980273246765)
[2025-02-13 03:21:35,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:35,868][root][INFO] - Training Epoch: 1/2, step 6465/7134 completed (loss: 0.018143631517887115, acc: 0.9948586225509644)
[2025-02-13 03:21:36,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:36,280][root][INFO] - Training Epoch: 1/2, step 6466/7134 completed (loss: 0.026302078738808632, acc: 0.9907529950141907)
[2025-02-13 03:21:36,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:36,708][root][INFO] - Training Epoch: 1/2, step 6467/7134 completed (loss: 0.045736346393823624, acc: 0.9877862334251404)
[2025-02-13 03:21:36,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:37,139][root][INFO] - Training Epoch: 1/2, step 6468/7134 completed (loss: 0.046303343027830124, acc: 0.9887429475784302)
[2025-02-13 03:21:37,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:37,602][root][INFO] - Training Epoch: 1/2, step 6469/7134 completed (loss: 0.054002486169338226, acc: 0.986601710319519)
[2025-02-13 03:21:37,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:37,972][root][INFO] - Training Epoch: 1/2, step 6470/7134 completed (loss: 0.045577362179756165, acc: 0.9902200698852539)
[2025-02-13 03:21:38,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:38,411][root][INFO] - Training Epoch: 1/2, step 6471/7134 completed (loss: 0.008478878997266293, acc: 0.9983713626861572)
[2025-02-13 03:21:38,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:38,880][root][INFO] - Training Epoch: 1/2, step 6472/7134 completed (loss: 0.07388059049844742, acc: 0.9858155846595764)
[2025-02-13 03:21:39,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:39,314][root][INFO] - Training Epoch: 1/2, step 6473/7134 completed (loss: 0.017034079879522324, acc: 0.9973190426826477)
[2025-02-13 03:21:39,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:39,726][root][INFO] - Training Epoch: 1/2, step 6474/7134 completed (loss: 0.024238863959908485, acc: 0.9959623217582703)
[2025-02-13 03:21:39,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:40,112][root][INFO] - Training Epoch: 1/2, step 6475/7134 completed (loss: 0.07143955677747726, acc: 0.9833333492279053)
[2025-02-13 03:21:40,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:40,460][root][INFO] - Training Epoch: 1/2, step 6476/7134 completed (loss: 0.07797020673751831, acc: 0.9834983348846436)
[2025-02-13 03:21:40,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:40,851][root][INFO] - Training Epoch: 1/2, step 6477/7134 completed (loss: 0.030488591641187668, acc: 0.9936440587043762)
[2025-02-13 03:21:40,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:41,257][root][INFO] - Training Epoch: 1/2, step 6478/7134 completed (loss: 0.03014550358057022, acc: 0.9889298677444458)
[2025-02-13 03:21:41,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:41,680][root][INFO] - Training Epoch: 1/2, step 6479/7134 completed (loss: 0.031276099383831024, acc: 0.9882746934890747)
[2025-02-13 03:21:41,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:42,077][root][INFO] - Training Epoch: 1/2, step 6480/7134 completed (loss: 0.06324087083339691, acc: 0.9845361113548279)
[2025-02-13 03:21:42,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:42,484][root][INFO] - Training Epoch: 1/2, step 6481/7134 completed (loss: 0.01896866038441658, acc: 0.9914966225624084)
[2025-02-13 03:21:42,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:42,878][root][INFO] - Training Epoch: 1/2, step 6482/7134 completed (loss: 0.031009037047624588, acc: 0.9925000071525574)
[2025-02-13 03:21:43,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:43,287][root][INFO] - Training Epoch: 1/2, step 6483/7134 completed (loss: 0.08875814825296402, acc: 0.9855595827102661)
[2025-02-13 03:21:43,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:43,695][root][INFO] - Training Epoch: 1/2, step 6484/7134 completed (loss: 0.025920817628502846, acc: 0.9912023544311523)
[2025-02-13 03:21:43,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:44,153][root][INFO] - Training Epoch: 1/2, step 6485/7134 completed (loss: 0.029120763763785362, acc: 0.992094874382019)
[2025-02-13 03:21:44,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:44,555][root][INFO] - Training Epoch: 1/2, step 6486/7134 completed (loss: 0.13097617030143738, acc: 0.9727272987365723)
[2025-02-13 03:21:44,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:44,887][root][INFO] - Training Epoch: 1/2, step 6487/7134 completed (loss: 0.16406473517417908, acc: 0.9574899077415466)
[2025-02-13 03:21:45,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:45,269][root][INFO] - Training Epoch: 1/2, step 6488/7134 completed (loss: 0.0841219574213028, acc: 0.9698188900947571)
[2025-02-13 03:21:45,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:45,671][root][INFO] - Training Epoch: 1/2, step 6489/7134 completed (loss: 0.12477873265743256, acc: 0.9738805890083313)
[2025-02-13 03:21:45,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:46,037][root][INFO] - Training Epoch: 1/2, step 6490/7134 completed (loss: 0.059317514300346375, acc: 0.9914236664772034)
[2025-02-13 03:21:46,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:46,470][root][INFO] - Training Epoch: 1/2, step 6491/7134 completed (loss: 0.08961495012044907, acc: 0.9811066389083862)
[2025-02-13 03:21:46,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:46,887][root][INFO] - Training Epoch: 1/2, step 6492/7134 completed (loss: 0.03891437128186226, acc: 0.9870634078979492)
[2025-02-13 03:21:47,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:47,346][root][INFO] - Training Epoch: 1/2, step 6493/7134 completed (loss: 0.08234242349863052, acc: 0.9796437621116638)
[2025-02-13 03:21:47,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:47,733][root][INFO] - Training Epoch: 1/2, step 6494/7134 completed (loss: 0.0456218458712101, acc: 0.9852398633956909)
[2025-02-13 03:21:47,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:48,159][root][INFO] - Training Epoch: 1/2, step 6495/7134 completed (loss: 0.0651463121175766, acc: 0.9817671775817871)
[2025-02-13 03:21:48,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:48,605][root][INFO] - Training Epoch: 1/2, step 6496/7134 completed (loss: 0.050900865346193314, acc: 0.983627200126648)
[2025-02-13 03:21:48,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:49,025][root][INFO] - Training Epoch: 1/2, step 6497/7134 completed (loss: 0.1225910559296608, acc: 0.9751166701316833)
[2025-02-13 03:21:49,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:49,454][root][INFO] - Training Epoch: 1/2, step 6498/7134 completed (loss: 0.02074163407087326, acc: 0.995121955871582)
[2025-02-13 03:21:49,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:49,881][root][INFO] - Training Epoch: 1/2, step 6499/7134 completed (loss: 0.04806634038686752, acc: 0.9813200235366821)
[2025-02-13 03:21:50,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:50,340][root][INFO] - Training Epoch: 1/2, step 6500/7134 completed (loss: 0.08297231793403625, acc: 0.9719887971878052)
[2025-02-13 03:21:50,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:50,770][root][INFO] - Training Epoch: 1/2, step 6501/7134 completed (loss: 0.11838037520647049, acc: 0.9667519330978394)
[2025-02-13 03:21:50,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:51,264][root][INFO] - Training Epoch: 1/2, step 6502/7134 completed (loss: 0.03009927086532116, acc: 0.9904988408088684)
[2025-02-13 03:21:51,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:51,668][root][INFO] - Training Epoch: 1/2, step 6503/7134 completed (loss: 0.05403834208846092, acc: 0.9853372573852539)
[2025-02-13 03:21:51,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:52,060][root][INFO] - Training Epoch: 1/2, step 6504/7134 completed (loss: 0.06721170991659164, acc: 0.9810426831245422)
[2025-02-13 03:21:52,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:52,492][root][INFO] - Training Epoch: 1/2, step 6505/7134 completed (loss: 0.08473736047744751, acc: 0.9797101616859436)
[2025-02-13 03:21:52,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:52,957][root][INFO] - Training Epoch: 1/2, step 6506/7134 completed (loss: 0.0856720581650734, acc: 0.9771987199783325)
[2025-02-13 03:21:53,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:53,398][root][INFO] - Training Epoch: 1/2, step 6507/7134 completed (loss: 0.10004544258117676, acc: 0.9727685451507568)
[2025-02-13 03:21:53,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:53,852][root][INFO] - Training Epoch: 1/2, step 6508/7134 completed (loss: 0.06760382652282715, acc: 0.9894366264343262)
[2025-02-13 03:21:53,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:54,297][root][INFO] - Training Epoch: 1/2, step 6509/7134 completed (loss: 0.04202549159526825, acc: 0.9900332093238831)
[2025-02-13 03:21:54,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:54,789][root][INFO] - Training Epoch: 1/2, step 6510/7134 completed (loss: 0.05336529389023781, acc: 0.9873149991035461)
[2025-02-13 03:21:54,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:55,220][root][INFO] - Training Epoch: 1/2, step 6511/7134 completed (loss: 0.07709565758705139, acc: 0.9834586381912231)
[2025-02-13 03:21:55,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:55,488][root][INFO] - Training Epoch: 1/2, step 6512/7134 completed (loss: 0.03560752794146538, acc: 0.9885931611061096)
[2025-02-13 03:21:55,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:55,905][root][INFO] - Training Epoch: 1/2, step 6513/7134 completed (loss: 0.05489219352602959, acc: 0.9845678806304932)
[2025-02-13 03:21:56,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:56,311][root][INFO] - Training Epoch: 1/2, step 6514/7134 completed (loss: 0.05679669231176376, acc: 0.984375)
[2025-02-13 03:21:56,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:56,776][root][INFO] - Training Epoch: 1/2, step 6515/7134 completed (loss: 0.055721621960401535, acc: 0.9860405921936035)
[2025-02-13 03:21:56,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:57,229][root][INFO] - Training Epoch: 1/2, step 6516/7134 completed (loss: 0.09850224107503891, acc: 0.9721485376358032)
[2025-02-13 03:21:57,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:57,668][root][INFO] - Training Epoch: 1/2, step 6517/7134 completed (loss: 0.028280021622776985, acc: 0.9907894730567932)
[2025-02-13 03:21:57,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:58,107][root][INFO] - Training Epoch: 1/2, step 6518/7134 completed (loss: 0.034319598227739334, acc: 0.9875862002372742)
[2025-02-13 03:21:58,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:58,522][root][INFO] - Training Epoch: 1/2, step 6519/7134 completed (loss: 0.043904975056648254, acc: 0.9893617033958435)
[2025-02-13 03:21:58,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:58,922][root][INFO] - Training Epoch: 1/2, step 6520/7134 completed (loss: 0.036854781210422516, acc: 0.9875583052635193)
[2025-02-13 03:21:59,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:59,329][root][INFO] - Training Epoch: 1/2, step 6521/7134 completed (loss: 0.03489858657121658, acc: 0.9869791865348816)
[2025-02-13 03:21:59,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:59,721][root][INFO] - Training Epoch: 1/2, step 6522/7134 completed (loss: 0.05060987547039986, acc: 0.9811320900917053)
[2025-02-13 03:21:59,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:00,110][root][INFO] - Training Epoch: 1/2, step 6523/7134 completed (loss: 0.10133565217256546, acc: 0.97096186876297)
[2025-02-13 03:22:00,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:00,545][root][INFO] - Training Epoch: 1/2, step 6524/7134 completed (loss: 0.08985867351293564, acc: 0.9845758080482483)
[2025-02-13 03:22:00,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:00,961][root][INFO] - Training Epoch: 1/2, step 6525/7134 completed (loss: 0.02311559021472931, acc: 0.9919678568840027)
[2025-02-13 03:22:01,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:01,416][root][INFO] - Training Epoch: 1/2, step 6526/7134 completed (loss: 0.08992396295070648, acc: 0.9820742607116699)
[2025-02-13 03:22:01,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:01,829][root][INFO] - Training Epoch: 1/2, step 6527/7134 completed (loss: 0.0510987751185894, acc: 0.9896142482757568)
[2025-02-13 03:22:01,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:02,214][root][INFO] - Training Epoch: 1/2, step 6528/7134 completed (loss: 0.06741945445537567, acc: 0.9855595827102661)
[2025-02-13 03:22:02,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:02,603][root][INFO] - Training Epoch: 1/2, step 6529/7134 completed (loss: 0.09716938436031342, acc: 0.9793233275413513)
[2025-02-13 03:22:02,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:03,027][root][INFO] - Training Epoch: 1/2, step 6530/7134 completed (loss: 0.04216577112674713, acc: 0.9871612191200256)
[2025-02-13 03:22:03,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:03,443][root][INFO] - Training Epoch: 1/2, step 6531/7134 completed (loss: 0.030170267447829247, acc: 0.9913294911384583)
[2025-02-13 03:22:03,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:03,880][root][INFO] - Training Epoch: 1/2, step 6532/7134 completed (loss: 0.06412573158740997, acc: 0.9862328171730042)
[2025-02-13 03:22:04,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:04,304][root][INFO] - Training Epoch: 1/2, step 6533/7134 completed (loss: 0.024378405883908272, acc: 0.9925373196601868)
[2025-02-13 03:22:04,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:04,699][root][INFO] - Training Epoch: 1/2, step 6534/7134 completed (loss: 0.0568380169570446, acc: 0.9906166195869446)
[2025-02-13 03:22:04,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:05,091][root][INFO] - Training Epoch: 1/2, step 6535/7134 completed (loss: 0.02977265790104866, acc: 0.9906322956085205)
[2025-02-13 03:22:05,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:05,491][root][INFO] - Training Epoch: 1/2, step 6536/7134 completed (loss: 0.06244169920682907, acc: 0.9821428656578064)
[2025-02-13 03:22:05,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:05,874][root][INFO] - Training Epoch: 1/2, step 6537/7134 completed (loss: 0.07945997267961502, acc: 0.9690265655517578)
[2025-02-13 03:22:06,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:06,284][root][INFO] - Training Epoch: 1/2, step 6538/7134 completed (loss: 0.0807403102517128, acc: 0.9786856174468994)
[2025-02-13 03:22:06,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:06,568][root][INFO] - Training Epoch: 1/2, step 6539/7134 completed (loss: 0.01225089468061924, acc: 0.9969879388809204)
[2025-02-13 03:22:06,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:06,903][root][INFO] - Training Epoch: 1/2, step 6540/7134 completed (loss: 0.290516197681427, acc: 0.9398906826972961)
[2025-02-13 03:22:07,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:07,362][root][INFO] - Training Epoch: 1/2, step 6541/7134 completed (loss: 0.06787167489528656, acc: 0.9803921580314636)
[2025-02-13 03:22:07,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:07,815][root][INFO] - Training Epoch: 1/2, step 6542/7134 completed (loss: 0.06008094549179077, acc: 0.9796609878540039)
[2025-02-13 03:22:07,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:08,220][root][INFO] - Training Epoch: 1/2, step 6543/7134 completed (loss: 0.04848864674568176, acc: 0.9853249192237854)
[2025-02-13 03:22:08,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:08,651][root][INFO] - Training Epoch: 1/2, step 6544/7134 completed (loss: 0.05122829228639603, acc: 0.9900596141815186)
[2025-02-13 03:22:08,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:09,055][root][INFO] - Training Epoch: 1/2, step 6545/7134 completed (loss: 0.036698322743177414, acc: 0.9925650358200073)
[2025-02-13 03:22:09,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:09,435][root][INFO] - Training Epoch: 1/2, step 6546/7134 completed (loss: 0.05920618772506714, acc: 0.9885993599891663)
[2025-02-13 03:22:09,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:09,839][root][INFO] - Training Epoch: 1/2, step 6547/7134 completed (loss: 0.12770266830921173, acc: 0.9778534770011902)
[2025-02-13 03:22:09,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:10,171][root][INFO] - Training Epoch: 1/2, step 6548/7134 completed (loss: 0.02699464187026024, acc: 0.9920634627342224)
[2025-02-13 03:22:10,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:10,571][root][INFO] - Training Epoch: 1/2, step 6549/7134 completed (loss: 0.0971817746758461, acc: 0.9766990542411804)
[2025-02-13 03:22:10,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:10,970][root][INFO] - Training Epoch: 1/2, step 6550/7134 completed (loss: 0.09218588471412659, acc: 0.9784615635871887)
[2025-02-13 03:22:11,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:11,421][root][INFO] - Training Epoch: 1/2, step 6551/7134 completed (loss: 0.02179507166147232, acc: 0.9954441785812378)
[2025-02-13 03:22:11,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:11,882][root][INFO] - Training Epoch: 1/2, step 6552/7134 completed (loss: 0.04069502279162407, acc: 0.9930278658866882)
[2025-02-13 03:22:12,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:12,341][root][INFO] - Training Epoch: 1/2, step 6553/7134 completed (loss: 0.03936411067843437, acc: 0.9872773289680481)
[2025-02-13 03:22:12,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:12,802][root][INFO] - Training Epoch: 1/2, step 6554/7134 completed (loss: 0.04473140835762024, acc: 0.9913513660430908)
[2025-02-13 03:22:12,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:13,278][root][INFO] - Training Epoch: 1/2, step 6555/7134 completed (loss: 0.07351808995008469, acc: 0.9768611788749695)
[2025-02-13 03:22:13,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:13,705][root][INFO] - Training Epoch: 1/2, step 6556/7134 completed (loss: 0.045677781105041504, acc: 0.9873684048652649)
[2025-02-13 03:22:13,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:14,168][root][INFO] - Training Epoch: 1/2, step 6557/7134 completed (loss: 0.04277072474360466, acc: 0.9889112710952759)
[2025-02-13 03:22:14,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:14,645][root][INFO] - Training Epoch: 1/2, step 6558/7134 completed (loss: 0.06414534896612167, acc: 0.9826086759567261)
[2025-02-13 03:22:14,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:15,082][root][INFO] - Training Epoch: 1/2, step 6559/7134 completed (loss: 0.042410511523485184, acc: 0.9861303567886353)
[2025-02-13 03:22:15,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:15,602][root][INFO] - Training Epoch: 1/2, step 6560/7134 completed (loss: 0.0367206372320652, acc: 0.9916765689849854)
[2025-02-13 03:22:15,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:16,031][root][INFO] - Training Epoch: 1/2, step 6561/7134 completed (loss: 0.028590399771928787, acc: 0.9906432628631592)
[2025-02-13 03:22:16,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:16,461][root][INFO] - Training Epoch: 1/2, step 6562/7134 completed (loss: 0.06349960714578629, acc: 0.9790732264518738)
[2025-02-13 03:22:16,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:16,918][root][INFO] - Training Epoch: 1/2, step 6563/7134 completed (loss: 0.04424972087144852, acc: 0.9883585572242737)
[2025-02-13 03:22:17,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:17,363][root][INFO] - Training Epoch: 1/2, step 6564/7134 completed (loss: 0.061799269169569016, acc: 0.9851552248001099)
[2025-02-13 03:22:17,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:17,828][root][INFO] - Training Epoch: 1/2, step 6565/7134 completed (loss: 0.04099797084927559, acc: 0.9873149991035461)
[2025-02-13 03:22:17,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:18,320][root][INFO] - Training Epoch: 1/2, step 6566/7134 completed (loss: 0.03563854098320007, acc: 0.9829059839248657)
[2025-02-13 03:22:18,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:18,807][root][INFO] - Training Epoch: 1/2, step 6567/7134 completed (loss: 0.02063876949250698, acc: 0.9939209818840027)
[2025-02-13 03:22:18,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:19,265][root][INFO] - Training Epoch: 1/2, step 6568/7134 completed (loss: 0.028528675436973572, acc: 0.9936948418617249)
[2025-02-13 03:22:19,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:19,706][root][INFO] - Training Epoch: 1/2, step 6569/7134 completed (loss: 0.01941157877445221, acc: 0.9967032670974731)
[2025-02-13 03:22:19,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:20,152][root][INFO] - Training Epoch: 1/2, step 6570/7134 completed (loss: 0.02997172437608242, acc: 0.99303138256073)
[2025-02-13 03:22:20,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:20,546][root][INFO] - Training Epoch: 1/2, step 6571/7134 completed (loss: 0.026817116886377335, acc: 0.9934554696083069)
[2025-02-13 03:22:20,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:20,982][root][INFO] - Training Epoch: 1/2, step 6572/7134 completed (loss: 0.0430481843650341, acc: 0.9889135360717773)
[2025-02-13 03:22:21,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:21,437][root][INFO] - Training Epoch: 1/2, step 6573/7134 completed (loss: 0.03559443727135658, acc: 0.9909706711769104)
[2025-02-13 03:22:21,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:21,903][root][INFO] - Training Epoch: 1/2, step 6574/7134 completed (loss: 0.020392142236232758, acc: 0.9928425550460815)
[2025-02-13 03:22:22,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:22,394][root][INFO] - Training Epoch: 1/2, step 6575/7134 completed (loss: 0.042802512645721436, acc: 0.9845303893089294)
[2025-02-13 03:22:22,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:22,840][root][INFO] - Training Epoch: 1/2, step 6576/7134 completed (loss: 0.02106330730021, acc: 0.9947478771209717)
[2025-02-13 03:22:22,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:23,286][root][INFO] - Training Epoch: 1/2, step 6577/7134 completed (loss: 0.0640605017542839, acc: 0.981792688369751)
[2025-02-13 03:22:23,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:23,914][root][INFO] - Training Epoch: 1/2, step 6578/7134 completed (loss: 0.027726784348487854, acc: 0.9911032319068909)
[2025-02-13 03:22:24,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:24,301][root][INFO] - Training Epoch: 1/2, step 6579/7134 completed (loss: 0.039780814200639725, acc: 0.9872958064079285)
[2025-02-13 03:22:24,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:24,758][root][INFO] - Training Epoch: 1/2, step 6580/7134 completed (loss: 0.029087724164128304, acc: 0.9905882477760315)
[2025-02-13 03:22:24,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:25,188][root][INFO] - Training Epoch: 1/2, step 6581/7134 completed (loss: 0.036459024995565414, acc: 0.9889298677444458)
[2025-02-13 03:22:25,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:25,574][root][INFO] - Training Epoch: 1/2, step 6582/7134 completed (loss: 0.05696261301636696, acc: 0.9827337861061096)
[2025-02-13 03:22:25,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:26,023][root][INFO] - Training Epoch: 1/2, step 6583/7134 completed (loss: 0.01905648224055767, acc: 0.9942792057991028)
[2025-02-13 03:22:26,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:26,468][root][INFO] - Training Epoch: 1/2, step 6584/7134 completed (loss: 0.051391344517469406, acc: 0.9865360856056213)
[2025-02-13 03:22:26,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:26,900][root][INFO] - Training Epoch: 1/2, step 6585/7134 completed (loss: 0.04492368549108505, acc: 0.9819079041481018)
[2025-02-13 03:22:27,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:27,330][root][INFO] - Training Epoch: 1/2, step 6586/7134 completed (loss: 0.03463881090283394, acc: 0.9909326434135437)
[2025-02-13 03:22:27,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:27,775][root][INFO] - Training Epoch: 1/2, step 6587/7134 completed (loss: 0.07286817580461502, acc: 0.9811320900917053)
[2025-02-13 03:22:27,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:28,186][root][INFO] - Training Epoch: 1/2, step 6588/7134 completed (loss: 0.05144646018743515, acc: 0.9899371266365051)
[2025-02-13 03:22:28,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:28,614][root][INFO] - Training Epoch: 1/2, step 6589/7134 completed (loss: 0.04349077120423317, acc: 0.9889349937438965)
[2025-02-13 03:22:28,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:29,068][root][INFO] - Training Epoch: 1/2, step 6590/7134 completed (loss: 0.0510953813791275, acc: 0.9825378060340881)
[2025-02-13 03:22:29,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:29,522][root][INFO] - Training Epoch: 1/2, step 6591/7134 completed (loss: 0.04582047089934349, acc: 0.986369252204895)
[2025-02-13 03:22:29,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:29,971][root][INFO] - Training Epoch: 1/2, step 6592/7134 completed (loss: 0.07153208553791046, acc: 0.9852440357208252)
[2025-02-13 03:22:30,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:30,382][root][INFO] - Training Epoch: 1/2, step 6593/7134 completed (loss: 0.045919738709926605, acc: 0.9878234267234802)
[2025-02-13 03:22:30,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:30,821][root][INFO] - Training Epoch: 1/2, step 6594/7134 completed (loss: 0.03440569341182709, acc: 0.9906103014945984)
[2025-02-13 03:22:30,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:31,247][root][INFO] - Training Epoch: 1/2, step 6595/7134 completed (loss: 0.056952882558107376, acc: 0.9881305694580078)
[2025-02-13 03:22:31,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:31,639][root][INFO] - Training Epoch: 1/2, step 6596/7134 completed (loss: 0.03990121930837631, acc: 0.9908952713012695)
[2025-02-13 03:22:31,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:32,077][root][INFO] - Training Epoch: 1/2, step 6597/7134 completed (loss: 0.05044301226735115, acc: 0.989386796951294)
[2025-02-13 03:22:32,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:32,509][root][INFO] - Training Epoch: 1/2, step 6598/7134 completed (loss: 0.046254437416791916, acc: 0.9879194498062134)
[2025-02-13 03:22:32,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:32,964][root][INFO] - Training Epoch: 1/2, step 6599/7134 completed (loss: 0.059476833790540695, acc: 0.9867149591445923)
[2025-02-13 03:22:33,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:33,365][root][INFO] - Training Epoch: 1/2, step 6600/7134 completed (loss: 0.053234685212373734, acc: 0.9825673699378967)
[2025-02-13 03:22:33,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:33,815][root][INFO] - Training Epoch: 1/2, step 6601/7134 completed (loss: 0.06516904383897781, acc: 0.9823943376541138)
[2025-02-13 03:22:33,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:34,255][root][INFO] - Training Epoch: 1/2, step 6602/7134 completed (loss: 0.03932959586381912, acc: 0.9915764331817627)
[2025-02-13 03:22:34,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:34,703][root][INFO] - Training Epoch: 1/2, step 6603/7134 completed (loss: 0.04105145111680031, acc: 0.9859747290611267)
[2025-02-13 03:22:34,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:35,075][root][INFO] - Training Epoch: 1/2, step 6604/7134 completed (loss: 0.05965065583586693, acc: 0.9798850417137146)
[2025-02-13 03:22:35,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:35,481][root][INFO] - Training Epoch: 1/2, step 6605/7134 completed (loss: 0.09881748259067535, acc: 0.9709543585777283)
[2025-02-13 03:22:35,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:35,928][root][INFO] - Training Epoch: 1/2, step 6606/7134 completed (loss: 0.039674311876297, acc: 0.9898989796638489)
[2025-02-13 03:22:36,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:36,379][root][INFO] - Training Epoch: 1/2, step 6607/7134 completed (loss: 0.09560273587703705, acc: 0.978723406791687)
[2025-02-13 03:22:36,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:36,826][root][INFO] - Training Epoch: 1/2, step 6608/7134 completed (loss: 0.03248735889792442, acc: 0.9920318722724915)
[2025-02-13 03:22:36,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:37,257][root][INFO] - Training Epoch: 1/2, step 6609/7134 completed (loss: 0.052462149411439896, acc: 0.9853300452232361)
[2025-02-13 03:22:37,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:37,667][root][INFO] - Training Epoch: 1/2, step 6610/7134 completed (loss: 0.09235725551843643, acc: 0.9784366488456726)
[2025-02-13 03:22:37,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:38,058][root][INFO] - Training Epoch: 1/2, step 6611/7134 completed (loss: 0.14193759858608246, acc: 0.9661538600921631)
[2025-02-13 03:22:38,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:38,523][root][INFO] - Training Epoch: 1/2, step 6612/7134 completed (loss: 0.06877297908067703, acc: 0.9824970960617065)
[2025-02-13 03:22:38,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:38,990][root][INFO] - Training Epoch: 1/2, step 6613/7134 completed (loss: 0.03835620358586311, acc: 0.9895833134651184)
[2025-02-13 03:22:39,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:39,421][root][INFO] - Training Epoch: 1/2, step 6614/7134 completed (loss: 0.07351738214492798, acc: 0.9845984578132629)
[2025-02-13 03:22:39,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:39,849][root][INFO] - Training Epoch: 1/2, step 6615/7134 completed (loss: 0.05126853287220001, acc: 0.9830268621444702)
[2025-02-13 03:22:39,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:40,304][root][INFO] - Training Epoch: 1/2, step 6616/7134 completed (loss: 0.04390104115009308, acc: 0.9894366264343262)
[2025-02-13 03:22:40,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:40,689][root][INFO] - Training Epoch: 1/2, step 6617/7134 completed (loss: 0.060556862503290176, acc: 0.9827089309692383)
[2025-02-13 03:22:40,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:41,101][root][INFO] - Training Epoch: 1/2, step 6618/7134 completed (loss: 0.035591427236795425, acc: 0.9881129264831543)
[2025-02-13 03:22:41,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:41,562][root][INFO] - Training Epoch: 1/2, step 6619/7134 completed (loss: 0.04021277278661728, acc: 0.9876681566238403)
[2025-02-13 03:22:41,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:42,011][root][INFO] - Training Epoch: 1/2, step 6620/7134 completed (loss: 0.05020403861999512, acc: 0.9834938049316406)
[2025-02-13 03:22:42,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:42,444][root][INFO] - Training Epoch: 1/2, step 6621/7134 completed (loss: 0.03571772202849388, acc: 0.989595353603363)
[2025-02-13 03:22:42,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:42,875][root][INFO] - Training Epoch: 1/2, step 6622/7134 completed (loss: 0.06485089659690857, acc: 0.9822404384613037)
[2025-02-13 03:22:43,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:43,309][root][INFO] - Training Epoch: 1/2, step 6623/7134 completed (loss: 0.02743517793715, acc: 0.9899371266365051)
[2025-02-13 03:22:43,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:43,736][root][INFO] - Training Epoch: 1/2, step 6624/7134 completed (loss: 0.03115890920162201, acc: 0.9856770634651184)
[2025-02-13 03:22:43,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:44,160][root][INFO] - Training Epoch: 1/2, step 6625/7134 completed (loss: 0.023420017212629318, acc: 0.9903714060783386)
[2025-02-13 03:22:44,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:44,609][root][INFO] - Training Epoch: 1/2, step 6626/7134 completed (loss: 0.019205987453460693, acc: 0.9951515197753906)
[2025-02-13 03:22:44,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:45,073][root][INFO] - Training Epoch: 1/2, step 6627/7134 completed (loss: 0.03128823637962341, acc: 0.9881080985069275)
[2025-02-13 03:22:45,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:45,523][root][INFO] - Training Epoch: 1/2, step 6628/7134 completed (loss: 0.06453536450862885, acc: 0.9822485446929932)
[2025-02-13 03:22:45,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:45,909][root][INFO] - Training Epoch: 1/2, step 6629/7134 completed (loss: 0.06583660840988159, acc: 0.9884467124938965)
[2025-02-13 03:22:46,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:46,392][root][INFO] - Training Epoch: 1/2, step 6630/7134 completed (loss: 0.058146312832832336, acc: 0.9843924045562744)
[2025-02-13 03:22:46,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:46,827][root][INFO] - Training Epoch: 1/2, step 6631/7134 completed (loss: 0.04787677899003029, acc: 0.9855832457542419)
[2025-02-13 03:22:46,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:47,306][root][INFO] - Training Epoch: 1/2, step 6632/7134 completed (loss: 0.04141195863485336, acc: 0.9876265525817871)
[2025-02-13 03:22:47,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:47,734][root][INFO] - Training Epoch: 1/2, step 6633/7134 completed (loss: 0.0491056852042675, acc: 0.9890244007110596)
[2025-02-13 03:22:47,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:48,144][root][INFO] - Training Epoch: 1/2, step 6634/7134 completed (loss: 0.05887427181005478, acc: 0.9838998317718506)
[2025-02-13 03:22:48,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:48,529][root][INFO] - Training Epoch: 1/2, step 6635/7134 completed (loss: 0.12433820217847824, acc: 0.971867024898529)
[2025-02-13 03:22:48,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:48,982][root][INFO] - Training Epoch: 1/2, step 6636/7134 completed (loss: 0.05217595770955086, acc: 0.982876718044281)
[2025-02-13 03:22:49,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:49,405][root][INFO] - Training Epoch: 1/2, step 6637/7134 completed (loss: 0.044784605503082275, acc: 0.9898107647895813)
[2025-02-13 03:22:49,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:49,836][root][INFO] - Training Epoch: 1/2, step 6638/7134 completed (loss: 0.028258932754397392, acc: 0.9923760890960693)
[2025-02-13 03:22:49,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:50,281][root][INFO] - Training Epoch: 1/2, step 6639/7134 completed (loss: 0.03206288442015648, acc: 0.9910581111907959)
[2025-02-13 03:22:50,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:50,727][root][INFO] - Training Epoch: 1/2, step 6640/7134 completed (loss: 0.022887863218784332, acc: 0.9908116459846497)
[2025-02-13 03:22:50,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:51,174][root][INFO] - Training Epoch: 1/2, step 6641/7134 completed (loss: 0.09377200156450272, acc: 0.9765533208847046)
[2025-02-13 03:22:51,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:51,608][root][INFO] - Training Epoch: 1/2, step 6642/7134 completed (loss: 0.049567677080631256, acc: 0.983433723449707)
[2025-02-13 03:22:51,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:52,069][root][INFO] - Training Epoch: 1/2, step 6643/7134 completed (loss: 0.0384051650762558, acc: 0.9907529950141907)
[2025-02-13 03:22:52,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:52,527][root][INFO] - Training Epoch: 1/2, step 6644/7134 completed (loss: 0.06540659815073013, acc: 0.9847406148910522)
[2025-02-13 03:22:52,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:52,956][root][INFO] - Training Epoch: 1/2, step 6645/7134 completed (loss: 0.029731862246990204, acc: 0.9926289916038513)
[2025-02-13 03:22:53,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:53,429][root][INFO] - Training Epoch: 1/2, step 6646/7134 completed (loss: 0.04250229895114899, acc: 0.9878854751586914)
[2025-02-13 03:22:53,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:53,860][root][INFO] - Training Epoch: 1/2, step 6647/7134 completed (loss: 0.02560625970363617, acc: 0.9932065010070801)
[2025-02-13 03:22:54,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:54,307][root][INFO] - Training Epoch: 1/2, step 6648/7134 completed (loss: 0.09770096838474274, acc: 0.9760000109672546)
[2025-02-13 03:22:54,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:54,774][root][INFO] - Training Epoch: 1/2, step 6649/7134 completed (loss: 0.057511523365974426, acc: 0.9834710955619812)
[2025-02-13 03:22:54,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:55,259][root][INFO] - Training Epoch: 1/2, step 6650/7134 completed (loss: 0.026994401589035988, acc: 0.9921875)
[2025-02-13 03:22:55,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:55,737][root][INFO] - Training Epoch: 1/2, step 6651/7134 completed (loss: 0.02784392610192299, acc: 0.9931787252426147)
[2025-02-13 03:22:55,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:56,183][root][INFO] - Training Epoch: 1/2, step 6652/7134 completed (loss: 0.045621249824762344, acc: 0.9867149591445923)
[2025-02-13 03:22:56,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:56,640][root][INFO] - Training Epoch: 1/2, step 6653/7134 completed (loss: 0.044551149010658264, acc: 0.9835706353187561)
[2025-02-13 03:22:56,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:57,067][root][INFO] - Training Epoch: 1/2, step 6654/7134 completed (loss: 0.08658469468355179, acc: 0.9798449873924255)
[2025-02-13 03:22:57,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:57,494][root][INFO] - Training Epoch: 1/2, step 6655/7134 completed (loss: 0.05669471621513367, acc: 0.9826302528381348)
[2025-02-13 03:22:57,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:57,908][root][INFO] - Training Epoch: 1/2, step 6656/7134 completed (loss: 0.0660855770111084, acc: 0.983460545539856)
[2025-02-13 03:22:58,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:58,341][root][INFO] - Training Epoch: 1/2, step 6657/7134 completed (loss: 0.05477107688784599, acc: 0.9837398529052734)
[2025-02-13 03:22:58,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:58,796][root][INFO] - Training Epoch: 1/2, step 6658/7134 completed (loss: 0.01841777190566063, acc: 0.9922580718994141)
[2025-02-13 03:22:58,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:59,270][root][INFO] - Training Epoch: 1/2, step 6659/7134 completed (loss: 0.05138453468680382, acc: 0.9869791865348816)
[2025-02-13 03:22:59,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:59,723][root][INFO] - Training Epoch: 1/2, step 6660/7134 completed (loss: 0.029594892635941505, acc: 0.9920634627342224)
[2025-02-13 03:22:59,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:00,208][root][INFO] - Training Epoch: 1/2, step 6661/7134 completed (loss: 0.03238046541810036, acc: 0.9863861203193665)
[2025-02-13 03:23:00,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:00,618][root][INFO] - Training Epoch: 1/2, step 6662/7134 completed (loss: 0.0646599531173706, acc: 0.9785407781600952)
[2025-02-13 03:23:00,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:01,064][root][INFO] - Training Epoch: 1/2, step 6663/7134 completed (loss: 0.03527991846203804, acc: 0.9890710115432739)
[2025-02-13 03:23:01,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:01,459][root][INFO] - Training Epoch: 1/2, step 6664/7134 completed (loss: 0.12478387355804443, acc: 0.9612545967102051)
[2025-02-13 03:23:01,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:01,852][root][INFO] - Training Epoch: 1/2, step 6665/7134 completed (loss: 0.1015033945441246, acc: 0.9626623392105103)
[2025-02-13 03:23:01,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:02,295][root][INFO] - Training Epoch: 1/2, step 6666/7134 completed (loss: 0.032294049859046936, acc: 0.989847719669342)
[2025-02-13 03:23:02,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:02,707][root][INFO] - Training Epoch: 1/2, step 6667/7134 completed (loss: 0.0504625178873539, acc: 0.982332170009613)
[2025-02-13 03:23:02,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:03,093][root][INFO] - Training Epoch: 1/2, step 6668/7134 completed (loss: 0.01584608294069767, acc: 0.9930675625801086)
[2025-02-13 03:23:03,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:03,480][root][INFO] - Training Epoch: 1/2, step 6669/7134 completed (loss: 0.03864308074116707, acc: 0.9882550239562988)
[2025-02-13 03:23:03,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:03,860][root][INFO] - Training Epoch: 1/2, step 6670/7134 completed (loss: 0.032885827124118805, acc: 0.9916387796401978)
[2025-02-13 03:23:04,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:04,281][root][INFO] - Training Epoch: 1/2, step 6671/7134 completed (loss: 0.031191306188702583, acc: 0.9897360801696777)
[2025-02-13 03:23:04,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:04,691][root][INFO] - Training Epoch: 1/2, step 6672/7134 completed (loss: 0.02572740986943245, acc: 0.9887429475784302)
[2025-02-13 03:23:04,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:05,115][root][INFO] - Training Epoch: 1/2, step 6673/7134 completed (loss: 0.02646365761756897, acc: 0.9897210001945496)
[2025-02-13 03:23:05,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:05,504][root][INFO] - Training Epoch: 1/2, step 6674/7134 completed (loss: 0.019127901643514633, acc: 0.9941973090171814)
[2025-02-13 03:23:05,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:05,909][root][INFO] - Training Epoch: 1/2, step 6675/7134 completed (loss: 0.03886038437485695, acc: 0.9950617551803589)
[2025-02-13 03:23:06,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:06,319][root][INFO] - Training Epoch: 1/2, step 6676/7134 completed (loss: 0.03457066789269447, acc: 0.9880668520927429)
[2025-02-13 03:23:06,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:06,689][root][INFO] - Training Epoch: 1/2, step 6677/7134 completed (loss: 0.07139384746551514, acc: 0.9809160232543945)
[2025-02-13 03:23:06,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:07,086][root][INFO] - Training Epoch: 1/2, step 6678/7134 completed (loss: 0.017350945621728897, acc: 0.9964285492897034)
[2025-02-13 03:23:07,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:07,447][root][INFO] - Training Epoch: 1/2, step 6679/7134 completed (loss: 0.014092200435698032, acc: 0.9956331849098206)
[2025-02-13 03:23:07,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:07,801][root][INFO] - Training Epoch: 1/2, step 6680/7134 completed (loss: 0.011904614977538586, acc: 0.9972972869873047)
[2025-02-13 03:23:07,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:08,210][root][INFO] - Training Epoch: 1/2, step 6681/7134 completed (loss: 0.010988127440214157, acc: 0.9958419799804688)
[2025-02-13 03:23:08,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:08,604][root][INFO] - Training Epoch: 1/2, step 6682/7134 completed (loss: 0.04695175215601921, acc: 0.9906542301177979)
[2025-02-13 03:23:08,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:08,958][root][INFO] - Training Epoch: 1/2, step 6683/7134 completed (loss: 0.025457199662923813, acc: 0.9890109896659851)
[2025-02-13 03:23:09,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:09,359][root][INFO] - Training Epoch: 1/2, step 6684/7134 completed (loss: 0.039180219173431396, acc: 0.987500011920929)
[2025-02-13 03:23:09,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:09,750][root][INFO] - Training Epoch: 1/2, step 6685/7134 completed (loss: 0.03275972977280617, acc: 0.9923664331436157)
[2025-02-13 03:23:09,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:10,158][root][INFO] - Training Epoch: 1/2, step 6686/7134 completed (loss: 0.0875590518116951, acc: 0.9793388247489929)
[2025-02-13 03:23:10,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:10,517][root][INFO] - Training Epoch: 1/2, step 6687/7134 completed (loss: 0.08751935511827469, acc: 0.9691358208656311)
[2025-02-13 03:23:10,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:10,970][root][INFO] - Training Epoch: 1/2, step 6688/7134 completed (loss: 0.1082226037979126, acc: 0.975944995880127)
[2025-02-13 03:23:11,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:11,381][root][INFO] - Training Epoch: 1/2, step 6689/7134 completed (loss: 0.07142925262451172, acc: 0.9841583967208862)
[2025-02-13 03:23:11,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:11,761][root][INFO] - Training Epoch: 1/2, step 6690/7134 completed (loss: 0.15189312398433685, acc: 0.9539951682090759)
[2025-02-13 03:23:11,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:12,172][root][INFO] - Training Epoch: 1/2, step 6691/7134 completed (loss: 0.030218513682484627, acc: 0.9934533834457397)
[2025-02-13 03:23:12,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:12,518][root][INFO] - Training Epoch: 1/2, step 6692/7134 completed (loss: 0.03894323855638504, acc: 0.9851694703102112)
[2025-02-13 03:23:12,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:12,900][root][INFO] - Training Epoch: 1/2, step 6693/7134 completed (loss: 0.043462276458740234, acc: 0.9818481802940369)
[2025-02-13 03:23:13,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:13,318][root][INFO] - Training Epoch: 1/2, step 6694/7134 completed (loss: 0.07792405784130096, acc: 0.9785932898521423)
[2025-02-13 03:23:13,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:13,731][root][INFO] - Training Epoch: 1/2, step 6695/7134 completed (loss: 0.053213778883218765, acc: 0.9876881241798401)
[2025-02-13 03:23:13,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:14,149][root][INFO] - Training Epoch: 1/2, step 6696/7134 completed (loss: 0.08793505281209946, acc: 0.9845201373100281)
[2025-02-13 03:23:14,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:14,552][root][INFO] - Training Epoch: 1/2, step 6697/7134 completed (loss: 0.0945705696940422, acc: 0.9688524603843689)
[2025-02-13 03:23:14,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:14,967][root][INFO] - Training Epoch: 1/2, step 6698/7134 completed (loss: 0.028600484132766724, acc: 0.9896142482757568)
[2025-02-13 03:23:15,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:15,385][root][INFO] - Training Epoch: 1/2, step 6699/7134 completed (loss: 0.05296703800559044, acc: 0.9886105060577393)
[2025-02-13 03:23:15,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:15,806][root][INFO] - Training Epoch: 1/2, step 6700/7134 completed (loss: 0.07999330759048462, acc: 0.9779411554336548)
[2025-02-13 03:23:15,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:16,200][root][INFO] - Training Epoch: 1/2, step 6701/7134 completed (loss: 0.12761983275413513, acc: 0.9682539701461792)
[2025-02-13 03:23:16,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:16,589][root][INFO] - Training Epoch: 1/2, step 6702/7134 completed (loss: 0.05984361842274666, acc: 0.9783890247344971)
[2025-02-13 03:23:16,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:16,996][root][INFO] - Training Epoch: 1/2, step 6703/7134 completed (loss: 0.11753363162279129, acc: 0.9671052694320679)
[2025-02-13 03:23:17,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:17,422][root][INFO] - Training Epoch: 1/2, step 6704/7134 completed (loss: 0.1401250958442688, acc: 0.963777482509613)
[2025-02-13 03:23:17,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:17,847][root][INFO] - Training Epoch: 1/2, step 6705/7134 completed (loss: 0.06640070676803589, acc: 0.9869961142539978)
[2025-02-13 03:23:17,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:18,213][root][INFO] - Training Epoch: 1/2, step 6706/7134 completed (loss: 0.05342848598957062, acc: 0.9834710955619812)
[2025-02-13 03:23:18,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:18,624][root][INFO] - Training Epoch: 1/2, step 6707/7134 completed (loss: 0.07739346474409103, acc: 0.9783037304878235)
[2025-02-13 03:23:18,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:19,057][root][INFO] - Training Epoch: 1/2, step 6708/7134 completed (loss: 0.059147465974092484, acc: 0.9834983348846436)
[2025-02-13 03:23:19,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:19,474][root][INFO] - Training Epoch: 1/2, step 6709/7134 completed (loss: 0.03880373015999794, acc: 0.9895209670066833)
[2025-02-13 03:23:19,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:19,883][root][INFO] - Training Epoch: 1/2, step 6710/7134 completed (loss: 0.10314718633890152, acc: 0.97074955701828)
[2025-02-13 03:23:20,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:20,325][root][INFO] - Training Epoch: 1/2, step 6711/7134 completed (loss: 0.043386273086071014, acc: 0.9936507940292358)
[2025-02-13 03:23:20,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:20,724][root][INFO] - Training Epoch: 1/2, step 6712/7134 completed (loss: 0.06812155246734619, acc: 0.9818456768989563)
[2025-02-13 03:23:20,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:21,227][root][INFO] - Training Epoch: 1/2, step 6713/7134 completed (loss: 0.06055489182472229, acc: 0.9826338887214661)
[2025-02-13 03:23:21,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:21,651][root][INFO] - Training Epoch: 1/2, step 6714/7134 completed (loss: 0.04502253606915474, acc: 0.9821693897247314)
[2025-02-13 03:23:21,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:22,057][root][INFO] - Training Epoch: 1/2, step 6715/7134 completed (loss: 0.0740635097026825, acc: 0.9748822450637817)
[2025-02-13 03:23:22,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:22,470][root][INFO] - Training Epoch: 1/2, step 6716/7134 completed (loss: 0.01933511719107628, acc: 0.9940119981765747)
[2025-02-13 03:23:22,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:22,875][root][INFO] - Training Epoch: 1/2, step 6717/7134 completed (loss: 0.038884349167346954, acc: 0.988727867603302)
[2025-02-13 03:23:23,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:23,279][root][INFO] - Training Epoch: 1/2, step 6718/7134 completed (loss: 0.0765087679028511, acc: 0.978787899017334)
[2025-02-13 03:23:23,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:23,681][root][INFO] - Training Epoch: 1/2, step 6719/7134 completed (loss: 0.03557443991303444, acc: 0.9901960492134094)
[2025-02-13 03:23:23,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:24,096][root][INFO] - Training Epoch: 1/2, step 6720/7134 completed (loss: 0.08443547040224075, acc: 0.9832402467727661)
[2025-02-13 03:23:24,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:24,479][root][INFO] - Training Epoch: 1/2, step 6721/7134 completed (loss: 0.06623189151287079, acc: 0.9843993782997131)
[2025-02-13 03:23:24,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:24,816][root][INFO] - Training Epoch: 1/2, step 6722/7134 completed (loss: 0.1072683185338974, acc: 0.9718804955482483)
[2025-02-13 03:23:24,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:25,234][root][INFO] - Training Epoch: 1/2, step 6723/7134 completed (loss: 0.028633343055844307, acc: 0.9888888597488403)
[2025-02-13 03:23:25,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:25,636][root][INFO] - Training Epoch: 1/2, step 6724/7134 completed (loss: 0.051985740661621094, acc: 0.9838945865631104)
[2025-02-13 03:23:25,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:26,006][root][INFO] - Training Epoch: 1/2, step 6725/7134 completed (loss: 0.03915558010339737, acc: 0.9863481521606445)
[2025-02-13 03:23:26,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:26,383][root][INFO] - Training Epoch: 1/2, step 6726/7134 completed (loss: 0.08817669004201889, acc: 0.9799196720123291)
[2025-02-13 03:23:26,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:26,774][root][INFO] - Training Epoch: 1/2, step 6727/7134 completed (loss: 0.07043122500181198, acc: 0.9841549396514893)
[2025-02-13 03:23:26,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:27,202][root][INFO] - Training Epoch: 1/2, step 6728/7134 completed (loss: 0.06064993143081665, acc: 0.9892473220825195)
[2025-02-13 03:23:27,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:27,640][root][INFO] - Training Epoch: 1/2, step 6729/7134 completed (loss: 0.039298415184020996, acc: 0.99048912525177)
[2025-02-13 03:23:27,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:28,083][root][INFO] - Training Epoch: 1/2, step 6730/7134 completed (loss: 0.039280809462070465, acc: 0.9921507239341736)
[2025-02-13 03:23:28,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:28,514][root][INFO] - Training Epoch: 1/2, step 6731/7134 completed (loss: 0.04457844793796539, acc: 0.9864253401756287)
[2025-02-13 03:23:28,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:28,911][root][INFO] - Training Epoch: 1/2, step 6732/7134 completed (loss: 0.05772901326417923, acc: 0.9858490824699402)
[2025-02-13 03:23:29,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:29,299][root][INFO] - Training Epoch: 1/2, step 6733/7134 completed (loss: 0.046377185732126236, acc: 0.9886147975921631)
[2025-02-13 03:23:29,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:29,681][root][INFO] - Training Epoch: 1/2, step 6734/7134 completed (loss: 0.06825718283653259, acc: 0.9869158864021301)
[2025-02-13 03:23:29,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:30,077][root][INFO] - Training Epoch: 1/2, step 6735/7134 completed (loss: 0.07440482079982758, acc: 0.981566846370697)
[2025-02-13 03:23:30,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:30,531][root][INFO] - Training Epoch: 1/2, step 6736/7134 completed (loss: 0.05363829433917999, acc: 0.9874652028083801)
[2025-02-13 03:23:30,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:30,958][root][INFO] - Training Epoch: 1/2, step 6737/7134 completed (loss: 0.06474632769823074, acc: 0.9821717739105225)
[2025-02-13 03:23:31,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:31,355][root][INFO] - Training Epoch: 1/2, step 6738/7134 completed (loss: 0.050370484590530396, acc: 0.986940324306488)
[2025-02-13 03:23:31,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:31,685][root][INFO] - Training Epoch: 1/2, step 6739/7134 completed (loss: 0.03966331109404564, acc: 0.9871323704719543)
[2025-02-13 03:23:31,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:32,107][root][INFO] - Training Epoch: 1/2, step 6740/7134 completed (loss: 0.049440376460552216, acc: 0.9881154298782349)
[2025-02-13 03:23:32,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:32,512][root][INFO] - Training Epoch: 1/2, step 6741/7134 completed (loss: 0.03650674596428871, acc: 0.9898403286933899)
[2025-02-13 03:23:32,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:32,892][root][INFO] - Training Epoch: 1/2, step 6742/7134 completed (loss: 0.028566401451826096, acc: 0.9927849769592285)
[2025-02-13 03:23:33,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:33,345][root][INFO] - Training Epoch: 1/2, step 6743/7134 completed (loss: 0.01878226175904274, acc: 0.9959127902984619)
[2025-02-13 03:23:33,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:33,750][root][INFO] - Training Epoch: 1/2, step 6744/7134 completed (loss: 0.02775096893310547, acc: 0.9935897588729858)
[2025-02-13 03:23:33,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:34,153][root][INFO] - Training Epoch: 1/2, step 6745/7134 completed (loss: 0.009363234974443913, acc: 0.9972489476203918)
[2025-02-13 03:23:34,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:34,545][root][INFO] - Training Epoch: 1/2, step 6746/7134 completed (loss: 0.023617558181285858, acc: 0.9894179701805115)
[2025-02-13 03:23:34,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:34,958][root][INFO] - Training Epoch: 1/2, step 6747/7134 completed (loss: 0.03038671240210533, acc: 0.992546558380127)
[2025-02-13 03:23:35,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:35,433][root][INFO] - Training Epoch: 1/2, step 6748/7134 completed (loss: 0.026094451546669006, acc: 0.9954904317855835)
[2025-02-13 03:23:35,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:35,902][root][INFO] - Training Epoch: 1/2, step 6749/7134 completed (loss: 0.04819527640938759, acc: 0.9891067743301392)
[2025-02-13 03:23:36,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:36,357][root][INFO] - Training Epoch: 1/2, step 6750/7134 completed (loss: 0.019885988906025887, acc: 0.9965397715568542)
[2025-02-13 03:23:36,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:36,821][root][INFO] - Training Epoch: 1/2, step 6751/7134 completed (loss: 0.017166469246149063, acc: 0.9954338073730469)
[2025-02-13 03:23:36,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:37,236][root][INFO] - Training Epoch: 1/2, step 6752/7134 completed (loss: 0.020496508106589317, acc: 0.9898403286933899)
[2025-02-13 03:23:37,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:37,687][root][INFO] - Training Epoch: 1/2, step 6753/7134 completed (loss: 0.01924951747059822, acc: 0.9965596199035645)
[2025-02-13 03:23:37,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:38,104][root][INFO] - Training Epoch: 1/2, step 6754/7134 completed (loss: 0.0071080648340284824, acc: 0.9975903630256653)
[2025-02-13 03:23:38,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:38,545][root][INFO] - Training Epoch: 1/2, step 6755/7134 completed (loss: 0.013677412644028664, acc: 0.9963054060935974)
[2025-02-13 03:23:38,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:38,973][root][INFO] - Training Epoch: 1/2, step 6756/7134 completed (loss: 0.007837309502065182, acc: 0.9986594915390015)
[2025-02-13 03:23:39,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:39,377][root][INFO] - Training Epoch: 1/2, step 6757/7134 completed (loss: 0.023327501490712166, acc: 0.9939024448394775)
[2025-02-13 03:23:39,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:39,803][root][INFO] - Training Epoch: 1/2, step 6758/7134 completed (loss: 0.01817391999065876, acc: 0.9961190223693848)
[2025-02-13 03:23:39,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:40,258][root][INFO] - Training Epoch: 1/2, step 6759/7134 completed (loss: 0.04567903280258179, acc: 0.9876957535743713)
[2025-02-13 03:23:40,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:40,695][root][INFO] - Training Epoch: 1/2, step 6760/7134 completed (loss: 0.05004842206835747, acc: 0.9855642914772034)
[2025-02-13 03:23:40,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:41,124][root][INFO] - Training Epoch: 1/2, step 6761/7134 completed (loss: 0.03512478619813919, acc: 0.9903614521026611)
[2025-02-13 03:23:41,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:41,565][root][INFO] - Training Epoch: 1/2, step 6762/7134 completed (loss: 0.05150426924228668, acc: 0.9901960492134094)
[2025-02-13 03:23:41,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:42,010][root][INFO] - Training Epoch: 1/2, step 6763/7134 completed (loss: 0.017336314544081688, acc: 0.9949811697006226)
[2025-02-13 03:23:42,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:42,410][root][INFO] - Training Epoch: 1/2, step 6764/7134 completed (loss: 0.029835863038897514, acc: 0.9928160905838013)
[2025-02-13 03:23:42,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:42,890][root][INFO] - Training Epoch: 1/2, step 6765/7134 completed (loss: 0.025219090282917023, acc: 0.9880159497261047)
[2025-02-13 03:23:43,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:43,291][root][INFO] - Training Epoch: 1/2, step 6766/7134 completed (loss: 0.020995846018195152, acc: 0.9917241334915161)
[2025-02-13 03:23:43,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:43,618][root][INFO] - Training Epoch: 1/2, step 6767/7134 completed (loss: 0.06581172347068787, acc: 0.9887387156486511)
[2025-02-13 03:23:43,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:44,057][root][INFO] - Training Epoch: 1/2, step 6768/7134 completed (loss: 0.026928644627332687, acc: 0.9904943108558655)
[2025-02-13 03:23:44,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:44,460][root][INFO] - Training Epoch: 1/2, step 6769/7134 completed (loss: 0.018838031217455864, acc: 0.9922839403152466)
[2025-02-13 03:23:44,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:44,890][root][INFO] - Training Epoch: 1/2, step 6770/7134 completed (loss: 0.04666215926408768, acc: 0.9812080264091492)
[2025-02-13 03:23:45,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:45,325][root][INFO] - Training Epoch: 1/2, step 6771/7134 completed (loss: 0.024922169744968414, acc: 0.9936908483505249)
[2025-02-13 03:23:45,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:45,748][root][INFO] - Training Epoch: 1/2, step 6772/7134 completed (loss: 0.09798463433980942, acc: 0.9786096215248108)
[2025-02-13 03:23:45,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:46,143][root][INFO] - Training Epoch: 1/2, step 6773/7134 completed (loss: 0.0491274893283844, acc: 0.9857594966888428)
[2025-02-13 03:23:46,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:46,563][root][INFO] - Training Epoch: 1/2, step 6774/7134 completed (loss: 0.0619531087577343, acc: 0.987261176109314)
[2025-02-13 03:23:46,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:46,962][root][INFO] - Training Epoch: 1/2, step 6775/7134 completed (loss: 0.020432496443390846, acc: 0.9943289160728455)
[2025-02-13 03:23:47,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:47,372][root][INFO] - Training Epoch: 1/2, step 6776/7134 completed (loss: 0.04265149310231209, acc: 0.9851064085960388)
[2025-02-13 03:23:47,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:47,785][root][INFO] - Training Epoch: 1/2, step 6777/7134 completed (loss: 0.03328949213027954, acc: 0.991946280002594)
[2025-02-13 03:23:47,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:48,224][root][INFO] - Training Epoch: 1/2, step 6778/7134 completed (loss: 0.06456760317087173, acc: 0.9836552739143372)
[2025-02-13 03:23:48,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:48,642][root][INFO] - Training Epoch: 1/2, step 6779/7134 completed (loss: 0.031042039394378662, acc: 0.9880596995353699)
[2025-02-13 03:23:48,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:49,062][root][INFO] - Training Epoch: 1/2, step 6780/7134 completed (loss: 0.021471073850989342, acc: 0.9925261735916138)
[2025-02-13 03:23:49,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:49,497][root][INFO] - Training Epoch: 1/2, step 6781/7134 completed (loss: 0.031411148607730865, acc: 0.9910179376602173)
[2025-02-13 03:23:49,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:49,898][root][INFO] - Training Epoch: 1/2, step 6782/7134 completed (loss: 0.01942262053489685, acc: 0.9982638955116272)
[2025-02-13 03:23:50,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:50,246][root][INFO] - Training Epoch: 1/2, step 6783/7134 completed (loss: 0.05120869725942612, acc: 0.9904030561447144)
[2025-02-13 03:23:50,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:50,667][root][INFO] - Training Epoch: 1/2, step 6784/7134 completed (loss: 0.035311684012413025, acc: 0.9916805028915405)
[2025-02-13 03:23:50,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:51,085][root][INFO] - Training Epoch: 1/2, step 6785/7134 completed (loss: 0.024478338658809662, acc: 0.9889937043190002)
[2025-02-13 03:23:51,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:51,530][root][INFO] - Training Epoch: 1/2, step 6786/7134 completed (loss: 0.010927128605544567, acc: 0.9956834316253662)
[2025-02-13 03:23:51,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:51,970][root][INFO] - Training Epoch: 1/2, step 6787/7134 completed (loss: 0.0300610288977623, acc: 0.9894737005233765)
[2025-02-13 03:23:52,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:52,380][root][INFO] - Training Epoch: 1/2, step 6788/7134 completed (loss: 0.01979290135204792, acc: 0.9964028596878052)
[2025-02-13 03:23:52,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:52,769][root][INFO] - Training Epoch: 1/2, step 6789/7134 completed (loss: 0.02125932276248932, acc: 0.994163453578949)
[2025-02-13 03:23:52,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:53,198][root][INFO] - Training Epoch: 1/2, step 6790/7134 completed (loss: 0.061446502804756165, acc: 0.9870689511299133)
[2025-02-13 03:23:53,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:53,599][root][INFO] - Training Epoch: 1/2, step 6791/7134 completed (loss: 0.024864671751856804, acc: 0.9931856989860535)
[2025-02-13 03:23:53,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:54,012][root][INFO] - Training Epoch: 1/2, step 6792/7134 completed (loss: 0.02546541765332222, acc: 0.9908257126808167)
[2025-02-13 03:23:54,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:54,446][root][INFO] - Training Epoch: 1/2, step 6793/7134 completed (loss: 0.028570784255862236, acc: 0.9894514679908752)
[2025-02-13 03:23:54,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:54,861][root][INFO] - Training Epoch: 1/2, step 6794/7134 completed (loss: 0.028639184311032295, acc: 0.9967159032821655)
[2025-02-13 03:23:54,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:55,288][root][INFO] - Training Epoch: 1/2, step 6795/7134 completed (loss: 0.01371240708976984, acc: 0.9961038827896118)
[2025-02-13 03:23:55,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:55,701][root][INFO] - Training Epoch: 1/2, step 6796/7134 completed (loss: 0.015209193341434002, acc: 0.9956011772155762)
[2025-02-13 03:23:55,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:56,115][root][INFO] - Training Epoch: 1/2, step 6797/7134 completed (loss: 0.07959665358066559, acc: 0.9796215295791626)
[2025-02-13 03:23:56,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:56,526][root][INFO] - Training Epoch: 1/2, step 6798/7134 completed (loss: 0.04100082442164421, acc: 0.9834710955619812)
[2025-02-13 03:23:56,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:56,940][root][INFO] - Training Epoch: 1/2, step 6799/7134 completed (loss: 0.030056195333600044, acc: 0.9904240965843201)
[2025-02-13 03:23:57,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:57,341][root][INFO] - Training Epoch: 1/2, step 6800/7134 completed (loss: 0.03870441019535065, acc: 0.9875690340995789)
[2025-02-13 03:23:57,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:57,792][root][INFO] - Training Epoch: 1/2, step 6801/7134 completed (loss: 0.03557492047548294, acc: 0.9905511736869812)
[2025-02-13 03:23:57,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:58,195][root][INFO] - Training Epoch: 1/2, step 6802/7134 completed (loss: 0.027947066351771355, acc: 0.9912152290344238)
[2025-02-13 03:23:58,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:58,625][root][INFO] - Training Epoch: 1/2, step 6803/7134 completed (loss: 0.01884339191019535, acc: 0.9937106966972351)
[2025-02-13 03:23:58,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:59,022][root][INFO] - Training Epoch: 1/2, step 6804/7134 completed (loss: 0.03016108274459839, acc: 0.9921875)
[2025-02-13 03:23:59,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:59,465][root][INFO] - Training Epoch: 1/2, step 6805/7134 completed (loss: 0.056796930730342865, acc: 0.9855769276618958)
[2025-02-13 03:23:59,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:59,896][root][INFO] - Training Epoch: 1/2, step 6806/7134 completed (loss: 0.054380372166633606, acc: 0.9898734092712402)
[2025-02-13 03:24:00,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:00,354][root][INFO] - Training Epoch: 1/2, step 6807/7134 completed (loss: 0.01964714005589485, acc: 0.9956896305084229)
[2025-02-13 03:24:00,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:00,775][root][INFO] - Training Epoch: 1/2, step 6808/7134 completed (loss: 0.03715858235955238, acc: 0.9904305934906006)
[2025-02-13 03:24:00,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:01,218][root][INFO] - Training Epoch: 1/2, step 6809/7134 completed (loss: 0.0523807592689991, acc: 0.989159882068634)
[2025-02-13 03:24:01,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:01,617][root][INFO] - Training Epoch: 1/2, step 6810/7134 completed (loss: 0.025969669222831726, acc: 0.9947368502616882)
[2025-02-13 03:24:01,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:02,066][root][INFO] - Training Epoch: 1/2, step 6811/7134 completed (loss: 0.05286980792880058, acc: 0.9868938326835632)
[2025-02-13 03:24:02,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:02,471][root][INFO] - Training Epoch: 1/2, step 6812/7134 completed (loss: 0.036973197013139725, acc: 0.990275502204895)
[2025-02-13 03:24:02,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:02,887][root][INFO] - Training Epoch: 1/2, step 6813/7134 completed (loss: 0.011859326623380184, acc: 0.9985915422439575)
[2025-02-13 03:24:03,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:03,306][root][INFO] - Training Epoch: 1/2, step 6814/7134 completed (loss: 0.03405684605240822, acc: 0.9905914068222046)
[2025-02-13 03:24:03,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:03,732][root][INFO] - Training Epoch: 1/2, step 6815/7134 completed (loss: 0.04192885383963585, acc: 0.9881734848022461)
[2025-02-13 03:24:03,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:04,151][root][INFO] - Training Epoch: 1/2, step 6816/7134 completed (loss: 0.03635149821639061, acc: 0.9933110475540161)
[2025-02-13 03:24:04,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:04,597][root][INFO] - Training Epoch: 1/2, step 6817/7134 completed (loss: 0.05935417115688324, acc: 0.9863247871398926)
[2025-02-13 03:24:04,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:05,016][root][INFO] - Training Epoch: 1/2, step 6818/7134 completed (loss: 0.021195001900196075, acc: 0.9910045266151428)
[2025-02-13 03:24:05,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:05,441][root][INFO] - Training Epoch: 1/2, step 6819/7134 completed (loss: 0.054145876318216324, acc: 0.9820144176483154)
[2025-02-13 03:24:05,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:05,847][root][INFO] - Training Epoch: 1/2, step 6820/7134 completed (loss: 0.032240431755781174, acc: 0.9927927851676941)
[2025-02-13 03:24:05,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:06,200][root][INFO] - Training Epoch: 1/2, step 6821/7134 completed (loss: 0.02578376792371273, acc: 0.9939393997192383)
[2025-02-13 03:24:06,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:06,618][root][INFO] - Training Epoch: 1/2, step 6822/7134 completed (loss: 0.05036180466413498, acc: 0.9895287752151489)
[2025-02-13 03:24:06,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:07,037][root][INFO] - Training Epoch: 1/2, step 6823/7134 completed (loss: 0.03659866750240326, acc: 0.9928469061851501)
[2025-02-13 03:24:07,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:07,364][root][INFO] - Training Epoch: 1/2, step 6824/7134 completed (loss: 0.07947912812232971, acc: 0.9737903475761414)
[2025-02-13 03:24:07,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:07,751][root][INFO] - Training Epoch: 1/2, step 6825/7134 completed (loss: 0.025292444974184036, acc: 0.992682933807373)
[2025-02-13 03:24:07,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:08,139][root][INFO] - Training Epoch: 1/2, step 6826/7134 completed (loss: 0.05156208574771881, acc: 0.9865319728851318)
[2025-02-13 03:24:08,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:08,562][root][INFO] - Training Epoch: 1/2, step 6827/7134 completed (loss: 0.13411006331443787, acc: 0.9664948582649231)
[2025-02-13 03:24:08,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:08,982][root][INFO] - Training Epoch: 1/2, step 6828/7134 completed (loss: 0.054234642535448074, acc: 0.9835766553878784)
[2025-02-13 03:24:09,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:09,406][root][INFO] - Training Epoch: 1/2, step 6829/7134 completed (loss: 0.06326358020305634, acc: 0.9821958541870117)
[2025-02-13 03:24:09,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:09,799][root][INFO] - Training Epoch: 1/2, step 6830/7134 completed (loss: 0.039203524589538574, acc: 0.9851729869842529)
[2025-02-13 03:24:09,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:10,215][root][INFO] - Training Epoch: 1/2, step 6831/7134 completed (loss: 0.047635845839977264, acc: 0.9865591526031494)
[2025-02-13 03:24:10,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:10,622][root][INFO] - Training Epoch: 1/2, step 6832/7134 completed (loss: 0.061749618500471115, acc: 0.9815157055854797)
[2025-02-13 03:24:10,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:11,026][root][INFO] - Training Epoch: 1/2, step 6833/7134 completed (loss: 0.04764460772275925, acc: 0.9903537034988403)
[2025-02-13 03:24:11,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:11,371][root][INFO] - Training Epoch: 1/2, step 6834/7134 completed (loss: 0.052260495722293854, acc: 0.9846677780151367)
[2025-02-13 03:24:11,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:11,760][root][INFO] - Training Epoch: 1/2, step 6835/7134 completed (loss: 0.03757098317146301, acc: 0.9886363744735718)
[2025-02-13 03:24:11,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:12,157][root][INFO] - Training Epoch: 1/2, step 6836/7134 completed (loss: 0.0561908483505249, acc: 0.9811557531356812)
[2025-02-13 03:24:12,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:12,602][root][INFO] - Training Epoch: 1/2, step 6837/7134 completed (loss: 0.04369721561670303, acc: 0.9901840686798096)
[2025-02-13 03:24:12,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:13,005][root][INFO] - Training Epoch: 1/2, step 6838/7134 completed (loss: 0.07935424894094467, acc: 0.9757834672927856)
[2025-02-13 03:24:13,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:13,390][root][INFO] - Training Epoch: 1/2, step 6839/7134 completed (loss: 0.103289894759655, acc: 0.9636963605880737)
[2025-02-13 03:24:13,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:13,793][root][INFO] - Training Epoch: 1/2, step 6840/7134 completed (loss: 0.08407150954008102, acc: 0.9785100221633911)
[2025-02-13 03:24:13,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:14,195][root][INFO] - Training Epoch: 1/2, step 6841/7134 completed (loss: 0.06103816255927086, acc: 0.9797979593276978)
[2025-02-13 03:24:14,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:14,641][root][INFO] - Training Epoch: 1/2, step 6842/7134 completed (loss: 0.04376330226659775, acc: 0.9849624037742615)
[2025-02-13 03:24:14,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:15,094][root][INFO] - Training Epoch: 1/2, step 6843/7134 completed (loss: 0.07121838629245758, acc: 0.9790382385253906)
[2025-02-13 03:24:15,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:15,506][root][INFO] - Training Epoch: 1/2, step 6844/7134 completed (loss: 0.040909431874752045, acc: 0.9879102110862732)
[2025-02-13 03:24:15,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:15,946][root][INFO] - Training Epoch: 1/2, step 6845/7134 completed (loss: 0.03818380460143089, acc: 0.9897959232330322)
[2025-02-13 03:24:16,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:16,351][root][INFO] - Training Epoch: 1/2, step 6846/7134 completed (loss: 0.050985872745513916, acc: 0.984240710735321)
[2025-02-13 03:24:16,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:16,774][root][INFO] - Training Epoch: 1/2, step 6847/7134 completed (loss: 0.053466491401195526, acc: 0.9833333492279053)
[2025-02-13 03:24:16,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:17,167][root][INFO] - Training Epoch: 1/2, step 6848/7134 completed (loss: 0.023544836789369583, acc: 0.9945945739746094)
[2025-02-13 03:24:17,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:17,581][root][INFO] - Training Epoch: 1/2, step 6849/7134 completed (loss: 0.04307379201054573, acc: 0.9862542748451233)
[2025-02-13 03:24:17,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:17,976][root][INFO] - Training Epoch: 1/2, step 6850/7134 completed (loss: 0.03221212327480316, acc: 0.9908466935157776)
[2025-02-13 03:24:18,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:18,401][root][INFO] - Training Epoch: 1/2, step 6851/7134 completed (loss: 0.06269001215696335, acc: 0.9788434505462646)
[2025-02-13 03:24:18,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:18,830][root][INFO] - Training Epoch: 1/2, step 6852/7134 completed (loss: 0.030863719061017036, acc: 0.9930070042610168)
[2025-02-13 03:24:18,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:19,297][root][INFO] - Training Epoch: 1/2, step 6853/7134 completed (loss: 0.04340299218893051, acc: 0.9889349937438965)
[2025-02-13 03:24:19,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:19,645][root][INFO] - Training Epoch: 1/2, step 6854/7134 completed (loss: 0.0399387888610363, acc: 0.9934924244880676)
[2025-02-13 03:24:19,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:20,034][root][INFO] - Training Epoch: 1/2, step 6855/7134 completed (loss: 0.012662398628890514, acc: 0.9952830076217651)
[2025-02-13 03:24:20,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:20,456][root][INFO] - Training Epoch: 1/2, step 6856/7134 completed (loss: 0.011695623397827148, acc: 0.995708167552948)
[2025-02-13 03:24:20,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:20,894][root][INFO] - Training Epoch: 1/2, step 6857/7134 completed (loss: 0.03503291681408882, acc: 0.9897360801696777)
[2025-02-13 03:24:20,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:21,168][root][INFO] - Training Epoch: 1/2, step 6858/7134 completed (loss: 0.0099186385050416, acc: 0.9963099360466003)
[2025-02-13 03:24:21,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:21,614][root][INFO] - Training Epoch: 1/2, step 6859/7134 completed (loss: 0.013085144571959972, acc: 0.9974392056465149)
[2025-02-13 03:24:21,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:22,029][root][INFO] - Training Epoch: 1/2, step 6860/7134 completed (loss: 0.020892778411507607, acc: 0.9929078221321106)
[2025-02-13 03:24:22,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:22,412][root][INFO] - Training Epoch: 1/2, step 6861/7134 completed (loss: 0.020050374791026115, acc: 0.9982699155807495)
[2025-02-13 03:24:22,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:22,855][root][INFO] - Training Epoch: 1/2, step 6862/7134 completed (loss: 0.04206164553761482, acc: 0.9885714054107666)
[2025-02-13 03:24:22,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:23,268][root][INFO] - Training Epoch: 1/2, step 6863/7134 completed (loss: 0.0151585778221488, acc: 0.9957627058029175)
[2025-02-13 03:24:23,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:23,712][root][INFO] - Training Epoch: 1/2, step 6864/7134 completed (loss: 0.01834150403738022, acc: 0.9933444261550903)
[2025-02-13 03:24:23,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:24,126][root][INFO] - Training Epoch: 1/2, step 6865/7134 completed (loss: 0.0481986440718174, acc: 0.983505129814148)
[2025-02-13 03:24:24,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:24,525][root][INFO] - Training Epoch: 1/2, step 6866/7134 completed (loss: 0.007662036921828985, acc: 0.9984802603721619)
[2025-02-13 03:24:24,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:24,940][root][INFO] - Training Epoch: 1/2, step 6867/7134 completed (loss: 0.008273014798760414, acc: 0.9962962865829468)
[2025-02-13 03:24:25,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:25,264][root][INFO] - Training Epoch: 1/2, step 6868/7134 completed (loss: 0.003515287535265088, acc: 1.0)
[2025-02-13 03:24:25,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:25,610][root][INFO] - Training Epoch: 1/2, step 6869/7134 completed (loss: 0.008780078962445259, acc: 1.0)
[2025-02-13 03:24:25,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:26,044][root][INFO] - Training Epoch: 1/2, step 6870/7134 completed (loss: 0.017000460997223854, acc: 0.9951140284538269)
[2025-02-13 03:24:26,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:26,483][root][INFO] - Training Epoch: 1/2, step 6871/7134 completed (loss: 0.005921522155404091, acc: 0.9981273412704468)
[2025-02-13 03:24:26,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:26,884][root][INFO] - Training Epoch: 1/2, step 6872/7134 completed (loss: 0.009767013601958752, acc: 0.9944598078727722)
[2025-02-13 03:24:27,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:27,313][root][INFO] - Training Epoch: 1/2, step 6873/7134 completed (loss: 0.01069968193769455, acc: 0.9971988797187805)
[2025-02-13 03:24:27,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:27,716][root][INFO] - Training Epoch: 1/2, step 6874/7134 completed (loss: 0.02097279392182827, acc: 0.9946808218955994)
[2025-02-13 03:24:27,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:28,133][root][INFO] - Training Epoch: 1/2, step 6875/7134 completed (loss: 0.01837964914739132, acc: 0.9938176274299622)
[2025-02-13 03:24:28,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:28,531][root][INFO] - Training Epoch: 1/2, step 6876/7134 completed (loss: 0.031843941658735275, acc: 0.989847719669342)
[2025-02-13 03:24:28,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:28,965][root][INFO] - Training Epoch: 1/2, step 6877/7134 completed (loss: 0.020529454573988914, acc: 0.992977499961853)
[2025-02-13 03:24:29,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:29,382][root][INFO] - Training Epoch: 1/2, step 6878/7134 completed (loss: 0.018484093248844147, acc: 0.9940029978752136)
[2025-02-13 03:24:29,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:29,711][root][INFO] - Training Epoch: 1/2, step 6879/7134 completed (loss: 0.028848011046648026, acc: 0.9895397424697876)
[2025-02-13 03:24:29,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:30,166][root][INFO] - Training Epoch: 1/2, step 6880/7134 completed (loss: 0.021540183573961258, acc: 0.9941792488098145)
[2025-02-13 03:24:30,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:30,608][root][INFO] - Training Epoch: 1/2, step 6881/7134 completed (loss: 0.08121339231729507, acc: 0.9854192137718201)
[2025-02-13 03:24:30,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:30,982][root][INFO] - Training Epoch: 1/2, step 6882/7134 completed (loss: 0.044002655893564224, acc: 0.9849624037742615)
[2025-02-13 03:24:31,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:31,381][root][INFO] - Training Epoch: 1/2, step 6883/7134 completed (loss: 0.03198227658867836, acc: 0.9950494766235352)
[2025-02-13 03:24:31,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:31,819][root][INFO] - Training Epoch: 1/2, step 6884/7134 completed (loss: 0.06565587967634201, acc: 0.989924430847168)
[2025-02-13 03:24:31,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:32,194][root][INFO] - Training Epoch: 1/2, step 6885/7134 completed (loss: 0.07353239506483078, acc: 0.9888392686843872)
[2025-02-13 03:24:32,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:32,603][root][INFO] - Training Epoch: 1/2, step 6886/7134 completed (loss: 0.029902217909693718, acc: 0.9857369065284729)
[2025-02-13 03:24:32,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:32,992][root][INFO] - Training Epoch: 1/2, step 6887/7134 completed (loss: 0.05529329180717468, acc: 0.9856262803077698)
[2025-02-13 03:24:33,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:33,428][root][INFO] - Training Epoch: 1/2, step 6888/7134 completed (loss: 0.02765258587896824, acc: 0.9916943311691284)
[2025-02-13 03:24:33,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:33,861][root][INFO] - Training Epoch: 1/2, step 6889/7134 completed (loss: 0.018930718302726746, acc: 0.9954057931900024)
[2025-02-13 03:24:33,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:34,299][root][INFO] - Training Epoch: 1/2, step 6890/7134 completed (loss: 0.03066015988588333, acc: 0.9907529950141907)
[2025-02-13 03:24:34,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:34,704][root][INFO] - Training Epoch: 1/2, step 6891/7134 completed (loss: 0.028159363195300102, acc: 0.9911110997200012)
[2025-02-13 03:24:34,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:35,142][root][INFO] - Training Epoch: 1/2, step 6892/7134 completed (loss: 0.028129061684012413, acc: 0.9908854365348816)
[2025-02-13 03:24:35,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:35,571][root][INFO] - Training Epoch: 1/2, step 6893/7134 completed (loss: 0.05748501420021057, acc: 0.980140209197998)
[2025-02-13 03:24:35,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:36,006][root][INFO] - Training Epoch: 1/2, step 6894/7134 completed (loss: 0.054187845438718796, acc: 0.9855263233184814)
[2025-02-13 03:24:36,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:36,448][root][INFO] - Training Epoch: 1/2, step 6895/7134 completed (loss: 0.04908636212348938, acc: 0.9820144176483154)
[2025-02-13 03:24:36,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:36,835][root][INFO] - Training Epoch: 1/2, step 6896/7134 completed (loss: 0.025482501834630966, acc: 0.9898550510406494)
[2025-02-13 03:24:36,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:37,272][root][INFO] - Training Epoch: 1/2, step 6897/7134 completed (loss: 0.02159108966588974, acc: 0.9938650131225586)
[2025-02-13 03:24:37,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:37,673][root][INFO] - Training Epoch: 1/2, step 6898/7134 completed (loss: 0.021011438220739365, acc: 0.9945054650306702)
[2025-02-13 03:24:37,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:38,110][root][INFO] - Training Epoch: 1/2, step 6899/7134 completed (loss: 0.029258498921990395, acc: 0.9899497628211975)
[2025-02-13 03:24:38,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:38,546][root][INFO] - Training Epoch: 1/2, step 6900/7134 completed (loss: 0.047105442732572556, acc: 0.9801980257034302)
[2025-02-13 03:24:38,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:38,987][root][INFO] - Training Epoch: 1/2, step 6901/7134 completed (loss: 0.03339057043194771, acc: 0.9920739531517029)
[2025-02-13 03:24:39,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:39,444][root][INFO] - Training Epoch: 1/2, step 6902/7134 completed (loss: 0.03882024809718132, acc: 0.9861303567886353)
[2025-02-13 03:24:39,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:39,877][root][INFO] - Training Epoch: 1/2, step 6903/7134 completed (loss: 0.028439445421099663, acc: 0.9936628937721252)
[2025-02-13 03:24:40,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:40,319][root][INFO] - Training Epoch: 1/2, step 6904/7134 completed (loss: 0.0143580948933959, acc: 0.996268630027771)
[2025-02-13 03:24:40,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:40,765][root][INFO] - Training Epoch: 1/2, step 6905/7134 completed (loss: 0.02867697924375534, acc: 0.9910485744476318)
[2025-02-13 03:24:40,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:41,224][root][INFO] - Training Epoch: 1/2, step 6906/7134 completed (loss: 0.0399249903857708, acc: 0.9928143620491028)
[2025-02-13 03:24:41,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:41,667][root][INFO] - Training Epoch: 1/2, step 6907/7134 completed (loss: 0.03760330379009247, acc: 0.9913151264190674)
[2025-02-13 03:24:41,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:42,100][root][INFO] - Training Epoch: 1/2, step 6908/7134 completed (loss: 0.029449544847011566, acc: 0.9922279715538025)
[2025-02-13 03:24:42,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:42,535][root][INFO] - Training Epoch: 1/2, step 6909/7134 completed (loss: 0.02024216204881668, acc: 0.996052622795105)
[2025-02-13 03:24:42,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:42,968][root][INFO] - Training Epoch: 1/2, step 6910/7134 completed (loss: 0.016308236867189407, acc: 0.9961783289909363)
[2025-02-13 03:24:43,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:43,351][root][INFO] - Training Epoch: 1/2, step 6911/7134 completed (loss: 0.05535618215799332, acc: 0.9883720874786377)
[2025-02-13 03:24:43,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:43,792][root][INFO] - Training Epoch: 1/2, step 6912/7134 completed (loss: 0.028966201469302177, acc: 0.9962546825408936)
[2025-02-13 03:24:43,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:44,266][root][INFO] - Training Epoch: 1/2, step 6913/7134 completed (loss: 0.036127906292676926, acc: 0.986601710319519)
[2025-02-13 03:24:44,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:44,665][root][INFO] - Training Epoch: 1/2, step 6914/7134 completed (loss: 0.069719597697258, acc: 0.9865319728851318)
[2025-02-13 03:24:44,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:45,105][root][INFO] - Training Epoch: 1/2, step 6915/7134 completed (loss: 0.03514675050973892, acc: 0.984455943107605)
[2025-02-13 03:24:45,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:45,556][root][INFO] - Training Epoch: 1/2, step 6916/7134 completed (loss: 0.029915178194642067, acc: 0.9938650131225586)
[2025-02-13 03:24:45,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:46,018][root][INFO] - Training Epoch: 1/2, step 6917/7134 completed (loss: 0.0674506351351738, acc: 0.9848484992980957)
[2025-02-13 03:24:46,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:46,434][root][INFO] - Training Epoch: 1/2, step 6918/7134 completed (loss: 0.04192609712481499, acc: 0.9859485030174255)
[2025-02-13 03:24:46,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:46,868][root][INFO] - Training Epoch: 1/2, step 6919/7134 completed (loss: 0.04388239234685898, acc: 0.9896193742752075)
[2025-02-13 03:24:46,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:47,272][root][INFO] - Training Epoch: 1/2, step 6920/7134 completed (loss: 0.020917804911732674, acc: 0.9914529919624329)
[2025-02-13 03:24:47,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:47,726][root][INFO] - Training Epoch: 1/2, step 6921/7134 completed (loss: 0.07356608659029007, acc: 0.9815242290496826)
[2025-02-13 03:24:47,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:48,105][root][INFO] - Training Epoch: 1/2, step 6922/7134 completed (loss: 0.06638554483652115, acc: 0.9859594106674194)
[2025-02-13 03:24:48,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:48,546][root][INFO] - Training Epoch: 1/2, step 6923/7134 completed (loss: 0.05884441360831261, acc: 0.9883268475532532)
[2025-02-13 03:24:48,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:48,986][root][INFO] - Training Epoch: 1/2, step 6924/7134 completed (loss: 0.02824668027460575, acc: 0.9863760471343994)
[2025-02-13 03:24:49,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:49,412][root][INFO] - Training Epoch: 1/2, step 6925/7134 completed (loss: 0.031163794919848442, acc: 0.9895833134651184)
[2025-02-13 03:24:49,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:49,826][root][INFO] - Training Epoch: 1/2, step 6926/7134 completed (loss: 0.05633604899048805, acc: 0.978300154209137)
[2025-02-13 03:24:49,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:50,277][root][INFO] - Training Epoch: 1/2, step 6927/7134 completed (loss: 0.04323718324303627, acc: 0.986810564994812)
[2025-02-13 03:24:50,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:50,662][root][INFO] - Training Epoch: 1/2, step 6928/7134 completed (loss: 0.07134377211332321, acc: 0.9823874831199646)
[2025-02-13 03:24:50,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:51,155][root][INFO] - Training Epoch: 1/2, step 6929/7134 completed (loss: 0.06077529489994049, acc: 0.9830508232116699)
[2025-02-13 03:24:51,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:51,615][root][INFO] - Training Epoch: 1/2, step 6930/7134 completed (loss: 0.0773928314447403, acc: 0.9826302528381348)
[2025-02-13 03:24:51,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:52,078][root][INFO] - Training Epoch: 1/2, step 6931/7134 completed (loss: 0.04628423973917961, acc: 0.9853658676147461)
[2025-02-13 03:24:52,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:52,443][root][INFO] - Training Epoch: 1/2, step 6932/7134 completed (loss: 0.055856846272945404, acc: 0.9779411554336548)
[2025-02-13 03:24:52,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:52,866][root][INFO] - Training Epoch: 1/2, step 6933/7134 completed (loss: 0.08944631367921829, acc: 0.9793014526367188)
[2025-02-13 03:24:52,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:53,236][root][INFO] - Training Epoch: 1/2, step 6934/7134 completed (loss: 0.05369286239147186, acc: 0.9807383418083191)
[2025-02-13 03:24:53,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:53,693][root][INFO] - Training Epoch: 1/2, step 6935/7134 completed (loss: 0.059145908802747726, acc: 0.985602080821991)
[2025-02-13 03:24:53,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:54,147][root][INFO] - Training Epoch: 1/2, step 6936/7134 completed (loss: 0.03691798448562622, acc: 0.9866666793823242)
[2025-02-13 03:24:54,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:54,559][root][INFO] - Training Epoch: 1/2, step 6937/7134 completed (loss: 0.11340128630399704, acc: 0.9719789624214172)
[2025-02-13 03:24:54,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:55,004][root][INFO] - Training Epoch: 1/2, step 6938/7134 completed (loss: 0.132112056016922, acc: 0.96875)
[2025-02-13 03:24:55,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:55,467][root][INFO] - Training Epoch: 1/2, step 6939/7134 completed (loss: 0.09593262523412704, acc: 0.9791377186775208)
[2025-02-13 03:24:55,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:55,867][root][INFO] - Training Epoch: 1/2, step 6940/7134 completed (loss: 0.08449599146842957, acc: 0.9811643958091736)
[2025-02-13 03:24:55,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:56,223][root][INFO] - Training Epoch: 1/2, step 6941/7134 completed (loss: 0.03961716964840889, acc: 0.9913194179534912)
[2025-02-13 03:24:56,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:56,633][root][INFO] - Training Epoch: 1/2, step 6942/7134 completed (loss: 0.0758485347032547, acc: 0.9767123460769653)
[2025-02-13 03:24:56,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:56,975][root][INFO] - Training Epoch: 1/2, step 6943/7134 completed (loss: 0.0437711663544178, acc: 0.991631805896759)
[2025-02-13 03:24:57,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:57,366][root][INFO] - Training Epoch: 1/2, step 6944/7134 completed (loss: 0.040628354996442795, acc: 0.9920381903648376)
[2025-02-13 03:24:57,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:57,724][root][INFO] - Training Epoch: 1/2, step 6945/7134 completed (loss: 0.0773029550909996, acc: 0.9842932224273682)
[2025-02-13 03:24:57,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:58,053][root][INFO] - Training Epoch: 1/2, step 6946/7134 completed (loss: 0.050170738250017166, acc: 0.982807993888855)
[2025-02-13 03:24:58,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:58,452][root][INFO] - Training Epoch: 1/2, step 6947/7134 completed (loss: 0.044261906296014786, acc: 0.9826086759567261)
[2025-02-13 03:24:58,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:58,874][root][INFO] - Training Epoch: 1/2, step 6948/7134 completed (loss: 0.0679415613412857, acc: 0.9782244563102722)
[2025-02-13 03:24:59,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:59,268][root][INFO] - Training Epoch: 1/2, step 6949/7134 completed (loss: 0.06044052168726921, acc: 0.9850427508354187)
[2025-02-13 03:24:59,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:59,596][root][INFO] - Training Epoch: 1/2, step 6950/7134 completed (loss: 0.027303006500005722, acc: 0.9974489808082581)
[2025-02-13 03:24:59,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:00,013][root][INFO] - Training Epoch: 1/2, step 6951/7134 completed (loss: 0.030801966786384583, acc: 0.991525411605835)
[2025-02-13 03:25:00,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:00,355][root][INFO] - Training Epoch: 1/2, step 6952/7134 completed (loss: 0.02816663309931755, acc: 0.9929906725883484)
[2025-02-13 03:25:00,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:00,752][root][INFO] - Training Epoch: 1/2, step 6953/7134 completed (loss: 0.01642315462231636, acc: 0.9934425950050354)
[2025-02-13 03:25:00,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:01,154][root][INFO] - Training Epoch: 1/2, step 6954/7134 completed (loss: 0.09322132915258408, acc: 0.9829424023628235)
[2025-02-13 03:25:01,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:01,519][root][INFO] - Training Epoch: 1/2, step 6955/7134 completed (loss: 0.07086435705423355, acc: 0.9817073345184326)
[2025-02-13 03:25:01,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:01,927][root][INFO] - Training Epoch: 1/2, step 6956/7134 completed (loss: 0.0555659644305706, acc: 0.9875665903091431)
[2025-02-13 03:25:02,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:02,326][root][INFO] - Training Epoch: 1/2, step 6957/7134 completed (loss: 0.024346405640244484, acc: 0.9941291809082031)
[2025-02-13 03:25:02,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:02,710][root][INFO] - Training Epoch: 1/2, step 6958/7134 completed (loss: 0.07169225811958313, acc: 0.9766082167625427)
[2025-02-13 03:25:02,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:03,101][root][INFO] - Training Epoch: 1/2, step 6959/7134 completed (loss: 0.05534903332591057, acc: 0.9822616577148438)
[2025-02-13 03:25:03,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:03,504][root][INFO] - Training Epoch: 1/2, step 6960/7134 completed (loss: 0.040215667337179184, acc: 0.9902439117431641)
[2025-02-13 03:25:03,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:03,901][root][INFO] - Training Epoch: 1/2, step 6961/7134 completed (loss: 0.04189314693212509, acc: 0.9898989796638489)
[2025-02-13 03:25:04,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:04,301][root][INFO] - Training Epoch: 1/2, step 6962/7134 completed (loss: 0.048091936856508255, acc: 0.9878261089324951)
[2025-02-13 03:25:04,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:04,707][root][INFO] - Training Epoch: 1/2, step 6963/7134 completed (loss: 0.024786287918686867, acc: 0.9932659864425659)
[2025-02-13 03:25:04,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:05,116][root][INFO] - Training Epoch: 1/2, step 6964/7134 completed (loss: 0.033171456307172775, acc: 0.9891696572303772)
[2025-02-13 03:25:05,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:05,525][root][INFO] - Training Epoch: 1/2, step 6965/7134 completed (loss: 0.02342708595097065, acc: 0.9945054650306702)
[2025-02-13 03:25:05,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:05,884][root][INFO] - Training Epoch: 1/2, step 6966/7134 completed (loss: 0.05497385561466217, acc: 0.9800498485565186)
[2025-02-13 03:25:06,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:06,288][root][INFO] - Training Epoch: 1/2, step 6967/7134 completed (loss: 0.025187896564602852, acc: 0.993966817855835)
[2025-02-13 03:25:06,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:06,685][root][INFO] - Training Epoch: 1/2, step 6968/7134 completed (loss: 0.03774368762969971, acc: 0.9896551966667175)
[2025-02-13 03:25:06,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:07,077][root][INFO] - Training Epoch: 1/2, step 6969/7134 completed (loss: 0.08493503928184509, acc: 0.9812889695167542)
[2025-02-13 03:25:07,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:07,477][root][INFO] - Training Epoch: 1/2, step 6970/7134 completed (loss: 0.08464133739471436, acc: 0.9789473414421082)
[2025-02-13 03:25:07,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:07,888][root][INFO] - Training Epoch: 1/2, step 6971/7134 completed (loss: 0.09514512866735458, acc: 0.9775474667549133)
[2025-02-13 03:25:08,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:08,299][root][INFO] - Training Epoch: 1/2, step 6972/7134 completed (loss: 0.020846758037805557, acc: 0.9910873174667358)
[2025-02-13 03:25:08,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:08,692][root][INFO] - Training Epoch: 1/2, step 6973/7134 completed (loss: 0.04445188120007515, acc: 0.9911894202232361)
[2025-02-13 03:25:08,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:09,089][root][INFO] - Training Epoch: 1/2, step 6974/7134 completed (loss: 0.04466203972697258, acc: 0.9896551966667175)
[2025-02-13 03:25:09,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:09,479][root][INFO] - Training Epoch: 1/2, step 6975/7134 completed (loss: 0.03711613640189171, acc: 0.9934924244880676)
[2025-02-13 03:25:09,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:09,878][root][INFO] - Training Epoch: 1/2, step 6976/7134 completed (loss: 0.051979582756757736, acc: 0.984402060508728)
[2025-02-13 03:25:10,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:10,274][root][INFO] - Training Epoch: 1/2, step 6977/7134 completed (loss: 0.03916141390800476, acc: 0.9887217879295349)
[2025-02-13 03:25:10,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:10,685][root][INFO] - Training Epoch: 1/2, step 6978/7134 completed (loss: 0.029806090518832207, acc: 0.9886178970336914)
[2025-02-13 03:25:10,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:11,122][root][INFO] - Training Epoch: 1/2, step 6979/7134 completed (loss: 0.08395162969827652, acc: 0.9664633870124817)
[2025-02-13 03:25:11,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:11,570][root][INFO] - Training Epoch: 1/2, step 6980/7134 completed (loss: 0.1203179582953453, acc: 0.9672955870628357)
[2025-02-13 03:25:11,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:12,030][root][INFO] - Training Epoch: 1/2, step 6981/7134 completed (loss: 0.11538383364677429, acc: 0.9680715203285217)
[2025-02-13 03:25:12,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:12,491][root][INFO] - Training Epoch: 1/2, step 6982/7134 completed (loss: 0.12646161019802094, acc: 0.9658952355384827)
[2025-02-13 03:25:12,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:12,938][root][INFO] - Training Epoch: 1/2, step 6983/7134 completed (loss: 0.1093255802989006, acc: 0.9777448177337646)
[2025-02-13 03:25:13,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:13,388][root][INFO] - Training Epoch: 1/2, step 6984/7134 completed (loss: 0.04897637665271759, acc: 0.986066460609436)
[2025-02-13 03:25:13,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:13,861][root][INFO] - Training Epoch: 1/2, step 6985/7134 completed (loss: 0.1416824907064438, acc: 0.9682051539421082)
[2025-02-13 03:25:13,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:14,290][root][INFO] - Training Epoch: 1/2, step 6986/7134 completed (loss: 0.05039633437991142, acc: 0.98959881067276)
[2025-02-13 03:25:14,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:14,763][root][INFO] - Training Epoch: 1/2, step 6987/7134 completed (loss: 0.07783760130405426, acc: 0.9767676591873169)
[2025-02-13 03:25:14,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:15,045][root][INFO] - Training Epoch: 1/2, step 6988/7134 completed (loss: 0.0568818524479866, acc: 0.9840954542160034)
[2025-02-13 03:25:15,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:15,399][root][INFO] - Training Epoch: 1/2, step 6989/7134 completed (loss: 0.10332031548023224, acc: 0.9672726988792419)
[2025-02-13 03:25:15,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:15,783][root][INFO] - Training Epoch: 1/2, step 6990/7134 completed (loss: 0.02915826067328453, acc: 0.9910714030265808)
[2025-02-13 03:25:15,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:16,089][root][INFO] - Training Epoch: 1/2, step 6991/7134 completed (loss: 0.089738629758358, acc: 0.9774436354637146)
[2025-02-13 03:25:16,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:16,445][root][INFO] - Training Epoch: 1/2, step 6992/7134 completed (loss: 0.10114405304193497, acc: 0.9768421053886414)
[2025-02-13 03:25:16,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:16,874][root][INFO] - Training Epoch: 1/2, step 6993/7134 completed (loss: 0.11810063570737839, acc: 0.9698340892791748)
[2025-02-13 03:25:17,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:17,344][root][INFO] - Training Epoch: 1/2, step 6994/7134 completed (loss: 0.14550116658210754, acc: 0.971107542514801)
[2025-02-13 03:25:17,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:17,775][root][INFO] - Training Epoch: 1/2, step 6995/7134 completed (loss: 0.04511309787631035, acc: 0.9908376932144165)
[2025-02-13 03:25:17,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:18,184][root][INFO] - Training Epoch: 1/2, step 6996/7134 completed (loss: 0.04301682859659195, acc: 0.9886363744735718)
[2025-02-13 03:25:18,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:18,590][root][INFO] - Training Epoch: 1/2, step 6997/7134 completed (loss: 0.05626215413212776, acc: 0.985981285572052)
[2025-02-13 03:25:18,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:18,908][root][INFO] - Training Epoch: 1/2, step 6998/7134 completed (loss: 0.11899139732122421, acc: 0.9656862616539001)
[2025-02-13 03:25:19,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:19,368][root][INFO] - Training Epoch: 1/2, step 6999/7134 completed (loss: 0.044225335121154785, acc: 0.98562091588974)
[2025-02-13 03:25:19,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:19,686][root][INFO] - Training Epoch: 1/2, step 7000/7134 completed (loss: 0.04172423854470253, acc: 0.9894067645072937)
[2025-02-13 03:25:19,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:20,121][root][INFO] - Training Epoch: 1/2, step 7001/7134 completed (loss: 0.09498748183250427, acc: 0.9789156913757324)
[2025-02-13 03:25:20,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:20,563][root][INFO] - Training Epoch: 1/2, step 7002/7134 completed (loss: 0.02015233412384987, acc: 0.9918864369392395)
[2025-02-13 03:25:20,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:21,005][root][INFO] - Training Epoch: 1/2, step 7003/7134 completed (loss: 0.03383147716522217, acc: 0.9868565201759338)
[2025-02-13 03:25:21,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:21,462][root][INFO] - Training Epoch: 1/2, step 7004/7134 completed (loss: 0.09669820219278336, acc: 0.9763779640197754)
[2025-02-13 03:25:21,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:21,920][root][INFO] - Training Epoch: 1/2, step 7005/7134 completed (loss: 0.07427658140659332, acc: 0.9811066389083862)
[2025-02-13 03:25:22,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:22,373][root][INFO] - Training Epoch: 1/2, step 7006/7134 completed (loss: 0.04249953478574753, acc: 0.9835841059684753)
[2025-02-13 03:25:22,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:22,814][root][INFO] - Training Epoch: 1/2, step 7007/7134 completed (loss: 0.05076479911804199, acc: 0.985029935836792)
[2025-02-13 03:25:22,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:23,236][root][INFO] - Training Epoch: 1/2, step 7008/7134 completed (loss: 0.03737280145287514, acc: 0.9913686513900757)
[2025-02-13 03:25:23,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:23,683][root][INFO] - Training Epoch: 1/2, step 7009/7134 completed (loss: 0.049071334302425385, acc: 0.9849537014961243)
[2025-02-13 03:25:23,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:24,092][root][INFO] - Training Epoch: 1/2, step 7010/7134 completed (loss: 0.019041290506720543, acc: 0.9953774809837341)
[2025-02-13 03:25:24,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:24,542][root][INFO] - Training Epoch: 1/2, step 7011/7134 completed (loss: 0.02558700554072857, acc: 0.9879807829856873)
[2025-02-13 03:25:24,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:24,932][root][INFO] - Training Epoch: 1/2, step 7012/7134 completed (loss: 0.10220424830913544, acc: 0.9795918464660645)
[2025-02-13 03:25:25,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:25,373][root][INFO] - Training Epoch: 1/2, step 7013/7134 completed (loss: 0.01499919407069683, acc: 0.9927954077720642)
[2025-02-13 03:25:25,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:25,782][root][INFO] - Training Epoch: 1/2, step 7014/7134 completed (loss: 0.034462325274944305, acc: 0.988727867603302)
[2025-02-13 03:25:25,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:26,160][root][INFO] - Training Epoch: 1/2, step 7015/7134 completed (loss: 0.042079824954271317, acc: 0.9767441749572754)
[2025-02-13 03:25:26,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:26,610][root][INFO] - Training Epoch: 1/2, step 7016/7134 completed (loss: 0.04881833493709564, acc: 0.9797688126564026)
[2025-02-13 03:25:26,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:26,961][root][INFO] - Training Epoch: 1/2, step 7017/7134 completed (loss: 0.06576398760080338, acc: 0.9854771494865417)
[2025-02-13 03:25:27,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:27,377][root][INFO] - Training Epoch: 1/2, step 7018/7134 completed (loss: 0.046695757657289505, acc: 0.987596869468689)
[2025-02-13 03:25:27,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:27,771][root][INFO] - Training Epoch: 1/2, step 7019/7134 completed (loss: 0.1044786274433136, acc: 0.9722222089767456)
[2025-02-13 03:25:27,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:28,178][root][INFO] - Training Epoch: 1/2, step 7020/7134 completed (loss: 0.025344090536236763, acc: 0.9915373921394348)
[2025-02-13 03:25:28,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:28,645][root][INFO] - Training Epoch: 1/2, step 7021/7134 completed (loss: 0.029210031032562256, acc: 0.9931856989860535)
[2025-02-13 03:25:28,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:28,999][root][INFO] - Training Epoch: 1/2, step 7022/7134 completed (loss: 0.1225133091211319, acc: 0.9629629850387573)
[2025-02-13 03:25:29,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:29,374][root][INFO] - Training Epoch: 1/2, step 7023/7134 completed (loss: 0.09057708084583282, acc: 0.9761336445808411)
[2025-02-13 03:25:29,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:29,702][root][INFO] - Training Epoch: 1/2, step 7024/7134 completed (loss: 0.05208953469991684, acc: 0.987261176109314)
[2025-02-13 03:25:29,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:30,075][root][INFO] - Training Epoch: 1/2, step 7025/7134 completed (loss: 0.17984654009342194, acc: 0.954023003578186)
[2025-02-13 03:25:30,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:30,412][root][INFO] - Training Epoch: 1/2, step 7026/7134 completed (loss: 0.1314241886138916, acc: 0.9498069286346436)
[2025-02-13 03:25:30,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:30,735][root][INFO] - Training Epoch: 1/2, step 7027/7134 completed (loss: 0.04689987003803253, acc: 0.9878048896789551)
[2025-02-13 03:25:30,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:31,096][root][INFO] - Training Epoch: 1/2, step 7028/7134 completed (loss: 0.02918163686990738, acc: 0.990314781665802)
[2025-02-13 03:25:31,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:31,445][root][INFO] - Training Epoch: 1/2, step 7029/7134 completed (loss: 0.04707261174917221, acc: 0.9814385175704956)
[2025-02-13 03:25:31,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:31,792][root][INFO] - Training Epoch: 1/2, step 7030/7134 completed (loss: 0.13758619129657745, acc: 0.9668246507644653)
[2025-02-13 03:25:31,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:32,167][root][INFO] - Training Epoch: 1/2, step 7031/7134 completed (loss: 0.10440172255039215, acc: 0.9644669890403748)
[2025-02-13 03:25:32,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:32,500][root][INFO] - Training Epoch: 1/2, step 7032/7134 completed (loss: 0.10366260260343552, acc: 0.9593495726585388)
[2025-02-13 03:25:32,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:32,819][root][INFO] - Training Epoch: 1/2, step 7033/7134 completed (loss: 0.07674578577280045, acc: 0.973607063293457)
[2025-02-13 03:25:32,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:33,168][root][INFO] - Training Epoch: 1/2, step 7034/7134 completed (loss: 0.11890354752540588, acc: 0.971731424331665)
[2025-02-13 03:25:33,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:33,504][root][INFO] - Training Epoch: 1/2, step 7035/7134 completed (loss: 0.1218995526432991, acc: 0.9603174328804016)
[2025-02-13 03:25:33,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:33,791][root][INFO] - Training Epoch: 1/2, step 7036/7134 completed (loss: 0.06812556087970734, acc: 0.9794238805770874)
[2025-02-13 03:25:33,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:34,073][root][INFO] - Training Epoch: 1/2, step 7037/7134 completed (loss: 0.041633300483226776, acc: 0.984375)
[2025-02-13 03:25:34,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:34,422][root][INFO] - Training Epoch: 1/2, step 7038/7134 completed (loss: 0.13686464726924896, acc: 0.960422158241272)
[2025-02-13 03:25:34,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:34,780][root][INFO] - Training Epoch: 1/2, step 7039/7134 completed (loss: 0.0700349435210228, acc: 0.9806763529777527)
[2025-02-13 03:25:34,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:35,116][root][INFO] - Training Epoch: 1/2, step 7040/7134 completed (loss: 0.17490865290164948, acc: 0.9367815852165222)
[2025-02-13 03:25:35,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:35,449][root][INFO] - Training Epoch: 1/2, step 7041/7134 completed (loss: 0.1364622414112091, acc: 0.9654088020324707)
[2025-02-13 03:25:35,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:35,793][root][INFO] - Training Epoch: 1/2, step 7042/7134 completed (loss: 0.11065638810396194, acc: 0.9569230675697327)
[2025-02-13 03:25:35,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:36,116][root][INFO] - Training Epoch: 1/2, step 7043/7134 completed (loss: 0.10097178816795349, acc: 0.9679999947547913)
[2025-02-13 03:25:36,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:36,446][root][INFO] - Training Epoch: 1/2, step 7044/7134 completed (loss: 0.1315459907054901, acc: 0.963302731513977)
[2025-02-13 03:25:36,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:36,842][root][INFO] - Training Epoch: 1/2, step 7045/7134 completed (loss: 0.045154083520174026, acc: 0.9897959232330322)
[2025-02-13 03:25:36,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:37,287][root][INFO] - Training Epoch: 1/2, step 7046/7134 completed (loss: 0.09231454133987427, acc: 0.9754098653793335)
[2025-02-13 03:25:37,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:37,718][root][INFO] - Training Epoch: 1/2, step 7047/7134 completed (loss: 0.08691863715648651, acc: 0.9785714149475098)
[2025-02-13 03:25:37,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:38,154][root][INFO] - Training Epoch: 1/2, step 7048/7134 completed (loss: 0.01129382848739624, acc: 0.9976498484611511)
[2025-02-13 03:25:38,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:38,558][root][INFO] - Training Epoch: 1/2, step 7049/7134 completed (loss: 0.036608874797821045, acc: 0.9893048405647278)
[2025-02-13 03:25:38,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:38,973][root][INFO] - Training Epoch: 1/2, step 7050/7134 completed (loss: 0.03546921908855438, acc: 0.9909326434135437)
[2025-02-13 03:25:39,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:39,414][root][INFO] - Training Epoch: 1/2, step 7051/7134 completed (loss: 0.057573117315769196, acc: 0.983565092086792)
[2025-02-13 03:25:39,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:39,864][root][INFO] - Training Epoch: 1/2, step 7052/7134 completed (loss: 0.048070330172777176, acc: 0.9880159497261047)
[2025-02-13 03:25:40,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:40,314][root][INFO] - Training Epoch: 1/2, step 7053/7134 completed (loss: 0.032124266028404236, acc: 0.9914737939834595)
[2025-02-13 03:25:40,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:40,752][root][INFO] - Training Epoch: 1/2, step 7054/7134 completed (loss: 0.08771105855703354, acc: 0.9778481125831604)
[2025-02-13 03:25:40,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:41,179][root][INFO] - Training Epoch: 1/2, step 7055/7134 completed (loss: 0.044775377959012985, acc: 0.9872449040412903)
[2025-02-13 03:25:41,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:41,651][root][INFO] - Training Epoch: 1/2, step 7056/7134 completed (loss: 0.06145242974162102, acc: 0.9843937754631042)
[2025-02-13 03:25:41,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:42,111][root][INFO] - Training Epoch: 1/2, step 7057/7134 completed (loss: 0.13159100711345673, acc: 0.9712328910827637)
[2025-02-13 03:25:42,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:42,535][root][INFO] - Training Epoch: 1/2, step 7058/7134 completed (loss: 0.06495559215545654, acc: 0.9803149700164795)
[2025-02-13 03:25:42,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:42,961][root][INFO] - Training Epoch: 1/2, step 7059/7134 completed (loss: 0.03523564711213112, acc: 0.9888268113136292)
[2025-02-13 03:25:43,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:43,361][root][INFO] - Training Epoch: 1/2, step 7060/7134 completed (loss: 0.08192411810159683, acc: 0.9790322780609131)
[2025-02-13 03:25:43,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:43,787][root][INFO] - Training Epoch: 1/2, step 7061/7134 completed (loss: 0.05105955898761749, acc: 0.98828125)
[2025-02-13 03:25:43,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:44,193][root][INFO] - Training Epoch: 1/2, step 7062/7134 completed (loss: 0.15164488554000854, acc: 0.9666666388511658)
[2025-02-13 03:25:44,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:44,622][root][INFO] - Training Epoch: 1/2, step 7063/7134 completed (loss: 0.07574871927499771, acc: 0.9803921580314636)
[2025-02-13 03:25:44,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:45,083][root][INFO] - Training Epoch: 1/2, step 7064/7134 completed (loss: 0.0308048315346241, acc: 0.9940652847290039)
[2025-02-13 03:25:45,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:45,530][root][INFO] - Training Epoch: 1/2, step 7065/7134 completed (loss: 0.02672816999256611, acc: 0.9918887615203857)
[2025-02-13 03:25:45,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:45,982][root][INFO] - Training Epoch: 1/2, step 7066/7134 completed (loss: 0.03429984673857689, acc: 0.9928656220436096)
[2025-02-13 03:25:46,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:46,413][root][INFO] - Training Epoch: 1/2, step 7067/7134 completed (loss: 0.034330323338508606, acc: 0.9885350465774536)
[2025-02-13 03:25:46,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:46,868][root][INFO] - Training Epoch: 1/2, step 7068/7134 completed (loss: 0.028974398970603943, acc: 0.9891435503959656)
[2025-02-13 03:25:47,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:47,344][root][INFO] - Training Epoch: 1/2, step 7069/7134 completed (loss: 0.02535691112279892, acc: 0.993096649646759)
[2025-02-13 03:25:47,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:47,775][root][INFO] - Training Epoch: 1/2, step 7070/7134 completed (loss: 0.029899071902036667, acc: 0.991769552230835)
[2025-02-13 03:25:47,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:48,193][root][INFO] - Training Epoch: 1/2, step 7071/7134 completed (loss: 0.0692097395658493, acc: 0.9868420958518982)
[2025-02-13 03:25:48,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:48,626][root][INFO] - Training Epoch: 1/2, step 7072/7134 completed (loss: 0.04225030541419983, acc: 0.9878787994384766)
[2025-02-13 03:25:48,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:49,097][root][INFO] - Training Epoch: 1/2, step 7073/7134 completed (loss: 0.040146034210920334, acc: 0.9863481521606445)
[2025-02-13 03:25:49,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:49,483][root][INFO] - Training Epoch: 1/2, step 7074/7134 completed (loss: 0.03772987052798271, acc: 0.9923518300056458)
[2025-02-13 03:25:49,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:49,907][root][INFO] - Training Epoch: 1/2, step 7075/7134 completed (loss: 0.03666295111179352, acc: 0.988041877746582)
[2025-02-13 03:25:50,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:50,303][root][INFO] - Training Epoch: 1/2, step 7076/7134 completed (loss: 0.06083162873983383, acc: 0.9857549667358398)
[2025-02-13 03:25:50,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:50,688][root][INFO] - Training Epoch: 1/2, step 7077/7134 completed (loss: 0.06417039781808853, acc: 0.9819672107696533)
[2025-02-13 03:25:50,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:51,085][root][INFO] - Training Epoch: 1/2, step 7078/7134 completed (loss: 0.05029568076133728, acc: 0.9791666865348816)
[2025-02-13 03:25:51,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:51,475][root][INFO] - Training Epoch: 1/2, step 7079/7134 completed (loss: 0.04181531071662903, acc: 0.9819375872612)
[2025-02-13 03:25:51,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:51,862][root][INFO] - Training Epoch: 1/2, step 7080/7134 completed (loss: 0.03708332031965256, acc: 0.9867109656333923)
[2025-02-13 03:25:52,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:52,289][root][INFO] - Training Epoch: 1/2, step 7081/7134 completed (loss: 0.05150168389081955, acc: 0.9831697344779968)
[2025-02-13 03:25:52,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:52,699][root][INFO] - Training Epoch: 1/2, step 7082/7134 completed (loss: 0.05014876648783684, acc: 0.987075924873352)
[2025-02-13 03:25:52,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:53,105][root][INFO] - Training Epoch: 1/2, step 7083/7134 completed (loss: 0.022711722180247307, acc: 0.9949832558631897)
[2025-02-13 03:25:53,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:53,525][root][INFO] - Training Epoch: 1/2, step 7084/7134 completed (loss: 0.04754791036248207, acc: 0.9840348362922668)
[2025-02-13 03:25:53,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:53,922][root][INFO] - Training Epoch: 1/2, step 7085/7134 completed (loss: 0.04814852774143219, acc: 0.9861111044883728)
[2025-02-13 03:25:54,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:54,351][root][INFO] - Training Epoch: 1/2, step 7086/7134 completed (loss: 0.036400794982910156, acc: 0.9863201379776001)
[2025-02-13 03:25:54,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:54,749][root][INFO] - Training Epoch: 1/2, step 7087/7134 completed (loss: 0.03740634396672249, acc: 0.9926062822341919)
[2025-02-13 03:25:54,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:55,158][root][INFO] - Training Epoch: 1/2, step 7088/7134 completed (loss: 0.05935453623533249, acc: 0.9850993156433105)
[2025-02-13 03:25:55,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:55,561][root][INFO] - Training Epoch: 1/2, step 7089/7134 completed (loss: 0.11454108357429504, acc: 0.977707028388977)
[2025-02-13 03:25:55,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:55,986][root][INFO] - Training Epoch: 1/2, step 7090/7134 completed (loss: 0.0414377897977829, acc: 0.989159882068634)
[2025-02-13 03:25:56,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:56,391][root][INFO] - Training Epoch: 1/2, step 7091/7134 completed (loss: 0.00857964064925909, acc: 0.9986072182655334)
[2025-02-13 03:25:56,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:56,812][root][INFO] - Training Epoch: 1/2, step 7092/7134 completed (loss: 0.0345517136156559, acc: 0.9879931211471558)
[2025-02-13 03:25:56,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:57,174][root][INFO] - Training Epoch: 1/2, step 7093/7134 completed (loss: 0.054462987929582596, acc: 0.988095223903656)
[2025-02-13 03:25:57,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:57,565][root][INFO] - Training Epoch: 1/2, step 7094/7134 completed (loss: 0.027306845411658287, acc: 0.9930875301361084)
[2025-02-13 03:25:57,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:57,968][root][INFO] - Training Epoch: 1/2, step 7095/7134 completed (loss: 0.04234476387500763, acc: 0.9896694421768188)
[2025-02-13 03:25:58,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:58,360][root][INFO] - Training Epoch: 1/2, step 7096/7134 completed (loss: 0.06382394582033157, acc: 0.9815126061439514)
[2025-02-13 03:25:58,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:58,775][root][INFO] - Training Epoch: 1/2, step 7097/7134 completed (loss: 0.06874129176139832, acc: 0.9841498732566833)
[2025-02-13 03:25:58,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:59,178][root][INFO] - Training Epoch: 1/2, step 7098/7134 completed (loss: 0.04280555248260498, acc: 0.9871588945388794)
[2025-02-13 03:25:59,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:59,535][root][INFO] - Training Epoch: 1/2, step 7099/7134 completed (loss: 0.04396839812397957, acc: 0.9829059839248657)
[2025-02-13 03:25:59,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:59,944][root][INFO] - Training Epoch: 1/2, step 7100/7134 completed (loss: 0.07543382793664932, acc: 0.9751937985420227)
[2025-02-13 03:26:00,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:00,291][root][INFO] - Training Epoch: 1/2, step 7101/7134 completed (loss: 0.09514867514371872, acc: 0.9821109175682068)
[2025-02-13 03:26:00,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:00,716][root][INFO] - Training Epoch: 1/2, step 7102/7134 completed (loss: 0.015459121204912663, acc: 0.998062014579773)
[2025-02-13 03:26:00,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:01,006][root][INFO] - Training Epoch: 1/2, step 7103/7134 completed (loss: 0.04035104438662529, acc: 0.9879032373428345)
[2025-02-13 03:26:01,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:01,401][root][INFO] - Training Epoch: 1/2, step 7104/7134 completed (loss: 0.028575098142027855, acc: 0.9958847761154175)
[2025-02-13 03:26:01,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:01,804][root][INFO] - Training Epoch: 1/2, step 7105/7134 completed (loss: 0.0424564965069294, acc: 0.9900000095367432)
[2025-02-13 03:26:01,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:02,180][root][INFO] - Training Epoch: 1/2, step 7106/7134 completed (loss: 0.029069703072309494, acc: 0.989983320236206)
[2025-02-13 03:26:02,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:02,586][root][INFO] - Training Epoch: 1/2, step 7107/7134 completed (loss: 0.03627563267946243, acc: 0.989130437374115)
[2025-02-13 03:26:02,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:02,985][root][INFO] - Training Epoch: 1/2, step 7108/7134 completed (loss: 0.0396156869828701, acc: 0.9900000095367432)
[2025-02-13 03:26:03,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:03,353][root][INFO] - Training Epoch: 1/2, step 7109/7134 completed (loss: 0.07336898893117905, acc: 0.9699812531471252)
[2025-02-13 03:26:03,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:03,758][root][INFO] - Training Epoch: 1/2, step 7110/7134 completed (loss: 0.04109783098101616, acc: 0.9830508232116699)
[2025-02-13 03:26:03,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:04,097][root][INFO] - Training Epoch: 1/2, step 7111/7134 completed (loss: 0.07932276278734207, acc: 0.9733924865722656)
[2025-02-13 03:26:04,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:04,501][root][INFO] - Training Epoch: 1/2, step 7112/7134 completed (loss: 0.049626048654317856, acc: 0.9876543283462524)
[2025-02-13 03:26:04,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:04,931][root][INFO] - Training Epoch: 1/2, step 7113/7134 completed (loss: 0.09991098940372467, acc: 0.965299665927887)
[2025-02-13 03:26:05,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:05,324][root][INFO] - Training Epoch: 1/2, step 7114/7134 completed (loss: 0.04189632087945938, acc: 0.9885277152061462)
[2025-02-13 03:26:05,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:05,714][root][INFO] - Training Epoch: 1/2, step 7115/7134 completed (loss: 0.034489769488573074, acc: 0.9894179701805115)
[2025-02-13 03:26:05,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:06,111][root][INFO] - Training Epoch: 1/2, step 7116/7134 completed (loss: 0.030152607709169388, acc: 0.9894551634788513)
[2025-02-13 03:26:06,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:06,499][root][INFO] - Training Epoch: 1/2, step 7117/7134 completed (loss: 0.0184169989079237, acc: 0.9950000047683716)
[2025-02-13 03:26:06,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:06,913][root][INFO] - Training Epoch: 1/2, step 7118/7134 completed (loss: 0.054606009274721146, acc: 0.9883720874786377)
[2025-02-13 03:26:07,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:07,306][root][INFO] - Training Epoch: 1/2, step 7119/7134 completed (loss: 0.02778710424900055, acc: 0.9918166995048523)
[2025-02-13 03:26:07,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:07,665][root][INFO] - Training Epoch: 1/2, step 7120/7134 completed (loss: 0.052865345031023026, acc: 0.9836448431015015)
[2025-02-13 03:26:07,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:08,050][root][INFO] - Training Epoch: 1/2, step 7121/7134 completed (loss: 0.029206719249486923, acc: 0.9894366264343262)
[2025-02-13 03:26:08,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:08,398][root][INFO] - Training Epoch: 1/2, step 7122/7134 completed (loss: 0.044041190296411514, acc: 0.9839572310447693)
[2025-02-13 03:26:08,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:08,717][root][INFO] - Training Epoch: 1/2, step 7123/7134 completed (loss: 0.03073006309568882, acc: 0.9853658676147461)
[2025-02-13 03:26:08,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:09,114][root][INFO] - Training Epoch: 1/2, step 7124/7134 completed (loss: 0.03370477631688118, acc: 0.9906191229820251)
[2025-02-13 03:26:09,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:09,466][root][INFO] - Training Epoch: 1/2, step 7125/7134 completed (loss: 0.02816867083311081, acc: 0.9867330193519592)
[2025-02-13 03:26:09,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:09,866][root][INFO] - Training Epoch: 1/2, step 7126/7134 completed (loss: 0.09244280308485031, acc: 0.9713321924209595)
[2025-02-13 03:26:09,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:10,245][root][INFO] - Training Epoch: 1/2, step 7127/7134 completed (loss: 0.04220599681138992, acc: 0.9840764403343201)
[2025-02-13 03:26:10,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:10,637][root][INFO] - Training Epoch: 1/2, step 7128/7134 completed (loss: 0.04504822939634323, acc: 0.9869918823242188)
[2025-02-13 03:26:10,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:11,036][root][INFO] - Training Epoch: 1/2, step 7129/7134 completed (loss: 0.08041968941688538, acc: 0.9753915071487427)
[2025-02-13 03:26:11,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:11,449][root][INFO] - Training Epoch: 1/2, step 7130/7134 completed (loss: 0.11743500828742981, acc: 0.9624999761581421)
[2025-02-13 03:26:11,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:11,842][root][INFO] - Training Epoch: 1/2, step 7131/7134 completed (loss: 0.021331047639250755, acc: 0.9907192587852478)
[2025-02-13 03:26:12,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:12,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:13,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:13,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:13,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:14,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:14,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:14,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:15,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:15,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:16,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:16,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:16,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:17,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:17,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:17,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:18,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:18,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:19,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:19,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:19,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:20,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:20,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:20,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:21,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:21,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:21,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:22,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:22,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:23,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:23,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:23,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:24,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:24,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:25,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:25,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:25,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:26,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:26,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:26,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:27,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:27,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:27,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:28,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:28,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:29,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:29,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:29,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:30,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:30,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:31,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:31,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:31,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:32,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:32,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:32,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:33,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:33,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:33,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:34,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:34,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:34,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:35,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:35,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:36,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:36,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:36,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:37,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:37,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:37,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:38,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:38,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:38,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:39,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:39,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:40,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:40,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:40,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:41,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:41,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:41,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:42,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:42,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:42,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:43,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:43,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:44,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:44,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:44,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:45,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:45,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:45,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:46,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:46,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:46,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:47,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:47,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:48,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:48,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:48,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:49,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:49,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:49,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:50,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:50,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:50,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:51,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:51,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:51,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:52,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:52,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:53,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:53,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:53,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:54,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:54,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:54,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:55,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:55,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:55,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:56,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:56,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:56,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:57,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:57,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:57,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:58,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:58,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:59,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:59,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:59,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:00,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:00,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:00,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:01,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:01,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:02,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:02,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:02,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:03,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:03,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:03,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:04,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:04,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:04,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:05,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:05,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:06,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:06,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:06,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:07,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:07,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:07,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:08,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:08,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:08,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:09,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:09,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:09,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:10,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:10,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:10,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:11,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:11,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:12,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:12,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:12,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:13,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:13,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:14,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:14,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:14,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:15,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:15,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:15,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:16,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:16,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:16,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:17,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:17,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:17,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:18,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:18,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:19,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:19,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:19,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:20,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:20,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:20,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:21,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:21,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:21,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:22,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:22,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:22,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:23,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:23,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:23,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:24,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:24,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:24,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:25,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:25,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:25,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:26,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:26,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:27,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:27,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:27,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:28,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:28,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:28,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:29,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:29,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:29,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:30,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:30,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:30,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:31,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:31,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:31,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:32,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:32,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:32,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:33,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:33,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:33,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:34,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:34,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:34,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:35,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:35,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:36,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:36,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:36,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:37,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:37,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:37,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:38,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:38,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:38,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:39,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:39,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:39,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:40,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:40,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:41,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:41,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:41,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:42,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:42,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:43,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:43,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:43,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:44,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:44,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:44,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:45,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:45,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:45,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:46,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:46,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:46,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:47,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:47,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:48,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:48,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:48,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:49,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:49,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:49,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:50,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:50,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:51,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:51,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:52,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:52,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:52,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:53,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:53,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:53,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:54,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:55,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:55,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:55,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:56,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:56,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:57,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:57,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:57,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:58,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:58,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:58,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:59,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:59,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:00,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:00,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:01,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:01,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:01,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:02,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:02,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:02,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:03,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:03,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:03,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:04,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:04,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:04,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:05,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:05,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:05,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:06,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:06,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:06,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:07,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:07,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:07,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:08,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:08,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:09,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:09,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:09,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:10,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:10,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:11,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:11,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:11,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:12,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:12,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:12,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:13,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:13,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:13,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:14,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:14,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:14,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:15,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:15,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:16,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:16,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:17,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:17,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:17,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:18,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:18,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:19,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:19,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:19,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:19,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:20,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:20,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:21,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:21,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:21,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:22,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:22,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:22,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:23,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:23,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:24,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:24,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:25,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:25,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:26,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:26,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:27,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:27,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:27,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:28,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:28,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:28,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:29,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:29,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:29,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:30,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:30,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:30,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:31,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:31,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:31,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:32,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:32,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:32,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:33,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:33,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:33,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:34,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:34,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:34,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:35,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:35,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:35,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:36,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:36,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:36,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:37,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:37,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:37,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:38,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:38,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:38,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:39,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:39,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:39,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:40,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:40,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:40,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:41,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:41,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:41,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:42,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:42,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:42,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:43,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:43,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:43,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:44,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:44,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:44,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:45,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:45,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:45,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:46,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:46,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:47,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:47,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:47,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:48,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:48,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:48,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:49,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:49,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:49,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:50,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:50,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:50,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:51,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:51,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:51,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:52,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:52,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:52,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:53,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:53,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:54,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:54,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:54,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:55,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:55,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:55,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:55,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:56,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:56,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:56,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:57,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:57,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:57,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:58,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:58,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:58,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:59,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:59,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:59,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:00,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:00,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:00,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:01,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:01,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:02,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:02,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:02,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:03,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:03,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:03,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:04,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:04,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:04,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:05,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:05,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:05,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:06,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:06,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:06,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:07,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:07,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:08,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:08,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:08,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:09,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:09,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:09,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:10,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:10,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:10,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:11,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:11,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:12,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:12,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:12,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:13,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:13,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:14,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:14,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:14,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:15,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:15,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:15,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:16,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:16,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:16,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:17,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:17,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:17,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:18,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:18,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:18,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:19,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:19,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:19,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:20,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:20,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:20,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:21,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:21,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:22,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:22,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:22,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:23,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:23,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:23,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:24,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:24,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:24,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:25,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:25,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:25,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:26,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:26,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:26,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:27,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:27,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:27,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:28,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:28,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:28,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:29,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:29,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:29,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:30,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:30,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:31,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:31,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:31,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:32,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:32,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:32,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:33,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:33,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:33,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:34,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:34,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:34,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:35,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:35,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:36,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:36,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:36,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:37,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:37,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:37,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:38,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:38,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:38,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:39,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:39,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:40,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:40,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:40,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:41,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:41,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:42,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:42,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:42,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:43,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:43,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:43,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:44,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:44,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:44,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:45,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:45,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:45,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:46,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:46,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:47,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:47,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:47,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:48,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:48,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:48,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:49,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:49,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:49,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:50,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:50,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:50,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:51,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:51,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:51,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:52,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:52,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:52,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:53,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:53,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:53,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:54,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:54,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:54,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:55,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:55,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:55,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:56,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:56,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:57,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:57,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:58,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:59,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:59,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:59,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:00,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:00,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:01,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:01,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:02,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:02,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:02,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:03,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:03,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:03,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:04,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:04,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:04,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:05,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:05,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:06,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:06,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:06,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:07,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:07,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:08,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:08,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:08,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:09,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:09,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:09,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:10,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:10,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:10,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:11,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:11,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:11,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:12,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:12,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:12,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:13,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:13,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:13,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:14,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:14,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:15,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:15,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:15,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:16,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:16,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:17,348][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0590, device='cuda:0') eval_epoch_loss=tensor(0.0573, device='cuda:0') eval_epoch_acc=tensor(0.9842, device='cuda:0')
[2025-02-13 03:30:17,350][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:30:17,350][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:30:17,607][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_7132_loss_0.057319674640893936/model.pt
[2025-02-13 03:30:17,614][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:30:17,614][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.057319674640893936
[2025-02-13 03:30:17,615][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9841892719268799
[2025-02-13 03:30:17,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:17,959][root][INFO] - Training Epoch: 1/2, step 7132/7134 completed (loss: 0.03515179455280304, acc: 0.9898989796638489)
[2025-02-13 03:30:18,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:18,323][root][INFO] - Training Epoch: 1/2, step 7133/7134 completed (loss: 0.021539965644478798, acc: 0.9907578825950623)
[2025-02-13 03:30:18,811][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=1.1209, train_epoch_loss=0.1141, epoch time 3969.342839989811s
[2025-02-13 03:30:18,811][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2025-02-13 03:30:18,811][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 30 GB
[2025-02-13 03:30:18,812][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2025-02-13 03:30:18,812][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2025-02-13 03:30:18,812][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-13 03:30:19,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:19,729][root][INFO] - Training Epoch: 2/2, step 0/7134 completed (loss: 0.045406240969896317, acc: 0.9881129264831543)
[2025-02-13 03:30:19,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:20,164][root][INFO] - Training Epoch: 2/2, step 1/7134 completed (loss: 0.03915921598672867, acc: 0.9888424277305603)
[2025-02-13 03:30:20,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:20,583][root][INFO] - Training Epoch: 2/2, step 2/7134 completed (loss: 0.02636781893670559, acc: 0.993306577205658)
[2025-02-13 03:30:20,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:21,013][root][INFO] - Training Epoch: 2/2, step 3/7134 completed (loss: 0.0075323437340557575, acc: 1.0)
[2025-02-13 03:30:21,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:21,448][root][INFO] - Training Epoch: 2/2, step 4/7134 completed (loss: 0.0397120825946331, acc: 0.9836289286613464)
[2025-02-13 03:30:21,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:21,875][root][INFO] - Training Epoch: 2/2, step 5/7134 completed (loss: 0.012827659957110882, acc: 0.9949238300323486)
[2025-02-13 03:30:22,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:22,301][root][INFO] - Training Epoch: 2/2, step 6/7134 completed (loss: 0.010694368742406368, acc: 0.9983948469161987)
[2025-02-13 03:30:22,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:22,734][root][INFO] - Training Epoch: 2/2, step 7/7134 completed (loss: 0.06005934253334999, acc: 0.9838107228279114)
[2025-02-13 03:30:22,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:23,193][root][INFO] - Training Epoch: 2/2, step 8/7134 completed (loss: 0.060551974922418594, acc: 0.9793672561645508)
[2025-02-13 03:30:23,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:23,596][root][INFO] - Training Epoch: 2/2, step 9/7134 completed (loss: 0.013843339867889881, acc: 0.9968992471694946)
[2025-02-13 03:30:23,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:24,095][root][INFO] - Training Epoch: 2/2, step 10/7134 completed (loss: 0.03660716861486435, acc: 0.9886202216148376)
[2025-02-13 03:30:24,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:24,527][root][INFO] - Training Epoch: 2/2, step 11/7134 completed (loss: 0.010963240638375282, acc: 0.998123824596405)
[2025-02-13 03:30:24,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:24,984][root][INFO] - Training Epoch: 2/2, step 12/7134 completed (loss: 0.011802202090620995, acc: 0.9959623217582703)
[2025-02-13 03:30:25,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:25,402][root][INFO] - Training Epoch: 2/2, step 13/7134 completed (loss: 0.03324964642524719, acc: 0.9903714060783386)
[2025-02-13 03:30:25,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:25,812][root][INFO] - Training Epoch: 2/2, step 14/7134 completed (loss: 0.021706953644752502, acc: 0.9949579834938049)
[2025-02-13 03:30:25,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:26,216][root][INFO] - Training Epoch: 2/2, step 15/7134 completed (loss: 0.027570901438593864, acc: 0.9948365092277527)
[2025-02-13 03:30:26,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:26,632][root][INFO] - Training Epoch: 2/2, step 16/7134 completed (loss: 0.004779847338795662, acc: 0.9979674816131592)
[2025-02-13 03:30:26,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:27,006][root][INFO] - Training Epoch: 2/2, step 17/7134 completed (loss: 0.08610471338033676, acc: 0.9783783555030823)
[2025-02-13 03:30:27,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:27,428][root][INFO] - Training Epoch: 2/2, step 18/7134 completed (loss: 0.00880026537925005, acc: 0.9985795617103577)
[2025-02-13 03:30:27,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:27,849][root][INFO] - Training Epoch: 2/2, step 19/7134 completed (loss: 0.03432606905698776, acc: 0.9903448224067688)
[2025-02-13 03:30:27,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:28,259][root][INFO] - Training Epoch: 2/2, step 20/7134 completed (loss: 0.016632677987217903, acc: 0.9934210777282715)
[2025-02-13 03:30:28,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:28,713][root][INFO] - Training Epoch: 2/2, step 21/7134 completed (loss: 0.010164964944124222, acc: 0.9974226951599121)
[2025-02-13 03:30:28,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:29,155][root][INFO] - Training Epoch: 2/2, step 22/7134 completed (loss: 0.0032361072953790426, acc: 0.9985590577125549)
[2025-02-13 03:30:29,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:29,577][root][INFO] - Training Epoch: 2/2, step 23/7134 completed (loss: 0.013985848054289818, acc: 0.9959239363670349)
[2025-02-13 03:30:29,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:29,997][root][INFO] - Training Epoch: 2/2, step 24/7134 completed (loss: 0.012730710208415985, acc: 0.9944289922714233)
[2025-02-13 03:30:30,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:30,455][root][INFO] - Training Epoch: 2/2, step 25/7134 completed (loss: 0.026217812672257423, acc: 0.9900568127632141)
[2025-02-13 03:30:30,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:30,886][root][INFO] - Training Epoch: 2/2, step 26/7134 completed (loss: 0.0310908742249012, acc: 0.991150438785553)
[2025-02-13 03:30:31,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:31,305][root][INFO] - Training Epoch: 2/2, step 27/7134 completed (loss: 0.011237021535634995, acc: 0.9985228776931763)
[2025-02-13 03:30:31,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:31,764][root][INFO] - Training Epoch: 2/2, step 28/7134 completed (loss: 0.06181444227695465, acc: 0.9855421781539917)
[2025-02-13 03:30:31,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:32,221][root][INFO] - Training Epoch: 2/2, step 29/7134 completed (loss: 0.05302714556455612, acc: 0.9872521162033081)
[2025-02-13 03:30:32,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:32,684][root][INFO] - Training Epoch: 2/2, step 30/7134 completed (loss: 0.02060745283961296, acc: 0.993220329284668)
[2025-02-13 03:30:32,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:33,137][root][INFO] - Training Epoch: 2/2, step 31/7134 completed (loss: 0.04150086268782616, acc: 0.9894067645072937)
[2025-02-13 03:30:33,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:33,595][root][INFO] - Training Epoch: 2/2, step 32/7134 completed (loss: 0.046375054866075516, acc: 0.9843953251838684)
[2025-02-13 03:30:33,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:33,965][root][INFO] - Training Epoch: 2/2, step 33/7134 completed (loss: 0.12499725073575974, acc: 0.9780876636505127)
[2025-02-13 03:30:34,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:34,414][root][INFO] - Training Epoch: 2/2, step 34/7134 completed (loss: 0.054045628756284714, acc: 0.9847596883773804)
[2025-02-13 03:30:34,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:34,920][root][INFO] - Training Epoch: 2/2, step 35/7134 completed (loss: 0.06184206157922745, acc: 0.9858623743057251)
[2025-02-13 03:30:35,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:35,401][root][INFO] - Training Epoch: 2/2, step 36/7134 completed (loss: 0.04960031807422638, acc: 0.9956569075584412)
[2025-02-13 03:30:35,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:35,859][root][INFO] - Training Epoch: 2/2, step 37/7134 completed (loss: 0.027438241988420486, acc: 0.9887359142303467)
[2025-02-13 03:30:35,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:36,257][root][INFO] - Training Epoch: 2/2, step 38/7134 completed (loss: 0.031120700761675835, acc: 0.992559552192688)
[2025-02-13 03:30:36,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:36,715][root][INFO] - Training Epoch: 2/2, step 39/7134 completed (loss: 0.009719743393361568, acc: 0.9959404468536377)
[2025-02-13 03:30:36,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:37,109][root][INFO] - Training Epoch: 2/2, step 40/7134 completed (loss: 0.01725410483777523, acc: 0.9932735562324524)
[2025-02-13 03:30:37,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:37,504][root][INFO] - Training Epoch: 2/2, step 41/7134 completed (loss: 0.031039826571941376, acc: 0.9902912378311157)
[2025-02-13 03:30:37,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:37,929][root][INFO] - Training Epoch: 2/2, step 42/7134 completed (loss: 0.05332082882523537, acc: 0.9873772859573364)
[2025-02-13 03:30:38,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:38,328][root][INFO] - Training Epoch: 2/2, step 43/7134 completed (loss: 0.013415739871561527, acc: 0.9935483932495117)
[2025-02-13 03:30:38,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:38,750][root][INFO] - Training Epoch: 2/2, step 44/7134 completed (loss: 0.04476235806941986, acc: 0.9910827875137329)
[2025-02-13 03:30:38,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:39,225][root][INFO] - Training Epoch: 2/2, step 45/7134 completed (loss: 0.04215957969427109, acc: 0.9934994578361511)
[2025-02-13 03:30:39,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:39,648][root][INFO] - Training Epoch: 2/2, step 46/7134 completed (loss: 0.04550428315997124, acc: 0.9906666874885559)
[2025-02-13 03:30:39,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:40,114][root][INFO] - Training Epoch: 2/2, step 47/7134 completed (loss: 0.046524811536073685, acc: 0.9855700135231018)
[2025-02-13 03:30:40,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:40,599][root][INFO] - Training Epoch: 2/2, step 48/7134 completed (loss: 0.0252989474684, acc: 0.9921348094940186)
[2025-02-13 03:30:40,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:41,055][root][INFO] - Training Epoch: 2/2, step 49/7134 completed (loss: 0.01794058084487915, acc: 0.9972565174102783)
[2025-02-13 03:30:41,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:41,545][root][INFO] - Training Epoch: 2/2, step 50/7134 completed (loss: 0.014388250187039375, acc: 0.9965397715568542)
[2025-02-13 03:30:41,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:41,965][root][INFO] - Training Epoch: 2/2, step 51/7134 completed (loss: 0.015058689750730991, acc: 0.9961928725242615)
[2025-02-13 03:30:42,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:42,361][root][INFO] - Training Epoch: 2/2, step 52/7134 completed (loss: 0.014993681572377682, acc: 0.995121955871582)
[2025-02-13 03:30:42,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:42,761][root][INFO] - Training Epoch: 2/2, step 53/7134 completed (loss: 0.011222289875149727, acc: 0.9960629940032959)
[2025-02-13 03:30:42,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:43,219][root][INFO] - Training Epoch: 2/2, step 54/7134 completed (loss: 0.007867462933063507, acc: 0.9976984858512878)
[2025-02-13 03:30:43,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:43,642][root][INFO] - Training Epoch: 2/2, step 55/7134 completed (loss: 0.020137380808591843, acc: 0.9949238300323486)
[2025-02-13 03:30:43,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:44,089][root][INFO] - Training Epoch: 2/2, step 56/7134 completed (loss: 0.0150730200111866, acc: 0.9928571581840515)
[2025-02-13 03:30:44,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:44,535][root][INFO] - Training Epoch: 2/2, step 57/7134 completed (loss: 0.05916672199964523, acc: 0.9863353967666626)
[2025-02-13 03:30:44,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:44,995][root][INFO] - Training Epoch: 2/2, step 58/7134 completed (loss: 0.060001764446496964, acc: 0.984635055065155)
[2025-02-13 03:30:45,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:45,459][root][INFO] - Training Epoch: 2/2, step 59/7134 completed (loss: 0.021308831870555878, acc: 0.9940898418426514)
[2025-02-13 03:30:45,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:45,929][root][INFO] - Training Epoch: 2/2, step 60/7134 completed (loss: 0.03686276450753212, acc: 0.9900568127632141)
[2025-02-13 03:30:46,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:46,368][root][INFO] - Training Epoch: 2/2, step 61/7134 completed (loss: 0.03636525943875313, acc: 0.9889841079711914)
[2025-02-13 03:30:46,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:46,803][root][INFO] - Training Epoch: 2/2, step 62/7134 completed (loss: 0.027943959459662437, acc: 0.9898348450660706)
[2025-02-13 03:30:46,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:47,250][root][INFO] - Training Epoch: 2/2, step 63/7134 completed (loss: 0.06235090270638466, acc: 0.9863013625144958)
[2025-02-13 03:30:47,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:47,713][root][INFO] - Training Epoch: 2/2, step 64/7134 completed (loss: 0.019643748179078102, acc: 0.9905914068222046)
[2025-02-13 03:30:47,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:48,177][root][INFO] - Training Epoch: 2/2, step 65/7134 completed (loss: 0.05616579204797745, acc: 0.9815725088119507)
[2025-02-13 03:30:48,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:48,609][root][INFO] - Training Epoch: 2/2, step 66/7134 completed (loss: 0.06766272336244583, acc: 0.9820788502693176)
[2025-02-13 03:30:48,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:49,071][root][INFO] - Training Epoch: 2/2, step 67/7134 completed (loss: 0.05247388407588005, acc: 0.9803921580314636)
[2025-02-13 03:30:49,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:49,500][root][INFO] - Training Epoch: 2/2, step 68/7134 completed (loss: 0.053054485470056534, acc: 0.9793548583984375)
[2025-02-13 03:30:49,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:49,955][root][INFO] - Training Epoch: 2/2, step 69/7134 completed (loss: 0.07003898918628693, acc: 0.9788029789924622)
[2025-02-13 03:30:50,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:50,396][root][INFO] - Training Epoch: 2/2, step 70/7134 completed (loss: 0.0620480440557003, acc: 0.9824561476707458)
[2025-02-13 03:30:50,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:50,820][root][INFO] - Training Epoch: 2/2, step 71/7134 completed (loss: 0.0292219165712595, acc: 0.9915966391563416)
[2025-02-13 03:30:50,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:51,273][root][INFO] - Training Epoch: 2/2, step 72/7134 completed (loss: 0.03989759087562561, acc: 0.9872123003005981)
[2025-02-13 03:30:51,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:51,689][root][INFO] - Training Epoch: 2/2, step 73/7134 completed (loss: 0.0340987928211689, acc: 0.99048912525177)
[2025-02-13 03:30:51,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:52,117][root][INFO] - Training Epoch: 2/2, step 74/7134 completed (loss: 0.044155996292829514, acc: 0.9818941354751587)
[2025-02-13 03:30:52,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:52,594][root][INFO] - Training Epoch: 2/2, step 75/7134 completed (loss: 0.08819402009248734, acc: 0.980567991733551)
[2025-02-13 03:30:52,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:53,043][root][INFO] - Training Epoch: 2/2, step 76/7134 completed (loss: 0.025925053283572197, acc: 0.99370276927948)
[2025-02-13 03:30:53,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:53,482][root][INFO] - Training Epoch: 2/2, step 77/7134 completed (loss: 0.03354911133646965, acc: 0.9924812316894531)
[2025-02-13 03:30:53,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:53,928][root][INFO] - Training Epoch: 2/2, step 78/7134 completed (loss: 0.0540960431098938, acc: 0.9858490824699402)
[2025-02-13 03:30:54,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:54,350][root][INFO] - Training Epoch: 2/2, step 79/7134 completed (loss: 0.04782555252313614, acc: 0.9768518805503845)
[2025-02-13 03:30:54,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:54,796][root][INFO] - Training Epoch: 2/2, step 80/7134 completed (loss: 0.03316250443458557, acc: 0.9947916865348816)
[2025-02-13 03:30:54,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:55,253][root][INFO] - Training Epoch: 2/2, step 81/7134 completed (loss: 0.029527226462960243, acc: 0.9921466112136841)
[2025-02-13 03:30:55,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:55,730][root][INFO] - Training Epoch: 2/2, step 82/7134 completed (loss: 0.03592268377542496, acc: 0.9915134310722351)
[2025-02-13 03:30:55,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:56,183][root][INFO] - Training Epoch: 2/2, step 83/7134 completed (loss: 0.027208419516682625, acc: 0.9910025596618652)
[2025-02-13 03:30:56,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:56,624][root][INFO] - Training Epoch: 2/2, step 84/7134 completed (loss: 0.0320933498442173, acc: 0.9896373152732849)
[2025-02-13 03:30:56,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:57,011][root][INFO] - Training Epoch: 2/2, step 85/7134 completed (loss: 0.048670444637537, acc: 0.9856801629066467)
[2025-02-13 03:30:57,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:57,419][root][INFO] - Training Epoch: 2/2, step 86/7134 completed (loss: 0.029201118275523186, acc: 0.9862385392189026)
[2025-02-13 03:30:57,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:57,875][root][INFO] - Training Epoch: 2/2, step 87/7134 completed (loss: 0.12345194816589355, acc: 0.9706314206123352)
[2025-02-13 03:30:58,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:58,337][root][INFO] - Training Epoch: 2/2, step 88/7134 completed (loss: 0.11389254778623581, acc: 0.9723374843597412)
[2025-02-13 03:30:58,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:58,790][root][INFO] - Training Epoch: 2/2, step 89/7134 completed (loss: 0.036344002932310104, acc: 0.9897304177284241)
[2025-02-13 03:30:58,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:59,214][root][INFO] - Training Epoch: 2/2, step 90/7134 completed (loss: 0.044959139078855515, acc: 0.9877622127532959)
[2025-02-13 03:30:59,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:59,626][root][INFO] - Training Epoch: 2/2, step 91/7134 completed (loss: 0.04110541567206383, acc: 0.9877551198005676)
[2025-02-13 03:30:59,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:00,087][root][INFO] - Training Epoch: 2/2, step 92/7134 completed (loss: 0.04640350863337517, acc: 0.9845722317695618)
[2025-02-13 03:31:00,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:00,532][root][INFO] - Training Epoch: 2/2, step 93/7134 completed (loss: 0.027605228126049042, acc: 0.9917355179786682)
[2025-02-13 03:31:00,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:00,883][root][INFO] - Training Epoch: 2/2, step 94/7134 completed (loss: 0.08318024128675461, acc: 0.9767981171607971)
[2025-02-13 03:31:01,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:01,332][root][INFO] - Training Epoch: 2/2, step 95/7134 completed (loss: 0.03882487118244171, acc: 0.9861809015274048)
[2025-02-13 03:31:01,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:01,706][root][INFO] - Training Epoch: 2/2, step 96/7134 completed (loss: 0.03377338498830795, acc: 0.9888268113136292)
[2025-02-13 03:31:01,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:01,972][root][INFO] - Training Epoch: 2/2, step 97/7134 completed (loss: 0.20738935470581055, acc: 0.9638554453849792)
[2025-02-13 03:31:02,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:02,433][root][INFO] - Training Epoch: 2/2, step 98/7134 completed (loss: 0.06131628155708313, acc: 0.9773463010787964)
[2025-02-13 03:31:02,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:02,851][root][INFO] - Training Epoch: 2/2, step 99/7134 completed (loss: 0.09118222445249557, acc: 0.9779816269874573)
[2025-02-13 03:31:03,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:03,328][root][INFO] - Training Epoch: 2/2, step 100/7134 completed (loss: 0.03383580222725868, acc: 0.9923076629638672)
[2025-02-13 03:31:03,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:03,729][root][INFO] - Training Epoch: 2/2, step 101/7134 completed (loss: 0.04433946684002876, acc: 0.9819079041481018)
[2025-02-13 03:31:03,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:04,173][root][INFO] - Training Epoch: 2/2, step 102/7134 completed (loss: 0.029584215953946114, acc: 0.9914529919624329)
[2025-02-13 03:31:04,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:04,563][root][INFO] - Training Epoch: 2/2, step 103/7134 completed (loss: 0.04324805736541748, acc: 0.9833333492279053)
[2025-02-13 03:31:04,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:05,009][root][INFO] - Training Epoch: 2/2, step 104/7134 completed (loss: 0.0329052209854126, acc: 0.9848942756652832)
[2025-02-13 03:31:05,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:05,461][root][INFO] - Training Epoch: 2/2, step 105/7134 completed (loss: 0.030187904834747314, acc: 0.990212082862854)
[2025-02-13 03:31:05,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:05,909][root][INFO] - Training Epoch: 2/2, step 106/7134 completed (loss: 0.05814220383763313, acc: 0.9786700010299683)
[2025-02-13 03:31:06,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:06,334][root][INFO] - Training Epoch: 2/2, step 107/7134 completed (loss: 0.15148484706878662, acc: 0.9527220726013184)
[2025-02-13 03:31:06,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:06,799][root][INFO] - Training Epoch: 2/2, step 108/7134 completed (loss: 0.05824341997504234, acc: 0.9832134246826172)
[2025-02-13 03:31:06,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:07,194][root][INFO] - Training Epoch: 2/2, step 109/7134 completed (loss: 0.048015620559453964, acc: 0.9871323704719543)
[2025-02-13 03:31:07,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:07,671][root][INFO] - Training Epoch: 2/2, step 110/7134 completed (loss: 0.05396975576877594, acc: 0.9858989715576172)
[2025-02-13 03:31:07,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:08,081][root][INFO] - Training Epoch: 2/2, step 111/7134 completed (loss: 0.04231254383921623, acc: 0.9895470142364502)
[2025-02-13 03:31:08,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:08,477][root][INFO] - Training Epoch: 2/2, step 112/7134 completed (loss: 0.033028505742549896, acc: 0.990755021572113)
[2025-02-13 03:31:08,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:08,925][root][INFO] - Training Epoch: 2/2, step 113/7134 completed (loss: 0.023800106719136238, acc: 0.9962025284767151)
[2025-02-13 03:31:09,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:09,368][root][INFO] - Training Epoch: 2/2, step 114/7134 completed (loss: 0.03328961506485939, acc: 0.9854111671447754)
[2025-02-13 03:31:09,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:09,698][root][INFO] - Training Epoch: 2/2, step 115/7134 completed (loss: 0.08550114929676056, acc: 0.9685039520263672)
[2025-02-13 03:31:09,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:10,183][root][INFO] - Training Epoch: 2/2, step 116/7134 completed (loss: 0.052512649446725845, acc: 0.9799764156341553)
[2025-02-13 03:31:10,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:10,600][root][INFO] - Training Epoch: 2/2, step 117/7134 completed (loss: 0.04365520551800728, acc: 0.9861496090888977)
[2025-02-13 03:31:10,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:11,081][root][INFO] - Training Epoch: 2/2, step 118/7134 completed (loss: 0.03171174228191376, acc: 0.9885203838348389)
[2025-02-13 03:31:11,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:11,530][root][INFO] - Training Epoch: 2/2, step 119/7134 completed (loss: 0.04915420711040497, acc: 0.9842615127563477)
[2025-02-13 03:31:11,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:11,955][root][INFO] - Training Epoch: 2/2, step 120/7134 completed (loss: 0.01495580654591322, acc: 0.9960681796073914)
[2025-02-13 03:31:12,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:12,378][root][INFO] - Training Epoch: 2/2, step 121/7134 completed (loss: 0.03734811022877693, acc: 0.9894319772720337)
[2025-02-13 03:31:12,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:12,815][root][INFO] - Training Epoch: 2/2, step 122/7134 completed (loss: 0.019261522218585014, acc: 0.990728497505188)
[2025-02-13 03:31:12,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:13,204][root][INFO] - Training Epoch: 2/2, step 123/7134 completed (loss: 0.014490279369056225, acc: 0.9954954981803894)
[2025-02-13 03:31:13,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:13,602][root][INFO] - Training Epoch: 2/2, step 124/7134 completed (loss: 0.02434147708117962, acc: 0.9879310131072998)
[2025-02-13 03:31:13,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:14,069][root][INFO] - Training Epoch: 2/2, step 125/7134 completed (loss: 0.03768559545278549, acc: 0.9856194853782654)
[2025-02-13 03:31:14,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:14,496][root][INFO] - Training Epoch: 2/2, step 126/7134 completed (loss: 0.029335027560591698, acc: 0.9906666874885559)
[2025-02-13 03:31:14,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:14,956][root][INFO] - Training Epoch: 2/2, step 127/7134 completed (loss: 0.01827738620340824, acc: 0.9925925731658936)
[2025-02-13 03:31:15,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:15,408][root][INFO] - Training Epoch: 2/2, step 128/7134 completed (loss: 0.025360215455293655, acc: 0.9942330121994019)
[2025-02-13 03:31:15,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:15,880][root][INFO] - Training Epoch: 2/2, step 129/7134 completed (loss: 0.04475283622741699, acc: 0.9914215803146362)
[2025-02-13 03:31:16,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:16,320][root][INFO] - Training Epoch: 2/2, step 130/7134 completed (loss: 0.02385902591049671, acc: 0.9894737005233765)
[2025-02-13 03:31:16,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:16,767][root][INFO] - Training Epoch: 2/2, step 131/7134 completed (loss: 0.019804587587714195, acc: 0.9934210777282715)
[2025-02-13 03:31:16,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:17,194][root][INFO] - Training Epoch: 2/2, step 132/7134 completed (loss: 0.02432582527399063, acc: 0.994194507598877)
[2025-02-13 03:31:17,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:17,610][root][INFO] - Training Epoch: 2/2, step 133/7134 completed (loss: 0.009361855685710907, acc: 0.9983360767364502)
[2025-02-13 03:31:17,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:18,055][root][INFO] - Training Epoch: 2/2, step 134/7134 completed (loss: 0.025623919442296028, acc: 0.9946902394294739)
[2025-02-13 03:31:18,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:18,507][root][INFO] - Training Epoch: 2/2, step 135/7134 completed (loss: 0.025846276432275772, acc: 0.996666669845581)
[2025-02-13 03:31:18,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:18,956][root][INFO] - Training Epoch: 2/2, step 136/7134 completed (loss: 0.023417530581355095, acc: 0.9952210187911987)
[2025-02-13 03:31:19,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:19,420][root][INFO] - Training Epoch: 2/2, step 137/7134 completed (loss: 0.049425940960645676, acc: 0.9876998662948608)
[2025-02-13 03:31:19,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:19,881][root][INFO] - Training Epoch: 2/2, step 138/7134 completed (loss: 0.029982810840010643, acc: 0.9898862242698669)
[2025-02-13 03:31:20,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:20,355][root][INFO] - Training Epoch: 2/2, step 139/7134 completed (loss: 0.051633693277835846, acc: 0.9897698163986206)
[2025-02-13 03:31:20,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:20,795][root][INFO] - Training Epoch: 2/2, step 140/7134 completed (loss: 0.016494497656822205, acc: 0.9940298795700073)
[2025-02-13 03:31:20,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:21,218][root][INFO] - Training Epoch: 2/2, step 141/7134 completed (loss: 0.010626337490975857, acc: 0.9964622855186462)
[2025-02-13 03:31:21,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:21,683][root][INFO] - Training Epoch: 2/2, step 142/7134 completed (loss: 0.06690718233585358, acc: 0.9847009778022766)
[2025-02-13 03:31:21,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:22,128][root][INFO] - Training Epoch: 2/2, step 143/7134 completed (loss: 0.009297914803028107, acc: 0.9973924160003662)
[2025-02-13 03:31:22,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:22,582][root][INFO] - Training Epoch: 2/2, step 144/7134 completed (loss: 0.08702848851680756, acc: 0.9782971739768982)
[2025-02-13 03:31:22,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:23,018][root][INFO] - Training Epoch: 2/2, step 145/7134 completed (loss: 0.1392877846956253, acc: 0.9619718194007874)
[2025-02-13 03:31:23,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:23,471][root][INFO] - Training Epoch: 2/2, step 146/7134 completed (loss: 0.0568661168217659, acc: 0.9800570011138916)
[2025-02-13 03:31:23,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:23,882][root][INFO] - Training Epoch: 2/2, step 147/7134 completed (loss: 0.06682217866182327, acc: 0.9792027473449707)
[2025-02-13 03:31:24,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:24,286][root][INFO] - Training Epoch: 2/2, step 148/7134 completed (loss: 0.13246126472949982, acc: 0.9605568647384644)
[2025-02-13 03:31:24,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:24,716][root][INFO] - Training Epoch: 2/2, step 149/7134 completed (loss: 0.10567290335893631, acc: 0.9767742156982422)
[2025-02-13 03:31:24,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:25,178][root][INFO] - Training Epoch: 2/2, step 150/7134 completed (loss: 0.06439327448606491, acc: 0.9745989441871643)
[2025-02-13 03:31:25,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:25,633][root][INFO] - Training Epoch: 2/2, step 151/7134 completed (loss: 0.07353543490171432, acc: 0.9773269891738892)
[2025-02-13 03:31:25,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:26,083][root][INFO] - Training Epoch: 2/2, step 152/7134 completed (loss: 0.059845320880413055, acc: 0.9794988632202148)
[2025-02-13 03:31:26,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:26,523][root][INFO] - Training Epoch: 2/2, step 153/7134 completed (loss: 0.09351722151041031, acc: 0.9790502786636353)
[2025-02-13 03:31:26,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:26,925][root][INFO] - Training Epoch: 2/2, step 154/7134 completed (loss: 0.05917574465274811, acc: 0.9878419637680054)
[2025-02-13 03:31:27,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:27,388][root][INFO] - Training Epoch: 2/2, step 155/7134 completed (loss: 0.03897577151656151, acc: 0.9883117079734802)
[2025-02-13 03:31:27,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:27,859][root][INFO] - Training Epoch: 2/2, step 156/7134 completed (loss: 0.044651344418525696, acc: 0.9859976768493652)
[2025-02-13 03:31:28,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:28,344][root][INFO] - Training Epoch: 2/2, step 157/7134 completed (loss: 0.08260651677846909, acc: 0.9795180559158325)
[2025-02-13 03:31:28,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:28,808][root][INFO] - Training Epoch: 2/2, step 158/7134 completed (loss: 0.024835513904690742, acc: 0.9907529950141907)
[2025-02-13 03:31:28,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:29,268][root][INFO] - Training Epoch: 2/2, step 159/7134 completed (loss: 0.059159498661756516, acc: 0.9826839566230774)
[2025-02-13 03:31:29,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:29,660][root][INFO] - Training Epoch: 2/2, step 160/7134 completed (loss: 0.06900715082883835, acc: 0.97773277759552)
[2025-02-13 03:31:29,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:29,942][root][INFO] - Training Epoch: 2/2, step 161/7134 completed (loss: 0.022901810705661774, acc: 0.9912790656089783)
[2025-02-13 03:31:30,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:30,280][root][INFO] - Training Epoch: 2/2, step 162/7134 completed (loss: 0.05009962245821953, acc: 0.9795396327972412)
[2025-02-13 03:31:30,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:30,750][root][INFO] - Training Epoch: 2/2, step 163/7134 completed (loss: 0.07421790808439255, acc: 0.985228955745697)
[2025-02-13 03:31:30,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:31,108][root][INFO] - Training Epoch: 2/2, step 164/7134 completed (loss: 0.03679496422410011, acc: 0.9936169981956482)
[2025-02-13 03:31:31,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:31,491][root][INFO] - Training Epoch: 2/2, step 165/7134 completed (loss: 0.07463311403989792, acc: 0.9832535982131958)
[2025-02-13 03:31:31,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:31,756][root][INFO] - Training Epoch: 2/2, step 166/7134 completed (loss: 0.04542700573801994, acc: 0.9910447597503662)
[2025-02-13 03:31:31,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:32,200][root][INFO] - Training Epoch: 2/2, step 167/7134 completed (loss: 0.4776803255081177, acc: 0.9126559495925903)
[2025-02-13 03:31:32,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:32,488][root][INFO] - Training Epoch: 2/2, step 168/7134 completed (loss: 1.4301611185073853, acc: 0.7137255072593689)
[2025-02-13 03:31:32,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:32,828][root][INFO] - Training Epoch: 2/2, step 169/7134 completed (loss: 0.9119888544082642, acc: 0.7759103775024414)
[2025-02-13 03:31:32,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:33,202][root][INFO] - Training Epoch: 2/2, step 170/7134 completed (loss: 0.3584889769554138, acc: 0.9032257795333862)
[2025-02-13 03:31:33,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:33,550][root][INFO] - Training Epoch: 2/2, step 171/7134 completed (loss: 0.4941166043281555, acc: 0.8831169009208679)
[2025-02-13 03:31:33,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:33,942][root][INFO] - Training Epoch: 2/2, step 172/7134 completed (loss: 0.29718711972236633, acc: 0.90220046043396)
[2025-02-13 03:31:34,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:34,296][root][INFO] - Training Epoch: 2/2, step 173/7134 completed (loss: 0.23048901557922363, acc: 0.9469026327133179)
[2025-02-13 03:31:34,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:34,678][root][INFO] - Training Epoch: 2/2, step 174/7134 completed (loss: 0.22167877852916718, acc: 0.9436619877815247)
[2025-02-13 03:31:34,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:35,099][root][INFO] - Training Epoch: 2/2, step 175/7134 completed (loss: 0.1692209541797638, acc: 0.9563636183738708)
[2025-02-13 03:31:35,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:35,573][root][INFO] - Training Epoch: 2/2, step 176/7134 completed (loss: 0.13427191972732544, acc: 0.9594202637672424)
[2025-02-13 03:31:35,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:35,997][root][INFO] - Training Epoch: 2/2, step 177/7134 completed (loss: 0.17703202366828918, acc: 0.9462962746620178)
[2025-02-13 03:31:36,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:36,260][root][INFO] - Training Epoch: 2/2, step 178/7134 completed (loss: 0.1215878427028656, acc: 0.9534883499145508)
[2025-02-13 03:31:36,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:36,678][root][INFO] - Training Epoch: 2/2, step 179/7134 completed (loss: 0.11624351143836975, acc: 0.9613152742385864)
[2025-02-13 03:31:36,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:37,136][root][INFO] - Training Epoch: 2/2, step 180/7134 completed (loss: 0.0852876603603363, acc: 0.9794344305992126)
[2025-02-13 03:31:37,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:37,551][root][INFO] - Training Epoch: 2/2, step 181/7134 completed (loss: 0.04204728081822395, acc: 0.9881423115730286)
[2025-02-13 03:31:37,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:38,017][root][INFO] - Training Epoch: 2/2, step 182/7134 completed (loss: 0.04527869448065758, acc: 0.9853479862213135)
[2025-02-13 03:31:38,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:38,462][root][INFO] - Training Epoch: 2/2, step 183/7134 completed (loss: 0.057861510664224625, acc: 0.9877384305000305)
[2025-02-13 03:31:38,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:38,883][root][INFO] - Training Epoch: 2/2, step 184/7134 completed (loss: 0.09251923859119415, acc: 0.9769585132598877)
[2025-02-13 03:31:39,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:39,315][root][INFO] - Training Epoch: 2/2, step 185/7134 completed (loss: 0.023136373609304428, acc: 0.9928469061851501)
[2025-02-13 03:31:39,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:39,762][root][INFO] - Training Epoch: 2/2, step 186/7134 completed (loss: 0.06576066464185715, acc: 0.9775640964508057)
[2025-02-13 03:31:39,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:40,176][root][INFO] - Training Epoch: 2/2, step 187/7134 completed (loss: 0.06963463872671127, acc: 0.9832402467727661)
[2025-02-13 03:31:40,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:40,610][root][INFO] - Training Epoch: 2/2, step 188/7134 completed (loss: 0.13864880800247192, acc: 0.9624999761581421)
[2025-02-13 03:31:40,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:40,962][root][INFO] - Training Epoch: 2/2, step 189/7134 completed (loss: 0.08812740445137024, acc: 0.9807692170143127)
[2025-02-13 03:31:41,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:41,413][root][INFO] - Training Epoch: 2/2, step 190/7134 completed (loss: 0.17014901340007782, acc: 0.9687987565994263)
[2025-02-13 03:31:41,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:41,797][root][INFO] - Training Epoch: 2/2, step 191/7134 completed (loss: 0.054154712706804276, acc: 0.9829059839248657)
[2025-02-13 03:31:41,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:42,271][root][INFO] - Training Epoch: 2/2, step 192/7134 completed (loss: 0.017595624551177025, acc: 0.993127167224884)
[2025-02-13 03:31:42,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:42,698][root][INFO] - Training Epoch: 2/2, step 193/7134 completed (loss: 0.06302240490913391, acc: 0.9798657894134521)
[2025-02-13 03:31:42,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:43,148][root][INFO] - Training Epoch: 2/2, step 194/7134 completed (loss: 0.057990849018096924, acc: 0.9799138903617859)
[2025-02-13 03:31:43,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:43,565][root][INFO] - Training Epoch: 2/2, step 195/7134 completed (loss: 0.06184057518839836, acc: 0.9798319339752197)
[2025-02-13 03:31:43,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:44,015][root][INFO] - Training Epoch: 2/2, step 196/7134 completed (loss: 0.07755731046199799, acc: 0.9799465537071228)
[2025-02-13 03:31:44,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:44,464][root][INFO] - Training Epoch: 2/2, step 197/7134 completed (loss: 0.06719831377267838, acc: 0.9886040091514587)
[2025-02-13 03:31:44,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:44,918][root][INFO] - Training Epoch: 2/2, step 198/7134 completed (loss: 0.04347969591617584, acc: 0.98591548204422)
[2025-02-13 03:31:45,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:45,337][root][INFO] - Training Epoch: 2/2, step 199/7134 completed (loss: 0.08551310002803802, acc: 0.9704433679580688)
[2025-02-13 03:31:45,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:45,797][root][INFO] - Training Epoch: 2/2, step 200/7134 completed (loss: 0.02989836037158966, acc: 0.990604043006897)
[2025-02-13 03:31:45,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:46,229][root][INFO] - Training Epoch: 2/2, step 201/7134 completed (loss: 0.05334971845149994, acc: 0.9873595237731934)
[2025-02-13 03:31:46,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:46,658][root][INFO] - Training Epoch: 2/2, step 202/7134 completed (loss: 0.05075808987021446, acc: 0.9820689558982849)
[2025-02-13 03:31:46,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:47,091][root][INFO] - Training Epoch: 2/2, step 203/7134 completed (loss: 0.061030369251966476, acc: 0.9793388247489929)
[2025-02-13 03:31:47,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:47,533][root][INFO] - Training Epoch: 2/2, step 204/7134 completed (loss: 0.05402592942118645, acc: 0.9802431464195251)
[2025-02-13 03:31:47,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:47,985][root][INFO] - Training Epoch: 2/2, step 205/7134 completed (loss: 0.0967726781964302, acc: 0.9767140746116638)
[2025-02-13 03:31:48,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:48,425][root][INFO] - Training Epoch: 2/2, step 206/7134 completed (loss: 0.0930686891078949, acc: 0.9727126955986023)
[2025-02-13 03:31:48,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:48,849][root][INFO] - Training Epoch: 2/2, step 207/7134 completed (loss: 0.07118921726942062, acc: 0.979522168636322)
[2025-02-13 03:31:48,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:49,278][root][INFO] - Training Epoch: 2/2, step 208/7134 completed (loss: 0.12249704450368881, acc: 0.9666666388511658)
[2025-02-13 03:31:49,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:49,726][root][INFO] - Training Epoch: 2/2, step 209/7134 completed (loss: 0.12637442350387573, acc: 0.9676165580749512)
[2025-02-13 03:31:49,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:50,185][root][INFO] - Training Epoch: 2/2, step 210/7134 completed (loss: 0.17523130774497986, acc: 0.9627851247787476)
[2025-02-13 03:31:50,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:50,528][root][INFO] - Training Epoch: 2/2, step 211/7134 completed (loss: 0.1443396508693695, acc: 0.9640522599220276)
[2025-02-13 03:31:50,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:50,941][root][INFO] - Training Epoch: 2/2, step 212/7134 completed (loss: 0.2299790233373642, acc: 0.9566115736961365)
[2025-02-13 03:31:51,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:51,361][root][INFO] - Training Epoch: 2/2, step 213/7134 completed (loss: 0.07853934168815613, acc: 0.9773463010787964)
[2025-02-13 03:31:51,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:51,722][root][INFO] - Training Epoch: 2/2, step 214/7134 completed (loss: 0.06305010616779327, acc: 0.9850427508354187)
[2025-02-13 03:31:51,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:52,133][root][INFO] - Training Epoch: 2/2, step 215/7134 completed (loss: 0.08552813529968262, acc: 0.9826388955116272)
[2025-02-13 03:31:52,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:52,547][root][INFO] - Training Epoch: 2/2, step 216/7134 completed (loss: 0.06017214432358742, acc: 0.9861111044883728)
[2025-02-13 03:31:52,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:53,017][root][INFO] - Training Epoch: 2/2, step 217/7134 completed (loss: 0.06416132301092148, acc: 0.9866666793823242)
[2025-02-13 03:31:53,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:53,461][root][INFO] - Training Epoch: 2/2, step 218/7134 completed (loss: 0.023874733597040176, acc: 0.9961832165718079)
[2025-02-13 03:31:53,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:53,881][root][INFO] - Training Epoch: 2/2, step 219/7134 completed (loss: 0.05253429338335991, acc: 0.9863547682762146)
[2025-02-13 03:31:53,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:54,279][root][INFO] - Training Epoch: 2/2, step 220/7134 completed (loss: 0.04925986006855965, acc: 0.9895397424697876)
[2025-02-13 03:31:54,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:54,719][root][INFO] - Training Epoch: 2/2, step 221/7134 completed (loss: 0.03476673737168312, acc: 0.9925373196601868)
[2025-02-13 03:31:54,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:55,183][root][INFO] - Training Epoch: 2/2, step 222/7134 completed (loss: 0.058899473398923874, acc: 0.9869109988212585)
[2025-02-13 03:31:55,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:55,620][root][INFO] - Training Epoch: 2/2, step 223/7134 completed (loss: 0.07983721047639847, acc: 0.9781771302223206)
[2025-02-13 03:31:55,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:56,044][root][INFO] - Training Epoch: 2/2, step 224/7134 completed (loss: 0.11241743713617325, acc: 0.9675745964050293)
[2025-02-13 03:31:56,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:56,468][root][INFO] - Training Epoch: 2/2, step 225/7134 completed (loss: 0.1092434972524643, acc: 0.9704749584197998)
[2025-02-13 03:31:56,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:56,916][root][INFO] - Training Epoch: 2/2, step 226/7134 completed (loss: 0.04891923442482948, acc: 0.9902200698852539)
[2025-02-13 03:31:57,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:57,370][root][INFO] - Training Epoch: 2/2, step 227/7134 completed (loss: 0.16202397644519806, acc: 0.9652605652809143)
[2025-02-13 03:31:57,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:57,812][root][INFO] - Training Epoch: 2/2, step 228/7134 completed (loss: 0.06991103291511536, acc: 0.9797394871711731)
[2025-02-13 03:31:57,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:58,256][root][INFO] - Training Epoch: 2/2, step 229/7134 completed (loss: 0.10611969977617264, acc: 0.9721518754959106)
[2025-02-13 03:31:58,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:58,708][root][INFO] - Training Epoch: 2/2, step 230/7134 completed (loss: 0.06225300952792168, acc: 0.9766584634780884)
[2025-02-13 03:31:58,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:59,129][root][INFO] - Training Epoch: 2/2, step 231/7134 completed (loss: 0.06271762400865555, acc: 0.9792843461036682)
[2025-02-13 03:31:59,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:59,547][root][INFO] - Training Epoch: 2/2, step 232/7134 completed (loss: 0.04669648036360741, acc: 0.9835255146026611)
[2025-02-13 03:31:59,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:59,972][root][INFO] - Training Epoch: 2/2, step 233/7134 completed (loss: 0.06552741676568985, acc: 0.9857369065284729)
[2025-02-13 03:32:00,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:00,397][root][INFO] - Training Epoch: 2/2, step 234/7134 completed (loss: 0.058399781584739685, acc: 0.9847972989082336)
[2025-02-13 03:32:00,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:00,852][root][INFO] - Training Epoch: 2/2, step 235/7134 completed (loss: 0.10416820645332336, acc: 0.9800570011138916)
[2025-02-13 03:32:00,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:01,268][root][INFO] - Training Epoch: 2/2, step 236/7134 completed (loss: 0.02923683449625969, acc: 0.989062488079071)
[2025-02-13 03:32:01,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:01,692][root][INFO] - Training Epoch: 2/2, step 237/7134 completed (loss: 0.08067839592695236, acc: 0.9761549830436707)
[2025-02-13 03:32:01,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:02,060][root][INFO] - Training Epoch: 2/2, step 238/7134 completed (loss: 0.10512956231832504, acc: 0.97947758436203)
[2025-02-13 03:32:02,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:02,452][root][INFO] - Training Epoch: 2/2, step 239/7134 completed (loss: 0.023679303005337715, acc: 0.993697464466095)
[2025-02-13 03:32:02,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:02,862][root][INFO] - Training Epoch: 2/2, step 240/7134 completed (loss: 0.0830456092953682, acc: 0.9788135886192322)
[2025-02-13 03:32:03,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:03,285][root][INFO] - Training Epoch: 2/2, step 241/7134 completed (loss: 0.1115509644150734, acc: 0.979066014289856)
[2025-02-13 03:32:03,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:03,679][root][INFO] - Training Epoch: 2/2, step 242/7134 completed (loss: 0.09387052059173584, acc: 0.9653379321098328)
[2025-02-13 03:32:03,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:04,075][root][INFO] - Training Epoch: 2/2, step 243/7134 completed (loss: 0.10373543947935104, acc: 0.9727891087532043)
[2025-02-13 03:32:04,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:04,490][root][INFO] - Training Epoch: 2/2, step 244/7134 completed (loss: 0.035062652081251144, acc: 0.9879336357116699)
[2025-02-13 03:32:04,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:04,914][root][INFO] - Training Epoch: 2/2, step 245/7134 completed (loss: 0.09947285801172256, acc: 0.9772329330444336)
[2025-02-13 03:32:05,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:05,330][root][INFO] - Training Epoch: 2/2, step 246/7134 completed (loss: 0.06077226251363754, acc: 0.9858044385910034)
[2025-02-13 03:32:05,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:05,745][root][INFO] - Training Epoch: 2/2, step 247/7134 completed (loss: 0.07390972226858139, acc: 0.9764065146446228)
[2025-02-13 03:32:05,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:06,180][root][INFO] - Training Epoch: 2/2, step 248/7134 completed (loss: 0.04600709304213524, acc: 0.9825581312179565)
[2025-02-13 03:32:06,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:06,521][root][INFO] - Training Epoch: 2/2, step 249/7134 completed (loss: 0.05134829506278038, acc: 0.9812332391738892)
[2025-02-13 03:32:06,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:06,933][root][INFO] - Training Epoch: 2/2, step 250/7134 completed (loss: 0.06078118830919266, acc: 0.982594907283783)
[2025-02-13 03:32:07,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:07,345][root][INFO] - Training Epoch: 2/2, step 251/7134 completed (loss: 0.05181245878338814, acc: 0.9846153855323792)
[2025-02-13 03:32:07,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:07,725][root][INFO] - Training Epoch: 2/2, step 252/7134 completed (loss: 0.030666276812553406, acc: 0.9901639223098755)
[2025-02-13 03:32:07,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:08,167][root][INFO] - Training Epoch: 2/2, step 253/7134 completed (loss: 0.03604297339916229, acc: 0.987522304058075)
[2025-02-13 03:32:08,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:08,587][root][INFO] - Training Epoch: 2/2, step 254/7134 completed (loss: 0.03305890038609505, acc: 0.9892857074737549)
[2025-02-13 03:32:08,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:08,999][root][INFO] - Training Epoch: 2/2, step 255/7134 completed (loss: 0.04423712566494942, acc: 0.9824561476707458)
[2025-02-13 03:32:09,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:09,368][root][INFO] - Training Epoch: 2/2, step 256/7134 completed (loss: 0.0823865681886673, acc: 0.9742646813392639)
[2025-02-13 03:32:09,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:09,832][root][INFO] - Training Epoch: 2/2, step 257/7134 completed (loss: 0.055660635232925415, acc: 0.9846625924110413)
[2025-02-13 03:32:09,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:10,241][root][INFO] - Training Epoch: 2/2, step 258/7134 completed (loss: 0.039907343685626984, acc: 0.9851239919662476)
[2025-02-13 03:32:10,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:10,670][root][INFO] - Training Epoch: 2/2, step 259/7134 completed (loss: 0.048370618373155594, acc: 0.9837925434112549)
[2025-02-13 03:32:10,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:11,120][root][INFO] - Training Epoch: 2/2, step 260/7134 completed (loss: 0.03365834429860115, acc: 0.9915397763252258)
[2025-02-13 03:32:11,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:11,512][root][INFO] - Training Epoch: 2/2, step 261/7134 completed (loss: 0.04638369381427765, acc: 0.987730085849762)
[2025-02-13 03:32:11,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:11,865][root][INFO] - Training Epoch: 2/2, step 262/7134 completed (loss: 0.020258795469999313, acc: 0.9916805028915405)
[2025-02-13 03:32:11,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:12,284][root][INFO] - Training Epoch: 2/2, step 263/7134 completed (loss: 0.05456375703215599, acc: 0.9872286319732666)
[2025-02-13 03:32:12,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:12,738][root][INFO] - Training Epoch: 2/2, step 264/7134 completed (loss: 0.04169755056500435, acc: 0.9871495366096497)
[2025-02-13 03:32:12,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:13,186][root][INFO] - Training Epoch: 2/2, step 265/7134 completed (loss: 0.06046314910054207, acc: 0.9832473993301392)
[2025-02-13 03:32:13,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:13,631][root][INFO] - Training Epoch: 2/2, step 266/7134 completed (loss: 0.03770475089550018, acc: 0.9885321259498596)
[2025-02-13 03:32:13,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:14,109][root][INFO] - Training Epoch: 2/2, step 267/7134 completed (loss: 0.05005352571606636, acc: 0.9793103337287903)
[2025-02-13 03:32:14,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:14,582][root][INFO] - Training Epoch: 2/2, step 268/7134 completed (loss: 0.0642537996172905, acc: 0.9844357967376709)
[2025-02-13 03:32:14,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:15,035][root][INFO] - Training Epoch: 2/2, step 269/7134 completed (loss: 0.050396353006362915, acc: 0.9833101630210876)
[2025-02-13 03:32:15,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:15,455][root][INFO] - Training Epoch: 2/2, step 270/7134 completed (loss: 0.04465114697813988, acc: 0.9880596995353699)
[2025-02-13 03:32:15,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:15,911][root][INFO] - Training Epoch: 2/2, step 271/7134 completed (loss: 0.04836604744195938, acc: 0.9836065769195557)
[2025-02-13 03:32:16,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:16,350][root][INFO] - Training Epoch: 2/2, step 272/7134 completed (loss: 0.032547805458307266, acc: 0.9885495901107788)
[2025-02-13 03:32:16,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:16,796][root][INFO] - Training Epoch: 2/2, step 273/7134 completed (loss: 0.02413284219801426, acc: 0.9880239367485046)
[2025-02-13 03:32:16,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:17,185][root][INFO] - Training Epoch: 2/2, step 274/7134 completed (loss: 0.03294408321380615, acc: 0.993122398853302)
[2025-02-13 03:32:17,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:17,655][root][INFO] - Training Epoch: 2/2, step 275/7134 completed (loss: 0.05249736085534096, acc: 0.988399088382721)
[2025-02-13 03:32:17,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:18,116][root][INFO] - Training Epoch: 2/2, step 276/7134 completed (loss: 0.025084584951400757, acc: 0.9902557730674744)
[2025-02-13 03:32:18,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:18,560][root][INFO] - Training Epoch: 2/2, step 277/7134 completed (loss: 0.05388482287526131, acc: 0.9871299862861633)
[2025-02-13 03:32:18,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:18,994][root][INFO] - Training Epoch: 2/2, step 278/7134 completed (loss: 0.03085296042263508, acc: 0.992337167263031)
[2025-02-13 03:32:19,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:19,445][root][INFO] - Training Epoch: 2/2, step 279/7134 completed (loss: 0.03918430209159851, acc: 0.990813672542572)
[2025-02-13 03:32:19,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:19,850][root][INFO] - Training Epoch: 2/2, step 280/7134 completed (loss: 0.03763937950134277, acc: 0.9856630563735962)
[2025-02-13 03:32:19,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:20,265][root][INFO] - Training Epoch: 2/2, step 281/7134 completed (loss: 0.056152403354644775, acc: 0.9819999933242798)
[2025-02-13 03:32:20,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:20,692][root][INFO] - Training Epoch: 2/2, step 282/7134 completed (loss: 0.07008504867553711, acc: 0.970992386341095)
[2025-02-13 03:32:20,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:21,114][root][INFO] - Training Epoch: 2/2, step 283/7134 completed (loss: 0.06590774655342102, acc: 0.9847198724746704)
[2025-02-13 03:32:21,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:21,509][root][INFO] - Training Epoch: 2/2, step 284/7134 completed (loss: 0.036939993500709534, acc: 0.9815436005592346)
[2025-02-13 03:32:21,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:21,910][root][INFO] - Training Epoch: 2/2, step 285/7134 completed (loss: 0.03908521682024002, acc: 0.9823633432388306)
[2025-02-13 03:32:22,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:22,307][root][INFO] - Training Epoch: 2/2, step 286/7134 completed (loss: 0.06287239491939545, acc: 0.9876106381416321)
[2025-02-13 03:32:22,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:22,737][root][INFO] - Training Epoch: 2/2, step 287/7134 completed (loss: 0.04726213216781616, acc: 0.9839486479759216)
[2025-02-13 03:32:22,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:23,174][root][INFO] - Training Epoch: 2/2, step 288/7134 completed (loss: 0.06380289047956467, acc: 0.9834254384040833)
[2025-02-13 03:32:23,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:23,626][root][INFO] - Training Epoch: 2/2, step 289/7134 completed (loss: 0.04172321781516075, acc: 0.9819168448448181)
[2025-02-13 03:32:23,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:24,026][root][INFO] - Training Epoch: 2/2, step 290/7134 completed (loss: 0.02772989496588707, acc: 0.9908814430236816)
[2025-02-13 03:32:24,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:24,447][root][INFO] - Training Epoch: 2/2, step 291/7134 completed (loss: 0.08228569477796555, acc: 0.9769737124443054)
[2025-02-13 03:32:24,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:24,879][root][INFO] - Training Epoch: 2/2, step 292/7134 completed (loss: 0.09079008549451828, acc: 0.9723926186561584)
[2025-02-13 03:32:25,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:25,282][root][INFO] - Training Epoch: 2/2, step 293/7134 completed (loss: 0.06624671816825867, acc: 0.9839357137680054)
[2025-02-13 03:32:25,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:25,715][root][INFO] - Training Epoch: 2/2, step 294/7134 completed (loss: 0.048584721982479095, acc: 0.9844054579734802)
[2025-02-13 03:32:25,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:26,173][root][INFO] - Training Epoch: 2/2, step 295/7134 completed (loss: 0.07016812264919281, acc: 0.9812286496162415)
[2025-02-13 03:32:26,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:26,601][root][INFO] - Training Epoch: 2/2, step 296/7134 completed (loss: 0.031003180891275406, acc: 0.9906976819038391)
[2025-02-13 03:32:26,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:27,051][root][INFO] - Training Epoch: 2/2, step 297/7134 completed (loss: 0.024848993867635727, acc: 0.9928951859474182)
[2025-02-13 03:32:27,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:27,479][root][INFO] - Training Epoch: 2/2, step 298/7134 completed (loss: 0.020885618403553963, acc: 0.9919999837875366)
[2025-02-13 03:32:27,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:27,887][root][INFO] - Training Epoch: 2/2, step 299/7134 completed (loss: 0.05127059295773506, acc: 0.9819548726081848)
[2025-02-13 03:32:28,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:28,303][root][INFO] - Training Epoch: 2/2, step 300/7134 completed (loss: 0.029756629839539528, acc: 0.9886731505393982)
[2025-02-13 03:32:28,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:28,740][root][INFO] - Training Epoch: 2/2, step 301/7134 completed (loss: 0.028377829119563103, acc: 0.9886845946311951)
[2025-02-13 03:32:28,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:29,143][root][INFO] - Training Epoch: 2/2, step 302/7134 completed (loss: 0.0675991028547287, acc: 0.9824561476707458)
[2025-02-13 03:32:29,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:29,503][root][INFO] - Training Epoch: 2/2, step 303/7134 completed (loss: 0.04718217998743057, acc: 0.9864077568054199)
[2025-02-13 03:32:29,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:29,901][root][INFO] - Training Epoch: 2/2, step 304/7134 completed (loss: 0.026668647304177284, acc: 0.9870370626449585)
[2025-02-13 03:32:30,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:30,326][root][INFO] - Training Epoch: 2/2, step 305/7134 completed (loss: 0.03676334768533707, acc: 0.9920508861541748)
[2025-02-13 03:32:30,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:30,733][root][INFO] - Training Epoch: 2/2, step 306/7134 completed (loss: 0.011011441238224506, acc: 0.9981883764266968)
[2025-02-13 03:32:30,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:31,163][root][INFO] - Training Epoch: 2/2, step 307/7134 completed (loss: 0.0624142661690712, acc: 0.9786477088928223)
[2025-02-13 03:32:31,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:31,585][root][INFO] - Training Epoch: 2/2, step 308/7134 completed (loss: 0.01889495924115181, acc: 0.9950658082962036)
[2025-02-13 03:32:31,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:32,022][root][INFO] - Training Epoch: 2/2, step 309/7134 completed (loss: 0.0406540110707283, acc: 0.982758641242981)
[2025-02-13 03:32:32,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:32,482][root][INFO] - Training Epoch: 2/2, step 310/7134 completed (loss: 0.01957976073026657, acc: 0.9956140518188477)
[2025-02-13 03:32:32,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:32,898][root][INFO] - Training Epoch: 2/2, step 311/7134 completed (loss: 0.022926095873117447, acc: 0.993779182434082)
[2025-02-13 03:32:33,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:33,296][root][INFO] - Training Epoch: 2/2, step 312/7134 completed (loss: 0.015259025618433952, acc: 0.998266875743866)
[2025-02-13 03:32:33,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:33,724][root][INFO] - Training Epoch: 2/2, step 313/7134 completed (loss: 0.02289828658103943, acc: 0.991304337978363)
[2025-02-13 03:32:33,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:34,139][root][INFO] - Training Epoch: 2/2, step 314/7134 completed (loss: 0.030601542443037033, acc: 0.9906250238418579)
[2025-02-13 03:32:34,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:34,562][root][INFO] - Training Epoch: 2/2, step 315/7134 completed (loss: 0.03558119386434555, acc: 0.9918032884597778)
[2025-02-13 03:32:34,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:34,942][root][INFO] - Training Epoch: 2/2, step 316/7134 completed (loss: 0.046063050627708435, acc: 0.987364649772644)
[2025-02-13 03:32:35,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:35,378][root][INFO] - Training Epoch: 2/2, step 317/7134 completed (loss: 0.007921010255813599, acc: 0.9972936511039734)
[2025-02-13 03:32:35,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:35,839][root][INFO] - Training Epoch: 2/2, step 318/7134 completed (loss: 0.07755123823881149, acc: 0.9829059839248657)
[2025-02-13 03:32:35,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:36,257][root][INFO] - Training Epoch: 2/2, step 319/7134 completed (loss: 0.06071984022855759, acc: 0.9859747290611267)
[2025-02-13 03:32:36,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:36,706][root][INFO] - Training Epoch: 2/2, step 320/7134 completed (loss: 0.038354869931936264, acc: 0.9914407730102539)
[2025-02-13 03:32:36,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:37,173][root][INFO] - Training Epoch: 2/2, step 321/7134 completed (loss: 0.10063279420137405, acc: 0.977011501789093)
[2025-02-13 03:32:37,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:37,642][root][INFO] - Training Epoch: 2/2, step 322/7134 completed (loss: 0.05943665653467178, acc: 0.9804804921150208)
[2025-02-13 03:32:37,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:38,074][root][INFO] - Training Epoch: 2/2, step 323/7134 completed (loss: 0.07259650528430939, acc: 0.9833119511604309)
[2025-02-13 03:32:38,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:38,540][root][INFO] - Training Epoch: 2/2, step 324/7134 completed (loss: 0.05973422899842262, acc: 0.9850402474403381)
[2025-02-13 03:32:38,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:38,989][root][INFO] - Training Epoch: 2/2, step 325/7134 completed (loss: 0.02198910340666771, acc: 0.9884318709373474)
[2025-02-13 03:32:39,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:39,411][root][INFO] - Training Epoch: 2/2, step 326/7134 completed (loss: 0.03095098026096821, acc: 0.9939098954200745)
[2025-02-13 03:32:39,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:39,874][root][INFO] - Training Epoch: 2/2, step 327/7134 completed (loss: 0.05359429121017456, acc: 0.9887387156486511)
[2025-02-13 03:32:40,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:40,347][root][INFO] - Training Epoch: 2/2, step 328/7134 completed (loss: 0.040599431842565536, acc: 0.9896296262741089)
[2025-02-13 03:32:40,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:40,817][root][INFO] - Training Epoch: 2/2, step 329/7134 completed (loss: 0.013211648911237717, acc: 0.9964747428894043)
[2025-02-13 03:32:40,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:41,284][root][INFO] - Training Epoch: 2/2, step 330/7134 completed (loss: 0.03305327519774437, acc: 0.9879153966903687)
[2025-02-13 03:32:41,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:41,733][root][INFO] - Training Epoch: 2/2, step 331/7134 completed (loss: 0.030801139771938324, acc: 0.9931585192680359)
[2025-02-13 03:32:41,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:42,171][root][INFO] - Training Epoch: 2/2, step 332/7134 completed (loss: 0.014453609474003315, acc: 0.9940968155860901)
[2025-02-13 03:32:42,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:42,596][root][INFO] - Training Epoch: 2/2, step 333/7134 completed (loss: 0.03955025225877762, acc: 0.989051103591919)
[2025-02-13 03:32:42,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:43,035][root][INFO] - Training Epoch: 2/2, step 334/7134 completed (loss: 0.01870078593492508, acc: 0.9923954606056213)
[2025-02-13 03:32:43,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:43,479][root][INFO] - Training Epoch: 2/2, step 335/7134 completed (loss: 0.019154733046889305, acc: 0.9956331849098206)
[2025-02-13 03:32:43,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:43,934][root][INFO] - Training Epoch: 2/2, step 336/7134 completed (loss: 0.05004477500915527, acc: 0.9856287240982056)
[2025-02-13 03:32:44,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:44,377][root][INFO] - Training Epoch: 2/2, step 337/7134 completed (loss: 0.022230803966522217, acc: 0.9923312664031982)
[2025-02-13 03:32:44,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:44,816][root][INFO] - Training Epoch: 2/2, step 338/7134 completed (loss: 0.05628285929560661, acc: 0.98740553855896)
[2025-02-13 03:32:44,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:45,258][root][INFO] - Training Epoch: 2/2, step 339/7134 completed (loss: 0.02331489883363247, acc: 0.9887499809265137)
[2025-02-13 03:32:45,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:45,661][root][INFO] - Training Epoch: 2/2, step 340/7134 completed (loss: 0.04108607396483421, acc: 0.9847328066825867)
[2025-02-13 03:32:45,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:46,115][root][INFO] - Training Epoch: 2/2, step 341/7134 completed (loss: 0.01865045167505741, acc: 0.9936948418617249)
[2025-02-13 03:32:46,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:46,561][root][INFO] - Training Epoch: 2/2, step 342/7134 completed (loss: 0.013159731402993202, acc: 0.9954545497894287)
[2025-02-13 03:32:46,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:46,994][root][INFO] - Training Epoch: 2/2, step 343/7134 completed (loss: 0.04106978327035904, acc: 0.9829867482185364)
[2025-02-13 03:32:47,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:47,400][root][INFO] - Training Epoch: 2/2, step 344/7134 completed (loss: 0.0121779665350914, acc: 0.9945255517959595)
[2025-02-13 03:32:47,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:47,782][root][INFO] - Training Epoch: 2/2, step 345/7134 completed (loss: 0.028038425371050835, acc: 0.9946428537368774)
[2025-02-13 03:32:47,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:48,230][root][INFO] - Training Epoch: 2/2, step 346/7134 completed (loss: 0.02786163240671158, acc: 0.9904610514640808)
[2025-02-13 03:32:48,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:48,658][root][INFO] - Training Epoch: 2/2, step 347/7134 completed (loss: 0.026649443432688713, acc: 0.9929873943328857)
[2025-02-13 03:32:48,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:49,111][root][INFO] - Training Epoch: 2/2, step 348/7134 completed (loss: 0.011603281833231449, acc: 0.998643159866333)
[2025-02-13 03:32:49,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:49,533][root][INFO] - Training Epoch: 2/2, step 349/7134 completed (loss: 0.02871258556842804, acc: 0.990604043006897)
[2025-02-13 03:32:49,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:50,032][root][INFO] - Training Epoch: 2/2, step 350/7134 completed (loss: 0.04411445930600166, acc: 0.9891696572303772)
[2025-02-13 03:32:50,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:50,503][root][INFO] - Training Epoch: 2/2, step 351/7134 completed (loss: 0.03390469774603844, acc: 0.9949109554290771)
[2025-02-13 03:32:50,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:50,927][root][INFO] - Training Epoch: 2/2, step 352/7134 completed (loss: 0.02054019458591938, acc: 0.9945651888847351)
[2025-02-13 03:32:51,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:51,391][root][INFO] - Training Epoch: 2/2, step 353/7134 completed (loss: 0.030594991520047188, acc: 0.9906914830207825)
[2025-02-13 03:32:51,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:51,846][root][INFO] - Training Epoch: 2/2, step 354/7134 completed (loss: 0.017502550035715103, acc: 0.9965075850486755)
[2025-02-13 03:32:51,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:52,271][root][INFO] - Training Epoch: 2/2, step 355/7134 completed (loss: 0.05765102803707123, acc: 0.9888268113136292)
[2025-02-13 03:32:52,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:52,734][root][INFO] - Training Epoch: 2/2, step 356/7134 completed (loss: 0.020997168496251106, acc: 0.9920634627342224)
[2025-02-13 03:32:52,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:53,223][root][INFO] - Training Epoch: 2/2, step 357/7134 completed (loss: 0.019498025998473167, acc: 0.9936386942863464)
[2025-02-13 03:32:53,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:53,658][root][INFO] - Training Epoch: 2/2, step 358/7134 completed (loss: 0.029090436175465584, acc: 0.9890859723091125)
[2025-02-13 03:32:53,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:54,116][root][INFO] - Training Epoch: 2/2, step 359/7134 completed (loss: 0.017218081280589104, acc: 0.9932432174682617)
[2025-02-13 03:32:54,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:54,536][root][INFO] - Training Epoch: 2/2, step 360/7134 completed (loss: 0.02486223727464676, acc: 0.9939758777618408)
[2025-02-13 03:32:54,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:54,963][root][INFO] - Training Epoch: 2/2, step 361/7134 completed (loss: 0.03557400405406952, acc: 0.9897210001945496)
[2025-02-13 03:32:55,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:55,377][root][INFO] - Training Epoch: 2/2, step 362/7134 completed (loss: 0.04094746708869934, acc: 0.9880239367485046)
[2025-02-13 03:32:55,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:55,839][root][INFO] - Training Epoch: 2/2, step 363/7134 completed (loss: 0.04334339499473572, acc: 0.9895833134651184)
[2025-02-13 03:32:55,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:56,230][root][INFO] - Training Epoch: 2/2, step 364/7134 completed (loss: 0.020208686590194702, acc: 0.9953271150588989)
[2025-02-13 03:32:56,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:56,690][root][INFO] - Training Epoch: 2/2, step 365/7134 completed (loss: 0.025027792900800705, acc: 0.9917355179786682)
[2025-02-13 03:32:56,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:57,120][root][INFO] - Training Epoch: 2/2, step 366/7134 completed (loss: 0.03317423164844513, acc: 0.9874607920646667)
[2025-02-13 03:32:57,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:57,576][root][INFO] - Training Epoch: 2/2, step 367/7134 completed (loss: 0.02891821227967739, acc: 0.9912499785423279)
[2025-02-13 03:32:57,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:58,005][root][INFO] - Training Epoch: 2/2, step 368/7134 completed (loss: 0.013460952788591385, acc: 0.9955423474311829)
[2025-02-13 03:32:58,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:58,412][root][INFO] - Training Epoch: 2/2, step 369/7134 completed (loss: 0.02352137304842472, acc: 0.9938744306564331)
[2025-02-13 03:32:58,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:58,846][root][INFO] - Training Epoch: 2/2, step 370/7134 completed (loss: 0.04992973431944847, acc: 0.9868995547294617)
[2025-02-13 03:32:58,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:59,267][root][INFO] - Training Epoch: 2/2, step 371/7134 completed (loss: 0.025832396000623703, acc: 0.9909774661064148)
[2025-02-13 03:32:59,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:59,693][root][INFO] - Training Epoch: 2/2, step 372/7134 completed (loss: 0.031210042536258698, acc: 0.9910846948623657)
[2025-02-13 03:32:59,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:00,117][root][INFO] - Training Epoch: 2/2, step 373/7134 completed (loss: 0.017328551039099693, acc: 0.994020938873291)
[2025-02-13 03:33:00,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:00,558][root][INFO] - Training Epoch: 2/2, step 374/7134 completed (loss: 0.031848255544900894, acc: 0.9903537034988403)
[2025-02-13 03:33:00,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:00,965][root][INFO] - Training Epoch: 2/2, step 375/7134 completed (loss: 0.04561690241098404, acc: 0.9906291961669922)
[2025-02-13 03:33:01,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:01,419][root][INFO] - Training Epoch: 2/2, step 376/7134 completed (loss: 0.042128901928663254, acc: 0.9862068891525269)
[2025-02-13 03:33:01,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:01,865][root][INFO] - Training Epoch: 2/2, step 377/7134 completed (loss: 0.03155864030122757, acc: 0.9876203536987305)
[2025-02-13 03:33:01,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:02,270][root][INFO] - Training Epoch: 2/2, step 378/7134 completed (loss: 0.02424785867333412, acc: 0.9904631972312927)
[2025-02-13 03:33:02,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:02,716][root][INFO] - Training Epoch: 2/2, step 379/7134 completed (loss: 0.011452523060142994, acc: 0.9962311387062073)
[2025-02-13 03:33:02,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:03,149][root][INFO] - Training Epoch: 2/2, step 380/7134 completed (loss: 0.05053617060184479, acc: 0.9829171895980835)
[2025-02-13 03:33:03,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:03,603][root][INFO] - Training Epoch: 2/2, step 381/7134 completed (loss: 0.04388302564620972, acc: 0.9922480583190918)
[2025-02-13 03:33:03,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:04,045][root][INFO] - Training Epoch: 2/2, step 382/7134 completed (loss: 0.022612279281020164, acc: 0.9922978281974792)
[2025-02-13 03:33:04,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:04,466][root][INFO] - Training Epoch: 2/2, step 383/7134 completed (loss: 0.03724776580929756, acc: 0.9905481934547424)
[2025-02-13 03:33:04,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:04,873][root][INFO] - Training Epoch: 2/2, step 384/7134 completed (loss: 0.02239326946437359, acc: 0.9959946870803833)
[2025-02-13 03:33:04,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:05,284][root][INFO] - Training Epoch: 2/2, step 385/7134 completed (loss: 0.024555891752243042, acc: 0.9952903985977173)
[2025-02-13 03:33:05,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:05,612][root][INFO] - Training Epoch: 2/2, step 386/7134 completed (loss: 0.013717032968997955, acc: 0.9959100484848022)
[2025-02-13 03:33:05,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:06,069][root][INFO] - Training Epoch: 2/2, step 387/7134 completed (loss: 0.033570967614650726, acc: 0.9854227304458618)
[2025-02-13 03:33:06,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:06,521][root][INFO] - Training Epoch: 2/2, step 388/7134 completed (loss: 0.022932270541787148, acc: 0.9941314458847046)
[2025-02-13 03:33:06,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:06,994][root][INFO] - Training Epoch: 2/2, step 389/7134 completed (loss: 0.05789143964648247, acc: 0.9820936918258667)
[2025-02-13 03:33:07,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:07,390][root][INFO] - Training Epoch: 2/2, step 390/7134 completed (loss: 0.025596624240279198, acc: 0.9967637658119202)
[2025-02-13 03:33:07,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:07,796][root][INFO] - Training Epoch: 2/2, step 391/7134 completed (loss: 0.03546725958585739, acc: 0.988095223903656)
[2025-02-13 03:33:07,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:08,243][root][INFO] - Training Epoch: 2/2, step 392/7134 completed (loss: 0.034190740436315536, acc: 0.9866666793823242)
[2025-02-13 03:33:08,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:08,656][root][INFO] - Training Epoch: 2/2, step 393/7134 completed (loss: 0.026768827810883522, acc: 0.9861111044883728)
[2025-02-13 03:33:08,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:09,051][root][INFO] - Training Epoch: 2/2, step 394/7134 completed (loss: 0.029209483414888382, acc: 0.9902724027633667)
[2025-02-13 03:33:09,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:09,460][root][INFO] - Training Epoch: 2/2, step 395/7134 completed (loss: 0.06787849217653275, acc: 0.9876543283462524)
[2025-02-13 03:33:09,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:09,871][root][INFO] - Training Epoch: 2/2, step 396/7134 completed (loss: 0.016015473753213882, acc: 0.9978494644165039)
[2025-02-13 03:33:10,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:10,333][root][INFO] - Training Epoch: 2/2, step 397/7134 completed (loss: 0.06396616250276566, acc: 0.9907407164573669)
[2025-02-13 03:33:10,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:10,780][root][INFO] - Training Epoch: 2/2, step 398/7134 completed (loss: 0.028569430112838745, acc: 0.9935566782951355)
[2025-02-13 03:33:10,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:11,181][root][INFO] - Training Epoch: 2/2, step 399/7134 completed (loss: 0.05915694683790207, acc: 0.9873217344284058)
[2025-02-13 03:33:11,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:11,638][root][INFO] - Training Epoch: 2/2, step 400/7134 completed (loss: 0.0946555957198143, acc: 0.9746268391609192)
[2025-02-13 03:33:11,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:12,101][root][INFO] - Training Epoch: 2/2, step 401/7134 completed (loss: 0.06245962902903557, acc: 0.9840686321258545)
[2025-02-13 03:33:12,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:12,552][root][INFO] - Training Epoch: 2/2, step 402/7134 completed (loss: 0.024075187742710114, acc: 0.9949302673339844)
[2025-02-13 03:33:12,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:13,044][root][INFO] - Training Epoch: 2/2, step 403/7134 completed (loss: 0.031256258487701416, acc: 0.9924405813217163)
[2025-02-13 03:33:13,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:13,466][root][INFO] - Training Epoch: 2/2, step 404/7134 completed (loss: 0.02885323390364647, acc: 0.9884615540504456)
[2025-02-13 03:33:13,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:13,928][root][INFO] - Training Epoch: 2/2, step 405/7134 completed (loss: 0.103118397295475, acc: 0.9797507524490356)
[2025-02-13 03:33:14,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:14,379][root][INFO] - Training Epoch: 2/2, step 406/7134 completed (loss: 0.058311574161052704, acc: 0.9898989796638489)
[2025-02-13 03:33:14,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:14,765][root][INFO] - Training Epoch: 2/2, step 407/7134 completed (loss: 0.12536142766475677, acc: 0.9711864590644836)
[2025-02-13 03:33:14,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:15,180][root][INFO] - Training Epoch: 2/2, step 408/7134 completed (loss: 0.10125106573104858, acc: 0.9766803979873657)
[2025-02-13 03:33:15,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:15,633][root][INFO] - Training Epoch: 2/2, step 409/7134 completed (loss: 0.052455734461545944, acc: 0.9823899269104004)
[2025-02-13 03:33:15,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:16,116][root][INFO] - Training Epoch: 2/2, step 410/7134 completed (loss: 0.06409304589033127, acc: 0.9807121753692627)
[2025-02-13 03:33:16,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:16,583][root][INFO] - Training Epoch: 2/2, step 411/7134 completed (loss: 0.04326144978404045, acc: 0.9896432757377625)
[2025-02-13 03:33:16,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:17,047][root][INFO] - Training Epoch: 2/2, step 412/7134 completed (loss: 0.033951614052057266, acc: 0.991990864276886)
[2025-02-13 03:33:17,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:17,473][root][INFO] - Training Epoch: 2/2, step 413/7134 completed (loss: 0.04181920737028122, acc: 0.9866488575935364)
[2025-02-13 03:33:17,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:17,889][root][INFO] - Training Epoch: 2/2, step 414/7134 completed (loss: 0.035535670816898346, acc: 0.9866468906402588)
[2025-02-13 03:33:18,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:18,344][root][INFO] - Training Epoch: 2/2, step 415/7134 completed (loss: 0.03882945328950882, acc: 0.9942129850387573)
[2025-02-13 03:33:18,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:18,825][root][INFO] - Training Epoch: 2/2, step 416/7134 completed (loss: 0.025468388572335243, acc: 0.9929161667823792)
[2025-02-13 03:33:18,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:19,272][root][INFO] - Training Epoch: 2/2, step 417/7134 completed (loss: 0.08752515912055969, acc: 0.979619562625885)
[2025-02-13 03:33:19,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:19,744][root][INFO] - Training Epoch: 2/2, step 418/7134 completed (loss: 0.03172691911458969, acc: 0.9862227439880371)
[2025-02-13 03:33:19,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:20,208][root][INFO] - Training Epoch: 2/2, step 419/7134 completed (loss: 0.03854825720191002, acc: 0.9888337254524231)
[2025-02-13 03:33:20,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:20,614][root][INFO] - Training Epoch: 2/2, step 420/7134 completed (loss: 0.029130296781659126, acc: 0.9881556630134583)
[2025-02-13 03:33:20,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:21,041][root][INFO] - Training Epoch: 2/2, step 421/7134 completed (loss: 0.029759222641587257, acc: 0.987261176109314)
[2025-02-13 03:33:21,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:21,475][root][INFO] - Training Epoch: 2/2, step 422/7134 completed (loss: 0.037823379039764404, acc: 0.9852941036224365)
[2025-02-13 03:33:21,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:21,934][root][INFO] - Training Epoch: 2/2, step 423/7134 completed (loss: 0.015764307230710983, acc: 0.9942857027053833)
[2025-02-13 03:33:22,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:22,344][root][INFO] - Training Epoch: 2/2, step 424/7134 completed (loss: 0.03188243508338928, acc: 0.9857594966888428)
[2025-02-13 03:33:22,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:22,753][root][INFO] - Training Epoch: 2/2, step 425/7134 completed (loss: 0.012950550764799118, acc: 0.9970930218696594)
[2025-02-13 03:33:22,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:23,177][root][INFO] - Training Epoch: 2/2, step 426/7134 completed (loss: 0.013262358494102955, acc: 0.9972144961357117)
[2025-02-13 03:33:23,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:23,595][root][INFO] - Training Epoch: 2/2, step 427/7134 completed (loss: 0.019311940297484398, acc: 0.9939302206039429)
[2025-02-13 03:33:23,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:24,029][root][INFO] - Training Epoch: 2/2, step 428/7134 completed (loss: 0.019579820334911346, acc: 0.9969879388809204)
[2025-02-13 03:33:24,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:24,453][root][INFO] - Training Epoch: 2/2, step 429/7134 completed (loss: 0.03707974776625633, acc: 0.9872813820838928)
[2025-02-13 03:33:24,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:24,879][root][INFO] - Training Epoch: 2/2, step 430/7134 completed (loss: 0.020801037549972534, acc: 0.9928264021873474)
[2025-02-13 03:33:25,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:25,288][root][INFO] - Training Epoch: 2/2, step 431/7134 completed (loss: 0.024072496220469475, acc: 0.9942362904548645)
[2025-02-13 03:33:25,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:25,685][root][INFO] - Training Epoch: 2/2, step 432/7134 completed (loss: 0.013960449956357479, acc: 0.9945155382156372)
[2025-02-13 03:33:25,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:26,143][root][INFO] - Training Epoch: 2/2, step 433/7134 completed (loss: 0.026314008980989456, acc: 0.991847813129425)
[2025-02-13 03:33:26,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:26,575][root][INFO] - Training Epoch: 2/2, step 434/7134 completed (loss: 0.02017050050199032, acc: 0.9912152290344238)
[2025-02-13 03:33:26,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:27,021][root][INFO] - Training Epoch: 2/2, step 435/7134 completed (loss: 0.024848945438861847, acc: 0.9943262338638306)
[2025-02-13 03:33:27,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:27,430][root][INFO] - Training Epoch: 2/2, step 436/7134 completed (loss: 0.032963722944259644, acc: 0.9921259880065918)
[2025-02-13 03:33:27,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:27,835][root][INFO] - Training Epoch: 2/2, step 437/7134 completed (loss: 0.04816192016005516, acc: 0.9893993139266968)
[2025-02-13 03:33:27,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:28,231][root][INFO] - Training Epoch: 2/2, step 438/7134 completed (loss: 0.027617117390036583, acc: 0.9865384697914124)
[2025-02-13 03:33:28,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:28,687][root][INFO] - Training Epoch: 2/2, step 439/7134 completed (loss: 0.033142101019620895, acc: 0.9921466112136841)
[2025-02-13 03:33:28,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:29,083][root][INFO] - Training Epoch: 2/2, step 440/7134 completed (loss: 0.025334244593977928, acc: 0.9878683090209961)
[2025-02-13 03:33:29,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:29,508][root][INFO] - Training Epoch: 2/2, step 441/7134 completed (loss: 0.010576550848782063, acc: 0.995502233505249)
[2025-02-13 03:33:29,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:29,928][root][INFO] - Training Epoch: 2/2, step 442/7134 completed (loss: 0.014087740331888199, acc: 0.9981343150138855)
[2025-02-13 03:33:30,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:30,373][root][INFO] - Training Epoch: 2/2, step 443/7134 completed (loss: 0.0035977966617792845, acc: 1.0)
[2025-02-13 03:33:30,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:30,769][root][INFO] - Training Epoch: 2/2, step 444/7134 completed (loss: 0.0077897352166473866, acc: 0.9983333349227905)
[2025-02-13 03:33:30,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:31,199][root][INFO] - Training Epoch: 2/2, step 445/7134 completed (loss: 0.029776273295283318, acc: 0.991253674030304)
[2025-02-13 03:33:31,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:31,641][root][INFO] - Training Epoch: 2/2, step 446/7134 completed (loss: 0.00750237051397562, acc: 0.9958217144012451)
[2025-02-13 03:33:31,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:32,053][root][INFO] - Training Epoch: 2/2, step 447/7134 completed (loss: 0.016238607466220856, acc: 0.9958391189575195)
[2025-02-13 03:33:32,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:32,521][root][INFO] - Training Epoch: 2/2, step 448/7134 completed (loss: 0.05638102442026138, acc: 0.9846547245979309)
[2025-02-13 03:33:32,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:32,988][root][INFO] - Training Epoch: 2/2, step 449/7134 completed (loss: 0.11975826323032379, acc: 0.9751309156417847)
[2025-02-13 03:33:33,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:33,461][root][INFO] - Training Epoch: 2/2, step 450/7134 completed (loss: 0.051115456968545914, acc: 0.9883227348327637)
[2025-02-13 03:33:33,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:33,930][root][INFO] - Training Epoch: 2/2, step 451/7134 completed (loss: 0.08891734480857849, acc: 0.9762187600135803)
[2025-02-13 03:33:34,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:34,394][root][INFO] - Training Epoch: 2/2, step 452/7134 completed (loss: 0.08167976140975952, acc: 0.9684931635856628)
[2025-02-13 03:33:34,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:34,772][root][INFO] - Training Epoch: 2/2, step 453/7134 completed (loss: 0.05729798227548599, acc: 0.9816513657569885)
[2025-02-13 03:33:34,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:35,243][root][INFO] - Training Epoch: 2/2, step 454/7134 completed (loss: 0.07738173753023148, acc: 0.9784714579582214)
[2025-02-13 03:33:35,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:35,700][root][INFO] - Training Epoch: 2/2, step 455/7134 completed (loss: 0.09485767781734467, acc: 0.9764705896377563)
[2025-02-13 03:33:35,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:36,142][root][INFO] - Training Epoch: 2/2, step 456/7134 completed (loss: 0.019842540845274925, acc: 0.9929178357124329)
[2025-02-13 03:33:36,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:36,593][root][INFO] - Training Epoch: 2/2, step 457/7134 completed (loss: 0.028804145753383636, acc: 0.994966447353363)
[2025-02-13 03:33:36,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:36,990][root][INFO] - Training Epoch: 2/2, step 458/7134 completed (loss: 0.027014102786779404, acc: 0.9940357804298401)
[2025-02-13 03:33:37,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:37,382][root][INFO] - Training Epoch: 2/2, step 459/7134 completed (loss: 0.07925298810005188, acc: 0.9777777791023254)
[2025-02-13 03:33:37,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:37,786][root][INFO] - Training Epoch: 2/2, step 460/7134 completed (loss: 0.07329651713371277, acc: 0.9842519760131836)
[2025-02-13 03:33:37,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:38,253][root][INFO] - Training Epoch: 2/2, step 461/7134 completed (loss: 0.036824412643909454, acc: 0.9873772859573364)
[2025-02-13 03:33:38,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:38,765][root][INFO] - Training Epoch: 2/2, step 462/7134 completed (loss: 0.0759044960141182, acc: 0.9778037667274475)
[2025-02-13 03:33:38,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:39,241][root][INFO] - Training Epoch: 2/2, step 463/7134 completed (loss: 0.0644615888595581, acc: 0.9836888313293457)
[2025-02-13 03:33:39,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:39,697][root][INFO] - Training Epoch: 2/2, step 464/7134 completed (loss: 0.04905308410525322, acc: 0.9830028414726257)
[2025-02-13 03:33:39,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:40,110][root][INFO] - Training Epoch: 2/2, step 465/7134 completed (loss: 0.07931706309318542, acc: 0.9791122674942017)
[2025-02-13 03:33:40,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:40,566][root][INFO] - Training Epoch: 2/2, step 466/7134 completed (loss: 0.10082109272480011, acc: 0.9729729890823364)
[2025-02-13 03:33:40,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:41,009][root][INFO] - Training Epoch: 2/2, step 467/7134 completed (loss: 0.09457121789455414, acc: 0.9716874361038208)
[2025-02-13 03:33:41,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:41,388][root][INFO] - Training Epoch: 2/2, step 468/7134 completed (loss: 0.16600826382637024, acc: 0.9628610610961914)
[2025-02-13 03:33:41,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:41,825][root][INFO] - Training Epoch: 2/2, step 469/7134 completed (loss: 0.08317822217941284, acc: 0.9769452214241028)
[2025-02-13 03:33:41,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:42,248][root][INFO] - Training Epoch: 2/2, step 470/7134 completed (loss: 0.08923619985580444, acc: 0.9795918464660645)
[2025-02-13 03:33:42,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:42,688][root][INFO] - Training Epoch: 2/2, step 471/7134 completed (loss: 0.07380697131156921, acc: 0.9798816442489624)
[2025-02-13 03:33:42,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:43,115][root][INFO] - Training Epoch: 2/2, step 472/7134 completed (loss: 0.09103626012802124, acc: 0.9780058860778809)
[2025-02-13 03:33:43,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:43,532][root][INFO] - Training Epoch: 2/2, step 473/7134 completed (loss: 0.04765850678086281, acc: 0.9830247163772583)
[2025-02-13 03:33:43,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:43,981][root][INFO] - Training Epoch: 2/2, step 474/7134 completed (loss: 0.07864192128181458, acc: 0.9815546870231628)
[2025-02-13 03:33:44,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:44,394][root][INFO] - Training Epoch: 2/2, step 475/7134 completed (loss: 0.11260538548231125, acc: 0.9735293984413147)
[2025-02-13 03:33:44,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:44,821][root][INFO] - Training Epoch: 2/2, step 476/7134 completed (loss: 0.07530330121517181, acc: 0.9699140191078186)
[2025-02-13 03:33:44,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:45,303][root][INFO] - Training Epoch: 2/2, step 477/7134 completed (loss: 0.05668538063764572, acc: 0.9828850626945496)
[2025-02-13 03:33:45,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:45,710][root][INFO] - Training Epoch: 2/2, step 478/7134 completed (loss: 0.12124019116163254, acc: 0.957716703414917)
[2025-02-13 03:33:45,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:46,116][root][INFO] - Training Epoch: 2/2, step 479/7134 completed (loss: 0.03337594121694565, acc: 0.9885222315788269)
[2025-02-13 03:33:46,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:46,531][root][INFO] - Training Epoch: 2/2, step 480/7134 completed (loss: 0.025321893393993378, acc: 0.9941605925559998)
[2025-02-13 03:33:46,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:46,979][root][INFO] - Training Epoch: 2/2, step 481/7134 completed (loss: 0.04322640970349312, acc: 0.9856770634651184)
[2025-02-13 03:33:47,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:47,396][root][INFO] - Training Epoch: 2/2, step 482/7134 completed (loss: 0.018789811059832573, acc: 0.9928571581840515)
[2025-02-13 03:33:47,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:47,851][root][INFO] - Training Epoch: 2/2, step 483/7134 completed (loss: 0.011747286655008793, acc: 0.9975429773330688)
[2025-02-13 03:33:47,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:48,282][root][INFO] - Training Epoch: 2/2, step 484/7134 completed (loss: 0.06815481185913086, acc: 0.9852941036224365)
[2025-02-13 03:33:48,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:48,705][root][INFO] - Training Epoch: 2/2, step 485/7134 completed (loss: 0.08262410759925842, acc: 0.9848713874816895)
[2025-02-13 03:33:48,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:49,133][root][INFO] - Training Epoch: 2/2, step 486/7134 completed (loss: 0.04371137171983719, acc: 0.9901315569877625)
[2025-02-13 03:33:49,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:49,552][root][INFO] - Training Epoch: 2/2, step 487/7134 completed (loss: 0.048156049102544785, acc: 0.988252580165863)
[2025-02-13 03:33:49,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:49,953][root][INFO] - Training Epoch: 2/2, step 488/7134 completed (loss: 0.09776278585195541, acc: 0.9728260636329651)
[2025-02-13 03:33:50,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:50,371][root][INFO] - Training Epoch: 2/2, step 489/7134 completed (loss: 0.08221370726823807, acc: 0.978151261806488)
[2025-02-13 03:33:50,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:50,797][root][INFO] - Training Epoch: 2/2, step 490/7134 completed (loss: 0.04303315281867981, acc: 0.9959183931350708)
[2025-02-13 03:33:50,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:51,201][root][INFO] - Training Epoch: 2/2, step 491/7134 completed (loss: 0.03915630280971527, acc: 0.9901315569877625)
[2025-02-13 03:33:51,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:51,617][root][INFO] - Training Epoch: 2/2, step 492/7134 completed (loss: 0.009700891561806202, acc: 0.998487114906311)
[2025-02-13 03:33:51,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:52,063][root][INFO] - Training Epoch: 2/2, step 493/7134 completed (loss: 0.0591462142765522, acc: 0.9785100221633911)
[2025-02-13 03:33:52,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:52,482][root][INFO] - Training Epoch: 2/2, step 494/7134 completed (loss: 0.02152843028306961, acc: 0.9921875)
[2025-02-13 03:33:52,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:52,889][root][INFO] - Training Epoch: 2/2, step 495/7134 completed (loss: 0.029485376551747322, acc: 0.9882352948188782)
[2025-02-13 03:33:52,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:53,220][root][INFO] - Training Epoch: 2/2, step 496/7134 completed (loss: 0.048671040683984756, acc: 0.989159882068634)
[2025-02-13 03:33:53,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:53,624][root][INFO] - Training Epoch: 2/2, step 497/7134 completed (loss: 0.06759557873010635, acc: 0.9806201457977295)
[2025-02-13 03:33:53,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:54,045][root][INFO] - Training Epoch: 2/2, step 498/7134 completed (loss: 0.028349079191684723, acc: 0.9935897588729858)
[2025-02-13 03:33:54,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:54,509][root][INFO] - Training Epoch: 2/2, step 499/7134 completed (loss: 0.014953713864088058, acc: 0.9958275556564331)
[2025-02-13 03:33:54,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:54,936][root][INFO] - Training Epoch: 2/2, step 500/7134 completed (loss: 0.03187984600663185, acc: 0.9919999837875366)
[2025-02-13 03:33:55,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:55,394][root][INFO] - Training Epoch: 2/2, step 501/7134 completed (loss: 0.014879134483635426, acc: 0.9958449006080627)
[2025-02-13 03:33:55,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:55,786][root][INFO] - Training Epoch: 2/2, step 502/7134 completed (loss: 0.016780924052000046, acc: 0.9954128265380859)
[2025-02-13 03:33:55,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:56,188][root][INFO] - Training Epoch: 2/2, step 503/7134 completed (loss: 0.03825782611966133, acc: 0.9865771532058716)
[2025-02-13 03:33:56,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:56,636][root][INFO] - Training Epoch: 2/2, step 504/7134 completed (loss: 0.017574846744537354, acc: 0.994413435459137)
[2025-02-13 03:33:56,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:57,051][root][INFO] - Training Epoch: 2/2, step 505/7134 completed (loss: 0.01614905707538128, acc: 0.9947643876075745)
[2025-02-13 03:33:57,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:57,513][root][INFO] - Training Epoch: 2/2, step 506/7134 completed (loss: 0.028114868327975273, acc: 0.9937810897827148)
[2025-02-13 03:33:57,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:57,975][root][INFO] - Training Epoch: 2/2, step 507/7134 completed (loss: 0.046176727861166, acc: 0.99210524559021)
[2025-02-13 03:33:58,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:58,418][root][INFO] - Training Epoch: 2/2, step 508/7134 completed (loss: 0.03963407129049301, acc: 0.9846368432044983)
[2025-02-13 03:33:58,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:58,879][root][INFO] - Training Epoch: 2/2, step 509/7134 completed (loss: 0.04165024310350418, acc: 0.9829984307289124)
[2025-02-13 03:33:59,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:59,259][root][INFO] - Training Epoch: 2/2, step 510/7134 completed (loss: 0.0981653556227684, acc: 0.9792099595069885)
[2025-02-13 03:33:59,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:59,658][root][INFO] - Training Epoch: 2/2, step 511/7134 completed (loss: 0.056001391261816025, acc: 0.9853300452232361)
[2025-02-13 03:33:59,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:00,079][root][INFO] - Training Epoch: 2/2, step 512/7134 completed (loss: 0.08038253337144852, acc: 0.9859719276428223)
[2025-02-13 03:34:00,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:00,478][root][INFO] - Training Epoch: 2/2, step 513/7134 completed (loss: 0.07588287442922592, acc: 0.9810671210289001)
[2025-02-13 03:34:00,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:00,791][root][INFO] - Training Epoch: 2/2, step 514/7134 completed (loss: 0.0797024518251419, acc: 0.9748743772506714)
[2025-02-13 03:34:00,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:01,265][root][INFO] - Training Epoch: 2/2, step 515/7134 completed (loss: 0.029312096536159515, acc: 0.9915134310722351)
[2025-02-13 03:34:01,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:01,597][root][INFO] - Training Epoch: 2/2, step 516/7134 completed (loss: 0.03572140634059906, acc: 0.9931350350379944)
[2025-02-13 03:34:01,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:02,020][root][INFO] - Training Epoch: 2/2, step 517/7134 completed (loss: 0.09153177589178085, acc: 0.9767025113105774)
[2025-02-13 03:34:02,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:02,472][root][INFO] - Training Epoch: 2/2, step 518/7134 completed (loss: 0.040072306990623474, acc: 0.9925037622451782)
[2025-02-13 03:34:02,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:02,891][root][INFO] - Training Epoch: 2/2, step 519/7134 completed (loss: 0.03218566253781319, acc: 0.9896907210350037)
[2025-02-13 03:34:03,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:03,310][root][INFO] - Training Epoch: 2/2, step 520/7134 completed (loss: 0.01640027016401291, acc: 0.9952531456947327)
[2025-02-13 03:34:03,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:03,742][root][INFO] - Training Epoch: 2/2, step 521/7134 completed (loss: 0.05391871556639671, acc: 0.9893190860748291)
[2025-02-13 03:34:03,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:04,174][root][INFO] - Training Epoch: 2/2, step 522/7134 completed (loss: 0.029946910217404366, acc: 0.9932975769042969)
[2025-02-13 03:34:04,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:04,635][root][INFO] - Training Epoch: 2/2, step 523/7134 completed (loss: 0.017750797793269157, acc: 0.9956140518188477)
[2025-02-13 03:34:04,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:05,075][root][INFO] - Training Epoch: 2/2, step 524/7134 completed (loss: 0.0413946732878685, acc: 0.9892473220825195)
[2025-02-13 03:34:05,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:05,489][root][INFO] - Training Epoch: 2/2, step 525/7134 completed (loss: 0.07242047786712646, acc: 0.9814528822898865)
[2025-02-13 03:34:05,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:05,941][root][INFO] - Training Epoch: 2/2, step 526/7134 completed (loss: 0.03930612653493881, acc: 0.9890410900115967)
[2025-02-13 03:34:06,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:06,353][root][INFO] - Training Epoch: 2/2, step 527/7134 completed (loss: 0.040365103632211685, acc: 0.9925373196601868)
[2025-02-13 03:34:06,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:06,806][root][INFO] - Training Epoch: 2/2, step 528/7134 completed (loss: 0.004292580299079418, acc: 1.0)
[2025-02-13 03:34:06,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:07,269][root][INFO] - Training Epoch: 2/2, step 529/7134 completed (loss: 0.05676183104515076, acc: 0.9873015880584717)
[2025-02-13 03:34:07,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:07,688][root][INFO] - Training Epoch: 2/2, step 530/7134 completed (loss: 0.026645081117749214, acc: 0.9931389093399048)
[2025-02-13 03:34:07,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:08,044][root][INFO] - Training Epoch: 2/2, step 531/7134 completed (loss: 0.08703017234802246, acc: 0.9829059839248657)
[2025-02-13 03:34:08,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:08,460][root][INFO] - Training Epoch: 2/2, step 532/7134 completed (loss: 0.01840978115797043, acc: 0.9958419799804688)
[2025-02-13 03:34:08,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:08,881][root][INFO] - Training Epoch: 2/2, step 533/7134 completed (loss: 0.08162444084882736, acc: 0.9793814420700073)
[2025-02-13 03:34:09,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:09,331][root][INFO] - Training Epoch: 2/2, step 534/7134 completed (loss: 0.018826641142368317, acc: 0.9958791136741638)
[2025-02-13 03:34:09,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:09,794][root][INFO] - Training Epoch: 2/2, step 535/7134 completed (loss: 0.034611139446496964, acc: 0.9929701089859009)
[2025-02-13 03:34:09,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:10,240][root][INFO] - Training Epoch: 2/2, step 536/7134 completed (loss: 0.02971666119992733, acc: 0.9912408590316772)
[2025-02-13 03:34:10,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:10,652][root][INFO] - Training Epoch: 2/2, step 537/7134 completed (loss: 0.03643748536705971, acc: 0.9867452383041382)
[2025-02-13 03:34:10,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:11,068][root][INFO] - Training Epoch: 2/2, step 538/7134 completed (loss: 0.03329523652791977, acc: 0.9896142482757568)
[2025-02-13 03:34:11,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:11,485][root][INFO] - Training Epoch: 2/2, step 539/7134 completed (loss: 0.03683081269264221, acc: 0.9896193742752075)
[2025-02-13 03:34:11,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:11,887][root][INFO] - Training Epoch: 2/2, step 540/7134 completed (loss: 0.027099544182419777, acc: 0.9878787994384766)
[2025-02-13 03:34:12,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:12,331][root][INFO] - Training Epoch: 2/2, step 541/7134 completed (loss: 0.05935361608862877, acc: 0.9853723645210266)
[2025-02-13 03:34:12,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:12,654][root][INFO] - Training Epoch: 2/2, step 542/7134 completed (loss: 0.07632039487361908, acc: 0.9835680723190308)
[2025-02-13 03:34:12,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:13,055][root][INFO] - Training Epoch: 2/2, step 543/7134 completed (loss: 0.08607140183448792, acc: 0.9710668921470642)
[2025-02-13 03:34:13,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:13,493][root][INFO] - Training Epoch: 2/2, step 544/7134 completed (loss: 0.06644739210605621, acc: 0.9794952869415283)
[2025-02-13 03:34:13,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:13,943][root][INFO] - Training Epoch: 2/2, step 545/7134 completed (loss: 0.04831133037805557, acc: 0.9855072498321533)
[2025-02-13 03:34:14,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:14,370][root][INFO] - Training Epoch: 2/2, step 546/7134 completed (loss: 0.11134849488735199, acc: 0.9692671298980713)
[2025-02-13 03:34:14,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:14,824][root][INFO] - Training Epoch: 2/2, step 547/7134 completed (loss: 0.0523449145257473, acc: 0.9883889555931091)
[2025-02-13 03:34:14,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:15,259][root][INFO] - Training Epoch: 2/2, step 548/7134 completed (loss: 0.031879130750894547, acc: 0.9934810996055603)
[2025-02-13 03:34:15,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:15,682][root][INFO] - Training Epoch: 2/2, step 549/7134 completed (loss: 0.06282762438058853, acc: 0.9736024737358093)
[2025-02-13 03:34:15,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:16,104][root][INFO] - Training Epoch: 2/2, step 550/7134 completed (loss: 0.05548287183046341, acc: 0.9846416115760803)
[2025-02-13 03:34:16,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:16,468][root][INFO] - Training Epoch: 2/2, step 551/7134 completed (loss: 0.04711895063519478, acc: 0.9852670431137085)
[2025-02-13 03:34:16,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:16,883][root][INFO] - Training Epoch: 2/2, step 552/7134 completed (loss: 0.044388607144355774, acc: 0.9829059839248657)
[2025-02-13 03:34:16,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:17,238][root][INFO] - Training Epoch: 2/2, step 553/7134 completed (loss: 0.042987484484910965, acc: 0.984649121761322)
[2025-02-13 03:34:17,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:17,677][root][INFO] - Training Epoch: 2/2, step 554/7134 completed (loss: 0.0749991312623024, acc: 0.980028510093689)
[2025-02-13 03:34:17,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:18,100][root][INFO] - Training Epoch: 2/2, step 555/7134 completed (loss: 0.055235959589481354, acc: 0.9810426831245422)
[2025-02-13 03:34:18,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:18,525][root][INFO] - Training Epoch: 2/2, step 556/7134 completed (loss: 0.03763619810342789, acc: 0.9900990128517151)
[2025-02-13 03:34:18,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:18,943][root][INFO] - Training Epoch: 2/2, step 557/7134 completed (loss: 0.054055992513895035, acc: 0.9830795526504517)
[2025-02-13 03:34:19,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:19,367][root][INFO] - Training Epoch: 2/2, step 558/7134 completed (loss: 0.022900806739926338, acc: 0.9943262338638306)
[2025-02-13 03:34:19,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:19,807][root][INFO] - Training Epoch: 2/2, step 559/7134 completed (loss: 0.09003561735153198, acc: 0.9672364592552185)
[2025-02-13 03:34:19,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:20,220][root][INFO] - Training Epoch: 2/2, step 560/7134 completed (loss: 0.03587809577584267, acc: 0.9912126660346985)
[2025-02-13 03:34:20,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:20,683][root][INFO] - Training Epoch: 2/2, step 561/7134 completed (loss: 0.0630553737282753, acc: 0.9795657992362976)
[2025-02-13 03:34:20,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:21,102][root][INFO] - Training Epoch: 2/2, step 562/7134 completed (loss: 0.03390049561858177, acc: 0.9854304790496826)
[2025-02-13 03:34:21,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:21,504][root][INFO] - Training Epoch: 2/2, step 563/7134 completed (loss: 0.07652287185192108, acc: 0.9762658476829529)
[2025-02-13 03:34:21,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:21,955][root][INFO] - Training Epoch: 2/2, step 564/7134 completed (loss: 0.04510171338915825, acc: 0.9876390695571899)
[2025-02-13 03:34:22,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:22,350][root][INFO] - Training Epoch: 2/2, step 565/7134 completed (loss: 0.05771384388208389, acc: 0.9765493869781494)
[2025-02-13 03:34:22,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:22,753][root][INFO] - Training Epoch: 2/2, step 566/7134 completed (loss: 0.08301568776369095, acc: 0.9744779467582703)
[2025-02-13 03:34:22,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:23,180][root][INFO] - Training Epoch: 2/2, step 567/7134 completed (loss: 0.04928358271718025, acc: 0.9901823401451111)
[2025-02-13 03:34:23,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:23,612][root][INFO] - Training Epoch: 2/2, step 568/7134 completed (loss: 0.06073050945997238, acc: 0.98562091588974)
[2025-02-13 03:34:23,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:24,030][root][INFO] - Training Epoch: 2/2, step 569/7134 completed (loss: 0.028295787051320076, acc: 0.9925925731658936)
[2025-02-13 03:34:24,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:24,506][root][INFO] - Training Epoch: 2/2, step 570/7134 completed (loss: 0.03054729476571083, acc: 0.9894242286682129)
[2025-02-13 03:34:24,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:24,955][root][INFO] - Training Epoch: 2/2, step 571/7134 completed (loss: 0.02258746325969696, acc: 0.992443323135376)
[2025-02-13 03:34:25,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:25,393][root][INFO] - Training Epoch: 2/2, step 572/7134 completed (loss: 0.02464328519999981, acc: 0.9897040128707886)
[2025-02-13 03:34:25,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:25,837][root][INFO] - Training Epoch: 2/2, step 573/7134 completed (loss: 0.013110755942761898, acc: 0.9948849081993103)
[2025-02-13 03:34:25,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:26,247][root][INFO] - Training Epoch: 2/2, step 574/7134 completed (loss: 0.027145229279994965, acc: 0.9934123754501343)
[2025-02-13 03:34:26,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:26,719][root][INFO] - Training Epoch: 2/2, step 575/7134 completed (loss: 0.02394784800708294, acc: 0.9922879338264465)
[2025-02-13 03:34:26,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:27,153][root][INFO] - Training Epoch: 2/2, step 576/7134 completed (loss: 0.024889670312404633, acc: 0.9962871074676514)
[2025-02-13 03:34:27,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:27,601][root][INFO] - Training Epoch: 2/2, step 577/7134 completed (loss: 0.027952423319220543, acc: 0.9881423115730286)
[2025-02-13 03:34:27,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:28,072][root][INFO] - Training Epoch: 2/2, step 578/7134 completed (loss: 0.0372721366584301, acc: 0.988399088382721)
[2025-02-13 03:34:28,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:28,490][root][INFO] - Training Epoch: 2/2, step 579/7134 completed (loss: 0.060059044510126114, acc: 0.9845956563949585)
[2025-02-13 03:34:28,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:28,921][root][INFO] - Training Epoch: 2/2, step 580/7134 completed (loss: 0.05847373977303505, acc: 0.9804941415786743)
[2025-02-13 03:34:29,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:29,327][root][INFO] - Training Epoch: 2/2, step 581/7134 completed (loss: 0.029294490814208984, acc: 0.991631805896759)
[2025-02-13 03:34:29,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:29,782][root][INFO] - Training Epoch: 2/2, step 582/7134 completed (loss: 0.02829386293888092, acc: 0.9892215728759766)
[2025-02-13 03:34:29,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:30,236][root][INFO] - Training Epoch: 2/2, step 583/7134 completed (loss: 0.028225457295775414, acc: 0.9909090995788574)
[2025-02-13 03:34:30,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:30,691][root][INFO] - Training Epoch: 2/2, step 584/7134 completed (loss: 0.014007709920406342, acc: 0.9964115023612976)
[2025-02-13 03:34:30,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:31,127][root][INFO] - Training Epoch: 2/2, step 585/7134 completed (loss: 0.026305517181754112, acc: 0.9939613342285156)
[2025-02-13 03:34:31,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:31,546][root][INFO] - Training Epoch: 2/2, step 586/7134 completed (loss: 0.03064758889377117, acc: 0.9895697236061096)
[2025-02-13 03:34:31,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:31,977][root][INFO] - Training Epoch: 2/2, step 587/7134 completed (loss: 0.03938274458050728, acc: 0.9898348450660706)
[2025-02-13 03:34:32,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:32,429][root][INFO] - Training Epoch: 2/2, step 588/7134 completed (loss: 0.025089148432016373, acc: 0.9937205910682678)
[2025-02-13 03:34:32,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:32,863][root][INFO] - Training Epoch: 2/2, step 589/7134 completed (loss: 0.027493562549352646, acc: 0.9897210001945496)
[2025-02-13 03:34:33,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:33,295][root][INFO] - Training Epoch: 2/2, step 590/7134 completed (loss: 0.02279851585626602, acc: 0.995488703250885)
[2025-02-13 03:34:33,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:33,737][root][INFO] - Training Epoch: 2/2, step 591/7134 completed (loss: 0.028536517173051834, acc: 0.9927361011505127)
[2025-02-13 03:34:33,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:34,162][root][INFO] - Training Epoch: 2/2, step 592/7134 completed (loss: 0.02515161596238613, acc: 0.9923858046531677)
[2025-02-13 03:34:34,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:34,588][root][INFO] - Training Epoch: 2/2, step 593/7134 completed (loss: 0.04440474137663841, acc: 0.9868766665458679)
[2025-02-13 03:34:34,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:35,039][root][INFO] - Training Epoch: 2/2, step 594/7134 completed (loss: 0.05644404888153076, acc: 0.9874213933944702)
[2025-02-13 03:34:35,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:35,515][root][INFO] - Training Epoch: 2/2, step 595/7134 completed (loss: 0.03242028132081032, acc: 0.9898862242698669)
[2025-02-13 03:34:35,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:35,945][root][INFO] - Training Epoch: 2/2, step 596/7134 completed (loss: 0.03731195628643036, acc: 0.9909793734550476)
[2025-02-13 03:34:36,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:36,420][root][INFO] - Training Epoch: 2/2, step 597/7134 completed (loss: 0.12228653579950333, acc: 0.9710144996643066)
[2025-02-13 03:34:36,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:36,868][root][INFO] - Training Epoch: 2/2, step 598/7134 completed (loss: 0.05887996032834053, acc: 0.9824086427688599)
[2025-02-13 03:34:37,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:37,300][root][INFO] - Training Epoch: 2/2, step 599/7134 completed (loss: 0.03071601316332817, acc: 0.9874686598777771)
[2025-02-13 03:34:37,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:37,762][root][INFO] - Training Epoch: 2/2, step 600/7134 completed (loss: 0.03964259847998619, acc: 0.9927797913551331)
[2025-02-13 03:34:37,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:38,226][root][INFO] - Training Epoch: 2/2, step 601/7134 completed (loss: 0.04044584557414055, acc: 0.9861687421798706)
[2025-02-13 03:34:38,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:38,675][root][INFO] - Training Epoch: 2/2, step 602/7134 completed (loss: 0.042732059955596924, acc: 0.9907192587852478)
[2025-02-13 03:34:38,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:39,121][root][INFO] - Training Epoch: 2/2, step 603/7134 completed (loss: 0.03693193942308426, acc: 0.9884318709373474)
[2025-02-13 03:34:39,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:39,510][root][INFO] - Training Epoch: 2/2, step 604/7134 completed (loss: 0.041293978691101074, acc: 0.985358715057373)
[2025-02-13 03:34:39,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:39,945][root][INFO] - Training Epoch: 2/2, step 605/7134 completed (loss: 0.02264806069433689, acc: 0.9948186278343201)
[2025-02-13 03:34:40,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:40,395][root][INFO] - Training Epoch: 2/2, step 606/7134 completed (loss: 0.09797104448080063, acc: 0.9802225232124329)
[2025-02-13 03:34:40,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:40,844][root][INFO] - Training Epoch: 2/2, step 607/7134 completed (loss: 0.03211596980690956, acc: 0.9906542301177979)
[2025-02-13 03:34:40,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:41,305][root][INFO] - Training Epoch: 2/2, step 608/7134 completed (loss: 0.027997374534606934, acc: 0.9922279715538025)
[2025-02-13 03:34:41,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:41,764][root][INFO] - Training Epoch: 2/2, step 609/7134 completed (loss: 0.039140112698078156, acc: 0.9871194362640381)
[2025-02-13 03:34:41,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:42,221][root][INFO] - Training Epoch: 2/2, step 610/7134 completed (loss: 0.05469126254320145, acc: 0.9810366630554199)
[2025-02-13 03:34:42,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:42,670][root][INFO] - Training Epoch: 2/2, step 611/7134 completed (loss: 0.023274021223187447, acc: 0.9939024448394775)
[2025-02-13 03:34:42,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:43,085][root][INFO] - Training Epoch: 2/2, step 612/7134 completed (loss: 0.025831911712884903, acc: 0.9907651543617249)
[2025-02-13 03:34:43,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:43,544][root][INFO] - Training Epoch: 2/2, step 613/7134 completed (loss: 0.03359520062804222, acc: 0.9882766604423523)
[2025-02-13 03:34:43,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:43,994][root][INFO] - Training Epoch: 2/2, step 614/7134 completed (loss: 0.022413037717342377, acc: 0.9909399747848511)
[2025-02-13 03:34:44,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:44,441][root][INFO] - Training Epoch: 2/2, step 615/7134 completed (loss: 0.010045534931123257, acc: 0.9975460171699524)
[2025-02-13 03:34:44,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:44,907][root][INFO] - Training Epoch: 2/2, step 616/7134 completed (loss: 0.03043098747730255, acc: 0.9920454621315002)
[2025-02-13 03:34:45,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:45,364][root][INFO] - Training Epoch: 2/2, step 617/7134 completed (loss: 0.03525456041097641, acc: 0.9897494316101074)
[2025-02-13 03:34:45,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:45,805][root][INFO] - Training Epoch: 2/2, step 618/7134 completed (loss: 0.0239097960293293, acc: 0.9933155179023743)
[2025-02-13 03:34:45,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:46,157][root][INFO] - Training Epoch: 2/2, step 619/7134 completed (loss: 0.03780030459165573, acc: 0.9919028282165527)
[2025-02-13 03:34:46,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:46,609][root][INFO] - Training Epoch: 2/2, step 620/7134 completed (loss: 0.07163502275943756, acc: 0.987261176109314)
[2025-02-13 03:34:46,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:47,042][root][INFO] - Training Epoch: 2/2, step 621/7134 completed (loss: 0.037278663367033005, acc: 0.9878234267234802)
[2025-02-13 03:34:47,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:47,474][root][INFO] - Training Epoch: 2/2, step 622/7134 completed (loss: 0.04184810444712639, acc: 0.9885222315788269)
[2025-02-13 03:34:47,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:47,868][root][INFO] - Training Epoch: 2/2, step 623/7134 completed (loss: 0.07227524369955063, acc: 0.9868804812431335)
[2025-02-13 03:34:48,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:48,321][root][INFO] - Training Epoch: 2/2, step 624/7134 completed (loss: 0.021243643015623093, acc: 0.9972972869873047)
[2025-02-13 03:34:48,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:48,773][root][INFO] - Training Epoch: 2/2, step 625/7134 completed (loss: 0.060684144496917725, acc: 0.9854881167411804)
[2025-02-13 03:34:48,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:49,175][root][INFO] - Training Epoch: 2/2, step 626/7134 completed (loss: 0.07770666480064392, acc: 0.9828178882598877)
[2025-02-13 03:34:49,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:49,599][root][INFO] - Training Epoch: 2/2, step 627/7134 completed (loss: 0.0340435728430748, acc: 0.9896755218505859)
[2025-02-13 03:34:49,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:50,024][root][INFO] - Training Epoch: 2/2, step 628/7134 completed (loss: 0.03392983227968216, acc: 0.9916782379150391)
[2025-02-13 03:34:50,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:50,437][root][INFO] - Training Epoch: 2/2, step 629/7134 completed (loss: 0.06335291266441345, acc: 0.9871244430541992)
[2025-02-13 03:34:50,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:50,882][root][INFO] - Training Epoch: 2/2, step 630/7134 completed (loss: 0.03735021874308586, acc: 0.9818652868270874)
[2025-02-13 03:34:51,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:51,296][root][INFO] - Training Epoch: 2/2, step 631/7134 completed (loss: 0.030669212341308594, acc: 0.99210524559021)
[2025-02-13 03:34:51,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:51,740][root][INFO] - Training Epoch: 2/2, step 632/7134 completed (loss: 0.021042924374341965, acc: 0.9935275316238403)
[2025-02-13 03:34:51,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:52,157][root][INFO] - Training Epoch: 2/2, step 633/7134 completed (loss: 0.030764171853661537, acc: 0.9927431344985962)
[2025-02-13 03:34:52,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:52,574][root][INFO] - Training Epoch: 2/2, step 634/7134 completed (loss: 0.018213991075754166, acc: 0.9936708807945251)
[2025-02-13 03:34:52,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:53,013][root][INFO] - Training Epoch: 2/2, step 635/7134 completed (loss: 0.02607508935034275, acc: 0.9923664331436157)
[2025-02-13 03:34:53,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:53,446][root][INFO] - Training Epoch: 2/2, step 636/7134 completed (loss: 0.08340677618980408, acc: 0.9820689558982849)
[2025-02-13 03:34:53,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:53,853][root][INFO] - Training Epoch: 2/2, step 637/7134 completed (loss: 0.03755141794681549, acc: 0.9888268113136292)
[2025-02-13 03:34:53,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:54,267][root][INFO] - Training Epoch: 2/2, step 638/7134 completed (loss: 0.05142725631594658, acc: 0.9875195026397705)
[2025-02-13 03:34:54,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:54,679][root][INFO] - Training Epoch: 2/2, step 639/7134 completed (loss: 0.018761882558465004, acc: 0.9933333396911621)
[2025-02-13 03:34:54,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:55,083][root][INFO] - Training Epoch: 2/2, step 640/7134 completed (loss: 0.06118053197860718, acc: 0.9843478202819824)
[2025-02-13 03:34:55,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:55,490][root][INFO] - Training Epoch: 2/2, step 641/7134 completed (loss: 0.050655853003263474, acc: 0.9869067072868347)
[2025-02-13 03:34:55,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:55,917][root][INFO] - Training Epoch: 2/2, step 642/7134 completed (loss: 0.023849494755268097, acc: 0.9931972622871399)
[2025-02-13 03:34:56,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:56,353][root][INFO] - Training Epoch: 2/2, step 643/7134 completed (loss: 0.012971386313438416, acc: 0.9961488842964172)
[2025-02-13 03:34:56,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:56,772][root][INFO] - Training Epoch: 2/2, step 644/7134 completed (loss: 0.025404106825590134, acc: 0.9903475046157837)
[2025-02-13 03:34:56,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:57,176][root][INFO] - Training Epoch: 2/2, step 645/7134 completed (loss: 0.03186791390180588, acc: 0.9932659864425659)
[2025-02-13 03:34:57,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:57,602][root][INFO] - Training Epoch: 2/2, step 646/7134 completed (loss: 0.03620316460728645, acc: 0.9908257126808167)
[2025-02-13 03:34:57,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:58,018][root][INFO] - Training Epoch: 2/2, step 647/7134 completed (loss: 0.006733396556228399, acc: 0.9983079433441162)
[2025-02-13 03:34:58,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:58,446][root][INFO] - Training Epoch: 2/2, step 648/7134 completed (loss: 0.08008512109518051, acc: 0.9790576100349426)
[2025-02-13 03:34:58,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:58,890][root][INFO] - Training Epoch: 2/2, step 649/7134 completed (loss: 0.02850140631198883, acc: 0.9897435903549194)
[2025-02-13 03:34:59,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:59,319][root][INFO] - Training Epoch: 2/2, step 650/7134 completed (loss: 0.060160644352436066, acc: 0.9827315807342529)
[2025-02-13 03:34:59,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:59,748][root][INFO] - Training Epoch: 2/2, step 651/7134 completed (loss: 0.1006908267736435, acc: 0.9740061163902283)
[2025-02-13 03:34:59,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:00,181][root][INFO] - Training Epoch: 2/2, step 652/7134 completed (loss: 0.02964860014617443, acc: 0.993127167224884)
[2025-02-13 03:35:00,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:00,604][root][INFO] - Training Epoch: 2/2, step 653/7134 completed (loss: 0.027910679578781128, acc: 0.9886524677276611)
[2025-02-13 03:35:00,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:01,008][root][INFO] - Training Epoch: 2/2, step 654/7134 completed (loss: 0.10703890770673752, acc: 0.9697624444961548)
[2025-02-13 03:35:01,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:01,449][root][INFO] - Training Epoch: 2/2, step 655/7134 completed (loss: 0.053209539502859116, acc: 0.9858871102333069)
[2025-02-13 03:35:01,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:01,872][root][INFO] - Training Epoch: 2/2, step 656/7134 completed (loss: 0.0626126155257225, acc: 0.9841688871383667)
[2025-02-13 03:35:02,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:02,332][root][INFO] - Training Epoch: 2/2, step 657/7134 completed (loss: 0.04240786284208298, acc: 0.9887640476226807)
[2025-02-13 03:35:02,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:02,767][root][INFO] - Training Epoch: 2/2, step 658/7134 completed (loss: 0.0357138030230999, acc: 0.990777313709259)
[2025-02-13 03:35:02,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:03,223][root][INFO] - Training Epoch: 2/2, step 659/7134 completed (loss: 0.019342996180057526, acc: 0.992668628692627)
[2025-02-13 03:35:03,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:03,688][root][INFO] - Training Epoch: 2/2, step 660/7134 completed (loss: 0.0594274066388607, acc: 0.9853801131248474)
[2025-02-13 03:35:03,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:04,169][root][INFO] - Training Epoch: 2/2, step 661/7134 completed (loss: 0.09550163149833679, acc: 0.98221755027771)
[2025-02-13 03:35:04,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:04,642][root][INFO] - Training Epoch: 2/2, step 662/7134 completed (loss: 0.031526245176792145, acc: 0.9906166195869446)
[2025-02-13 03:35:04,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:05,138][root][INFO] - Training Epoch: 2/2, step 663/7134 completed (loss: 0.06265813857316971, acc: 0.9831649661064148)
[2025-02-13 03:35:05,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:05,568][root][INFO] - Training Epoch: 2/2, step 664/7134 completed (loss: 0.05194266140460968, acc: 0.9900000095367432)
[2025-02-13 03:35:05,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:06,005][root][INFO] - Training Epoch: 2/2, step 665/7134 completed (loss: 0.05616694688796997, acc: 0.9878869652748108)
[2025-02-13 03:35:06,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:06,488][root][INFO] - Training Epoch: 2/2, step 666/7134 completed (loss: 0.07521944493055344, acc: 0.9807692170143127)
[2025-02-13 03:35:06,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:06,939][root][INFO] - Training Epoch: 2/2, step 667/7134 completed (loss: 0.07000666856765747, acc: 0.9837996959686279)
[2025-02-13 03:35:07,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:07,392][root][INFO] - Training Epoch: 2/2, step 668/7134 completed (loss: 0.03527076169848442, acc: 0.9898605942726135)
[2025-02-13 03:35:07,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:07,874][root][INFO] - Training Epoch: 2/2, step 669/7134 completed (loss: 0.031746845692396164, acc: 0.9931787252426147)
[2025-02-13 03:35:08,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:08,370][root][INFO] - Training Epoch: 2/2, step 670/7134 completed (loss: 0.05155444145202637, acc: 0.9856262803077698)
[2025-02-13 03:35:08,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:08,859][root][INFO] - Training Epoch: 2/2, step 671/7134 completed (loss: 0.04764318838715553, acc: 0.9882100820541382)
[2025-02-13 03:35:08,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:09,269][root][INFO] - Training Epoch: 2/2, step 672/7134 completed (loss: 0.06853343546390533, acc: 0.9880775213241577)
[2025-02-13 03:35:09,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:09,719][root][INFO] - Training Epoch: 2/2, step 673/7134 completed (loss: 0.04731201380491257, acc: 0.9877150058746338)
[2025-02-13 03:35:09,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:10,168][root][INFO] - Training Epoch: 2/2, step 674/7134 completed (loss: 0.05473305284976959, acc: 0.9819004535675049)
[2025-02-13 03:35:10,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:10,631][root][INFO] - Training Epoch: 2/2, step 675/7134 completed (loss: 0.07133138179779053, acc: 0.9843546152114868)
[2025-02-13 03:35:10,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:11,075][root][INFO] - Training Epoch: 2/2, step 676/7134 completed (loss: 0.05259448289871216, acc: 0.9869822263717651)
[2025-02-13 03:35:11,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:11,547][root][INFO] - Training Epoch: 2/2, step 677/7134 completed (loss: 0.014513755217194557, acc: 0.9963008761405945)
[2025-02-13 03:35:11,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:12,004][root][INFO] - Training Epoch: 2/2, step 678/7134 completed (loss: 0.032123811542987823, acc: 0.9892328381538391)
[2025-02-13 03:35:12,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:12,398][root][INFO] - Training Epoch: 2/2, step 679/7134 completed (loss: 0.020660409703850746, acc: 0.9943289160728455)
[2025-02-13 03:35:12,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:12,849][root][INFO] - Training Epoch: 2/2, step 680/7134 completed (loss: 0.07807028293609619, acc: 0.9770889282226562)
[2025-02-13 03:35:12,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:13,320][root][INFO] - Training Epoch: 2/2, step 681/7134 completed (loss: 0.046656377613544464, acc: 0.9865030646324158)
[2025-02-13 03:35:13,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:13,738][root][INFO] - Training Epoch: 2/2, step 682/7134 completed (loss: 0.03281966596841812, acc: 0.9884726405143738)
[2025-02-13 03:35:13,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:14,207][root][INFO] - Training Epoch: 2/2, step 683/7134 completed (loss: 0.050453707575798035, acc: 0.9835391044616699)
[2025-02-13 03:35:14,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:14,670][root][INFO] - Training Epoch: 2/2, step 684/7134 completed (loss: 0.029424654319882393, acc: 0.9916897416114807)
[2025-02-13 03:35:14,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:15,121][root][INFO] - Training Epoch: 2/2, step 685/7134 completed (loss: 0.024778762832283974, acc: 0.9937655925750732)
[2025-02-13 03:35:15,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:15,575][root][INFO] - Training Epoch: 2/2, step 686/7134 completed (loss: 0.017849020659923553, acc: 0.9933949708938599)
[2025-02-13 03:35:15,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:16,030][root][INFO] - Training Epoch: 2/2, step 687/7134 completed (loss: 0.02127787470817566, acc: 0.9941973090171814)
[2025-02-13 03:35:16,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:16,479][root][INFO] - Training Epoch: 2/2, step 688/7134 completed (loss: 0.013206671923398972, acc: 0.9970414042472839)
[2025-02-13 03:35:16,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:16,883][root][INFO] - Training Epoch: 2/2, step 689/7134 completed (loss: 0.02529001608490944, acc: 0.9931623935699463)
[2025-02-13 03:35:17,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:17,305][root][INFO] - Training Epoch: 2/2, step 690/7134 completed (loss: 0.016446903347969055, acc: 0.995230495929718)
[2025-02-13 03:35:17,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:17,726][root][INFO] - Training Epoch: 2/2, step 691/7134 completed (loss: 0.055446743965148926, acc: 0.9856321811676025)
[2025-02-13 03:35:17,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:18,182][root][INFO] - Training Epoch: 2/2, step 692/7134 completed (loss: 0.06154501438140869, acc: 0.9853333234786987)
[2025-02-13 03:35:18,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:18,599][root][INFO] - Training Epoch: 2/2, step 693/7134 completed (loss: 0.04037553071975708, acc: 0.9838449358940125)
[2025-02-13 03:35:18,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:19,023][root][INFO] - Training Epoch: 2/2, step 694/7134 completed (loss: 0.061316125094890594, acc: 0.9868612885475159)
[2025-02-13 03:35:19,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:19,473][root][INFO] - Training Epoch: 2/2, step 695/7134 completed (loss: 0.08140284568071365, acc: 0.9810671210289001)
[2025-02-13 03:35:19,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:19,882][root][INFO] - Training Epoch: 2/2, step 696/7134 completed (loss: 0.07438436150550842, acc: 0.9805825352668762)
[2025-02-13 03:35:20,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:20,340][root][INFO] - Training Epoch: 2/2, step 697/7134 completed (loss: 0.033908210694789886, acc: 0.9925558567047119)
[2025-02-13 03:35:20,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:20,738][root][INFO] - Training Epoch: 2/2, step 698/7134 completed (loss: 0.01390839833766222, acc: 0.9967426657676697)
[2025-02-13 03:35:20,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:21,203][root][INFO] - Training Epoch: 2/2, step 699/7134 completed (loss: 0.02358333207666874, acc: 0.9906542301177979)
[2025-02-13 03:35:21,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:21,626][root][INFO] - Training Epoch: 2/2, step 700/7134 completed (loss: 0.0492129884660244, acc: 0.9906716346740723)
[2025-02-13 03:35:21,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:22,037][root][INFO] - Training Epoch: 2/2, step 701/7134 completed (loss: 0.0386168509721756, acc: 0.9840425252914429)
[2025-02-13 03:35:22,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:22,467][root][INFO] - Training Epoch: 2/2, step 702/7134 completed (loss: 0.02860119752585888, acc: 0.9918830990791321)
[2025-02-13 03:35:22,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:22,874][root][INFO] - Training Epoch: 2/2, step 703/7134 completed (loss: 0.026835525408387184, acc: 0.9903069734573364)
[2025-02-13 03:35:23,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:23,332][root][INFO] - Training Epoch: 2/2, step 704/7134 completed (loss: 0.03353692963719368, acc: 0.9878419637680054)
[2025-02-13 03:35:23,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:23,800][root][INFO] - Training Epoch: 2/2, step 705/7134 completed (loss: 0.05541061982512474, acc: 0.9798882603645325)
[2025-02-13 03:35:23,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:24,266][root][INFO] - Training Epoch: 2/2, step 706/7134 completed (loss: 0.020273515954613686, acc: 0.9934354424476624)
[2025-02-13 03:35:24,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:24,710][root][INFO] - Training Epoch: 2/2, step 707/7134 completed (loss: 0.03201720118522644, acc: 0.9880095720291138)
[2025-02-13 03:35:24,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:25,184][root][INFO] - Training Epoch: 2/2, step 708/7134 completed (loss: 0.024290839210152626, acc: 0.9913259148597717)
[2025-02-13 03:35:25,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:25,627][root][INFO] - Training Epoch: 2/2, step 709/7134 completed (loss: 0.026339182630181313, acc: 0.9940333962440491)
[2025-02-13 03:35:25,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:26,091][root][INFO] - Training Epoch: 2/2, step 710/7134 completed (loss: 0.021815361455082893, acc: 0.9939467310905457)
[2025-02-13 03:35:26,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:26,518][root][INFO] - Training Epoch: 2/2, step 711/7134 completed (loss: 0.03458483889698982, acc: 0.990554928779602)
[2025-02-13 03:35:26,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:26,958][root][INFO] - Training Epoch: 2/2, step 712/7134 completed (loss: 0.02325942926108837, acc: 0.9926739931106567)
[2025-02-13 03:35:27,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:27,424][root][INFO] - Training Epoch: 2/2, step 713/7134 completed (loss: 0.014729401096701622, acc: 0.9965397715568542)
[2025-02-13 03:35:27,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:27,758][root][INFO] - Training Epoch: 2/2, step 714/7134 completed (loss: 0.03017454408109188, acc: 0.990338146686554)
[2025-02-13 03:35:27,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:28,234][root][INFO] - Training Epoch: 2/2, step 715/7134 completed (loss: 0.022353487089276314, acc: 0.9900568127632141)
[2025-02-13 03:35:28,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:28,686][root][INFO] - Training Epoch: 2/2, step 716/7134 completed (loss: 0.013995620422065258, acc: 0.9954493641853333)
[2025-02-13 03:35:28,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:29,124][root][INFO] - Training Epoch: 2/2, step 717/7134 completed (loss: 0.04625925049185753, acc: 0.9962073564529419)
[2025-02-13 03:35:29,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:29,595][root][INFO] - Training Epoch: 2/2, step 718/7134 completed (loss: 0.013776648789644241, acc: 0.9962406158447266)
[2025-02-13 03:35:29,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:29,973][root][INFO] - Training Epoch: 2/2, step 719/7134 completed (loss: 0.011903186328709126, acc: 0.998275876045227)
[2025-02-13 03:35:30,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:30,423][root][INFO] - Training Epoch: 2/2, step 720/7134 completed (loss: 0.020047627389431, acc: 0.9962406158447266)
[2025-02-13 03:35:30,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:30,890][root][INFO] - Training Epoch: 2/2, step 721/7134 completed (loss: 0.030125848948955536, acc: 0.98975670337677)
[2025-02-13 03:35:31,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:31,378][root][INFO] - Training Epoch: 2/2, step 722/7134 completed (loss: 0.01847808063030243, acc: 0.9929988384246826)
[2025-02-13 03:35:31,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:31,828][root][INFO] - Training Epoch: 2/2, step 723/7134 completed (loss: 0.013428638689219952, acc: 0.9963680505752563)
[2025-02-13 03:35:31,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:32,242][root][INFO] - Training Epoch: 2/2, step 724/7134 completed (loss: 0.02610923908650875, acc: 0.9942775368690491)
[2025-02-13 03:35:32,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:32,691][root][INFO] - Training Epoch: 2/2, step 725/7134 completed (loss: 0.01390074286609888, acc: 0.9942113161087036)
[2025-02-13 03:35:32,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:33,140][root][INFO] - Training Epoch: 2/2, step 726/7134 completed (loss: 0.02082790806889534, acc: 0.991631805896759)
[2025-02-13 03:35:33,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:33,575][root][INFO] - Training Epoch: 2/2, step 727/7134 completed (loss: 0.00737247709184885, acc: 0.9971140027046204)
[2025-02-13 03:35:33,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:34,010][root][INFO] - Training Epoch: 2/2, step 728/7134 completed (loss: 0.019132327288389206, acc: 0.9931129217147827)
[2025-02-13 03:35:34,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:34,463][root][INFO] - Training Epoch: 2/2, step 729/7134 completed (loss: 0.011675924994051456, acc: 0.9944367408752441)
[2025-02-13 03:35:34,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:34,911][root][INFO] - Training Epoch: 2/2, step 730/7134 completed (loss: 0.024080727249383926, acc: 0.993514895439148)
[2025-02-13 03:35:35,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:35,366][root][INFO] - Training Epoch: 2/2, step 731/7134 completed (loss: 0.02100907452404499, acc: 0.9946666955947876)
[2025-02-13 03:35:35,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:35,789][root][INFO] - Training Epoch: 2/2, step 732/7134 completed (loss: 0.05314519256353378, acc: 0.9792453050613403)
[2025-02-13 03:35:35,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:36,159][root][INFO] - Training Epoch: 2/2, step 733/7134 completed (loss: 0.03991156816482544, acc: 0.980461835861206)
[2025-02-13 03:35:36,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:36,567][root][INFO] - Training Epoch: 2/2, step 734/7134 completed (loss: 0.0198830496519804, acc: 0.9923195242881775)
[2025-02-13 03:35:36,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:36,986][root][INFO] - Training Epoch: 2/2, step 735/7134 completed (loss: 0.018067331984639168, acc: 0.9903537034988403)
[2025-02-13 03:35:37,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:37,394][root][INFO] - Training Epoch: 2/2, step 736/7134 completed (loss: 0.044290587306022644, acc: 0.9824175834655762)
[2025-02-13 03:35:37,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:37,818][root][INFO] - Training Epoch: 2/2, step 737/7134 completed (loss: 0.11414705961942673, acc: 0.9776875972747803)
[2025-02-13 03:35:37,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:38,226][root][INFO] - Training Epoch: 2/2, step 738/7134 completed (loss: 0.010342366993427277, acc: 0.9961538314819336)
[2025-02-13 03:35:38,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:38,649][root][INFO] - Training Epoch: 2/2, step 739/7134 completed (loss: 0.017923176288604736, acc: 0.9963302612304688)
[2025-02-13 03:35:38,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:39,057][root][INFO] - Training Epoch: 2/2, step 740/7134 completed (loss: 0.006322748959064484, acc: 0.9984126687049866)
[2025-02-13 03:35:39,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:39,472][root][INFO] - Training Epoch: 2/2, step 741/7134 completed (loss: 0.029354805126786232, acc: 0.9905362725257874)
[2025-02-13 03:35:39,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:39,844][root][INFO] - Training Epoch: 2/2, step 742/7134 completed (loss: 0.0389217771589756, acc: 0.9874213933944702)
[2025-02-13 03:35:39,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:40,280][root][INFO] - Training Epoch: 2/2, step 743/7134 completed (loss: 0.04252813756465912, acc: 0.9864457845687866)
[2025-02-13 03:35:40,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:40,674][root][INFO] - Training Epoch: 2/2, step 744/7134 completed (loss: 0.02427351288497448, acc: 0.9920634627342224)
[2025-02-13 03:35:40,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:41,085][root][INFO] - Training Epoch: 2/2, step 745/7134 completed (loss: 0.03175194188952446, acc: 0.9939576983451843)
[2025-02-13 03:35:41,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:41,512][root][INFO] - Training Epoch: 2/2, step 746/7134 completed (loss: 0.04506206512451172, acc: 0.9901477694511414)
[2025-02-13 03:35:41,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:41,875][root][INFO] - Training Epoch: 2/2, step 747/7134 completed (loss: 0.1131022647023201, acc: 0.97508305311203)
[2025-02-13 03:35:42,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:42,337][root][INFO] - Training Epoch: 2/2, step 748/7134 completed (loss: 0.024419687688350677, acc: 0.9924812316894531)
[2025-02-13 03:35:42,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:42,768][root][INFO] - Training Epoch: 2/2, step 749/7134 completed (loss: 0.018428748473525047, acc: 0.9956772327423096)
[2025-02-13 03:35:42,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:43,240][root][INFO] - Training Epoch: 2/2, step 750/7134 completed (loss: 0.025761665776371956, acc: 0.9945945739746094)
[2025-02-13 03:35:43,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:43,656][root][INFO] - Training Epoch: 2/2, step 751/7134 completed (loss: 0.06194089725613594, acc: 0.9829642176628113)
[2025-02-13 03:35:43,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:44,022][root][INFO] - Training Epoch: 2/2, step 752/7134 completed (loss: 0.08401037007570267, acc: 0.9849849939346313)
[2025-02-13 03:35:44,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:44,429][root][INFO] - Training Epoch: 2/2, step 753/7134 completed (loss: 0.0430217944085598, acc: 0.9881756901741028)
[2025-02-13 03:35:44,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:44,848][root][INFO] - Training Epoch: 2/2, step 754/7134 completed (loss: 0.014192501083016396, acc: 0.9969834089279175)
[2025-02-13 03:35:44,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:45,277][root][INFO] - Training Epoch: 2/2, step 755/7134 completed (loss: 0.08758829534053802, acc: 0.9768977165222168)
[2025-02-13 03:35:45,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:45,666][root][INFO] - Training Epoch: 2/2, step 756/7134 completed (loss: 0.034417226910591125, acc: 0.9906542301177979)
[2025-02-13 03:35:45,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:46,073][root][INFO] - Training Epoch: 2/2, step 757/7134 completed (loss: 0.021448709070682526, acc: 0.9961685538291931)
[2025-02-13 03:35:46,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:46,487][root][INFO] - Training Epoch: 2/2, step 758/7134 completed (loss: 0.042333438992500305, acc: 0.9913941621780396)
[2025-02-13 03:35:46,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:46,863][root][INFO] - Training Epoch: 2/2, step 759/7134 completed (loss: 0.03157778084278107, acc: 0.9864341020584106)
[2025-02-13 03:35:46,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:47,271][root][INFO] - Training Epoch: 2/2, step 760/7134 completed (loss: 0.013293695636093616, acc: 0.9965338110923767)
[2025-02-13 03:35:47,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:47,675][root][INFO] - Training Epoch: 2/2, step 761/7134 completed (loss: 0.03215266391634941, acc: 0.9906103014945984)
[2025-02-13 03:35:47,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:48,082][root][INFO] - Training Epoch: 2/2, step 762/7134 completed (loss: 0.01942857727408409, acc: 0.9928315281867981)
[2025-02-13 03:35:48,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:48,485][root][INFO] - Training Epoch: 2/2, step 763/7134 completed (loss: 0.055797677487134933, acc: 0.9820846915245056)
[2025-02-13 03:35:48,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:48,912][root][INFO] - Training Epoch: 2/2, step 764/7134 completed (loss: 0.02582201361656189, acc: 0.991391658782959)
[2025-02-13 03:35:49,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:49,362][root][INFO] - Training Epoch: 2/2, step 765/7134 completed (loss: 0.015053686685860157, acc: 0.9956076145172119)
[2025-02-13 03:35:49,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:49,798][root][INFO] - Training Epoch: 2/2, step 766/7134 completed (loss: 0.012789744883775711, acc: 0.997032642364502)
[2025-02-13 03:35:49,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:50,220][root][INFO] - Training Epoch: 2/2, step 767/7134 completed (loss: 0.02037067525088787, acc: 0.9970717430114746)
[2025-02-13 03:35:50,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:50,652][root][INFO] - Training Epoch: 2/2, step 768/7134 completed (loss: 0.03917010873556137, acc: 0.9915013909339905)
[2025-02-13 03:35:50,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:51,081][root][INFO] - Training Epoch: 2/2, step 769/7134 completed (loss: 0.0126083604991436, acc: 0.9971346855163574)
[2025-02-13 03:35:51,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:51,486][root][INFO] - Training Epoch: 2/2, step 770/7134 completed (loss: 0.028219208121299744, acc: 0.9892802238464355)
[2025-02-13 03:35:51,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:51,855][root][INFO] - Training Epoch: 2/2, step 771/7134 completed (loss: 0.02289782650768757, acc: 0.9920791983604431)
[2025-02-13 03:35:51,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:52,259][root][INFO] - Training Epoch: 2/2, step 772/7134 completed (loss: 0.03531237319111824, acc: 0.9882746934890747)
[2025-02-13 03:35:52,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:52,716][root][INFO] - Training Epoch: 2/2, step 773/7134 completed (loss: 0.036117780953645706, acc: 0.9910025596618652)
[2025-02-13 03:35:52,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:53,123][root][INFO] - Training Epoch: 2/2, step 774/7134 completed (loss: 0.014076589606702328, acc: 0.9950166344642639)
[2025-02-13 03:35:53,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:53,581][root][INFO] - Training Epoch: 2/2, step 775/7134 completed (loss: 0.019348643720149994, acc: 0.9946737885475159)
[2025-02-13 03:35:53,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:54,011][root][INFO] - Training Epoch: 2/2, step 776/7134 completed (loss: 0.028345772996544838, acc: 0.9926035404205322)
[2025-02-13 03:35:54,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:54,414][root][INFO] - Training Epoch: 2/2, step 777/7134 completed (loss: 0.02123047597706318, acc: 0.9935794472694397)
[2025-02-13 03:35:54,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:54,842][root][INFO] - Training Epoch: 2/2, step 778/7134 completed (loss: 0.04017772898077965, acc: 0.9935897588729858)
[2025-02-13 03:35:54,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:55,266][root][INFO] - Training Epoch: 2/2, step 779/7134 completed (loss: 0.041339095681905746, acc: 0.9868612885475159)
[2025-02-13 03:35:55,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:55,705][root][INFO] - Training Epoch: 2/2, step 780/7134 completed (loss: 0.039479173719882965, acc: 0.9923954606056213)
[2025-02-13 03:35:55,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:56,191][root][INFO] - Training Epoch: 2/2, step 781/7134 completed (loss: 0.03865040838718414, acc: 0.9890109896659851)
[2025-02-13 03:35:56,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:56,590][root][INFO] - Training Epoch: 2/2, step 782/7134 completed (loss: 0.032692648470401764, acc: 0.9889240264892578)
[2025-02-13 03:35:56,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:57,059][root][INFO] - Training Epoch: 2/2, step 783/7134 completed (loss: 0.030274344608187675, acc: 0.9910827875137329)
[2025-02-13 03:35:57,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:57,496][root][INFO] - Training Epoch: 2/2, step 784/7134 completed (loss: 0.05087855085730553, acc: 0.986601710319519)
[2025-02-13 03:35:57,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:57,940][root][INFO] - Training Epoch: 2/2, step 785/7134 completed (loss: 0.037300631403923035, acc: 0.9866220951080322)
[2025-02-13 03:35:58,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:58,344][root][INFO] - Training Epoch: 2/2, step 786/7134 completed (loss: 0.03962535038590431, acc: 0.9907063245773315)
[2025-02-13 03:35:58,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:58,703][root][INFO] - Training Epoch: 2/2, step 787/7134 completed (loss: 0.005842133890837431, acc: 1.0)
[2025-02-13 03:35:58,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:59,160][root][INFO] - Training Epoch: 2/2, step 788/7134 completed (loss: 0.05867435783147812, acc: 0.98531574010849)
[2025-02-13 03:35:59,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:59,574][root][INFO] - Training Epoch: 2/2, step 789/7134 completed (loss: 0.03213941678404808, acc: 0.989313006401062)
[2025-02-13 03:35:59,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:59,987][root][INFO] - Training Epoch: 2/2, step 790/7134 completed (loss: 0.06420356780290604, acc: 0.9852070808410645)
[2025-02-13 03:36:00,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:00,393][root][INFO] - Training Epoch: 2/2, step 791/7134 completed (loss: 0.059529103338718414, acc: 0.9861660003662109)
[2025-02-13 03:36:00,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:00,807][root][INFO] - Training Epoch: 2/2, step 792/7134 completed (loss: 0.043533679097890854, acc: 0.989180862903595)
[2025-02-13 03:36:00,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:01,228][root][INFO] - Training Epoch: 2/2, step 793/7134 completed (loss: 0.016633938997983932, acc: 0.9940828680992126)
[2025-02-13 03:36:01,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:01,644][root][INFO] - Training Epoch: 2/2, step 794/7134 completed (loss: 0.032430898398160934, acc: 0.9860140085220337)
[2025-02-13 03:36:01,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:01,985][root][INFO] - Training Epoch: 2/2, step 795/7134 completed (loss: 0.06920687854290009, acc: 0.9813084006309509)
[2025-02-13 03:36:02,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:02,385][root][INFO] - Training Epoch: 2/2, step 796/7134 completed (loss: 0.06306169182062149, acc: 0.9772727489471436)
[2025-02-13 03:36:02,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:02,751][root][INFO] - Training Epoch: 2/2, step 797/7134 completed (loss: 0.04133208096027374, acc: 0.9839357137680054)
[2025-02-13 03:36:02,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:03,109][root][INFO] - Training Epoch: 2/2, step 798/7134 completed (loss: 0.02340603806078434, acc: 0.9920634627342224)
[2025-02-13 03:36:03,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:03,473][root][INFO] - Training Epoch: 2/2, step 799/7134 completed (loss: 0.03302498161792755, acc: 0.9870967864990234)
[2025-02-13 03:36:03,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:03,874][root][INFO] - Training Epoch: 2/2, step 800/7134 completed (loss: 0.05730534344911575, acc: 0.9869646430015564)
[2025-02-13 03:36:04,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:04,300][root][INFO] - Training Epoch: 2/2, step 801/7134 completed (loss: 0.05329205468297005, acc: 0.9855999946594238)
[2025-02-13 03:36:04,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:04,728][root][INFO] - Training Epoch: 2/2, step 802/7134 completed (loss: 0.05227931588888168, acc: 0.9855072498321533)
[2025-02-13 03:36:04,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:05,102][root][INFO] - Training Epoch: 2/2, step 803/7134 completed (loss: 0.022109797224402428, acc: 0.9928571581840515)
[2025-02-13 03:36:05,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:05,506][root][INFO] - Training Epoch: 2/2, step 804/7134 completed (loss: 0.02718810923397541, acc: 0.9979633688926697)
[2025-02-13 03:36:05,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:05,919][root][INFO] - Training Epoch: 2/2, step 805/7134 completed (loss: 0.04523638263344765, acc: 0.9756554365158081)
[2025-02-13 03:36:06,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:06,353][root][INFO] - Training Epoch: 2/2, step 806/7134 completed (loss: 0.015580526553094387, acc: 0.9972066879272461)
[2025-02-13 03:36:06,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:06,813][root][INFO] - Training Epoch: 2/2, step 807/7134 completed (loss: 0.03747699409723282, acc: 0.9902676343917847)
[2025-02-13 03:36:06,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:07,254][root][INFO] - Training Epoch: 2/2, step 808/7134 completed (loss: 0.016731232404708862, acc: 0.9976047873497009)
[2025-02-13 03:36:07,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:07,686][root][INFO] - Training Epoch: 2/2, step 809/7134 completed (loss: 0.014426372945308685, acc: 0.9970282316207886)
[2025-02-13 03:36:07,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:08,105][root][INFO] - Training Epoch: 2/2, step 810/7134 completed (loss: 0.03517621010541916, acc: 0.9905533194541931)
[2025-02-13 03:36:08,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:08,487][root][INFO] - Training Epoch: 2/2, step 811/7134 completed (loss: 0.08234729617834091, acc: 0.982425332069397)
[2025-02-13 03:36:08,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:08,928][root][INFO] - Training Epoch: 2/2, step 812/7134 completed (loss: 0.0270827729254961, acc: 0.9924337863922119)
[2025-02-13 03:36:09,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:09,349][root][INFO] - Training Epoch: 2/2, step 813/7134 completed (loss: 0.02532220631837845, acc: 0.9927431344985962)
[2025-02-13 03:36:09,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:09,805][root][INFO] - Training Epoch: 2/2, step 814/7134 completed (loss: 0.04156599938869476, acc: 0.9896774291992188)
[2025-02-13 03:36:09,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:10,254][root][INFO] - Training Epoch: 2/2, step 815/7134 completed (loss: 0.04386665299534798, acc: 0.984544038772583)
[2025-02-13 03:36:10,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:10,700][root][INFO] - Training Epoch: 2/2, step 816/7134 completed (loss: 0.05091056227684021, acc: 0.9779661297798157)
[2025-02-13 03:36:10,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:11,134][root][INFO] - Training Epoch: 2/2, step 817/7134 completed (loss: 0.07621719688177109, acc: 0.9780621528625488)
[2025-02-13 03:36:11,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:11,551][root][INFO] - Training Epoch: 2/2, step 818/7134 completed (loss: 0.05391952022910118, acc: 0.975944995880127)
[2025-02-13 03:36:11,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:11,960][root][INFO] - Training Epoch: 2/2, step 819/7134 completed (loss: 0.05003030225634575, acc: 0.9904000163078308)
[2025-02-13 03:36:12,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:12,407][root][INFO] - Training Epoch: 2/2, step 820/7134 completed (loss: 0.04243452101945877, acc: 0.9907264113426208)
[2025-02-13 03:36:12,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:12,868][root][INFO] - Training Epoch: 2/2, step 821/7134 completed (loss: 0.026200678199529648, acc: 0.9959785342216492)
[2025-02-13 03:36:13,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:13,273][root][INFO] - Training Epoch: 2/2, step 822/7134 completed (loss: 0.03172895684838295, acc: 0.9905808568000793)
[2025-02-13 03:36:13,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:13,689][root][INFO] - Training Epoch: 2/2, step 823/7134 completed (loss: 0.012766114436089993, acc: 0.9943714737892151)
[2025-02-13 03:36:13,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:14,102][root][INFO] - Training Epoch: 2/2, step 824/7134 completed (loss: 0.06823554635047913, acc: 0.9770318269729614)
[2025-02-13 03:36:14,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:14,559][root][INFO] - Training Epoch: 2/2, step 825/7134 completed (loss: 0.033383794128894806, acc: 0.990777313709259)
[2025-02-13 03:36:14,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:15,021][root][INFO] - Training Epoch: 2/2, step 826/7134 completed (loss: 0.02102658525109291, acc: 0.9918256402015686)
[2025-02-13 03:36:15,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:15,455][root][INFO] - Training Epoch: 2/2, step 827/7134 completed (loss: 0.03563141077756882, acc: 0.9895012974739075)
[2025-02-13 03:36:15,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:15,864][root][INFO] - Training Epoch: 2/2, step 828/7134 completed (loss: 0.03839730843901634, acc: 0.9906687140464783)
[2025-02-13 03:36:16,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:16,282][root][INFO] - Training Epoch: 2/2, step 829/7134 completed (loss: 0.03009582869708538, acc: 0.9927184581756592)
[2025-02-13 03:36:16,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:16,749][root][INFO] - Training Epoch: 2/2, step 830/7134 completed (loss: 0.02516915835440159, acc: 0.9900332093238831)
[2025-02-13 03:36:16,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:17,187][root][INFO] - Training Epoch: 2/2, step 831/7134 completed (loss: 0.010909264907240868, acc: 0.9972602725028992)
[2025-02-13 03:36:17,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:17,607][root][INFO] - Training Epoch: 2/2, step 832/7134 completed (loss: 0.07936134934425354, acc: 0.9812286496162415)
[2025-02-13 03:36:17,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:18,069][root][INFO] - Training Epoch: 2/2, step 833/7134 completed (loss: 0.027910016477108, acc: 0.9901960492134094)
[2025-02-13 03:36:18,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:18,508][root][INFO] - Training Epoch: 2/2, step 834/7134 completed (loss: 0.02208678424358368, acc: 0.9930070042610168)
[2025-02-13 03:36:18,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:18,964][root][INFO] - Training Epoch: 2/2, step 835/7134 completed (loss: 0.038994308561086655, acc: 0.9902098178863525)
[2025-02-13 03:36:19,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:19,430][root][INFO] - Training Epoch: 2/2, step 836/7134 completed (loss: 0.025384197011590004, acc: 0.9907192587852478)
[2025-02-13 03:36:19,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:19,861][root][INFO] - Training Epoch: 2/2, step 837/7134 completed (loss: 0.0333118736743927, acc: 0.9918256402015686)
[2025-02-13 03:36:20,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:20,296][root][INFO] - Training Epoch: 2/2, step 838/7134 completed (loss: 0.02019497938454151, acc: 0.995230495929718)
[2025-02-13 03:36:20,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:20,750][root][INFO] - Training Epoch: 2/2, step 839/7134 completed (loss: 0.01963002234697342, acc: 0.9904305934906006)
[2025-02-13 03:36:20,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:21,169][root][INFO] - Training Epoch: 2/2, step 840/7134 completed (loss: 0.017280234023928642, acc: 0.9972106218338013)
[2025-02-13 03:36:21,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:21,600][root][INFO] - Training Epoch: 2/2, step 841/7134 completed (loss: 0.036530882120132446, acc: 0.9875518679618835)
[2025-02-13 03:36:21,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:22,064][root][INFO] - Training Epoch: 2/2, step 842/7134 completed (loss: 0.026858652010560036, acc: 0.9919137358665466)
[2025-02-13 03:36:22,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:22,514][root][INFO] - Training Epoch: 2/2, step 843/7134 completed (loss: 0.028064366430044174, acc: 0.9930459260940552)
[2025-02-13 03:36:22,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:22,926][root][INFO] - Training Epoch: 2/2, step 844/7134 completed (loss: 0.026732617989182472, acc: 0.9873617887496948)
[2025-02-13 03:36:23,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:23,340][root][INFO] - Training Epoch: 2/2, step 845/7134 completed (loss: 0.035616423934698105, acc: 0.9876106381416321)
[2025-02-13 03:36:23,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:23,791][root][INFO] - Training Epoch: 2/2, step 846/7134 completed (loss: 0.0260205939412117, acc: 0.9910979270935059)
[2025-02-13 03:36:23,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:24,229][root][INFO] - Training Epoch: 2/2, step 847/7134 completed (loss: 0.00851873867213726, acc: 1.0)
[2025-02-13 03:36:24,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:24,658][root][INFO] - Training Epoch: 2/2, step 848/7134 completed (loss: 0.036234356462955475, acc: 0.990111231803894)
[2025-02-13 03:36:24,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:25,111][root][INFO] - Training Epoch: 2/2, step 849/7134 completed (loss: 0.018982641398906708, acc: 0.9927448630332947)
[2025-02-13 03:36:25,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:25,571][root][INFO] - Training Epoch: 2/2, step 850/7134 completed (loss: 0.01601550541818142, acc: 0.9932885766029358)
[2025-02-13 03:36:25,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:26,021][root][INFO] - Training Epoch: 2/2, step 851/7134 completed (loss: 0.03415626287460327, acc: 0.9894875288009644)
[2025-02-13 03:36:26,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:26,477][root][INFO] - Training Epoch: 2/2, step 852/7134 completed (loss: 0.020200662314891815, acc: 0.9938949942588806)
[2025-02-13 03:36:26,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:26,949][root][INFO] - Training Epoch: 2/2, step 853/7134 completed (loss: 0.014579549431800842, acc: 0.9955947399139404)
[2025-02-13 03:36:27,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:27,380][root][INFO] - Training Epoch: 2/2, step 854/7134 completed (loss: 0.03230850026011467, acc: 0.9925373196601868)
[2025-02-13 03:36:27,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:27,856][root][INFO] - Training Epoch: 2/2, step 855/7134 completed (loss: 0.03083362616598606, acc: 0.9941452145576477)
[2025-02-13 03:36:27,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:28,298][root][INFO] - Training Epoch: 2/2, step 856/7134 completed (loss: 0.017432693392038345, acc: 0.9939467310905457)
[2025-02-13 03:36:28,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:28,721][root][INFO] - Training Epoch: 2/2, step 857/7134 completed (loss: 0.03125249966979027, acc: 0.9911894202232361)
[2025-02-13 03:36:28,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:29,162][root][INFO] - Training Epoch: 2/2, step 858/7134 completed (loss: 0.04435412958264351, acc: 0.9863184094429016)
[2025-02-13 03:36:29,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:29,621][root][INFO] - Training Epoch: 2/2, step 859/7134 completed (loss: 0.04825166240334511, acc: 0.9845971465110779)
[2025-02-13 03:36:29,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:30,071][root][INFO] - Training Epoch: 2/2, step 860/7134 completed (loss: 0.03703542426228523, acc: 0.9898862242698669)
[2025-02-13 03:36:30,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:30,540][root][INFO] - Training Epoch: 2/2, step 861/7134 completed (loss: 0.07632147520780563, acc: 0.9794149398803711)
[2025-02-13 03:36:30,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:30,971][root][INFO] - Training Epoch: 2/2, step 862/7134 completed (loss: 0.03159362077713013, acc: 0.9915764331817627)
[2025-02-13 03:36:31,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:31,435][root][INFO] - Training Epoch: 2/2, step 863/7134 completed (loss: 0.025369392707943916, acc: 0.99068683385849)
[2025-02-13 03:36:31,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:31,885][root][INFO] - Training Epoch: 2/2, step 864/7134 completed (loss: 0.07349648326635361, acc: 0.9805309772491455)
[2025-02-13 03:36:32,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:32,345][root][INFO] - Training Epoch: 2/2, step 865/7134 completed (loss: 0.0495164580643177, acc: 0.9805352687835693)
[2025-02-13 03:36:32,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:32,799][root][INFO] - Training Epoch: 2/2, step 866/7134 completed (loss: 0.041450370103120804, acc: 0.9851852059364319)
[2025-02-13 03:36:32,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:33,220][root][INFO] - Training Epoch: 2/2, step 867/7134 completed (loss: 0.06352724879980087, acc: 0.9774718284606934)
[2025-02-13 03:36:33,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:33,681][root][INFO] - Training Epoch: 2/2, step 868/7134 completed (loss: 0.06627209484577179, acc: 0.985602080821991)
[2025-02-13 03:36:33,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:34,015][root][INFO] - Training Epoch: 2/2, step 869/7134 completed (loss: 0.05234100669622421, acc: 0.977707028388977)
[2025-02-13 03:36:34,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:34,452][root][INFO] - Training Epoch: 2/2, step 870/7134 completed (loss: 0.02855795808136463, acc: 0.9878706336021423)
[2025-02-13 03:36:34,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:34,870][root][INFO] - Training Epoch: 2/2, step 871/7134 completed (loss: 0.0567096509039402, acc: 0.9885993599891663)
[2025-02-13 03:36:35,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:35,294][root][INFO] - Training Epoch: 2/2, step 872/7134 completed (loss: 0.051138196140527725, acc: 0.9874411225318909)
[2025-02-13 03:36:35,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:35,738][root][INFO] - Training Epoch: 2/2, step 873/7134 completed (loss: 0.05368250608444214, acc: 0.9873239398002625)
[2025-02-13 03:36:35,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:36,195][root][INFO] - Training Epoch: 2/2, step 874/7134 completed (loss: 0.024529188871383667, acc: 0.9922958612442017)
[2025-02-13 03:36:36,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:36,614][root][INFO] - Training Epoch: 2/2, step 875/7134 completed (loss: 0.029680032283067703, acc: 0.9897511005401611)
[2025-02-13 03:36:36,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:37,049][root][INFO] - Training Epoch: 2/2, step 876/7134 completed (loss: 0.028153447434306145, acc: 0.9897660613059998)
[2025-02-13 03:36:37,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:37,504][root][INFO] - Training Epoch: 2/2, step 877/7134 completed (loss: 0.0654841959476471, acc: 0.9841059446334839)
[2025-02-13 03:36:37,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:37,935][root][INFO] - Training Epoch: 2/2, step 878/7134 completed (loss: 0.04658065736293793, acc: 0.982758641242981)
[2025-02-13 03:36:38,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:38,361][root][INFO] - Training Epoch: 2/2, step 879/7134 completed (loss: 0.07132898271083832, acc: 0.9854439496994019)
[2025-02-13 03:36:38,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:38,776][root][INFO] - Training Epoch: 2/2, step 880/7134 completed (loss: 0.011445454321801662, acc: 0.9970887899398804)
[2025-02-13 03:36:38,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:39,174][root][INFO] - Training Epoch: 2/2, step 881/7134 completed (loss: 0.013988422229886055, acc: 0.9959920048713684)
[2025-02-13 03:36:39,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:39,596][root][INFO] - Training Epoch: 2/2, step 882/7134 completed (loss: 0.02185714803636074, acc: 0.9928057789802551)
[2025-02-13 03:36:39,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:39,999][root][INFO] - Training Epoch: 2/2, step 883/7134 completed (loss: 0.05501866713166237, acc: 0.9831804037094116)
[2025-02-13 03:36:40,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:40,417][root][INFO] - Training Epoch: 2/2, step 884/7134 completed (loss: 0.03145608305931091, acc: 0.9865951538085938)
[2025-02-13 03:36:40,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:40,841][root][INFO] - Training Epoch: 2/2, step 885/7134 completed (loss: 0.005731227807700634, acc: 1.0)
[2025-02-13 03:36:40,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:41,287][root][INFO] - Training Epoch: 2/2, step 886/7134 completed (loss: 0.06901172548532486, acc: 0.9844054579734802)
[2025-02-13 03:36:41,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:41,654][root][INFO] - Training Epoch: 2/2, step 887/7134 completed (loss: 0.03277965635061264, acc: 0.9946902394294739)
[2025-02-13 03:36:41,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:42,080][root][INFO] - Training Epoch: 2/2, step 888/7134 completed (loss: 0.06196267157793045, acc: 0.9863429665565491)
[2025-02-13 03:36:42,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:42,465][root][INFO] - Training Epoch: 2/2, step 889/7134 completed (loss: 0.016781147569417953, acc: 0.9933333396911621)
[2025-02-13 03:36:42,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:42,875][root][INFO] - Training Epoch: 2/2, step 890/7134 completed (loss: 0.04889175668358803, acc: 0.9898132681846619)
[2025-02-13 03:36:43,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:43,323][root][INFO] - Training Epoch: 2/2, step 891/7134 completed (loss: 0.06303753703832626, acc: 0.9860917925834656)
[2025-02-13 03:36:43,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:43,743][root][INFO] - Training Epoch: 2/2, step 892/7134 completed (loss: 0.055811963975429535, acc: 0.9883720874786377)
[2025-02-13 03:36:43,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:44,144][root][INFO] - Training Epoch: 2/2, step 893/7134 completed (loss: 0.04110446199774742, acc: 0.9891641139984131)
[2025-02-13 03:36:44,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:44,581][root][INFO] - Training Epoch: 2/2, step 894/7134 completed (loss: 0.03759131580591202, acc: 0.9888268113136292)
[2025-02-13 03:36:44,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:44,972][root][INFO] - Training Epoch: 2/2, step 895/7134 completed (loss: 0.016663148999214172, acc: 0.9931623935699463)
[2025-02-13 03:36:45,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:45,368][root][INFO] - Training Epoch: 2/2, step 896/7134 completed (loss: 0.0286689605563879, acc: 0.9940000176429749)
[2025-02-13 03:36:45,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:45,824][root][INFO] - Training Epoch: 2/2, step 897/7134 completed (loss: 0.0317748598754406, acc: 0.9902507066726685)
[2025-02-13 03:36:45,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:46,287][root][INFO] - Training Epoch: 2/2, step 898/7134 completed (loss: 0.012008204124867916, acc: 0.9987546801567078)
[2025-02-13 03:36:46,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:46,728][root][INFO] - Training Epoch: 2/2, step 899/7134 completed (loss: 0.02996193617582321, acc: 0.9937810897827148)
[2025-02-13 03:36:46,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:47,145][root][INFO] - Training Epoch: 2/2, step 900/7134 completed (loss: 0.01806342974305153, acc: 0.9929676651954651)
[2025-02-13 03:36:47,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:47,595][root][INFO] - Training Epoch: 2/2, step 901/7134 completed (loss: 0.020946867763996124, acc: 0.9946666955947876)
[2025-02-13 03:36:47,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:47,992][root][INFO] - Training Epoch: 2/2, step 902/7134 completed (loss: 0.012692985124886036, acc: 0.9971305727958679)
[2025-02-13 03:36:48,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:48,439][root][INFO] - Training Epoch: 2/2, step 903/7134 completed (loss: 0.004018757957965136, acc: 1.0)
[2025-02-13 03:36:48,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:48,901][root][INFO] - Training Epoch: 2/2, step 904/7134 completed (loss: 0.014788302592933178, acc: 0.99609375)
[2025-02-13 03:36:49,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:49,357][root][INFO] - Training Epoch: 2/2, step 905/7134 completed (loss: 0.011875213123857975, acc: 0.9973045587539673)
[2025-02-13 03:36:49,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:49,798][root][INFO] - Training Epoch: 2/2, step 906/7134 completed (loss: 0.022348899394273758, acc: 0.9919571280479431)
[2025-02-13 03:36:49,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:50,244][root][INFO] - Training Epoch: 2/2, step 907/7134 completed (loss: 0.014210107736289501, acc: 0.9973683953285217)
[2025-02-13 03:36:50,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:50,680][root][INFO] - Training Epoch: 2/2, step 908/7134 completed (loss: 0.01647527515888214, acc: 0.9934980273246765)
[2025-02-13 03:36:50,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:51,094][root][INFO] - Training Epoch: 2/2, step 909/7134 completed (loss: 0.04064194858074188, acc: 0.9864864945411682)
[2025-02-13 03:36:51,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:51,558][root][INFO] - Training Epoch: 2/2, step 910/7134 completed (loss: 0.0313776433467865, acc: 0.9906542301177979)
[2025-02-13 03:36:51,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:52,002][root][INFO] - Training Epoch: 2/2, step 911/7134 completed (loss: 0.012842066586017609, acc: 0.9958041906356812)
[2025-02-13 03:36:52,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:52,459][root][INFO] - Training Epoch: 2/2, step 912/7134 completed (loss: 0.03550272434949875, acc: 0.9899117350578308)
[2025-02-13 03:36:52,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:52,917][root][INFO] - Training Epoch: 2/2, step 913/7134 completed (loss: 0.017554273828864098, acc: 0.9937185645103455)
[2025-02-13 03:36:53,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:53,349][root][INFO] - Training Epoch: 2/2, step 914/7134 completed (loss: 0.030385131016373634, acc: 0.9910714030265808)
[2025-02-13 03:36:53,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:53,748][root][INFO] - Training Epoch: 2/2, step 915/7134 completed (loss: 0.06947382539510727, acc: 0.9870550036430359)
[2025-02-13 03:36:53,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:54,154][root][INFO] - Training Epoch: 2/2, step 916/7134 completed (loss: 0.021709397435188293, acc: 0.9947643876075745)
[2025-02-13 03:36:54,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:54,563][root][INFO] - Training Epoch: 2/2, step 917/7134 completed (loss: 0.046138517558574677, acc: 0.9864253401756287)
[2025-02-13 03:36:54,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:54,991][root][INFO] - Training Epoch: 2/2, step 918/7134 completed (loss: 0.01671508327126503, acc: 0.9954057931900024)
[2025-02-13 03:36:55,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:55,387][root][INFO] - Training Epoch: 2/2, step 919/7134 completed (loss: 0.010542322881519794, acc: 0.9942280054092407)
[2025-02-13 03:36:55,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:55,800][root][INFO] - Training Epoch: 2/2, step 920/7134 completed (loss: 0.023962002247571945, acc: 0.9915966391563416)
[2025-02-13 03:36:55,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:56,215][root][INFO] - Training Epoch: 2/2, step 921/7134 completed (loss: 0.01041228137910366, acc: 0.9972899556159973)
[2025-02-13 03:36:56,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:56,619][root][INFO] - Training Epoch: 2/2, step 922/7134 completed (loss: 0.029823003336787224, acc: 0.9885246157646179)
[2025-02-13 03:36:56,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:57,045][root][INFO] - Training Epoch: 2/2, step 923/7134 completed (loss: 0.05836017057299614, acc: 0.9841269850730896)
[2025-02-13 03:36:57,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:57,548][root][INFO] - Training Epoch: 2/2, step 924/7134 completed (loss: 0.013484451919794083, acc: 0.9939613342285156)
[2025-02-13 03:36:57,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:58,022][root][INFO] - Training Epoch: 2/2, step 925/7134 completed (loss: 0.029432397335767746, acc: 0.9911894202232361)
[2025-02-13 03:36:58,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:58,485][root][INFO] - Training Epoch: 2/2, step 926/7134 completed (loss: 0.040311217308044434, acc: 0.9925093650817871)
[2025-02-13 03:36:58,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:58,933][root][INFO] - Training Epoch: 2/2, step 927/7134 completed (loss: 0.057232312858104706, acc: 0.9934640526771545)
[2025-02-13 03:36:59,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:59,386][root][INFO] - Training Epoch: 2/2, step 928/7134 completed (loss: 0.03235439583659172, acc: 0.990138053894043)
[2025-02-13 03:36:59,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:59,800][root][INFO] - Training Epoch: 2/2, step 929/7134 completed (loss: 0.01688254624605179, acc: 0.9952606558799744)
[2025-02-13 03:36:59,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:00,158][root][INFO] - Training Epoch: 2/2, step 930/7134 completed (loss: 0.04337277263402939, acc: 0.9858871102333069)
[2025-02-13 03:37:00,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:00,563][root][INFO] - Training Epoch: 2/2, step 931/7134 completed (loss: 0.04948988929390907, acc: 0.9876543283462524)
[2025-02-13 03:37:00,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:00,953][root][INFO] - Training Epoch: 2/2, step 932/7134 completed (loss: 0.03735174611210823, acc: 0.9885386824607849)
[2025-02-13 03:37:01,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:01,372][root][INFO] - Training Epoch: 2/2, step 933/7134 completed (loss: 0.04165099561214447, acc: 0.9901269674301147)
[2025-02-13 03:37:01,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:01,798][root][INFO] - Training Epoch: 2/2, step 934/7134 completed (loss: 0.01872580125927925, acc: 0.9934554696083069)
[2025-02-13 03:37:01,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:02,219][root][INFO] - Training Epoch: 2/2, step 935/7134 completed (loss: 0.024705184623599052, acc: 0.989983320236206)
[2025-02-13 03:37:02,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:02,647][root][INFO] - Training Epoch: 2/2, step 936/7134 completed (loss: 0.029316242784261703, acc: 0.9929701089859009)
[2025-02-13 03:37:02,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:03,080][root][INFO] - Training Epoch: 2/2, step 937/7134 completed (loss: 0.02491791546344757, acc: 0.9929577708244324)
[2025-02-13 03:37:03,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:03,511][root][INFO] - Training Epoch: 2/2, step 938/7134 completed (loss: 0.027341414242982864, acc: 0.9885386824607849)
[2025-02-13 03:37:03,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:03,916][root][INFO] - Training Epoch: 2/2, step 939/7134 completed (loss: 0.03532220795750618, acc: 0.9875389337539673)
[2025-02-13 03:37:04,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:04,376][root][INFO] - Training Epoch: 2/2, step 940/7134 completed (loss: 0.03595936670899391, acc: 0.9856938719749451)
[2025-02-13 03:37:04,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:04,837][root][INFO] - Training Epoch: 2/2, step 941/7134 completed (loss: 0.018295347690582275, acc: 0.9944827556610107)
[2025-02-13 03:37:04,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:05,268][root][INFO] - Training Epoch: 2/2, step 942/7134 completed (loss: 0.037271227687597275, acc: 0.994194507598877)
[2025-02-13 03:37:05,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:05,695][root][INFO] - Training Epoch: 2/2, step 943/7134 completed (loss: 0.019336778670549393, acc: 0.9939117431640625)
[2025-02-13 03:37:05,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:06,083][root][INFO] - Training Epoch: 2/2, step 944/7134 completed (loss: 0.028818398714065552, acc: 0.9910045266151428)
[2025-02-13 03:37:06,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:06,495][root][INFO] - Training Epoch: 2/2, step 945/7134 completed (loss: 0.04115032032132149, acc: 0.9944055676460266)
[2025-02-13 03:37:06,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:06,866][root][INFO] - Training Epoch: 2/2, step 946/7134 completed (loss: 0.007037682458758354, acc: 0.9978586435317993)
[2025-02-13 03:37:06,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:07,261][root][INFO] - Training Epoch: 2/2, step 947/7134 completed (loss: 0.03053184226155281, acc: 0.9931153059005737)
[2025-02-13 03:37:07,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:07,698][root][INFO] - Training Epoch: 2/2, step 948/7134 completed (loss: 0.02780703641474247, acc: 0.9916550517082214)
[2025-02-13 03:37:07,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:08,179][root][INFO] - Training Epoch: 2/2, step 949/7134 completed (loss: 0.02206527441740036, acc: 0.9929873943328857)
[2025-02-13 03:37:08,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:08,603][root][INFO] - Training Epoch: 2/2, step 950/7134 completed (loss: 0.02268357202410698, acc: 0.9891975522041321)
[2025-02-13 03:37:08,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:08,999][root][INFO] - Training Epoch: 2/2, step 951/7134 completed (loss: 0.01846453733742237, acc: 0.9947183132171631)
[2025-02-13 03:37:09,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:09,400][root][INFO] - Training Epoch: 2/2, step 952/7134 completed (loss: 0.04958983510732651, acc: 0.9850249290466309)
[2025-02-13 03:37:09,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:09,870][root][INFO] - Training Epoch: 2/2, step 953/7134 completed (loss: 0.05331635847687721, acc: 0.9852761030197144)
[2025-02-13 03:37:10,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:10,325][root][INFO] - Training Epoch: 2/2, step 954/7134 completed (loss: 0.04579336196184158, acc: 0.988252580165863)
[2025-02-13 03:37:10,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:10,790][root][INFO] - Training Epoch: 2/2, step 955/7134 completed (loss: 0.034613236784935, acc: 0.9906291961669922)
[2025-02-13 03:37:10,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:11,228][root][INFO] - Training Epoch: 2/2, step 956/7134 completed (loss: 0.018316609784960747, acc: 0.9939024448394775)
[2025-02-13 03:37:11,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:11,677][root][INFO] - Training Epoch: 2/2, step 957/7134 completed (loss: 0.019129682332277298, acc: 0.9920364022254944)
[2025-02-13 03:37:11,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:12,071][root][INFO] - Training Epoch: 2/2, step 958/7134 completed (loss: 0.04020240157842636, acc: 0.9908088445663452)
[2025-02-13 03:37:12,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:12,497][root][INFO] - Training Epoch: 2/2, step 959/7134 completed (loss: 0.07832034677267075, acc: 0.98128342628479)
[2025-02-13 03:37:12,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:12,917][root][INFO] - Training Epoch: 2/2, step 960/7134 completed (loss: 0.053928472101688385, acc: 0.9792060256004333)
[2025-02-13 03:37:13,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:13,345][root][INFO] - Training Epoch: 2/2, step 961/7134 completed (loss: 0.04698353260755539, acc: 0.986522912979126)
[2025-02-13 03:37:13,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:13,788][root][INFO] - Training Epoch: 2/2, step 962/7134 completed (loss: 0.09922639280557632, acc: 0.973793089389801)
[2025-02-13 03:37:13,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:14,203][root][INFO] - Training Epoch: 2/2, step 963/7134 completed (loss: 0.050938647240400314, acc: 0.9832317233085632)
[2025-02-13 03:37:14,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:14,624][root][INFO] - Training Epoch: 2/2, step 964/7134 completed (loss: 0.11872564256191254, acc: 0.9685157537460327)
[2025-02-13 03:37:14,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:15,050][root][INFO] - Training Epoch: 2/2, step 965/7134 completed (loss: 0.05641978234052658, acc: 0.9813486337661743)
[2025-02-13 03:37:15,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:15,456][root][INFO] - Training Epoch: 2/2, step 966/7134 completed (loss: 0.040294963866472244, acc: 0.9901639223098755)
[2025-02-13 03:37:15,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:15,907][root][INFO] - Training Epoch: 2/2, step 967/7134 completed (loss: 0.03546769171953201, acc: 0.9918144345283508)
[2025-02-13 03:37:16,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:16,305][root][INFO] - Training Epoch: 2/2, step 968/7134 completed (loss: 0.0971217155456543, acc: 0.9721670150756836)
[2025-02-13 03:37:16,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:16,704][root][INFO] - Training Epoch: 2/2, step 969/7134 completed (loss: 0.03464440256357193, acc: 0.9864253401756287)
[2025-02-13 03:37:16,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:17,099][root][INFO] - Training Epoch: 2/2, step 970/7134 completed (loss: 0.059473808854818344, acc: 0.9847328066825867)
[2025-02-13 03:37:17,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:17,549][root][INFO] - Training Epoch: 2/2, step 971/7134 completed (loss: 0.02824375219643116, acc: 0.9865030646324158)
[2025-02-13 03:37:17,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:18,017][root][INFO] - Training Epoch: 2/2, step 972/7134 completed (loss: 0.046313732862472534, acc: 0.9875466823577881)
[2025-02-13 03:37:18,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:18,442][root][INFO] - Training Epoch: 2/2, step 973/7134 completed (loss: 0.09837660938501358, acc: 0.971794843673706)
[2025-02-13 03:37:18,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:18,915][root][INFO] - Training Epoch: 2/2, step 974/7134 completed (loss: 0.05411474034190178, acc: 0.9851767420768738)
[2025-02-13 03:37:19,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:19,357][root][INFO] - Training Epoch: 2/2, step 975/7134 completed (loss: 0.07111090421676636, acc: 0.9868420958518982)
[2025-02-13 03:37:19,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:19,801][root][INFO] - Training Epoch: 2/2, step 976/7134 completed (loss: 0.050187673419713974, acc: 0.9844236969947815)
[2025-02-13 03:37:19,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:20,247][root][INFO] - Training Epoch: 2/2, step 977/7134 completed (loss: 0.049691151827573776, acc: 0.9870129823684692)
[2025-02-13 03:37:20,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:20,687][root][INFO] - Training Epoch: 2/2, step 978/7134 completed (loss: 0.02698422037065029, acc: 0.9927797913551331)
[2025-02-13 03:37:20,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:21,134][root][INFO] - Training Epoch: 2/2, step 979/7134 completed (loss: 0.03714434802532196, acc: 0.990641713142395)
[2025-02-13 03:37:21,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:21,566][root][INFO] - Training Epoch: 2/2, step 980/7134 completed (loss: 0.06277070939540863, acc: 0.987500011920929)
[2025-02-13 03:37:21,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:21,953][root][INFO] - Training Epoch: 2/2, step 981/7134 completed (loss: 0.10594689100980759, acc: 0.9823151230812073)
[2025-02-13 03:37:22,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:22,399][root][INFO] - Training Epoch: 2/2, step 982/7134 completed (loss: 0.03178911656141281, acc: 0.9859550595283508)
[2025-02-13 03:37:22,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:22,822][root][INFO] - Training Epoch: 2/2, step 983/7134 completed (loss: 0.055270470678806305, acc: 0.9880136847496033)
[2025-02-13 03:37:22,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:23,244][root][INFO] - Training Epoch: 2/2, step 984/7134 completed (loss: 0.04203382134437561, acc: 0.9908088445663452)
[2025-02-13 03:37:23,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:23,697][root][INFO] - Training Epoch: 2/2, step 985/7134 completed (loss: 0.035503000020980835, acc: 0.9897959232330322)
[2025-02-13 03:37:23,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:24,129][root][INFO] - Training Epoch: 2/2, step 986/7134 completed (loss: 0.016129085794091225, acc: 0.9943342804908752)
[2025-02-13 03:37:24,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:24,509][root][INFO] - Training Epoch: 2/2, step 987/7134 completed (loss: 0.02891525812447071, acc: 0.9919614195823669)
[2025-02-13 03:37:24,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:24,927][root][INFO] - Training Epoch: 2/2, step 988/7134 completed (loss: 0.09455118328332901, acc: 0.9750346541404724)
[2025-02-13 03:37:25,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:25,357][root][INFO] - Training Epoch: 2/2, step 989/7134 completed (loss: 0.06547053158283234, acc: 0.977746844291687)
[2025-02-13 03:37:25,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:25,793][root][INFO] - Training Epoch: 2/2, step 990/7134 completed (loss: 0.0451645590364933, acc: 0.9873577952384949)
[2025-02-13 03:37:25,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:26,220][root][INFO] - Training Epoch: 2/2, step 991/7134 completed (loss: 0.05517531558871269, acc: 0.9796556830406189)
[2025-02-13 03:37:26,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:26,678][root][INFO] - Training Epoch: 2/2, step 992/7134 completed (loss: 0.04065493494272232, acc: 0.9828947186470032)
[2025-02-13 03:37:26,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:27,105][root][INFO] - Training Epoch: 2/2, step 993/7134 completed (loss: 0.05051526054739952, acc: 0.9836333990097046)
[2025-02-13 03:37:27,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:27,583][root][INFO] - Training Epoch: 2/2, step 994/7134 completed (loss: 0.04387778043746948, acc: 0.9864864945411682)
[2025-02-13 03:37:27,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:28,013][root][INFO] - Training Epoch: 2/2, step 995/7134 completed (loss: 0.034526266157627106, acc: 0.9845678806304932)
[2025-02-13 03:37:28,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:28,437][root][INFO] - Training Epoch: 2/2, step 996/7134 completed (loss: 0.041040677577257156, acc: 0.9846368432044983)
[2025-02-13 03:37:28,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:28,870][root][INFO] - Training Epoch: 2/2, step 997/7134 completed (loss: 0.0318673774600029, acc: 0.9885386824607849)
[2025-02-13 03:37:29,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:29,302][root][INFO] - Training Epoch: 2/2, step 998/7134 completed (loss: 0.033985886722803116, acc: 0.9943661689758301)
[2025-02-13 03:37:29,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:29,715][root][INFO] - Training Epoch: 2/2, step 999/7134 completed (loss: 0.018111132085323334, acc: 0.9892473220825195)
[2025-02-13 03:37:29,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:30,132][root][INFO] - Training Epoch: 2/2, step 1000/7134 completed (loss: 0.028663231059908867, acc: 0.9897260069847107)
[2025-02-13 03:37:30,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:30,537][root][INFO] - Training Epoch: 2/2, step 1001/7134 completed (loss: 0.03766569122672081, acc: 0.991919219493866)
[2025-02-13 03:37:30,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:30,986][root][INFO] - Training Epoch: 2/2, step 1002/7134 completed (loss: 0.027702581137418747, acc: 0.9928673505783081)
[2025-02-13 03:37:31,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:31,430][root][INFO] - Training Epoch: 2/2, step 1003/7134 completed (loss: 0.03701256960630417, acc: 0.9910846948623657)
[2025-02-13 03:37:31,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:31,831][root][INFO] - Training Epoch: 2/2, step 1004/7134 completed (loss: 0.06447908282279968, acc: 0.983505129814148)
[2025-02-13 03:37:31,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:32,260][root][INFO] - Training Epoch: 2/2, step 1005/7134 completed (loss: 0.08409103006124496, acc: 0.9834437370300293)
[2025-02-13 03:37:32,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:32,674][root][INFO] - Training Epoch: 2/2, step 1006/7134 completed (loss: 0.0506618358194828, acc: 0.9860724210739136)
[2025-02-13 03:37:32,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:33,095][root][INFO] - Training Epoch: 2/2, step 1007/7134 completed (loss: 0.06666632741689682, acc: 0.9780219793319702)
[2025-02-13 03:37:33,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:33,531][root][INFO] - Training Epoch: 2/2, step 1008/7134 completed (loss: 0.049495890736579895, acc: 0.9872340559959412)
[2025-02-13 03:37:33,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:33,952][root][INFO] - Training Epoch: 2/2, step 1009/7134 completed (loss: 0.049437399953603745, acc: 0.9911190271377563)
[2025-02-13 03:37:34,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:34,361][root][INFO] - Training Epoch: 2/2, step 1010/7134 completed (loss: 0.03234114870429039, acc: 0.9861660003662109)
[2025-02-13 03:37:34,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:34,796][root][INFO] - Training Epoch: 2/2, step 1011/7134 completed (loss: 0.03751440718770027, acc: 0.9889349937438965)
[2025-02-13 03:37:34,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:35,223][root][INFO] - Training Epoch: 2/2, step 1012/7134 completed (loss: 0.04459477961063385, acc: 0.9872521162033081)
[2025-02-13 03:37:35,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:35,655][root][INFO] - Training Epoch: 2/2, step 1013/7134 completed (loss: 0.04867666959762573, acc: 0.984240710735321)
[2025-02-13 03:37:35,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:36,078][root][INFO] - Training Epoch: 2/2, step 1014/7134 completed (loss: 0.037223897874355316, acc: 0.9885057210922241)
[2025-02-13 03:37:36,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:36,501][root][INFO] - Training Epoch: 2/2, step 1015/7134 completed (loss: 0.043345384299755096, acc: 0.9879518151283264)
[2025-02-13 03:37:36,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:36,904][root][INFO] - Training Epoch: 2/2, step 1016/7134 completed (loss: 0.04538557305932045, acc: 0.990439772605896)
[2025-02-13 03:37:37,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:37,366][root][INFO] - Training Epoch: 2/2, step 1017/7134 completed (loss: 0.013569551520049572, acc: 0.9946308732032776)
[2025-02-13 03:37:37,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:37,794][root][INFO] - Training Epoch: 2/2, step 1018/7134 completed (loss: 0.054602254182100296, acc: 0.9868228435516357)
[2025-02-13 03:37:37,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:38,214][root][INFO] - Training Epoch: 2/2, step 1019/7134 completed (loss: 0.03790940344333649, acc: 0.9919871687889099)
[2025-02-13 03:37:38,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:38,614][root][INFO] - Training Epoch: 2/2, step 1020/7134 completed (loss: 0.00918743945658207, acc: 0.9985074400901794)
[2025-02-13 03:37:38,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:39,037][root][INFO] - Training Epoch: 2/2, step 1021/7134 completed (loss: 0.018551845103502274, acc: 0.9968152642250061)
[2025-02-13 03:37:39,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:39,483][root][INFO] - Training Epoch: 2/2, step 1022/7134 completed (loss: 0.02782982401549816, acc: 0.992094874382019)
[2025-02-13 03:37:39,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:39,915][root][INFO] - Training Epoch: 2/2, step 1023/7134 completed (loss: 0.043247777968645096, acc: 0.9909909963607788)
[2025-02-13 03:37:40,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:40,312][root][INFO] - Training Epoch: 2/2, step 1024/7134 completed (loss: 0.051763519644737244, acc: 0.985637366771698)
[2025-02-13 03:37:40,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:40,722][root][INFO] - Training Epoch: 2/2, step 1025/7134 completed (loss: 0.014937041327357292, acc: 0.9964413046836853)
[2025-02-13 03:37:40,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:41,142][root][INFO] - Training Epoch: 2/2, step 1026/7134 completed (loss: 0.048655763268470764, acc: 0.9873684048652649)
[2025-02-13 03:37:41,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:41,559][root][INFO] - Training Epoch: 2/2, step 1027/7134 completed (loss: 0.011790833435952663, acc: 0.9986467957496643)
[2025-02-13 03:37:41,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:41,986][root][INFO] - Training Epoch: 2/2, step 1028/7134 completed (loss: 0.020697511732578278, acc: 0.994020938873291)
[2025-02-13 03:37:42,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:42,436][root][INFO] - Training Epoch: 2/2, step 1029/7134 completed (loss: 0.006962564308196306, acc: 1.0)
[2025-02-13 03:37:42,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:42,867][root][INFO] - Training Epoch: 2/2, step 1030/7134 completed (loss: 0.011160509660840034, acc: 0.9973045587539673)
[2025-02-13 03:37:42,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:43,268][root][INFO] - Training Epoch: 2/2, step 1031/7134 completed (loss: 0.03762761130928993, acc: 0.9878048896789551)
[2025-02-13 03:37:43,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:43,721][root][INFO] - Training Epoch: 2/2, step 1032/7134 completed (loss: 0.03208182379603386, acc: 0.9942396283149719)
[2025-02-13 03:37:43,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:44,180][root][INFO] - Training Epoch: 2/2, step 1033/7134 completed (loss: 0.025153599679470062, acc: 0.9928264021873474)
[2025-02-13 03:37:44,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:44,630][root][INFO] - Training Epoch: 2/2, step 1034/7134 completed (loss: 0.032802991569042206, acc: 0.9935170412063599)
[2025-02-13 03:37:44,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:44,989][root][INFO] - Training Epoch: 2/2, step 1035/7134 completed (loss: 0.1343768835067749, acc: 0.9666666388511658)
[2025-02-13 03:37:45,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:45,350][root][INFO] - Training Epoch: 2/2, step 1036/7134 completed (loss: 0.12524768710136414, acc: 0.9733333587646484)
[2025-02-13 03:37:45,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:45,801][root][INFO] - Training Epoch: 2/2, step 1037/7134 completed (loss: 0.016238898038864136, acc: 0.9952380657196045)
[2025-02-13 03:37:45,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:46,248][root][INFO] - Training Epoch: 2/2, step 1038/7134 completed (loss: 0.0157737135887146, acc: 0.9966443181037903)
[2025-02-13 03:37:46,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:46,638][root][INFO] - Training Epoch: 2/2, step 1039/7134 completed (loss: 0.018137196078896523, acc: 0.9958592057228088)
[2025-02-13 03:37:46,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:47,084][root][INFO] - Training Epoch: 2/2, step 1040/7134 completed (loss: 0.00766978831961751, acc: 0.9982876777648926)
[2025-02-13 03:37:47,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:47,506][root][INFO] - Training Epoch: 2/2, step 1041/7134 completed (loss: 0.031585726886987686, acc: 0.9922118186950684)
[2025-02-13 03:37:47,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:47,951][root][INFO] - Training Epoch: 2/2, step 1042/7134 completed (loss: 0.025415871292352676, acc: 0.9884615540504456)
[2025-02-13 03:37:48,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:48,408][root][INFO] - Training Epoch: 2/2, step 1043/7134 completed (loss: 0.02201097458600998, acc: 0.9915764331817627)
[2025-02-13 03:37:48,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:48,842][root][INFO] - Training Epoch: 2/2, step 1044/7134 completed (loss: 0.030987830832600594, acc: 0.9945205450057983)
[2025-02-13 03:37:48,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:49,231][root][INFO] - Training Epoch: 2/2, step 1045/7134 completed (loss: 0.084372878074646, acc: 0.9806138873100281)
[2025-02-13 03:37:49,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:49,644][root][INFO] - Training Epoch: 2/2, step 1046/7134 completed (loss: 0.03008667379617691, acc: 0.992277979850769)
[2025-02-13 03:37:49,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:50,080][root][INFO] - Training Epoch: 2/2, step 1047/7134 completed (loss: 0.058524180203676224, acc: 0.9792592525482178)
[2025-02-13 03:37:50,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:50,586][root][INFO] - Training Epoch: 2/2, step 1048/7134 completed (loss: 0.009436517022550106, acc: 0.9956011772155762)
[2025-02-13 03:37:50,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:51,014][root][INFO] - Training Epoch: 2/2, step 1049/7134 completed (loss: 0.028409374877810478, acc: 0.9912152290344238)
[2025-02-13 03:37:51,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:51,450][root][INFO] - Training Epoch: 2/2, step 1050/7134 completed (loss: 0.03366740047931671, acc: 0.9863387942314148)
[2025-02-13 03:37:51,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:51,901][root][INFO] - Training Epoch: 2/2, step 1051/7134 completed (loss: 0.01892613060772419, acc: 0.9917808175086975)
[2025-02-13 03:37:52,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:52,314][root][INFO] - Training Epoch: 2/2, step 1052/7134 completed (loss: 0.04832196235656738, acc: 0.9822294116020203)
[2025-02-13 03:37:52,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:52,729][root][INFO] - Training Epoch: 2/2, step 1053/7134 completed (loss: 0.019440582022070885, acc: 0.9915493130683899)
[2025-02-13 03:37:52,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:53,165][root][INFO] - Training Epoch: 2/2, step 1054/7134 completed (loss: 0.021465469151735306, acc: 0.9933775067329407)
[2025-02-13 03:37:53,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:53,572][root][INFO] - Training Epoch: 2/2, step 1055/7134 completed (loss: 0.07951897382736206, acc: 0.9733924865722656)
[2025-02-13 03:37:53,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:53,996][root][INFO] - Training Epoch: 2/2, step 1056/7134 completed (loss: 0.05608176067471504, acc: 0.981574535369873)
[2025-02-13 03:37:54,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:54,424][root][INFO] - Training Epoch: 2/2, step 1057/7134 completed (loss: 0.03121795319020748, acc: 0.9884393215179443)
[2025-02-13 03:37:54,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:54,889][root][INFO] - Training Epoch: 2/2, step 1058/7134 completed (loss: 0.029677720740437508, acc: 0.9956521987915039)
[2025-02-13 03:37:55,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:55,349][root][INFO] - Training Epoch: 2/2, step 1059/7134 completed (loss: 0.04139632731676102, acc: 0.9896774291992188)
[2025-02-13 03:37:55,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:55,782][root][INFO] - Training Epoch: 2/2, step 1060/7134 completed (loss: 0.03769170120358467, acc: 0.9905362725257874)
[2025-02-13 03:37:55,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:56,246][root][INFO] - Training Epoch: 2/2, step 1061/7134 completed (loss: 0.0253320150077343, acc: 0.992732584476471)
[2025-02-13 03:37:56,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:56,657][root][INFO] - Training Epoch: 2/2, step 1062/7134 completed (loss: 0.037586014717817307, acc: 0.9847457408905029)
[2025-02-13 03:37:56,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:57,086][root][INFO] - Training Epoch: 2/2, step 1063/7134 completed (loss: 0.008429802022874355, acc: 0.9983792304992676)
[2025-02-13 03:37:57,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:57,546][root][INFO] - Training Epoch: 2/2, step 1064/7134 completed (loss: 0.02903641015291214, acc: 0.9907407164573669)
[2025-02-13 03:37:57,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:57,984][root][INFO] - Training Epoch: 2/2, step 1065/7134 completed (loss: 0.016960347071290016, acc: 0.9945799708366394)
[2025-02-13 03:37:58,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:58,410][root][INFO] - Training Epoch: 2/2, step 1066/7134 completed (loss: 0.048169322311878204, acc: 0.9832214713096619)
[2025-02-13 03:37:58,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:58,824][root][INFO] - Training Epoch: 2/2, step 1067/7134 completed (loss: 0.0412011593580246, acc: 0.9892141819000244)
[2025-02-13 03:37:58,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:59,236][root][INFO] - Training Epoch: 2/2, step 1068/7134 completed (loss: 0.03647078573703766, acc: 0.9868667721748352)
[2025-02-13 03:37:59,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:59,659][root][INFO] - Training Epoch: 2/2, step 1069/7134 completed (loss: 0.03947441652417183, acc: 0.9849749803543091)
[2025-02-13 03:37:59,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:00,053][root][INFO] - Training Epoch: 2/2, step 1070/7134 completed (loss: 0.02688111551105976, acc: 0.9901477694511414)
[2025-02-13 03:38:00,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:00,504][root][INFO] - Training Epoch: 2/2, step 1071/7134 completed (loss: 0.012468220666050911, acc: 0.9946452379226685)
[2025-02-13 03:38:00,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:00,947][root][INFO] - Training Epoch: 2/2, step 1072/7134 completed (loss: 0.003257300704717636, acc: 1.0)
[2025-02-13 03:38:01,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:01,372][root][INFO] - Training Epoch: 2/2, step 1073/7134 completed (loss: 0.011564385145902634, acc: 0.9942362904548645)
[2025-02-13 03:38:01,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:01,775][root][INFO] - Training Epoch: 2/2, step 1074/7134 completed (loss: 0.05113743990659714, acc: 0.9861830472946167)
[2025-02-13 03:38:01,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:02,217][root][INFO] - Training Epoch: 2/2, step 1075/7134 completed (loss: 0.0620177760720253, acc: 0.9815837740898132)
[2025-02-13 03:38:02,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:02,664][root][INFO] - Training Epoch: 2/2, step 1076/7134 completed (loss: 0.07453679293394089, acc: 0.9760638475418091)
[2025-02-13 03:38:02,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:03,061][root][INFO] - Training Epoch: 2/2, step 1077/7134 completed (loss: 0.06164805218577385, acc: 0.9729729890823364)
[2025-02-13 03:38:03,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:03,507][root][INFO] - Training Epoch: 2/2, step 1078/7134 completed (loss: 0.07747118175029755, acc: 0.9822294116020203)
[2025-02-13 03:38:03,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:03,952][root][INFO] - Training Epoch: 2/2, step 1079/7134 completed (loss: 0.02238866128027439, acc: 0.9930796027183533)
[2025-02-13 03:38:04,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:04,343][root][INFO] - Training Epoch: 2/2, step 1080/7134 completed (loss: 0.04728802293539047, acc: 0.9943609237670898)
[2025-02-13 03:38:04,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:04,779][root][INFO] - Training Epoch: 2/2, step 1081/7134 completed (loss: 0.01563052460551262, acc: 0.9943898916244507)
[2025-02-13 03:38:04,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:05,274][root][INFO] - Training Epoch: 2/2, step 1082/7134 completed (loss: 0.013025878928601742, acc: 0.9969167709350586)
[2025-02-13 03:38:05,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:05,737][root][INFO] - Training Epoch: 2/2, step 1083/7134 completed (loss: 0.022536197677254677, acc: 0.9909604787826538)
[2025-02-13 03:38:05,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:06,194][root][INFO] - Training Epoch: 2/2, step 1084/7134 completed (loss: 0.02400020882487297, acc: 0.9913259148597717)
[2025-02-13 03:38:06,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:06,627][root][INFO] - Training Epoch: 2/2, step 1085/7134 completed (loss: 0.032204147428274155, acc: 0.9906291961669922)
[2025-02-13 03:38:06,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:07,029][root][INFO] - Training Epoch: 2/2, step 1086/7134 completed (loss: 0.04850633442401886, acc: 0.9848771095275879)
[2025-02-13 03:38:07,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:07,485][root][INFO] - Training Epoch: 2/2, step 1087/7134 completed (loss: 0.021700482815504074, acc: 0.9914634227752686)
[2025-02-13 03:38:07,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:07,909][root][INFO] - Training Epoch: 2/2, step 1088/7134 completed (loss: 0.024823764339089394, acc: 0.99301677942276)
[2025-02-13 03:38:08,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:08,330][root][INFO] - Training Epoch: 2/2, step 1089/7134 completed (loss: 0.04180153086781502, acc: 0.9910581111907959)
[2025-02-13 03:38:08,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:08,761][root][INFO] - Training Epoch: 2/2, step 1090/7134 completed (loss: 0.030902573838829994, acc: 0.9875195026397705)
[2025-02-13 03:38:08,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:09,175][root][INFO] - Training Epoch: 2/2, step 1091/7134 completed (loss: 0.016472969204187393, acc: 0.9954338073730469)
[2025-02-13 03:38:09,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:09,624][root][INFO] - Training Epoch: 2/2, step 1092/7134 completed (loss: 0.009985954500734806, acc: 0.9971098303794861)
[2025-02-13 03:38:09,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:10,051][root][INFO] - Training Epoch: 2/2, step 1093/7134 completed (loss: 0.09633532166481018, acc: 0.9717868566513062)
[2025-02-13 03:38:10,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:10,478][root][INFO] - Training Epoch: 2/2, step 1094/7134 completed (loss: 0.061808645725250244, acc: 0.9832713603973389)
[2025-02-13 03:38:10,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:10,954][root][INFO] - Training Epoch: 2/2, step 1095/7134 completed (loss: 0.03862395137548447, acc: 0.9878197312355042)
[2025-02-13 03:38:11,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:11,425][root][INFO] - Training Epoch: 2/2, step 1096/7134 completed (loss: 0.02478565275669098, acc: 0.9949367046356201)
[2025-02-13 03:38:11,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:11,875][root][INFO] - Training Epoch: 2/2, step 1097/7134 completed (loss: 0.07369089871644974, acc: 0.9845505356788635)
[2025-02-13 03:38:12,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:12,308][root][INFO] - Training Epoch: 2/2, step 1098/7134 completed (loss: 0.020538801327347755, acc: 0.9925834536552429)
[2025-02-13 03:38:12,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:12,758][root][INFO] - Training Epoch: 2/2, step 1099/7134 completed (loss: 0.03185180574655533, acc: 0.9910256266593933)
[2025-02-13 03:38:12,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:13,217][root][INFO] - Training Epoch: 2/2, step 1100/7134 completed (loss: 0.037125490605831146, acc: 0.98777174949646)
[2025-02-13 03:38:13,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:13,670][root][INFO] - Training Epoch: 2/2, step 1101/7134 completed (loss: 0.023114535957574844, acc: 0.9961038827896118)
[2025-02-13 03:38:13,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:14,088][root][INFO] - Training Epoch: 2/2, step 1102/7134 completed (loss: 0.04863164573907852, acc: 0.9843444228172302)
[2025-02-13 03:38:14,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:14,547][root][INFO] - Training Epoch: 2/2, step 1103/7134 completed (loss: 0.06953328847885132, acc: 0.984000027179718)
[2025-02-13 03:38:14,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:14,964][root][INFO] - Training Epoch: 2/2, step 1104/7134 completed (loss: 0.030438348650932312, acc: 0.9927954077720642)
[2025-02-13 03:38:15,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:15,389][root][INFO] - Training Epoch: 2/2, step 1105/7134 completed (loss: 0.03227178752422333, acc: 0.9905660152435303)
[2025-02-13 03:38:15,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:15,805][root][INFO] - Training Epoch: 2/2, step 1106/7134 completed (loss: 0.042290907353162766, acc: 0.9848484992980957)
[2025-02-13 03:38:15,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:16,278][root][INFO] - Training Epoch: 2/2, step 1107/7134 completed (loss: 0.028359107673168182, acc: 0.991793692111969)
[2025-02-13 03:38:16,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:16,722][root][INFO] - Training Epoch: 2/2, step 1108/7134 completed (loss: 0.012129583396017551, acc: 1.0)
[2025-02-13 03:38:16,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:17,176][root][INFO] - Training Epoch: 2/2, step 1109/7134 completed (loss: 0.030850427225232124, acc: 0.9916201233863831)
[2025-02-13 03:38:17,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:17,602][root][INFO] - Training Epoch: 2/2, step 1110/7134 completed (loss: 0.04568561539053917, acc: 0.9868938326835632)
[2025-02-13 03:38:17,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:18,026][root][INFO] - Training Epoch: 2/2, step 1111/7134 completed (loss: 0.048158615827560425, acc: 0.9834254384040833)
[2025-02-13 03:38:18,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:18,415][root][INFO] - Training Epoch: 2/2, step 1112/7134 completed (loss: 0.034894004464149475, acc: 0.9882352948188782)
[2025-02-13 03:38:18,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:18,847][root][INFO] - Training Epoch: 2/2, step 1113/7134 completed (loss: 0.02530078776180744, acc: 0.9897040128707886)
[2025-02-13 03:38:18,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:19,297][root][INFO] - Training Epoch: 2/2, step 1114/7134 completed (loss: 0.04225575551390648, acc: 0.9878493547439575)
[2025-02-13 03:38:19,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:19,725][root][INFO] - Training Epoch: 2/2, step 1115/7134 completed (loss: 0.03165791928768158, acc: 0.9921383857727051)
[2025-02-13 03:38:19,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:20,126][root][INFO] - Training Epoch: 2/2, step 1116/7134 completed (loss: 0.013863285072147846, acc: 0.996268630027771)
[2025-02-13 03:38:20,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:20,537][root][INFO] - Training Epoch: 2/2, step 1117/7134 completed (loss: 0.03824220970273018, acc: 0.9892638325691223)
[2025-02-13 03:38:20,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:20,930][root][INFO] - Training Epoch: 2/2, step 1118/7134 completed (loss: 0.015184219926595688, acc: 0.9948275685310364)
[2025-02-13 03:38:21,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:21,335][root][INFO] - Training Epoch: 2/2, step 1119/7134 completed (loss: 0.035532984882593155, acc: 0.9873617887496948)
[2025-02-13 03:38:21,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:21,757][root][INFO] - Training Epoch: 2/2, step 1120/7134 completed (loss: 0.034505560994148254, acc: 0.9827044010162354)
[2025-02-13 03:38:21,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:22,173][root][INFO] - Training Epoch: 2/2, step 1121/7134 completed (loss: 0.03941019997000694, acc: 0.9867060780525208)
[2025-02-13 03:38:22,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:22,545][root][INFO] - Training Epoch: 2/2, step 1122/7134 completed (loss: 0.03481651097536087, acc: 0.9883494973182678)
[2025-02-13 03:38:22,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:22,959][root][INFO] - Training Epoch: 2/2, step 1123/7134 completed (loss: 0.016507433727383614, acc: 0.9935794472694397)
[2025-02-13 03:38:23,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:23,390][root][INFO] - Training Epoch: 2/2, step 1124/7134 completed (loss: 0.02182801254093647, acc: 0.9922600388526917)
[2025-02-13 03:38:23,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:23,821][root][INFO] - Training Epoch: 2/2, step 1125/7134 completed (loss: 0.01681225746870041, acc: 0.9932340979576111)
[2025-02-13 03:38:23,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:24,243][root][INFO] - Training Epoch: 2/2, step 1126/7134 completed (loss: 0.028350137174129486, acc: 0.9903314709663391)
[2025-02-13 03:38:24,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:24,660][root][INFO] - Training Epoch: 2/2, step 1127/7134 completed (loss: 0.0205091405659914, acc: 0.9919999837875366)
[2025-02-13 03:38:24,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:25,077][root][INFO] - Training Epoch: 2/2, step 1128/7134 completed (loss: 0.030396848917007446, acc: 0.9878472089767456)
[2025-02-13 03:38:25,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:25,456][root][INFO] - Training Epoch: 2/2, step 1129/7134 completed (loss: 0.04178471863269806, acc: 0.9865471124649048)
[2025-02-13 03:38:25,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:25,900][root][INFO] - Training Epoch: 2/2, step 1130/7134 completed (loss: 0.020491886883974075, acc: 0.993537962436676)
[2025-02-13 03:38:26,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:26,375][root][INFO] - Training Epoch: 2/2, step 1131/7134 completed (loss: 0.03939121961593628, acc: 0.9882075190544128)
[2025-02-13 03:38:26,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:26,787][root][INFO] - Training Epoch: 2/2, step 1132/7134 completed (loss: 0.09187186509370804, acc: 0.9785605072975159)
[2025-02-13 03:38:26,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:27,248][root][INFO] - Training Epoch: 2/2, step 1133/7134 completed (loss: 0.09899061918258667, acc: 0.9771754741668701)
[2025-02-13 03:38:27,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:27,685][root][INFO] - Training Epoch: 2/2, step 1134/7134 completed (loss: 0.06559618562459946, acc: 0.9788029789924622)
[2025-02-13 03:38:27,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:28,089][root][INFO] - Training Epoch: 2/2, step 1135/7134 completed (loss: 0.06428676843643188, acc: 0.9824000000953674)
[2025-02-13 03:38:28,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:28,541][root][INFO] - Training Epoch: 2/2, step 1136/7134 completed (loss: 0.06477232277393341, acc: 0.9850339889526367)
[2025-02-13 03:38:28,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:28,867][root][INFO] - Training Epoch: 2/2, step 1137/7134 completed (loss: 0.04378849267959595, acc: 0.9840909242630005)
[2025-02-13 03:38:29,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:29,342][root][INFO] - Training Epoch: 2/2, step 1138/7134 completed (loss: 0.03796035796403885, acc: 0.9904371500015259)
[2025-02-13 03:38:29,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:29,752][root][INFO] - Training Epoch: 2/2, step 1139/7134 completed (loss: 0.06549844145774841, acc: 0.9780775904655457)
[2025-02-13 03:38:29,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:30,174][root][INFO] - Training Epoch: 2/2, step 1140/7134 completed (loss: 0.02076311782002449, acc: 0.9924812316894531)
[2025-02-13 03:38:30,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:30,600][root][INFO] - Training Epoch: 2/2, step 1141/7134 completed (loss: 0.024343155324459076, acc: 0.9929742217063904)
[2025-02-13 03:38:30,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:31,043][root][INFO] - Training Epoch: 2/2, step 1142/7134 completed (loss: 0.04785565659403801, acc: 0.9934895634651184)
[2025-02-13 03:38:31,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:31,457][root][INFO] - Training Epoch: 2/2, step 1143/7134 completed (loss: 0.06648588180541992, acc: 0.9819193482398987)
[2025-02-13 03:38:31,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:31,899][root][INFO] - Training Epoch: 2/2, step 1144/7134 completed (loss: 0.06551013141870499, acc: 0.9825000166893005)
[2025-02-13 03:38:32,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:32,346][root][INFO] - Training Epoch: 2/2, step 1145/7134 completed (loss: 0.032066550105810165, acc: 0.9952038526535034)
[2025-02-13 03:38:32,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:32,756][root][INFO] - Training Epoch: 2/2, step 1146/7134 completed (loss: 0.03223951533436775, acc: 0.9909420013427734)
[2025-02-13 03:38:32,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:33,177][root][INFO] - Training Epoch: 2/2, step 1147/7134 completed (loss: 0.018642153590917587, acc: 0.996259331703186)
[2025-02-13 03:38:33,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:33,602][root][INFO] - Training Epoch: 2/2, step 1148/7134 completed (loss: 0.04956934228539467, acc: 0.9846416115760803)
[2025-02-13 03:38:33,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:34,007][root][INFO] - Training Epoch: 2/2, step 1149/7134 completed (loss: 0.07406782358884811, acc: 0.9714285731315613)
[2025-02-13 03:38:34,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:34,455][root][INFO] - Training Epoch: 2/2, step 1150/7134 completed (loss: 0.024795029312372208, acc: 0.9968992471694946)
[2025-02-13 03:38:34,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:34,867][root][INFO] - Training Epoch: 2/2, step 1151/7134 completed (loss: 0.008932195603847504, acc: 1.0)
[2025-02-13 03:38:35,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:35,291][root][INFO] - Training Epoch: 2/2, step 1152/7134 completed (loss: 0.02313758060336113, acc: 0.9914529919624329)
[2025-02-13 03:38:35,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:35,729][root][INFO] - Training Epoch: 2/2, step 1153/7134 completed (loss: 0.07201778143644333, acc: 0.9822888374328613)
[2025-02-13 03:38:35,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:36,145][root][INFO] - Training Epoch: 2/2, step 1154/7134 completed (loss: 0.04853098466992378, acc: 0.991150438785553)
[2025-02-13 03:38:36,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:36,573][root][INFO] - Training Epoch: 2/2, step 1155/7134 completed (loss: 0.05542125180363655, acc: 0.9758771657943726)
[2025-02-13 03:38:36,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:37,024][root][INFO] - Training Epoch: 2/2, step 1156/7134 completed (loss: 0.01286298967897892, acc: 0.996927797794342)
[2025-02-13 03:38:37,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:37,442][root][INFO] - Training Epoch: 2/2, step 1157/7134 completed (loss: 0.04618744179606438, acc: 0.9915110468864441)
[2025-02-13 03:38:37,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:37,849][root][INFO] - Training Epoch: 2/2, step 1158/7134 completed (loss: 0.03485919162631035, acc: 0.9921104311943054)
[2025-02-13 03:38:37,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:38,295][root][INFO] - Training Epoch: 2/2, step 1159/7134 completed (loss: 0.04984140023589134, acc: 0.9843971729278564)
[2025-02-13 03:38:38,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:38,745][root][INFO] - Training Epoch: 2/2, step 1160/7134 completed (loss: 0.01935507170855999, acc: 0.9955357313156128)
[2025-02-13 03:38:38,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:39,170][root][INFO] - Training Epoch: 2/2, step 1161/7134 completed (loss: 0.08600056171417236, acc: 0.9820554852485657)
[2025-02-13 03:38:39,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:39,576][root][INFO] - Training Epoch: 2/2, step 1162/7134 completed (loss: 0.023110752925276756, acc: 0.9917355179786682)
[2025-02-13 03:38:39,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:39,977][root][INFO] - Training Epoch: 2/2, step 1163/7134 completed (loss: 0.0400364026427269, acc: 0.9914675951004028)
[2025-02-13 03:38:40,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:40,399][root][INFO] - Training Epoch: 2/2, step 1164/7134 completed (loss: 0.045829612761735916, acc: 0.9930459260940552)
[2025-02-13 03:38:40,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:40,869][root][INFO] - Training Epoch: 2/2, step 1165/7134 completed (loss: 0.026440106332302094, acc: 0.9959294199943542)
[2025-02-13 03:38:41,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:41,291][root][INFO] - Training Epoch: 2/2, step 1166/7134 completed (loss: 0.016894742846488953, acc: 0.9958847761154175)
[2025-02-13 03:38:41,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:41,708][root][INFO] - Training Epoch: 2/2, step 1167/7134 completed (loss: 0.01201001275330782, acc: 0.9960707426071167)
[2025-02-13 03:38:41,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:42,175][root][INFO] - Training Epoch: 2/2, step 1168/7134 completed (loss: 0.010218274779617786, acc: 0.996052622795105)
[2025-02-13 03:38:42,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:42,613][root][INFO] - Training Epoch: 2/2, step 1169/7134 completed (loss: 0.03316126763820648, acc: 0.9878378510475159)
[2025-02-13 03:38:42,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:43,050][root][INFO] - Training Epoch: 2/2, step 1170/7134 completed (loss: 0.03729948401451111, acc: 0.9915730357170105)
[2025-02-13 03:38:43,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:43,407][root][INFO] - Training Epoch: 2/2, step 1171/7134 completed (loss: 0.016777798533439636, acc: 0.9916527271270752)
[2025-02-13 03:38:43,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:43,825][root][INFO] - Training Epoch: 2/2, step 1172/7134 completed (loss: 0.03245120123028755, acc: 0.9878970980644226)
[2025-02-13 03:38:43,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:44,249][root][INFO] - Training Epoch: 2/2, step 1173/7134 completed (loss: 0.03399374708533287, acc: 0.9927536249160767)
[2025-02-13 03:38:44,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:44,664][root][INFO] - Training Epoch: 2/2, step 1174/7134 completed (loss: 0.0145967872813344, acc: 0.9969969987869263)
[2025-02-13 03:38:44,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:45,101][root][INFO] - Training Epoch: 2/2, step 1175/7134 completed (loss: 0.019219033420085907, acc: 0.9942029118537903)
[2025-02-13 03:38:45,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:45,523][root][INFO] - Training Epoch: 2/2, step 1176/7134 completed (loss: 0.005010133143514395, acc: 0.9982876777648926)
[2025-02-13 03:38:45,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:45,920][root][INFO] - Training Epoch: 2/2, step 1177/7134 completed (loss: 0.010106991045176983, acc: 0.998039186000824)
[2025-02-13 03:38:46,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:46,276][root][INFO] - Training Epoch: 2/2, step 1178/7134 completed (loss: 0.02371852472424507, acc: 0.9929906725883484)
[2025-02-13 03:38:46,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:46,698][root][INFO] - Training Epoch: 2/2, step 1179/7134 completed (loss: 0.017324406653642654, acc: 0.9961758852005005)
[2025-02-13 03:38:46,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:47,157][root][INFO] - Training Epoch: 2/2, step 1180/7134 completed (loss: 0.009694371372461319, acc: 0.9963503479957581)
[2025-02-13 03:38:47,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:47,583][root][INFO] - Training Epoch: 2/2, step 1181/7134 completed (loss: 0.028525900095701218, acc: 0.9913669228553772)
[2025-02-13 03:38:47,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:47,985][root][INFO] - Training Epoch: 2/2, step 1182/7134 completed (loss: 0.003610474057495594, acc: 1.0)
[2025-02-13 03:38:48,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:48,434][root][INFO] - Training Epoch: 2/2, step 1183/7134 completed (loss: 0.01696394942700863, acc: 0.9941262602806091)
[2025-02-13 03:38:48,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:48,858][root][INFO] - Training Epoch: 2/2, step 1184/7134 completed (loss: 0.04241512343287468, acc: 0.991416335105896)
[2025-02-13 03:38:48,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:49,269][root][INFO] - Training Epoch: 2/2, step 1185/7134 completed (loss: 0.007952721789479256, acc: 0.9984962344169617)
[2025-02-13 03:38:49,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:49,695][root][INFO] - Training Epoch: 2/2, step 1186/7134 completed (loss: 0.02103469893336296, acc: 0.9987030029296875)
[2025-02-13 03:38:49,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:50,123][root][INFO] - Training Epoch: 2/2, step 1187/7134 completed (loss: 0.02373930811882019, acc: 0.9930747747421265)
[2025-02-13 03:38:50,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:50,564][root][INFO] - Training Epoch: 2/2, step 1188/7134 completed (loss: 0.013367262668907642, acc: 0.9973614811897278)
[2025-02-13 03:38:50,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:50,991][root][INFO] - Training Epoch: 2/2, step 1189/7134 completed (loss: 0.01150570809841156, acc: 0.9971098303794861)
[2025-02-13 03:38:51,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:51,402][root][INFO] - Training Epoch: 2/2, step 1190/7134 completed (loss: 0.01782311126589775, acc: 0.9949109554290771)
[2025-02-13 03:38:51,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:51,831][root][INFO] - Training Epoch: 2/2, step 1191/7134 completed (loss: 0.03371042013168335, acc: 0.9911373853683472)
[2025-02-13 03:38:51,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:52,270][root][INFO] - Training Epoch: 2/2, step 1192/7134 completed (loss: 0.012506573460996151, acc: 0.9960681796073914)
[2025-02-13 03:38:52,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:52,700][root][INFO] - Training Epoch: 2/2, step 1193/7134 completed (loss: 0.04970019683241844, acc: 0.9904305934906006)
[2025-02-13 03:38:52,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:53,124][root][INFO] - Training Epoch: 2/2, step 1194/7134 completed (loss: 0.03973333537578583, acc: 0.9903846383094788)
[2025-02-13 03:38:53,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:53,555][root][INFO] - Training Epoch: 2/2, step 1195/7134 completed (loss: 0.02958609163761139, acc: 0.9885621070861816)
[2025-02-13 03:38:53,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:54,000][root][INFO] - Training Epoch: 2/2, step 1196/7134 completed (loss: 0.056083329021930695, acc: 0.9852458834648132)
[2025-02-13 03:38:54,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:54,410][root][INFO] - Training Epoch: 2/2, step 1197/7134 completed (loss: 0.031217074021697044, acc: 0.9897360801696777)
[2025-02-13 03:38:54,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:54,834][root][INFO] - Training Epoch: 2/2, step 1198/7134 completed (loss: 0.025891093537211418, acc: 0.9913669228553772)
[2025-02-13 03:38:54,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:55,297][root][INFO] - Training Epoch: 2/2, step 1199/7134 completed (loss: 0.016883675009012222, acc: 0.9955817461013794)
[2025-02-13 03:38:55,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:55,721][root][INFO] - Training Epoch: 2/2, step 1200/7134 completed (loss: 0.03876693174242973, acc: 0.9897330403327942)
[2025-02-13 03:38:55,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:56,179][root][INFO] - Training Epoch: 2/2, step 1201/7134 completed (loss: 0.02903576008975506, acc: 0.9926362037658691)
[2025-02-13 03:38:56,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:56,641][root][INFO] - Training Epoch: 2/2, step 1202/7134 completed (loss: 0.0390111543238163, acc: 0.9892328381538391)
[2025-02-13 03:38:56,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:57,046][root][INFO] - Training Epoch: 2/2, step 1203/7134 completed (loss: 0.00833592563867569, acc: 0.9954476356506348)
[2025-02-13 03:38:57,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:57,457][root][INFO] - Training Epoch: 2/2, step 1204/7134 completed (loss: 0.030652383342385292, acc: 0.9906716346740723)
[2025-02-13 03:38:57,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:57,898][root][INFO] - Training Epoch: 2/2, step 1205/7134 completed (loss: 0.07615005970001221, acc: 0.9816901683807373)
[2025-02-13 03:38:58,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:58,351][root][INFO] - Training Epoch: 2/2, step 1206/7134 completed (loss: 0.02722998894751072, acc: 0.9913151264190674)
[2025-02-13 03:38:58,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:58,807][root][INFO] - Training Epoch: 2/2, step 1207/7134 completed (loss: 0.04015118256211281, acc: 0.9886934757232666)
[2025-02-13 03:38:58,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:59,227][root][INFO] - Training Epoch: 2/2, step 1208/7134 completed (loss: 0.04280718043446541, acc: 0.9850746393203735)
[2025-02-13 03:38:59,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:59,648][root][INFO] - Training Epoch: 2/2, step 1209/7134 completed (loss: 0.09129894524812698, acc: 0.9798902869224548)
[2025-02-13 03:38:59,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:00,073][root][INFO] - Training Epoch: 2/2, step 1210/7134 completed (loss: 0.0625331774353981, acc: 0.9776714444160461)
[2025-02-13 03:39:00,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:00,490][root][INFO] - Training Epoch: 2/2, step 1211/7134 completed (loss: 0.09822925925254822, acc: 0.9614643454551697)
[2025-02-13 03:39:00,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:00,900][root][INFO] - Training Epoch: 2/2, step 1212/7134 completed (loss: 0.08912624418735504, acc: 0.980182945728302)
[2025-02-13 03:39:01,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:01,348][root][INFO] - Training Epoch: 2/2, step 1213/7134 completed (loss: 0.024398986250162125, acc: 0.9900990128517151)
[2025-02-13 03:39:01,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:01,766][root][INFO] - Training Epoch: 2/2, step 1214/7134 completed (loss: 0.05979064851999283, acc: 0.9815384745597839)
[2025-02-13 03:39:01,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:02,211][root][INFO] - Training Epoch: 2/2, step 1215/7134 completed (loss: 0.07696110010147095, acc: 0.9820689558982849)
[2025-02-13 03:39:02,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:02,651][root][INFO] - Training Epoch: 2/2, step 1216/7134 completed (loss: 0.029608970507979393, acc: 0.9941605925559998)
[2025-02-13 03:39:02,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:03,111][root][INFO] - Training Epoch: 2/2, step 1217/7134 completed (loss: 0.04191069304943085, acc: 0.9886363744735718)
[2025-02-13 03:39:03,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:03,575][root][INFO] - Training Epoch: 2/2, step 1218/7134 completed (loss: 0.09341614693403244, acc: 0.9805970191955566)
[2025-02-13 03:39:03,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:04,037][root][INFO] - Training Epoch: 2/2, step 1219/7134 completed (loss: 0.07730643451213837, acc: 0.9806678295135498)
[2025-02-13 03:39:04,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:04,501][root][INFO] - Training Epoch: 2/2, step 1220/7134 completed (loss: 0.07170513272285461, acc: 0.9835025668144226)
[2025-02-13 03:39:04,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:04,992][root][INFO] - Training Epoch: 2/2, step 1221/7134 completed (loss: 0.06546720862388611, acc: 0.9857456088066101)
[2025-02-13 03:39:05,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:05,448][root][INFO] - Training Epoch: 2/2, step 1222/7134 completed (loss: 0.059201065450906754, acc: 0.9856972694396973)
[2025-02-13 03:39:05,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:05,901][root][INFO] - Training Epoch: 2/2, step 1223/7134 completed (loss: 0.1427278071641922, acc: 0.9766803979873657)
[2025-02-13 03:39:06,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:06,367][root][INFO] - Training Epoch: 2/2, step 1224/7134 completed (loss: 0.04757923260331154, acc: 0.9911699891090393)
[2025-02-13 03:39:06,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:06,834][root][INFO] - Training Epoch: 2/2, step 1225/7134 completed (loss: 0.055343665182590485, acc: 0.9841089844703674)
[2025-02-13 03:39:06,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:07,283][root][INFO] - Training Epoch: 2/2, step 1226/7134 completed (loss: 0.10938501358032227, acc: 0.9718309640884399)
[2025-02-13 03:39:07,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:07,728][root][INFO] - Training Epoch: 2/2, step 1227/7134 completed (loss: 0.07500258833169937, acc: 0.9758713245391846)
[2025-02-13 03:39:07,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:08,168][root][INFO] - Training Epoch: 2/2, step 1228/7134 completed (loss: 0.08695574849843979, acc: 0.9799714088439941)
[2025-02-13 03:39:08,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:08,616][root][INFO] - Training Epoch: 2/2, step 1229/7134 completed (loss: 0.03344034031033516, acc: 0.9939975738525391)
[2025-02-13 03:39:08,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:09,087][root][INFO] - Training Epoch: 2/2, step 1230/7134 completed (loss: 0.08462414890527725, acc: 0.9807909727096558)
[2025-02-13 03:39:09,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:09,586][root][INFO] - Training Epoch: 2/2, step 1231/7134 completed (loss: 0.051027506589889526, acc: 0.988624632358551)
[2025-02-13 03:39:09,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:10,073][root][INFO] - Training Epoch: 2/2, step 1232/7134 completed (loss: 0.06715678423643112, acc: 0.9850904941558838)
[2025-02-13 03:39:10,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:10,533][root][INFO] - Training Epoch: 2/2, step 1233/7134 completed (loss: 0.11197896301746368, acc: 0.9722543358802795)
[2025-02-13 03:39:10,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:10,993][root][INFO] - Training Epoch: 2/2, step 1234/7134 completed (loss: 0.0315362885594368, acc: 0.9922360181808472)
[2025-02-13 03:39:11,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:11,454][root][INFO] - Training Epoch: 2/2, step 1235/7134 completed (loss: 0.07212001085281372, acc: 0.9844124913215637)
[2025-02-13 03:39:11,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:11,911][root][INFO] - Training Epoch: 2/2, step 1236/7134 completed (loss: 0.11120437830686569, acc: 0.9736456871032715)
[2025-02-13 03:39:12,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:12,364][root][INFO] - Training Epoch: 2/2, step 1237/7134 completed (loss: 0.07100991904735565, acc: 0.97826087474823)
[2025-02-13 03:39:12,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:12,815][root][INFO] - Training Epoch: 2/2, step 1238/7134 completed (loss: 0.11920732259750366, acc: 0.9650654792785645)
[2025-02-13 03:39:12,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:13,267][root][INFO] - Training Epoch: 2/2, step 1239/7134 completed (loss: 0.0850968062877655, acc: 0.9713876843452454)
[2025-02-13 03:39:13,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:13,729][root][INFO] - Training Epoch: 2/2, step 1240/7134 completed (loss: 0.06740614026784897, acc: 0.9828042387962341)
[2025-02-13 03:39:13,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:14,007][root][INFO] - Training Epoch: 2/2, step 1241/7134 completed (loss: 0.03579946979880333, acc: 0.9839357137680054)
[2025-02-13 03:39:14,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:14,475][root][INFO] - Training Epoch: 2/2, step 1242/7134 completed (loss: 0.04353789985179901, acc: 0.9903846383094788)
[2025-02-13 03:39:14,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:14,952][root][INFO] - Training Epoch: 2/2, step 1243/7134 completed (loss: 0.056680355221033096, acc: 0.9853556752204895)
[2025-02-13 03:39:15,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:15,400][root][INFO] - Training Epoch: 2/2, step 1244/7134 completed (loss: 0.08832161128520966, acc: 0.9850993156433105)
[2025-02-13 03:39:15,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:15,872][root][INFO] - Training Epoch: 2/2, step 1245/7134 completed (loss: 0.05952765792608261, acc: 0.982367753982544)
[2025-02-13 03:39:16,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:16,294][root][INFO] - Training Epoch: 2/2, step 1246/7134 completed (loss: 0.09187500178813934, acc: 0.9794608354568481)
[2025-02-13 03:39:16,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:16,736][root][INFO] - Training Epoch: 2/2, step 1247/7134 completed (loss: 0.03796780854463577, acc: 0.9884225726127625)
[2025-02-13 03:39:16,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:17,161][root][INFO] - Training Epoch: 2/2, step 1248/7134 completed (loss: 0.07653525471687317, acc: 0.9771167039871216)
[2025-02-13 03:39:17,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:17,575][root][INFO] - Training Epoch: 2/2, step 1249/7134 completed (loss: 0.026701858267188072, acc: 0.9946619272232056)
[2025-02-13 03:39:17,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:17,990][root][INFO] - Training Epoch: 2/2, step 1250/7134 completed (loss: 0.02851535938680172, acc: 0.9886685609817505)
[2025-02-13 03:39:18,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:18,418][root][INFO] - Training Epoch: 2/2, step 1251/7134 completed (loss: 0.02312173880636692, acc: 0.991769552230835)
[2025-02-13 03:39:18,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:18,839][root][INFO] - Training Epoch: 2/2, step 1252/7134 completed (loss: 0.03214459493756294, acc: 0.9922839403152466)
[2025-02-13 03:39:18,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:19,266][root][INFO] - Training Epoch: 2/2, step 1253/7134 completed (loss: 0.06327962875366211, acc: 0.9864864945411682)
[2025-02-13 03:39:19,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:19,695][root][INFO] - Training Epoch: 2/2, step 1254/7134 completed (loss: 0.06781377643346786, acc: 0.9772727489471436)
[2025-02-13 03:39:19,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:20,117][root][INFO] - Training Epoch: 2/2, step 1255/7134 completed (loss: 0.03818522393703461, acc: 0.991253674030304)
[2025-02-13 03:39:20,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:20,515][root][INFO] - Training Epoch: 2/2, step 1256/7134 completed (loss: 0.03657326474785805, acc: 0.9871382713317871)
[2025-02-13 03:39:20,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:20,942][root][INFO] - Training Epoch: 2/2, step 1257/7134 completed (loss: 0.025137973949313164, acc: 0.9903069734573364)
[2025-02-13 03:39:21,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:21,371][root][INFO] - Training Epoch: 2/2, step 1258/7134 completed (loss: 0.05881787836551666, acc: 0.9795022010803223)
[2025-02-13 03:39:21,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:21,773][root][INFO] - Training Epoch: 2/2, step 1259/7134 completed (loss: 0.05684394761919975, acc: 0.9892703890800476)
[2025-02-13 03:39:21,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:22,205][root][INFO] - Training Epoch: 2/2, step 1260/7134 completed (loss: 0.014382836408913136, acc: 0.9985548853874207)
[2025-02-13 03:39:22,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:22,650][root][INFO] - Training Epoch: 2/2, step 1261/7134 completed (loss: 0.040554486215114594, acc: 0.9910979270935059)
[2025-02-13 03:39:22,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:22,999][root][INFO] - Training Epoch: 2/2, step 1262/7134 completed (loss: 0.00617817509919405, acc: 0.9981684684753418)
[2025-02-13 03:39:23,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:23,401][root][INFO] - Training Epoch: 2/2, step 1263/7134 completed (loss: 0.011568202637135983, acc: 0.9957143068313599)
[2025-02-13 03:39:23,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:23,830][root][INFO] - Training Epoch: 2/2, step 1264/7134 completed (loss: 0.01652655564248562, acc: 0.9953271150588989)
[2025-02-13 03:39:23,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:24,293][root][INFO] - Training Epoch: 2/2, step 1265/7134 completed (loss: 0.015883296728134155, acc: 0.9937343597412109)
[2025-02-13 03:39:24,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:24,727][root][INFO] - Training Epoch: 2/2, step 1266/7134 completed (loss: 0.014189101755619049, acc: 0.9965576529502869)
[2025-02-13 03:39:24,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:25,146][root][INFO] - Training Epoch: 2/2, step 1267/7134 completed (loss: 0.027516407892107964, acc: 0.9931034445762634)
[2025-02-13 03:39:25,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:25,585][root][INFO] - Training Epoch: 2/2, step 1268/7134 completed (loss: 0.030766485258936882, acc: 0.9881831407546997)
[2025-02-13 03:39:25,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:26,014][root][INFO] - Training Epoch: 2/2, step 1269/7134 completed (loss: 0.06887272745370865, acc: 0.9793103337287903)
[2025-02-13 03:39:26,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:26,438][root][INFO] - Training Epoch: 2/2, step 1270/7134 completed (loss: 0.020253535360097885, acc: 0.9930264949798584)
[2025-02-13 03:39:26,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:26,797][root][INFO] - Training Epoch: 2/2, step 1271/7134 completed (loss: 0.06575048714876175, acc: 0.9814814925193787)
[2025-02-13 03:39:26,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:27,250][root][INFO] - Training Epoch: 2/2, step 1272/7134 completed (loss: 0.020376665517687798, acc: 0.9933949708938599)
[2025-02-13 03:39:27,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:27,665][root][INFO] - Training Epoch: 2/2, step 1273/7134 completed (loss: 0.02128979004919529, acc: 0.9951612949371338)
[2025-02-13 03:39:27,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:28,087][root][INFO] - Training Epoch: 2/2, step 1274/7134 completed (loss: 0.0232137069106102, acc: 0.9915540814399719)
[2025-02-13 03:39:28,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:28,500][root][INFO] - Training Epoch: 2/2, step 1275/7134 completed (loss: 0.028606154024600983, acc: 0.9928186535835266)
[2025-02-13 03:39:28,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:28,910][root][INFO] - Training Epoch: 2/2, step 1276/7134 completed (loss: 0.016110321506857872, acc: 0.9951612949371338)
[2025-02-13 03:39:29,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:29,311][root][INFO] - Training Epoch: 2/2, step 1277/7134 completed (loss: 0.027929527685046196, acc: 0.9903069734573364)
[2025-02-13 03:39:29,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:29,757][root][INFO] - Training Epoch: 2/2, step 1278/7134 completed (loss: 0.04168394207954407, acc: 0.9844478964805603)
[2025-02-13 03:39:29,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:30,187][root][INFO] - Training Epoch: 2/2, step 1279/7134 completed (loss: 0.04613463580608368, acc: 0.9888337254524231)
[2025-02-13 03:39:30,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:30,607][root][INFO] - Training Epoch: 2/2, step 1280/7134 completed (loss: 0.05917855724692345, acc: 0.9842312932014465)
[2025-02-13 03:39:30,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:31,018][root][INFO] - Training Epoch: 2/2, step 1281/7134 completed (loss: 0.036468930542469025, acc: 0.9913941621780396)
[2025-02-13 03:39:31,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:31,472][root][INFO] - Training Epoch: 2/2, step 1282/7134 completed (loss: 0.10665494948625565, acc: 0.9719251394271851)
[2025-02-13 03:39:31,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:31,934][root][INFO] - Training Epoch: 2/2, step 1283/7134 completed (loss: 0.05896587669849396, acc: 0.9876760840415955)
[2025-02-13 03:39:32,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:32,372][root][INFO] - Training Epoch: 2/2, step 1284/7134 completed (loss: 0.07021791487932205, acc: 0.9836512207984924)
[2025-02-13 03:39:32,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:32,777][root][INFO] - Training Epoch: 2/2, step 1285/7134 completed (loss: 0.05089676380157471, acc: 0.9898989796638489)
[2025-02-13 03:39:32,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:33,229][root][INFO] - Training Epoch: 2/2, step 1286/7134 completed (loss: 0.03579850494861603, acc: 0.9942775368690491)
[2025-02-13 03:39:33,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:33,626][root][INFO] - Training Epoch: 2/2, step 1287/7134 completed (loss: 0.08533782511949539, acc: 0.985049843788147)
[2025-02-13 03:39:33,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:34,042][root][INFO] - Training Epoch: 2/2, step 1288/7134 completed (loss: 0.04407896101474762, acc: 0.9860405921936035)
[2025-02-13 03:39:34,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:34,454][root][INFO] - Training Epoch: 2/2, step 1289/7134 completed (loss: 0.04637833684682846, acc: 0.9865996837615967)
[2025-02-13 03:39:34,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:34,890][root][INFO] - Training Epoch: 2/2, step 1290/7134 completed (loss: 0.04174013063311577, acc: 0.9887323975563049)
[2025-02-13 03:39:35,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:35,370][root][INFO] - Training Epoch: 2/2, step 1291/7134 completed (loss: 0.026853282004594803, acc: 0.9972106218338013)
[2025-02-13 03:39:35,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:35,808][root][INFO] - Training Epoch: 2/2, step 1292/7134 completed (loss: 0.03022667020559311, acc: 0.9935064911842346)
[2025-02-13 03:39:35,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:36,266][root][INFO] - Training Epoch: 2/2, step 1293/7134 completed (loss: 0.037416793406009674, acc: 0.9883585572242737)
[2025-02-13 03:39:36,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:36,702][root][INFO] - Training Epoch: 2/2, step 1294/7134 completed (loss: 0.037639081478118896, acc: 0.9895969033241272)
[2025-02-13 03:39:36,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:37,150][root][INFO] - Training Epoch: 2/2, step 1295/7134 completed (loss: 0.03393605723977089, acc: 0.9864176511764526)
[2025-02-13 03:39:37,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:37,584][root][INFO] - Training Epoch: 2/2, step 1296/7134 completed (loss: 0.010214217938482761, acc: 0.997787594795227)
[2025-02-13 03:39:37,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:37,949][root][INFO] - Training Epoch: 2/2, step 1297/7134 completed (loss: 0.03258915990591049, acc: 0.9854469895362854)
[2025-02-13 03:39:38,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:38,371][root][INFO] - Training Epoch: 2/2, step 1298/7134 completed (loss: 0.08430884778499603, acc: 0.980215847492218)
[2025-02-13 03:39:38,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:38,711][root][INFO] - Training Epoch: 2/2, step 1299/7134 completed (loss: 0.012999688275158405, acc: 1.0)
[2025-02-13 03:39:38,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:39,167][root][INFO] - Training Epoch: 2/2, step 1300/7134 completed (loss: 0.028102805837988853, acc: 0.9902557730674744)
[2025-02-13 03:39:39,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:39,587][root][INFO] - Training Epoch: 2/2, step 1301/7134 completed (loss: 0.008631730452179909, acc: 0.998516321182251)
[2025-02-13 03:39:39,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:40,043][root][INFO] - Training Epoch: 2/2, step 1302/7134 completed (loss: 0.04690783470869064, acc: 0.9877232313156128)
[2025-02-13 03:39:40,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:40,510][root][INFO] - Training Epoch: 2/2, step 1303/7134 completed (loss: 0.05784905329346657, acc: 0.9879807829856873)
[2025-02-13 03:39:40,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:40,940][root][INFO] - Training Epoch: 2/2, step 1304/7134 completed (loss: 0.06738647818565369, acc: 0.9868247509002686)
[2025-02-13 03:39:41,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:41,402][root][INFO] - Training Epoch: 2/2, step 1305/7134 completed (loss: 0.036890335381031036, acc: 0.9923858046531677)
[2025-02-13 03:39:41,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:41,879][root][INFO] - Training Epoch: 2/2, step 1306/7134 completed (loss: 0.05211777612566948, acc: 0.9863429665565491)
[2025-02-13 03:39:42,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:42,276][root][INFO] - Training Epoch: 2/2, step 1307/7134 completed (loss: 0.028971806168556213, acc: 0.9905213117599487)
[2025-02-13 03:39:42,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:42,722][root][INFO] - Training Epoch: 2/2, step 1308/7134 completed (loss: 0.028736410662531853, acc: 0.9924812316894531)
[2025-02-13 03:39:42,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:43,192][root][INFO] - Training Epoch: 2/2, step 1309/7134 completed (loss: 0.0417514331638813, acc: 0.9904305934906006)
[2025-02-13 03:39:43,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:43,661][root][INFO] - Training Epoch: 2/2, step 1310/7134 completed (loss: 0.04553666338324547, acc: 0.9850339889526367)
[2025-02-13 03:39:43,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:44,100][root][INFO] - Training Epoch: 2/2, step 1311/7134 completed (loss: 0.06529166549444199, acc: 0.9819004535675049)
[2025-02-13 03:39:44,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:44,561][root][INFO] - Training Epoch: 2/2, step 1312/7134 completed (loss: 0.07224602997303009, acc: 0.9838274717330933)
[2025-02-13 03:39:44,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:45,017][root][INFO] - Training Epoch: 2/2, step 1313/7134 completed (loss: 0.06212550401687622, acc: 0.9844192862510681)
[2025-02-13 03:39:45,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:45,438][root][INFO] - Training Epoch: 2/2, step 1314/7134 completed (loss: 0.04980406537652016, acc: 0.980701744556427)
[2025-02-13 03:39:45,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:45,867][root][INFO] - Training Epoch: 2/2, step 1315/7134 completed (loss: 0.14720141887664795, acc: 0.9666666388511658)
[2025-02-13 03:39:46,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:46,320][root][INFO] - Training Epoch: 2/2, step 1316/7134 completed (loss: 0.03209109976887703, acc: 0.9887005686759949)
[2025-02-13 03:39:46,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:46,636][root][INFO] - Training Epoch: 2/2, step 1317/7134 completed (loss: 0.1198364868760109, acc: 0.9674796462059021)
[2025-02-13 03:39:46,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:47,069][root][INFO] - Training Epoch: 2/2, step 1318/7134 completed (loss: 0.05199863761663437, acc: 0.9836512207984924)
[2025-02-13 03:39:47,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:47,388][root][INFO] - Training Epoch: 2/2, step 1319/7134 completed (loss: 0.0709492415189743, acc: 0.9885583519935608)
[2025-02-13 03:39:47,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:47,793][root][INFO] - Training Epoch: 2/2, step 1320/7134 completed (loss: 0.1012500748038292, acc: 0.9771987199783325)
[2025-02-13 03:39:47,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:48,285][root][INFO] - Training Epoch: 2/2, step 1321/7134 completed (loss: 0.02994498983025551, acc: 0.9914215803146362)
[2025-02-13 03:39:48,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:48,730][root][INFO] - Training Epoch: 2/2, step 1322/7134 completed (loss: 0.0653197318315506, acc: 0.9852700233459473)
[2025-02-13 03:39:48,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:49,170][root][INFO] - Training Epoch: 2/2, step 1323/7134 completed (loss: 0.047409363090991974, acc: 0.987500011920929)
[2025-02-13 03:39:49,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:49,611][root][INFO] - Training Epoch: 2/2, step 1324/7134 completed (loss: 0.03449476882815361, acc: 0.9934210777282715)
[2025-02-13 03:39:49,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:49,968][root][INFO] - Training Epoch: 2/2, step 1325/7134 completed (loss: 0.03130289912223816, acc: 0.9909297227859497)
[2025-02-13 03:39:50,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:50,441][root][INFO] - Training Epoch: 2/2, step 1326/7134 completed (loss: 0.06159958615899086, acc: 0.9868276715278625)
[2025-02-13 03:39:50,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:50,889][root][INFO] - Training Epoch: 2/2, step 1327/7134 completed (loss: 0.05163060128688812, acc: 0.9895052313804626)
[2025-02-13 03:39:51,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:51,331][root][INFO] - Training Epoch: 2/2, step 1328/7134 completed (loss: 0.04731116071343422, acc: 0.9906976819038391)
[2025-02-13 03:39:51,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:51,773][root][INFO] - Training Epoch: 2/2, step 1329/7134 completed (loss: 0.06280913203954697, acc: 0.9834254384040833)
[2025-02-13 03:39:51,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:52,155][root][INFO] - Training Epoch: 2/2, step 1330/7134 completed (loss: 0.02923622913658619, acc: 0.989708423614502)
[2025-02-13 03:39:52,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:52,612][root][INFO] - Training Epoch: 2/2, step 1331/7134 completed (loss: 0.015394456684589386, acc: 0.9951140284538269)
[2025-02-13 03:39:52,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:52,942][root][INFO] - Training Epoch: 2/2, step 1332/7134 completed (loss: 0.009647316299378872, acc: 0.9978166222572327)
[2025-02-13 03:39:53,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:53,400][root][INFO] - Training Epoch: 2/2, step 1333/7134 completed (loss: 0.04995085671544075, acc: 0.9846938848495483)
[2025-02-13 03:39:53,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:53,850][root][INFO] - Training Epoch: 2/2, step 1334/7134 completed (loss: 0.08555653691291809, acc: 0.97826087474823)
[2025-02-13 03:39:53,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:54,262][root][INFO] - Training Epoch: 2/2, step 1335/7134 completed (loss: 0.11264211684465408, acc: 0.9840213060379028)
[2025-02-13 03:39:54,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:54,705][root][INFO] - Training Epoch: 2/2, step 1336/7134 completed (loss: 0.023581305518746376, acc: 0.9932340979576111)
[2025-02-13 03:39:54,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:55,170][root][INFO] - Training Epoch: 2/2, step 1337/7134 completed (loss: 0.020198233425617218, acc: 0.9907975196838379)
[2025-02-13 03:39:55,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:55,452][root][INFO] - Training Epoch: 2/2, step 1338/7134 completed (loss: 0.05151878297328949, acc: 0.9919678568840027)
[2025-02-13 03:39:55,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:55,823][root][INFO] - Training Epoch: 2/2, step 1339/7134 completed (loss: 0.07205414772033691, acc: 0.9747191071510315)
[2025-02-13 03:39:55,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:56,241][root][INFO] - Training Epoch: 2/2, step 1340/7134 completed (loss: 0.05964041128754616, acc: 0.9865871667861938)
[2025-02-13 03:39:56,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:56,592][root][INFO] - Training Epoch: 2/2, step 1341/7134 completed (loss: 0.08097932487726212, acc: 0.9858757257461548)
[2025-02-13 03:39:56,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:57,012][root][INFO] - Training Epoch: 2/2, step 1342/7134 completed (loss: 0.007229417096823454, acc: 0.998389720916748)
[2025-02-13 03:39:57,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:57,467][root][INFO] - Training Epoch: 2/2, step 1343/7134 completed (loss: 0.0837978720664978, acc: 0.9838998317718506)
[2025-02-13 03:39:57,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:57,856][root][INFO] - Training Epoch: 2/2, step 1344/7134 completed (loss: 0.04330205172300339, acc: 0.9864253401756287)
[2025-02-13 03:39:57,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:58,149][root][INFO] - Training Epoch: 2/2, step 1345/7134 completed (loss: 0.023173050954937935, acc: 0.9867374300956726)
[2025-02-13 03:39:58,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:58,619][root][INFO] - Training Epoch: 2/2, step 1346/7134 completed (loss: 0.04868042469024658, acc: 0.9893190860748291)
[2025-02-13 03:39:58,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:59,036][root][INFO] - Training Epoch: 2/2, step 1347/7134 completed (loss: 0.05808492749929428, acc: 0.9860627055168152)
[2025-02-13 03:39:59,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:59,475][root][INFO] - Training Epoch: 2/2, step 1348/7134 completed (loss: 0.021507233381271362, acc: 0.9960681796073914)
[2025-02-13 03:39:59,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:59,913][root][INFO] - Training Epoch: 2/2, step 1349/7134 completed (loss: 0.0512884147465229, acc: 0.9931623935699463)
[2025-02-13 03:40:00,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:00,332][root][INFO] - Training Epoch: 2/2, step 1350/7134 completed (loss: 0.025366036221385002, acc: 0.9942113161087036)
[2025-02-13 03:40:00,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:00,733][root][INFO] - Training Epoch: 2/2, step 1351/7134 completed (loss: 0.018798166885972023, acc: 0.994106113910675)
[2025-02-13 03:40:00,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:01,144][root][INFO] - Training Epoch: 2/2, step 1352/7134 completed (loss: 0.03373430669307709, acc: 0.992559552192688)
[2025-02-13 03:40:01,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:01,547][root][INFO] - Training Epoch: 2/2, step 1353/7134 completed (loss: 0.04155789315700531, acc: 0.9885057210922241)
[2025-02-13 03:40:01,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:01,976][root][INFO] - Training Epoch: 2/2, step 1354/7134 completed (loss: 0.011678923852741718, acc: 0.9951377511024475)
[2025-02-13 03:40:02,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:02,380][root][INFO] - Training Epoch: 2/2, step 1355/7134 completed (loss: 0.008200973272323608, acc: 0.9984423518180847)
[2025-02-13 03:40:02,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:02,839][root][INFO] - Training Epoch: 2/2, step 1356/7134 completed (loss: 0.028187982738018036, acc: 0.9947368502616882)
[2025-02-13 03:40:02,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:03,276][root][INFO] - Training Epoch: 2/2, step 1357/7134 completed (loss: 0.013172515667974949, acc: 0.996216893196106)
[2025-02-13 03:40:03,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:03,730][root][INFO] - Training Epoch: 2/2, step 1358/7134 completed (loss: 0.02772746980190277, acc: 0.9922077655792236)
[2025-02-13 03:40:03,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:04,154][root][INFO] - Training Epoch: 2/2, step 1359/7134 completed (loss: 0.011313366703689098, acc: 0.9979757070541382)
[2025-02-13 03:40:04,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:04,579][root][INFO] - Training Epoch: 2/2, step 1360/7134 completed (loss: 0.011316072195768356, acc: 0.9959677457809448)
[2025-02-13 03:40:04,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:05,034][root][INFO] - Training Epoch: 2/2, step 1361/7134 completed (loss: 0.020027240738272667, acc: 0.9956204295158386)
[2025-02-13 03:40:05,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:05,466][root][INFO] - Training Epoch: 2/2, step 1362/7134 completed (loss: 0.018436824902892113, acc: 0.9959016442298889)
[2025-02-13 03:40:05,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:05,902][root][INFO] - Training Epoch: 2/2, step 1363/7134 completed (loss: 0.01534756738692522, acc: 0.9959785342216492)
[2025-02-13 03:40:06,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:06,336][root][INFO] - Training Epoch: 2/2, step 1364/7134 completed (loss: 0.015247576870024204, acc: 0.9942857027053833)
[2025-02-13 03:40:06,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:06,740][root][INFO] - Training Epoch: 2/2, step 1365/7134 completed (loss: 0.003794994903728366, acc: 1.0)
[2025-02-13 03:40:06,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:07,186][root][INFO] - Training Epoch: 2/2, step 1366/7134 completed (loss: 0.004786430858075619, acc: 1.0)
[2025-02-13 03:40:07,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:07,601][root][INFO] - Training Epoch: 2/2, step 1367/7134 completed (loss: 0.00156811170745641, acc: 1.0)
[2025-02-13 03:40:07,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:08,056][root][INFO] - Training Epoch: 2/2, step 1368/7134 completed (loss: 0.023758910596370697, acc: 0.9908536672592163)
[2025-02-13 03:40:08,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:08,477][root][INFO] - Training Epoch: 2/2, step 1369/7134 completed (loss: 0.026662195101380348, acc: 0.9912434220314026)
[2025-02-13 03:40:08,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:08,907][root][INFO] - Training Epoch: 2/2, step 1370/7134 completed (loss: 0.03689732402563095, acc: 0.993678867816925)
[2025-02-13 03:40:09,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:09,353][root][INFO] - Training Epoch: 2/2, step 1371/7134 completed (loss: 0.024086078628897667, acc: 0.9956331849098206)
[2025-02-13 03:40:09,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:09,770][root][INFO] - Training Epoch: 2/2, step 1372/7134 completed (loss: 0.04137784615159035, acc: 0.9906367063522339)
[2025-02-13 03:40:09,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:10,219][root][INFO] - Training Epoch: 2/2, step 1373/7134 completed (loss: 0.017040997743606567, acc: 0.9940944910049438)
[2025-02-13 03:40:10,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:10,650][root][INFO] - Training Epoch: 2/2, step 1374/7134 completed (loss: 0.026086723431944847, acc: 0.9881188273429871)
[2025-02-13 03:40:10,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:11,025][root][INFO] - Training Epoch: 2/2, step 1375/7134 completed (loss: 0.03532135859131813, acc: 0.9831730723381042)
[2025-02-13 03:40:11,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:11,440][root][INFO] - Training Epoch: 2/2, step 1376/7134 completed (loss: 0.05475342646241188, acc: 0.9832985401153564)
[2025-02-13 03:40:11,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:11,872][root][INFO] - Training Epoch: 2/2, step 1377/7134 completed (loss: 0.015410278923809528, acc: 0.9952940940856934)
[2025-02-13 03:40:11,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:12,239][root][INFO] - Training Epoch: 2/2, step 1378/7134 completed (loss: 0.06030585616827011, acc: 0.9824561476707458)
[2025-02-13 03:40:12,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:12,612][root][INFO] - Training Epoch: 2/2, step 1379/7134 completed (loss: 0.0382017120718956, acc: 0.9870967864990234)
[2025-02-13 03:40:12,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:13,039][root][INFO] - Training Epoch: 2/2, step 1380/7134 completed (loss: 0.02600647322833538, acc: 0.9943925142288208)
[2025-02-13 03:40:13,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:13,468][root][INFO] - Training Epoch: 2/2, step 1381/7134 completed (loss: 0.043715301901102066, acc: 0.9850746393203735)
[2025-02-13 03:40:13,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:13,878][root][INFO] - Training Epoch: 2/2, step 1382/7134 completed (loss: 0.05313384160399437, acc: 0.9857142567634583)
[2025-02-13 03:40:14,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:14,314][root][INFO] - Training Epoch: 2/2, step 1383/7134 completed (loss: 0.025411099195480347, acc: 0.9896729588508606)
[2025-02-13 03:40:14,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:14,732][root][INFO] - Training Epoch: 2/2, step 1384/7134 completed (loss: 0.056251343339681625, acc: 0.9733777046203613)
[2025-02-13 03:40:14,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:15,141][root][INFO] - Training Epoch: 2/2, step 1385/7134 completed (loss: 0.04747661203145981, acc: 0.9871559739112854)
[2025-02-13 03:40:15,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:15,574][root][INFO] - Training Epoch: 2/2, step 1386/7134 completed (loss: 0.034892305731773376, acc: 0.9896755218505859)
[2025-02-13 03:40:15,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:15,995][root][INFO] - Training Epoch: 2/2, step 1387/7134 completed (loss: 0.05066107586026192, acc: 0.9871060252189636)
[2025-02-13 03:40:16,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:16,406][root][INFO] - Training Epoch: 2/2, step 1388/7134 completed (loss: 0.04036765545606613, acc: 0.9880749583244324)
[2025-02-13 03:40:16,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:16,830][root][INFO] - Training Epoch: 2/2, step 1389/7134 completed (loss: 0.028825119137763977, acc: 0.9886547923088074)
[2025-02-13 03:40:16,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:17,261][root][INFO] - Training Epoch: 2/2, step 1390/7134 completed (loss: 0.026934929192066193, acc: 0.9937597513198853)
[2025-02-13 03:40:17,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:17,660][root][INFO] - Training Epoch: 2/2, step 1391/7134 completed (loss: 0.0686384066939354, acc: 0.9771528840065002)
[2025-02-13 03:40:17,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:18,085][root][INFO] - Training Epoch: 2/2, step 1392/7134 completed (loss: 0.038405537605285645, acc: 0.9861591458320618)
[2025-02-13 03:40:18,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:18,490][root][INFO] - Training Epoch: 2/2, step 1393/7134 completed (loss: 0.043743766844272614, acc: 0.9838709831237793)
[2025-02-13 03:40:18,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:18,898][root][INFO] - Training Epoch: 2/2, step 1394/7134 completed (loss: 0.018344860523939133, acc: 0.9920634627342224)
[2025-02-13 03:40:19,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:19,275][root][INFO] - Training Epoch: 2/2, step 1395/7134 completed (loss: 0.05233307555317879, acc: 0.990138053894043)
[2025-02-13 03:40:19,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:19,674][root][INFO] - Training Epoch: 2/2, step 1396/7134 completed (loss: 0.03663073480129242, acc: 0.9915540814399719)
[2025-02-13 03:40:19,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:20,083][root][INFO] - Training Epoch: 2/2, step 1397/7134 completed (loss: 0.05863326042890549, acc: 0.9848229289054871)
[2025-02-13 03:40:20,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:20,483][root][INFO] - Training Epoch: 2/2, step 1398/7134 completed (loss: 0.023218579590320587, acc: 0.993914783000946)
[2025-02-13 03:40:20,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:20,904][root][INFO] - Training Epoch: 2/2, step 1399/7134 completed (loss: 0.04834666848182678, acc: 0.9836956262588501)
[2025-02-13 03:40:21,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:21,334][root][INFO] - Training Epoch: 2/2, step 1400/7134 completed (loss: 0.04069811850786209, acc: 0.9895287752151489)
[2025-02-13 03:40:21,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:21,759][root][INFO] - Training Epoch: 2/2, step 1401/7134 completed (loss: 0.01998576521873474, acc: 0.9917627573013306)
[2025-02-13 03:40:21,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:22,158][root][INFO] - Training Epoch: 2/2, step 1402/7134 completed (loss: 0.020179735496640205, acc: 0.9930915236473083)
[2025-02-13 03:40:22,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:22,597][root][INFO] - Training Epoch: 2/2, step 1403/7134 completed (loss: 0.027749745175242424, acc: 0.9879518151283264)
[2025-02-13 03:40:22,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:23,054][root][INFO] - Training Epoch: 2/2, step 1404/7134 completed (loss: 0.029806086793541908, acc: 0.9933686852455139)
[2025-02-13 03:40:23,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:23,468][root][INFO] - Training Epoch: 2/2, step 1405/7134 completed (loss: 0.026771824806928635, acc: 0.9871794581413269)
[2025-02-13 03:40:23,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:23,915][root][INFO] - Training Epoch: 2/2, step 1406/7134 completed (loss: 0.006547225639224052, acc: 0.9985632300376892)
[2025-02-13 03:40:24,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:24,342][root][INFO] - Training Epoch: 2/2, step 1407/7134 completed (loss: 0.022393031045794487, acc: 0.9921773076057434)
[2025-02-13 03:40:24,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:24,776][root][INFO] - Training Epoch: 2/2, step 1408/7134 completed (loss: 0.016026858240365982, acc: 0.9935897588729858)
[2025-02-13 03:40:24,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:25,211][root][INFO] - Training Epoch: 2/2, step 1409/7134 completed (loss: 0.019024381414055824, acc: 0.9910581111907959)
[2025-02-13 03:40:25,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:25,664][root][INFO] - Training Epoch: 2/2, step 1410/7134 completed (loss: 0.028780996799468994, acc: 0.9947916865348816)
[2025-02-13 03:40:25,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:26,102][root][INFO] - Training Epoch: 2/2, step 1411/7134 completed (loss: 0.011453421786427498, acc: 0.9973404407501221)
[2025-02-13 03:40:26,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:26,570][root][INFO] - Training Epoch: 2/2, step 1412/7134 completed (loss: 0.00850646011531353, acc: 0.9988123774528503)
[2025-02-13 03:40:26,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:27,018][root][INFO] - Training Epoch: 2/2, step 1413/7134 completed (loss: 0.04405774176120758, acc: 0.9913669228553772)
[2025-02-13 03:40:27,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:27,438][root][INFO] - Training Epoch: 2/2, step 1414/7134 completed (loss: 0.012536418624222279, acc: 0.9968847632408142)
[2025-02-13 03:40:27,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:27,861][root][INFO] - Training Epoch: 2/2, step 1415/7134 completed (loss: 0.02061057835817337, acc: 0.9933775067329407)
[2025-02-13 03:40:28,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:28,324][root][INFO] - Training Epoch: 2/2, step 1416/7134 completed (loss: 0.010118345730006695, acc: 0.9985954761505127)
[2025-02-13 03:40:28,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:28,733][root][INFO] - Training Epoch: 2/2, step 1417/7134 completed (loss: 0.013666845858097076, acc: 0.9982993006706238)
[2025-02-13 03:40:28,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:29,114][root][INFO] - Training Epoch: 2/2, step 1418/7134 completed (loss: 0.01770925149321556, acc: 0.9961832165718079)
[2025-02-13 03:40:29,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:29,575][root][INFO] - Training Epoch: 2/2, step 1419/7134 completed (loss: 0.028828570619225502, acc: 0.9946019053459167)
[2025-02-13 03:40:29,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:30,006][root][INFO] - Training Epoch: 2/2, step 1420/7134 completed (loss: 0.021053429692983627, acc: 0.993261456489563)
[2025-02-13 03:40:30,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:30,436][root][INFO] - Training Epoch: 2/2, step 1421/7134 completed (loss: 0.017562227323651314, acc: 0.995192289352417)
[2025-02-13 03:40:30,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:30,851][root][INFO] - Training Epoch: 2/2, step 1422/7134 completed (loss: 0.06937705725431442, acc: 0.9814814925193787)
[2025-02-13 03:40:30,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:31,244][root][INFO] - Training Epoch: 2/2, step 1423/7134 completed (loss: 0.03305661305785179, acc: 0.9938775300979614)
[2025-02-13 03:40:31,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:31,667][root][INFO] - Training Epoch: 2/2, step 1424/7134 completed (loss: 0.06173732131719589, acc: 0.9790874719619751)
[2025-02-13 03:40:31,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:32,097][root][INFO] - Training Epoch: 2/2, step 1425/7134 completed (loss: 0.03106648474931717, acc: 0.9880775213241577)
[2025-02-13 03:40:32,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:32,556][root][INFO] - Training Epoch: 2/2, step 1426/7134 completed (loss: 0.028179755434393883, acc: 0.9897172451019287)
[2025-02-13 03:40:32,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:33,018][root][INFO] - Training Epoch: 2/2, step 1427/7134 completed (loss: 0.044412657618522644, acc: 0.9795657992362976)
[2025-02-13 03:40:33,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:33,419][root][INFO] - Training Epoch: 2/2, step 1428/7134 completed (loss: 0.21583497524261475, acc: 0.9425981640815735)
[2025-02-13 03:40:33,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:33,864][root][INFO] - Training Epoch: 2/2, step 1429/7134 completed (loss: 0.03142913058400154, acc: 0.9937629699707031)
[2025-02-13 03:40:34,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:34,328][root][INFO] - Training Epoch: 2/2, step 1430/7134 completed (loss: 0.05312316119670868, acc: 0.9852398633956909)
[2025-02-13 03:40:34,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:34,759][root][INFO] - Training Epoch: 2/2, step 1431/7134 completed (loss: 0.0203729048371315, acc: 0.9918588995933533)
[2025-02-13 03:40:34,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:35,211][root][INFO] - Training Epoch: 2/2, step 1432/7134 completed (loss: 0.025456896051764488, acc: 0.992277979850769)
[2025-02-13 03:40:35,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:35,660][root][INFO] - Training Epoch: 2/2, step 1433/7134 completed (loss: 0.022443898022174835, acc: 0.9921466112136841)
[2025-02-13 03:40:35,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:36,096][root][INFO] - Training Epoch: 2/2, step 1434/7134 completed (loss: 0.04395563155412674, acc: 0.9852744340896606)
[2025-02-13 03:40:36,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:36,554][root][INFO] - Training Epoch: 2/2, step 1435/7134 completed (loss: 0.047293733805418015, acc: 0.9780058860778809)
[2025-02-13 03:40:36,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:36,998][root][INFO] - Training Epoch: 2/2, step 1436/7134 completed (loss: 0.08409176766872406, acc: 0.9682741165161133)
[2025-02-13 03:40:37,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:37,422][root][INFO] - Training Epoch: 2/2, step 1437/7134 completed (loss: 0.040810227394104004, acc: 0.9860724210739136)
[2025-02-13 03:40:37,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:37,876][root][INFO] - Training Epoch: 2/2, step 1438/7134 completed (loss: 0.030774980783462524, acc: 0.9912408590316772)
[2025-02-13 03:40:38,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:38,331][root][INFO] - Training Epoch: 2/2, step 1439/7134 completed (loss: 0.032451532781124115, acc: 0.9870129823684692)
[2025-02-13 03:40:38,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:38,742][root][INFO] - Training Epoch: 2/2, step 1440/7134 completed (loss: 0.06677669286727905, acc: 0.9724612832069397)
[2025-02-13 03:40:38,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:39,187][root][INFO] - Training Epoch: 2/2, step 1441/7134 completed (loss: 0.03114684857428074, acc: 0.9897360801696777)
[2025-02-13 03:40:39,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:39,532][root][INFO] - Training Epoch: 2/2, step 1442/7134 completed (loss: 0.025947032496333122, acc: 0.9880383014678955)
[2025-02-13 03:40:39,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:39,959][root][INFO] - Training Epoch: 2/2, step 1443/7134 completed (loss: 0.06605580449104309, acc: 0.9806678295135498)
[2025-02-13 03:40:40,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:40,384][root][INFO] - Training Epoch: 2/2, step 1444/7134 completed (loss: 0.032606545835733414, acc: 0.9922178983688354)
[2025-02-13 03:40:40,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:40,833][root][INFO] - Training Epoch: 2/2, step 1445/7134 completed (loss: 0.0531008206307888, acc: 0.9833610653877258)
[2025-02-13 03:40:40,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:41,257][root][INFO] - Training Epoch: 2/2, step 1446/7134 completed (loss: 0.0252060629427433, acc: 0.9928876161575317)
[2025-02-13 03:40:41,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:41,682][root][INFO] - Training Epoch: 2/2, step 1447/7134 completed (loss: 0.032375186681747437, acc: 0.9886524677276611)
[2025-02-13 03:40:41,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:42,063][root][INFO] - Training Epoch: 2/2, step 1448/7134 completed (loss: 0.051480766385793686, acc: 0.9830508232116699)
[2025-02-13 03:40:42,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:42,502][root][INFO] - Training Epoch: 2/2, step 1449/7134 completed (loss: 0.06537774205207825, acc: 0.9845505356788635)
[2025-02-13 03:40:42,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:42,906][root][INFO] - Training Epoch: 2/2, step 1450/7134 completed (loss: 0.023414215072989464, acc: 0.9898403286933899)
[2025-02-13 03:40:43,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:43,319][root][INFO] - Training Epoch: 2/2, step 1451/7134 completed (loss: 0.021672483533620834, acc: 0.9956076145172119)
[2025-02-13 03:40:43,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:43,725][root][INFO] - Training Epoch: 2/2, step 1452/7134 completed (loss: 0.06815601140260696, acc: 0.984375)
[2025-02-13 03:40:43,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:44,120][root][INFO] - Training Epoch: 2/2, step 1453/7134 completed (loss: 0.068466417491436, acc: 0.9857369065284729)
[2025-02-13 03:40:44,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:44,573][root][INFO] - Training Epoch: 2/2, step 1454/7134 completed (loss: 0.06081327795982361, acc: 0.982503354549408)
[2025-02-13 03:40:44,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:44,980][root][INFO] - Training Epoch: 2/2, step 1455/7134 completed (loss: 0.05960312485694885, acc: 0.9816513657569885)
[2025-02-13 03:40:45,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:45,376][root][INFO] - Training Epoch: 2/2, step 1456/7134 completed (loss: 0.029398329555988312, acc: 0.9978166222572327)
[2025-02-13 03:40:45,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:45,786][root][INFO] - Training Epoch: 2/2, step 1457/7134 completed (loss: 0.030718322843313217, acc: 0.9891473054885864)
[2025-02-13 03:40:45,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:46,206][root][INFO] - Training Epoch: 2/2, step 1458/7134 completed (loss: 0.011020882986485958, acc: 0.9985272288322449)
[2025-02-13 03:40:46,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:46,620][root][INFO] - Training Epoch: 2/2, step 1459/7134 completed (loss: 0.03203358128666878, acc: 0.9893428087234497)
[2025-02-13 03:40:46,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:47,057][root][INFO] - Training Epoch: 2/2, step 1460/7134 completed (loss: 0.07536271959543228, acc: 0.9802631735801697)
[2025-02-13 03:40:47,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:47,470][root][INFO] - Training Epoch: 2/2, step 1461/7134 completed (loss: 0.024173956364393234, acc: 0.9872408509254456)
[2025-02-13 03:40:47,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:47,907][root][INFO] - Training Epoch: 2/2, step 1462/7134 completed (loss: 0.08313914388418198, acc: 0.9821428656578064)
[2025-02-13 03:40:48,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:48,335][root][INFO] - Training Epoch: 2/2, step 1463/7134 completed (loss: 0.0699385553598404, acc: 0.9790794849395752)
[2025-02-13 03:40:48,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:48,763][root][INFO] - Training Epoch: 2/2, step 1464/7134 completed (loss: 0.07875644415616989, acc: 0.9849849939346313)
[2025-02-13 03:40:48,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:49,195][root][INFO] - Training Epoch: 2/2, step 1465/7134 completed (loss: 0.046215519309043884, acc: 0.9874607920646667)
[2025-02-13 03:40:49,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:49,635][root][INFO] - Training Epoch: 2/2, step 1466/7134 completed (loss: 0.016053153201937675, acc: 0.9971671104431152)
[2025-02-13 03:40:49,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:50,070][root][INFO] - Training Epoch: 2/2, step 1467/7134 completed (loss: 0.012705670669674873, acc: 0.9971181750297546)
[2025-02-13 03:40:50,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:50,518][root][INFO] - Training Epoch: 2/2, step 1468/7134 completed (loss: 0.02828558161854744, acc: 0.9894319772720337)
[2025-02-13 03:40:50,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:50,952][root][INFO] - Training Epoch: 2/2, step 1469/7134 completed (loss: 0.02988572232425213, acc: 0.9932705163955688)
[2025-02-13 03:40:51,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:51,364][root][INFO] - Training Epoch: 2/2, step 1470/7134 completed (loss: 0.026016240939497948, acc: 0.9913294911384583)
[2025-02-13 03:40:51,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:51,780][root][INFO] - Training Epoch: 2/2, step 1471/7134 completed (loss: 0.030270623043179512, acc: 0.9893428087234497)
[2025-02-13 03:40:51,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:52,228][root][INFO] - Training Epoch: 2/2, step 1472/7134 completed (loss: 0.030588777735829353, acc: 0.9911949634552002)
[2025-02-13 03:40:52,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:52,655][root][INFO] - Training Epoch: 2/2, step 1473/7134 completed (loss: 0.027956223115324974, acc: 0.9896142482757568)
[2025-02-13 03:40:52,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:53,061][root][INFO] - Training Epoch: 2/2, step 1474/7134 completed (loss: 0.04962329566478729, acc: 0.9798657894134521)
[2025-02-13 03:40:53,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:53,502][root][INFO] - Training Epoch: 2/2, step 1475/7134 completed (loss: 0.058303434401750565, acc: 0.9907651543617249)
[2025-02-13 03:40:53,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:53,928][root][INFO] - Training Epoch: 2/2, step 1476/7134 completed (loss: 0.03115403652191162, acc: 0.9875776171684265)
[2025-02-13 03:40:54,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:54,321][root][INFO] - Training Epoch: 2/2, step 1477/7134 completed (loss: 0.037864942103624344, acc: 0.9860383868217468)
[2025-02-13 03:40:54,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:54,762][root][INFO] - Training Epoch: 2/2, step 1478/7134 completed (loss: 0.04305458441376686, acc: 0.9927007555961609)
[2025-02-13 03:40:54,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:55,199][root][INFO] - Training Epoch: 2/2, step 1479/7134 completed (loss: 0.02371806465089321, acc: 0.9899425506591797)
[2025-02-13 03:40:55,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:55,641][root][INFO] - Training Epoch: 2/2, step 1480/7134 completed (loss: 0.015793215483427048, acc: 0.9922077655792236)
[2025-02-13 03:40:55,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:56,102][root][INFO] - Training Epoch: 2/2, step 1481/7134 completed (loss: 0.032137680798769, acc: 0.9934210777282715)
[2025-02-13 03:40:56,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:56,550][root][INFO] - Training Epoch: 2/2, step 1482/7134 completed (loss: 0.041222114115953445, acc: 0.9872159361839294)
[2025-02-13 03:40:56,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:56,970][root][INFO] - Training Epoch: 2/2, step 1483/7134 completed (loss: 0.022161390632390976, acc: 0.9921011328697205)
[2025-02-13 03:40:57,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:57,408][root][INFO] - Training Epoch: 2/2, step 1484/7134 completed (loss: 0.0520813949406147, acc: 0.986994206905365)
[2025-02-13 03:40:57,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:57,835][root][INFO] - Training Epoch: 2/2, step 1485/7134 completed (loss: 0.02188032679259777, acc: 0.991150438785553)
[2025-02-13 03:40:57,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:58,255][root][INFO] - Training Epoch: 2/2, step 1486/7134 completed (loss: 0.0424303337931633, acc: 0.9891745448112488)
[2025-02-13 03:40:58,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:58,698][root][INFO] - Training Epoch: 2/2, step 1487/7134 completed (loss: 0.011974249966442585, acc: 0.9960784316062927)
[2025-02-13 03:40:58,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:59,174][root][INFO] - Training Epoch: 2/2, step 1488/7134 completed (loss: 0.03144640102982521, acc: 0.9935829043388367)
[2025-02-13 03:40:59,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:59,611][root][INFO] - Training Epoch: 2/2, step 1489/7134 completed (loss: 0.022741518914699554, acc: 0.9906542301177979)
[2025-02-13 03:40:59,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:00,084][root][INFO] - Training Epoch: 2/2, step 1490/7134 completed (loss: 0.012566315941512585, acc: 0.9957671761512756)
[2025-02-13 03:41:00,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:00,529][root][INFO] - Training Epoch: 2/2, step 1491/7134 completed (loss: 0.0131556810811162, acc: 0.9965517520904541)
[2025-02-13 03:41:00,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:00,972][root][INFO] - Training Epoch: 2/2, step 1492/7134 completed (loss: 0.05374743044376373, acc: 0.9872390031814575)
[2025-02-13 03:41:01,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:01,417][root][INFO] - Training Epoch: 2/2, step 1493/7134 completed (loss: 0.025351503863930702, acc: 0.9965437650680542)
[2025-02-13 03:41:01,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:01,879][root][INFO] - Training Epoch: 2/2, step 1494/7134 completed (loss: 0.013815733604133129, acc: 0.9963503479957581)
[2025-02-13 03:41:02,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:02,324][root][INFO] - Training Epoch: 2/2, step 1495/7134 completed (loss: 0.01753813587129116, acc: 0.9952095746994019)
[2025-02-13 03:41:02,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:02,776][root][INFO] - Training Epoch: 2/2, step 1496/7134 completed (loss: 0.03354354575276375, acc: 0.994301974773407)
[2025-02-13 03:41:02,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:03,233][root][INFO] - Training Epoch: 2/2, step 1497/7134 completed (loss: 0.03240315988659859, acc: 0.9886877536773682)
[2025-02-13 03:41:03,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:03,637][root][INFO] - Training Epoch: 2/2, step 1498/7134 completed (loss: 0.012799742631614208, acc: 0.9986824989318848)
[2025-02-13 03:41:03,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:04,111][root][INFO] - Training Epoch: 2/2, step 1499/7134 completed (loss: 0.020734993740916252, acc: 0.9952550530433655)
[2025-02-13 03:41:04,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:04,481][root][INFO] - Training Epoch: 2/2, step 1500/7134 completed (loss: 0.010000801645219326, acc: 0.996688723564148)
[2025-02-13 03:41:04,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:04,937][root][INFO] - Training Epoch: 2/2, step 1501/7134 completed (loss: 0.03544517233967781, acc: 0.99262535572052)
[2025-02-13 03:41:05,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:05,391][root][INFO] - Training Epoch: 2/2, step 1502/7134 completed (loss: 0.034665413200855255, acc: 0.990980863571167)
[2025-02-13 03:41:05,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:05,796][root][INFO] - Training Epoch: 2/2, step 1503/7134 completed (loss: 0.06518203765153885, acc: 0.9815340638160706)
[2025-02-13 03:41:05,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:06,215][root][INFO] - Training Epoch: 2/2, step 1504/7134 completed (loss: 0.011112596839666367, acc: 0.99622642993927)
[2025-02-13 03:41:06,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:06,631][root][INFO] - Training Epoch: 2/2, step 1505/7134 completed (loss: 0.03767096996307373, acc: 0.9922118186950684)
[2025-02-13 03:41:06,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:07,081][root][INFO] - Training Epoch: 2/2, step 1506/7134 completed (loss: 0.05046549811959267, acc: 0.9832826852798462)
[2025-02-13 03:41:07,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:07,494][root][INFO] - Training Epoch: 2/2, step 1507/7134 completed (loss: 0.02162283845245838, acc: 0.9934554696083069)
[2025-02-13 03:41:07,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:07,961][root][INFO] - Training Epoch: 2/2, step 1508/7134 completed (loss: 0.018310964107513428, acc: 0.9945295453071594)
[2025-02-13 03:41:08,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:08,405][root][INFO] - Training Epoch: 2/2, step 1509/7134 completed (loss: 0.013831126503646374, acc: 0.9971988797187805)
[2025-02-13 03:41:08,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:08,862][root][INFO] - Training Epoch: 2/2, step 1510/7134 completed (loss: 0.04369882121682167, acc: 0.9866666793823242)
[2025-02-13 03:41:08,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:09,302][root][INFO] - Training Epoch: 2/2, step 1511/7134 completed (loss: 0.06611262261867523, acc: 0.9815602898597717)
[2025-02-13 03:41:09,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:09,752][root][INFO] - Training Epoch: 2/2, step 1512/7134 completed (loss: 0.01917419768869877, acc: 0.990867555141449)
[2025-02-13 03:41:09,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:10,178][root][INFO] - Training Epoch: 2/2, step 1513/7134 completed (loss: 0.016542932018637657, acc: 0.9920634627342224)
[2025-02-13 03:41:10,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:10,634][root][INFO] - Training Epoch: 2/2, step 1514/7134 completed (loss: 0.03839946910738945, acc: 0.9950310587882996)
[2025-02-13 03:41:10,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:11,080][root][INFO] - Training Epoch: 2/2, step 1515/7134 completed (loss: 0.024646973237395287, acc: 0.985981285572052)
[2025-02-13 03:41:11,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:11,341][root][INFO] - Training Epoch: 2/2, step 1516/7134 completed (loss: 0.0418522022664547, acc: 0.9857549667358398)
[2025-02-13 03:41:11,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:11,684][root][INFO] - Training Epoch: 2/2, step 1517/7134 completed (loss: 0.040870632976293564, acc: 0.9873949289321899)
[2025-02-13 03:41:11,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:12,039][root][INFO] - Training Epoch: 2/2, step 1518/7134 completed (loss: 0.013021816499531269, acc: 0.9975903630256653)
[2025-02-13 03:41:12,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:12,463][root][INFO] - Training Epoch: 2/2, step 1519/7134 completed (loss: 0.018469179049134254, acc: 0.9930875301361084)
[2025-02-13 03:41:12,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:12,922][root][INFO] - Training Epoch: 2/2, step 1520/7134 completed (loss: 0.02876373939216137, acc: 0.9906396269798279)
[2025-02-13 03:41:13,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:13,415][root][INFO] - Training Epoch: 2/2, step 1521/7134 completed (loss: 0.0957033634185791, acc: 0.9720670580863953)
[2025-02-13 03:41:13,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:13,828][root][INFO] - Training Epoch: 2/2, step 1522/7134 completed (loss: 0.0650319829583168, acc: 0.9749518036842346)
[2025-02-13 03:41:13,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:14,248][root][INFO] - Training Epoch: 2/2, step 1523/7134 completed (loss: 0.10239953547716141, acc: 0.9700149893760681)
[2025-02-13 03:41:14,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:14,564][root][INFO] - Training Epoch: 2/2, step 1524/7134 completed (loss: 0.026345329359173775, acc: 0.9966443181037903)
[2025-02-13 03:41:14,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:15,006][root][INFO] - Training Epoch: 2/2, step 1525/7134 completed (loss: 0.02151680365204811, acc: 0.9932126402854919)
[2025-02-13 03:41:15,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:15,484][root][INFO] - Training Epoch: 2/2, step 1526/7134 completed (loss: 0.0688697099685669, acc: 0.9825834631919861)
[2025-02-13 03:41:15,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:15,975][root][INFO] - Training Epoch: 2/2, step 1527/7134 completed (loss: 0.10455728322267532, acc: 0.9749582409858704)
[2025-02-13 03:41:16,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:16,470][root][INFO] - Training Epoch: 2/2, step 1528/7134 completed (loss: 0.04981271177530289, acc: 0.988727867603302)
[2025-02-13 03:41:16,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:16,938][root][INFO] - Training Epoch: 2/2, step 1529/7134 completed (loss: 0.029422152787446976, acc: 0.9892984628677368)
[2025-02-13 03:41:17,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:17,360][root][INFO] - Training Epoch: 2/2, step 1530/7134 completed (loss: 0.06117209047079086, acc: 0.9885277152061462)
[2025-02-13 03:41:17,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:17,774][root][INFO] - Training Epoch: 2/2, step 1531/7134 completed (loss: 0.16261406242847443, acc: 0.9592198729515076)
[2025-02-13 03:41:17,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:18,252][root][INFO] - Training Epoch: 2/2, step 1532/7134 completed (loss: 0.029829755425453186, acc: 0.989595353603363)
[2025-02-13 03:41:18,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:18,690][root][INFO] - Training Epoch: 2/2, step 1533/7134 completed (loss: 0.026357993483543396, acc: 0.9903181195259094)
[2025-02-13 03:41:18,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:19,141][root][INFO] - Training Epoch: 2/2, step 1534/7134 completed (loss: 0.0484728068113327, acc: 0.9889196753501892)
[2025-02-13 03:41:19,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:19,631][root][INFO] - Training Epoch: 2/2, step 1535/7134 completed (loss: 0.03137923777103424, acc: 0.9904191493988037)
[2025-02-13 03:41:19,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:20,080][root][INFO] - Training Epoch: 2/2, step 1536/7134 completed (loss: 0.019297240301966667, acc: 0.9949430823326111)
[2025-02-13 03:41:20,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:20,486][root][INFO] - Training Epoch: 2/2, step 1537/7134 completed (loss: 0.049982067197561264, acc: 0.9842932224273682)
[2025-02-13 03:41:20,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:20,916][root][INFO] - Training Epoch: 2/2, step 1538/7134 completed (loss: 0.06348052620887756, acc: 0.9884763360023499)
[2025-02-13 03:41:21,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:21,362][root][INFO] - Training Epoch: 2/2, step 1539/7134 completed (loss: 0.029115445911884308, acc: 0.9894598126411438)
[2025-02-13 03:41:21,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:21,811][root][INFO] - Training Epoch: 2/2, step 1540/7134 completed (loss: 0.018544618040323257, acc: 0.9946380853652954)
[2025-02-13 03:41:21,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:22,258][root][INFO] - Training Epoch: 2/2, step 1541/7134 completed (loss: 0.04568730294704437, acc: 0.9811320900917053)
[2025-02-13 03:41:22,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:22,740][root][INFO] - Training Epoch: 2/2, step 1542/7134 completed (loss: 0.020899450406432152, acc: 0.9905660152435303)
[2025-02-13 03:41:22,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:23,195][root][INFO] - Training Epoch: 2/2, step 1543/7134 completed (loss: 0.015536394901573658, acc: 0.994535505771637)
[2025-02-13 03:41:23,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:23,653][root][INFO] - Training Epoch: 2/2, step 1544/7134 completed (loss: 0.01603098027408123, acc: 0.9951456189155579)
[2025-02-13 03:41:23,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:24,057][root][INFO] - Training Epoch: 2/2, step 1545/7134 completed (loss: 0.03254437446594238, acc: 0.991391658782959)
[2025-02-13 03:41:24,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:24,526][root][INFO] - Training Epoch: 2/2, step 1546/7134 completed (loss: 0.07727733254432678, acc: 0.9831697344779968)
[2025-02-13 03:41:24,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:24,963][root][INFO] - Training Epoch: 2/2, step 1547/7134 completed (loss: 0.030506057664752007, acc: 0.9895522594451904)
[2025-02-13 03:41:25,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:25,426][root][INFO] - Training Epoch: 2/2, step 1548/7134 completed (loss: 0.030573179945349693, acc: 0.9908987283706665)
[2025-02-13 03:41:25,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:25,891][root][INFO] - Training Epoch: 2/2, step 1549/7134 completed (loss: 0.022304082289338112, acc: 0.9933554530143738)
[2025-02-13 03:41:26,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:26,348][root][INFO] - Training Epoch: 2/2, step 1550/7134 completed (loss: 0.02043292112648487, acc: 0.9911280274391174)
[2025-02-13 03:41:26,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:26,774][root][INFO] - Training Epoch: 2/2, step 1551/7134 completed (loss: 0.02167196199297905, acc: 0.9909090995788574)
[2025-02-13 03:41:26,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:27,248][root][INFO] - Training Epoch: 2/2, step 1552/7134 completed (loss: 0.03640284389257431, acc: 0.9870689511299133)
[2025-02-13 03:41:27,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:27,665][root][INFO] - Training Epoch: 2/2, step 1553/7134 completed (loss: 0.045365918427705765, acc: 0.9908972978591919)
[2025-02-13 03:41:27,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:28,138][root][INFO] - Training Epoch: 2/2, step 1554/7134 completed (loss: 0.029928620904684067, acc: 0.988726019859314)
[2025-02-13 03:41:28,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:28,587][root][INFO] - Training Epoch: 2/2, step 1555/7134 completed (loss: 0.028945861384272575, acc: 0.9899216294288635)
[2025-02-13 03:41:28,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:29,042][root][INFO] - Training Epoch: 2/2, step 1556/7134 completed (loss: 0.02074330858886242, acc: 0.9917743802070618)
[2025-02-13 03:41:29,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:29,509][root][INFO] - Training Epoch: 2/2, step 1557/7134 completed (loss: 0.010130996815860271, acc: 0.9977628588676453)
[2025-02-13 03:41:29,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:29,929][root][INFO] - Training Epoch: 2/2, step 1558/7134 completed (loss: 0.027155762538313866, acc: 0.9901546835899353)
[2025-02-13 03:41:30,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:30,363][root][INFO] - Training Epoch: 2/2, step 1559/7134 completed (loss: 0.010744325816631317, acc: 0.9946452379226685)
[2025-02-13 03:41:30,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:30,816][root][INFO] - Training Epoch: 2/2, step 1560/7134 completed (loss: 0.023627663031220436, acc: 0.9923760890960693)
[2025-02-13 03:41:30,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:31,275][root][INFO] - Training Epoch: 2/2, step 1561/7134 completed (loss: 0.08111358433961868, acc: 0.9783315062522888)
[2025-02-13 03:41:31,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:31,696][root][INFO] - Training Epoch: 2/2, step 1562/7134 completed (loss: 0.028353983536362648, acc: 0.9924356937408447)
[2025-02-13 03:41:31,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:32,135][root][INFO] - Training Epoch: 2/2, step 1563/7134 completed (loss: 0.045747436583042145, acc: 0.9874285459518433)
[2025-02-13 03:41:32,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:32,582][root][INFO] - Training Epoch: 2/2, step 1564/7134 completed (loss: 0.07460323721170425, acc: 0.9752475023269653)
[2025-02-13 03:41:32,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:32,980][root][INFO] - Training Epoch: 2/2, step 1565/7134 completed (loss: 0.04294336959719658, acc: 0.9865546226501465)
[2025-02-13 03:41:33,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:33,407][root][INFO] - Training Epoch: 2/2, step 1566/7134 completed (loss: 0.022444793954491615, acc: 0.9931600689888)
[2025-02-13 03:41:33,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:33,910][root][INFO] - Training Epoch: 2/2, step 1567/7134 completed (loss: 0.038527294993400574, acc: 0.9925187230110168)
[2025-02-13 03:41:34,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:34,362][root][INFO] - Training Epoch: 2/2, step 1568/7134 completed (loss: 0.04701634868979454, acc: 0.9834319353103638)
[2025-02-13 03:41:34,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:34,814][root][INFO] - Training Epoch: 2/2, step 1569/7134 completed (loss: 0.05942041799426079, acc: 0.9853801131248474)
[2025-02-13 03:41:34,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:35,295][root][INFO] - Training Epoch: 2/2, step 1570/7134 completed (loss: 0.08742620795965195, acc: 0.9823594093322754)
[2025-02-13 03:41:35,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:35,727][root][INFO] - Training Epoch: 2/2, step 1571/7134 completed (loss: 0.04349209740757942, acc: 0.9893238544464111)
[2025-02-13 03:41:35,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:36,204][root][INFO] - Training Epoch: 2/2, step 1572/7134 completed (loss: 0.03394880145788193, acc: 0.9905771613121033)
[2025-02-13 03:41:36,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:36,629][root][INFO] - Training Epoch: 2/2, step 1573/7134 completed (loss: 0.01399064902216196, acc: 0.994194507598877)
[2025-02-13 03:41:36,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:37,105][root][INFO] - Training Epoch: 2/2, step 1574/7134 completed (loss: 0.04646158590912819, acc: 0.9901531934738159)
[2025-02-13 03:41:37,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:37,548][root][INFO] - Training Epoch: 2/2, step 1575/7134 completed (loss: 0.047657500952482224, acc: 0.9889135360717773)
[2025-02-13 03:41:37,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:38,002][root][INFO] - Training Epoch: 2/2, step 1576/7134 completed (loss: 0.03976988047361374, acc: 0.9918224215507507)
[2025-02-13 03:41:38,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:38,448][root][INFO] - Training Epoch: 2/2, step 1577/7134 completed (loss: 0.0441751554608345, acc: 0.9882214665412903)
[2025-02-13 03:41:38,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:38,893][root][INFO] - Training Epoch: 2/2, step 1578/7134 completed (loss: 0.033635351806879044, acc: 0.9878214001655579)
[2025-02-13 03:41:39,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:39,374][root][INFO] - Training Epoch: 2/2, step 1579/7134 completed (loss: 0.037882234901189804, acc: 0.9884792566299438)
[2025-02-13 03:41:39,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:39,840][root][INFO] - Training Epoch: 2/2, step 1580/7134 completed (loss: 0.06825641542673111, acc: 0.9801762104034424)
[2025-02-13 03:41:39,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:40,181][root][INFO] - Training Epoch: 2/2, step 1581/7134 completed (loss: 0.05939076095819473, acc: 0.9830247163772583)
[2025-02-13 03:41:40,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:40,631][root][INFO] - Training Epoch: 2/2, step 1582/7134 completed (loss: 0.04105929657816887, acc: 0.9906103014945984)
[2025-02-13 03:41:40,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:41,053][root][INFO] - Training Epoch: 2/2, step 1583/7134 completed (loss: 0.021005406975746155, acc: 0.9942693114280701)
[2025-02-13 03:41:41,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:41,534][root][INFO] - Training Epoch: 2/2, step 1584/7134 completed (loss: 0.03524433448910713, acc: 0.9903640151023865)
[2025-02-13 03:41:41,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:41,981][root][INFO] - Training Epoch: 2/2, step 1585/7134 completed (loss: 0.020621662959456444, acc: 0.9917550086975098)
[2025-02-13 03:41:42,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:42,410][root][INFO] - Training Epoch: 2/2, step 1586/7134 completed (loss: 0.05284114181995392, acc: 0.9881235361099243)
[2025-02-13 03:41:42,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:42,854][root][INFO] - Training Epoch: 2/2, step 1587/7134 completed (loss: 0.03245527297258377, acc: 0.9905808568000793)
[2025-02-13 03:41:42,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:43,301][root][INFO] - Training Epoch: 2/2, step 1588/7134 completed (loss: 0.06662391871213913, acc: 0.9866844415664673)
[2025-02-13 03:41:43,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:43,732][root][INFO] - Training Epoch: 2/2, step 1589/7134 completed (loss: 0.0445992648601532, acc: 0.991183876991272)
[2025-02-13 03:41:43,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:44,144][root][INFO] - Training Epoch: 2/2, step 1590/7134 completed (loss: 0.029598424211144447, acc: 0.9909365773200989)
[2025-02-13 03:41:44,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:44,554][root][INFO] - Training Epoch: 2/2, step 1591/7134 completed (loss: 0.047255150973796844, acc: 0.989847719669342)
[2025-02-13 03:41:44,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:45,014][root][INFO] - Training Epoch: 2/2, step 1592/7134 completed (loss: 0.022734425961971283, acc: 0.9928673505783081)
[2025-02-13 03:41:45,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:45,494][root][INFO] - Training Epoch: 2/2, step 1593/7134 completed (loss: 0.0423445999622345, acc: 0.9873563051223755)
[2025-02-13 03:41:45,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:45,933][root][INFO] - Training Epoch: 2/2, step 1594/7134 completed (loss: 0.023364368826150894, acc: 0.9917355179786682)
[2025-02-13 03:41:46,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:46,367][root][INFO] - Training Epoch: 2/2, step 1595/7134 completed (loss: 0.019698400050401688, acc: 0.9964157938957214)
[2025-02-13 03:41:46,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:46,795][root][INFO] - Training Epoch: 2/2, step 1596/7134 completed (loss: 0.05848462134599686, acc: 0.9889763593673706)
[2025-02-13 03:41:46,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:47,221][root][INFO] - Training Epoch: 2/2, step 1597/7134 completed (loss: 0.01775314286351204, acc: 0.9957020282745361)
[2025-02-13 03:41:47,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:47,626][root][INFO] - Training Epoch: 2/2, step 1598/7134 completed (loss: 0.07787442952394485, acc: 0.9800398945808411)
[2025-02-13 03:41:47,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:48,057][root][INFO] - Training Epoch: 2/2, step 1599/7134 completed (loss: 0.09373629838228226, acc: 0.9785124063491821)
[2025-02-13 03:41:48,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:48,512][root][INFO] - Training Epoch: 2/2, step 1600/7134 completed (loss: 0.046635523438453674, acc: 0.9896073937416077)
[2025-02-13 03:41:48,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:48,935][root][INFO] - Training Epoch: 2/2, step 1601/7134 completed (loss: 0.024417735636234283, acc: 0.9917469024658203)
[2025-02-13 03:41:49,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:49,389][root][INFO] - Training Epoch: 2/2, step 1602/7134 completed (loss: 0.04452759400010109, acc: 0.9924585223197937)
[2025-02-13 03:41:49,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:49,763][root][INFO] - Training Epoch: 2/2, step 1603/7134 completed (loss: 0.0454946905374527, acc: 0.9897698163986206)
[2025-02-13 03:41:49,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:50,205][root][INFO] - Training Epoch: 2/2, step 1604/7134 completed (loss: 0.019654862582683563, acc: 0.9960212111473083)
[2025-02-13 03:41:50,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:50,633][root][INFO] - Training Epoch: 2/2, step 1605/7134 completed (loss: 0.037508606910705566, acc: 0.9921773076057434)
[2025-02-13 03:41:50,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:51,054][root][INFO] - Training Epoch: 2/2, step 1606/7134 completed (loss: 0.017878275364637375, acc: 0.9954057931900024)
[2025-02-13 03:41:51,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:51,415][root][INFO] - Training Epoch: 2/2, step 1607/7134 completed (loss: 0.016811687499284744, acc: 0.9959266781806946)
[2025-02-13 03:41:51,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:51,823][root][INFO] - Training Epoch: 2/2, step 1608/7134 completed (loss: 0.010929496958851814, acc: 0.9962825179100037)
[2025-02-13 03:41:51,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:52,273][root][INFO] - Training Epoch: 2/2, step 1609/7134 completed (loss: 0.03495177999138832, acc: 0.9937694668769836)
[2025-02-13 03:41:52,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:52,694][root][INFO] - Training Epoch: 2/2, step 1610/7134 completed (loss: 0.047068413347005844, acc: 0.9933221936225891)
[2025-02-13 03:41:52,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:53,119][root][INFO] - Training Epoch: 2/2, step 1611/7134 completed (loss: 0.05906464159488678, acc: 0.9867724776268005)
[2025-02-13 03:41:53,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:53,542][root][INFO] - Training Epoch: 2/2, step 1612/7134 completed (loss: 0.06328064203262329, acc: 0.9847328066825867)
[2025-02-13 03:41:53,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:53,972][root][INFO] - Training Epoch: 2/2, step 1613/7134 completed (loss: 0.06302111595869064, acc: 0.9841897487640381)
[2025-02-13 03:41:54,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:54,404][root][INFO] - Training Epoch: 2/2, step 1614/7134 completed (loss: 0.039918202906847, acc: 0.9921362996101379)
[2025-02-13 03:41:54,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:54,884][root][INFO] - Training Epoch: 2/2, step 1615/7134 completed (loss: 0.04004542529582977, acc: 0.9865771532058716)
[2025-02-13 03:41:55,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:55,327][root][INFO] - Training Epoch: 2/2, step 1616/7134 completed (loss: 0.021386345848441124, acc: 0.9942280054092407)
[2025-02-13 03:41:55,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:55,752][root][INFO] - Training Epoch: 2/2, step 1617/7134 completed (loss: 0.046527232974767685, acc: 0.9890795350074768)
[2025-02-13 03:41:55,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:56,175][root][INFO] - Training Epoch: 2/2, step 1618/7134 completed (loss: 0.06493864953517914, acc: 0.9842105507850647)
[2025-02-13 03:41:56,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:56,606][root][INFO] - Training Epoch: 2/2, step 1619/7134 completed (loss: 0.03202349320054054, acc: 0.9931880235671997)
[2025-02-13 03:41:56,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:57,039][root][INFO] - Training Epoch: 2/2, step 1620/7134 completed (loss: 0.059719644486904144, acc: 0.9803328514099121)
[2025-02-13 03:41:57,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:57,485][root][INFO] - Training Epoch: 2/2, step 1621/7134 completed (loss: 0.08021651953458786, acc: 0.9716216325759888)
[2025-02-13 03:41:57,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:57,908][root][INFO] - Training Epoch: 2/2, step 1622/7134 completed (loss: 0.04900763928890228, acc: 0.9874652028083801)
[2025-02-13 03:41:58,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:58,364][root][INFO] - Training Epoch: 2/2, step 1623/7134 completed (loss: 0.04391086846590042, acc: 0.9873595237731934)
[2025-02-13 03:41:58,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:58,806][root][INFO] - Training Epoch: 2/2, step 1624/7134 completed (loss: 0.04962419345974922, acc: 0.9897210001945496)
[2025-02-13 03:41:58,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:59,247][root][INFO] - Training Epoch: 2/2, step 1625/7134 completed (loss: 0.02475390024483204, acc: 0.9914529919624329)
[2025-02-13 03:41:59,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:59,672][root][INFO] - Training Epoch: 2/2, step 1626/7134 completed (loss: 0.09376047551631927, acc: 0.9814814925193787)
[2025-02-13 03:41:59,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:00,077][root][INFO] - Training Epoch: 2/2, step 1627/7134 completed (loss: 0.022258268669247627, acc: 0.9908758997917175)
[2025-02-13 03:42:00,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:00,501][root][INFO] - Training Epoch: 2/2, step 1628/7134 completed (loss: 0.009816402569413185, acc: 0.9970845580101013)
[2025-02-13 03:42:00,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:00,908][root][INFO] - Training Epoch: 2/2, step 1629/7134 completed (loss: 0.0304206945002079, acc: 0.9872727394104004)
[2025-02-13 03:42:01,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:01,326][root][INFO] - Training Epoch: 2/2, step 1630/7134 completed (loss: 0.0229368694126606, acc: 0.9940740466117859)
[2025-02-13 03:42:01,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:01,753][root][INFO] - Training Epoch: 2/2, step 1631/7134 completed (loss: 0.04235062375664711, acc: 0.9871382713317871)
[2025-02-13 03:42:01,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:02,157][root][INFO] - Training Epoch: 2/2, step 1632/7134 completed (loss: 0.015384756028652191, acc: 0.9966832399368286)
[2025-02-13 03:42:02,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:02,572][root][INFO] - Training Epoch: 2/2, step 1633/7134 completed (loss: 0.015664076432585716, acc: 0.9952903985977173)
[2025-02-13 03:42:02,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:02,992][root][INFO] - Training Epoch: 2/2, step 1634/7134 completed (loss: 0.013694516383111477, acc: 0.9969372153282166)
[2025-02-13 03:42:03,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:03,392][root][INFO] - Training Epoch: 2/2, step 1635/7134 completed (loss: 0.02188437432050705, acc: 0.9918533563613892)
[2025-02-13 03:42:03,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:03,808][root][INFO] - Training Epoch: 2/2, step 1636/7134 completed (loss: 0.03463929891586304, acc: 0.9885714054107666)
[2025-02-13 03:42:03,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:04,236][root][INFO] - Training Epoch: 2/2, step 1637/7134 completed (loss: 0.019412025809288025, acc: 0.9971751570701599)
[2025-02-13 03:42:04,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:04,667][root][INFO] - Training Epoch: 2/2, step 1638/7134 completed (loss: 0.06208045408129692, acc: 0.9830220937728882)
[2025-02-13 03:42:04,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:05,104][root][INFO] - Training Epoch: 2/2, step 1639/7134 completed (loss: 0.11159838736057281, acc: 0.9748892188072205)
[2025-02-13 03:42:05,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:05,561][root][INFO] - Training Epoch: 2/2, step 1640/7134 completed (loss: 0.04512782394886017, acc: 0.9862805008888245)
[2025-02-13 03:42:05,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:05,982][root][INFO] - Training Epoch: 2/2, step 1641/7134 completed (loss: 0.009742219932377338, acc: 0.9985632300376892)
[2025-02-13 03:42:06,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:06,429][root][INFO] - Training Epoch: 2/2, step 1642/7134 completed (loss: 0.06183731555938721, acc: 0.9789473414421082)
[2025-02-13 03:42:06,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:06,842][root][INFO] - Training Epoch: 2/2, step 1643/7134 completed (loss: 0.08921755105257034, acc: 0.9774696826934814)
[2025-02-13 03:42:06,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:07,295][root][INFO] - Training Epoch: 2/2, step 1644/7134 completed (loss: 0.046107061207294464, acc: 0.9895697236061096)
[2025-02-13 03:42:07,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:07,727][root][INFO] - Training Epoch: 2/2, step 1645/7134 completed (loss: 0.06430716812610626, acc: 0.9841954112052917)
[2025-02-13 03:42:07,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:08,200][root][INFO] - Training Epoch: 2/2, step 1646/7134 completed (loss: 0.06267556548118591, acc: 0.9840255379676819)
[2025-02-13 03:42:08,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:08,647][root][INFO] - Training Epoch: 2/2, step 1647/7134 completed (loss: 0.03033115714788437, acc: 0.9899216294288635)
[2025-02-13 03:42:08,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:09,094][root][INFO] - Training Epoch: 2/2, step 1648/7134 completed (loss: 0.059578683227300644, acc: 0.9840425252914429)
[2025-02-13 03:42:09,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:09,549][root][INFO] - Training Epoch: 2/2, step 1649/7134 completed (loss: 0.027649257332086563, acc: 0.9922308325767517)
[2025-02-13 03:42:09,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:10,014][root][INFO] - Training Epoch: 2/2, step 1650/7134 completed (loss: 0.03030642308294773, acc: 0.9871134161949158)
[2025-02-13 03:42:10,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:10,476][root][INFO] - Training Epoch: 2/2, step 1651/7134 completed (loss: 0.0540238656103611, acc: 0.9878318309783936)
[2025-02-13 03:42:10,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:10,957][root][INFO] - Training Epoch: 2/2, step 1652/7134 completed (loss: 0.027579061686992645, acc: 0.994547426700592)
[2025-02-13 03:42:11,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:11,413][root][INFO] - Training Epoch: 2/2, step 1653/7134 completed (loss: 0.034071460366249084, acc: 0.9875466823577881)
[2025-02-13 03:42:11,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:11,915][root][INFO] - Training Epoch: 2/2, step 1654/7134 completed (loss: 0.04139253869652748, acc: 0.9898062944412231)
[2025-02-13 03:42:12,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:12,386][root][INFO] - Training Epoch: 2/2, step 1655/7134 completed (loss: 0.04861203581094742, acc: 0.9900867342948914)
[2025-02-13 03:42:12,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:12,838][root][INFO] - Training Epoch: 2/2, step 1656/7134 completed (loss: 0.04858371987938881, acc: 0.9887387156486511)
[2025-02-13 03:42:12,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:13,336][root][INFO] - Training Epoch: 2/2, step 1657/7134 completed (loss: 0.0420490987598896, acc: 0.9881278276443481)
[2025-02-13 03:42:13,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:13,814][root][INFO] - Training Epoch: 2/2, step 1658/7134 completed (loss: 0.018275799229741096, acc: 0.9938775300979614)
[2025-02-13 03:42:13,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:14,282][root][INFO] - Training Epoch: 2/2, step 1659/7134 completed (loss: 0.03971817344427109, acc: 0.9857819676399231)
[2025-02-13 03:42:14,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:14,765][root][INFO] - Training Epoch: 2/2, step 1660/7134 completed (loss: 0.031079689040780067, acc: 0.9939613342285156)
[2025-02-13 03:42:14,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:15,241][root][INFO] - Training Epoch: 2/2, step 1661/7134 completed (loss: 0.04625438153743744, acc: 0.9835886359214783)
[2025-02-13 03:42:15,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:15,702][root][INFO] - Training Epoch: 2/2, step 1662/7134 completed (loss: 0.03956087678670883, acc: 0.9915164113044739)
[2025-02-13 03:42:15,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:16,193][root][INFO] - Training Epoch: 2/2, step 1663/7134 completed (loss: 0.042730171233415604, acc: 0.990750253200531)
[2025-02-13 03:42:16,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:16,635][root][INFO] - Training Epoch: 2/2, step 1664/7134 completed (loss: 0.02384342812001705, acc: 0.9916567206382751)
[2025-02-13 03:42:16,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:17,094][root][INFO] - Training Epoch: 2/2, step 1665/7134 completed (loss: 0.027802210301160812, acc: 0.9884892106056213)
[2025-02-13 03:42:17,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:17,712][root][INFO] - Training Epoch: 2/2, step 1666/7134 completed (loss: 0.019027769565582275, acc: 0.9960079789161682)
[2025-02-13 03:42:17,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:18,220][root][INFO] - Training Epoch: 2/2, step 1667/7134 completed (loss: 0.03424056991934776, acc: 0.9881235361099243)
[2025-02-13 03:42:18,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:18,691][root][INFO] - Training Epoch: 2/2, step 1668/7134 completed (loss: 0.016335057094693184, acc: 0.9947478771209717)
[2025-02-13 03:42:18,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:19,223][root][INFO] - Training Epoch: 2/2, step 1669/7134 completed (loss: 0.02088128589093685, acc: 0.9934086799621582)
[2025-02-13 03:42:19,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:19,740][root][INFO] - Training Epoch: 2/2, step 1670/7134 completed (loss: 0.05672694370150566, acc: 0.9866412281990051)
[2025-02-13 03:42:19,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:20,222][root][INFO] - Training Epoch: 2/2, step 1671/7134 completed (loss: 0.04807589575648308, acc: 0.9880525469779968)
[2025-02-13 03:42:20,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:20,675][root][INFO] - Training Epoch: 2/2, step 1672/7134 completed (loss: 0.050380632281303406, acc: 0.9867197871208191)
[2025-02-13 03:42:20,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:21,130][root][INFO] - Training Epoch: 2/2, step 1673/7134 completed (loss: 0.04089678078889847, acc: 0.9932795763015747)
[2025-02-13 03:42:21,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:21,543][root][INFO] - Training Epoch: 2/2, step 1674/7134 completed (loss: 0.046240370720624924, acc: 0.9902234673500061)
[2025-02-13 03:42:21,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:21,999][root][INFO] - Training Epoch: 2/2, step 1675/7134 completed (loss: 0.07986637204885483, acc: 0.9836734533309937)
[2025-02-13 03:42:22,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:22,430][root][INFO] - Training Epoch: 2/2, step 1676/7134 completed (loss: 0.022791437804698944, acc: 0.9930939078330994)
[2025-02-13 03:42:22,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:22,851][root][INFO] - Training Epoch: 2/2, step 1677/7134 completed (loss: 0.07321200519800186, acc: 0.9819967150688171)
[2025-02-13 03:42:23,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:23,308][root][INFO] - Training Epoch: 2/2, step 1678/7134 completed (loss: 0.019111761823296547, acc: 0.9931507110595703)
[2025-02-13 03:42:23,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:23,733][root][INFO] - Training Epoch: 2/2, step 1679/7134 completed (loss: 0.05005159229040146, acc: 0.985358715057373)
[2025-02-13 03:42:23,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:24,192][root][INFO] - Training Epoch: 2/2, step 1680/7134 completed (loss: 0.035198722034692764, acc: 0.9932975769042969)
[2025-02-13 03:42:24,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:24,616][root][INFO] - Training Epoch: 2/2, step 1681/7134 completed (loss: 0.03486018627882004, acc: 0.9928977489471436)
[2025-02-13 03:42:24,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:25,067][root][INFO] - Training Epoch: 2/2, step 1682/7134 completed (loss: 0.06817349046468735, acc: 0.9798271059989929)
[2025-02-13 03:42:25,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:25,488][root][INFO] - Training Epoch: 2/2, step 1683/7134 completed (loss: 0.027392594143748283, acc: 0.991909384727478)
[2025-02-13 03:42:25,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:25,918][root][INFO] - Training Epoch: 2/2, step 1684/7134 completed (loss: 0.038010384887456894, acc: 0.9863013625144958)
[2025-02-13 03:42:26,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:26,355][root][INFO] - Training Epoch: 2/2, step 1685/7134 completed (loss: 0.043910201638936996, acc: 0.9874826073646545)
[2025-02-13 03:42:26,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:26,767][root][INFO] - Training Epoch: 2/2, step 1686/7134 completed (loss: 0.03340110555291176, acc: 0.9881756901741028)
[2025-02-13 03:42:26,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:27,195][root][INFO] - Training Epoch: 2/2, step 1687/7134 completed (loss: 0.03142046555876732, acc: 0.9885931611061096)
[2025-02-13 03:42:27,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:27,601][root][INFO] - Training Epoch: 2/2, step 1688/7134 completed (loss: 0.04726453125476837, acc: 0.9907264113426208)
[2025-02-13 03:42:27,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:28,011][root][INFO] - Training Epoch: 2/2, step 1689/7134 completed (loss: 0.02708420157432556, acc: 0.9927849769592285)
[2025-02-13 03:42:28,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:28,410][root][INFO] - Training Epoch: 2/2, step 1690/7134 completed (loss: 0.03103557787835598, acc: 0.9939024448394775)
[2025-02-13 03:42:28,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:28,816][root][INFO] - Training Epoch: 2/2, step 1691/7134 completed (loss: 0.0079396553337574, acc: 0.996688723564148)
[2025-02-13 03:42:28,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:29,228][root][INFO] - Training Epoch: 2/2, step 1692/7134 completed (loss: 0.04732547700405121, acc: 0.9831365942955017)
[2025-02-13 03:42:29,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:29,654][root][INFO] - Training Epoch: 2/2, step 1693/7134 completed (loss: 0.031577229499816895, acc: 0.992514967918396)
[2025-02-13 03:42:29,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:30,079][root][INFO] - Training Epoch: 2/2, step 1694/7134 completed (loss: 0.033907752484083176, acc: 0.9874804615974426)
[2025-02-13 03:42:30,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:30,514][root][INFO] - Training Epoch: 2/2, step 1695/7134 completed (loss: 0.007905997335910797, acc: 1.0)
[2025-02-13 03:42:30,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:30,915][root][INFO] - Training Epoch: 2/2, step 1696/7134 completed (loss: 0.04971058666706085, acc: 0.9849246144294739)
[2025-02-13 03:42:31,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:31,319][root][INFO] - Training Epoch: 2/2, step 1697/7134 completed (loss: 0.03185364603996277, acc: 0.992175281047821)
[2025-02-13 03:42:31,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:31,747][root][INFO] - Training Epoch: 2/2, step 1698/7134 completed (loss: 0.01875733956694603, acc: 0.9952977895736694)
[2025-02-13 03:42:31,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:32,191][root][INFO] - Training Epoch: 2/2, step 1699/7134 completed (loss: 0.025730522349476814, acc: 0.987889289855957)
[2025-02-13 03:42:32,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:32,649][root][INFO] - Training Epoch: 2/2, step 1700/7134 completed (loss: 0.03724595531821251, acc: 0.9838998317718506)
[2025-02-13 03:42:32,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:33,078][root][INFO] - Training Epoch: 2/2, step 1701/7134 completed (loss: 0.041111186146736145, acc: 0.9847009778022766)
[2025-02-13 03:42:33,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:33,514][root][INFO] - Training Epoch: 2/2, step 1702/7134 completed (loss: 0.067762091755867, acc: 0.9838509559631348)
[2025-02-13 03:42:33,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:33,937][root][INFO] - Training Epoch: 2/2, step 1703/7134 completed (loss: 0.05679692327976227, acc: 0.980663001537323)
[2025-02-13 03:42:34,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:34,376][root][INFO] - Training Epoch: 2/2, step 1704/7134 completed (loss: 0.09356848150491714, acc: 0.9741848111152649)
[2025-02-13 03:42:34,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:34,818][root][INFO] - Training Epoch: 2/2, step 1705/7134 completed (loss: 0.050371114164590836, acc: 0.9842209219932556)
[2025-02-13 03:42:34,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:35,095][root][INFO] - Training Epoch: 2/2, step 1706/7134 completed (loss: 0.03056417778134346, acc: 0.9848484992980957)
[2025-02-13 03:42:35,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:35,516][root][INFO] - Training Epoch: 2/2, step 1707/7134 completed (loss: 0.03360632061958313, acc: 0.9957716464996338)
[2025-02-13 03:42:35,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:35,940][root][INFO] - Training Epoch: 2/2, step 1708/7134 completed (loss: 0.04054667800664902, acc: 0.9853658676147461)
[2025-02-13 03:42:36,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:36,359][root][INFO] - Training Epoch: 2/2, step 1709/7134 completed (loss: 0.041958682239055634, acc: 0.9810344576835632)
[2025-02-13 03:42:36,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:36,818][root][INFO] - Training Epoch: 2/2, step 1710/7134 completed (loss: 0.046852052211761475, acc: 0.9856321811676025)
[2025-02-13 03:42:36,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:37,215][root][INFO] - Training Epoch: 2/2, step 1711/7134 completed (loss: 0.04381968080997467, acc: 0.9879102110862732)
[2025-02-13 03:42:37,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:37,610][root][INFO] - Training Epoch: 2/2, step 1712/7134 completed (loss: 0.0788818746805191, acc: 0.9719298481941223)
[2025-02-13 03:42:37,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:38,006][root][INFO] - Training Epoch: 2/2, step 1713/7134 completed (loss: 0.033683955669403076, acc: 0.9920159578323364)
[2025-02-13 03:42:38,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:38,442][root][INFO] - Training Epoch: 2/2, step 1714/7134 completed (loss: 0.057495035231113434, acc: 0.9835526347160339)
[2025-02-13 03:42:38,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:38,882][root][INFO] - Training Epoch: 2/2, step 1715/7134 completed (loss: 0.02641269564628601, acc: 0.9959431886672974)
[2025-02-13 03:42:39,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:39,284][root][INFO] - Training Epoch: 2/2, step 1716/7134 completed (loss: 0.06518890708684921, acc: 0.9783333539962769)
[2025-02-13 03:42:39,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:39,676][root][INFO] - Training Epoch: 2/2, step 1717/7134 completed (loss: 0.026664400473237038, acc: 0.9883268475532532)
[2025-02-13 03:42:39,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:40,073][root][INFO] - Training Epoch: 2/2, step 1718/7134 completed (loss: 0.057496219873428345, acc: 0.9821826219558716)
[2025-02-13 03:42:40,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:40,517][root][INFO] - Training Epoch: 2/2, step 1719/7134 completed (loss: 0.08186682313680649, acc: 0.9876712560653687)
[2025-02-13 03:42:40,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:40,891][root][INFO] - Training Epoch: 2/2, step 1720/7134 completed (loss: 0.06280060857534409, acc: 0.9710144996643066)
[2025-02-13 03:42:41,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:41,253][root][INFO] - Training Epoch: 2/2, step 1721/7134 completed (loss: 0.035183876752853394, acc: 0.9865092635154724)
[2025-02-13 03:42:41,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:41,620][root][INFO] - Training Epoch: 2/2, step 1722/7134 completed (loss: 0.014648075215518475, acc: 0.99210524559021)
[2025-02-13 03:42:41,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:42,032][root][INFO] - Training Epoch: 2/2, step 1723/7134 completed (loss: 0.08526017516851425, acc: 0.9836065769195557)
[2025-02-13 03:42:42,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:42,456][root][INFO] - Training Epoch: 2/2, step 1724/7134 completed (loss: 0.07416193932294846, acc: 0.9748822450637817)
[2025-02-13 03:42:42,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:42,778][root][INFO] - Training Epoch: 2/2, step 1725/7134 completed (loss: 0.019196931272745132, acc: 0.9935691356658936)
[2025-02-13 03:42:42,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:43,124][root][INFO] - Training Epoch: 2/2, step 1726/7134 completed (loss: 0.036766767501831055, acc: 0.9843049049377441)
[2025-02-13 03:42:43,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:43,538][root][INFO] - Training Epoch: 2/2, step 1727/7134 completed (loss: 0.026445560157299042, acc: 0.9927007555961609)
[2025-02-13 03:42:43,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:43,968][root][INFO] - Training Epoch: 2/2, step 1728/7134 completed (loss: 0.030799193307757378, acc: 0.9917218685150146)
[2025-02-13 03:42:44,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:44,362][root][INFO] - Training Epoch: 2/2, step 1729/7134 completed (loss: 0.05111170932650566, acc: 0.9877800345420837)
[2025-02-13 03:42:44,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:44,792][root][INFO] - Training Epoch: 2/2, step 1730/7134 completed (loss: 0.0209000576287508, acc: 0.9935759902000427)
[2025-02-13 03:42:44,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:45,082][root][INFO] - Training Epoch: 2/2, step 1731/7134 completed (loss: 0.08391152322292328, acc: 0.9884393215179443)
[2025-02-13 03:42:45,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:45,462][root][INFO] - Training Epoch: 2/2, step 1732/7134 completed (loss: 0.04379149153828621, acc: 0.987679660320282)
[2025-02-13 03:42:45,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:45,814][root][INFO] - Training Epoch: 2/2, step 1733/7134 completed (loss: 0.04375796765089035, acc: 0.9835391044616699)
[2025-02-13 03:42:45,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:46,292][root][INFO] - Training Epoch: 2/2, step 1734/7134 completed (loss: 0.05317457765340805, acc: 0.9877150058746338)
[2025-02-13 03:42:46,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:46,736][root][INFO] - Training Epoch: 2/2, step 1735/7134 completed (loss: 0.016644487157464027, acc: 0.9957924485206604)
[2025-02-13 03:42:46,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:47,192][root][INFO] - Training Epoch: 2/2, step 1736/7134 completed (loss: 0.04072422534227371, acc: 0.9842424392700195)
[2025-02-13 03:42:47,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:47,639][root][INFO] - Training Epoch: 2/2, step 1737/7134 completed (loss: 0.034686412662267685, acc: 0.9943820238113403)
[2025-02-13 03:42:47,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:48,093][root][INFO] - Training Epoch: 2/2, step 1738/7134 completed (loss: 0.018287671729922295, acc: 0.9915561079978943)
[2025-02-13 03:42:48,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:48,544][root][INFO] - Training Epoch: 2/2, step 1739/7134 completed (loss: 0.03727811947464943, acc: 0.9820359349250793)
[2025-02-13 03:42:48,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:48,994][root][INFO] - Training Epoch: 2/2, step 1740/7134 completed (loss: 0.011885382235050201, acc: 0.9970760345458984)
[2025-02-13 03:42:49,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:49,462][root][INFO] - Training Epoch: 2/2, step 1741/7134 completed (loss: 0.04508895426988602, acc: 0.9854497313499451)
[2025-02-13 03:42:49,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:49,888][root][INFO] - Training Epoch: 2/2, step 1742/7134 completed (loss: 0.04169464111328125, acc: 0.9843971729278564)
[2025-02-13 03:42:50,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:50,342][root][INFO] - Training Epoch: 2/2, step 1743/7134 completed (loss: 0.051650628447532654, acc: 0.9831932783126831)
[2025-02-13 03:42:50,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:50,773][root][INFO] - Training Epoch: 2/2, step 1744/7134 completed (loss: 0.035386938601732254, acc: 0.9951140284538269)
[2025-02-13 03:42:50,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:51,222][root][INFO] - Training Epoch: 2/2, step 1745/7134 completed (loss: 0.03392458334565163, acc: 0.9887217879295349)
[2025-02-13 03:42:51,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:51,672][root][INFO] - Training Epoch: 2/2, step 1746/7134 completed (loss: 0.01562188658863306, acc: 0.9956395626068115)
[2025-02-13 03:42:51,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:52,144][root][INFO] - Training Epoch: 2/2, step 1747/7134 completed (loss: 0.039947930723428726, acc: 0.987484335899353)
[2025-02-13 03:42:52,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:52,610][root][INFO] - Training Epoch: 2/2, step 1748/7134 completed (loss: 0.030635956674814224, acc: 0.9913151264190674)
[2025-02-13 03:42:52,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:53,077][root][INFO] - Training Epoch: 2/2, step 1749/7134 completed (loss: 0.03849203884601593, acc: 0.9931895732879639)
[2025-02-13 03:42:53,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:53,545][root][INFO] - Training Epoch: 2/2, step 1750/7134 completed (loss: 0.028327416628599167, acc: 0.9928057789802551)
[2025-02-13 03:42:53,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:53,992][root][INFO] - Training Epoch: 2/2, step 1751/7134 completed (loss: 0.050774313509464264, acc: 0.9806060791015625)
[2025-02-13 03:42:54,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:54,443][root][INFO] - Training Epoch: 2/2, step 1752/7134 completed (loss: 0.05826118215918541, acc: 0.9819587469100952)
[2025-02-13 03:42:54,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:54,874][root][INFO] - Training Epoch: 2/2, step 1753/7134 completed (loss: 0.049684032797813416, acc: 0.9908972978591919)
[2025-02-13 03:42:55,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:55,299][root][INFO] - Training Epoch: 2/2, step 1754/7134 completed (loss: 0.0474756620824337, acc: 0.9884726405143738)
[2025-02-13 03:42:55,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:55,728][root][INFO] - Training Epoch: 2/2, step 1755/7134 completed (loss: 0.01473518367856741, acc: 0.9947090148925781)
[2025-02-13 03:42:55,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:56,170][root][INFO] - Training Epoch: 2/2, step 1756/7134 completed (loss: 0.045736756175756454, acc: 0.9882965087890625)
[2025-02-13 03:42:56,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:56,583][root][INFO] - Training Epoch: 2/2, step 1757/7134 completed (loss: 0.051392845809459686, acc: 0.9833610653877258)
[2025-02-13 03:42:56,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:57,045][root][INFO] - Training Epoch: 2/2, step 1758/7134 completed (loss: 0.02132953703403473, acc: 0.993686854839325)
[2025-02-13 03:42:57,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:57,482][root][INFO] - Training Epoch: 2/2, step 1759/7134 completed (loss: 0.03554091602563858, acc: 0.9888734221458435)
[2025-02-13 03:42:57,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:57,902][root][INFO] - Training Epoch: 2/2, step 1760/7134 completed (loss: 0.025440016761422157, acc: 0.9915966391563416)
[2025-02-13 03:42:58,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:58,290][root][INFO] - Training Epoch: 2/2, step 1761/7134 completed (loss: 0.02181827276945114, acc: 0.9914772510528564)
[2025-02-13 03:42:58,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:58,710][root][INFO] - Training Epoch: 2/2, step 1762/7134 completed (loss: 0.011160042136907578, acc: 1.0)
[2025-02-13 03:42:58,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:59,115][root][INFO] - Training Epoch: 2/2, step 1763/7134 completed (loss: 0.03898054361343384, acc: 0.9871794581413269)
[2025-02-13 03:42:59,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:59,528][root][INFO] - Training Epoch: 2/2, step 1764/7134 completed (loss: 0.01673811487853527, acc: 0.9925650358200073)
[2025-02-13 03:42:59,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:59,963][root][INFO] - Training Epoch: 2/2, step 1765/7134 completed (loss: 0.02087073214352131, acc: 0.9953703880310059)
[2025-02-13 03:43:00,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:00,377][root][INFO] - Training Epoch: 2/2, step 1766/7134 completed (loss: 0.01666657067835331, acc: 0.9956011772155762)
[2025-02-13 03:43:00,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:00,812][root][INFO] - Training Epoch: 2/2, step 1767/7134 completed (loss: 0.051193349063396454, acc: 0.985855758190155)
[2025-02-13 03:43:00,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:01,234][root][INFO] - Training Epoch: 2/2, step 1768/7134 completed (loss: 0.030947713181376457, acc: 0.9884393215179443)
[2025-02-13 03:43:01,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:01,665][root][INFO] - Training Epoch: 2/2, step 1769/7134 completed (loss: 0.032667770981788635, acc: 0.9897058606147766)
[2025-02-13 03:43:01,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:02,070][root][INFO] - Training Epoch: 2/2, step 1770/7134 completed (loss: 0.0185813307762146, acc: 0.9908952713012695)
[2025-02-13 03:43:02,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:02,495][root][INFO] - Training Epoch: 2/2, step 1771/7134 completed (loss: 0.013376310467720032, acc: 0.998603343963623)
[2025-02-13 03:43:02,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:02,889][root][INFO] - Training Epoch: 2/2, step 1772/7134 completed (loss: 0.021920453757047653, acc: 0.9916247725486755)
[2025-02-13 03:43:03,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:03,320][root][INFO] - Training Epoch: 2/2, step 1773/7134 completed (loss: 0.012239097617566586, acc: 0.9970930218696594)
[2025-02-13 03:43:03,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:03,714][root][INFO] - Training Epoch: 2/2, step 1774/7134 completed (loss: 0.008219655603170395, acc: 0.9966777563095093)
[2025-02-13 03:43:03,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:04,108][root][INFO] - Training Epoch: 2/2, step 1775/7134 completed (loss: 0.011038237251341343, acc: 0.996874988079071)
[2025-02-13 03:43:04,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:04,517][root][INFO] - Training Epoch: 2/2, step 1776/7134 completed (loss: 0.037702515721321106, acc: 0.9895209670066833)
[2025-02-13 03:43:04,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:04,927][root][INFO] - Training Epoch: 2/2, step 1777/7134 completed (loss: 0.012831049971282482, acc: 0.9970414042472839)
[2025-02-13 03:43:05,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:05,345][root][INFO] - Training Epoch: 2/2, step 1778/7134 completed (loss: 0.04389689117670059, acc: 0.9926035404205322)
[2025-02-13 03:43:05,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:05,751][root][INFO] - Training Epoch: 2/2, step 1779/7134 completed (loss: 0.0166593249887228, acc: 0.9959595799446106)
[2025-02-13 03:43:05,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:06,224][root][INFO] - Training Epoch: 2/2, step 1780/7134 completed (loss: 0.030036576092243195, acc: 0.9957567453384399)
[2025-02-13 03:43:06,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:07,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:07,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:07,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:08,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:08,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:08,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:09,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:09,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:10,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:10,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:10,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:11,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:11,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:11,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:12,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:12,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:12,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:13,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:13,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:14,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:14,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:14,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:15,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:15,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:15,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:16,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:16,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:16,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:17,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:17,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:17,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:18,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:18,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:18,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:19,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:19,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:19,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:20,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:20,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:20,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:21,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:21,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:21,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:22,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:22,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:23,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:23,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:23,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:24,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:24,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:24,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:25,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:25,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:25,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:26,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:26,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:26,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:27,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:27,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:27,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:28,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:28,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:28,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:29,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:29,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:29,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:30,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:30,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:30,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:31,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:31,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:31,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:32,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:32,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:32,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:33,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:33,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:33,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:34,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:34,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:34,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:35,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:35,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:35,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:36,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:36,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:36,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:37,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:37,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:37,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:38,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:38,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:38,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:39,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:39,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:39,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:40,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:40,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:40,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:41,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:41,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:41,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:42,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:42,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:42,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:43,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:43,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:43,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:44,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:44,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:45,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:45,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:45,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:45,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:46,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:46,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:46,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:47,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:47,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:47,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:48,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:48,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:49,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:49,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:49,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:50,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:50,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:50,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:51,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:51,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:52,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:52,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:52,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:53,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:53,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:53,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:54,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:54,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:54,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:55,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:55,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:55,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:56,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:56,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:57,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:57,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:57,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:58,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:58,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:58,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:59,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:59,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:59,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:00,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:00,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:00,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:01,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:01,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:01,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:02,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:02,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:02,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:03,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:03,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:03,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:04,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:04,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:04,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:05,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:05,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:05,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:06,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:06,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:07,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:07,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:07,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:08,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:08,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:08,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:09,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:09,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:09,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:10,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:10,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:10,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:11,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:11,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:11,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:12,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:12,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:12,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:13,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:13,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:13,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:14,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:14,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:14,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:15,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:15,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:15,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:16,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:16,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:16,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:17,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:17,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:17,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:18,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:18,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:18,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:19,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:19,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:19,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:20,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:20,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:20,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:21,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:21,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:21,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:22,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:22,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:22,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:23,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:23,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:23,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:24,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:24,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:24,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:25,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:25,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:25,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:26,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:26,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:27,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:27,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:27,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:28,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:28,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:28,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:28,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:29,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:29,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:29,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:30,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:30,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:30,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:31,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:32,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:32,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:32,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:33,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:33,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:33,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:34,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:34,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:34,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:35,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:35,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:35,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:36,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:36,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:36,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:37,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:37,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:38,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:38,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:38,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:39,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:39,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:39,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:40,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:40,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:40,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:41,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:41,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:42,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:42,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:42,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:43,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:43,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:43,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:44,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:44,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:45,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:45,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:45,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:46,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:46,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:47,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:47,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:47,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:48,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:48,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:49,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:49,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:49,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:50,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:50,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:50,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:51,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:51,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:51,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:52,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:52,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:52,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:53,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:53,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:53,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:54,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:54,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:54,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:55,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:55,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:55,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:56,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:56,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:56,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:57,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:57,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:58,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:58,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:58,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:59,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:59,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:59,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:00,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:00,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:01,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:01,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:01,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:02,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:02,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:03,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:03,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:03,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:03,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:04,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:04,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:05,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:05,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:05,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:06,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:06,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:07,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:07,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:07,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:08,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:08,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:08,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:09,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:09,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:09,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:10,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:10,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:10,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:11,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:11,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:11,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:12,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:12,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:13,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:13,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:14,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:15,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:15,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:16,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:16,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:16,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:17,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:17,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:17,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:18,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:18,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:18,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:19,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:19,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:19,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:20,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:20,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:20,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:21,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:21,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:21,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:22,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:22,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:22,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:23,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:23,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:23,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:24,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:24,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:24,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:25,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:25,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:25,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:26,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:26,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:26,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:27,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:27,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:27,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:28,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:28,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:28,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:29,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:29,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:29,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:30,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:30,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:30,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:31,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:31,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:31,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:32,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:32,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:32,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:33,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:33,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:34,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:34,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:34,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:34,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:35,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:35,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:36,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:36,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:36,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:37,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:37,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:37,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:38,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:38,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:38,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:39,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:39,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:39,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:40,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:40,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:40,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:41,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:41,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:41,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:42,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:42,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:42,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:43,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:43,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:43,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:44,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:44,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:44,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:45,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:45,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:45,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:46,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:46,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:46,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:47,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:47,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:47,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:48,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:48,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:48,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:49,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:49,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:49,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:50,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:50,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:50,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:51,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:51,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:51,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:52,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:52,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:53,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:53,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:53,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:54,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:54,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:54,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:55,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:55,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:56,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:56,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:56,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:57,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:57,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:57,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:58,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:58,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:58,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:59,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:59,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:00,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:00,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:00,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:01,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:01,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:01,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:02,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:02,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:03,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:03,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:04,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:04,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:04,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:05,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:05,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:05,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:06,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:06,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:06,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:07,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:07,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:07,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:08,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:08,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:09,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:09,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:09,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:09,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:10,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:10,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:11,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:11,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:11,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:12,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:12,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:12,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:13,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:13,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:13,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:14,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:14,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:14,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:15,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:15,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:15,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:16,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:16,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:16,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:17,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:17,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:17,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:18,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:18,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:18,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:19,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:19,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:20,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:20,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:20,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:21,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:21,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:21,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:22,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:22,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:22,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:23,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:23,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:24,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:24,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:24,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:25,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:25,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:25,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:26,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:26,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:26,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:27,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:27,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:27,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:28,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:28,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:28,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:29,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:29,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:29,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:30,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:30,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:31,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:31,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:32,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:32,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:32,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:33,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:33,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:33,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:34,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:34,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:35,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:35,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:35,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:36,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:36,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:36,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:37,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:37,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:38,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:38,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:38,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:38,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:39,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:39,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:39,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:40,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:40,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:41,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:41,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:41,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:42,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:42,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:42,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:43,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:43,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:43,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:44,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:44,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:44,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:45,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:45,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:45,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:46,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:46,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:47,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:47,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:48,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:48,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:49,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:49,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:50,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:50,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:51,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:51,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:51,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:52,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:52,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:53,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:53,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:53,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:54,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:54,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:54,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:55,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:55,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:55,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:56,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:56,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:57,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:57,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:57,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:58,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:58,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:58,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:59,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:59,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:59,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:00,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:00,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:00,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:01,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:01,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:01,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:02,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:02,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:02,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:03,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:03,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:03,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:04,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:04,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:05,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:05,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:05,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:06,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:06,956][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0528, device='cuda:0') eval_epoch_loss=tensor(0.0515, device='cuda:0') eval_epoch_acc=tensor(0.9865, device='cuda:0')
[2025-02-13 03:47:06,958][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:47:06,959][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:47:07,320][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_1781_loss_0.05146584287285805/model.pt
[2025-02-13 03:47:07,327][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:47:07,328][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.05146584287285805
[2025-02-13 03:47:07,328][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9864792227745056
[2025-02-13 03:47:07,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:07,757][root][INFO] - Training Epoch: 2/2, step 1781/7134 completed (loss: 0.014243446290493011, acc: 0.9938931465148926)
[2025-02-13 03:47:07,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:08,151][root][INFO] - Training Epoch: 2/2, step 1782/7134 completed (loss: 0.012709807604551315, acc: 0.9939117431640625)
[2025-02-13 03:47:08,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:08,578][root][INFO] - Training Epoch: 2/2, step 1783/7134 completed (loss: 0.037407759577035904, acc: 0.9905405640602112)
[2025-02-13 03:47:08,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:09,016][root][INFO] - Training Epoch: 2/2, step 1784/7134 completed (loss: 0.0186893492937088, acc: 0.9941349029541016)
[2025-02-13 03:47:09,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:09,460][root][INFO] - Training Epoch: 2/2, step 1785/7134 completed (loss: 0.021621059626340866, acc: 0.9944979548454285)
[2025-02-13 03:47:09,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:09,863][root][INFO] - Training Epoch: 2/2, step 1786/7134 completed (loss: 0.02803698368370533, acc: 0.9937597513198853)
[2025-02-13 03:47:10,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:10,286][root][INFO] - Training Epoch: 2/2, step 1787/7134 completed (loss: 0.018795359879732132, acc: 0.9946428537368774)
[2025-02-13 03:47:10,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:10,679][root][INFO] - Training Epoch: 2/2, step 1788/7134 completed (loss: 0.02645554021000862, acc: 0.9901800155639648)
[2025-02-13 03:47:10,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:11,097][root][INFO] - Training Epoch: 2/2, step 1789/7134 completed (loss: 0.01447168830782175, acc: 0.996927797794342)
[2025-02-13 03:47:11,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:11,527][root][INFO] - Training Epoch: 2/2, step 1790/7134 completed (loss: 0.1276054084300995, acc: 0.9822866320610046)
[2025-02-13 03:47:11,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:11,930][root][INFO] - Training Epoch: 2/2, step 1791/7134 completed (loss: 0.0989246666431427, acc: 0.9814550876617432)
[2025-02-13 03:47:12,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:12,380][root][INFO] - Training Epoch: 2/2, step 1792/7134 completed (loss: 0.15070433914661407, acc: 0.9756097793579102)
[2025-02-13 03:47:12,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:12,822][root][INFO] - Training Epoch: 2/2, step 1793/7134 completed (loss: 0.11601582169532776, acc: 0.9743243455886841)
[2025-02-13 03:47:12,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:13,271][root][INFO] - Training Epoch: 2/2, step 1794/7134 completed (loss: 0.07688456028699875, acc: 0.9795597195625305)
[2025-02-13 03:47:13,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:13,686][root][INFO] - Training Epoch: 2/2, step 1795/7134 completed (loss: 0.1305561512708664, acc: 0.9680851101875305)
[2025-02-13 03:47:13,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:14,099][root][INFO] - Training Epoch: 2/2, step 1796/7134 completed (loss: 0.1082838773727417, acc: 0.9686520099639893)
[2025-02-13 03:47:14,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:14,519][root][INFO] - Training Epoch: 2/2, step 1797/7134 completed (loss: 0.05268704891204834, acc: 0.9831932783126831)
[2025-02-13 03:47:14,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:14,964][root][INFO] - Training Epoch: 2/2, step 1798/7134 completed (loss: 0.0545695424079895, acc: 0.9826254844665527)
[2025-02-13 03:47:15,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:15,406][root][INFO] - Training Epoch: 2/2, step 1799/7134 completed (loss: 0.07209363579750061, acc: 0.9861809015274048)
[2025-02-13 03:47:15,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:15,858][root][INFO] - Training Epoch: 2/2, step 1800/7134 completed (loss: 0.0979931429028511, acc: 0.9684579372406006)
[2025-02-13 03:47:15,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:16,301][root][INFO] - Training Epoch: 2/2, step 1801/7134 completed (loss: 0.06174704059958458, acc: 0.9830508232116699)
[2025-02-13 03:47:16,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:16,739][root][INFO] - Training Epoch: 2/2, step 1802/7134 completed (loss: 0.06588118523359299, acc: 0.9837296605110168)
[2025-02-13 03:47:16,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:17,157][root][INFO] - Training Epoch: 2/2, step 1803/7134 completed (loss: 0.05022718384861946, acc: 0.9845916628837585)
[2025-02-13 03:47:17,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:17,583][root][INFO] - Training Epoch: 2/2, step 1804/7134 completed (loss: 0.03210698440670967, acc: 0.9901719689369202)
[2025-02-13 03:47:17,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:17,986][root][INFO] - Training Epoch: 2/2, step 1805/7134 completed (loss: 0.0631927102804184, acc: 0.9769119620323181)
[2025-02-13 03:47:18,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:18,388][root][INFO] - Training Epoch: 2/2, step 1806/7134 completed (loss: 0.04274127259850502, acc: 0.9907894730567932)
[2025-02-13 03:47:18,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:18,797][root][INFO] - Training Epoch: 2/2, step 1807/7134 completed (loss: 0.04769318923354149, acc: 0.9844412803649902)
[2025-02-13 03:47:18,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:19,201][root][INFO] - Training Epoch: 2/2, step 1808/7134 completed (loss: 0.028193620964884758, acc: 0.9921976327896118)
[2025-02-13 03:47:19,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:19,649][root][INFO] - Training Epoch: 2/2, step 1809/7134 completed (loss: 0.03084683232009411, acc: 0.9932432174682617)
[2025-02-13 03:47:19,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:20,099][root][INFO] - Training Epoch: 2/2, step 1810/7134 completed (loss: 0.016946475952863693, acc: 0.9948453903198242)
[2025-02-13 03:47:20,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:20,523][root][INFO] - Training Epoch: 2/2, step 1811/7134 completed (loss: 0.0690578743815422, acc: 0.9810040593147278)
[2025-02-13 03:47:20,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:20,959][root][INFO] - Training Epoch: 2/2, step 1812/7134 completed (loss: 0.02866649068892002, acc: 0.9910714030265808)
[2025-02-13 03:47:21,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:21,407][root][INFO] - Training Epoch: 2/2, step 1813/7134 completed (loss: 0.050020892173051834, acc: 0.9880239367485046)
[2025-02-13 03:47:21,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:21,863][root][INFO] - Training Epoch: 2/2, step 1814/7134 completed (loss: 0.01991497166454792, acc: 0.9941725134849548)
[2025-02-13 03:47:21,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:22,294][root][INFO] - Training Epoch: 2/2, step 1815/7134 completed (loss: 0.031541455537080765, acc: 0.9913793206214905)
[2025-02-13 03:47:22,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:22,701][root][INFO] - Training Epoch: 2/2, step 1816/7134 completed (loss: 0.04179694503545761, acc: 0.9890453815460205)
[2025-02-13 03:47:22,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:23,148][root][INFO] - Training Epoch: 2/2, step 1817/7134 completed (loss: 0.03949522227048874, acc: 0.9904534816741943)
[2025-02-13 03:47:23,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:23,599][root][INFO] - Training Epoch: 2/2, step 1818/7134 completed (loss: 0.05294419825077057, acc: 0.9900867342948914)
[2025-02-13 03:47:23,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:24,040][root][INFO] - Training Epoch: 2/2, step 1819/7134 completed (loss: 0.02646307647228241, acc: 0.9937655925750732)
[2025-02-13 03:47:24,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:24,544][root][INFO] - Training Epoch: 2/2, step 1820/7134 completed (loss: 0.016758235171437263, acc: 0.9934498071670532)
[2025-02-13 03:47:24,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:24,979][root][INFO] - Training Epoch: 2/2, step 1821/7134 completed (loss: 0.023427290841937065, acc: 0.9945295453071594)
[2025-02-13 03:47:25,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:25,385][root][INFO] - Training Epoch: 2/2, step 1822/7134 completed (loss: 0.00998133048415184, acc: 0.9970760345458984)
[2025-02-13 03:47:25,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:25,818][root][INFO] - Training Epoch: 2/2, step 1823/7134 completed (loss: 0.025831498205661774, acc: 0.9931787252426147)
[2025-02-13 03:47:25,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:26,254][root][INFO] - Training Epoch: 2/2, step 1824/7134 completed (loss: 0.01721065305173397, acc: 0.995055615901947)
[2025-02-13 03:47:26,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:26,704][root][INFO] - Training Epoch: 2/2, step 1825/7134 completed (loss: 0.025244783610105515, acc: 0.9943946003913879)
[2025-02-13 03:47:26,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:27,143][root][INFO] - Training Epoch: 2/2, step 1826/7134 completed (loss: 0.01404501497745514, acc: 0.9932705163955688)
[2025-02-13 03:47:27,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:27,585][root][INFO] - Training Epoch: 2/2, step 1827/7134 completed (loss: 0.04337841644883156, acc: 0.9900771975517273)
[2025-02-13 03:47:27,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:28,020][root][INFO] - Training Epoch: 2/2, step 1828/7134 completed (loss: 0.007677936926484108, acc: 0.9974259734153748)
[2025-02-13 03:47:28,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:28,456][root][INFO] - Training Epoch: 2/2, step 1829/7134 completed (loss: 0.0195647943764925, acc: 0.9937888383865356)
[2025-02-13 03:47:28,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:28,903][root][INFO] - Training Epoch: 2/2, step 1830/7134 completed (loss: 0.018447550013661385, acc: 0.9939467310905457)
[2025-02-13 03:47:29,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:29,295][root][INFO] - Training Epoch: 2/2, step 1831/7134 completed (loss: 0.00583311403170228, acc: 1.0)
[2025-02-13 03:47:29,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:29,697][root][INFO] - Training Epoch: 2/2, step 1832/7134 completed (loss: 0.06788264960050583, acc: 0.9840989112854004)
[2025-02-13 03:47:29,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:30,134][root][INFO] - Training Epoch: 2/2, step 1833/7134 completed (loss: 0.17824876308441162, acc: 0.9619450569152832)
[2025-02-13 03:47:30,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:30,579][root][INFO] - Training Epoch: 2/2, step 1834/7134 completed (loss: 0.0468447171151638, acc: 0.98591548204422)
[2025-02-13 03:47:30,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:31,000][root][INFO] - Training Epoch: 2/2, step 1835/7134 completed (loss: 0.06856828182935715, acc: 0.9840116500854492)
[2025-02-13 03:47:31,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:31,442][root][INFO] - Training Epoch: 2/2, step 1836/7134 completed (loss: 0.025266772136092186, acc: 0.991725742816925)
[2025-02-13 03:47:31,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:31,878][root][INFO] - Training Epoch: 2/2, step 1837/7134 completed (loss: 0.018398882821202278, acc: 0.9901719689369202)
[2025-02-13 03:47:31,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:32,206][root][INFO] - Training Epoch: 2/2, step 1838/7134 completed (loss: 0.008503790013492107, acc: 0.9979591965675354)
[2025-02-13 03:47:32,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:32,652][root][INFO] - Training Epoch: 2/2, step 1839/7134 completed (loss: 0.02905462123453617, acc: 0.9909326434135437)
[2025-02-13 03:47:32,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:33,096][root][INFO] - Training Epoch: 2/2, step 1840/7134 completed (loss: 0.029990633949637413, acc: 0.9930264949798584)
[2025-02-13 03:47:33,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:33,505][root][INFO] - Training Epoch: 2/2, step 1841/7134 completed (loss: 0.026331620290875435, acc: 0.9914893507957458)
[2025-02-13 03:47:33,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:33,952][root][INFO] - Training Epoch: 2/2, step 1842/7134 completed (loss: 0.06824950128793716, acc: 0.9877551198005676)
[2025-02-13 03:47:34,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:34,368][root][INFO] - Training Epoch: 2/2, step 1843/7134 completed (loss: 0.05002441257238388, acc: 0.9862499833106995)
[2025-02-13 03:47:34,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:34,814][root][INFO] - Training Epoch: 2/2, step 1844/7134 completed (loss: 0.03202228248119354, acc: 0.9934959411621094)
[2025-02-13 03:47:34,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:35,272][root][INFO] - Training Epoch: 2/2, step 1845/7134 completed (loss: 0.036689043045043945, acc: 0.9895615577697754)
[2025-02-13 03:47:35,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:35,690][root][INFO] - Training Epoch: 2/2, step 1846/7134 completed (loss: 0.018730908632278442, acc: 0.995726466178894)
[2025-02-13 03:47:35,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:36,149][root][INFO] - Training Epoch: 2/2, step 1847/7134 completed (loss: 0.021665720269083977, acc: 0.9953917264938354)
[2025-02-13 03:47:36,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:36,600][root][INFO] - Training Epoch: 2/2, step 1848/7134 completed (loss: 0.04339481517672539, acc: 0.991525411605835)
[2025-02-13 03:47:36,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:37,031][root][INFO] - Training Epoch: 2/2, step 1849/7134 completed (loss: 0.060956940054893494, acc: 0.982425332069397)
[2025-02-13 03:47:37,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:37,484][root][INFO] - Training Epoch: 2/2, step 1850/7134 completed (loss: 0.020785389468073845, acc: 0.9938875436782837)
[2025-02-13 03:47:37,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:37,930][root][INFO] - Training Epoch: 2/2, step 1851/7134 completed (loss: 0.04655279964208603, acc: 0.9865689873695374)
[2025-02-13 03:47:38,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:38,340][root][INFO] - Training Epoch: 2/2, step 1852/7134 completed (loss: 0.036595236510038376, acc: 0.9875776171684265)
[2025-02-13 03:47:38,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:38,777][root][INFO] - Training Epoch: 2/2, step 1853/7134 completed (loss: 0.0405159667134285, acc: 0.9916267991065979)
[2025-02-13 03:47:38,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:39,217][root][INFO] - Training Epoch: 2/2, step 1854/7134 completed (loss: 0.05079168081283569, acc: 0.9797859787940979)
[2025-02-13 03:47:39,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:39,661][root][INFO] - Training Epoch: 2/2, step 1855/7134 completed (loss: 0.04897112026810646, acc: 0.9872832298278809)
[2025-02-13 03:47:39,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:40,076][root][INFO] - Training Epoch: 2/2, step 1856/7134 completed (loss: 0.041644178330898285, acc: 0.9889298677444458)
[2025-02-13 03:47:40,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:40,539][root][INFO] - Training Epoch: 2/2, step 1857/7134 completed (loss: 0.05089164897799492, acc: 0.9864531755447388)
[2025-02-13 03:47:40,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:40,985][root][INFO] - Training Epoch: 2/2, step 1858/7134 completed (loss: 0.02344762347638607, acc: 0.9916267991065979)
[2025-02-13 03:47:41,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:41,447][root][INFO] - Training Epoch: 2/2, step 1859/7134 completed (loss: 0.015366882085800171, acc: 0.9965870380401611)
[2025-02-13 03:47:41,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:41,893][root][INFO] - Training Epoch: 2/2, step 1860/7134 completed (loss: 0.015228222124278545, acc: 0.9962025284767151)
[2025-02-13 03:47:42,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:42,327][root][INFO] - Training Epoch: 2/2, step 1861/7134 completed (loss: 0.0510110966861248, acc: 0.9909560680389404)
[2025-02-13 03:47:42,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:42,759][root][INFO] - Training Epoch: 2/2, step 1862/7134 completed (loss: 0.01916705258190632, acc: 0.9945725798606873)
[2025-02-13 03:47:42,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:43,195][root][INFO] - Training Epoch: 2/2, step 1863/7134 completed (loss: 0.02293250523507595, acc: 0.9928057789802551)
[2025-02-13 03:47:43,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:43,609][root][INFO] - Training Epoch: 2/2, step 1864/7134 completed (loss: 0.12852367758750916, acc: 0.9687075018882751)
[2025-02-13 03:47:43,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:43,999][root][INFO] - Training Epoch: 2/2, step 1865/7134 completed (loss: 0.025900287553668022, acc: 0.9881556630134583)
[2025-02-13 03:47:44,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:44,426][root][INFO] - Training Epoch: 2/2, step 1866/7134 completed (loss: 0.01341235265135765, acc: 0.994397759437561)
[2025-02-13 03:47:44,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:44,884][root][INFO] - Training Epoch: 2/2, step 1867/7134 completed (loss: 0.02476687915623188, acc: 0.9911727905273438)
[2025-02-13 03:47:45,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:45,335][root][INFO] - Training Epoch: 2/2, step 1868/7134 completed (loss: 0.06336380541324615, acc: 0.9851484894752502)
[2025-02-13 03:47:45,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:45,753][root][INFO] - Training Epoch: 2/2, step 1869/7134 completed (loss: 0.0391041599214077, acc: 0.9885807633399963)
[2025-02-13 03:47:45,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:46,159][root][INFO] - Training Epoch: 2/2, step 1870/7134 completed (loss: 0.010062286630272865, acc: 0.9986979365348816)
[2025-02-13 03:47:46,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:46,599][root][INFO] - Training Epoch: 2/2, step 1871/7134 completed (loss: 0.065973661839962, acc: 0.9885807633399963)
[2025-02-13 03:47:46,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:46,857][root][INFO] - Training Epoch: 2/2, step 1872/7134 completed (loss: 0.4399150311946869, acc: 0.9171270728111267)
[2025-02-13 03:47:47,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:47,272][root][INFO] - Training Epoch: 2/2, step 1873/7134 completed (loss: 0.11577360332012177, acc: 0.9691211581230164)
[2025-02-13 03:47:47,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:47,723][root][INFO] - Training Epoch: 2/2, step 1874/7134 completed (loss: 0.049628060311079025, acc: 0.9842725992202759)
[2025-02-13 03:47:47,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:48,119][root][INFO] - Training Epoch: 2/2, step 1875/7134 completed (loss: 0.028840908780694008, acc: 0.9956331849098206)
[2025-02-13 03:47:48,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:48,533][root][INFO] - Training Epoch: 2/2, step 1876/7134 completed (loss: 0.11092624068260193, acc: 0.9660441279411316)
[2025-02-13 03:47:48,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:48,970][root][INFO] - Training Epoch: 2/2, step 1877/7134 completed (loss: 0.046212535351514816, acc: 0.989924430847168)
[2025-02-13 03:47:49,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:49,402][root][INFO] - Training Epoch: 2/2, step 1878/7134 completed (loss: 0.0724235400557518, acc: 0.973805844783783)
[2025-02-13 03:47:49,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:49,820][root][INFO] - Training Epoch: 2/2, step 1879/7134 completed (loss: 0.06811896711587906, acc: 0.9771615266799927)
[2025-02-13 03:47:49,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:50,188][root][INFO] - Training Epoch: 2/2, step 1880/7134 completed (loss: 0.054662659764289856, acc: 0.98591548204422)
[2025-02-13 03:47:50,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:50,612][root][INFO] - Training Epoch: 2/2, step 1881/7134 completed (loss: 0.06663811206817627, acc: 0.983660101890564)
[2025-02-13 03:47:50,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:50,947][root][INFO] - Training Epoch: 2/2, step 1882/7134 completed (loss: 0.046330347657203674, acc: 0.9852216839790344)
[2025-02-13 03:47:51,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:51,365][root][INFO] - Training Epoch: 2/2, step 1883/7134 completed (loss: 0.059040751308202744, acc: 0.9822834730148315)
[2025-02-13 03:47:51,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:51,774][root][INFO] - Training Epoch: 2/2, step 1884/7134 completed (loss: 0.07979513704776764, acc: 0.9747235178947449)
[2025-02-13 03:47:51,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:52,177][root][INFO] - Training Epoch: 2/2, step 1885/7134 completed (loss: 0.058656297624111176, acc: 0.9812382459640503)
[2025-02-13 03:47:52,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:52,591][root][INFO] - Training Epoch: 2/2, step 1886/7134 completed (loss: 0.08757927268743515, acc: 0.9749478101730347)
[2025-02-13 03:47:52,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:52,995][root][INFO] - Training Epoch: 2/2, step 1887/7134 completed (loss: 0.06271515786647797, acc: 0.9810246825218201)
[2025-02-13 03:47:53,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:53,425][root][INFO] - Training Epoch: 2/2, step 1888/7134 completed (loss: 0.06725505739450455, acc: 0.984644889831543)
[2025-02-13 03:47:53,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:53,873][root][INFO] - Training Epoch: 2/2, step 1889/7134 completed (loss: 0.03434194624423981, acc: 0.986994206905365)
[2025-02-13 03:47:54,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:54,298][root][INFO] - Training Epoch: 2/2, step 1890/7134 completed (loss: 0.02771654725074768, acc: 0.9922600388526917)
[2025-02-13 03:47:54,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:54,711][root][INFO] - Training Epoch: 2/2, step 1891/7134 completed (loss: 0.08968015015125275, acc: 0.9740259647369385)
[2025-02-13 03:47:54,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:55,128][root][INFO] - Training Epoch: 2/2, step 1892/7134 completed (loss: 0.05638352036476135, acc: 0.984544038772583)
[2025-02-13 03:47:55,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:55,531][root][INFO] - Training Epoch: 2/2, step 1893/7134 completed (loss: 0.030990520492196083, acc: 0.9875195026397705)
[2025-02-13 03:47:55,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:55,941][root][INFO] - Training Epoch: 2/2, step 1894/7134 completed (loss: 0.09074443578720093, acc: 0.9791666865348816)
[2025-02-13 03:47:56,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:56,283][root][INFO] - Training Epoch: 2/2, step 1895/7134 completed (loss: 0.21449735760688782, acc: 0.9411764740943909)
[2025-02-13 03:47:56,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:56,687][root][INFO] - Training Epoch: 2/2, step 1896/7134 completed (loss: 0.06749512255191803, acc: 0.9764705896377563)
[2025-02-13 03:47:56,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:57,112][root][INFO] - Training Epoch: 2/2, step 1897/7134 completed (loss: 0.01718321070075035, acc: 0.9944444298744202)
[2025-02-13 03:47:57,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:57,512][root][INFO] - Training Epoch: 2/2, step 1898/7134 completed (loss: 0.03613005205988884, acc: 0.9883913993835449)
[2025-02-13 03:47:57,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:57,868][root][INFO] - Training Epoch: 2/2, step 1899/7134 completed (loss: 0.04900703579187393, acc: 0.9873417615890503)
[2025-02-13 03:47:57,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:58,278][root][INFO] - Training Epoch: 2/2, step 1900/7134 completed (loss: 0.02196669578552246, acc: 0.9967637658119202)
[2025-02-13 03:47:58,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:58,692][root][INFO] - Training Epoch: 2/2, step 1901/7134 completed (loss: 0.05785509943962097, acc: 0.9853747487068176)
[2025-02-13 03:47:58,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:59,093][root][INFO] - Training Epoch: 2/2, step 1902/7134 completed (loss: 0.027964185923337936, acc: 0.9929701089859009)
[2025-02-13 03:47:59,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:59,507][root][INFO] - Training Epoch: 2/2, step 1903/7134 completed (loss: 0.022621070966124535, acc: 0.9930555820465088)
[2025-02-13 03:47:59,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:59,898][root][INFO] - Training Epoch: 2/2, step 1904/7134 completed (loss: 0.020837394520640373, acc: 0.991525411605835)
[2025-02-13 03:48:00,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:00,285][root][INFO] - Training Epoch: 2/2, step 1905/7134 completed (loss: 0.04067414999008179, acc: 0.9897750616073608)
[2025-02-13 03:48:00,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:00,670][root][INFO] - Training Epoch: 2/2, step 1906/7134 completed (loss: 0.04555322602391243, acc: 0.9863547682762146)
[2025-02-13 03:48:00,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:01,065][root][INFO] - Training Epoch: 2/2, step 1907/7134 completed (loss: 0.03710397705435753, acc: 0.9932773113250732)
[2025-02-13 03:48:01,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:01,507][root][INFO] - Training Epoch: 2/2, step 1908/7134 completed (loss: 0.03123866766691208, acc: 0.9914070963859558)
[2025-02-13 03:48:01,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:01,921][root][INFO] - Training Epoch: 2/2, step 1909/7134 completed (loss: 0.03477148711681366, acc: 0.9867549538612366)
[2025-02-13 03:48:02,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:02,371][root][INFO] - Training Epoch: 2/2, step 1910/7134 completed (loss: 0.034846171736717224, acc: 0.9923245906829834)
[2025-02-13 03:48:02,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:02,821][root][INFO] - Training Epoch: 2/2, step 1911/7134 completed (loss: 0.0359380878508091, acc: 0.9915048480033875)
[2025-02-13 03:48:02,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:03,263][root][INFO] - Training Epoch: 2/2, step 1912/7134 completed (loss: 0.010324111208319664, acc: 0.9960052967071533)
[2025-02-13 03:48:03,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:03,631][root][INFO] - Training Epoch: 2/2, step 1913/7134 completed (loss: 0.06657575815916061, acc: 0.9870129823684692)
[2025-02-13 03:48:03,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:04,062][root][INFO] - Training Epoch: 2/2, step 1914/7134 completed (loss: 0.055372633039951324, acc: 0.9864681959152222)
[2025-02-13 03:48:04,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:04,454][root][INFO] - Training Epoch: 2/2, step 1915/7134 completed (loss: 0.04146203026175499, acc: 0.9869494438171387)
[2025-02-13 03:48:04,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:04,856][root][INFO] - Training Epoch: 2/2, step 1916/7134 completed (loss: 0.03323950991034508, acc: 0.9853836894035339)
[2025-02-13 03:48:04,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:05,292][root][INFO] - Training Epoch: 2/2, step 1917/7134 completed (loss: 0.036690179258584976, acc: 0.9915966391563416)
[2025-02-13 03:48:05,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:05,680][root][INFO] - Training Epoch: 2/2, step 1918/7134 completed (loss: 0.05184422805905342, acc: 0.9890282154083252)
[2025-02-13 03:48:05,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:06,134][root][INFO] - Training Epoch: 2/2, step 1919/7134 completed (loss: 0.041659023612737656, acc: 0.9870874881744385)
[2025-02-13 03:48:06,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:06,563][root][INFO] - Training Epoch: 2/2, step 1920/7134 completed (loss: 0.024514909833669662, acc: 0.9969135522842407)
[2025-02-13 03:48:06,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:06,967][root][INFO] - Training Epoch: 2/2, step 1921/7134 completed (loss: 0.022625653073191643, acc: 0.9944598078727722)
[2025-02-13 03:48:07,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:07,429][root][INFO] - Training Epoch: 2/2, step 1922/7134 completed (loss: 0.030292311683297157, acc: 0.9940357804298401)
[2025-02-13 03:48:07,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:07,858][root][INFO] - Training Epoch: 2/2, step 1923/7134 completed (loss: 0.021763402968645096, acc: 0.9916247725486755)
[2025-02-13 03:48:07,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:08,284][root][INFO] - Training Epoch: 2/2, step 1924/7134 completed (loss: 0.04321639984846115, acc: 0.9871244430541992)
[2025-02-13 03:48:08,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:08,746][root][INFO] - Training Epoch: 2/2, step 1925/7134 completed (loss: 0.055466178804636, acc: 0.9830687642097473)
[2025-02-13 03:48:08,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:09,206][root][INFO] - Training Epoch: 2/2, step 1926/7134 completed (loss: 0.015629291534423828, acc: 0.9948927760124207)
[2025-02-13 03:48:09,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:09,667][root][INFO] - Training Epoch: 2/2, step 1927/7134 completed (loss: 0.05184895545244217, acc: 0.9922651648521423)
[2025-02-13 03:48:09,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:10,131][root][INFO] - Training Epoch: 2/2, step 1928/7134 completed (loss: 0.031155183911323547, acc: 0.9908814430236816)
[2025-02-13 03:48:10,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:10,538][root][INFO] - Training Epoch: 2/2, step 1929/7134 completed (loss: 0.021230921149253845, acc: 0.9957627058029175)
[2025-02-13 03:48:10,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:10,982][root][INFO] - Training Epoch: 2/2, step 1930/7134 completed (loss: 0.07163448631763458, acc: 0.9803149700164795)
[2025-02-13 03:48:11,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:11,441][root][INFO] - Training Epoch: 2/2, step 1931/7134 completed (loss: 0.11107523739337921, acc: 0.9772440195083618)
[2025-02-13 03:48:11,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:11,832][root][INFO] - Training Epoch: 2/2, step 1932/7134 completed (loss: 0.081949882209301, acc: 0.9761193990707397)
[2025-02-13 03:48:11,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:12,268][root][INFO] - Training Epoch: 2/2, step 1933/7134 completed (loss: 0.12211286276578903, acc: 0.970588207244873)
[2025-02-13 03:48:12,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:12,674][root][INFO] - Training Epoch: 2/2, step 1934/7134 completed (loss: 0.037519071251153946, acc: 0.9971751570701599)
[2025-02-13 03:48:12,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:13,121][root][INFO] - Training Epoch: 2/2, step 1935/7134 completed (loss: 0.1158207356929779, acc: 0.9694322943687439)
[2025-02-13 03:48:13,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:13,568][root][INFO] - Training Epoch: 2/2, step 1936/7134 completed (loss: 0.019656112417578697, acc: 0.9952606558799744)
[2025-02-13 03:48:13,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:13,957][root][INFO] - Training Epoch: 2/2, step 1937/7134 completed (loss: 0.03584633395075798, acc: 0.9886363744735718)
[2025-02-13 03:48:14,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:14,377][root][INFO] - Training Epoch: 2/2, step 1938/7134 completed (loss: 0.06160915270447731, acc: 0.9886040091514587)
[2025-02-13 03:48:14,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:14,788][root][INFO] - Training Epoch: 2/2, step 1939/7134 completed (loss: 0.07257167249917984, acc: 0.9894067645072937)
[2025-02-13 03:48:14,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:15,223][root][INFO] - Training Epoch: 2/2, step 1940/7134 completed (loss: 0.06308242678642273, acc: 0.9795918464660645)
[2025-02-13 03:48:15,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:15,672][root][INFO] - Training Epoch: 2/2, step 1941/7134 completed (loss: 0.049480851739645004, acc: 0.9826202988624573)
[2025-02-13 03:48:15,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:16,064][root][INFO] - Training Epoch: 2/2, step 1942/7134 completed (loss: 0.04644077271223068, acc: 0.9878048896789551)
[2025-02-13 03:48:16,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:16,522][root][INFO] - Training Epoch: 2/2, step 1943/7134 completed (loss: 0.048956118524074554, acc: 0.9841463565826416)
[2025-02-13 03:48:16,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:16,957][root][INFO] - Training Epoch: 2/2, step 1944/7134 completed (loss: 0.03199710696935654, acc: 0.9890310764312744)
[2025-02-13 03:48:17,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:17,385][root][INFO] - Training Epoch: 2/2, step 1945/7134 completed (loss: 0.04480037838220596, acc: 0.9803149700164795)
[2025-02-13 03:48:17,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:17,808][root][INFO] - Training Epoch: 2/2, step 1946/7134 completed (loss: 0.03809559345245361, acc: 0.9864176511764526)
[2025-02-13 03:48:17,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:18,237][root][INFO] - Training Epoch: 2/2, step 1947/7134 completed (loss: 0.057058293372392654, acc: 0.9802631735801697)
[2025-02-13 03:48:18,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:18,649][root][INFO] - Training Epoch: 2/2, step 1948/7134 completed (loss: 0.0830564871430397, acc: 0.9753320813179016)
[2025-02-13 03:48:18,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:19,001][root][INFO] - Training Epoch: 2/2, step 1949/7134 completed (loss: 0.03935914859175682, acc: 0.9866220951080322)
[2025-02-13 03:48:19,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:19,435][root][INFO] - Training Epoch: 2/2, step 1950/7134 completed (loss: 0.046518757939338684, acc: 0.985401451587677)
[2025-02-13 03:48:19,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:19,898][root][INFO] - Training Epoch: 2/2, step 1951/7134 completed (loss: 0.05511163920164108, acc: 0.9916387796401978)
[2025-02-13 03:48:20,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:20,313][root][INFO] - Training Epoch: 2/2, step 1952/7134 completed (loss: 0.07777871936559677, acc: 0.980033278465271)
[2025-02-13 03:48:20,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:20,737][root][INFO] - Training Epoch: 2/2, step 1953/7134 completed (loss: 0.0311735887080431, acc: 0.99071204662323)
[2025-02-13 03:48:20,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:21,100][root][INFO] - Training Epoch: 2/2, step 1954/7134 completed (loss: 0.025181211531162262, acc: 0.995121955871582)
[2025-02-13 03:48:21,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:21,502][root][INFO] - Training Epoch: 2/2, step 1955/7134 completed (loss: 0.03531232103705406, acc: 0.9894551634788513)
[2025-02-13 03:48:21,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:21,803][root][INFO] - Training Epoch: 2/2, step 1956/7134 completed (loss: 0.023090926930308342, acc: 0.9919354915618896)
[2025-02-13 03:48:21,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:22,191][root][INFO] - Training Epoch: 2/2, step 1957/7134 completed (loss: 0.049456000328063965, acc: 0.9851852059364319)
[2025-02-13 03:48:22,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:22,453][root][INFO] - Training Epoch: 2/2, step 1958/7134 completed (loss: 0.034270476549863815, acc: 0.9942693114280701)
[2025-02-13 03:48:22,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:22,831][root][INFO] - Training Epoch: 2/2, step 1959/7134 completed (loss: 0.023295694962143898, acc: 0.9936034083366394)
[2025-02-13 03:48:22,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:23,241][root][INFO] - Training Epoch: 2/2, step 1960/7134 completed (loss: 0.01299750991165638, acc: 0.9962406158447266)
[2025-02-13 03:48:23,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:23,601][root][INFO] - Training Epoch: 2/2, step 1961/7134 completed (loss: 0.031774021685123444, acc: 0.9966555237770081)
[2025-02-13 03:48:23,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:24,016][root][INFO] - Training Epoch: 2/2, step 1962/7134 completed (loss: 0.04340397194027901, acc: 0.9916201233863831)
[2025-02-13 03:48:24,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:24,413][root][INFO] - Training Epoch: 2/2, step 1963/7134 completed (loss: 0.03499350696802139, acc: 0.9913644194602966)
[2025-02-13 03:48:24,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:24,833][root][INFO] - Training Epoch: 2/2, step 1964/7134 completed (loss: 0.04226088523864746, acc: 0.9841269850730896)
[2025-02-13 03:48:24,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:25,245][root][INFO] - Training Epoch: 2/2, step 1965/7134 completed (loss: 0.019471649080514908, acc: 0.9944211840629578)
[2025-02-13 03:48:25,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:25,662][root][INFO] - Training Epoch: 2/2, step 1966/7134 completed (loss: 0.02790069580078125, acc: 0.9908257126808167)
[2025-02-13 03:48:25,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:26,109][root][INFO] - Training Epoch: 2/2, step 1967/7134 completed (loss: 0.06382282823324203, acc: 0.9808917045593262)
[2025-02-13 03:48:26,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:26,560][root][INFO] - Training Epoch: 2/2, step 1968/7134 completed (loss: 0.02030865289270878, acc: 0.9925925731658936)
[2025-02-13 03:48:26,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:26,971][root][INFO] - Training Epoch: 2/2, step 1969/7134 completed (loss: 0.03429699316620827, acc: 0.9906666874885559)
[2025-02-13 03:48:27,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:27,405][root][INFO] - Training Epoch: 2/2, step 1970/7134 completed (loss: 0.022550512105226517, acc: 0.9923312664031982)
[2025-02-13 03:48:27,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:27,815][root][INFO] - Training Epoch: 2/2, step 1971/7134 completed (loss: 0.039446476846933365, acc: 0.988063633441925)
[2025-02-13 03:48:27,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:28,168][root][INFO] - Training Epoch: 2/2, step 1972/7134 completed (loss: 0.18329210579395294, acc: 0.9484029412269592)
[2025-02-13 03:48:28,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:28,567][root][INFO] - Training Epoch: 2/2, step 1973/7134 completed (loss: 0.09009268879890442, acc: 0.9638095498085022)
[2025-02-13 03:48:28,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:29,015][root][INFO] - Training Epoch: 2/2, step 1974/7134 completed (loss: 0.03299691155552864, acc: 0.9886934757232666)
[2025-02-13 03:48:29,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:29,423][root][INFO] - Training Epoch: 2/2, step 1975/7134 completed (loss: 0.020221566781401634, acc: 0.9969696998596191)
[2025-02-13 03:48:29,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:29,870][root][INFO] - Training Epoch: 2/2, step 1976/7134 completed (loss: 0.04210277646780014, acc: 0.987542450428009)
[2025-02-13 03:48:30,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:30,307][root][INFO] - Training Epoch: 2/2, step 1977/7134 completed (loss: 0.03366589918732643, acc: 0.9884058237075806)
[2025-02-13 03:48:30,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:30,731][root][INFO] - Training Epoch: 2/2, step 1978/7134 completed (loss: 0.017225176095962524, acc: 0.9973154067993164)
[2025-02-13 03:48:30,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:31,180][root][INFO] - Training Epoch: 2/2, step 1979/7134 completed (loss: 0.021569812670350075, acc: 0.9934895634651184)
[2025-02-13 03:48:31,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:31,631][root][INFO] - Training Epoch: 2/2, step 1980/7134 completed (loss: 0.09973559528589249, acc: 0.9804432988166809)
[2025-02-13 03:48:31,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:32,075][root][INFO] - Training Epoch: 2/2, step 1981/7134 completed (loss: 0.057105932384729385, acc: 0.9790209531784058)
[2025-02-13 03:48:32,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:32,488][root][INFO] - Training Epoch: 2/2, step 1982/7134 completed (loss: 0.03506812825798988, acc: 0.9871299862861633)
[2025-02-13 03:48:32,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:32,901][root][INFO] - Training Epoch: 2/2, step 1983/7134 completed (loss: 0.024676548317074776, acc: 0.9949109554290771)
[2025-02-13 03:48:33,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:33,317][root][INFO] - Training Epoch: 2/2, step 1984/7134 completed (loss: 0.013326122425496578, acc: 0.9959623217582703)
[2025-02-13 03:48:33,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:33,729][root][INFO] - Training Epoch: 2/2, step 1985/7134 completed (loss: 0.02512262389063835, acc: 0.9902507066726685)
[2025-02-13 03:48:33,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:34,167][root][INFO] - Training Epoch: 2/2, step 1986/7134 completed (loss: 0.04341619461774826, acc: 0.9884169697761536)
[2025-02-13 03:48:34,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:34,610][root][INFO] - Training Epoch: 2/2, step 1987/7134 completed (loss: 0.0466177836060524, acc: 0.9872521162033081)
[2025-02-13 03:48:34,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:35,044][root][INFO] - Training Epoch: 2/2, step 1988/7134 completed (loss: 0.03959212079644203, acc: 0.9871612191200256)
[2025-02-13 03:48:35,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:35,436][root][INFO] - Training Epoch: 2/2, step 1989/7134 completed (loss: 0.060391079634428024, acc: 0.9856459498405457)
[2025-02-13 03:48:35,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:35,879][root][INFO] - Training Epoch: 2/2, step 1990/7134 completed (loss: 0.031720615923404694, acc: 0.9951456189155579)
[2025-02-13 03:48:36,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:36,320][root][INFO] - Training Epoch: 2/2, step 1991/7134 completed (loss: 0.022141676396131516, acc: 0.9916943311691284)
[2025-02-13 03:48:36,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:36,736][root][INFO] - Training Epoch: 2/2, step 1992/7134 completed (loss: 0.04042454808950424, acc: 0.9891473054885864)
[2025-02-13 03:48:36,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:37,138][root][INFO] - Training Epoch: 2/2, step 1993/7134 completed (loss: 0.011849860660731792, acc: 0.9982425570487976)
[2025-02-13 03:48:37,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:37,551][root][INFO] - Training Epoch: 2/2, step 1994/7134 completed (loss: 0.010865557007491589, acc: 0.9965517520904541)
[2025-02-13 03:48:37,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:37,964][root][INFO] - Training Epoch: 2/2, step 1995/7134 completed (loss: 0.026132693514227867, acc: 0.9938176274299622)
[2025-02-13 03:48:38,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:38,372][root][INFO] - Training Epoch: 2/2, step 1996/7134 completed (loss: 0.036991845816373825, acc: 0.9900826215744019)
[2025-02-13 03:48:38,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:38,780][root][INFO] - Training Epoch: 2/2, step 1997/7134 completed (loss: 0.02125735394656658, acc: 0.9970887899398804)
[2025-02-13 03:48:38,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:39,186][root][INFO] - Training Epoch: 2/2, step 1998/7134 completed (loss: 0.021769795566797256, acc: 0.9921507239341736)
[2025-02-13 03:48:39,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:39,596][root][INFO] - Training Epoch: 2/2, step 1999/7134 completed (loss: 0.03700752183794975, acc: 0.9881154298782349)
[2025-02-13 03:48:39,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:40,003][root][INFO] - Training Epoch: 2/2, step 2000/7134 completed (loss: 0.0303315632045269, acc: 0.9921135902404785)
[2025-02-13 03:48:40,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:40,391][root][INFO] - Training Epoch: 2/2, step 2001/7134 completed (loss: 0.01646954007446766, acc: 0.9954545497894287)
[2025-02-13 03:48:40,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:40,797][root][INFO] - Training Epoch: 2/2, step 2002/7134 completed (loss: 0.035353805869817734, acc: 0.991304337978363)
[2025-02-13 03:48:40,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:41,155][root][INFO] - Training Epoch: 2/2, step 2003/7134 completed (loss: 0.014350243844091892, acc: 0.9979166388511658)
[2025-02-13 03:48:41,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:41,515][root][INFO] - Training Epoch: 2/2, step 2004/7134 completed (loss: 0.1720397174358368, acc: 0.9578059315681458)
[2025-02-13 03:48:41,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:41,957][root][INFO] - Training Epoch: 2/2, step 2005/7134 completed (loss: 0.09475544840097427, acc: 0.9808219075202942)
[2025-02-13 03:48:42,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:42,309][root][INFO] - Training Epoch: 2/2, step 2006/7134 completed (loss: 0.017549147829413414, acc: 0.9942362904548645)
[2025-02-13 03:48:42,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:42,713][root][INFO] - Training Epoch: 2/2, step 2007/7134 completed (loss: 0.041286587715148926, acc: 0.9899598360061646)
[2025-02-13 03:48:42,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:43,102][root][INFO] - Training Epoch: 2/2, step 2008/7134 completed (loss: 0.051157671958208084, acc: 0.9786477088928223)
[2025-02-13 03:48:43,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:43,468][root][INFO] - Training Epoch: 2/2, step 2009/7134 completed (loss: 0.027591830119490623, acc: 0.9927797913551331)
[2025-02-13 03:48:43,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:43,862][root][INFO] - Training Epoch: 2/2, step 2010/7134 completed (loss: 0.019833305850625038, acc: 0.994854211807251)
[2025-02-13 03:48:43,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:44,249][root][INFO] - Training Epoch: 2/2, step 2011/7134 completed (loss: 0.015061506070196629, acc: 0.9960861206054688)
[2025-02-13 03:48:44,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:44,647][root][INFO] - Training Epoch: 2/2, step 2012/7134 completed (loss: 0.027384094893932343, acc: 0.9915433526039124)
[2025-02-13 03:48:44,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:45,071][root][INFO] - Training Epoch: 2/2, step 2013/7134 completed (loss: 0.0150441974401474, acc: 0.9980000257492065)
[2025-02-13 03:48:45,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:45,485][root][INFO] - Training Epoch: 2/2, step 2014/7134 completed (loss: 0.025755178183317184, acc: 0.9937008023262024)
[2025-02-13 03:48:45,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:45,898][root][INFO] - Training Epoch: 2/2, step 2015/7134 completed (loss: 0.02755708433687687, acc: 0.9917762875556946)
[2025-02-13 03:48:46,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:46,333][root][INFO] - Training Epoch: 2/2, step 2016/7134 completed (loss: 0.060836996883153915, acc: 0.9846153855323792)
[2025-02-13 03:48:46,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:46,756][root][INFO] - Training Epoch: 2/2, step 2017/7134 completed (loss: 0.06693799793720245, acc: 0.9851351380348206)
[2025-02-13 03:48:46,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:47,174][root][INFO] - Training Epoch: 2/2, step 2018/7134 completed (loss: 0.021381502971053123, acc: 0.9935897588729858)
[2025-02-13 03:48:47,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:47,575][root][INFO] - Training Epoch: 2/2, step 2019/7134 completed (loss: 0.03841359540820122, acc: 0.9875862002372742)
[2025-02-13 03:48:47,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:47,995][root][INFO] - Training Epoch: 2/2, step 2020/7134 completed (loss: 0.01370900496840477, acc: 0.9966158866882324)
[2025-02-13 03:48:48,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:48,421][root][INFO] - Training Epoch: 2/2, step 2021/7134 completed (loss: 0.049654800444841385, acc: 0.9870129823684692)
[2025-02-13 03:48:48,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:48,814][root][INFO] - Training Epoch: 2/2, step 2022/7134 completed (loss: 0.03607979044318199, acc: 0.9965338110923767)
[2025-02-13 03:48:48,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:49,232][root][INFO] - Training Epoch: 2/2, step 2023/7134 completed (loss: 0.03805137425661087, acc: 0.9883211851119995)
[2025-02-13 03:48:49,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:49,664][root][INFO] - Training Epoch: 2/2, step 2024/7134 completed (loss: 0.0439174547791481, acc: 0.9902777671813965)
[2025-02-13 03:48:49,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:50,097][root][INFO] - Training Epoch: 2/2, step 2025/7134 completed (loss: 0.03944594785571098, acc: 0.9919999837875366)
[2025-02-13 03:48:50,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:50,513][root][INFO] - Training Epoch: 2/2, step 2026/7134 completed (loss: 0.049913715571165085, acc: 0.9849397540092468)
[2025-02-13 03:48:50,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:50,926][root][INFO] - Training Epoch: 2/2, step 2027/7134 completed (loss: 0.026952778920531273, acc: 0.9933554530143738)
[2025-02-13 03:48:51,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:51,348][root][INFO] - Training Epoch: 2/2, step 2028/7134 completed (loss: 0.03131924942135811, acc: 0.9926900863647461)
[2025-02-13 03:48:51,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:51,765][root][INFO] - Training Epoch: 2/2, step 2029/7134 completed (loss: 0.038335952907800674, acc: 0.9886363744735718)
[2025-02-13 03:48:51,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:52,159][root][INFO] - Training Epoch: 2/2, step 2030/7134 completed (loss: 0.04706408455967903, acc: 0.9785276055335999)
[2025-02-13 03:48:52,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:52,605][root][INFO] - Training Epoch: 2/2, step 2031/7134 completed (loss: 0.03610654175281525, acc: 0.9889570474624634)
[2025-02-13 03:48:52,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:53,026][root][INFO] - Training Epoch: 2/2, step 2032/7134 completed (loss: 0.041576724499464035, acc: 0.9879032373428345)
[2025-02-13 03:48:53,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:53,463][root][INFO] - Training Epoch: 2/2, step 2033/7134 completed (loss: 0.029923515394330025, acc: 0.994854211807251)
[2025-02-13 03:48:53,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:53,858][root][INFO] - Training Epoch: 2/2, step 2034/7134 completed (loss: 0.05388893559575081, acc: 0.9877488613128662)
[2025-02-13 03:48:54,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:54,301][root][INFO] - Training Epoch: 2/2, step 2035/7134 completed (loss: 0.014111971482634544, acc: 0.9962639808654785)
[2025-02-13 03:48:54,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:54,688][root][INFO] - Training Epoch: 2/2, step 2036/7134 completed (loss: 0.02239907719194889, acc: 0.9948096871376038)
[2025-02-13 03:48:54,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:55,133][root][INFO] - Training Epoch: 2/2, step 2037/7134 completed (loss: 0.015662122517824173, acc: 0.9940652847290039)
[2025-02-13 03:48:55,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:55,582][root][INFO] - Training Epoch: 2/2, step 2038/7134 completed (loss: 0.020488666370511055, acc: 0.9929078221321106)
[2025-02-13 03:48:55,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:56,000][root][INFO] - Training Epoch: 2/2, step 2039/7134 completed (loss: 0.006883425638079643, acc: 1.0)
[2025-02-13 03:48:56,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:56,359][root][INFO] - Training Epoch: 2/2, step 2040/7134 completed (loss: 0.04769469052553177, acc: 0.991769552230835)
[2025-02-13 03:48:56,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:56,800][root][INFO] - Training Epoch: 2/2, step 2041/7134 completed (loss: 0.011439265683293343, acc: 0.9958791136741638)
[2025-02-13 03:48:56,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:57,199][root][INFO] - Training Epoch: 2/2, step 2042/7134 completed (loss: 0.01160829421132803, acc: 0.9979838728904724)
[2025-02-13 03:48:57,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:57,596][root][INFO] - Training Epoch: 2/2, step 2043/7134 completed (loss: 0.045282550156116486, acc: 0.9906250238418579)
[2025-02-13 03:48:57,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:58,024][root][INFO] - Training Epoch: 2/2, step 2044/7134 completed (loss: 0.02254505455493927, acc: 0.9964221715927124)
[2025-02-13 03:48:58,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:58,416][root][INFO] - Training Epoch: 2/2, step 2045/7134 completed (loss: 0.019674580544233322, acc: 0.994575023651123)
[2025-02-13 03:48:58,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:58,764][root][INFO] - Training Epoch: 2/2, step 2046/7134 completed (loss: 0.02478684112429619, acc: 0.9898374080657959)
[2025-02-13 03:48:58,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:59,185][root][INFO] - Training Epoch: 2/2, step 2047/7134 completed (loss: 0.026085421442985535, acc: 0.989983320236206)
[2025-02-13 03:48:59,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:59,603][root][INFO] - Training Epoch: 2/2, step 2048/7134 completed (loss: 0.04702354222536087, acc: 0.9865471124649048)
[2025-02-13 03:48:59,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:00,007][root][INFO] - Training Epoch: 2/2, step 2049/7134 completed (loss: 0.06698883324861526, acc: 0.981176495552063)
[2025-02-13 03:49:00,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:00,389][root][INFO] - Training Epoch: 2/2, step 2050/7134 completed (loss: 0.08544817566871643, acc: 0.9846153855323792)
[2025-02-13 03:49:00,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:00,787][root][INFO] - Training Epoch: 2/2, step 2051/7134 completed (loss: 0.052256252616643906, acc: 0.986328125)
[2025-02-13 03:49:00,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:01,197][root][INFO] - Training Epoch: 2/2, step 2052/7134 completed (loss: 0.06943486630916595, acc: 0.9829351305961609)
[2025-02-13 03:49:01,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:01,596][root][INFO] - Training Epoch: 2/2, step 2053/7134 completed (loss: 0.032710961997509, acc: 0.9919999837875366)
[2025-02-13 03:49:01,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:02,023][root][INFO] - Training Epoch: 2/2, step 2054/7134 completed (loss: 0.1111261174082756, acc: 0.9697452187538147)
[2025-02-13 03:49:02,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:02,449][root][INFO] - Training Epoch: 2/2, step 2055/7134 completed (loss: 0.08596962690353394, acc: 0.977952778339386)
[2025-02-13 03:49:02,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:02,905][root][INFO] - Training Epoch: 2/2, step 2056/7134 completed (loss: 0.05480145663022995, acc: 0.9831387996673584)
[2025-02-13 03:49:03,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:03,327][root][INFO] - Training Epoch: 2/2, step 2057/7134 completed (loss: 0.06315715610980988, acc: 0.9810040593147278)
[2025-02-13 03:49:03,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:03,769][root][INFO] - Training Epoch: 2/2, step 2058/7134 completed (loss: 0.0999860167503357, acc: 0.9770444631576538)
[2025-02-13 03:49:03,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:04,191][root][INFO] - Training Epoch: 2/2, step 2059/7134 completed (loss: 0.1109282597899437, acc: 0.9757365584373474)
[2025-02-13 03:49:04,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:04,617][root][INFO] - Training Epoch: 2/2, step 2060/7134 completed (loss: 0.03691644221544266, acc: 0.9899857044219971)
[2025-02-13 03:49:04,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:05,046][root][INFO] - Training Epoch: 2/2, step 2061/7134 completed (loss: 0.06538600474596024, acc: 0.980169951915741)
[2025-02-13 03:49:05,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:05,399][root][INFO] - Training Epoch: 2/2, step 2062/7134 completed (loss: 0.07324039936065674, acc: 0.9772295951843262)
[2025-02-13 03:49:05,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:05,787][root][INFO] - Training Epoch: 2/2, step 2063/7134 completed (loss: 0.075264111161232, acc: 0.97947758436203)
[2025-02-13 03:49:05,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:06,203][root][INFO] - Training Epoch: 2/2, step 2064/7134 completed (loss: 0.023289157077670097, acc: 0.9937499761581421)
[2025-02-13 03:49:06,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:06,603][root][INFO] - Training Epoch: 2/2, step 2065/7134 completed (loss: 0.07140569388866425, acc: 0.9842519760131836)
[2025-02-13 03:49:06,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:06,966][root][INFO] - Training Epoch: 2/2, step 2066/7134 completed (loss: 0.0448794960975647, acc: 0.9859719276428223)
[2025-02-13 03:49:07,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:07,367][root][INFO] - Training Epoch: 2/2, step 2067/7134 completed (loss: 0.02927020564675331, acc: 0.992409884929657)
[2025-02-13 03:49:07,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:07,768][root][INFO] - Training Epoch: 2/2, step 2068/7134 completed (loss: 0.05382578819990158, acc: 0.9836065769195557)
[2025-02-13 03:49:07,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:08,203][root][INFO] - Training Epoch: 2/2, step 2069/7134 completed (loss: 0.04209798946976662, acc: 0.9861538410186768)
[2025-02-13 03:49:08,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:08,602][root][INFO] - Training Epoch: 2/2, step 2070/7134 completed (loss: 0.04153409227728844, acc: 0.9904943108558655)
[2025-02-13 03:49:08,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:09,002][root][INFO] - Training Epoch: 2/2, step 2071/7134 completed (loss: 0.016334019601345062, acc: 0.9953161478042603)
[2025-02-13 03:49:09,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:09,388][root][INFO] - Training Epoch: 2/2, step 2072/7134 completed (loss: 0.02129816822707653, acc: 0.9925742745399475)
[2025-02-13 03:49:09,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:09,773][root][INFO] - Training Epoch: 2/2, step 2073/7134 completed (loss: 0.03450147435069084, acc: 0.9903846383094788)
[2025-02-13 03:49:09,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:10,225][root][INFO] - Training Epoch: 2/2, step 2074/7134 completed (loss: 0.03995034843683243, acc: 0.987089216709137)
[2025-02-13 03:49:10,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:10,625][root][INFO] - Training Epoch: 2/2, step 2075/7134 completed (loss: 0.045525021851062775, acc: 0.9816360473632812)
[2025-02-13 03:49:10,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:11,064][root][INFO] - Training Epoch: 2/2, step 2076/7134 completed (loss: 0.028139542788267136, acc: 0.9923076629638672)
[2025-02-13 03:49:11,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:11,495][root][INFO] - Training Epoch: 2/2, step 2077/7134 completed (loss: 0.032433267682790756, acc: 0.9907038807868958)
[2025-02-13 03:49:11,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:11,945][root][INFO] - Training Epoch: 2/2, step 2078/7134 completed (loss: 0.02576242946088314, acc: 0.9906976819038391)
[2025-02-13 03:49:12,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:12,382][root][INFO] - Training Epoch: 2/2, step 2079/7134 completed (loss: 0.01911047473549843, acc: 0.9933862686157227)
[2025-02-13 03:49:12,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:12,788][root][INFO] - Training Epoch: 2/2, step 2080/7134 completed (loss: 0.030144240707159042, acc: 0.9926470518112183)
[2025-02-13 03:49:12,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:13,209][root][INFO] - Training Epoch: 2/2, step 2081/7134 completed (loss: 0.010513312183320522, acc: 0.9970972537994385)
[2025-02-13 03:49:13,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:13,654][root][INFO] - Training Epoch: 2/2, step 2082/7134 completed (loss: 0.09732728451490402, acc: 0.9790356159210205)
[2025-02-13 03:49:13,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:14,034][root][INFO] - Training Epoch: 2/2, step 2083/7134 completed (loss: 0.05682152137160301, acc: 0.9925925731658936)
[2025-02-13 03:49:14,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:14,445][root][INFO] - Training Epoch: 2/2, step 2084/7134 completed (loss: 0.02902061678469181, acc: 0.9937888383865356)
[2025-02-13 03:49:14,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:14,848][root][INFO] - Training Epoch: 2/2, step 2085/7134 completed (loss: 0.018687047064304352, acc: 0.996303141117096)
[2025-02-13 03:49:14,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:15,208][root][INFO] - Training Epoch: 2/2, step 2086/7134 completed (loss: 0.024996649473905563, acc: 0.9885844588279724)
[2025-02-13 03:49:15,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:15,643][root][INFO] - Training Epoch: 2/2, step 2087/7134 completed (loss: 0.005438740365207195, acc: 1.0)
[2025-02-13 03:49:15,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:16,071][root][INFO] - Training Epoch: 2/2, step 2088/7134 completed (loss: 0.024395788088440895, acc: 0.9927954077720642)
[2025-02-13 03:49:16,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:16,504][root][INFO] - Training Epoch: 2/2, step 2089/7134 completed (loss: 0.020807676017284393, acc: 0.9952681660652161)
[2025-02-13 03:49:16,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:16,897][root][INFO] - Training Epoch: 2/2, step 2090/7134 completed (loss: 0.010091674514114857, acc: 0.998420238494873)
[2025-02-13 03:49:17,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:17,300][root][INFO] - Training Epoch: 2/2, step 2091/7134 completed (loss: 0.02728448063135147, acc: 0.9949324131011963)
[2025-02-13 03:49:17,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:17,745][root][INFO] - Training Epoch: 2/2, step 2092/7134 completed (loss: 0.012928239069879055, acc: 0.9984227418899536)
[2025-02-13 03:49:17,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:18,197][root][INFO] - Training Epoch: 2/2, step 2093/7134 completed (loss: 0.020728493109345436, acc: 0.9918588995933533)
[2025-02-13 03:49:18,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:18,555][root][INFO] - Training Epoch: 2/2, step 2094/7134 completed (loss: 0.04175649955868721, acc: 0.9918864369392395)
[2025-02-13 03:49:18,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:18,991][root][INFO] - Training Epoch: 2/2, step 2095/7134 completed (loss: 0.03532987833023071, acc: 0.9934036731719971)
[2025-02-13 03:49:19,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:19,380][root][INFO] - Training Epoch: 2/2, step 2096/7134 completed (loss: 0.021963467821478844, acc: 0.9948275685310364)
[2025-02-13 03:49:19,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:19,788][root][INFO] - Training Epoch: 2/2, step 2097/7134 completed (loss: 0.013267237693071365, acc: 0.9957924485206604)
[2025-02-13 03:49:19,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:20,193][root][INFO] - Training Epoch: 2/2, step 2098/7134 completed (loss: 0.020565617829561234, acc: 0.9967845678329468)
[2025-02-13 03:49:20,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:20,584][root][INFO] - Training Epoch: 2/2, step 2099/7134 completed (loss: 0.03262832388281822, acc: 0.988990843296051)
[2025-02-13 03:49:20,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:21,016][root][INFO] - Training Epoch: 2/2, step 2100/7134 completed (loss: 0.017641279846429825, acc: 0.9946879148483276)
[2025-02-13 03:49:21,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:21,436][root][INFO] - Training Epoch: 2/2, step 2101/7134 completed (loss: 0.016900796443223953, acc: 0.992443323135376)
[2025-02-13 03:49:21,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:21,831][root][INFO] - Training Epoch: 2/2, step 2102/7134 completed (loss: 0.035631466656923294, acc: 0.9897959232330322)
[2025-02-13 03:49:21,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:22,234][root][INFO] - Training Epoch: 2/2, step 2103/7134 completed (loss: 0.02685302309691906, acc: 0.9923312664031982)
[2025-02-13 03:49:22,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:22,676][root][INFO] - Training Epoch: 2/2, step 2104/7134 completed (loss: 0.017035139724612236, acc: 0.9974226951599121)
[2025-02-13 03:49:22,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:23,075][root][INFO] - Training Epoch: 2/2, step 2105/7134 completed (loss: 0.0561080239713192, acc: 0.9827855825424194)
[2025-02-13 03:49:23,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:23,484][root][INFO] - Training Epoch: 2/2, step 2106/7134 completed (loss: 0.04927898198366165, acc: 0.9899425506591797)
[2025-02-13 03:49:23,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:23,925][root][INFO] - Training Epoch: 2/2, step 2107/7134 completed (loss: 0.022458387538790703, acc: 0.9932523369789124)
[2025-02-13 03:49:24,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:24,365][root][INFO] - Training Epoch: 2/2, step 2108/7134 completed (loss: 0.024356670677661896, acc: 0.9910314083099365)
[2025-02-13 03:49:24,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:24,766][root][INFO] - Training Epoch: 2/2, step 2109/7134 completed (loss: 0.06456019729375839, acc: 0.9823633432388306)
[2025-02-13 03:49:24,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:25,222][root][INFO] - Training Epoch: 2/2, step 2110/7134 completed (loss: 0.058732397854328156, acc: 0.9825581312179565)
[2025-02-13 03:49:25,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:25,637][root][INFO] - Training Epoch: 2/2, step 2111/7134 completed (loss: 0.048658452928066254, acc: 0.9883268475532532)
[2025-02-13 03:49:25,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:26,010][root][INFO] - Training Epoch: 2/2, step 2112/7134 completed (loss: 0.06554901599884033, acc: 0.9792993664741516)
[2025-02-13 03:49:26,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:26,418][root][INFO] - Training Epoch: 2/2, step 2113/7134 completed (loss: 0.020724954083561897, acc: 0.992548406124115)
[2025-02-13 03:49:26,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:26,831][root][INFO] - Training Epoch: 2/2, step 2114/7134 completed (loss: 0.029100773856043816, acc: 0.9934640526771545)
[2025-02-13 03:49:26,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:27,273][root][INFO] - Training Epoch: 2/2, step 2115/7134 completed (loss: 0.030528973788022995, acc: 0.9924905896186829)
[2025-02-13 03:49:27,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:27,683][root][INFO] - Training Epoch: 2/2, step 2116/7134 completed (loss: 0.03909905254840851, acc: 0.9923312664031982)
[2025-02-13 03:49:27,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:28,107][root][INFO] - Training Epoch: 2/2, step 2117/7134 completed (loss: 0.01800423674285412, acc: 0.9937810897827148)
[2025-02-13 03:49:28,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:28,518][root][INFO] - Training Epoch: 2/2, step 2118/7134 completed (loss: 0.031256623566150665, acc: 0.9931787252426147)
[2025-02-13 03:49:28,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:28,932][root][INFO] - Training Epoch: 2/2, step 2119/7134 completed (loss: 0.03241495043039322, acc: 0.9947506785392761)
[2025-02-13 03:49:29,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:29,337][root][INFO] - Training Epoch: 2/2, step 2120/7134 completed (loss: 0.03491071239113808, acc: 0.990867555141449)
[2025-02-13 03:49:29,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:29,712][root][INFO] - Training Epoch: 2/2, step 2121/7134 completed (loss: 0.030972840264439583, acc: 0.9904761910438538)
[2025-02-13 03:49:29,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:30,134][root][INFO] - Training Epoch: 2/2, step 2122/7134 completed (loss: 0.01942787878215313, acc: 0.9947916865348816)
[2025-02-13 03:49:30,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:30,533][root][INFO] - Training Epoch: 2/2, step 2123/7134 completed (loss: 0.022407082840800285, acc: 0.9952830076217651)
[2025-02-13 03:49:30,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:30,926][root][INFO] - Training Epoch: 2/2, step 2124/7134 completed (loss: 0.020160777494311333, acc: 0.9906542301177979)
[2025-02-13 03:49:31,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:31,344][root][INFO] - Training Epoch: 2/2, step 2125/7134 completed (loss: 0.011182744987308979, acc: 0.9984848499298096)
[2025-02-13 03:49:31,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:31,768][root][INFO] - Training Epoch: 2/2, step 2126/7134 completed (loss: 0.024402225390076637, acc: 0.9949579834938049)
[2025-02-13 03:49:31,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:32,165][root][INFO] - Training Epoch: 2/2, step 2127/7134 completed (loss: 0.01721492037177086, acc: 0.9970501661300659)
[2025-02-13 03:49:32,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:32,599][root][INFO] - Training Epoch: 2/2, step 2128/7134 completed (loss: 0.021636618301272392, acc: 0.990813672542572)
[2025-02-13 03:49:32,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:33,020][root][INFO] - Training Epoch: 2/2, step 2129/7134 completed (loss: 0.03795270994305611, acc: 0.9864253401756287)
[2025-02-13 03:49:33,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:33,422][root][INFO] - Training Epoch: 2/2, step 2130/7134 completed (loss: 0.03091510385274887, acc: 0.9917491674423218)
[2025-02-13 03:49:33,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:33,802][root][INFO] - Training Epoch: 2/2, step 2131/7134 completed (loss: 0.016443295404314995, acc: 0.9969040155410767)
[2025-02-13 03:49:33,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:34,210][root][INFO] - Training Epoch: 2/2, step 2132/7134 completed (loss: 0.016037223860621452, acc: 0.9946091771125793)
[2025-02-13 03:49:34,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:34,625][root][INFO] - Training Epoch: 2/2, step 2133/7134 completed (loss: 0.023475397378206253, acc: 0.9933949708938599)
[2025-02-13 03:49:34,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:35,044][root][INFO] - Training Epoch: 2/2, step 2134/7134 completed (loss: 0.009645507670938969, acc: 0.9973226189613342)
[2025-02-13 03:49:35,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:35,440][root][INFO] - Training Epoch: 2/2, step 2135/7134 completed (loss: 0.02770570106804371, acc: 0.9894578456878662)
[2025-02-13 03:49:35,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:35,862][root][INFO] - Training Epoch: 2/2, step 2136/7134 completed (loss: 0.01997959613800049, acc: 0.9959946870803833)
[2025-02-13 03:49:36,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:36,308][root][INFO] - Training Epoch: 2/2, step 2137/7134 completed (loss: 0.014580887742340565, acc: 0.9948387145996094)
[2025-02-13 03:49:36,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:36,702][root][INFO] - Training Epoch: 2/2, step 2138/7134 completed (loss: 0.00787354912608862, acc: 0.9984756112098694)
[2025-02-13 03:49:36,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:37,118][root][INFO] - Training Epoch: 2/2, step 2139/7134 completed (loss: 0.011077134869992733, acc: 0.9958391189575195)
[2025-02-13 03:49:37,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:37,691][root][INFO] - Training Epoch: 2/2, step 2140/7134 completed (loss: 0.04344230517745018, acc: 0.9862259030342102)
[2025-02-13 03:49:37,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:38,004][root][INFO] - Training Epoch: 2/2, step 2141/7134 completed (loss: 0.020668191835284233, acc: 0.9941520690917969)
[2025-02-13 03:49:38,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:38,447][root][INFO] - Training Epoch: 2/2, step 2142/7134 completed (loss: 0.04292585328221321, acc: 0.9844192862510681)
[2025-02-13 03:49:38,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:38,886][root][INFO] - Training Epoch: 2/2, step 2143/7134 completed (loss: 0.019319932907819748, acc: 0.9947368502616882)
[2025-02-13 03:49:39,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:39,337][root][INFO] - Training Epoch: 2/2, step 2144/7134 completed (loss: 0.004465972073376179, acc: 0.9986720085144043)
[2025-02-13 03:49:39,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:39,777][root][INFO] - Training Epoch: 2/2, step 2145/7134 completed (loss: 0.0377061665058136, acc: 0.9842632412910461)
[2025-02-13 03:49:39,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:40,160][root][INFO] - Training Epoch: 2/2, step 2146/7134 completed (loss: 0.0200597383081913, acc: 0.9964788556098938)
[2025-02-13 03:49:40,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:40,622][root][INFO] - Training Epoch: 2/2, step 2147/7134 completed (loss: 0.029274219647049904, acc: 0.9887780547142029)
[2025-02-13 03:49:40,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:41,027][root][INFO] - Training Epoch: 2/2, step 2148/7134 completed (loss: 0.04272635281085968, acc: 0.9868995547294617)
[2025-02-13 03:49:41,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:41,453][root][INFO] - Training Epoch: 2/2, step 2149/7134 completed (loss: 0.007960151880979538, acc: 0.9985915422439575)
[2025-02-13 03:49:41,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:41,859][root][INFO] - Training Epoch: 2/2, step 2150/7134 completed (loss: 0.022784560918807983, acc: 0.9906666874885559)
[2025-02-13 03:49:41,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:42,273][root][INFO] - Training Epoch: 2/2, step 2151/7134 completed (loss: 0.01885242573916912, acc: 0.9928571581840515)
[2025-02-13 03:49:42,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:42,686][root][INFO] - Training Epoch: 2/2, step 2152/7134 completed (loss: 0.06422527134418488, acc: 0.9779411554336548)
[2025-02-13 03:49:42,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:43,194][root][INFO] - Training Epoch: 2/2, step 2153/7134 completed (loss: 0.040611930191516876, acc: 0.9900110960006714)
[2025-02-13 03:49:43,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:43,664][root][INFO] - Training Epoch: 2/2, step 2154/7134 completed (loss: 0.051637161523103714, acc: 0.983565092086792)
[2025-02-13 03:49:43,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:44,045][root][INFO] - Training Epoch: 2/2, step 2155/7134 completed (loss: 0.07679739594459534, acc: 0.9780219793319702)
[2025-02-13 03:49:44,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:44,448][root][INFO] - Training Epoch: 2/2, step 2156/7134 completed (loss: 0.018348855897784233, acc: 0.9930555820465088)
[2025-02-13 03:49:44,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:44,904][root][INFO] - Training Epoch: 2/2, step 2157/7134 completed (loss: 0.024120286107063293, acc: 0.9910614490509033)
[2025-02-13 03:49:45,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:45,368][root][INFO] - Training Epoch: 2/2, step 2158/7134 completed (loss: 0.0629316046833992, acc: 0.9897959232330322)
[2025-02-13 03:49:45,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:45,810][root][INFO] - Training Epoch: 2/2, step 2159/7134 completed (loss: 0.019895361736416817, acc: 0.9961685538291931)
[2025-02-13 03:49:45,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:46,259][root][INFO] - Training Epoch: 2/2, step 2160/7134 completed (loss: 0.0242353156208992, acc: 0.9917469024658203)
[2025-02-13 03:49:46,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:46,707][root][INFO] - Training Epoch: 2/2, step 2161/7134 completed (loss: 0.040683191269636154, acc: 0.98975670337677)
[2025-02-13 03:49:46,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:47,140][root][INFO] - Training Epoch: 2/2, step 2162/7134 completed (loss: 0.03074747510254383, acc: 0.9894099831581116)
[2025-02-13 03:49:47,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:47,596][root][INFO] - Training Epoch: 2/2, step 2163/7134 completed (loss: 0.037099190056324005, acc: 0.9889763593673706)
[2025-02-13 03:49:47,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:47,994][root][INFO] - Training Epoch: 2/2, step 2164/7134 completed (loss: 0.018450425937771797, acc: 0.9931389093399048)
[2025-02-13 03:49:48,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:48,446][root][INFO] - Training Epoch: 2/2, step 2165/7134 completed (loss: 0.024688322097063065, acc: 0.9928264021873474)
[2025-02-13 03:49:48,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:48,873][root][INFO] - Training Epoch: 2/2, step 2166/7134 completed (loss: 0.03221221640706062, acc: 0.9934895634651184)
[2025-02-13 03:49:49,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:49,283][root][INFO] - Training Epoch: 2/2, step 2167/7134 completed (loss: 0.03785770386457443, acc: 0.9910913109779358)
[2025-02-13 03:49:49,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:49,698][root][INFO] - Training Epoch: 2/2, step 2168/7134 completed (loss: 0.03104078769683838, acc: 0.9888888597488403)
[2025-02-13 03:49:49,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:50,140][root][INFO] - Training Epoch: 2/2, step 2169/7134 completed (loss: 0.041630685329437256, acc: 0.9930394291877747)
[2025-02-13 03:49:50,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:50,562][root][INFO] - Training Epoch: 2/2, step 2170/7134 completed (loss: 0.034189797937870026, acc: 0.9912663698196411)
[2025-02-13 03:49:50,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:51,019][root][INFO] - Training Epoch: 2/2, step 2171/7134 completed (loss: 0.037014108151197433, acc: 0.9909399747848511)
[2025-02-13 03:49:51,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:51,481][root][INFO] - Training Epoch: 2/2, step 2172/7134 completed (loss: 0.024249473586678505, acc: 0.9896694421768188)
[2025-02-13 03:49:51,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:51,911][root][INFO] - Training Epoch: 2/2, step 2173/7134 completed (loss: 0.04509498551487923, acc: 0.9909365773200989)
[2025-02-13 03:49:52,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:52,337][root][INFO] - Training Epoch: 2/2, step 2174/7134 completed (loss: 0.013408195227384567, acc: 0.9943262338638306)
[2025-02-13 03:49:52,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:52,762][root][INFO] - Training Epoch: 2/2, step 2175/7134 completed (loss: 0.07472552359104156, acc: 0.9861496090888977)
[2025-02-13 03:49:52,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:53,247][root][INFO] - Training Epoch: 2/2, step 2176/7134 completed (loss: 0.03641781583428383, acc: 0.9881516695022583)
[2025-02-13 03:49:53,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:53,702][root][INFO] - Training Epoch: 2/2, step 2177/7134 completed (loss: 0.028362426906824112, acc: 0.9896103739738464)
[2025-02-13 03:49:53,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:54,133][root][INFO] - Training Epoch: 2/2, step 2178/7134 completed (loss: 0.01648251712322235, acc: 0.9955849647521973)
[2025-02-13 03:49:54,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:54,577][root][INFO] - Training Epoch: 2/2, step 2179/7134 completed (loss: 0.028960095718503, acc: 0.9949367046356201)
[2025-02-13 03:49:54,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:55,031][root][INFO] - Training Epoch: 2/2, step 2180/7134 completed (loss: 0.026935070753097534, acc: 0.9905437231063843)
[2025-02-13 03:49:55,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:55,498][root][INFO] - Training Epoch: 2/2, step 2181/7134 completed (loss: 0.01889115944504738, acc: 0.995708167552948)
[2025-02-13 03:49:55,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:55,937][root][INFO] - Training Epoch: 2/2, step 2182/7134 completed (loss: 0.04283669590950012, acc: 0.9861591458320618)
[2025-02-13 03:49:56,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:56,420][root][INFO] - Training Epoch: 2/2, step 2183/7134 completed (loss: 0.027756545692682266, acc: 0.9941792488098145)
[2025-02-13 03:49:56,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:56,876][root][INFO] - Training Epoch: 2/2, step 2184/7134 completed (loss: 0.024597102776169777, acc: 0.9936102032661438)
[2025-02-13 03:49:57,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:57,308][root][INFO] - Training Epoch: 2/2, step 2185/7134 completed (loss: 0.05272785946726799, acc: 0.9858356714248657)
[2025-02-13 03:49:57,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:57,750][root][INFO] - Training Epoch: 2/2, step 2186/7134 completed (loss: 0.06970827281475067, acc: 0.9854133129119873)
[2025-02-13 03:49:57,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:58,179][root][INFO] - Training Epoch: 2/2, step 2187/7134 completed (loss: 0.08755387365818024, acc: 0.9807999730110168)
[2025-02-13 03:49:58,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:58,627][root][INFO] - Training Epoch: 2/2, step 2188/7134 completed (loss: 0.02846536971628666, acc: 0.9890561103820801)
[2025-02-13 03:49:58,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:59,038][root][INFO] - Training Epoch: 2/2, step 2189/7134 completed (loss: 0.0456143356859684, acc: 0.9816513657569885)
[2025-02-13 03:49:59,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:59,491][root][INFO] - Training Epoch: 2/2, step 2190/7134 completed (loss: 0.046989407390356064, acc: 0.9868074059486389)
[2025-02-13 03:49:59,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:59,861][root][INFO] - Training Epoch: 2/2, step 2191/7134 completed (loss: 0.026554645970463753, acc: 0.9945799708366394)
[2025-02-13 03:49:59,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:00,261][root][INFO] - Training Epoch: 2/2, step 2192/7134 completed (loss: 0.009986094199120998, acc: 0.9962335228919983)
[2025-02-13 03:50:00,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:00,700][root][INFO] - Training Epoch: 2/2, step 2193/7134 completed (loss: 0.03297512233257294, acc: 0.9933510422706604)
[2025-02-13 03:50:00,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:01,111][root][INFO] - Training Epoch: 2/2, step 2194/7134 completed (loss: 0.03733539581298828, acc: 0.9882746934890747)
[2025-02-13 03:50:01,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:01,535][root][INFO] - Training Epoch: 2/2, step 2195/7134 completed (loss: 0.015721958130598068, acc: 0.9936000108718872)
[2025-02-13 03:50:01,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:01,974][root][INFO] - Training Epoch: 2/2, step 2196/7134 completed (loss: 0.02617437206208706, acc: 0.9935232996940613)
[2025-02-13 03:50:02,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:02,360][root][INFO] - Training Epoch: 2/2, step 2197/7134 completed (loss: 0.029447495937347412, acc: 0.9890710115432739)
[2025-02-13 03:50:02,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:02,783][root][INFO] - Training Epoch: 2/2, step 2198/7134 completed (loss: 0.04186020791530609, acc: 0.9894419312477112)
[2025-02-13 03:50:02,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:03,200][root][INFO] - Training Epoch: 2/2, step 2199/7134 completed (loss: 0.014552492648363113, acc: 0.995468258857727)
[2025-02-13 03:50:03,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:03,626][root][INFO] - Training Epoch: 2/2, step 2200/7134 completed (loss: 0.02854068949818611, acc: 0.9927667379379272)
[2025-02-13 03:50:03,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:04,052][root][INFO] - Training Epoch: 2/2, step 2201/7134 completed (loss: 0.015140960924327374, acc: 0.9969696998596191)
[2025-02-13 03:50:04,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:04,468][root][INFO] - Training Epoch: 2/2, step 2202/7134 completed (loss: 0.02482445538043976, acc: 0.9916527271270752)
[2025-02-13 03:50:04,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:04,918][root][INFO] - Training Epoch: 2/2, step 2203/7134 completed (loss: 0.008162179961800575, acc: 0.9972602725028992)
[2025-02-13 03:50:05,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:05,328][root][INFO] - Training Epoch: 2/2, step 2204/7134 completed (loss: 0.012940964661538601, acc: 0.9941520690917969)
[2025-02-13 03:50:05,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:05,745][root][INFO] - Training Epoch: 2/2, step 2205/7134 completed (loss: 0.025953572243452072, acc: 0.9929824471473694)
[2025-02-13 03:50:05,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:06,148][root][INFO] - Training Epoch: 2/2, step 2206/7134 completed (loss: 0.04972166195511818, acc: 0.9859402179718018)
[2025-02-13 03:50:06,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:06,557][root][INFO] - Training Epoch: 2/2, step 2207/7134 completed (loss: 0.013853393495082855, acc: 0.9951534867286682)
[2025-02-13 03:50:06,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:06,961][root][INFO] - Training Epoch: 2/2, step 2208/7134 completed (loss: 0.041484616696834564, acc: 0.9917355179786682)
[2025-02-13 03:50:07,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:07,374][root][INFO] - Training Epoch: 2/2, step 2209/7134 completed (loss: 0.025483328849077225, acc: 0.9900990128517151)
[2025-02-13 03:50:07,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:07,778][root][INFO] - Training Epoch: 2/2, step 2210/7134 completed (loss: 0.03539443016052246, acc: 0.9893778562545776)
[2025-02-13 03:50:07,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:08,206][root][INFO] - Training Epoch: 2/2, step 2211/7134 completed (loss: 0.028799235820770264, acc: 0.9944598078727722)
[2025-02-13 03:50:08,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:08,631][root][INFO] - Training Epoch: 2/2, step 2212/7134 completed (loss: 0.007197273895144463, acc: 0.9986110925674438)
[2025-02-13 03:50:08,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:09,048][root][INFO] - Training Epoch: 2/2, step 2213/7134 completed (loss: 0.021527038887143135, acc: 0.9901130199432373)
[2025-02-13 03:50:09,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:09,463][root][INFO] - Training Epoch: 2/2, step 2214/7134 completed (loss: 0.03829691931605339, acc: 0.9880596995353699)
[2025-02-13 03:50:09,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:09,869][root][INFO] - Training Epoch: 2/2, step 2215/7134 completed (loss: 0.019400708377361298, acc: 0.992977499961853)
[2025-02-13 03:50:10,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:10,283][root][INFO] - Training Epoch: 2/2, step 2216/7134 completed (loss: 0.018832821398973465, acc: 0.9919893145561218)
[2025-02-13 03:50:10,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:10,709][root][INFO] - Training Epoch: 2/2, step 2217/7134 completed (loss: 0.027284041047096252, acc: 0.9925280213356018)
[2025-02-13 03:50:10,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:11,109][root][INFO] - Training Epoch: 2/2, step 2218/7134 completed (loss: 0.033553171902894974, acc: 0.9928160905838013)
[2025-02-13 03:50:11,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:11,509][root][INFO] - Training Epoch: 2/2, step 2219/7134 completed (loss: 0.013016967102885246, acc: 0.9953632354736328)
[2025-02-13 03:50:11,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:11,912][root][INFO] - Training Epoch: 2/2, step 2220/7134 completed (loss: 0.009432118386030197, acc: 1.0)
[2025-02-13 03:50:12,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:12,345][root][INFO] - Training Epoch: 2/2, step 2221/7134 completed (loss: 0.020527807995676994, acc: 0.9961038827896118)
[2025-02-13 03:50:12,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:12,754][root][INFO] - Training Epoch: 2/2, step 2222/7134 completed (loss: 0.024842670187354088, acc: 0.9927745461463928)
[2025-02-13 03:50:12,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:13,162][root][INFO] - Training Epoch: 2/2, step 2223/7134 completed (loss: 0.021633241325616837, acc: 0.9958158731460571)
[2025-02-13 03:50:13,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:13,595][root][INFO] - Training Epoch: 2/2, step 2224/7134 completed (loss: 0.015395061112940311, acc: 0.9958847761154175)
[2025-02-13 03:50:13,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:14,012][root][INFO] - Training Epoch: 2/2, step 2225/7134 completed (loss: 0.013404754921793938, acc: 0.9956584572792053)
[2025-02-13 03:50:14,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:14,430][root][INFO] - Training Epoch: 2/2, step 2226/7134 completed (loss: 0.021002814173698425, acc: 0.9923896789550781)
[2025-02-13 03:50:14,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:14,840][root][INFO] - Training Epoch: 2/2, step 2227/7134 completed (loss: 0.014891141094267368, acc: 0.9970717430114746)
[2025-02-13 03:50:14,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:15,227][root][INFO] - Training Epoch: 2/2, step 2228/7134 completed (loss: 0.011829793453216553, acc: 0.9967479705810547)
[2025-02-13 03:50:15,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:15,637][root][INFO] - Training Epoch: 2/2, step 2229/7134 completed (loss: 0.01908627338707447, acc: 0.9959999918937683)
[2025-02-13 03:50:15,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:16,029][root][INFO] - Training Epoch: 2/2, step 2230/7134 completed (loss: 0.017911432310938835, acc: 0.9967637658119202)
[2025-02-13 03:50:16,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:16,423][root][INFO] - Training Epoch: 2/2, step 2231/7134 completed (loss: 0.04174073040485382, acc: 0.9840637445449829)
[2025-02-13 03:50:16,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:16,841][root][INFO] - Training Epoch: 2/2, step 2232/7134 completed (loss: 0.012237755581736565, acc: 0.9946523904800415)
[2025-02-13 03:50:16,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:17,257][root][INFO] - Training Epoch: 2/2, step 2233/7134 completed (loss: 0.031070668250322342, acc: 0.9871976971626282)
[2025-02-13 03:50:17,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:17,687][root][INFO] - Training Epoch: 2/2, step 2234/7134 completed (loss: 0.0030820344109088182, acc: 1.0)
[2025-02-13 03:50:17,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:18,105][root][INFO] - Training Epoch: 2/2, step 2235/7134 completed (loss: 0.039188504219055176, acc: 0.9921630024909973)
[2025-02-13 03:50:18,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:18,501][root][INFO] - Training Epoch: 2/2, step 2236/7134 completed (loss: 0.020654335618019104, acc: 0.9925037622451782)
[2025-02-13 03:50:18,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:18,930][root][INFO] - Training Epoch: 2/2, step 2237/7134 completed (loss: 0.035384222865104675, acc: 0.9915134310722351)
[2025-02-13 03:50:19,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:19,380][root][INFO] - Training Epoch: 2/2, step 2238/7134 completed (loss: 0.06110963970422745, acc: 0.986975371837616)
[2025-02-13 03:50:19,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:19,822][root][INFO] - Training Epoch: 2/2, step 2239/7134 completed (loss: 0.08130739629268646, acc: 0.9818181991577148)
[2025-02-13 03:50:19,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:20,268][root][INFO] - Training Epoch: 2/2, step 2240/7134 completed (loss: 0.03528143838047981, acc: 0.9896103739738464)
[2025-02-13 03:50:20,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:20,650][root][INFO] - Training Epoch: 2/2, step 2241/7134 completed (loss: 0.06744009256362915, acc: 0.978723406791687)
[2025-02-13 03:50:20,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:21,035][root][INFO] - Training Epoch: 2/2, step 2242/7134 completed (loss: 0.08648443967103958, acc: 0.978622317314148)
[2025-02-13 03:50:21,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:21,499][root][INFO] - Training Epoch: 2/2, step 2243/7134 completed (loss: 0.0842982605099678, acc: 0.9770992398262024)
[2025-02-13 03:50:21,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:21,948][root][INFO] - Training Epoch: 2/2, step 2244/7134 completed (loss: 0.02552618831396103, acc: 0.9889937043190002)
[2025-02-13 03:50:22,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:22,343][root][INFO] - Training Epoch: 2/2, step 2245/7134 completed (loss: 0.06519972532987595, acc: 0.9773463010787964)
[2025-02-13 03:50:22,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:22,788][root][INFO] - Training Epoch: 2/2, step 2246/7134 completed (loss: 0.09390244632959366, acc: 0.9689189195632935)
[2025-02-13 03:50:22,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:23,205][root][INFO] - Training Epoch: 2/2, step 2247/7134 completed (loss: 0.07874102145433426, acc: 0.977011501789093)
[2025-02-13 03:50:23,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:23,639][root][INFO] - Training Epoch: 2/2, step 2248/7134 completed (loss: 0.052534665912389755, acc: 0.9875518679618835)
[2025-02-13 03:50:23,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:24,075][root][INFO] - Training Epoch: 2/2, step 2249/7134 completed (loss: 0.040846049785614014, acc: 0.9860334992408752)
[2025-02-13 03:50:24,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:24,508][root][INFO] - Training Epoch: 2/2, step 2250/7134 completed (loss: 0.0444251224398613, acc: 0.9893048405647278)
[2025-02-13 03:50:24,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:24,895][root][INFO] - Training Epoch: 2/2, step 2251/7134 completed (loss: 0.046976517885923386, acc: 0.9909297227859497)
[2025-02-13 03:50:25,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:25,299][root][INFO] - Training Epoch: 2/2, step 2252/7134 completed (loss: 0.04187539964914322, acc: 0.9845938086509705)
[2025-02-13 03:50:25,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:25,679][root][INFO] - Training Epoch: 2/2, step 2253/7134 completed (loss: 0.03387301787734032, acc: 0.9899665713310242)
[2025-02-13 03:50:25,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:26,059][root][INFO] - Training Epoch: 2/2, step 2254/7134 completed (loss: 0.10980558395385742, acc: 0.9681274890899658)
[2025-02-13 03:50:26,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:26,481][root][INFO] - Training Epoch: 2/2, step 2255/7134 completed (loss: 0.03537461906671524, acc: 0.9863429665565491)
[2025-02-13 03:50:26,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:26,820][root][INFO] - Training Epoch: 2/2, step 2256/7134 completed (loss: 0.07928866893053055, acc: 0.9668141603469849)
[2025-02-13 03:50:26,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:27,236][root][INFO] - Training Epoch: 2/2, step 2257/7134 completed (loss: 0.03089986927807331, acc: 0.9909365773200989)
[2025-02-13 03:50:27,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:27,641][root][INFO] - Training Epoch: 2/2, step 2258/7134 completed (loss: 0.034283414483070374, acc: 0.9903846383094788)
[2025-02-13 03:50:27,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:28,024][root][INFO] - Training Epoch: 2/2, step 2259/7134 completed (loss: 0.02821831777691841, acc: 0.9911110997200012)
[2025-02-13 03:50:28,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:28,423][root][INFO] - Training Epoch: 2/2, step 2260/7134 completed (loss: 0.0809440016746521, acc: 0.9772727489471436)
[2025-02-13 03:50:28,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:28,785][root][INFO] - Training Epoch: 2/2, step 2261/7134 completed (loss: 0.07750178873538971, acc: 0.970802903175354)
[2025-02-13 03:50:28,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:29,168][root][INFO] - Training Epoch: 2/2, step 2262/7134 completed (loss: 0.018358509987592697, acc: 0.9944751262664795)
[2025-02-13 03:50:29,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:29,571][root][INFO] - Training Epoch: 2/2, step 2263/7134 completed (loss: 0.037287380546331406, acc: 0.9891892075538635)
[2025-02-13 03:50:29,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:29,968][root][INFO] - Training Epoch: 2/2, step 2264/7134 completed (loss: 0.07173900306224823, acc: 0.9774965047836304)
[2025-02-13 03:50:30,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:30,394][root][INFO] - Training Epoch: 2/2, step 2265/7134 completed (loss: 0.07649768888950348, acc: 0.9810996651649475)
[2025-02-13 03:50:30,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:30,813][root][INFO] - Training Epoch: 2/2, step 2266/7134 completed (loss: 0.051843587309122086, acc: 0.9919571280479431)
[2025-02-13 03:50:30,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:31,214][root][INFO] - Training Epoch: 2/2, step 2267/7134 completed (loss: 0.01291667204350233, acc: 0.9979423880577087)
[2025-02-13 03:50:31,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:31,664][root][INFO] - Training Epoch: 2/2, step 2268/7134 completed (loss: 0.025648431852459908, acc: 0.991391658782959)
[2025-02-13 03:50:31,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:32,106][root][INFO] - Training Epoch: 2/2, step 2269/7134 completed (loss: 0.015514837577939034, acc: 0.9937106966972351)
[2025-02-13 03:50:32,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:32,513][root][INFO] - Training Epoch: 2/2, step 2270/7134 completed (loss: 0.0698309913277626, acc: 0.9767827391624451)
[2025-02-13 03:50:32,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:32,975][root][INFO] - Training Epoch: 2/2, step 2271/7134 completed (loss: 0.04235545173287392, acc: 0.9850402474403381)
[2025-02-13 03:50:33,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:33,414][root][INFO] - Training Epoch: 2/2, step 2272/7134 completed (loss: 0.030304541811347008, acc: 0.9926650524139404)
[2025-02-13 03:50:33,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:33,848][root][INFO] - Training Epoch: 2/2, step 2273/7134 completed (loss: 0.015589659102261066, acc: 0.9964072108268738)
[2025-02-13 03:50:33,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:34,295][root][INFO] - Training Epoch: 2/2, step 2274/7134 completed (loss: 0.09328256547451019, acc: 0.9808917045593262)
[2025-02-13 03:50:34,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:34,736][root][INFO] - Training Epoch: 2/2, step 2275/7134 completed (loss: 0.023561207577586174, acc: 0.9941314458847046)
[2025-02-13 03:50:34,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:35,187][root][INFO] - Training Epoch: 2/2, step 2276/7134 completed (loss: 0.04271586239337921, acc: 0.9905213117599487)
[2025-02-13 03:50:35,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:35,642][root][INFO] - Training Epoch: 2/2, step 2277/7134 completed (loss: 0.025800520554184914, acc: 0.9930716156959534)
[2025-02-13 03:50:35,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:36,071][root][INFO] - Training Epoch: 2/2, step 2278/7134 completed (loss: 0.09400322288274765, acc: 0.9792332053184509)
[2025-02-13 03:50:36,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:36,514][root][INFO] - Training Epoch: 2/2, step 2279/7134 completed (loss: 0.02789384126663208, acc: 0.9940968155860901)
[2025-02-13 03:50:36,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:36,966][root][INFO] - Training Epoch: 2/2, step 2280/7134 completed (loss: 0.03768020495772362, acc: 0.9917355179786682)
[2025-02-13 03:50:37,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:37,374][root][INFO] - Training Epoch: 2/2, step 2281/7134 completed (loss: 0.09396345168352127, acc: 0.9866666793823242)
[2025-02-13 03:50:37,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:37,817][root][INFO] - Training Epoch: 2/2, step 2282/7134 completed (loss: 0.012954158708453178, acc: 0.998763918876648)
[2025-02-13 03:50:37,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:38,254][root][INFO] - Training Epoch: 2/2, step 2283/7134 completed (loss: 0.02722356654703617, acc: 0.9926650524139404)
[2025-02-13 03:50:38,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:38,680][root][INFO] - Training Epoch: 2/2, step 2284/7134 completed (loss: 0.03551178425550461, acc: 0.9911373853683472)
[2025-02-13 03:50:38,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:39,148][root][INFO] - Training Epoch: 2/2, step 2285/7134 completed (loss: 0.015837175771594048, acc: 0.9955406785011292)
[2025-02-13 03:50:39,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:39,587][root][INFO] - Training Epoch: 2/2, step 2286/7134 completed (loss: 0.04048442840576172, acc: 0.9870298504829407)
[2025-02-13 03:50:39,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:40,050][root][INFO] - Training Epoch: 2/2, step 2287/7134 completed (loss: 0.024468787014484406, acc: 0.9865951538085938)
[2025-02-13 03:50:40,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:40,507][root][INFO] - Training Epoch: 2/2, step 2288/7134 completed (loss: 0.025273233652114868, acc: 0.9952885508537292)
[2025-02-13 03:50:40,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:40,919][root][INFO] - Training Epoch: 2/2, step 2289/7134 completed (loss: 0.02818901650607586, acc: 0.9910141229629517)
[2025-02-13 03:50:41,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:41,342][root][INFO] - Training Epoch: 2/2, step 2290/7134 completed (loss: 0.026205165311694145, acc: 0.993630588054657)
[2025-02-13 03:50:41,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:41,777][root][INFO] - Training Epoch: 2/2, step 2291/7134 completed (loss: 0.03718806430697441, acc: 0.9892037510871887)
[2025-02-13 03:50:41,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:42,209][root][INFO] - Training Epoch: 2/2, step 2292/7134 completed (loss: 0.06279970705509186, acc: 0.9870967864990234)
[2025-02-13 03:50:42,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:42,671][root][INFO] - Training Epoch: 2/2, step 2293/7134 completed (loss: 0.036469507962465286, acc: 0.9888641238212585)
[2025-02-13 03:50:42,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:43,112][root][INFO] - Training Epoch: 2/2, step 2294/7134 completed (loss: 0.03839142993092537, acc: 0.9876265525817871)
[2025-02-13 03:50:43,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:43,615][root][INFO] - Training Epoch: 2/2, step 2295/7134 completed (loss: 0.038692329078912735, acc: 0.9886040091514587)
[2025-02-13 03:50:43,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:44,074][root][INFO] - Training Epoch: 2/2, step 2296/7134 completed (loss: 0.018714796751737595, acc: 0.992553174495697)
[2025-02-13 03:50:44,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:44,543][root][INFO] - Training Epoch: 2/2, step 2297/7134 completed (loss: 0.023315206170082092, acc: 0.9928861856460571)
[2025-02-13 03:50:44,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:44,986][root][INFO] - Training Epoch: 2/2, step 2298/7134 completed (loss: 0.05956168845295906, acc: 0.9831755757331848)
[2025-02-13 03:50:45,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:45,451][root][INFO] - Training Epoch: 2/2, step 2299/7134 completed (loss: 0.03618483245372772, acc: 0.9874857664108276)
[2025-02-13 03:50:45,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:45,919][root][INFO] - Training Epoch: 2/2, step 2300/7134 completed (loss: 0.009418269619345665, acc: 0.9977452158927917)
[2025-02-13 03:50:46,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:46,379][root][INFO] - Training Epoch: 2/2, step 2301/7134 completed (loss: 0.03555786982178688, acc: 0.9920529723167419)
[2025-02-13 03:50:46,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:46,833][root][INFO] - Training Epoch: 2/2, step 2302/7134 completed (loss: 0.041445791721343994, acc: 0.9889705777168274)
[2025-02-13 03:50:46,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:47,250][root][INFO] - Training Epoch: 2/2, step 2303/7134 completed (loss: 0.02842698246240616, acc: 0.9886105060577393)
[2025-02-13 03:50:47,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:47,709][root][INFO] - Training Epoch: 2/2, step 2304/7134 completed (loss: 0.04866914451122284, acc: 0.9856114983558655)
[2025-02-13 03:50:47,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:48,177][root][INFO] - Training Epoch: 2/2, step 2305/7134 completed (loss: 0.021221941336989403, acc: 0.9946751594543457)
[2025-02-13 03:50:48,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:48,620][root][INFO] - Training Epoch: 2/2, step 2306/7134 completed (loss: 0.01716586761176586, acc: 0.9944567680358887)
[2025-02-13 03:50:48,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:49,082][root][INFO] - Training Epoch: 2/2, step 2307/7134 completed (loss: 0.0315471813082695, acc: 0.9927685856819153)
[2025-02-13 03:50:49,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:49,539][root][INFO] - Training Epoch: 2/2, step 2308/7134 completed (loss: 0.03463363274931908, acc: 0.9928498268127441)
[2025-02-13 03:50:49,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:49,968][root][INFO] - Training Epoch: 2/2, step 2309/7134 completed (loss: 0.016319775953888893, acc: 0.9964028596878052)
[2025-02-13 03:50:50,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:50,405][root][INFO] - Training Epoch: 2/2, step 2310/7134 completed (loss: 0.02532639168202877, acc: 0.993220329284668)
[2025-02-13 03:50:50,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:50,844][root][INFO] - Training Epoch: 2/2, step 2311/7134 completed (loss: 0.034791797399520874, acc: 0.9908257126808167)
[2025-02-13 03:50:50,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:51,304][root][INFO] - Training Epoch: 2/2, step 2312/7134 completed (loss: 0.019232437014579773, acc: 0.9967602491378784)
[2025-02-13 03:50:51,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:51,739][root][INFO] - Training Epoch: 2/2, step 2313/7134 completed (loss: 0.04223589226603508, acc: 0.9863013625144958)
[2025-02-13 03:50:51,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:52,209][root][INFO] - Training Epoch: 2/2, step 2314/7134 completed (loss: 0.02313574217259884, acc: 0.9944506287574768)
[2025-02-13 03:50:52,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:52,673][root][INFO] - Training Epoch: 2/2, step 2315/7134 completed (loss: 0.025002382695674896, acc: 0.9917647242546082)
[2025-02-13 03:50:52,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:53,120][root][INFO] - Training Epoch: 2/2, step 2316/7134 completed (loss: 0.02444497123360634, acc: 0.9930955171585083)
[2025-02-13 03:50:53,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:53,585][root][INFO] - Training Epoch: 2/2, step 2317/7134 completed (loss: 0.027468275278806686, acc: 0.9947090148925781)
[2025-02-13 03:50:53,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:54,051][root][INFO] - Training Epoch: 2/2, step 2318/7134 completed (loss: 0.012824325822293758, acc: 0.9958158731460571)
[2025-02-13 03:50:54,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:54,506][root][INFO] - Training Epoch: 2/2, step 2319/7134 completed (loss: 0.015629202127456665, acc: 0.9956236481666565)
[2025-02-13 03:50:54,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:54,962][root][INFO] - Training Epoch: 2/2, step 2320/7134 completed (loss: 0.016709979623556137, acc: 0.9943181872367859)
[2025-02-13 03:50:55,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:55,382][root][INFO] - Training Epoch: 2/2, step 2321/7134 completed (loss: 0.023189658299088478, acc: 0.9944444298744202)
[2025-02-13 03:50:55,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:55,803][root][INFO] - Training Epoch: 2/2, step 2322/7134 completed (loss: 0.02385498397052288, acc: 0.9881481528282166)
[2025-02-13 03:50:55,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:56,203][root][INFO] - Training Epoch: 2/2, step 2323/7134 completed (loss: 0.032519061118364334, acc: 0.9858044385910034)
[2025-02-13 03:50:56,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:56,625][root][INFO] - Training Epoch: 2/2, step 2324/7134 completed (loss: 0.04654897376894951, acc: 0.9871794581413269)
[2025-02-13 03:50:56,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:57,051][root][INFO] - Training Epoch: 2/2, step 2325/7134 completed (loss: 0.039105374366045, acc: 0.988034188747406)
[2025-02-13 03:50:57,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:57,450][root][INFO] - Training Epoch: 2/2, step 2326/7134 completed (loss: 0.012780430726706982, acc: 0.9963235259056091)
[2025-02-13 03:50:57,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:57,845][root][INFO] - Training Epoch: 2/2, step 2327/7134 completed (loss: 0.016902634873986244, acc: 0.9950819611549377)
[2025-02-13 03:50:57,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:58,265][root][INFO] - Training Epoch: 2/2, step 2328/7134 completed (loss: 0.06879281997680664, acc: 0.9856630563735962)
[2025-02-13 03:50:58,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:58,678][root][INFO] - Training Epoch: 2/2, step 2329/7134 completed (loss: 0.023824529722332954, acc: 0.9984496235847473)
[2025-02-13 03:50:58,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:59,073][root][INFO] - Training Epoch: 2/2, step 2330/7134 completed (loss: 0.01969573274254799, acc: 0.9938931465148926)
[2025-02-13 03:50:59,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:59,480][root][INFO] - Training Epoch: 2/2, step 2331/7134 completed (loss: 0.012582209892570972, acc: 0.9969325065612793)
[2025-02-13 03:50:59,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:59,901][root][INFO] - Training Epoch: 2/2, step 2332/7134 completed (loss: 0.03115958720445633, acc: 0.9925705790519714)
[2025-02-13 03:51:00,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:00,314][root][INFO] - Training Epoch: 2/2, step 2333/7134 completed (loss: 0.012015447951853275, acc: 0.9955357313156128)
[2025-02-13 03:51:00,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:00,732][root][INFO] - Training Epoch: 2/2, step 2334/7134 completed (loss: 0.020884694531559944, acc: 0.9910447597503662)
[2025-02-13 03:51:00,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:01,158][root][INFO] - Training Epoch: 2/2, step 2335/7134 completed (loss: 0.028615470975637436, acc: 0.9935483932495117)
[2025-02-13 03:51:01,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:01,556][root][INFO] - Training Epoch: 2/2, step 2336/7134 completed (loss: 0.010688281618058681, acc: 0.9953632354736328)
[2025-02-13 03:51:01,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:01,969][root][INFO] - Training Epoch: 2/2, step 2337/7134 completed (loss: 0.002385492669418454, acc: 1.0)
[2025-02-13 03:51:02,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:02,386][root][INFO] - Training Epoch: 2/2, step 2338/7134 completed (loss: 0.022053584456443787, acc: 0.9953632354736328)
[2025-02-13 03:51:02,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:02,802][root][INFO] - Training Epoch: 2/2, step 2339/7134 completed (loss: 0.037962689995765686, acc: 0.990212082862854)
[2025-02-13 03:51:02,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:03,217][root][INFO] - Training Epoch: 2/2, step 2340/7134 completed (loss: 0.008879536762833595, acc: 0.9984917044639587)
[2025-02-13 03:51:03,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:03,624][root][INFO] - Training Epoch: 2/2, step 2341/7134 completed (loss: 0.00735132023692131, acc: 0.9981203079223633)
[2025-02-13 03:51:03,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:04,026][root][INFO] - Training Epoch: 2/2, step 2342/7134 completed (loss: 0.01113580446690321, acc: 0.9970458149909973)
[2025-02-13 03:51:04,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:04,427][root][INFO] - Training Epoch: 2/2, step 2343/7134 completed (loss: 0.014523252844810486, acc: 0.995199978351593)
[2025-02-13 03:51:04,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:04,841][root][INFO] - Training Epoch: 2/2, step 2344/7134 completed (loss: 0.009938064031302929, acc: 0.9968701004981995)
[2025-02-13 03:51:04,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:05,260][root][INFO] - Training Epoch: 2/2, step 2345/7134 completed (loss: 0.013231228105723858, acc: 0.9958041906356812)
[2025-02-13 03:51:05,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:05,679][root][INFO] - Training Epoch: 2/2, step 2346/7134 completed (loss: 0.012688705697655678, acc: 0.9943342804908752)
[2025-02-13 03:51:05,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:06,096][root][INFO] - Training Epoch: 2/2, step 2347/7134 completed (loss: 0.059911616146564484, acc: 0.9868420958518982)
[2025-02-13 03:51:06,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:06,539][root][INFO] - Training Epoch: 2/2, step 2348/7134 completed (loss: 0.07156429439783096, acc: 0.9785624146461487)
[2025-02-13 03:51:06,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:06,960][root][INFO] - Training Epoch: 2/2, step 2349/7134 completed (loss: 0.033315397799015045, acc: 0.9902777671813965)
[2025-02-13 03:51:07,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:07,405][root][INFO] - Training Epoch: 2/2, step 2350/7134 completed (loss: 0.0705408975481987, acc: 0.9832317233085632)
[2025-02-13 03:51:07,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:07,848][root][INFO] - Training Epoch: 2/2, step 2351/7134 completed (loss: 0.047402020543813705, acc: 0.9863387942314148)
[2025-02-13 03:51:07,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:08,255][root][INFO] - Training Epoch: 2/2, step 2352/7134 completed (loss: 0.032337792217731476, acc: 0.9919871687889099)
[2025-02-13 03:51:08,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:08,703][root][INFO] - Training Epoch: 2/2, step 2353/7134 completed (loss: 0.03856931999325752, acc: 0.9872340559959412)
[2025-02-13 03:51:08,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:09,136][root][INFO] - Training Epoch: 2/2, step 2354/7134 completed (loss: 0.025459492579102516, acc: 0.9911392331123352)
[2025-02-13 03:51:09,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:09,569][root][INFO] - Training Epoch: 2/2, step 2355/7134 completed (loss: 0.041825637221336365, acc: 0.990123450756073)
[2025-02-13 03:51:09,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:09,974][root][INFO] - Training Epoch: 2/2, step 2356/7134 completed (loss: 0.05003726854920387, acc: 0.9840989112854004)
[2025-02-13 03:51:10,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:10,365][root][INFO] - Training Epoch: 2/2, step 2357/7134 completed (loss: 0.047609396278858185, acc: 0.9821428656578064)
[2025-02-13 03:51:10,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:10,815][root][INFO] - Training Epoch: 2/2, step 2358/7134 completed (loss: 0.03581194579601288, acc: 0.9903614521026611)
[2025-02-13 03:51:10,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:11,230][root][INFO] - Training Epoch: 2/2, step 2359/7134 completed (loss: 0.0404214970767498, acc: 0.98531574010849)
[2025-02-13 03:51:11,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:11,690][root][INFO] - Training Epoch: 2/2, step 2360/7134 completed (loss: 0.021200105547904968, acc: 0.9941434860229492)
[2025-02-13 03:51:11,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:12,108][root][INFO] - Training Epoch: 2/2, step 2361/7134 completed (loss: 0.040400248020887375, acc: 0.9866864085197449)
[2025-02-13 03:51:12,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:12,416][root][INFO] - Training Epoch: 2/2, step 2362/7134 completed (loss: 0.0317828506231308, acc: 0.9910314083099365)
[2025-02-13 03:51:12,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:12,871][root][INFO] - Training Epoch: 2/2, step 2363/7134 completed (loss: 0.03807472437620163, acc: 0.9907299876213074)
[2025-02-13 03:51:13,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:13,279][root][INFO] - Training Epoch: 2/2, step 2364/7134 completed (loss: 0.023363763466477394, acc: 0.9878234267234802)
[2025-02-13 03:51:13,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:13,685][root][INFO] - Training Epoch: 2/2, step 2365/7134 completed (loss: 0.016244467347860336, acc: 0.9931129217147827)
[2025-02-13 03:51:13,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:14,122][root][INFO] - Training Epoch: 2/2, step 2366/7134 completed (loss: 0.012771043926477432, acc: 0.9965277910232544)
[2025-02-13 03:51:14,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:14,583][root][INFO] - Training Epoch: 2/2, step 2367/7134 completed (loss: 0.042134400457143784, acc: 0.9899497628211975)
[2025-02-13 03:51:14,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:15,036][root][INFO] - Training Epoch: 2/2, step 2368/7134 completed (loss: 0.027708103880286217, acc: 0.9939393997192383)
[2025-02-13 03:51:15,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:15,481][root][INFO] - Training Epoch: 2/2, step 2369/7134 completed (loss: 0.02000471018254757, acc: 0.9917647242546082)
[2025-02-13 03:51:15,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:15,846][root][INFO] - Training Epoch: 2/2, step 2370/7134 completed (loss: 0.025359656661748886, acc: 0.9938271641731262)
[2025-02-13 03:51:15,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:16,301][root][INFO] - Training Epoch: 2/2, step 2371/7134 completed (loss: 0.01945401169359684, acc: 0.9956772327423096)
[2025-02-13 03:51:16,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:16,739][root][INFO] - Training Epoch: 2/2, step 2372/7134 completed (loss: 0.02888217754662037, acc: 0.9943310618400574)
[2025-02-13 03:51:16,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:17,153][root][INFO] - Training Epoch: 2/2, step 2373/7134 completed (loss: 0.016975870355963707, acc: 0.9975185990333557)
[2025-02-13 03:51:17,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:17,578][root][INFO] - Training Epoch: 2/2, step 2374/7134 completed (loss: 0.013077049516141415, acc: 0.993306577205658)
[2025-02-13 03:51:17,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:18,024][root][INFO] - Training Epoch: 2/2, step 2375/7134 completed (loss: 0.04911130294203758, acc: 0.989393949508667)
[2025-02-13 03:51:18,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:18,445][root][INFO] - Training Epoch: 2/2, step 2376/7134 completed (loss: 0.08905702084302902, acc: 0.9761499166488647)
[2025-02-13 03:51:18,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:18,888][root][INFO] - Training Epoch: 2/2, step 2377/7134 completed (loss: 0.060167498886585236, acc: 0.9844683408737183)
[2025-02-13 03:51:19,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:19,324][root][INFO] - Training Epoch: 2/2, step 2378/7134 completed (loss: 0.05354752764105797, acc: 0.9894366264343262)
[2025-02-13 03:51:19,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:19,765][root][INFO] - Training Epoch: 2/2, step 2379/7134 completed (loss: 0.0804012343287468, acc: 0.977931022644043)
[2025-02-13 03:51:19,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:20,206][root][INFO] - Training Epoch: 2/2, step 2380/7134 completed (loss: 0.03815160691738129, acc: 0.9891172647476196)
[2025-02-13 03:51:20,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:20,584][root][INFO] - Training Epoch: 2/2, step 2381/7134 completed (loss: 0.10748660564422607, acc: 0.9695431590080261)
[2025-02-13 03:51:20,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:20,983][root][INFO] - Training Epoch: 2/2, step 2382/7134 completed (loss: 0.03463078290224075, acc: 0.9892966151237488)
[2025-02-13 03:51:21,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:21,384][root][INFO] - Training Epoch: 2/2, step 2383/7134 completed (loss: 0.08019237965345383, acc: 0.9803921580314636)
[2025-02-13 03:51:21,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:21,801][root][INFO] - Training Epoch: 2/2, step 2384/7134 completed (loss: 0.06331668794155121, acc: 0.9843137264251709)
[2025-02-13 03:51:21,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:22,253][root][INFO] - Training Epoch: 2/2, step 2385/7134 completed (loss: 0.037700045853853226, acc: 0.9875690340995789)
[2025-02-13 03:51:22,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:22,669][root][INFO] - Training Epoch: 2/2, step 2386/7134 completed (loss: 0.036636706441640854, acc: 0.985602080821991)
[2025-02-13 03:51:22,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:23,137][root][INFO] - Training Epoch: 2/2, step 2387/7134 completed (loss: 0.0394057035446167, acc: 0.9895591735839844)
[2025-02-13 03:51:23,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:23,574][root][INFO] - Training Epoch: 2/2, step 2388/7134 completed (loss: 0.07765141129493713, acc: 0.9785459041595459)
[2025-02-13 03:51:23,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:24,061][root][INFO] - Training Epoch: 2/2, step 2389/7134 completed (loss: 0.05321386456489563, acc: 0.9862227439880371)
[2025-02-13 03:51:24,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:24,503][root][INFO] - Training Epoch: 2/2, step 2390/7134 completed (loss: 0.06139381229877472, acc: 0.9844683408737183)
[2025-02-13 03:51:24,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:24,932][root][INFO] - Training Epoch: 2/2, step 2391/7134 completed (loss: 0.06506166607141495, acc: 0.9796696305274963)
[2025-02-13 03:51:25,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:25,369][root][INFO] - Training Epoch: 2/2, step 2392/7134 completed (loss: 0.041486337780952454, acc: 0.9879840016365051)
[2025-02-13 03:51:25,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:25,796][root][INFO] - Training Epoch: 2/2, step 2393/7134 completed (loss: 0.05779355764389038, acc: 0.9805447459220886)
[2025-02-13 03:51:25,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:26,248][root][INFO] - Training Epoch: 2/2, step 2394/7134 completed (loss: 0.02749331295490265, acc: 0.9915151596069336)
[2025-02-13 03:51:26,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:26,665][root][INFO] - Training Epoch: 2/2, step 2395/7134 completed (loss: 0.11951036751270294, acc: 0.9674267172813416)
[2025-02-13 03:51:26,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:27,117][root][INFO] - Training Epoch: 2/2, step 2396/7134 completed (loss: 0.06557595729827881, acc: 0.9765840172767639)
[2025-02-13 03:51:27,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:27,499][root][INFO] - Training Epoch: 2/2, step 2397/7134 completed (loss: 0.08019386231899261, acc: 0.9737991094589233)
[2025-02-13 03:51:27,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:27,938][root][INFO] - Training Epoch: 2/2, step 2398/7134 completed (loss: 0.02902231737971306, acc: 0.9862448573112488)
[2025-02-13 03:51:28,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:28,372][root][INFO] - Training Epoch: 2/2, step 2399/7134 completed (loss: 0.045417629182338715, acc: 0.9842632412910461)
[2025-02-13 03:51:28,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:28,829][root][INFO] - Training Epoch: 2/2, step 2400/7134 completed (loss: 0.022328075021505356, acc: 0.9917241334915161)
[2025-02-13 03:51:28,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:29,247][root][INFO] - Training Epoch: 2/2, step 2401/7134 completed (loss: 0.09348804503679276, acc: 0.982594907283783)
[2025-02-13 03:51:29,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:29,667][root][INFO] - Training Epoch: 2/2, step 2402/7134 completed (loss: 0.06273537129163742, acc: 0.9760956168174744)
[2025-02-13 03:51:29,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:30,089][root][INFO] - Training Epoch: 2/2, step 2403/7134 completed (loss: 0.035536687821149826, acc: 0.9872340559959412)
[2025-02-13 03:51:30,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:30,486][root][INFO] - Training Epoch: 2/2, step 2404/7134 completed (loss: 0.026283804327249527, acc: 0.9918032884597778)
[2025-02-13 03:51:30,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:30,869][root][INFO] - Training Epoch: 2/2, step 2405/7134 completed (loss: 0.053457848727703094, acc: 0.97826087474823)
[2025-02-13 03:51:31,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:31,280][root][INFO] - Training Epoch: 2/2, step 2406/7134 completed (loss: 0.01219885516911745, acc: 0.9933510422706604)
[2025-02-13 03:51:31,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:31,719][root][INFO] - Training Epoch: 2/2, step 2407/7134 completed (loss: 0.025957783684134483, acc: 0.9893617033958435)
[2025-02-13 03:51:31,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:32,151][root][INFO] - Training Epoch: 2/2, step 2408/7134 completed (loss: 0.11085580289363861, acc: 0.9750000238418579)
[2025-02-13 03:51:32,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:32,577][root][INFO] - Training Epoch: 2/2, step 2409/7134 completed (loss: 0.030969807878136635, acc: 0.9877049326896667)
[2025-02-13 03:51:32,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:32,981][root][INFO] - Training Epoch: 2/2, step 2410/7134 completed (loss: 0.02339918352663517, acc: 0.991391658782959)
[2025-02-13 03:51:33,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:33,411][root][INFO] - Training Epoch: 2/2, step 2411/7134 completed (loss: 0.02561367303133011, acc: 0.9922380447387695)
[2025-02-13 03:51:33,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:33,817][root][INFO] - Training Epoch: 2/2, step 2412/7134 completed (loss: 0.034525662660598755, acc: 0.9884615540504456)
[2025-02-13 03:51:33,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:34,251][root][INFO] - Training Epoch: 2/2, step 2413/7134 completed (loss: 0.012470364570617676, acc: 0.9949173927307129)
[2025-02-13 03:51:34,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:34,704][root][INFO] - Training Epoch: 2/2, step 2414/7134 completed (loss: 0.018778275698423386, acc: 0.9925373196601868)
[2025-02-13 03:51:34,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:35,161][root][INFO] - Training Epoch: 2/2, step 2415/7134 completed (loss: 0.06517036259174347, acc: 0.9821673631668091)
[2025-02-13 03:51:35,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:35,586][root][INFO] - Training Epoch: 2/2, step 2416/7134 completed (loss: 0.025793178007006645, acc: 0.9930843710899353)
[2025-02-13 03:51:35,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:36,058][root][INFO] - Training Epoch: 2/2, step 2417/7134 completed (loss: 0.05975039303302765, acc: 0.9866220951080322)
[2025-02-13 03:51:36,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:36,448][root][INFO] - Training Epoch: 2/2, step 2418/7134 completed (loss: 0.025847937911748886, acc: 0.9929676651954651)
[2025-02-13 03:51:36,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:36,855][root][INFO] - Training Epoch: 2/2, step 2419/7134 completed (loss: 0.026173003017902374, acc: 0.9899857044219971)
[2025-02-13 03:51:36,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:37,267][root][INFO] - Training Epoch: 2/2, step 2420/7134 completed (loss: 0.0207233726978302, acc: 0.9950248599052429)
[2025-02-13 03:51:37,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:37,703][root][INFO] - Training Epoch: 2/2, step 2421/7134 completed (loss: 0.04431881755590439, acc: 0.9875621795654297)
[2025-02-13 03:51:37,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:38,167][root][INFO] - Training Epoch: 2/2, step 2422/7134 completed (loss: 0.014268244616687298, acc: 0.9976359605789185)
[2025-02-13 03:51:38,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:38,567][root][INFO] - Training Epoch: 2/2, step 2423/7134 completed (loss: 0.02583790011703968, acc: 0.9930264949798584)
[2025-02-13 03:51:38,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:38,982][root][INFO] - Training Epoch: 2/2, step 2424/7134 completed (loss: 0.024066871032118797, acc: 0.992977499961853)
[2025-02-13 03:51:39,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:39,395][root][INFO] - Training Epoch: 2/2, step 2425/7134 completed (loss: 0.03754476457834244, acc: 0.9879999756813049)
[2025-02-13 03:51:39,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:39,850][root][INFO] - Training Epoch: 2/2, step 2426/7134 completed (loss: 0.01631622016429901, acc: 0.9928656220436096)
[2025-02-13 03:51:39,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:40,297][root][INFO] - Training Epoch: 2/2, step 2427/7134 completed (loss: 0.010066096670925617, acc: 0.9976798295974731)
[2025-02-13 03:51:40,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:40,745][root][INFO] - Training Epoch: 2/2, step 2428/7134 completed (loss: 0.023867668583989143, acc: 0.9923567175865173)
[2025-02-13 03:51:40,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:41,173][root][INFO] - Training Epoch: 2/2, step 2429/7134 completed (loss: 0.03213144838809967, acc: 0.9890965819358826)
[2025-02-13 03:51:41,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:41,571][root][INFO] - Training Epoch: 2/2, step 2430/7134 completed (loss: 0.03132811188697815, acc: 0.9895678162574768)
[2025-02-13 03:51:41,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:42,003][root][INFO] - Training Epoch: 2/2, step 2431/7134 completed (loss: 0.04466342553496361, acc: 0.9853372573852539)
[2025-02-13 03:51:42,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:42,436][root][INFO] - Training Epoch: 2/2, step 2432/7134 completed (loss: 0.08804994821548462, acc: 0.979626476764679)
[2025-02-13 03:51:42,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:42,871][root][INFO] - Training Epoch: 2/2, step 2433/7134 completed (loss: 0.05152234807610512, acc: 0.9872159361839294)
[2025-02-13 03:51:43,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:43,267][root][INFO] - Training Epoch: 2/2, step 2434/7134 completed (loss: 0.0551602803170681, acc: 0.9844720363616943)
[2025-02-13 03:51:43,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:43,721][root][INFO] - Training Epoch: 2/2, step 2435/7134 completed (loss: 0.025151552632451057, acc: 0.9947299361228943)
[2025-02-13 03:51:43,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:44,154][root][INFO] - Training Epoch: 2/2, step 2436/7134 completed (loss: 0.03450048714876175, acc: 0.9871944189071655)
[2025-02-13 03:51:44,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:44,565][root][INFO] - Training Epoch: 2/2, step 2437/7134 completed (loss: 0.04087267443537712, acc: 0.9894419312477112)
[2025-02-13 03:51:44,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:45,018][root][INFO] - Training Epoch: 2/2, step 2438/7134 completed (loss: 0.040289074182510376, acc: 0.9904305934906006)
[2025-02-13 03:51:45,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:45,470][root][INFO] - Training Epoch: 2/2, step 2439/7134 completed (loss: 0.04110444709658623, acc: 0.9849300384521484)
[2025-02-13 03:51:45,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:45,858][root][INFO] - Training Epoch: 2/2, step 2440/7134 completed (loss: 0.035400670021772385, acc: 0.9887892603874207)
[2025-02-13 03:51:45,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:46,262][root][INFO] - Training Epoch: 2/2, step 2441/7134 completed (loss: 0.12947718799114227, acc: 0.9692898392677307)
[2025-02-13 03:51:46,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:46,690][root][INFO] - Training Epoch: 2/2, step 2442/7134 completed (loss: 0.05141491815447807, acc: 0.9818181991577148)
[2025-02-13 03:51:46,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:47,137][root][INFO] - Training Epoch: 2/2, step 2443/7134 completed (loss: 0.0330917090177536, acc: 0.9916550517082214)
[2025-02-13 03:51:47,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:47,551][root][INFO] - Training Epoch: 2/2, step 2444/7134 completed (loss: 0.04882417246699333, acc: 0.9791666865348816)
[2025-02-13 03:51:47,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:47,988][root][INFO] - Training Epoch: 2/2, step 2445/7134 completed (loss: 0.07287826389074326, acc: 0.9809644818305969)
[2025-02-13 03:51:48,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:48,454][root][INFO] - Training Epoch: 2/2, step 2446/7134 completed (loss: 0.0797530934214592, acc: 0.9791666865348816)
[2025-02-13 03:51:48,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:48,919][root][INFO] - Training Epoch: 2/2, step 2447/7134 completed (loss: 0.038695432245731354, acc: 0.9877913594245911)
[2025-02-13 03:51:49,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:49,343][root][INFO] - Training Epoch: 2/2, step 2448/7134 completed (loss: 0.04686401039361954, acc: 0.9893048405647278)
[2025-02-13 03:51:49,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:49,739][root][INFO] - Training Epoch: 2/2, step 2449/7134 completed (loss: 0.03917373716831207, acc: 0.9893898963928223)
[2025-02-13 03:51:49,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:50,162][root][INFO] - Training Epoch: 2/2, step 2450/7134 completed (loss: 0.02490982972085476, acc: 0.9961880445480347)
[2025-02-13 03:51:50,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:50,561][root][INFO] - Training Epoch: 2/2, step 2451/7134 completed (loss: 0.02472936548292637, acc: 0.9923567175865173)
[2025-02-13 03:51:50,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:51,000][root][INFO] - Training Epoch: 2/2, step 2452/7134 completed (loss: 0.021986911073327065, acc: 0.9921976327896118)
[2025-02-13 03:51:51,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:51,461][root][INFO] - Training Epoch: 2/2, step 2453/7134 completed (loss: 0.037118494510650635, acc: 0.9883871078491211)
[2025-02-13 03:51:51,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:51,852][root][INFO] - Training Epoch: 2/2, step 2454/7134 completed (loss: 0.03875722363591194, acc: 0.9816053509712219)
[2025-02-13 03:51:52,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:52,313][root][INFO] - Training Epoch: 2/2, step 2455/7134 completed (loss: 0.020113952457904816, acc: 0.9943342804908752)
[2025-02-13 03:51:52,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:52,738][root][INFO] - Training Epoch: 2/2, step 2456/7134 completed (loss: 0.05059082806110382, acc: 0.9844357967376709)
[2025-02-13 03:51:52,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:53,200][root][INFO] - Training Epoch: 2/2, step 2457/7134 completed (loss: 0.054492752999067307, acc: 0.981697142124176)
[2025-02-13 03:51:53,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:53,587][root][INFO] - Training Epoch: 2/2, step 2458/7134 completed (loss: 0.06928860396146774, acc: 0.9874213933944702)
[2025-02-13 03:51:53,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:54,043][root][INFO] - Training Epoch: 2/2, step 2459/7134 completed (loss: 0.015716521069407463, acc: 0.9955621361732483)
[2025-02-13 03:51:54,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:54,474][root][INFO] - Training Epoch: 2/2, step 2460/7134 completed (loss: 0.026649994775652885, acc: 0.9944674968719482)
[2025-02-13 03:51:54,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:54,896][root][INFO] - Training Epoch: 2/2, step 2461/7134 completed (loss: 0.023615621030330658, acc: 0.9927536249160767)
[2025-02-13 03:51:55,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:55,314][root][INFO] - Training Epoch: 2/2, step 2462/7134 completed (loss: 0.04360174387693405, acc: 0.9889655113220215)
[2025-02-13 03:51:55,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:55,752][root][INFO] - Training Epoch: 2/2, step 2463/7134 completed (loss: 0.017104825004935265, acc: 0.9909228682518005)
[2025-02-13 03:51:55,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:56,102][root][INFO] - Training Epoch: 2/2, step 2464/7134 completed (loss: 0.04302879795432091, acc: 0.9906367063522339)
[2025-02-13 03:51:56,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:56,526][root][INFO] - Training Epoch: 2/2, step 2465/7134 completed (loss: 0.037569914013147354, acc: 0.9880794882774353)
[2025-02-13 03:51:56,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:56,966][root][INFO] - Training Epoch: 2/2, step 2466/7134 completed (loss: 0.015541122294962406, acc: 0.9947019815444946)
[2025-02-13 03:51:57,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:57,393][root][INFO] - Training Epoch: 2/2, step 2467/7134 completed (loss: 0.030939655378460884, acc: 0.9897172451019287)
[2025-02-13 03:51:57,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:57,832][root][INFO] - Training Epoch: 2/2, step 2468/7134 completed (loss: 0.019999558106064796, acc: 0.9944055676460266)
[2025-02-13 03:51:57,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:58,274][root][INFO] - Training Epoch: 2/2, step 2469/7134 completed (loss: 0.014587722718715668, acc: 0.9959072470664978)
[2025-02-13 03:51:58,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:58,665][root][INFO] - Training Epoch: 2/2, step 2470/7134 completed (loss: 0.02883676439523697, acc: 0.9949495196342468)
[2025-02-13 03:51:58,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:59,075][root][INFO] - Training Epoch: 2/2, step 2471/7134 completed (loss: 0.017523285001516342, acc: 0.9933110475540161)
[2025-02-13 03:51:59,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:59,472][root][INFO] - Training Epoch: 2/2, step 2472/7134 completed (loss: 0.012983683496713638, acc: 0.9984848499298096)
[2025-02-13 03:51:59,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:59,891][root][INFO] - Training Epoch: 2/2, step 2473/7134 completed (loss: 0.018766511231660843, acc: 0.993852436542511)
[2025-02-13 03:52:00,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:00,307][root][INFO] - Training Epoch: 2/2, step 2474/7134 completed (loss: 0.013746143318712711, acc: 0.9957746267318726)
[2025-02-13 03:52:00,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:00,698][root][INFO] - Training Epoch: 2/2, step 2475/7134 completed (loss: 0.02125292271375656, acc: 0.989708423614502)
[2025-02-13 03:52:00,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:01,116][root][INFO] - Training Epoch: 2/2, step 2476/7134 completed (loss: 0.05221335589885712, acc: 0.9888579249382019)
[2025-02-13 03:52:01,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:01,517][root][INFO] - Training Epoch: 2/2, step 2477/7134 completed (loss: 0.02773169055581093, acc: 0.9944751262664795)
[2025-02-13 03:52:01,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:01,920][root][INFO] - Training Epoch: 2/2, step 2478/7134 completed (loss: 0.010012106038630009, acc: 0.9953271150588989)
[2025-02-13 03:52:02,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:02,308][root][INFO] - Training Epoch: 2/2, step 2479/7134 completed (loss: 0.07299982011318207, acc: 0.9793322682380676)
[2025-02-13 03:52:02,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:02,705][root][INFO] - Training Epoch: 2/2, step 2480/7134 completed (loss: 0.007221294566988945, acc: 0.9977777600288391)
[2025-02-13 03:52:02,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:03,102][root][INFO] - Training Epoch: 2/2, step 2481/7134 completed (loss: 0.03670148178935051, acc: 0.9875621795654297)
[2025-02-13 03:52:03,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:03,523][root][INFO] - Training Epoch: 2/2, step 2482/7134 completed (loss: 0.036158353090286255, acc: 0.9839486479759216)
[2025-02-13 03:52:03,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:03,951][root][INFO] - Training Epoch: 2/2, step 2483/7134 completed (loss: 0.012941880151629448, acc: 0.9958391189575195)
[2025-02-13 03:52:04,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:04,370][root][INFO] - Training Epoch: 2/2, step 2484/7134 completed (loss: 0.026481730863451958, acc: 0.9902777671813965)
[2025-02-13 03:52:04,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:04,782][root][INFO] - Training Epoch: 2/2, step 2485/7134 completed (loss: 0.017184661701321602, acc: 0.9952380657196045)
[2025-02-13 03:52:04,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:05,201][root][INFO] - Training Epoch: 2/2, step 2486/7134 completed (loss: 0.01953238993883133, acc: 0.9971264600753784)
[2025-02-13 03:52:05,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:05,600][root][INFO] - Training Epoch: 2/2, step 2487/7134 completed (loss: 0.0718914270401001, acc: 0.9858267903327942)
[2025-02-13 03:52:05,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:06,017][root][INFO] - Training Epoch: 2/2, step 2488/7134 completed (loss: 0.02801825851202011, acc: 0.9903314709663391)
[2025-02-13 03:52:06,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:06,414][root][INFO] - Training Epoch: 2/2, step 2489/7134 completed (loss: 0.017375105991959572, acc: 0.9944289922714233)
[2025-02-13 03:52:06,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:06,831][root][INFO] - Training Epoch: 2/2, step 2490/7134 completed (loss: 0.017781497910618782, acc: 0.9942938685417175)
[2025-02-13 03:52:06,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:07,234][root][INFO] - Training Epoch: 2/2, step 2491/7134 completed (loss: 0.09588580578565598, acc: 0.9814814925193787)
[2025-02-13 03:52:07,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:07,654][root][INFO] - Training Epoch: 2/2, step 2492/7134 completed (loss: 0.040494903922080994, acc: 0.985111653804779)
[2025-02-13 03:52:07,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:08,102][root][INFO] - Training Epoch: 2/2, step 2493/7134 completed (loss: 0.046418316662311554, acc: 0.9788732528686523)
[2025-02-13 03:52:08,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:08,510][root][INFO] - Training Epoch: 2/2, step 2494/7134 completed (loss: 0.043720535933971405, acc: 0.9849812388420105)
[2025-02-13 03:52:08,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:08,936][root][INFO] - Training Epoch: 2/2, step 2495/7134 completed (loss: 0.03734545782208443, acc: 0.9864314794540405)
[2025-02-13 03:52:09,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:09,371][root][INFO] - Training Epoch: 2/2, step 2496/7134 completed (loss: 0.018677981570363045, acc: 0.9971910119056702)
[2025-02-13 03:52:09,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:09,811][root][INFO] - Training Epoch: 2/2, step 2497/7134 completed (loss: 0.03784150630235672, acc: 0.989051103591919)
[2025-02-13 03:52:09,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:10,259][root][INFO] - Training Epoch: 2/2, step 2498/7134 completed (loss: 0.031548041850328445, acc: 0.9886234402656555)
[2025-02-13 03:52:10,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:10,708][root][INFO] - Training Epoch: 2/2, step 2499/7134 completed (loss: 0.023723455145955086, acc: 0.9939613342285156)
[2025-02-13 03:52:10,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:11,139][root][INFO] - Training Epoch: 2/2, step 2500/7134 completed (loss: 0.018398990854620934, acc: 0.9961389899253845)
[2025-02-13 03:52:11,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:11,570][root][INFO] - Training Epoch: 2/2, step 2501/7134 completed (loss: 0.03183293342590332, acc: 0.9885583519935608)
[2025-02-13 03:52:11,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:12,008][root][INFO] - Training Epoch: 2/2, step 2502/7134 completed (loss: 0.01625603251159191, acc: 0.9946996569633484)
[2025-02-13 03:52:12,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:12,398][root][INFO] - Training Epoch: 2/2, step 2503/7134 completed (loss: 0.03439490124583244, acc: 0.994301974773407)
[2025-02-13 03:52:12,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:12,819][root][INFO] - Training Epoch: 2/2, step 2504/7134 completed (loss: 0.031794216483831406, acc: 0.9906666874885559)
[2025-02-13 03:52:12,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:13,254][root][INFO] - Training Epoch: 2/2, step 2505/7134 completed (loss: 0.018988512456417084, acc: 0.9943116903305054)
[2025-02-13 03:52:13,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:13,724][root][INFO] - Training Epoch: 2/2, step 2506/7134 completed (loss: 0.027647843584418297, acc: 0.9919871687889099)
[2025-02-13 03:52:13,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:14,165][root][INFO] - Training Epoch: 2/2, step 2507/7134 completed (loss: 0.008743355050683022, acc: 1.0)
[2025-02-13 03:52:14,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:14,628][root][INFO] - Training Epoch: 2/2, step 2508/7134 completed (loss: 0.03795044496655464, acc: 0.9884792566299438)
[2025-02-13 03:52:14,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:15,053][root][INFO] - Training Epoch: 2/2, step 2509/7134 completed (loss: 0.014217322692275047, acc: 0.9963280558586121)
[2025-02-13 03:52:15,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:15,488][root][INFO] - Training Epoch: 2/2, step 2510/7134 completed (loss: 0.01669066958129406, acc: 0.9939302206039429)
[2025-02-13 03:52:15,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:15,921][root][INFO] - Training Epoch: 2/2, step 2511/7134 completed (loss: 0.03647952154278755, acc: 0.9943946003913879)
[2025-02-13 03:52:16,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:16,334][root][INFO] - Training Epoch: 2/2, step 2512/7134 completed (loss: 0.0552070252597332, acc: 0.9824561476707458)
[2025-02-13 03:52:16,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:16,783][root][INFO] - Training Epoch: 2/2, step 2513/7134 completed (loss: 0.02071884088218212, acc: 0.9940758347511292)
[2025-02-13 03:52:16,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:17,224][root][INFO] - Training Epoch: 2/2, step 2514/7134 completed (loss: 0.01867729425430298, acc: 0.9929988384246826)
[2025-02-13 03:52:17,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:17,660][root][INFO] - Training Epoch: 2/2, step 2515/7134 completed (loss: 0.009274076670408249, acc: 0.9986996054649353)
[2025-02-13 03:52:17,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:18,112][root][INFO] - Training Epoch: 2/2, step 2516/7134 completed (loss: 0.012010636739432812, acc: 0.9954545497894287)
[2025-02-13 03:52:18,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:18,570][root][INFO] - Training Epoch: 2/2, step 2517/7134 completed (loss: 0.036425020545721054, acc: 0.991963267326355)
[2025-02-13 03:52:18,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:18,977][root][INFO] - Training Epoch: 2/2, step 2518/7134 completed (loss: 0.019232558086514473, acc: 0.9942775368690491)
[2025-02-13 03:52:19,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:19,416][root][INFO] - Training Epoch: 2/2, step 2519/7134 completed (loss: 0.0060522169806063175, acc: 0.9986168742179871)
[2025-02-13 03:52:19,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:19,839][root][INFO] - Training Epoch: 2/2, step 2520/7134 completed (loss: 0.011953478679060936, acc: 0.994301974773407)
[2025-02-13 03:52:19,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:20,283][root][INFO] - Training Epoch: 2/2, step 2521/7134 completed (loss: 0.05974921956658363, acc: 0.9871299862861633)
[2025-02-13 03:52:20,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:20,686][root][INFO] - Training Epoch: 2/2, step 2522/7134 completed (loss: 0.025601733475923538, acc: 0.9914039969444275)
[2025-02-13 03:52:20,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:21,107][root][INFO] - Training Epoch: 2/2, step 2523/7134 completed (loss: 0.038646504282951355, acc: 0.9934554696083069)
[2025-02-13 03:52:21,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:21,527][root][INFO] - Training Epoch: 2/2, step 2524/7134 completed (loss: 0.004535383079200983, acc: 1.0)
[2025-02-13 03:52:21,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:21,917][root][INFO] - Training Epoch: 2/2, step 2525/7134 completed (loss: 0.028080318123102188, acc: 0.989983320236206)
[2025-02-13 03:52:22,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:22,246][root][INFO] - Training Epoch: 2/2, step 2526/7134 completed (loss: 0.00342005118727684, acc: 1.0)
[2025-02-13 03:52:22,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:22,683][root][INFO] - Training Epoch: 2/2, step 2527/7134 completed (loss: 0.02435092255473137, acc: 0.9914634227752686)
[2025-02-13 03:52:22,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:23,105][root][INFO] - Training Epoch: 2/2, step 2528/7134 completed (loss: 0.010266821831464767, acc: 0.99726402759552)
[2025-02-13 03:52:23,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:23,546][root][INFO] - Training Epoch: 2/2, step 2529/7134 completed (loss: 0.011385208927094936, acc: 0.998670220375061)
[2025-02-13 03:52:23,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:23,997][root][INFO] - Training Epoch: 2/2, step 2530/7134 completed (loss: 0.019136575981974602, acc: 0.9935064911842346)
[2025-02-13 03:52:24,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:24,406][root][INFO] - Training Epoch: 2/2, step 2531/7134 completed (loss: 0.01228636410087347, acc: 0.9958563446998596)
[2025-02-13 03:52:24,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:24,814][root][INFO] - Training Epoch: 2/2, step 2532/7134 completed (loss: 0.006845603697001934, acc: 0.9984591603279114)
[2025-02-13 03:52:24,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:25,223][root][INFO] - Training Epoch: 2/2, step 2533/7134 completed (loss: 0.023272816091775894, acc: 0.9924585223197937)
[2025-02-13 03:52:25,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:25,632][root][INFO] - Training Epoch: 2/2, step 2534/7134 completed (loss: 0.03743467479944229, acc: 0.9961783289909363)
[2025-02-13 03:52:25,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:26,047][root][INFO] - Training Epoch: 2/2, step 2535/7134 completed (loss: 0.028242867439985275, acc: 0.9922928810119629)
[2025-02-13 03:52:26,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:26,489][root][INFO] - Training Epoch: 2/2, step 2536/7134 completed (loss: 0.01955253817141056, acc: 0.9926199316978455)
[2025-02-13 03:52:26,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:26,891][root][INFO] - Training Epoch: 2/2, step 2537/7134 completed (loss: 0.05367250367999077, acc: 0.9862385392189026)
[2025-02-13 03:52:27,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:27,302][root][INFO] - Training Epoch: 2/2, step 2538/7134 completed (loss: 0.04872714728116989, acc: 0.9886685609817505)
[2025-02-13 03:52:27,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:27,654][root][INFO] - Training Epoch: 2/2, step 2539/7134 completed (loss: 0.017299562692642212, acc: 0.9946428537368774)
[2025-02-13 03:52:27,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:28,067][root][INFO] - Training Epoch: 2/2, step 2540/7134 completed (loss: 0.053550686687231064, acc: 0.9862385392189026)
[2025-02-13 03:52:28,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:28,469][root][INFO] - Training Epoch: 2/2, step 2541/7134 completed (loss: 0.01411372795701027, acc: 0.9950330853462219)
[2025-02-13 03:52:28,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:28,869][root][INFO] - Training Epoch: 2/2, step 2542/7134 completed (loss: 0.04971715807914734, acc: 0.9881656765937805)
[2025-02-13 03:52:29,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:29,303][root][INFO] - Training Epoch: 2/2, step 2543/7134 completed (loss: 0.06647349894046783, acc: 0.9877049326896667)
[2025-02-13 03:52:29,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:29,743][root][INFO] - Training Epoch: 2/2, step 2544/7134 completed (loss: 0.03138923645019531, acc: 0.9896907210350037)
[2025-02-13 03:52:29,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:30,165][root][INFO] - Training Epoch: 2/2, step 2545/7134 completed (loss: 0.03397800773382187, acc: 0.9930192232131958)
[2025-02-13 03:52:30,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:30,628][root][INFO] - Training Epoch: 2/2, step 2546/7134 completed (loss: 0.1053890585899353, acc: 0.9675870537757874)
[2025-02-13 03:52:30,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:31,039][root][INFO] - Training Epoch: 2/2, step 2547/7134 completed (loss: 0.05545768886804581, acc: 0.9838969111442566)
[2025-02-13 03:52:31,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:31,429][root][INFO] - Training Epoch: 2/2, step 2548/7134 completed (loss: 0.02498169057071209, acc: 0.9921259880065918)
[2025-02-13 03:52:31,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:31,837][root][INFO] - Training Epoch: 2/2, step 2549/7134 completed (loss: 0.0488203726708889, acc: 0.9852670431137085)
[2025-02-13 03:52:31,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:32,235][root][INFO] - Training Epoch: 2/2, step 2550/7134 completed (loss: 0.016375645995140076, acc: 0.9967373609542847)
[2025-02-13 03:52:32,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:32,637][root][INFO] - Training Epoch: 2/2, step 2551/7134 completed (loss: 0.06294258683919907, acc: 0.9884488582611084)
[2025-02-13 03:52:32,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:32,991][root][INFO] - Training Epoch: 2/2, step 2552/7134 completed (loss: 0.011057744733989239, acc: 0.9936908483505249)
[2025-02-13 03:52:33,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:33,286][root][INFO] - Training Epoch: 2/2, step 2553/7134 completed (loss: 0.02109368145465851, acc: 0.9909365773200989)
[2025-02-13 03:52:33,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:33,714][root][INFO] - Training Epoch: 2/2, step 2554/7134 completed (loss: 0.05876349285244942, acc: 0.9849246144294739)
[2025-02-13 03:52:33,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:34,106][root][INFO] - Training Epoch: 2/2, step 2555/7134 completed (loss: 0.026470808312296867, acc: 0.9917355179786682)
[2025-02-13 03:52:34,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:34,506][root][INFO] - Training Epoch: 2/2, step 2556/7134 completed (loss: 0.03787612542510033, acc: 0.9832285046577454)
[2025-02-13 03:52:34,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:34,935][root][INFO] - Training Epoch: 2/2, step 2557/7134 completed (loss: 0.03587760031223297, acc: 0.9901639223098755)
[2025-02-13 03:52:35,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:35,339][root][INFO] - Training Epoch: 2/2, step 2558/7134 completed (loss: 0.011056422255933285, acc: 0.9983818531036377)
[2025-02-13 03:52:35,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:35,755][root][INFO] - Training Epoch: 2/2, step 2559/7134 completed (loss: 0.013386407867074013, acc: 0.9972936511039734)
[2025-02-13 03:52:35,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:36,203][root][INFO] - Training Epoch: 2/2, step 2560/7134 completed (loss: 0.03330940380692482, acc: 0.9902439117431641)
[2025-02-13 03:52:36,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:36,614][root][INFO] - Training Epoch: 2/2, step 2561/7134 completed (loss: 0.02369067259132862, acc: 0.9934640526771545)
[2025-02-13 03:52:36,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:37,009][root][INFO] - Training Epoch: 2/2, step 2562/7134 completed (loss: 0.041671521961688995, acc: 0.9914529919624329)
[2025-02-13 03:52:37,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:37,433][root][INFO] - Training Epoch: 2/2, step 2563/7134 completed (loss: 0.03090851381421089, acc: 0.9956011772155762)
[2025-02-13 03:52:37,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:37,872][root][INFO] - Training Epoch: 2/2, step 2564/7134 completed (loss: 0.08469997346401215, acc: 0.9815436005592346)
[2025-02-13 03:52:38,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:38,275][root][INFO] - Training Epoch: 2/2, step 2565/7134 completed (loss: 0.019984325394034386, acc: 0.995468258857727)
[2025-02-13 03:52:38,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:38,724][root][INFO] - Training Epoch: 2/2, step 2566/7134 completed (loss: 0.026571085676550865, acc: 0.9887640476226807)
[2025-02-13 03:52:38,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:39,054][root][INFO] - Training Epoch: 2/2, step 2567/7134 completed (loss: 0.024663999676704407, acc: 0.9911816716194153)
[2025-02-13 03:52:39,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:39,466][root][INFO] - Training Epoch: 2/2, step 2568/7134 completed (loss: 0.04204928129911423, acc: 0.9855491518974304)
[2025-02-13 03:52:39,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:39,891][root][INFO] - Training Epoch: 2/2, step 2569/7134 completed (loss: 0.032704856246709824, acc: 0.990963876247406)
[2025-02-13 03:52:40,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:40,307][root][INFO] - Training Epoch: 2/2, step 2570/7134 completed (loss: 0.038759760558605194, acc: 0.9910045266151428)
[2025-02-13 03:52:40,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:40,691][root][INFO] - Training Epoch: 2/2, step 2571/7134 completed (loss: 0.025350866839289665, acc: 0.9912126660346985)
[2025-02-13 03:52:40,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:41,108][root][INFO] - Training Epoch: 2/2, step 2572/7134 completed (loss: 0.024395938962697983, acc: 0.9921135902404785)
[2025-02-13 03:52:41,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:41,518][root][INFO] - Training Epoch: 2/2, step 2573/7134 completed (loss: 0.04998307302594185, acc: 0.9852458834648132)
[2025-02-13 03:52:41,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:41,901][root][INFO] - Training Epoch: 2/2, step 2574/7134 completed (loss: 0.03013579174876213, acc: 0.9890829920768738)
[2025-02-13 03:52:42,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:42,293][root][INFO] - Training Epoch: 2/2, step 2575/7134 completed (loss: 0.015142548829317093, acc: 0.9944238066673279)
[2025-02-13 03:52:42,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:42,695][root][INFO] - Training Epoch: 2/2, step 2576/7134 completed (loss: 0.02216314896941185, acc: 0.9948275685310364)
[2025-02-13 03:52:42,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:43,105][root][INFO] - Training Epoch: 2/2, step 2577/7134 completed (loss: 0.056600913405418396, acc: 0.988034188747406)
[2025-02-13 03:52:43,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:43,511][root][INFO] - Training Epoch: 2/2, step 2578/7134 completed (loss: 0.05182487517595291, acc: 0.9831546545028687)
[2025-02-13 03:52:43,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:43,919][root][INFO] - Training Epoch: 2/2, step 2579/7134 completed (loss: 0.06479693949222565, acc: 0.9778481125831604)
[2025-02-13 03:52:44,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:44,361][root][INFO] - Training Epoch: 2/2, step 2580/7134 completed (loss: 0.04081714525818825, acc: 0.9869451522827148)
[2025-02-13 03:52:44,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:44,787][root][INFO] - Training Epoch: 2/2, step 2581/7134 completed (loss: 0.041093382984399796, acc: 0.9892328381538391)
[2025-02-13 03:52:44,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:45,223][root][INFO] - Training Epoch: 2/2, step 2582/7134 completed (loss: 0.042337436228990555, acc: 0.9810298085212708)
[2025-02-13 03:52:45,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:45,629][root][INFO] - Training Epoch: 2/2, step 2583/7134 completed (loss: 0.04685825854539871, acc: 0.9843527674674988)
[2025-02-13 03:52:45,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:45,999][root][INFO] - Training Epoch: 2/2, step 2584/7134 completed (loss: 0.07222367078065872, acc: 0.9790576100349426)
[2025-02-13 03:52:46,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:46,452][root][INFO] - Training Epoch: 2/2, step 2585/7134 completed (loss: 0.05046684294939041, acc: 0.9913669228553772)
[2025-02-13 03:52:46,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:46,884][root][INFO] - Training Epoch: 2/2, step 2586/7134 completed (loss: 0.04639837145805359, acc: 0.98591548204422)
[2025-02-13 03:52:47,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:47,328][root][INFO] - Training Epoch: 2/2, step 2587/7134 completed (loss: 0.03643471747636795, acc: 0.9889435172080994)
[2025-02-13 03:52:47,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:47,765][root][INFO] - Training Epoch: 2/2, step 2588/7134 completed (loss: 0.008200396783649921, acc: 0.9986357688903809)
[2025-02-13 03:52:47,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:48,168][root][INFO] - Training Epoch: 2/2, step 2589/7134 completed (loss: 0.060746390372514725, acc: 0.9893993139266968)
[2025-02-13 03:52:48,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:48,553][root][INFO] - Training Epoch: 2/2, step 2590/7134 completed (loss: 0.026344837620854378, acc: 0.9934640526771545)
[2025-02-13 03:52:48,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:48,984][root][INFO] - Training Epoch: 2/2, step 2591/7134 completed (loss: 0.06432264298200607, acc: 0.9833759665489197)
[2025-02-13 03:52:49,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:49,433][root][INFO] - Training Epoch: 2/2, step 2592/7134 completed (loss: 0.03749378025531769, acc: 0.9864698648452759)
[2025-02-13 03:52:49,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:49,876][root][INFO] - Training Epoch: 2/2, step 2593/7134 completed (loss: 0.036457717418670654, acc: 0.9921466112136841)
[2025-02-13 03:52:50,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:50,288][root][INFO] - Training Epoch: 2/2, step 2594/7134 completed (loss: 0.036523934453725815, acc: 0.9877675771713257)
[2025-02-13 03:52:50,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:50,725][root][INFO] - Training Epoch: 2/2, step 2595/7134 completed (loss: 0.02063428796827793, acc: 0.9946236610412598)
[2025-02-13 03:52:50,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:51,192][root][INFO] - Training Epoch: 2/2, step 2596/7134 completed (loss: 0.03381772339344025, acc: 0.9899749159812927)
[2025-02-13 03:52:51,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:51,629][root][INFO] - Training Epoch: 2/2, step 2597/7134 completed (loss: 0.04195452481508255, acc: 0.9891892075538635)
[2025-02-13 03:52:51,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:52,044][root][INFO] - Training Epoch: 2/2, step 2598/7134 completed (loss: 0.06761059910058975, acc: 0.9798761606216431)
[2025-02-13 03:52:52,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:52,447][root][INFO] - Training Epoch: 2/2, step 2599/7134 completed (loss: 0.012181089259684086, acc: 0.9947229623794556)
[2025-02-13 03:52:52,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:52,905][root][INFO] - Training Epoch: 2/2, step 2600/7134 completed (loss: 0.049092553555965424, acc: 0.984635055065155)
[2025-02-13 03:52:53,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:53,345][root][INFO] - Training Epoch: 2/2, step 2601/7134 completed (loss: 0.05874183773994446, acc: 0.9797022938728333)
[2025-02-13 03:52:53,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:53,781][root][INFO] - Training Epoch: 2/2, step 2602/7134 completed (loss: 0.038378361612558365, acc: 0.990314781665802)
[2025-02-13 03:52:53,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:54,237][root][INFO] - Training Epoch: 2/2, step 2603/7134 completed (loss: 0.041407469660043716, acc: 0.9891473054885864)
[2025-02-13 03:52:54,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:54,694][root][INFO] - Training Epoch: 2/2, step 2604/7134 completed (loss: 0.025831347331404686, acc: 0.9912280440330505)
[2025-02-13 03:52:54,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:55,127][root][INFO] - Training Epoch: 2/2, step 2605/7134 completed (loss: 0.06687068939208984, acc: 0.9765343070030212)
[2025-02-13 03:52:55,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:55,565][root][INFO] - Training Epoch: 2/2, step 2606/7134 completed (loss: 0.02274763584136963, acc: 0.9942445755004883)
[2025-02-13 03:52:55,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:56,006][root][INFO] - Training Epoch: 2/2, step 2607/7134 completed (loss: 0.036686547100543976, acc: 0.9835293889045715)
[2025-02-13 03:52:56,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:56,441][root][INFO] - Training Epoch: 2/2, step 2608/7134 completed (loss: 0.05484956130385399, acc: 0.9814586043357849)
[2025-02-13 03:52:56,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:56,875][root][INFO] - Training Epoch: 2/2, step 2609/7134 completed (loss: 0.037519171833992004, acc: 0.9888579249382019)
[2025-02-13 03:52:56,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:57,215][root][INFO] - Training Epoch: 2/2, step 2610/7134 completed (loss: 0.07015317678451538, acc: 0.9877862334251404)
[2025-02-13 03:52:57,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:57,679][root][INFO] - Training Epoch: 2/2, step 2611/7134 completed (loss: 0.06708666682243347, acc: 0.9804822206497192)
[2025-02-13 03:52:57,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:58,140][root][INFO] - Training Epoch: 2/2, step 2612/7134 completed (loss: 0.04941919445991516, acc: 0.9851936101913452)
[2025-02-13 03:52:58,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:58,616][root][INFO] - Training Epoch: 2/2, step 2613/7134 completed (loss: 0.03160838410258293, acc: 0.9898734092712402)
[2025-02-13 03:52:58,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:59,075][root][INFO] - Training Epoch: 2/2, step 2614/7134 completed (loss: 0.06372799724340439, acc: 0.980637788772583)
[2025-02-13 03:52:59,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:59,514][root][INFO] - Training Epoch: 2/2, step 2615/7134 completed (loss: 0.05098959058523178, acc: 0.9832904934883118)
[2025-02-13 03:52:59,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:59,925][root][INFO] - Training Epoch: 2/2, step 2616/7134 completed (loss: 0.062292374670505524, acc: 0.9814814925193787)
[2025-02-13 03:53:00,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:00,370][root][INFO] - Training Epoch: 2/2, step 2617/7134 completed (loss: 0.04870503395795822, acc: 0.9851552248001099)
[2025-02-13 03:53:00,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:00,736][root][INFO] - Training Epoch: 2/2, step 2618/7134 completed (loss: 0.029873106628656387, acc: 0.9917864203453064)
[2025-02-13 03:53:00,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:01,158][root][INFO] - Training Epoch: 2/2, step 2619/7134 completed (loss: 0.04774457961320877, acc: 0.9848993420600891)
[2025-02-13 03:53:01,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:01,600][root][INFO] - Training Epoch: 2/2, step 2620/7134 completed (loss: 0.04895373433828354, acc: 0.986369252204895)
[2025-02-13 03:53:01,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:02,041][root][INFO] - Training Epoch: 2/2, step 2621/7134 completed (loss: 0.035083744674921036, acc: 0.9952606558799744)
[2025-02-13 03:53:02,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:02,500][root][INFO] - Training Epoch: 2/2, step 2622/7134 completed (loss: 0.04260455071926117, acc: 0.9885433912277222)
[2025-02-13 03:53:02,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:02,958][root][INFO] - Training Epoch: 2/2, step 2623/7134 completed (loss: 0.045046549290418625, acc: 0.9839080572128296)
[2025-02-13 03:53:03,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:03,404][root][INFO] - Training Epoch: 2/2, step 2624/7134 completed (loss: 0.03837806358933449, acc: 0.9921259880065918)
[2025-02-13 03:53:03,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:03,845][root][INFO] - Training Epoch: 2/2, step 2625/7134 completed (loss: 0.037900689989328384, acc: 0.987500011920929)
[2025-02-13 03:53:03,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:04,288][root][INFO] - Training Epoch: 2/2, step 2626/7134 completed (loss: 0.051934659481048584, acc: 0.9876237511634827)
[2025-02-13 03:53:04,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:04,737][root][INFO] - Training Epoch: 2/2, step 2627/7134 completed (loss: 0.024380525574088097, acc: 0.9918808937072754)
[2025-02-13 03:53:04,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:05,157][root][INFO] - Training Epoch: 2/2, step 2628/7134 completed (loss: 0.04905293136835098, acc: 0.9861111044883728)
[2025-02-13 03:53:05,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:05,664][root][INFO] - Training Epoch: 2/2, step 2629/7134 completed (loss: 0.019601693376898766, acc: 0.993678867816925)
[2025-02-13 03:53:05,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:06,072][root][INFO] - Training Epoch: 2/2, step 2630/7134 completed (loss: 0.03566320613026619, acc: 0.9888178706169128)
[2025-02-13 03:53:06,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:06,510][root][INFO] - Training Epoch: 2/2, step 2631/7134 completed (loss: 0.02058192901313305, acc: 0.9941520690917969)
[2025-02-13 03:53:06,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:06,947][root][INFO] - Training Epoch: 2/2, step 2632/7134 completed (loss: 0.024035265669226646, acc: 0.9921414256095886)
[2025-02-13 03:53:07,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:07,387][root][INFO] - Training Epoch: 2/2, step 2633/7134 completed (loss: 0.03929492458701134, acc: 0.9900850057601929)
[2025-02-13 03:53:07,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:07,788][root][INFO] - Training Epoch: 2/2, step 2634/7134 completed (loss: 0.02057974971830845, acc: 0.9940029978752136)
[2025-02-13 03:53:07,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:08,234][root][INFO] - Training Epoch: 2/2, step 2635/7134 completed (loss: 0.0840117484331131, acc: 0.9819444417953491)
[2025-02-13 03:53:08,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:08,680][root][INFO] - Training Epoch: 2/2, step 2636/7134 completed (loss: 0.06379249691963196, acc: 0.9843527674674988)
[2025-02-13 03:53:08,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:09,094][root][INFO] - Training Epoch: 2/2, step 2637/7134 completed (loss: 0.14886999130249023, acc: 0.9626485705375671)
[2025-02-13 03:53:09,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:09,545][root][INFO] - Training Epoch: 2/2, step 2638/7134 completed (loss: 0.08254951983690262, acc: 0.9752281904220581)
[2025-02-13 03:53:09,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:09,953][root][INFO] - Training Epoch: 2/2, step 2639/7134 completed (loss: 0.06616394221782684, acc: 0.9751037359237671)
[2025-02-13 03:53:10,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:10,385][root][INFO] - Training Epoch: 2/2, step 2640/7134 completed (loss: 0.06469479948282242, acc: 0.9850187301635742)
[2025-02-13 03:53:10,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:10,818][root][INFO] - Training Epoch: 2/2, step 2641/7134 completed (loss: 0.056428007781505585, acc: 0.9779086709022522)
[2025-02-13 03:53:10,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:11,210][root][INFO] - Training Epoch: 2/2, step 2642/7134 completed (loss: 0.08322501927614212, acc: 0.9768683314323425)
[2025-02-13 03:53:11,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:11,639][root][INFO] - Training Epoch: 2/2, step 2643/7134 completed (loss: 0.04135487228631973, acc: 0.985049843788147)
[2025-02-13 03:53:11,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:12,113][root][INFO] - Training Epoch: 2/2, step 2644/7134 completed (loss: 0.06578819453716278, acc: 0.9854369163513184)
[2025-02-13 03:53:12,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:12,525][root][INFO] - Training Epoch: 2/2, step 2645/7134 completed (loss: 0.05118391662836075, acc: 0.9839650392532349)
[2025-02-13 03:53:12,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:12,923][root][INFO] - Training Epoch: 2/2, step 2646/7134 completed (loss: 0.12030695378780365, acc: 0.9633758068084717)
[2025-02-13 03:53:13,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:13,366][root][INFO] - Training Epoch: 2/2, step 2647/7134 completed (loss: 0.03591016307473183, acc: 0.9895969033241272)
[2025-02-13 03:53:13,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:13,791][root][INFO] - Training Epoch: 2/2, step 2648/7134 completed (loss: 0.03240268677473068, acc: 0.9896774291992188)
[2025-02-13 03:53:13,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:14,219][root][INFO] - Training Epoch: 2/2, step 2649/7134 completed (loss: 0.10227179527282715, acc: 0.9716840386390686)
[2025-02-13 03:53:14,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:14,613][root][INFO] - Training Epoch: 2/2, step 2650/7134 completed (loss: 0.10258294641971588, acc: 0.974281370639801)
[2025-02-13 03:53:14,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:15,049][root][INFO] - Training Epoch: 2/2, step 2651/7134 completed (loss: 0.03509536758065224, acc: 0.9880810379981995)
[2025-02-13 03:53:15,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:15,467][root][INFO] - Training Epoch: 2/2, step 2652/7134 completed (loss: 0.04028738662600517, acc: 0.9899117350578308)
[2025-02-13 03:53:15,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:15,856][root][INFO] - Training Epoch: 2/2, step 2653/7134 completed (loss: 0.03714257851243019, acc: 0.9819079041481018)
[2025-02-13 03:53:15,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:16,267][root][INFO] - Training Epoch: 2/2, step 2654/7134 completed (loss: 0.0415029302239418, acc: 0.9868852496147156)
[2025-02-13 03:53:16,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:16,665][root][INFO] - Training Epoch: 2/2, step 2655/7134 completed (loss: 0.0224718376994133, acc: 0.9909228682518005)
[2025-02-13 03:53:16,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:17,085][root][INFO] - Training Epoch: 2/2, step 2656/7134 completed (loss: 0.0498843714594841, acc: 0.9874826073646545)
[2025-02-13 03:53:17,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:17,492][root][INFO] - Training Epoch: 2/2, step 2657/7134 completed (loss: 0.029707729816436768, acc: 0.9935483932495117)
[2025-02-13 03:53:17,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:17,903][root][INFO] - Training Epoch: 2/2, step 2658/7134 completed (loss: 0.09922154992818832, acc: 0.9813084006309509)
[2025-02-13 03:53:18,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:18,277][root][INFO] - Training Epoch: 2/2, step 2659/7134 completed (loss: 0.045142173767089844, acc: 0.9865900278091431)
[2025-02-13 03:53:18,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:18,637][root][INFO] - Training Epoch: 2/2, step 2660/7134 completed (loss: 0.03503954038023949, acc: 0.9838709831237793)
[2025-02-13 03:53:18,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:19,050][root][INFO] - Training Epoch: 2/2, step 2661/7134 completed (loss: 0.04009915143251419, acc: 0.991584837436676)
[2025-02-13 03:53:19,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:19,492][root][INFO] - Training Epoch: 2/2, step 2662/7134 completed (loss: 0.01635659672319889, acc: 0.9943100810050964)
[2025-02-13 03:53:19,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:19,953][root][INFO] - Training Epoch: 2/2, step 2663/7134 completed (loss: 0.028860928490757942, acc: 0.9906432628631592)
[2025-02-13 03:53:20,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:20,393][root][INFO] - Training Epoch: 2/2, step 2664/7134 completed (loss: 0.01187160611152649, acc: 0.9948387145996094)
[2025-02-13 03:53:20,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:20,817][root][INFO] - Training Epoch: 2/2, step 2665/7134 completed (loss: 0.009833510033786297, acc: 0.9986979365348816)
[2025-02-13 03:53:20,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:21,276][root][INFO] - Training Epoch: 2/2, step 2666/7134 completed (loss: 0.023386243730783463, acc: 0.9914320707321167)
[2025-02-13 03:53:21,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:21,694][root][INFO] - Training Epoch: 2/2, step 2667/7134 completed (loss: 0.011493418365716934, acc: 0.9985835552215576)
[2025-02-13 03:53:21,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:22,108][root][INFO] - Training Epoch: 2/2, step 2668/7134 completed (loss: 0.023601850494742393, acc: 0.9959623217582703)
[2025-02-13 03:53:22,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:22,563][root][INFO] - Training Epoch: 2/2, step 2669/7134 completed (loss: 0.005919411312788725, acc: 0.9988492727279663)
[2025-02-13 03:53:22,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:22,986][root][INFO] - Training Epoch: 2/2, step 2670/7134 completed (loss: 0.0231948122382164, acc: 0.9908257126808167)
[2025-02-13 03:53:23,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:23,402][root][INFO] - Training Epoch: 2/2, step 2671/7134 completed (loss: 0.0171921756118536, acc: 0.9933155179023743)
[2025-02-13 03:53:23,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:23,843][root][INFO] - Training Epoch: 2/2, step 2672/7134 completed (loss: 0.012142757885158062, acc: 0.9956268072128296)
[2025-02-13 03:53:23,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:24,299][root][INFO] - Training Epoch: 2/2, step 2673/7134 completed (loss: 0.006466804537922144, acc: 0.997474730014801)
[2025-02-13 03:53:24,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:24,705][root][INFO] - Training Epoch: 2/2, step 2674/7134 completed (loss: 0.013957704417407513, acc: 0.9944211840629578)
[2025-02-13 03:53:24,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:25,154][root][INFO] - Training Epoch: 2/2, step 2675/7134 completed (loss: 0.004389616660773754, acc: 1.0)
[2025-02-13 03:53:25,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:25,579][root][INFO] - Training Epoch: 2/2, step 2676/7134 completed (loss: 0.01685520075261593, acc: 0.9935064911842346)
[2025-02-13 03:53:25,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:26,017][root][INFO] - Training Epoch: 2/2, step 2677/7134 completed (loss: 0.012647817842662334, acc: 0.9955357313156128)
[2025-02-13 03:53:26,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:26,469][root][INFO] - Training Epoch: 2/2, step 2678/7134 completed (loss: 0.0036447388119995594, acc: 1.0)
[2025-02-13 03:53:26,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:26,926][root][INFO] - Training Epoch: 2/2, step 2679/7134 completed (loss: 0.0098335649818182, acc: 0.996632993221283)
[2025-02-13 03:53:27,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:27,369][root][INFO] - Training Epoch: 2/2, step 2680/7134 completed (loss: 0.01421968825161457, acc: 0.9963008761405945)
[2025-02-13 03:53:27,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:27,768][root][INFO] - Training Epoch: 2/2, step 2681/7134 completed (loss: 0.008560901507735252, acc: 0.9969651103019714)
[2025-02-13 03:53:27,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:28,132][root][INFO] - Training Epoch: 2/2, step 2682/7134 completed (loss: 0.009803131222724915, acc: 0.9968101978302002)
[2025-02-13 03:53:28,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:28,518][root][INFO] - Training Epoch: 2/2, step 2683/7134 completed (loss: 0.004828593228012323, acc: 0.9984825253486633)
[2025-02-13 03:53:28,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:28,908][root][INFO] - Training Epoch: 2/2, step 2684/7134 completed (loss: 0.028387097641825676, acc: 0.9928315281867981)
[2025-02-13 03:53:29,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:29,364][root][INFO] - Training Epoch: 2/2, step 2685/7134 completed (loss: 0.019127123057842255, acc: 0.9960988163948059)
[2025-02-13 03:53:29,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:29,831][root][INFO] - Training Epoch: 2/2, step 2686/7134 completed (loss: 0.03212136775255203, acc: 0.9884259104728699)
[2025-02-13 03:53:29,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:30,256][root][INFO] - Training Epoch: 2/2, step 2687/7134 completed (loss: 0.027425289154052734, acc: 0.9911660552024841)
[2025-02-13 03:53:30,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:30,698][root][INFO] - Training Epoch: 2/2, step 2688/7134 completed (loss: 0.03067309968173504, acc: 0.9921383857727051)
[2025-02-13 03:53:30,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:31,048][root][INFO] - Training Epoch: 2/2, step 2689/7134 completed (loss: 0.031808022409677505, acc: 0.9927667379379272)
[2025-02-13 03:53:31,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:31,478][root][INFO] - Training Epoch: 2/2, step 2690/7134 completed (loss: 0.061911940574645996, acc: 0.9829984307289124)
[2025-02-13 03:53:31,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:31,826][root][INFO] - Training Epoch: 2/2, step 2691/7134 completed (loss: 0.0678286924958229, acc: 0.98828125)
[2025-02-13 03:53:31,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:32,253][root][INFO] - Training Epoch: 2/2, step 2692/7134 completed (loss: 0.08071647584438324, acc: 0.9757914543151855)
[2025-02-13 03:53:32,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:32,605][root][INFO] - Training Epoch: 2/2, step 2693/7134 completed (loss: 0.10609471052885056, acc: 0.9730769395828247)
[2025-02-13 03:53:32,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:33,021][root][INFO] - Training Epoch: 2/2, step 2694/7134 completed (loss: 0.09082164615392685, acc: 0.9838235378265381)
[2025-02-13 03:53:33,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:33,494][root][INFO] - Training Epoch: 2/2, step 2695/7134 completed (loss: 0.03675145283341408, acc: 0.9932998418807983)
[2025-02-13 03:53:33,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:33,931][root][INFO] - Training Epoch: 2/2, step 2696/7134 completed (loss: 0.07871390879154205, acc: 0.9745628237724304)
[2025-02-13 03:53:34,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:34,335][root][INFO] - Training Epoch: 2/2, step 2697/7134 completed (loss: 0.08704914152622223, acc: 0.9768875241279602)
[2025-02-13 03:53:34,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:34,780][root][INFO] - Training Epoch: 2/2, step 2698/7134 completed (loss: 0.030613617971539497, acc: 0.9897435903549194)
[2025-02-13 03:53:34,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:35,218][root][INFO] - Training Epoch: 2/2, step 2699/7134 completed (loss: 0.03761785477399826, acc: 0.9889624714851379)
[2025-02-13 03:53:35,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:35,532][root][INFO] - Training Epoch: 2/2, step 2700/7134 completed (loss: 0.04425051808357239, acc: 0.9910979270935059)
[2025-02-13 03:53:35,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:35,909][root][INFO] - Training Epoch: 2/2, step 2701/7134 completed (loss: 0.0716715157032013, acc: 0.978151261806488)
[2025-02-13 03:53:35,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:36,138][root][INFO] - Training Epoch: 2/2, step 2702/7134 completed (loss: 0.052170272916555405, acc: 0.9947916865348816)
[2025-02-13 03:53:36,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:36,469][root][INFO] - Training Epoch: 2/2, step 2703/7134 completed (loss: 0.021675720810890198, acc: 0.993630588054657)
[2025-02-13 03:53:36,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:36,922][root][INFO] - Training Epoch: 2/2, step 2704/7134 completed (loss: 0.05180018022656441, acc: 0.986522912979126)
[2025-02-13 03:53:37,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:37,360][root][INFO] - Training Epoch: 2/2, step 2705/7134 completed (loss: 0.09739364683628082, acc: 0.9768518805503845)
[2025-02-13 03:53:37,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:37,813][root][INFO] - Training Epoch: 2/2, step 2706/7134 completed (loss: 0.06156664341688156, acc: 0.9869375824928284)
[2025-02-13 03:53:37,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:38,271][root][INFO] - Training Epoch: 2/2, step 2707/7134 completed (loss: 0.031090496107935905, acc: 0.9896551966667175)
[2025-02-13 03:53:38,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:38,729][root][INFO] - Training Epoch: 2/2, step 2708/7134 completed (loss: 0.04992491751909256, acc: 0.9859747290611267)
[2025-02-13 03:53:38,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:39,188][root][INFO] - Training Epoch: 2/2, step 2709/7134 completed (loss: 0.03427208587527275, acc: 0.9951298832893372)
[2025-02-13 03:53:39,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:39,611][root][INFO] - Training Epoch: 2/2, step 2710/7134 completed (loss: 0.04064459726214409, acc: 0.9836829900741577)
[2025-02-13 03:53:39,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:40,021][root][INFO] - Training Epoch: 2/2, step 2711/7134 completed (loss: 0.03366747125983238, acc: 0.9858490824699402)
[2025-02-13 03:53:40,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:40,460][root][INFO] - Training Epoch: 2/2, step 2712/7134 completed (loss: 0.03041309490799904, acc: 0.9910714030265808)
[2025-02-13 03:53:40,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:40,905][root][INFO] - Training Epoch: 2/2, step 2713/7134 completed (loss: 0.04936182498931885, acc: 0.9916434288024902)
[2025-02-13 03:53:41,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:41,336][root][INFO] - Training Epoch: 2/2, step 2714/7134 completed (loss: 0.031572241336107254, acc: 0.9916805028915405)
[2025-02-13 03:53:41,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:41,772][root][INFO] - Training Epoch: 2/2, step 2715/7134 completed (loss: 0.027935899794101715, acc: 0.9924812316894531)
[2025-02-13 03:53:41,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:42,206][root][INFO] - Training Epoch: 2/2, step 2716/7134 completed (loss: 0.03698243573307991, acc: 0.9908257126808167)
[2025-02-13 03:53:42,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:42,631][root][INFO] - Training Epoch: 2/2, step 2717/7134 completed (loss: 0.062356796115636826, acc: 0.987679660320282)
[2025-02-13 03:53:42,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:43,064][root][INFO] - Training Epoch: 2/2, step 2718/7134 completed (loss: 0.12780922651290894, acc: 0.9683098793029785)
[2025-02-13 03:53:43,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:43,504][root][INFO] - Training Epoch: 2/2, step 2719/7134 completed (loss: 0.05403084680438042, acc: 0.9864636063575745)
[2025-02-13 03:53:43,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:43,935][root][INFO] - Training Epoch: 2/2, step 2720/7134 completed (loss: 0.04351402446627617, acc: 0.9842271208763123)
[2025-02-13 03:53:44,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:44,291][root][INFO] - Training Epoch: 2/2, step 2721/7134 completed (loss: 0.055111195892095566, acc: 0.9869565367698669)
[2025-02-13 03:53:44,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:44,578][root][INFO] - Training Epoch: 2/2, step 2722/7134 completed (loss: 0.097626693546772, acc: 0.9760869741439819)
[2025-02-13 03:53:44,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:45,008][root][INFO] - Training Epoch: 2/2, step 2723/7134 completed (loss: 0.09083086997270584, acc: 0.9741697311401367)
[2025-02-13 03:53:45,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:45,447][root][INFO] - Training Epoch: 2/2, step 2724/7134 completed (loss: 0.06234005466103554, acc: 0.9817739725112915)
[2025-02-13 03:53:45,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:45,876][root][INFO] - Training Epoch: 2/2, step 2725/7134 completed (loss: 0.01394590549170971, acc: 0.9950739145278931)
[2025-02-13 03:53:45,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:46,259][root][INFO] - Training Epoch: 2/2, step 2726/7134 completed (loss: 0.07218047976493835, acc: 0.9815497994422913)
[2025-02-13 03:53:46,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:46,727][root][INFO] - Training Epoch: 2/2, step 2727/7134 completed (loss: 0.037714116275310516, acc: 0.9870370626449585)
[2025-02-13 03:53:46,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:47,138][root][INFO] - Training Epoch: 2/2, step 2728/7134 completed (loss: 0.09017585963010788, acc: 0.9724770784378052)
[2025-02-13 03:53:47,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:47,522][root][INFO] - Training Epoch: 2/2, step 2729/7134 completed (loss: 0.05516795441508293, acc: 0.9860627055168152)
[2025-02-13 03:53:47,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:47,924][root][INFO] - Training Epoch: 2/2, step 2730/7134 completed (loss: 0.06669016182422638, acc: 0.9791666865348816)
[2025-02-13 03:53:48,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:48,303][root][INFO] - Training Epoch: 2/2, step 2731/7134 completed (loss: 0.08473560959100723, acc: 0.9756690859794617)
[2025-02-13 03:53:48,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:48,741][root][INFO] - Training Epoch: 2/2, step 2732/7134 completed (loss: 0.03948768600821495, acc: 0.9885786771774292)
[2025-02-13 03:53:48,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:49,173][root][INFO] - Training Epoch: 2/2, step 2733/7134 completed (loss: 0.04625803977251053, acc: 0.9877883195877075)
[2025-02-13 03:53:49,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:49,561][root][INFO] - Training Epoch: 2/2, step 2734/7134 completed (loss: 0.049908820539712906, acc: 0.98591548204422)
[2025-02-13 03:53:49,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:50,024][root][INFO] - Training Epoch: 2/2, step 2735/7134 completed (loss: 0.047100525349378586, acc: 0.9878048896789551)
[2025-02-13 03:53:50,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:50,380][root][INFO] - Training Epoch: 2/2, step 2736/7134 completed (loss: 0.06438067555427551, acc: 0.97947758436203)
[2025-02-13 03:53:50,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:50,789][root][INFO] - Training Epoch: 2/2, step 2737/7134 completed (loss: 0.039059847593307495, acc: 0.9884488582611084)
[2025-02-13 03:53:50,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:51,186][root][INFO] - Training Epoch: 2/2, step 2738/7134 completed (loss: 0.025783950462937355, acc: 0.9885714054107666)
[2025-02-13 03:53:51,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:51,590][root][INFO] - Training Epoch: 2/2, step 2739/7134 completed (loss: 0.05137253180146217, acc: 0.9854545593261719)
[2025-02-13 03:53:51,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:52,001][root][INFO] - Training Epoch: 2/2, step 2740/7134 completed (loss: 0.023818708956241608, acc: 0.9927431344985962)
[2025-02-13 03:53:52,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:52,375][root][INFO] - Training Epoch: 2/2, step 2741/7134 completed (loss: 0.061704155057668686, acc: 0.9813084006309509)
[2025-02-13 03:53:52,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:52,815][root][INFO] - Training Epoch: 2/2, step 2742/7134 completed (loss: 0.01521722786128521, acc: 0.9979166388511658)
[2025-02-13 03:53:52,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:53,199][root][INFO] - Training Epoch: 2/2, step 2743/7134 completed (loss: 0.010846235789358616, acc: 0.9973683953285217)
[2025-02-13 03:53:53,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:53,441][root][INFO] - Training Epoch: 2/2, step 2744/7134 completed (loss: 0.017837103456258774, acc: 0.9961685538291931)
[2025-02-13 03:53:53,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:53,795][root][INFO] - Training Epoch: 2/2, step 2745/7134 completed (loss: 0.0824810117483139, acc: 0.9773519039154053)
[2025-02-13 03:53:53,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:54,158][root][INFO] - Training Epoch: 2/2, step 2746/7134 completed (loss: 0.040470052510499954, acc: 0.9871794581413269)
[2025-02-13 03:53:54,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:54,546][root][INFO] - Training Epoch: 2/2, step 2747/7134 completed (loss: 0.025980794802308083, acc: 0.9926289916038513)
[2025-02-13 03:53:54,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:54,809][root][INFO] - Training Epoch: 2/2, step 2748/7134 completed (loss: 0.011807572096586227, acc: 0.9968152642250061)
[2025-02-13 03:53:54,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:55,117][root][INFO] - Training Epoch: 2/2, step 2749/7134 completed (loss: 0.05244733393192291, acc: 0.9890710115432739)
[2025-02-13 03:53:55,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:55,516][root][INFO] - Training Epoch: 2/2, step 2750/7134 completed (loss: 0.02279592491686344, acc: 0.9894737005233765)
[2025-02-13 03:53:55,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:55,892][root][INFO] - Training Epoch: 2/2, step 2751/7134 completed (loss: 0.053446222096681595, acc: 0.9848101139068604)
[2025-02-13 03:53:56,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:56,290][root][INFO] - Training Epoch: 2/2, step 2752/7134 completed (loss: 0.041860539466142654, acc: 0.9885931611061096)
[2025-02-13 03:53:56,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:56,687][root][INFO] - Training Epoch: 2/2, step 2753/7134 completed (loss: 0.06459025293588638, acc: 0.9774436354637146)
[2025-02-13 03:53:56,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:57,126][root][INFO] - Training Epoch: 2/2, step 2754/7134 completed (loss: 0.05729822814464569, acc: 0.9797160029411316)
[2025-02-13 03:53:57,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:57,531][root][INFO] - Training Epoch: 2/2, step 2755/7134 completed (loss: 0.03271203115582466, acc: 0.9961240291595459)
[2025-02-13 03:53:57,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:57,895][root][INFO] - Training Epoch: 2/2, step 2756/7134 completed (loss: 0.057639122009277344, acc: 0.9809523820877075)
[2025-02-13 03:53:57,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:58,169][root][INFO] - Training Epoch: 2/2, step 2757/7134 completed (loss: 0.07680615782737732, acc: 0.9808743000030518)
[2025-02-13 03:53:58,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:58,556][root][INFO] - Training Epoch: 2/2, step 2758/7134 completed (loss: 0.05450990051031113, acc: 0.9817351698875427)
[2025-02-13 03:53:58,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:58,895][root][INFO] - Training Epoch: 2/2, step 2759/7134 completed (loss: 0.02951887995004654, acc: 0.9866220951080322)
[2025-02-13 03:53:59,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:59,253][root][INFO] - Training Epoch: 2/2, step 2760/7134 completed (loss: 0.03293779492378235, acc: 0.9873015880584717)
[2025-02-13 03:53:59,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:59,664][root][INFO] - Training Epoch: 2/2, step 2761/7134 completed (loss: 0.08955646306276321, acc: 0.9760100841522217)
[2025-02-13 03:53:59,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:00,129][root][INFO] - Training Epoch: 2/2, step 2762/7134 completed (loss: 0.04921220615506172, acc: 0.9850402474403381)
[2025-02-13 03:54:00,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:00,564][root][INFO] - Training Epoch: 2/2, step 2763/7134 completed (loss: 0.05167756602168083, acc: 0.9861809015274048)
[2025-02-13 03:54:00,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:01,000][root][INFO] - Training Epoch: 2/2, step 2764/7134 completed (loss: 0.0917455404996872, acc: 0.9736495614051819)
[2025-02-13 03:54:01,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:01,439][root][INFO] - Training Epoch: 2/2, step 2765/7134 completed (loss: 0.03812741860747337, acc: 0.9928160905838013)
[2025-02-13 03:54:01,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:01,893][root][INFO] - Training Epoch: 2/2, step 2766/7134 completed (loss: 0.019660064950585365, acc: 0.993537962436676)
[2025-02-13 03:54:02,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:02,333][root][INFO] - Training Epoch: 2/2, step 2767/7134 completed (loss: 0.08318225294351578, acc: 0.9790123701095581)
[2025-02-13 03:54:02,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:02,758][root][INFO] - Training Epoch: 2/2, step 2768/7134 completed (loss: 0.05772676318883896, acc: 0.9801980257034302)
[2025-02-13 03:54:02,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:03,202][root][INFO] - Training Epoch: 2/2, step 2769/7134 completed (loss: 0.04722411185503006, acc: 0.9876126050949097)
[2025-02-13 03:54:03,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:03,657][root][INFO] - Training Epoch: 2/2, step 2770/7134 completed (loss: 0.02310759201645851, acc: 0.9930459260940552)
[2025-02-13 03:54:03,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:04,154][root][INFO] - Training Epoch: 2/2, step 2771/7134 completed (loss: 0.035337645560503006, acc: 0.9914255142211914)
[2025-02-13 03:54:04,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:04,618][root][INFO] - Training Epoch: 2/2, step 2772/7134 completed (loss: 0.028854455798864365, acc: 0.991051435470581)
[2025-02-13 03:54:04,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:05,081][root][INFO] - Training Epoch: 2/2, step 2773/7134 completed (loss: 0.022304236888885498, acc: 0.99314284324646)
[2025-02-13 03:54:05,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:05,533][root][INFO] - Training Epoch: 2/2, step 2774/7134 completed (loss: 0.06135312467813492, acc: 0.9824561476707458)
[2025-02-13 03:54:05,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:05,967][root][INFO] - Training Epoch: 2/2, step 2775/7134 completed (loss: 0.04062287509441376, acc: 0.9926380515098572)
[2025-02-13 03:54:06,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:06,455][root][INFO] - Training Epoch: 2/2, step 2776/7134 completed (loss: 0.026158533990383148, acc: 0.9907975196838379)
[2025-02-13 03:54:06,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:06,892][root][INFO] - Training Epoch: 2/2, step 2777/7134 completed (loss: 0.02599087916314602, acc: 0.9902439117431641)
[2025-02-13 03:54:07,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:07,343][root][INFO] - Training Epoch: 2/2, step 2778/7134 completed (loss: 0.03539992868900299, acc: 0.9918887615203857)
[2025-02-13 03:54:07,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:07,794][root][INFO] - Training Epoch: 2/2, step 2779/7134 completed (loss: 0.056844018399715424, acc: 0.9852320551872253)
[2025-02-13 03:54:07,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:08,249][root][INFO] - Training Epoch: 2/2, step 2780/7134 completed (loss: 0.056446537375450134, acc: 0.9873417615890503)
[2025-02-13 03:54:08,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:08,716][root][INFO] - Training Epoch: 2/2, step 2781/7134 completed (loss: 0.02572731114923954, acc: 0.9922279715538025)
[2025-02-13 03:54:08,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:09,172][root][INFO] - Training Epoch: 2/2, step 2782/7134 completed (loss: 0.035200826823711395, acc: 0.9905462265014648)
[2025-02-13 03:54:09,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:09,605][root][INFO] - Training Epoch: 2/2, step 2783/7134 completed (loss: 0.021385548636317253, acc: 0.9924924969673157)
[2025-02-13 03:54:09,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:10,061][root][INFO] - Training Epoch: 2/2, step 2784/7134 completed (loss: 0.029557395726442337, acc: 0.9947423934936523)
[2025-02-13 03:54:10,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:10,532][root][INFO] - Training Epoch: 2/2, step 2785/7134 completed (loss: 0.030005749315023422, acc: 0.9934498071670532)
[2025-02-13 03:54:10,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:10,992][root][INFO] - Training Epoch: 2/2, step 2786/7134 completed (loss: 0.03292921185493469, acc: 0.9906542301177979)
[2025-02-13 03:54:11,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:11,468][root][INFO] - Training Epoch: 2/2, step 2787/7134 completed (loss: 0.018851907923817635, acc: 0.9973439574241638)
[2025-02-13 03:54:11,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:11,903][root][INFO] - Training Epoch: 2/2, step 2788/7134 completed (loss: 0.029908617958426476, acc: 0.9882199168205261)
[2025-02-13 03:54:12,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:12,274][root][INFO] - Training Epoch: 2/2, step 2789/7134 completed (loss: 0.1327320784330368, acc: 0.9682996869087219)
[2025-02-13 03:54:12,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:12,653][root][INFO] - Training Epoch: 2/2, step 2790/7134 completed (loss: 0.13548868894577026, acc: 0.9668874144554138)
[2025-02-13 03:54:12,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:13,094][root][INFO] - Training Epoch: 2/2, step 2791/7134 completed (loss: 0.035780467092990875, acc: 0.9910314083099365)
[2025-02-13 03:54:13,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:13,554][root][INFO] - Training Epoch: 2/2, step 2792/7134 completed (loss: 0.04177715256810188, acc: 0.9918509721755981)
[2025-02-13 03:54:13,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:14,003][root][INFO] - Training Epoch: 2/2, step 2793/7134 completed (loss: 0.046449869871139526, acc: 0.9905533194541931)
[2025-02-13 03:54:14,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:14,342][root][INFO] - Training Epoch: 2/2, step 2794/7134 completed (loss: 0.033490054309368134, acc: 0.990227997303009)
[2025-02-13 03:54:14,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:14,755][root][INFO] - Training Epoch: 2/2, step 2795/7134 completed (loss: 0.11372213810682297, acc: 0.9698376059532166)
[2025-02-13 03:54:14,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:15,110][root][INFO] - Training Epoch: 2/2, step 2796/7134 completed (loss: 0.14604352414608002, acc: 0.9631336331367493)
[2025-02-13 03:54:15,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:15,365][root][INFO] - Training Epoch: 2/2, step 2797/7134 completed (loss: 0.0432627908885479, acc: 0.9873417615890503)
[2025-02-13 03:54:15,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:15,764][root][INFO] - Training Epoch: 2/2, step 2798/7134 completed (loss: 0.07653210312128067, acc: 0.970588207244873)
[2025-02-13 03:54:15,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:16,138][root][INFO] - Training Epoch: 2/2, step 2799/7134 completed (loss: 0.0785132497549057, acc: 0.9754816293716431)
[2025-02-13 03:54:16,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:16,497][root][INFO] - Training Epoch: 2/2, step 2800/7134 completed (loss: 0.08750587701797485, acc: 0.9771528840065002)
[2025-02-13 03:54:16,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:16,945][root][INFO] - Training Epoch: 2/2, step 2801/7134 completed (loss: 0.04174444079399109, acc: 0.9923273921012878)
[2025-02-13 03:54:17,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:17,398][root][INFO] - Training Epoch: 2/2, step 2802/7134 completed (loss: 0.047722358256578445, acc: 0.9875156283378601)
[2025-02-13 03:54:17,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:17,833][root][INFO] - Training Epoch: 2/2, step 2803/7134 completed (loss: 0.014575170353055, acc: 0.9957567453384399)
[2025-02-13 03:54:17,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:18,304][root][INFO] - Training Epoch: 2/2, step 2804/7134 completed (loss: 0.03283156827092171, acc: 0.9962871074676514)
[2025-02-13 03:54:18,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:18,722][root][INFO] - Training Epoch: 2/2, step 2805/7134 completed (loss: 0.022779542952775955, acc: 0.9960474371910095)
[2025-02-13 03:54:18,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:19,129][root][INFO] - Training Epoch: 2/2, step 2806/7134 completed (loss: 0.042487043887376785, acc: 0.9852458834648132)
[2025-02-13 03:54:19,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:19,523][root][INFO] - Training Epoch: 2/2, step 2807/7134 completed (loss: 0.05725081264972687, acc: 0.9896050095558167)
[2025-02-13 03:54:19,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:19,962][root][INFO] - Training Epoch: 2/2, step 2808/7134 completed (loss: 0.019643539562821388, acc: 0.99314284324646)
[2025-02-13 03:54:20,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:20,367][root][INFO] - Training Epoch: 2/2, step 2809/7134 completed (loss: 0.02714666724205017, acc: 0.9928698539733887)
[2025-02-13 03:54:20,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:20,858][root][INFO] - Training Epoch: 2/2, step 2810/7134 completed (loss: 0.03031938523054123, acc: 0.9916864633560181)
[2025-02-13 03:54:20,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:21,302][root][INFO] - Training Epoch: 2/2, step 2811/7134 completed (loss: 0.014232128858566284, acc: 0.9952267408370972)
[2025-02-13 03:54:21,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:21,746][root][INFO] - Training Epoch: 2/2, step 2812/7134 completed (loss: 0.022654810920357704, acc: 0.9891975522041321)
[2025-02-13 03:54:21,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:22,196][root][INFO] - Training Epoch: 2/2, step 2813/7134 completed (loss: 0.013315442018210888, acc: 0.996221661567688)
[2025-02-13 03:54:22,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:22,606][root][INFO] - Training Epoch: 2/2, step 2814/7134 completed (loss: 0.019471311941742897, acc: 0.9974026083946228)
[2025-02-13 03:54:22,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:23,048][root][INFO] - Training Epoch: 2/2, step 2815/7134 completed (loss: 0.047347258776426315, acc: 0.9881376028060913)
[2025-02-13 03:54:23,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:23,525][root][INFO] - Training Epoch: 2/2, step 2816/7134 completed (loss: 0.043889086693525314, acc: 0.9894598126411438)
[2025-02-13 03:54:23,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:24,009][root][INFO] - Training Epoch: 2/2, step 2817/7134 completed (loss: 0.04580867663025856, acc: 0.9884696006774902)
[2025-02-13 03:54:24,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:24,463][root][INFO] - Training Epoch: 2/2, step 2818/7134 completed (loss: 0.0343533493578434, acc: 0.9879385828971863)
[2025-02-13 03:54:24,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:24,860][root][INFO] - Training Epoch: 2/2, step 2819/7134 completed (loss: 0.00847594067454338, acc: 0.9983844757080078)
[2025-02-13 03:54:24,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:25,287][root][INFO] - Training Epoch: 2/2, step 2820/7134 completed (loss: 0.08892226964235306, acc: 0.969072163105011)
[2025-02-13 03:54:25,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:25,718][root][INFO] - Training Epoch: 2/2, step 2821/7134 completed (loss: 0.04517833888530731, acc: 0.9820788502693176)
[2025-02-13 03:54:25,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:26,158][root][INFO] - Training Epoch: 2/2, step 2822/7134 completed (loss: 0.05488581210374832, acc: 0.9797724485397339)
[2025-02-13 03:54:26,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:26,643][root][INFO] - Training Epoch: 2/2, step 2823/7134 completed (loss: 0.06418059021234512, acc: 0.978723406791687)
[2025-02-13 03:54:26,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:27,064][root][INFO] - Training Epoch: 2/2, step 2824/7134 completed (loss: 0.09108205139636993, acc: 0.975649356842041)
[2025-02-13 03:54:27,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:27,495][root][INFO] - Training Epoch: 2/2, step 2825/7134 completed (loss: 0.06447159498929977, acc: 0.9865771532058716)
[2025-02-13 03:54:27,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:27,931][root][INFO] - Training Epoch: 2/2, step 2826/7134 completed (loss: 0.0648324117064476, acc: 0.9819276928901672)
[2025-02-13 03:54:28,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:28,370][root][INFO] - Training Epoch: 2/2, step 2827/7134 completed (loss: 0.04729995131492615, acc: 0.9859514832496643)
[2025-02-13 03:54:28,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:28,811][root][INFO] - Training Epoch: 2/2, step 2828/7134 completed (loss: 0.03703088313341141, acc: 0.9887999892234802)
[2025-02-13 03:54:28,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:29,253][root][INFO] - Training Epoch: 2/2, step 2829/7134 completed (loss: 0.06989678740501404, acc: 0.9836289286613464)
[2025-02-13 03:54:29,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:29,687][root][INFO] - Training Epoch: 2/2, step 2830/7134 completed (loss: 0.01686488464474678, acc: 0.9963570237159729)
[2025-02-13 03:54:29,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:30,104][root][INFO] - Training Epoch: 2/2, step 2831/7134 completed (loss: 0.026481926441192627, acc: 0.9921011328697205)
[2025-02-13 03:54:30,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:30,528][root][INFO] - Training Epoch: 2/2, step 2832/7134 completed (loss: 0.05065318942070007, acc: 0.9903846383094788)
[2025-02-13 03:54:30,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:30,972][root][INFO] - Training Epoch: 2/2, step 2833/7134 completed (loss: 0.04905217885971069, acc: 0.9906432628631592)
[2025-02-13 03:54:31,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:31,416][root][INFO] - Training Epoch: 2/2, step 2834/7134 completed (loss: 0.0697355642914772, acc: 0.9809644818305969)
[2025-02-13 03:54:31,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:31,852][root][INFO] - Training Epoch: 2/2, step 2835/7134 completed (loss: 0.05190246179699898, acc: 0.9838449358940125)
[2025-02-13 03:54:31,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:32,245][root][INFO] - Training Epoch: 2/2, step 2836/7134 completed (loss: 0.05061083287000656, acc: 0.9857819676399231)
[2025-02-13 03:54:32,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:32,688][root][INFO] - Training Epoch: 2/2, step 2837/7134 completed (loss: 0.023659437894821167, acc: 0.9891892075538635)
[2025-02-13 03:54:32,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:33,175][root][INFO] - Training Epoch: 2/2, step 2838/7134 completed (loss: 0.07415252178907394, acc: 0.9769874215126038)
[2025-02-13 03:54:33,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:33,602][root][INFO] - Training Epoch: 2/2, step 2839/7134 completed (loss: 0.037977106869220734, acc: 0.9924127459526062)
[2025-02-13 03:54:33,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:34,066][root][INFO] - Training Epoch: 2/2, step 2840/7134 completed (loss: 0.03321968391537666, acc: 0.988950252532959)
[2025-02-13 03:54:34,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:34,501][root][INFO] - Training Epoch: 2/2, step 2841/7134 completed (loss: 0.04480509087443352, acc: 0.9861751198768616)
[2025-02-13 03:54:34,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:34,958][root][INFO] - Training Epoch: 2/2, step 2842/7134 completed (loss: 0.035430941730737686, acc: 0.9896103739738464)
[2025-02-13 03:54:35,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:35,368][root][INFO] - Training Epoch: 2/2, step 2843/7134 completed (loss: 0.027292650192975998, acc: 0.9926470518112183)
[2025-02-13 03:54:35,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:35,817][root][INFO] - Training Epoch: 2/2, step 2844/7134 completed (loss: 0.10564716160297394, acc: 0.9671682715415955)
[2025-02-13 03:54:35,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:36,225][root][INFO] - Training Epoch: 2/2, step 2845/7134 completed (loss: 0.03755231574177742, acc: 0.9913344979286194)
[2025-02-13 03:54:36,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:36,641][root][INFO] - Training Epoch: 2/2, step 2846/7134 completed (loss: 0.0699518471956253, acc: 0.9818781018257141)
[2025-02-13 03:54:36,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:37,090][root][INFO] - Training Epoch: 2/2, step 2847/7134 completed (loss: 0.051998794078826904, acc: 0.9868035316467285)
[2025-02-13 03:54:37,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:37,419][root][INFO] - Training Epoch: 2/2, step 2848/7134 completed (loss: 0.035831477493047714, acc: 0.9906759858131409)
[2025-02-13 03:54:37,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:37,848][root][INFO] - Training Epoch: 2/2, step 2849/7134 completed (loss: 0.05482644960284233, acc: 0.9903181195259094)
[2025-02-13 03:54:37,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:38,252][root][INFO] - Training Epoch: 2/2, step 2850/7134 completed (loss: 0.026995884254574776, acc: 0.9913978576660156)
[2025-02-13 03:54:38,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:38,663][root][INFO] - Training Epoch: 2/2, step 2851/7134 completed (loss: 0.02499459870159626, acc: 0.988950252532959)
[2025-02-13 03:54:38,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:39,054][root][INFO] - Training Epoch: 2/2, step 2852/7134 completed (loss: 0.020721422508358955, acc: 0.9940357804298401)
[2025-02-13 03:54:39,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:39,398][root][INFO] - Training Epoch: 2/2, step 2853/7134 completed (loss: 0.03116408921778202, acc: 0.9923858046531677)
[2025-02-13 03:54:39,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:39,820][root][INFO] - Training Epoch: 2/2, step 2854/7134 completed (loss: 0.04259320721030235, acc: 0.9851411581039429)
[2025-02-13 03:54:39,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:40,220][root][INFO] - Training Epoch: 2/2, step 2855/7134 completed (loss: 0.04095054045319557, acc: 0.9919224381446838)
[2025-02-13 03:54:40,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:40,670][root][INFO] - Training Epoch: 2/2, step 2856/7134 completed (loss: 0.011186149902641773, acc: 0.9971791505813599)
[2025-02-13 03:54:40,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:41,100][root][INFO] - Training Epoch: 2/2, step 2857/7134 completed (loss: 0.052179910242557526, acc: 0.986994206905365)
[2025-02-13 03:54:41,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:41,489][root][INFO] - Training Epoch: 2/2, step 2858/7134 completed (loss: 0.01255632471293211, acc: 0.9968454241752625)
[2025-02-13 03:54:41,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:41,912][root][INFO] - Training Epoch: 2/2, step 2859/7134 completed (loss: 0.04400545731186867, acc: 0.9818689227104187)
[2025-02-13 03:54:42,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:42,312][root][INFO] - Training Epoch: 2/2, step 2860/7134 completed (loss: 0.028431329876184464, acc: 0.988990843296051)
[2025-02-13 03:54:42,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:42,714][root][INFO] - Training Epoch: 2/2, step 2861/7134 completed (loss: 0.015385388396680355, acc: 0.9959431886672974)
[2025-02-13 03:54:42,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:43,111][root][INFO] - Training Epoch: 2/2, step 2862/7134 completed (loss: 0.016278492286801338, acc: 0.994854211807251)
[2025-02-13 03:54:43,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:43,558][root][INFO] - Training Epoch: 2/2, step 2863/7134 completed (loss: 0.011801887303590775, acc: 0.9949044585227966)
[2025-02-13 03:54:43,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:43,880][root][INFO] - Training Epoch: 2/2, step 2864/7134 completed (loss: 0.039775315672159195, acc: 0.9922077655792236)
[2025-02-13 03:54:44,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:44,295][root][INFO] - Training Epoch: 2/2, step 2865/7134 completed (loss: 0.022994086146354675, acc: 0.991304337978363)
[2025-02-13 03:54:44,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:44,699][root][INFO] - Training Epoch: 2/2, step 2866/7134 completed (loss: 0.02647007629275322, acc: 0.9925261735916138)
[2025-02-13 03:54:44,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:45,037][root][INFO] - Training Epoch: 2/2, step 2867/7134 completed (loss: 0.02634822204709053, acc: 0.9907975196838379)
[2025-02-13 03:54:45,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:45,427][root][INFO] - Training Epoch: 2/2, step 2868/7134 completed (loss: 0.007943409495055676, acc: 0.9971910119056702)
[2025-02-13 03:54:45,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:45,812][root][INFO] - Training Epoch: 2/2, step 2869/7134 completed (loss: 0.04769390448927879, acc: 0.9941434860229492)
[2025-02-13 03:54:45,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:46,225][root][INFO] - Training Epoch: 2/2, step 2870/7134 completed (loss: 0.057502295821905136, acc: 0.9874213933944702)
[2025-02-13 03:54:46,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:46,609][root][INFO] - Training Epoch: 2/2, step 2871/7134 completed (loss: 0.057461291551589966, acc: 0.9890795350074768)
[2025-02-13 03:54:46,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:47,022][root][INFO] - Training Epoch: 2/2, step 2872/7134 completed (loss: 0.004773324821144342, acc: 1.0)
[2025-02-13 03:54:47,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:47,425][root][INFO] - Training Epoch: 2/2, step 2873/7134 completed (loss: 0.022442635148763657, acc: 0.9927007555961609)
[2025-02-13 03:54:47,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:47,841][root][INFO] - Training Epoch: 2/2, step 2874/7134 completed (loss: 0.02442236803472042, acc: 0.9921996593475342)
[2025-02-13 03:54:47,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:48,266][root][INFO] - Training Epoch: 2/2, step 2875/7134 completed (loss: 0.030142461881041527, acc: 0.9900426864624023)
[2025-02-13 03:54:48,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:48,662][root][INFO] - Training Epoch: 2/2, step 2876/7134 completed (loss: 0.09620974212884903, acc: 0.981566846370697)
[2025-02-13 03:54:48,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:49,066][root][INFO] - Training Epoch: 2/2, step 2877/7134 completed (loss: 0.02210315130650997, acc: 0.9912663698196411)
[2025-02-13 03:54:49,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:49,456][root][INFO] - Training Epoch: 2/2, step 2878/7134 completed (loss: 0.024581650272011757, acc: 0.9957627058029175)
[2025-02-13 03:54:49,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:49,846][root][INFO] - Training Epoch: 2/2, step 2879/7134 completed (loss: 0.011296587064862251, acc: 0.9968652129173279)
[2025-02-13 03:54:49,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:50,259][root][INFO] - Training Epoch: 2/2, step 2880/7134 completed (loss: 0.013557511381804943, acc: 0.996874988079071)
[2025-02-13 03:54:50,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:50,635][root][INFO] - Training Epoch: 2/2, step 2881/7134 completed (loss: 0.027777383103966713, acc: 0.989266574382782)
[2025-02-13 03:54:50,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:51,068][root][INFO] - Training Epoch: 2/2, step 2882/7134 completed (loss: 0.0557234100997448, acc: 0.9839704036712646)
[2025-02-13 03:54:51,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:51,483][root][INFO] - Training Epoch: 2/2, step 2883/7134 completed (loss: 0.039389029145240784, acc: 0.9894179701805115)
[2025-02-13 03:54:51,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:51,879][root][INFO] - Training Epoch: 2/2, step 2884/7134 completed (loss: 0.08444567769765854, acc: 0.9713423848152161)
[2025-02-13 03:54:52,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:52,271][root][INFO] - Training Epoch: 2/2, step 2885/7134 completed (loss: 0.035258036106824875, acc: 0.987860381603241)
[2025-02-13 03:54:52,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:52,665][root][INFO] - Training Epoch: 2/2, step 2886/7134 completed (loss: 0.08198655396699905, acc: 0.9805970191955566)
[2025-02-13 03:54:52,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:53,115][root][INFO] - Training Epoch: 2/2, step 2887/7134 completed (loss: 0.049063652753829956, acc: 0.9869822263717651)
[2025-02-13 03:54:53,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:53,557][root][INFO] - Training Epoch: 2/2, step 2888/7134 completed (loss: 0.06406628340482712, acc: 0.9805825352668762)
[2025-02-13 03:54:53,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:53,978][root][INFO] - Training Epoch: 2/2, step 2889/7134 completed (loss: 0.016542909666895866, acc: 0.9930747747421265)
[2025-02-13 03:54:54,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:54,414][root][INFO] - Training Epoch: 2/2, step 2890/7134 completed (loss: 0.02448783442378044, acc: 0.9960052967071533)
[2025-02-13 03:54:54,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:54,850][root][INFO] - Training Epoch: 2/2, step 2891/7134 completed (loss: 0.045006826519966125, acc: 0.9907692074775696)
[2025-02-13 03:54:54,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:55,290][root][INFO] - Training Epoch: 2/2, step 2892/7134 completed (loss: 0.08235421031713486, acc: 0.9811083078384399)
[2025-02-13 03:54:55,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:55,730][root][INFO] - Training Epoch: 2/2, step 2893/7134 completed (loss: 0.05010959878563881, acc: 0.9861286282539368)
[2025-02-13 03:54:55,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:56,144][root][INFO] - Training Epoch: 2/2, step 2894/7134 completed (loss: 0.032989028841257095, acc: 0.9865546226501465)
[2025-02-13 03:54:56,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:56,523][root][INFO] - Training Epoch: 2/2, step 2895/7134 completed (loss: 0.021531395614147186, acc: 0.992438554763794)
[2025-02-13 03:54:56,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:56,958][root][INFO] - Training Epoch: 2/2, step 2896/7134 completed (loss: 0.027741318568587303, acc: 0.9898219108581543)
[2025-02-13 03:54:57,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:57,353][root][INFO] - Training Epoch: 2/2, step 2897/7134 completed (loss: 0.03883033245801926, acc: 0.9875665903091431)
[2025-02-13 03:54:57,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:57,766][root][INFO] - Training Epoch: 2/2, step 2898/7134 completed (loss: 0.03312660753726959, acc: 0.9921568632125854)
[2025-02-13 03:54:57,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:58,175][root][INFO] - Training Epoch: 2/2, step 2899/7134 completed (loss: 0.015749521553516388, acc: 0.9956395626068115)
[2025-02-13 03:54:58,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:58,613][root][INFO] - Training Epoch: 2/2, step 2900/7134 completed (loss: 0.032545220106840134, acc: 0.9938271641731262)
[2025-02-13 03:54:58,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:59,024][root][INFO] - Training Epoch: 2/2, step 2901/7134 completed (loss: 0.0313129648566246, acc: 0.9888535141944885)
[2025-02-13 03:54:59,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:59,372][root][INFO] - Training Epoch: 2/2, step 2902/7134 completed (loss: 0.03314695507287979, acc: 0.9868420958518982)
[2025-02-13 03:54:59,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:59,809][root][INFO] - Training Epoch: 2/2, step 2903/7134 completed (loss: 0.05293814837932587, acc: 0.9851552248001099)
[2025-02-13 03:54:59,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:00,247][root][INFO] - Training Epoch: 2/2, step 2904/7134 completed (loss: 0.016396453604102135, acc: 0.995006263256073)
[2025-02-13 03:55:00,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:00,683][root][INFO] - Training Epoch: 2/2, step 2905/7134 completed (loss: 0.033666908740997314, acc: 0.987484335899353)
[2025-02-13 03:55:00,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:01,124][root][INFO] - Training Epoch: 2/2, step 2906/7134 completed (loss: 0.005504396744072437, acc: 0.9988024234771729)
[2025-02-13 03:55:01,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:01,548][root][INFO] - Training Epoch: 2/2, step 2907/7134 completed (loss: 0.047351039946079254, acc: 0.9878869652748108)
[2025-02-13 03:55:01,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:01,980][root][INFO] - Training Epoch: 2/2, step 2908/7134 completed (loss: 0.023960387334227562, acc: 0.9948253631591797)
[2025-02-13 03:55:02,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:02,424][root][INFO] - Training Epoch: 2/2, step 2909/7134 completed (loss: 0.028403112664818764, acc: 0.9880095720291138)
[2025-02-13 03:55:02,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:02,870][root][INFO] - Training Epoch: 2/2, step 2910/7134 completed (loss: 0.06762062013149261, acc: 0.9828693866729736)
[2025-02-13 03:55:03,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:03,339][root][INFO] - Training Epoch: 2/2, step 2911/7134 completed (loss: 0.012936117127537727, acc: 0.9973154067993164)
[2025-02-13 03:55:03,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:03,805][root][INFO] - Training Epoch: 2/2, step 2912/7134 completed (loss: 0.026188591495156288, acc: 0.9903448224067688)
[2025-02-13 03:55:03,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:04,257][root][INFO] - Training Epoch: 2/2, step 2913/7134 completed (loss: 0.028628794476389885, acc: 0.9914529919624329)
[2025-02-13 03:55:04,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:04,721][root][INFO] - Training Epoch: 2/2, step 2914/7134 completed (loss: 0.022550005465745926, acc: 0.9975874423980713)
[2025-02-13 03:55:04,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:05,144][root][INFO] - Training Epoch: 2/2, step 2915/7134 completed (loss: 0.06628821045160294, acc: 0.9827089309692383)
[2025-02-13 03:55:05,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:05,545][root][INFO] - Training Epoch: 2/2, step 2916/7134 completed (loss: 0.05216759815812111, acc: 0.9845361113548279)
[2025-02-13 03:55:05,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:05,979][root][INFO] - Training Epoch: 2/2, step 2917/7134 completed (loss: 0.0331612303853035, acc: 0.9899497628211975)
[2025-02-13 03:55:06,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:06,411][root][INFO] - Training Epoch: 2/2, step 2918/7134 completed (loss: 0.018807316198945045, acc: 0.9918414950370789)
[2025-02-13 03:55:06,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:06,810][root][INFO] - Training Epoch: 2/2, step 2919/7134 completed (loss: 0.06335386633872986, acc: 0.9894242286682129)
[2025-02-13 03:55:06,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:07,290][root][INFO] - Training Epoch: 2/2, step 2920/7134 completed (loss: 0.019629361107945442, acc: 0.9920529723167419)
[2025-02-13 03:55:07,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:07,724][root][INFO] - Training Epoch: 2/2, step 2921/7134 completed (loss: 0.014059878885746002, acc: 0.9913793206214905)
[2025-02-13 03:55:07,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:08,135][root][INFO] - Training Epoch: 2/2, step 2922/7134 completed (loss: 0.06838195770978928, acc: 0.9886934757232666)
[2025-02-13 03:55:08,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:08,573][root][INFO] - Training Epoch: 2/2, step 2923/7134 completed (loss: 0.15568199753761292, acc: 0.9738863110542297)
[2025-02-13 03:55:08,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:09,001][root][INFO] - Training Epoch: 2/2, step 2924/7134 completed (loss: 0.014896820299327374, acc: 0.9964622855186462)
[2025-02-13 03:55:09,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:09,485][root][INFO] - Training Epoch: 2/2, step 2925/7134 completed (loss: 0.03980395570397377, acc: 0.9901719689369202)
[2025-02-13 03:55:09,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:09,949][root][INFO] - Training Epoch: 2/2, step 2926/7134 completed (loss: 0.14247013628482819, acc: 0.9759615659713745)
[2025-02-13 03:55:10,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:10,391][root][INFO] - Training Epoch: 2/2, step 2927/7134 completed (loss: 0.019845981150865555, acc: 0.9927448630332947)
[2025-02-13 03:55:10,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:10,825][root][INFO] - Training Epoch: 2/2, step 2928/7134 completed (loss: 0.02762318216264248, acc: 0.9939024448394775)
[2025-02-13 03:55:10,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:11,289][root][INFO] - Training Epoch: 2/2, step 2929/7134 completed (loss: 0.026740383356809616, acc: 0.9947780966758728)
[2025-02-13 03:55:11,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:11,741][root][INFO] - Training Epoch: 2/2, step 2930/7134 completed (loss: 0.04400155320763588, acc: 0.988252580165863)
[2025-02-13 03:55:11,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:12,154][root][INFO] - Training Epoch: 2/2, step 2931/7134 completed (loss: 0.04976360872387886, acc: 0.9850522875785828)
[2025-02-13 03:55:12,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:12,577][root][INFO] - Training Epoch: 2/2, step 2932/7134 completed (loss: 0.024101395159959793, acc: 0.9930475354194641)
[2025-02-13 03:55:12,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:13,015][root][INFO] - Training Epoch: 2/2, step 2933/7134 completed (loss: 0.028291206806898117, acc: 0.9870129823684692)
[2025-02-13 03:55:13,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:13,466][root][INFO] - Training Epoch: 2/2, step 2934/7134 completed (loss: 0.015464809723198414, acc: 0.9971791505813599)
[2025-02-13 03:55:13,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:13,903][root][INFO] - Training Epoch: 2/2, step 2935/7134 completed (loss: 0.02389497309923172, acc: 0.9944933652877808)
[2025-02-13 03:55:14,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:14,371][root][INFO] - Training Epoch: 2/2, step 2936/7134 completed (loss: 0.03914141282439232, acc: 0.9888097643852234)
[2025-02-13 03:55:14,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:14,777][root][INFO] - Training Epoch: 2/2, step 2937/7134 completed (loss: 0.044260647147893906, acc: 0.9885222315788269)
[2025-02-13 03:55:14,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:15,198][root][INFO] - Training Epoch: 2/2, step 2938/7134 completed (loss: 0.025033991783857346, acc: 0.994413435459137)
[2025-02-13 03:55:15,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:15,548][root][INFO] - Training Epoch: 2/2, step 2939/7134 completed (loss: 0.02451523207128048, acc: 0.9919871687889099)
[2025-02-13 03:55:15,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:15,966][root][INFO] - Training Epoch: 2/2, step 2940/7134 completed (loss: 0.04901963099837303, acc: 0.9890561103820801)
[2025-02-13 03:55:16,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:16,359][root][INFO] - Training Epoch: 2/2, step 2941/7134 completed (loss: 0.023315690457820892, acc: 0.9912917017936707)
[2025-02-13 03:55:16,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:16,738][root][INFO] - Training Epoch: 2/2, step 2942/7134 completed (loss: 0.028350187465548515, acc: 0.9925705790519714)
[2025-02-13 03:55:16,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:17,143][root][INFO] - Training Epoch: 2/2, step 2943/7134 completed (loss: 0.014274725690484047, acc: 0.9966273307800293)
[2025-02-13 03:55:17,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:17,554][root][INFO] - Training Epoch: 2/2, step 2944/7134 completed (loss: 0.030398426577448845, acc: 0.9911894202232361)
[2025-02-13 03:55:17,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:17,975][root][INFO] - Training Epoch: 2/2, step 2945/7134 completed (loss: 0.023043034598231316, acc: 0.9906687140464783)
[2025-02-13 03:55:18,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:18,354][root][INFO] - Training Epoch: 2/2, step 2946/7134 completed (loss: 0.03353777900338173, acc: 0.992732584476471)
[2025-02-13 03:55:18,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:18,755][root][INFO] - Training Epoch: 2/2, step 2947/7134 completed (loss: 0.045114025473594666, acc: 0.9842767119407654)
[2025-02-13 03:55:18,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:19,164][root][INFO] - Training Epoch: 2/2, step 2948/7134 completed (loss: 0.047923214733600616, acc: 0.9884678721427917)
[2025-02-13 03:55:19,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:19,536][root][INFO] - Training Epoch: 2/2, step 2949/7134 completed (loss: 0.0041847992688417435, acc: 1.0)
[2025-02-13 03:55:19,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:19,946][root][INFO] - Training Epoch: 2/2, step 2950/7134 completed (loss: 0.01855994202196598, acc: 0.9944827556610107)
[2025-02-13 03:55:20,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:20,354][root][INFO] - Training Epoch: 2/2, step 2951/7134 completed (loss: 0.011147328652441502, acc: 0.9971014261245728)
[2025-02-13 03:55:20,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:20,767][root][INFO] - Training Epoch: 2/2, step 2952/7134 completed (loss: 0.012262322939932346, acc: 0.9955489635467529)
[2025-02-13 03:55:20,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:21,165][root][INFO] - Training Epoch: 2/2, step 2953/7134 completed (loss: 0.025835828855633736, acc: 0.9898648858070374)
[2025-02-13 03:55:21,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:21,597][root][INFO] - Training Epoch: 2/2, step 2954/7134 completed (loss: 0.03357116878032684, acc: 0.9909909963607788)
[2025-02-13 03:55:21,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:21,974][root][INFO] - Training Epoch: 2/2, step 2955/7134 completed (loss: 0.022928155958652496, acc: 0.9928977489471436)
[2025-02-13 03:55:22,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:22,401][root][INFO] - Training Epoch: 2/2, step 2956/7134 completed (loss: 0.015260421670973301, acc: 0.9970717430114746)
[2025-02-13 03:55:22,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:22,827][root][INFO] - Training Epoch: 2/2, step 2957/7134 completed (loss: 0.0156854260712862, acc: 0.9973261952400208)
[2025-02-13 03:55:22,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:23,229][root][INFO] - Training Epoch: 2/2, step 2958/7134 completed (loss: 0.02002168633043766, acc: 0.9948717951774597)
[2025-02-13 03:55:23,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:23,628][root][INFO] - Training Epoch: 2/2, step 2959/7134 completed (loss: 0.013039813376963139, acc: 0.9941520690917969)
[2025-02-13 03:55:23,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:24,049][root][INFO] - Training Epoch: 2/2, step 2960/7134 completed (loss: 0.020478418096899986, acc: 0.9917355179786682)
[2025-02-13 03:55:24,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:24,491][root][INFO] - Training Epoch: 2/2, step 2961/7134 completed (loss: 0.013422999531030655, acc: 0.9959183931350708)
[2025-02-13 03:55:24,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:24,899][root][INFO] - Training Epoch: 2/2, step 2962/7134 completed (loss: 0.010404241271317005, acc: 0.9948253631591797)
[2025-02-13 03:55:25,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:25,347][root][INFO] - Training Epoch: 2/2, step 2963/7134 completed (loss: 0.016320759430527687, acc: 0.9947643876075745)
[2025-02-13 03:55:25,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:25,759][root][INFO] - Training Epoch: 2/2, step 2964/7134 completed (loss: 0.015515033155679703, acc: 0.9956076145172119)
[2025-02-13 03:55:25,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:26,168][root][INFO] - Training Epoch: 2/2, step 2965/7134 completed (loss: 0.05361327528953552, acc: 0.9880596995353699)
[2025-02-13 03:55:26,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:26,582][root][INFO] - Training Epoch: 2/2, step 2966/7134 completed (loss: 0.006840860471129417, acc: 0.9978678226470947)
[2025-02-13 03:55:26,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:27,000][root][INFO] - Training Epoch: 2/2, step 2967/7134 completed (loss: 0.03816048800945282, acc: 0.9888268113136292)
[2025-02-13 03:55:27,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:27,457][root][INFO] - Training Epoch: 2/2, step 2968/7134 completed (loss: 0.11622533947229385, acc: 0.9694533944129944)
[2025-02-13 03:55:27,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:27,864][root][INFO] - Training Epoch: 2/2, step 2969/7134 completed (loss: 0.2078433334827423, acc: 0.9497041702270508)
[2025-02-13 03:55:28,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:28,282][root][INFO] - Training Epoch: 2/2, step 2970/7134 completed (loss: 0.2608550786972046, acc: 0.9498910903930664)
[2025-02-13 03:55:28,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:28,714][root][INFO] - Training Epoch: 2/2, step 2971/7134 completed (loss: 0.05343851447105408, acc: 0.9842312932014465)
[2025-02-13 03:55:28,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:29,128][root][INFO] - Training Epoch: 2/2, step 2972/7134 completed (loss: 0.02731725387275219, acc: 0.9910979270935059)
[2025-02-13 03:55:29,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:29,561][root][INFO] - Training Epoch: 2/2, step 2973/7134 completed (loss: 0.05301254987716675, acc: 0.9797468185424805)
[2025-02-13 03:55:29,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:29,998][root][INFO] - Training Epoch: 2/2, step 2974/7134 completed (loss: 0.04187890887260437, acc: 0.9857327938079834)
[2025-02-13 03:55:30,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:30,436][root][INFO] - Training Epoch: 2/2, step 2975/7134 completed (loss: 0.032510701566934586, acc: 0.988950252532959)
[2025-02-13 03:55:30,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:30,902][root][INFO] - Training Epoch: 2/2, step 2976/7134 completed (loss: 0.048870693892240524, acc: 0.9819148778915405)
[2025-02-13 03:55:31,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:31,327][root][INFO] - Training Epoch: 2/2, step 2977/7134 completed (loss: 0.04624561965465546, acc: 0.986994206905365)
[2025-02-13 03:55:31,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:31,757][root][INFO] - Training Epoch: 2/2, step 2978/7134 completed (loss: 0.0554647333920002, acc: 0.980141818523407)
[2025-02-13 03:55:31,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:32,154][root][INFO] - Training Epoch: 2/2, step 2979/7134 completed (loss: 0.017616422846913338, acc: 0.9968847632408142)
[2025-02-13 03:55:32,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:32,596][root][INFO] - Training Epoch: 2/2, step 2980/7134 completed (loss: 0.048208653926849365, acc: 0.9923567175865173)
[2025-02-13 03:55:32,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:33,027][root][INFO] - Training Epoch: 2/2, step 2981/7134 completed (loss: 0.0698646679520607, acc: 0.9823608994483948)
[2025-02-13 03:55:33,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:33,440][root][INFO] - Training Epoch: 2/2, step 2982/7134 completed (loss: 0.044894877821207047, acc: 0.9882199168205261)
[2025-02-13 03:55:33,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:33,866][root][INFO] - Training Epoch: 2/2, step 2983/7134 completed (loss: 0.0765957310795784, acc: 0.9841269850730896)
[2025-02-13 03:55:34,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:34,310][root][INFO] - Training Epoch: 2/2, step 2984/7134 completed (loss: 0.02611919865012169, acc: 0.9911949634552002)
[2025-02-13 03:55:34,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:34,758][root][INFO] - Training Epoch: 2/2, step 2985/7134 completed (loss: 0.04103504866361618, acc: 0.9880810379981995)
[2025-02-13 03:55:34,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:35,223][root][INFO] - Training Epoch: 2/2, step 2986/7134 completed (loss: 0.017049936577677727, acc: 0.9976359605789185)
[2025-02-13 03:55:35,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:35,656][root][INFO] - Training Epoch: 2/2, step 2987/7134 completed (loss: 0.028467601165175438, acc: 0.9917550086975098)
[2025-02-13 03:55:35,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:36,092][root][INFO] - Training Epoch: 2/2, step 2988/7134 completed (loss: 0.01845223270356655, acc: 0.9951338171958923)
[2025-02-13 03:55:36,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:36,533][root][INFO] - Training Epoch: 2/2, step 2989/7134 completed (loss: 0.021937331184744835, acc: 0.9921568632125854)
[2025-02-13 03:55:36,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:36,919][root][INFO] - Training Epoch: 2/2, step 2990/7134 completed (loss: 0.1307094395160675, acc: 0.9685314893722534)
[2025-02-13 03:55:37,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:37,322][root][INFO] - Training Epoch: 2/2, step 2991/7134 completed (loss: 0.1173236146569252, acc: 0.9667128920555115)
[2025-02-13 03:55:37,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:37,720][root][INFO] - Training Epoch: 2/2, step 2992/7134 completed (loss: 0.07055749744176865, acc: 0.9834087491035461)
[2025-02-13 03:55:37,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:38,175][root][INFO] - Training Epoch: 2/2, step 2993/7134 completed (loss: 0.033119603991508484, acc: 0.9926650524139404)
[2025-02-13 03:55:38,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:38,593][root][INFO] - Training Epoch: 2/2, step 2994/7134 completed (loss: 0.021565090864896774, acc: 0.9928057789802551)
[2025-02-13 03:55:38,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:39,073][root][INFO] - Training Epoch: 2/2, step 2995/7134 completed (loss: 0.02151239477097988, acc: 0.9954802393913269)
[2025-02-13 03:55:39,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:39,507][root][INFO] - Training Epoch: 2/2, step 2996/7134 completed (loss: 0.03561890870332718, acc: 0.988304078578949)
[2025-02-13 03:55:39,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:39,966][root][INFO] - Training Epoch: 2/2, step 2997/7134 completed (loss: 0.0190605279058218, acc: 0.9937629699707031)
[2025-02-13 03:55:40,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:40,361][root][INFO] - Training Epoch: 2/2, step 2998/7134 completed (loss: 0.02267286740243435, acc: 0.996363639831543)
[2025-02-13 03:55:40,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:40,840][root][INFO] - Training Epoch: 2/2, step 2999/7134 completed (loss: 0.0178697407245636, acc: 0.9971056580543518)
[2025-02-13 03:55:40,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:41,307][root][INFO] - Training Epoch: 2/2, step 3000/7134 completed (loss: 0.026847124099731445, acc: 0.9914346933364868)
[2025-02-13 03:55:41,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:41,762][root][INFO] - Training Epoch: 2/2, step 3001/7134 completed (loss: 0.017823440954089165, acc: 0.9964454770088196)
[2025-02-13 03:55:41,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:42,204][root][INFO] - Training Epoch: 2/2, step 3002/7134 completed (loss: 0.027202846482396126, acc: 0.9886621236801147)
[2025-02-13 03:55:42,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:42,665][root][INFO] - Training Epoch: 2/2, step 3003/7134 completed (loss: 0.010195757262408733, acc: 0.9976798295974731)
[2025-02-13 03:55:42,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:43,113][root][INFO] - Training Epoch: 2/2, step 3004/7134 completed (loss: 0.007981113158166409, acc: 0.9977653622627258)
[2025-02-13 03:55:43,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:43,580][root][INFO] - Training Epoch: 2/2, step 3005/7134 completed (loss: 0.0076083410531282425, acc: 0.9988597631454468)
[2025-02-13 03:55:43,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:44,040][root][INFO] - Training Epoch: 2/2, step 3006/7134 completed (loss: 0.010969932191073895, acc: 0.9936143159866333)
[2025-02-13 03:55:44,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:44,473][root][INFO] - Training Epoch: 2/2, step 3007/7134 completed (loss: 0.02109352871775627, acc: 0.9929742217063904)
[2025-02-13 03:55:44,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:44,884][root][INFO] - Training Epoch: 2/2, step 3008/7134 completed (loss: 0.03957575559616089, acc: 0.9874125719070435)
[2025-02-13 03:55:45,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:45,286][root][INFO] - Training Epoch: 2/2, step 3009/7134 completed (loss: 0.0749010220170021, acc: 0.9810126423835754)
[2025-02-13 03:55:45,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:45,730][root][INFO] - Training Epoch: 2/2, step 3010/7134 completed (loss: 0.04622160270810127, acc: 0.988950252532959)
[2025-02-13 03:55:45,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:46,161][root][INFO] - Training Epoch: 2/2, step 3011/7134 completed (loss: 0.07700272649526596, acc: 0.984308123588562)
[2025-02-13 03:55:46,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:46,593][root][INFO] - Training Epoch: 2/2, step 3012/7134 completed (loss: 0.05202885717153549, acc: 0.9873217344284058)
[2025-02-13 03:55:46,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:47,059][root][INFO] - Training Epoch: 2/2, step 3013/7134 completed (loss: 0.010421322658658028, acc: 0.9976958632469177)
[2025-02-13 03:55:47,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:47,494][root][INFO] - Training Epoch: 2/2, step 3014/7134 completed (loss: 0.03343726322054863, acc: 0.9894490242004395)
[2025-02-13 03:55:47,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:47,890][root][INFO] - Training Epoch: 2/2, step 3015/7134 completed (loss: 0.029090825468301773, acc: 0.994490385055542)
[2025-02-13 03:55:48,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:48,331][root][INFO] - Training Epoch: 2/2, step 3016/7134 completed (loss: 0.014087604358792305, acc: 0.9943740963935852)
[2025-02-13 03:55:48,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:48,753][root][INFO] - Training Epoch: 2/2, step 3017/7134 completed (loss: 0.03431057184934616, acc: 0.9864406585693359)
[2025-02-13 03:55:48,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:49,138][root][INFO] - Training Epoch: 2/2, step 3018/7134 completed (loss: 0.02952759526669979, acc: 0.9950413107872009)
[2025-02-13 03:55:49,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:49,564][root][INFO] - Training Epoch: 2/2, step 3019/7134 completed (loss: 0.03544071316719055, acc: 0.9901960492134094)
[2025-02-13 03:55:49,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:49,971][root][INFO] - Training Epoch: 2/2, step 3020/7134 completed (loss: 0.038818359375, acc: 0.9910873174667358)
[2025-02-13 03:55:50,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:50,346][root][INFO] - Training Epoch: 2/2, step 3021/7134 completed (loss: 0.016505347564816475, acc: 0.9945454597473145)
[2025-02-13 03:55:50,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:50,765][root][INFO] - Training Epoch: 2/2, step 3022/7134 completed (loss: 0.032026540488004684, acc: 0.9951768517494202)
[2025-02-13 03:55:50,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:51,126][root][INFO] - Training Epoch: 2/2, step 3023/7134 completed (loss: 0.030574914067983627, acc: 0.9900596141815186)
[2025-02-13 03:55:51,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:51,553][root][INFO] - Training Epoch: 2/2, step 3024/7134 completed (loss: 0.01186318602412939, acc: 0.9961685538291931)
[2025-02-13 03:55:51,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:51,991][root][INFO] - Training Epoch: 2/2, step 3025/7134 completed (loss: 0.07092615962028503, acc: 0.9844827651977539)
[2025-02-13 03:55:52,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:52,394][root][INFO] - Training Epoch: 2/2, step 3026/7134 completed (loss: 0.030377892777323723, acc: 0.9928186535835266)
[2025-02-13 03:55:52,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:52,797][root][INFO] - Training Epoch: 2/2, step 3027/7134 completed (loss: 0.06091039255261421, acc: 0.9878787994384766)
[2025-02-13 03:55:52,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:53,123][root][INFO] - Training Epoch: 2/2, step 3028/7134 completed (loss: 0.01288228016346693, acc: 0.9954648613929749)
[2025-02-13 03:55:53,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:53,515][root][INFO] - Training Epoch: 2/2, step 3029/7134 completed (loss: 0.04326140508055687, acc: 0.9938176274299622)
[2025-02-13 03:55:53,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:53,968][root][INFO] - Training Epoch: 2/2, step 3030/7134 completed (loss: 0.023229902610182762, acc: 0.9923664331436157)
[2025-02-13 03:55:54,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:54,355][root][INFO] - Training Epoch: 2/2, step 3031/7134 completed (loss: 0.014119314029812813, acc: 0.9936440587043762)
[2025-02-13 03:55:54,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:54,768][root][INFO] - Training Epoch: 2/2, step 3032/7134 completed (loss: 0.047842442989349365, acc: 0.9891107082366943)
[2025-02-13 03:55:54,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:55,163][root][INFO] - Training Epoch: 2/2, step 3033/7134 completed (loss: 0.035981737077236176, acc: 0.9876543283462524)
[2025-02-13 03:55:55,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:55,569][root][INFO] - Training Epoch: 2/2, step 3034/7134 completed (loss: 0.03591550886631012, acc: 0.9872340559959412)
[2025-02-13 03:55:55,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:55,957][root][INFO] - Training Epoch: 2/2, step 3035/7134 completed (loss: 0.0107407933101058, acc: 0.995275616645813)
[2025-02-13 03:55:56,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:56,369][root][INFO] - Training Epoch: 2/2, step 3036/7134 completed (loss: 0.0526517778635025, acc: 0.9882943034172058)
[2025-02-13 03:55:56,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:56,808][root][INFO] - Training Epoch: 2/2, step 3037/7134 completed (loss: 0.020328443497419357, acc: 0.9938367009162903)
[2025-02-13 03:55:56,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:57,226][root][INFO] - Training Epoch: 2/2, step 3038/7134 completed (loss: 0.011786513961851597, acc: 0.9965217113494873)
[2025-02-13 03:55:57,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:57,633][root][INFO] - Training Epoch: 2/2, step 3039/7134 completed (loss: 0.04352465644478798, acc: 0.9938744306564331)
[2025-02-13 03:55:57,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:58,025][root][INFO] - Training Epoch: 2/2, step 3040/7134 completed (loss: 0.022637663409113884, acc: 0.994434118270874)
[2025-02-13 03:55:58,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:58,433][root][INFO] - Training Epoch: 2/2, step 3041/7134 completed (loss: 0.054045312106609344, acc: 0.9904305934906006)
[2025-02-13 03:55:58,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:58,840][root][INFO] - Training Epoch: 2/2, step 3042/7134 completed (loss: 0.022538956254720688, acc: 0.9940915703773499)
[2025-02-13 03:55:58,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:59,226][root][INFO] - Training Epoch: 2/2, step 3043/7134 completed (loss: 0.011856669560074806, acc: 0.9942965507507324)
[2025-02-13 03:55:59,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:59,611][root][INFO] - Training Epoch: 2/2, step 3044/7134 completed (loss: 0.02108670026063919, acc: 0.9948186278343201)
[2025-02-13 03:55:59,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:59,996][root][INFO] - Training Epoch: 2/2, step 3045/7134 completed (loss: 0.017894383519887924, acc: 0.994915246963501)
[2025-02-13 03:56:00,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:00,443][root][INFO] - Training Epoch: 2/2, step 3046/7134 completed (loss: 0.03262200206518173, acc: 0.9904371500015259)
[2025-02-13 03:56:00,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:00,874][root][INFO] - Training Epoch: 2/2, step 3047/7134 completed (loss: 0.007371032610535622, acc: 1.0)
[2025-02-13 03:56:01,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:01,364][root][INFO] - Training Epoch: 2/2, step 3048/7134 completed (loss: 0.018128646537661552, acc: 0.99370276927948)
[2025-02-13 03:56:01,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:01,773][root][INFO] - Training Epoch: 2/2, step 3049/7134 completed (loss: 0.013097308576107025, acc: 0.9941860437393188)
[2025-02-13 03:56:01,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:02,217][root][INFO] - Training Epoch: 2/2, step 3050/7134 completed (loss: 0.023810621351003647, acc: 0.9904988408088684)
[2025-02-13 03:56:02,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:02,624][root][INFO] - Training Epoch: 2/2, step 3051/7134 completed (loss: 0.03135582432150841, acc: 0.9864661693572998)
[2025-02-13 03:56:02,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:03,059][root][INFO] - Training Epoch: 2/2, step 3052/7134 completed (loss: 0.022316666319966316, acc: 0.9909793734550476)
[2025-02-13 03:56:03,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:03,489][root][INFO] - Training Epoch: 2/2, step 3053/7134 completed (loss: 0.014774019829928875, acc: 0.9927536249160767)
[2025-02-13 03:56:03,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:03,911][root][INFO] - Training Epoch: 2/2, step 3054/7134 completed (loss: 0.0442328155040741, acc: 0.984415590763092)
[2025-02-13 03:56:04,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:04,327][root][INFO] - Training Epoch: 2/2, step 3055/7134 completed (loss: 0.02797602489590645, acc: 0.988034188747406)
[2025-02-13 03:56:04,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:04,764][root][INFO] - Training Epoch: 2/2, step 3056/7134 completed (loss: 0.017312146723270416, acc: 0.9948586225509644)
[2025-02-13 03:56:04,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:05,224][root][INFO] - Training Epoch: 2/2, step 3057/7134 completed (loss: 0.016902290284633636, acc: 0.994452178478241)
[2025-02-13 03:56:05,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:05,653][root][INFO] - Training Epoch: 2/2, step 3058/7134 completed (loss: 0.04497203975915909, acc: 0.9909326434135437)
[2025-02-13 03:56:05,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:06,082][root][INFO] - Training Epoch: 2/2, step 3059/7134 completed (loss: 0.013497957028448582, acc: 0.9917808175086975)
[2025-02-13 03:56:06,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:06,520][root][INFO] - Training Epoch: 2/2, step 3060/7134 completed (loss: 0.023644352331757545, acc: 0.9928401112556458)
[2025-02-13 03:56:06,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:06,910][root][INFO] - Training Epoch: 2/2, step 3061/7134 completed (loss: 0.013869691640138626, acc: 0.9924242496490479)
[2025-02-13 03:56:07,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:07,354][root][INFO] - Training Epoch: 2/2, step 3062/7134 completed (loss: 0.00828255619853735, acc: 0.9986522793769836)
[2025-02-13 03:56:07,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:07,786][root][INFO] - Training Epoch: 2/2, step 3063/7134 completed (loss: 0.0240060705691576, acc: 0.9922978281974792)
[2025-02-13 03:56:07,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:08,204][root][INFO] - Training Epoch: 2/2, step 3064/7134 completed (loss: 0.01156655140221119, acc: 0.997183084487915)
[2025-02-13 03:56:08,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:08,641][root][INFO] - Training Epoch: 2/2, step 3065/7134 completed (loss: 0.01494472287595272, acc: 0.9959677457809448)
[2025-02-13 03:56:08,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:09,064][root][INFO] - Training Epoch: 2/2, step 3066/7134 completed (loss: 0.007223072461783886, acc: 0.9983870983123779)
[2025-02-13 03:56:09,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:09,437][root][INFO] - Training Epoch: 2/2, step 3067/7134 completed (loss: 0.012146511115133762, acc: 0.9981818199157715)
[2025-02-13 03:56:09,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:09,878][root][INFO] - Training Epoch: 2/2, step 3068/7134 completed (loss: 0.019603000953793526, acc: 0.9932340979576111)
[2025-02-13 03:56:10,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:10,321][root][INFO] - Training Epoch: 2/2, step 3069/7134 completed (loss: 0.014887544326484203, acc: 0.9976958632469177)
[2025-02-13 03:56:10,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:10,785][root][INFO] - Training Epoch: 2/2, step 3070/7134 completed (loss: 0.011514388024806976, acc: 0.9968186616897583)
[2025-02-13 03:56:10,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:11,226][root][INFO] - Training Epoch: 2/2, step 3071/7134 completed (loss: 0.019550403580069542, acc: 0.9948520064353943)
[2025-02-13 03:56:11,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:11,670][root][INFO] - Training Epoch: 2/2, step 3072/7134 completed (loss: 0.019520055502653122, acc: 0.9953325390815735)
[2025-02-13 03:56:11,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:12,073][root][INFO] - Training Epoch: 2/2, step 3073/7134 completed (loss: 0.008787238039076328, acc: 0.9974554777145386)
[2025-02-13 03:56:12,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:12,485][root][INFO] - Training Epoch: 2/2, step 3074/7134 completed (loss: 0.0179803054779768, acc: 0.9933444261550903)
[2025-02-13 03:56:12,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:12,901][root][INFO] - Training Epoch: 2/2, step 3075/7134 completed (loss: 0.014178171753883362, acc: 0.9958333373069763)
[2025-02-13 03:56:13,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:13,346][root][INFO] - Training Epoch: 2/2, step 3076/7134 completed (loss: 0.025766482576727867, acc: 0.9922308325767517)
[2025-02-13 03:56:13,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:13,829][root][INFO] - Training Epoch: 2/2, step 3077/7134 completed (loss: 0.04865088686347008, acc: 0.9846994280815125)
[2025-02-13 03:56:13,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:14,283][root][INFO] - Training Epoch: 2/2, step 3078/7134 completed (loss: 0.04451783373951912, acc: 0.9890710115432739)
[2025-02-13 03:56:14,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:14,721][root][INFO] - Training Epoch: 2/2, step 3079/7134 completed (loss: 0.02523191086947918, acc: 0.990123450756073)
[2025-02-13 03:56:14,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:15,178][root][INFO] - Training Epoch: 2/2, step 3080/7134 completed (loss: 0.05202804505825043, acc: 0.9858233332633972)
[2025-02-13 03:56:15,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:15,504][root][INFO] - Training Epoch: 2/2, step 3081/7134 completed (loss: 0.030831338837742805, acc: 0.993534505367279)
[2025-02-13 03:56:15,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:15,939][root][INFO] - Training Epoch: 2/2, step 3082/7134 completed (loss: 0.05727753788232803, acc: 0.9807460904121399)
[2025-02-13 03:56:16,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:16,383][root][INFO] - Training Epoch: 2/2, step 3083/7134 completed (loss: 0.039049603044986725, acc: 0.9882199168205261)
[2025-02-13 03:56:16,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:16,819][root][INFO] - Training Epoch: 2/2, step 3084/7134 completed (loss: 0.04320007562637329, acc: 0.988095223903656)
[2025-02-13 03:56:16,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:17,256][root][INFO] - Training Epoch: 2/2, step 3085/7134 completed (loss: 0.05526815354824066, acc: 0.9868612885475159)
[2025-02-13 03:56:17,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:17,733][root][INFO] - Training Epoch: 2/2, step 3086/7134 completed (loss: 0.044528912752866745, acc: 0.9831649661064148)
[2025-02-13 03:56:17,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:18,184][root][INFO] - Training Epoch: 2/2, step 3087/7134 completed (loss: 0.035633936524391174, acc: 0.9899569749832153)
[2025-02-13 03:56:18,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:18,642][root][INFO] - Training Epoch: 2/2, step 3088/7134 completed (loss: 0.027909157797694206, acc: 0.990980863571167)
[2025-02-13 03:56:18,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:19,117][root][INFO] - Training Epoch: 2/2, step 3089/7134 completed (loss: 0.03370732441544533, acc: 0.9902912378311157)
[2025-02-13 03:56:19,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:19,575][root][INFO] - Training Epoch: 2/2, step 3090/7134 completed (loss: 0.06943845748901367, acc: 0.9823369383811951)
[2025-02-13 03:56:19,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:20,044][root][INFO] - Training Epoch: 2/2, step 3091/7134 completed (loss: 0.04083295911550522, acc: 0.9907833933830261)
[2025-02-13 03:56:20,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:20,434][root][INFO] - Training Epoch: 2/2, step 3092/7134 completed (loss: 0.044149719178676605, acc: 0.9833641648292542)
[2025-02-13 03:56:20,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:20,880][root][INFO] - Training Epoch: 2/2, step 3093/7134 completed (loss: 0.03482304885983467, acc: 0.9900662302970886)
[2025-02-13 03:56:21,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:21,338][root][INFO] - Training Epoch: 2/2, step 3094/7134 completed (loss: 0.013951904140412807, acc: 0.9959127902984619)
[2025-02-13 03:56:21,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:21,756][root][INFO] - Training Epoch: 2/2, step 3095/7134 completed (loss: 0.04036787524819374, acc: 0.987500011920929)
[2025-02-13 03:56:21,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:22,194][root][INFO] - Training Epoch: 2/2, step 3096/7134 completed (loss: 0.061724260449409485, acc: 0.9820282459259033)
[2025-02-13 03:56:22,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:22,651][root][INFO] - Training Epoch: 2/2, step 3097/7134 completed (loss: 0.06041121482849121, acc: 0.9871794581413269)
[2025-02-13 03:56:22,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:23,092][root][INFO] - Training Epoch: 2/2, step 3098/7134 completed (loss: 0.021642742678523064, acc: 0.9952830076217651)
[2025-02-13 03:56:23,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:23,496][root][INFO] - Training Epoch: 2/2, step 3099/7134 completed (loss: 0.017175989225506783, acc: 0.9935170412063599)
[2025-02-13 03:56:23,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:23,972][root][INFO] - Training Epoch: 2/2, step 3100/7134 completed (loss: 0.025600112974643707, acc: 0.9936169981956482)
[2025-02-13 03:56:24,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:24,417][root][INFO] - Training Epoch: 2/2, step 3101/7134 completed (loss: 0.02320639044046402, acc: 0.9927007555961609)
[2025-02-13 03:56:24,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:24,879][root][INFO] - Training Epoch: 2/2, step 3102/7134 completed (loss: 0.025651998817920685, acc: 0.9881481528282166)
[2025-02-13 03:56:25,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:25,349][root][INFO] - Training Epoch: 2/2, step 3103/7134 completed (loss: 0.031084878370165825, acc: 0.9904153347015381)
[2025-02-13 03:56:25,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:25,697][root][INFO] - Training Epoch: 2/2, step 3104/7134 completed (loss: 0.013372519053518772, acc: 0.9961832165718079)
[2025-02-13 03:56:25,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:26,128][root][INFO] - Training Epoch: 2/2, step 3105/7134 completed (loss: 0.03591462969779968, acc: 0.9890109896659851)
[2025-02-13 03:56:26,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:26,548][root][INFO] - Training Epoch: 2/2, step 3106/7134 completed (loss: 0.017616134136915207, acc: 0.9933110475540161)
[2025-02-13 03:56:26,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:26,956][root][INFO] - Training Epoch: 2/2, step 3107/7134 completed (loss: 0.010680577717721462, acc: 0.9960784316062927)
[2025-02-13 03:56:27,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:27,396][root][INFO] - Training Epoch: 2/2, step 3108/7134 completed (loss: 0.03382158279418945, acc: 0.9938461780548096)
[2025-02-13 03:56:27,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:27,809][root][INFO] - Training Epoch: 2/2, step 3109/7134 completed (loss: 0.02083565853536129, acc: 0.9944649338722229)
[2025-02-13 03:56:27,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:28,228][root][INFO] - Training Epoch: 2/2, step 3110/7134 completed (loss: 0.02462870255112648, acc: 0.992682933807373)
[2025-02-13 03:56:28,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:28,638][root][INFO] - Training Epoch: 2/2, step 3111/7134 completed (loss: 0.11424875259399414, acc: 0.9707903861999512)
[2025-02-13 03:56:28,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:29,027][root][INFO] - Training Epoch: 2/2, step 3112/7134 completed (loss: 0.03693506494164467, acc: 0.9947368502616882)
[2025-02-13 03:56:29,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:29,472][root][INFO] - Training Epoch: 2/2, step 3113/7134 completed (loss: 0.03088383376598358, acc: 0.9901685118675232)
[2025-02-13 03:56:29,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:29,889][root][INFO] - Training Epoch: 2/2, step 3114/7134 completed (loss: 0.02271851897239685, acc: 0.9948275685310364)
[2025-02-13 03:56:30,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:30,293][root][INFO] - Training Epoch: 2/2, step 3115/7134 completed (loss: 0.05987914279103279, acc: 0.9845678806304932)
[2025-02-13 03:56:30,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:30,734][root][INFO] - Training Epoch: 2/2, step 3116/7134 completed (loss: 0.022281261160969734, acc: 0.9923076629638672)
[2025-02-13 03:56:30,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:31,171][root][INFO] - Training Epoch: 2/2, step 3117/7134 completed (loss: 0.0780307799577713, acc: 0.9850543737411499)
[2025-02-13 03:56:31,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:31,599][root][INFO] - Training Epoch: 2/2, step 3118/7134 completed (loss: 0.03685185685753822, acc: 0.990777313709259)
[2025-02-13 03:56:31,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:32,033][root][INFO] - Training Epoch: 2/2, step 3119/7134 completed (loss: 0.04683937504887581, acc: 0.9844357967376709)
[2025-02-13 03:56:32,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:32,435][root][INFO] - Training Epoch: 2/2, step 3120/7134 completed (loss: 0.04970540478825569, acc: 0.9929478168487549)
[2025-02-13 03:56:32,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:32,856][root][INFO] - Training Epoch: 2/2, step 3121/7134 completed (loss: 0.03095744550228119, acc: 0.9897172451019287)
[2025-02-13 03:56:33,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:33,332][root][INFO] - Training Epoch: 2/2, step 3122/7134 completed (loss: 0.03504142537713051, acc: 0.989534854888916)
[2025-02-13 03:56:33,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:33,775][root][INFO] - Training Epoch: 2/2, step 3123/7134 completed (loss: 0.05471113696694374, acc: 0.9825870394706726)
[2025-02-13 03:56:33,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:34,199][root][INFO] - Training Epoch: 2/2, step 3124/7134 completed (loss: 0.03660361468791962, acc: 0.9938119053840637)
[2025-02-13 03:56:34,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:34,612][root][INFO] - Training Epoch: 2/2, step 3125/7134 completed (loss: 0.01822550967335701, acc: 0.9939485788345337)
[2025-02-13 03:56:34,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:35,081][root][INFO] - Training Epoch: 2/2, step 3126/7134 completed (loss: 0.029209677129983902, acc: 0.9936467409133911)
[2025-02-13 03:56:35,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:35,520][root][INFO] - Training Epoch: 2/2, step 3127/7134 completed (loss: 0.026534391567111015, acc: 0.9906976819038391)
[2025-02-13 03:56:35,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:35,948][root][INFO] - Training Epoch: 2/2, step 3128/7134 completed (loss: 0.028803858906030655, acc: 0.9922580718994141)
[2025-02-13 03:56:36,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:36,405][root][INFO] - Training Epoch: 2/2, step 3129/7134 completed (loss: 0.025012889876961708, acc: 0.9913473129272461)
[2025-02-13 03:56:36,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:36,876][root][INFO] - Training Epoch: 2/2, step 3130/7134 completed (loss: 0.02609732374548912, acc: 0.9896789193153381)
[2025-02-13 03:56:37,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:37,303][root][INFO] - Training Epoch: 2/2, step 3131/7134 completed (loss: 0.04172113537788391, acc: 0.9884058237075806)
[2025-02-13 03:56:37,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:37,669][root][INFO] - Training Epoch: 2/2, step 3132/7134 completed (loss: 0.008237313479185104, acc: 1.0)
[2025-02-13 03:56:37,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:38,123][root][INFO] - Training Epoch: 2/2, step 3133/7134 completed (loss: 0.10355160385370255, acc: 0.9821428656578064)
[2025-02-13 03:56:38,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:38,567][root][INFO] - Training Epoch: 2/2, step 3134/7134 completed (loss: 0.028976377099752426, acc: 0.9908424615859985)
[2025-02-13 03:56:38,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:38,923][root][INFO] - Training Epoch: 2/2, step 3135/7134 completed (loss: 0.04801027849316597, acc: 0.986143171787262)
[2025-02-13 03:56:39,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:39,343][root][INFO] - Training Epoch: 2/2, step 3136/7134 completed (loss: 0.015207231044769287, acc: 0.9982608556747437)
[2025-02-13 03:56:39,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:39,757][root][INFO] - Training Epoch: 2/2, step 3137/7134 completed (loss: 0.03695995360612869, acc: 0.9841269850730896)
[2025-02-13 03:56:39,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:40,148][root][INFO] - Training Epoch: 2/2, step 3138/7134 completed (loss: 0.04259837418794632, acc: 0.9852941036224365)
[2025-02-13 03:56:40,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:40,491][root][INFO] - Training Epoch: 2/2, step 3139/7134 completed (loss: 0.06324466317892075, acc: 0.9785407781600952)
[2025-02-13 03:56:40,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:40,875][root][INFO] - Training Epoch: 2/2, step 3140/7134 completed (loss: 0.024992676451802254, acc: 0.9922178983688354)
[2025-02-13 03:56:41,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:41,277][root][INFO] - Training Epoch: 2/2, step 3141/7134 completed (loss: 0.048539336770772934, acc: 0.9842657446861267)
[2025-02-13 03:56:41,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:41,688][root][INFO] - Training Epoch: 2/2, step 3142/7134 completed (loss: 0.013066195882856846, acc: 0.9970104694366455)
[2025-02-13 03:56:41,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:42,101][root][INFO] - Training Epoch: 2/2, step 3143/7134 completed (loss: 0.015974482521414757, acc: 0.9967845678329468)
[2025-02-13 03:56:42,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:42,523][root][INFO] - Training Epoch: 2/2, step 3144/7134 completed (loss: 0.015204151161015034, acc: 0.9937106966972351)
[2025-02-13 03:56:42,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:42,934][root][INFO] - Training Epoch: 2/2, step 3145/7134 completed (loss: 0.04168800264596939, acc: 0.9912472367286682)
[2025-02-13 03:56:43,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:43,352][root][INFO] - Training Epoch: 2/2, step 3146/7134 completed (loss: 0.019717441871762276, acc: 0.98828125)
[2025-02-13 03:56:43,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:43,756][root][INFO] - Training Epoch: 2/2, step 3147/7134 completed (loss: 0.03451092913746834, acc: 0.9935794472694397)
[2025-02-13 03:56:43,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:44,198][root][INFO] - Training Epoch: 2/2, step 3148/7134 completed (loss: 0.027631834149360657, acc: 0.9923469424247742)
[2025-02-13 03:56:44,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:44,620][root][INFO] - Training Epoch: 2/2, step 3149/7134 completed (loss: 0.00724905775859952, acc: 0.9985358715057373)
[2025-02-13 03:56:44,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:45,039][root][INFO] - Training Epoch: 2/2, step 3150/7134 completed (loss: 0.1433446854352951, acc: 0.9704142212867737)
[2025-02-13 03:56:45,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:45,433][root][INFO] - Training Epoch: 2/2, step 3151/7134 completed (loss: 0.05782970041036606, acc: 0.984674334526062)
[2025-02-13 03:56:45,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:45,832][root][INFO] - Training Epoch: 2/2, step 3152/7134 completed (loss: 0.03464768826961517, acc: 0.9863636493682861)
[2025-02-13 03:56:45,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:46,254][root][INFO] - Training Epoch: 2/2, step 3153/7134 completed (loss: 0.042916085571050644, acc: 0.9862155318260193)
[2025-02-13 03:56:46,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:46,660][root][INFO] - Training Epoch: 2/2, step 3154/7134 completed (loss: 0.03171562775969505, acc: 0.9928951859474182)
[2025-02-13 03:56:46,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:47,049][root][INFO] - Training Epoch: 2/2, step 3155/7134 completed (loss: 0.0854613184928894, acc: 0.9756592512130737)
[2025-02-13 03:56:47,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:47,476][root][INFO] - Training Epoch: 2/2, step 3156/7134 completed (loss: 0.0792541354894638, acc: 0.9762258529663086)
[2025-02-13 03:56:47,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:47,871][root][INFO] - Training Epoch: 2/2, step 3157/7134 completed (loss: 0.014094115234911442, acc: 0.9935483932495117)
[2025-02-13 03:56:47,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:48,265][root][INFO] - Training Epoch: 2/2, step 3158/7134 completed (loss: 0.019618619233369827, acc: 0.9950980544090271)
[2025-02-13 03:56:48,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:48,665][root][INFO] - Training Epoch: 2/2, step 3159/7134 completed (loss: 0.0593591034412384, acc: 0.9828431606292725)
[2025-02-13 03:56:48,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:49,088][root][INFO] - Training Epoch: 2/2, step 3160/7134 completed (loss: 0.009899163618683815, acc: 0.9972337484359741)
[2025-02-13 03:56:49,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:49,528][root][INFO] - Training Epoch: 2/2, step 3161/7134 completed (loss: 0.027693601325154305, acc: 0.9915966391563416)
[2025-02-13 03:56:49,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:49,941][root][INFO] - Training Epoch: 2/2, step 3162/7134 completed (loss: 0.013830196112394333, acc: 0.9927113652229309)
[2025-02-13 03:56:50,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:50,331][root][INFO] - Training Epoch: 2/2, step 3163/7134 completed (loss: 0.03633471578359604, acc: 0.9883138537406921)
[2025-02-13 03:56:50,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:50,746][root][INFO] - Training Epoch: 2/2, step 3164/7134 completed (loss: 0.046468764543533325, acc: 0.9882746934890747)
[2025-02-13 03:56:50,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:51,155][root][INFO] - Training Epoch: 2/2, step 3165/7134 completed (loss: 0.02235475741326809, acc: 0.9933221936225891)
[2025-02-13 03:56:51,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:51,570][root][INFO] - Training Epoch: 2/2, step 3166/7134 completed (loss: 0.05195748433470726, acc: 0.9879699349403381)
[2025-02-13 03:56:51,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:51,998][root][INFO] - Training Epoch: 2/2, step 3167/7134 completed (loss: 0.09413153678178787, acc: 0.965624988079071)
[2025-02-13 03:56:52,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:52,404][root][INFO] - Training Epoch: 2/2, step 3168/7134 completed (loss: 0.07100063562393188, acc: 0.9771341681480408)
[2025-02-13 03:56:52,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:52,799][root][INFO] - Training Epoch: 2/2, step 3169/7134 completed (loss: 0.05930723994970322, acc: 0.9856557250022888)
[2025-02-13 03:56:52,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:53,212][root][INFO] - Training Epoch: 2/2, step 3170/7134 completed (loss: 0.03388535976409912, acc: 0.9818181991577148)
[2025-02-13 03:56:53,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:53,632][root][INFO] - Training Epoch: 2/2, step 3171/7134 completed (loss: 0.034909892827272415, acc: 0.9868938326835632)
[2025-02-13 03:56:53,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:54,025][root][INFO] - Training Epoch: 2/2, step 3172/7134 completed (loss: 0.009996877983212471, acc: 0.9961685538291931)
[2025-02-13 03:56:54,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:54,384][root][INFO] - Training Epoch: 2/2, step 3173/7134 completed (loss: 0.03754107654094696, acc: 0.9872449040412903)
[2025-02-13 03:56:54,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:54,815][root][INFO] - Training Epoch: 2/2, step 3174/7134 completed (loss: 0.07228699326515198, acc: 0.976331353187561)
[2025-02-13 03:56:54,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:55,235][root][INFO] - Training Epoch: 2/2, step 3175/7134 completed (loss: 0.045856837183237076, acc: 0.98591548204422)
[2025-02-13 03:56:55,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:55,643][root][INFO] - Training Epoch: 2/2, step 3176/7134 completed (loss: 0.07945328950881958, acc: 0.9789473414421082)
[2025-02-13 03:56:55,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:56,033][root][INFO] - Training Epoch: 2/2, step 3177/7134 completed (loss: 0.07085196673870087, acc: 0.9861111044883728)
[2025-02-13 03:56:56,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:56,436][root][INFO] - Training Epoch: 2/2, step 3178/7134 completed (loss: 0.024945499375462532, acc: 0.9950739145278931)
[2025-02-13 03:56:56,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:56,868][root][INFO] - Training Epoch: 2/2, step 3179/7134 completed (loss: 0.08584201335906982, acc: 0.9800994992256165)
[2025-02-13 03:56:56,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:57,266][root][INFO] - Training Epoch: 2/2, step 3180/7134 completed (loss: 0.02699870988726616, acc: 0.9957143068313599)
[2025-02-13 03:56:57,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:57,678][root][INFO] - Training Epoch: 2/2, step 3181/7134 completed (loss: 0.05612959340214729, acc: 0.9890795350074768)
[2025-02-13 03:56:57,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:58,106][root][INFO] - Training Epoch: 2/2, step 3182/7134 completed (loss: 0.03791934996843338, acc: 0.9940387606620789)
[2025-02-13 03:56:58,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:58,494][root][INFO] - Training Epoch: 2/2, step 3183/7134 completed (loss: 0.05042510852217674, acc: 0.9881556630134583)
[2025-02-13 03:56:58,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:58,920][root][INFO] - Training Epoch: 2/2, step 3184/7134 completed (loss: 0.047459714114665985, acc: 0.988095223903656)
[2025-02-13 03:56:59,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:59,328][root][INFO] - Training Epoch: 2/2, step 3185/7134 completed (loss: 0.012914983555674553, acc: 0.995726466178894)
[2025-02-13 03:56:59,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:59,744][root][INFO] - Training Epoch: 2/2, step 3186/7134 completed (loss: 0.014941425062716007, acc: 0.9970370531082153)
[2025-02-13 03:56:59,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:00,128][root][INFO] - Training Epoch: 2/2, step 3187/7134 completed (loss: 0.03740467131137848, acc: 0.9841772317886353)
[2025-02-13 03:57:00,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:00,540][root][INFO] - Training Epoch: 2/2, step 3188/7134 completed (loss: 0.029350070282816887, acc: 0.9935317039489746)
[2025-02-13 03:57:00,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:00,946][root][INFO] - Training Epoch: 2/2, step 3189/7134 completed (loss: 0.04847221449017525, acc: 0.9826338887214661)
[2025-02-13 03:57:01,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:01,385][root][INFO] - Training Epoch: 2/2, step 3190/7134 completed (loss: 0.04198652505874634, acc: 0.9898734092712402)
[2025-02-13 03:57:01,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:01,775][root][INFO] - Training Epoch: 2/2, step 3191/7134 completed (loss: 0.02860494703054428, acc: 0.9885714054107666)
[2025-02-13 03:57:01,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:02,169][root][INFO] - Training Epoch: 2/2, step 3192/7134 completed (loss: 0.04391998425126076, acc: 0.9886578321456909)
[2025-02-13 03:57:02,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:02,574][root][INFO] - Training Epoch: 2/2, step 3193/7134 completed (loss: 0.06243591383099556, acc: 0.980966329574585)
[2025-02-13 03:57:02,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:02,971][root][INFO] - Training Epoch: 2/2, step 3194/7134 completed (loss: 0.040738802403211594, acc: 0.983660101890564)
[2025-02-13 03:57:03,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:03,369][root][INFO] - Training Epoch: 2/2, step 3195/7134 completed (loss: 0.032646920531988144, acc: 0.9931389093399048)
[2025-02-13 03:57:03,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:03,765][root][INFO] - Training Epoch: 2/2, step 3196/7134 completed (loss: 0.024313434958457947, acc: 0.9887850284576416)
[2025-02-13 03:57:03,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:04,148][root][INFO] - Training Epoch: 2/2, step 3197/7134 completed (loss: 0.026047037914395332, acc: 0.997183084487915)
[2025-02-13 03:57:04,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:04,559][root][INFO] - Training Epoch: 2/2, step 3198/7134 completed (loss: 0.029358671978116035, acc: 0.9909228682518005)
[2025-02-13 03:57:04,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:04,953][root][INFO] - Training Epoch: 2/2, step 3199/7134 completed (loss: 0.029211916029453278, acc: 0.9941262602806091)
[2025-02-13 03:57:05,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:05,308][root][INFO] - Training Epoch: 2/2, step 3200/7134 completed (loss: 0.0365881510078907, acc: 0.9876977205276489)
[2025-02-13 03:57:05,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:05,724][root][INFO] - Training Epoch: 2/2, step 3201/7134 completed (loss: 0.09666988998651505, acc: 0.9804772138595581)
[2025-02-13 03:57:05,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:06,139][root][INFO] - Training Epoch: 2/2, step 3202/7134 completed (loss: 0.025248169898986816, acc: 0.9944674968719482)
[2025-02-13 03:57:06,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:06,535][root][INFO] - Training Epoch: 2/2, step 3203/7134 completed (loss: 0.028630303218960762, acc: 0.9921011328697205)
[2025-02-13 03:57:06,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:06,879][root][INFO] - Training Epoch: 2/2, step 3204/7134 completed (loss: 0.06846585124731064, acc: 0.9774436354637146)
[2025-02-13 03:57:07,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:07,280][root][INFO] - Training Epoch: 2/2, step 3205/7134 completed (loss: 0.006645070388913155, acc: 1.0)
[2025-02-13 03:57:07,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:07,724][root][INFO] - Training Epoch: 2/2, step 3206/7134 completed (loss: 0.04119123890995979, acc: 0.9913169145584106)
[2025-02-13 03:57:07,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:08,132][root][INFO] - Training Epoch: 2/2, step 3207/7134 completed (loss: 0.03217093273997307, acc: 0.9877488613128662)
[2025-02-13 03:57:08,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:08,554][root][INFO] - Training Epoch: 2/2, step 3208/7134 completed (loss: 0.07659820467233658, acc: 0.9852941036224365)
[2025-02-13 03:57:08,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:08,958][root][INFO] - Training Epoch: 2/2, step 3209/7134 completed (loss: 0.013517740182578564, acc: 0.9940476417541504)
[2025-02-13 03:57:09,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:09,379][root][INFO] - Training Epoch: 2/2, step 3210/7134 completed (loss: 0.009046723134815693, acc: 0.9984939694404602)
[2025-02-13 03:57:09,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:09,767][root][INFO] - Training Epoch: 2/2, step 3211/7134 completed (loss: 0.047378990799188614, acc: 0.9916840195655823)
[2025-02-13 03:57:09,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:10,184][root][INFO] - Training Epoch: 2/2, step 3212/7134 completed (loss: 0.006611322518438101, acc: 0.9986013770103455)
[2025-02-13 03:57:10,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:10,601][root][INFO] - Training Epoch: 2/2, step 3213/7134 completed (loss: 0.022864840924739838, acc: 0.9942611455917358)
[2025-02-13 03:57:10,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:11,038][root][INFO] - Training Epoch: 2/2, step 3214/7134 completed (loss: 0.02135283127427101, acc: 0.993306577205658)
[2025-02-13 03:57:11,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:11,472][root][INFO] - Training Epoch: 2/2, step 3215/7134 completed (loss: 0.03903365135192871, acc: 0.9859943985939026)
[2025-02-13 03:57:11,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:11,899][root][INFO] - Training Epoch: 2/2, step 3216/7134 completed (loss: 0.0190272219479084, acc: 0.9957020282745361)
[2025-02-13 03:57:12,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:12,344][root][INFO] - Training Epoch: 2/2, step 3217/7134 completed (loss: 0.010844088159501553, acc: 0.9987096786499023)
[2025-02-13 03:57:12,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:12,744][root][INFO] - Training Epoch: 2/2, step 3218/7134 completed (loss: 0.046188171952962875, acc: 0.9937402009963989)
[2025-02-13 03:57:12,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:13,155][root][INFO] - Training Epoch: 2/2, step 3219/7134 completed (loss: 0.044129278510808945, acc: 0.9867060780525208)
[2025-02-13 03:57:13,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:13,582][root][INFO] - Training Epoch: 2/2, step 3220/7134 completed (loss: 0.038866691291332245, acc: 0.9906542301177979)
[2025-02-13 03:57:13,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:14,019][root][INFO] - Training Epoch: 2/2, step 3221/7134 completed (loss: 0.0315907783806324, acc: 0.9876712560653687)
[2025-02-13 03:57:14,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:14,431][root][INFO] - Training Epoch: 2/2, step 3222/7134 completed (loss: 0.050339844077825546, acc: 0.9887359142303467)
[2025-02-13 03:57:14,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:14,846][root][INFO] - Training Epoch: 2/2, step 3223/7134 completed (loss: 0.0576857291162014, acc: 0.9888888597488403)
[2025-02-13 03:57:14,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:15,299][root][INFO] - Training Epoch: 2/2, step 3224/7134 completed (loss: 0.052869342267513275, acc: 0.9913169145584106)
[2025-02-13 03:57:15,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:15,741][root][INFO] - Training Epoch: 2/2, step 3225/7134 completed (loss: 0.03403790295124054, acc: 0.9898862242698669)
[2025-02-13 03:57:15,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:16,134][root][INFO] - Training Epoch: 2/2, step 3226/7134 completed (loss: 0.034310221672058105, acc: 0.985637366771698)
[2025-02-13 03:57:16,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:16,604][root][INFO] - Training Epoch: 2/2, step 3227/7134 completed (loss: 0.05802697688341141, acc: 0.9868131875991821)
[2025-02-13 03:57:16,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:17,055][root][INFO] - Training Epoch: 2/2, step 3228/7134 completed (loss: 0.027438418939709663, acc: 0.9908015727996826)
[2025-02-13 03:57:17,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:17,487][root][INFO] - Training Epoch: 2/2, step 3229/7134 completed (loss: 0.03307868912816048, acc: 0.9889841079711914)
[2025-02-13 03:57:17,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:17,895][root][INFO] - Training Epoch: 2/2, step 3230/7134 completed (loss: 0.041512381285429, acc: 0.9837278127670288)
[2025-02-13 03:57:18,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:18,254][root][INFO] - Training Epoch: 2/2, step 3231/7134 completed (loss: 0.03900131583213806, acc: 0.9924012422561646)
[2025-02-13 03:57:18,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:18,689][root][INFO] - Training Epoch: 2/2, step 3232/7134 completed (loss: 0.01392741221934557, acc: 0.9932659864425659)
[2025-02-13 03:57:18,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:19,087][root][INFO] - Training Epoch: 2/2, step 3233/7134 completed (loss: 0.04856037348508835, acc: 0.990138053894043)
[2025-02-13 03:57:19,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:19,478][root][INFO] - Training Epoch: 2/2, step 3234/7134 completed (loss: 0.02401299960911274, acc: 0.9952606558799744)
[2025-02-13 03:57:19,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:19,893][root][INFO] - Training Epoch: 2/2, step 3235/7134 completed (loss: 0.029331235215067863, acc: 0.9866666793823242)
[2025-02-13 03:57:20,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:20,282][root][INFO] - Training Epoch: 2/2, step 3236/7134 completed (loss: 0.0672936961054802, acc: 0.9852724671363831)
[2025-02-13 03:57:20,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:20,692][root][INFO] - Training Epoch: 2/2, step 3237/7134 completed (loss: 0.020573079586029053, acc: 0.9935587644577026)
[2025-02-13 03:57:20,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:21,102][root][INFO] - Training Epoch: 2/2, step 3238/7134 completed (loss: 0.03757518529891968, acc: 0.9907975196838379)
[2025-02-13 03:57:21,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:21,514][root][INFO] - Training Epoch: 2/2, step 3239/7134 completed (loss: 0.013619642704725266, acc: 0.9936908483505249)
[2025-02-13 03:57:21,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:21,919][root][INFO] - Training Epoch: 2/2, step 3240/7134 completed (loss: 0.03289701044559479, acc: 0.9910314083099365)
[2025-02-13 03:57:22,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:22,325][root][INFO] - Training Epoch: 2/2, step 3241/7134 completed (loss: 0.015667449682950974, acc: 0.9940652847290039)
[2025-02-13 03:57:22,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:22,733][root][INFO] - Training Epoch: 2/2, step 3242/7134 completed (loss: 0.0326707549393177, acc: 0.991830050945282)
[2025-02-13 03:57:22,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:23,160][root][INFO] - Training Epoch: 2/2, step 3243/7134 completed (loss: 0.02321634441614151, acc: 0.9959127902984619)
[2025-02-13 03:57:23,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:23,558][root][INFO] - Training Epoch: 2/2, step 3244/7134 completed (loss: 0.011199596337974072, acc: 0.9985652565956116)
[2025-02-13 03:57:23,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:23,998][root][INFO] - Training Epoch: 2/2, step 3245/7134 completed (loss: 0.024186719208955765, acc: 0.9961340427398682)
[2025-02-13 03:57:24,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:24,434][root][INFO] - Training Epoch: 2/2, step 3246/7134 completed (loss: 0.036246661096811295, acc: 0.985897421836853)
[2025-02-13 03:57:24,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:24,852][root][INFO] - Training Epoch: 2/2, step 3247/7134 completed (loss: 0.029317907989025116, acc: 0.990275502204895)
[2025-02-13 03:57:24,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:25,255][root][INFO] - Training Epoch: 2/2, step 3248/7134 completed (loss: 0.015185996890068054, acc: 0.9967159032821655)
[2025-02-13 03:57:25,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:25,670][root][INFO] - Training Epoch: 2/2, step 3249/7134 completed (loss: 0.0226907879114151, acc: 0.9958391189575195)
[2025-02-13 03:57:25,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:26,081][root][INFO] - Training Epoch: 2/2, step 3250/7134 completed (loss: 0.03853921592235565, acc: 0.9856114983558655)
[2025-02-13 03:57:26,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:26,489][root][INFO] - Training Epoch: 2/2, step 3251/7134 completed (loss: 0.022311747074127197, acc: 0.9942938685417175)
[2025-02-13 03:57:26,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:26,914][root][INFO] - Training Epoch: 2/2, step 3252/7134 completed (loss: 0.004177766386419535, acc: 0.99858158826828)
[2025-02-13 03:57:27,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:27,326][root][INFO] - Training Epoch: 2/2, step 3253/7134 completed (loss: 0.022618889808654785, acc: 0.995184600353241)
[2025-02-13 03:57:27,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:27,750][root][INFO] - Training Epoch: 2/2, step 3254/7134 completed (loss: 0.008771703578531742, acc: 0.9983136653900146)
[2025-02-13 03:57:27,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:28,157][root][INFO] - Training Epoch: 2/2, step 3255/7134 completed (loss: 0.005497067701071501, acc: 0.9973368644714355)
[2025-02-13 03:57:28,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:28,564][root][INFO] - Training Epoch: 2/2, step 3256/7134 completed (loss: 0.0172626581043005, acc: 0.995555579662323)
[2025-02-13 03:57:28,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:28,970][root][INFO] - Training Epoch: 2/2, step 3257/7134 completed (loss: 0.02033117040991783, acc: 0.994452178478241)
[2025-02-13 03:57:29,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:29,385][root][INFO] - Training Epoch: 2/2, step 3258/7134 completed (loss: 0.024011896923184395, acc: 0.9865384697914124)
[2025-02-13 03:57:29,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:29,794][root][INFO] - Training Epoch: 2/2, step 3259/7134 completed (loss: 0.0204536784440279, acc: 0.9955882430076599)
[2025-02-13 03:57:29,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:30,179][root][INFO] - Training Epoch: 2/2, step 3260/7134 completed (loss: 0.06810653209686279, acc: 0.9792284965515137)
[2025-02-13 03:57:30,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:30,589][root][INFO] - Training Epoch: 2/2, step 3261/7134 completed (loss: 0.03430042788386345, acc: 0.9865771532058716)
[2025-02-13 03:57:30,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:31,010][root][INFO] - Training Epoch: 2/2, step 3262/7134 completed (loss: 0.022105582058429718, acc: 0.9913169145584106)
[2025-02-13 03:57:31,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:31,417][root][INFO] - Training Epoch: 2/2, step 3263/7134 completed (loss: 0.0640225037932396, acc: 0.9867841601371765)
[2025-02-13 03:57:31,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:31,820][root][INFO] - Training Epoch: 2/2, step 3264/7134 completed (loss: 0.02002563141286373, acc: 0.9944444298744202)
[2025-02-13 03:57:31,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:32,245][root][INFO] - Training Epoch: 2/2, step 3265/7134 completed (loss: 0.08468907326459885, acc: 0.9772079586982727)
[2025-02-13 03:57:32,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:32,665][root][INFO] - Training Epoch: 2/2, step 3266/7134 completed (loss: 0.040212202817201614, acc: 0.9909909963607788)
[2025-02-13 03:57:32,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:33,084][root][INFO] - Training Epoch: 2/2, step 3267/7134 completed (loss: 0.06093801185488701, acc: 0.9813753366470337)
[2025-02-13 03:57:33,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:33,478][root][INFO] - Training Epoch: 2/2, step 3268/7134 completed (loss: 0.03787073493003845, acc: 0.990755021572113)
[2025-02-13 03:57:33,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:33,913][root][INFO] - Training Epoch: 2/2, step 3269/7134 completed (loss: 0.03156334161758423, acc: 0.9886363744735718)
[2025-02-13 03:57:34,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:34,351][root][INFO] - Training Epoch: 2/2, step 3270/7134 completed (loss: 0.01384410448372364, acc: 0.9952152967453003)
[2025-02-13 03:57:34,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:34,630][root][INFO] - Training Epoch: 2/2, step 3271/7134 completed (loss: 0.00968831405043602, acc: 0.9966555237770081)
[2025-02-13 03:57:34,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:35,044][root][INFO] - Training Epoch: 2/2, step 3272/7134 completed (loss: 0.027905579656362534, acc: 0.9954198598861694)
[2025-02-13 03:57:35,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:35,486][root][INFO] - Training Epoch: 2/2, step 3273/7134 completed (loss: 0.017142118886113167, acc: 0.9961685538291931)
[2025-02-13 03:57:35,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:35,922][root][INFO] - Training Epoch: 2/2, step 3274/7134 completed (loss: 0.05142117664217949, acc: 0.9897210001945496)
[2025-02-13 03:57:36,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:36,350][root][INFO] - Training Epoch: 2/2, step 3275/7134 completed (loss: 0.12254901230335236, acc: 0.9736841917037964)
[2025-02-13 03:57:36,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:36,769][root][INFO] - Training Epoch: 2/2, step 3276/7134 completed (loss: 0.05704197660088539, acc: 0.9834710955619812)
[2025-02-13 03:57:36,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:37,194][root][INFO] - Training Epoch: 2/2, step 3277/7134 completed (loss: 0.033853575587272644, acc: 0.9908536672592163)
[2025-02-13 03:57:37,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:37,545][root][INFO] - Training Epoch: 2/2, step 3278/7134 completed (loss: 0.04663698375225067, acc: 0.9789674878120422)
[2025-02-13 03:57:37,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:37,989][root][INFO] - Training Epoch: 2/2, step 3279/7134 completed (loss: 0.03301281854510307, acc: 0.992546558380127)
[2025-02-13 03:57:38,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:38,363][root][INFO] - Training Epoch: 2/2, step 3280/7134 completed (loss: 0.10976779460906982, acc: 0.9709543585777283)
[2025-02-13 03:57:38,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:38,789][root][INFO] - Training Epoch: 2/2, step 3281/7134 completed (loss: 0.05276497080922127, acc: 0.9855072498321533)
[2025-02-13 03:57:38,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:39,200][root][INFO] - Training Epoch: 2/2, step 3282/7134 completed (loss: 0.021900789812207222, acc: 0.9952531456947327)
[2025-02-13 03:57:39,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:39,633][root][INFO] - Training Epoch: 2/2, step 3283/7134 completed (loss: 0.027373816817998886, acc: 0.991752564907074)
[2025-02-13 03:57:39,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:40,045][root][INFO] - Training Epoch: 2/2, step 3284/7134 completed (loss: 0.05648352578282356, acc: 0.9847792983055115)
[2025-02-13 03:57:40,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:40,453][root][INFO] - Training Epoch: 2/2, step 3285/7134 completed (loss: 0.03325092792510986, acc: 0.9938080310821533)
[2025-02-13 03:57:40,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:40,870][root][INFO] - Training Epoch: 2/2, step 3286/7134 completed (loss: 0.03115110844373703, acc: 0.9969924688339233)
[2025-02-13 03:57:41,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:41,276][root][INFO] - Training Epoch: 2/2, step 3287/7134 completed (loss: 0.08829332888126373, acc: 0.9819079041481018)
[2025-02-13 03:57:41,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:41,701][root][INFO] - Training Epoch: 2/2, step 3288/7134 completed (loss: 0.04075395315885544, acc: 0.9895209670066833)
[2025-02-13 03:57:41,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:42,056][root][INFO] - Training Epoch: 2/2, step 3289/7134 completed (loss: 0.07140057533979416, acc: 0.9789473414421082)
[2025-02-13 03:57:42,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:42,447][root][INFO] - Training Epoch: 2/2, step 3290/7134 completed (loss: 0.04817777872085571, acc: 0.9913194179534912)
[2025-02-13 03:57:42,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:42,844][root][INFO] - Training Epoch: 2/2, step 3291/7134 completed (loss: 0.044717371463775635, acc: 0.9732739329338074)
[2025-02-13 03:57:42,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:43,243][root][INFO] - Training Epoch: 2/2, step 3292/7134 completed (loss: 0.03293837606906891, acc: 0.9894179701805115)
[2025-02-13 03:57:43,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:43,696][root][INFO] - Training Epoch: 2/2, step 3293/7134 completed (loss: 0.03232355788350105, acc: 0.9896755218505859)
[2025-02-13 03:57:43,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:44,112][root][INFO] - Training Epoch: 2/2, step 3294/7134 completed (loss: 0.06973384320735931, acc: 0.9785605072975159)
[2025-02-13 03:57:44,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:44,487][root][INFO] - Training Epoch: 2/2, step 3295/7134 completed (loss: 0.02012917585670948, acc: 0.9948805570602417)
[2025-02-13 03:57:44,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:44,880][root][INFO] - Training Epoch: 2/2, step 3296/7134 completed (loss: 0.05500215291976929, acc: 0.9780621528625488)
[2025-02-13 03:57:45,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:45,234][root][INFO] - Training Epoch: 2/2, step 3297/7134 completed (loss: 0.03983587026596069, acc: 0.9906103014945984)
[2025-02-13 03:57:45,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:45,634][root][INFO] - Training Epoch: 2/2, step 3298/7134 completed (loss: 0.09008512645959854, acc: 0.9760383367538452)
[2025-02-13 03:57:45,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:46,035][root][INFO] - Training Epoch: 2/2, step 3299/7134 completed (loss: 0.03744210675358772, acc: 0.9848993420600891)
[2025-02-13 03:57:46,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:46,429][root][INFO] - Training Epoch: 2/2, step 3300/7134 completed (loss: 0.057305485010147095, acc: 0.9839572310447693)
[2025-02-13 03:57:46,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:46,779][root][INFO] - Training Epoch: 2/2, step 3301/7134 completed (loss: 0.0365082286298275, acc: 0.9958506226539612)
[2025-02-13 03:57:46,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:47,172][root][INFO] - Training Epoch: 2/2, step 3302/7134 completed (loss: 0.04685744643211365, acc: 0.9898989796638489)
[2025-02-13 03:57:47,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:47,501][root][INFO] - Training Epoch: 2/2, step 3303/7134 completed (loss: 0.048925261944532394, acc: 0.9889867901802063)
[2025-02-13 03:57:47,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:47,939][root][INFO] - Training Epoch: 2/2, step 3304/7134 completed (loss: 0.02521355077624321, acc: 0.9916467666625977)
[2025-02-13 03:57:48,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:48,351][root][INFO] - Training Epoch: 2/2, step 3305/7134 completed (loss: 0.03129902854561806, acc: 0.9909502267837524)
[2025-02-13 03:57:48,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:48,821][root][INFO] - Training Epoch: 2/2, step 3306/7134 completed (loss: 0.025305168703198433, acc: 0.9948119521141052)
[2025-02-13 03:57:48,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:49,222][root][INFO] - Training Epoch: 2/2, step 3307/7134 completed (loss: 0.03287409245967865, acc: 0.9864636063575745)
[2025-02-13 03:57:49,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:49,629][root][INFO] - Training Epoch: 2/2, step 3308/7134 completed (loss: 0.038601815700531006, acc: 0.986994206905365)
[2025-02-13 03:57:49,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:50,025][root][INFO] - Training Epoch: 2/2, step 3309/7134 completed (loss: 0.060474980622529984, acc: 0.982300877571106)
[2025-02-13 03:57:50,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:50,468][root][INFO] - Training Epoch: 2/2, step 3310/7134 completed (loss: 0.03266005963087082, acc: 0.9936034083366394)
[2025-02-13 03:57:50,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:50,759][root][INFO] - Training Epoch: 2/2, step 3311/7134 completed (loss: 0.031140362843871117, acc: 0.9892473220825195)
[2025-02-13 03:57:50,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:51,160][root][INFO] - Training Epoch: 2/2, step 3312/7134 completed (loss: 0.01711914874613285, acc: 0.9945454597473145)
[2025-02-13 03:57:51,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:51,569][root][INFO] - Training Epoch: 2/2, step 3313/7134 completed (loss: 0.01299657765775919, acc: 0.9976133704185486)
[2025-02-13 03:57:51,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:51,973][root][INFO] - Training Epoch: 2/2, step 3314/7134 completed (loss: 0.04191235080361366, acc: 0.989130437374115)
[2025-02-13 03:57:52,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:52,416][root][INFO] - Training Epoch: 2/2, step 3315/7134 completed (loss: 0.06029898300766945, acc: 0.9850746393203735)
[2025-02-13 03:57:52,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:52,853][root][INFO] - Training Epoch: 2/2, step 3316/7134 completed (loss: 0.018133845180273056, acc: 0.9949579834938049)
[2025-02-13 03:57:52,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:53,270][root][INFO] - Training Epoch: 2/2, step 3317/7134 completed (loss: 0.07137142866849899, acc: 0.982692301273346)
[2025-02-13 03:57:53,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:53,537][root][INFO] - Training Epoch: 2/2, step 3318/7134 completed (loss: 0.02597171626985073, acc: 0.9891696572303772)
[2025-02-13 03:57:53,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:53,980][root][INFO] - Training Epoch: 2/2, step 3319/7134 completed (loss: 0.03216809779405594, acc: 0.9869706630706787)
[2025-02-13 03:57:54,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:54,421][root][INFO] - Training Epoch: 2/2, step 3320/7134 completed (loss: 0.06848860532045364, acc: 0.9829843044281006)
[2025-02-13 03:57:54,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:54,825][root][INFO] - Training Epoch: 2/2, step 3321/7134 completed (loss: 0.09857933968305588, acc: 0.9724770784378052)
[2025-02-13 03:57:54,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:55,281][root][INFO] - Training Epoch: 2/2, step 3322/7134 completed (loss: 0.03353014960885048, acc: 0.9909909963607788)
[2025-02-13 03:57:55,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:55,715][root][INFO] - Training Epoch: 2/2, step 3323/7134 completed (loss: 0.03602490574121475, acc: 0.9917241334915161)
[2025-02-13 03:57:55,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:56,175][root][INFO] - Training Epoch: 2/2, step 3324/7134 completed (loss: 0.05411193519830704, acc: 0.9858657121658325)
[2025-02-13 03:57:56,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:56,614][root][INFO] - Training Epoch: 2/2, step 3325/7134 completed (loss: 0.03935829922556877, acc: 0.9905914068222046)
[2025-02-13 03:57:56,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:57,039][root][INFO] - Training Epoch: 2/2, step 3326/7134 completed (loss: 0.05478869006037712, acc: 0.9811023473739624)
[2025-02-13 03:57:57,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:57,449][root][INFO] - Training Epoch: 2/2, step 3327/7134 completed (loss: 0.030773049220442772, acc: 0.9905277490615845)
[2025-02-13 03:57:57,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:57,907][root][INFO] - Training Epoch: 2/2, step 3328/7134 completed (loss: 0.03840986266732216, acc: 0.9949874877929688)
[2025-02-13 03:57:58,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:58,340][root][INFO] - Training Epoch: 2/2, step 3329/7134 completed (loss: 0.02432449348270893, acc: 0.9963459372520447)
[2025-02-13 03:57:58,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:58,795][root][INFO] - Training Epoch: 2/2, step 3330/7134 completed (loss: 0.04168454557657242, acc: 0.9884792566299438)
[2025-02-13 03:57:58,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:59,209][root][INFO] - Training Epoch: 2/2, step 3331/7134 completed (loss: 0.08595003932714462, acc: 0.9709302186965942)
[2025-02-13 03:57:59,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:59,649][root][INFO] - Training Epoch: 2/2, step 3332/7134 completed (loss: 0.10232186317443848, acc: 0.9740484356880188)
[2025-02-13 03:57:59,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:00,030][root][INFO] - Training Epoch: 2/2, step 3333/7134 completed (loss: 0.05685143172740936, acc: 0.9897540807723999)
[2025-02-13 03:58:00,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:00,446][root][INFO] - Training Epoch: 2/2, step 3334/7134 completed (loss: 0.053600676357746124, acc: 0.9878378510475159)
[2025-02-13 03:58:00,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:00,851][root][INFO] - Training Epoch: 2/2, step 3335/7134 completed (loss: 0.027905460447072983, acc: 0.9920886158943176)
[2025-02-13 03:58:00,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:01,264][root][INFO] - Training Epoch: 2/2, step 3336/7134 completed (loss: 0.012466936372220516, acc: 0.9970845580101013)
[2025-02-13 03:58:01,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:01,681][root][INFO] - Training Epoch: 2/2, step 3337/7134 completed (loss: 0.014491992071270943, acc: 0.9960552453994751)
[2025-02-13 03:58:01,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:02,114][root][INFO] - Training Epoch: 2/2, step 3338/7134 completed (loss: 0.011031911708414555, acc: 0.9975154995918274)
[2025-02-13 03:58:02,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:02,557][root][INFO] - Training Epoch: 2/2, step 3339/7134 completed (loss: 0.05918829143047333, acc: 0.9844868779182434)
[2025-02-13 03:58:02,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:02,970][root][INFO] - Training Epoch: 2/2, step 3340/7134 completed (loss: 0.09238513559103012, acc: 0.9728506803512573)
[2025-02-13 03:58:03,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:03,406][root][INFO] - Training Epoch: 2/2, step 3341/7134 completed (loss: 0.04631498456001282, acc: 0.9881481528282166)
[2025-02-13 03:58:03,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:03,822][root][INFO] - Training Epoch: 2/2, step 3342/7134 completed (loss: 0.03215253725647926, acc: 0.9898648858070374)
[2025-02-13 03:58:03,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:04,251][root][INFO] - Training Epoch: 2/2, step 3343/7134 completed (loss: 0.024106750264763832, acc: 0.9939516186714172)
[2025-02-13 03:58:04,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:04,673][root][INFO] - Training Epoch: 2/2, step 3344/7134 completed (loss: 0.013222871348261833, acc: 0.9965694546699524)
[2025-02-13 03:58:04,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:05,094][root][INFO] - Training Epoch: 2/2, step 3345/7134 completed (loss: 0.0715220645070076, acc: 0.9837925434112549)
[2025-02-13 03:58:05,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:05,520][root][INFO] - Training Epoch: 2/2, step 3346/7134 completed (loss: 0.034001827239990234, acc: 0.988304078578949)
[2025-02-13 03:58:05,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:05,918][root][INFO] - Training Epoch: 2/2, step 3347/7134 completed (loss: 0.062352146953344345, acc: 0.9837133288383484)
[2025-02-13 03:58:06,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:06,319][root][INFO] - Training Epoch: 2/2, step 3348/7134 completed (loss: 0.036864086985588074, acc: 0.9925925731658936)
[2025-02-13 03:58:06,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:06,684][root][INFO] - Training Epoch: 2/2, step 3349/7134 completed (loss: 0.026765037328004837, acc: 0.9929453134536743)
[2025-02-13 03:58:06,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:07,096][root][INFO] - Training Epoch: 2/2, step 3350/7134 completed (loss: 0.01941889151930809, acc: 0.9918699264526367)
[2025-02-13 03:58:07,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:07,495][root][INFO] - Training Epoch: 2/2, step 3351/7134 completed (loss: 0.0507197380065918, acc: 0.9867374300956726)
[2025-02-13 03:58:07,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:07,889][root][INFO] - Training Epoch: 2/2, step 3352/7134 completed (loss: 0.026376308873295784, acc: 0.9947826266288757)
[2025-02-13 03:58:08,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:08,282][root][INFO] - Training Epoch: 2/2, step 3353/7134 completed (loss: 0.03491704538464546, acc: 0.9881656765937805)
[2025-02-13 03:58:08,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:08,682][root][INFO] - Training Epoch: 2/2, step 3354/7134 completed (loss: 0.026056023314595222, acc: 0.9937984347343445)
[2025-02-13 03:58:08,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:09,097][root][INFO] - Training Epoch: 2/2, step 3355/7134 completed (loss: 0.009821758605539799, acc: 0.9971791505813599)
[2025-02-13 03:58:09,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:09,511][root][INFO] - Training Epoch: 2/2, step 3356/7134 completed (loss: 0.007216351106762886, acc: 0.9982638955116272)
[2025-02-13 03:58:09,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:09,927][root][INFO] - Training Epoch: 2/2, step 3357/7134 completed (loss: 0.03307085856795311, acc: 0.9913420081138611)
[2025-02-13 03:58:10,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:10,320][root][INFO] - Training Epoch: 2/2, step 3358/7134 completed (loss: 0.03573489189147949, acc: 0.9930796027183533)
[2025-02-13 03:58:10,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:10,715][root][INFO] - Training Epoch: 2/2, step 3359/7134 completed (loss: 0.02015303075313568, acc: 0.9935275316238403)
[2025-02-13 03:58:10,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:11,114][root][INFO] - Training Epoch: 2/2, step 3360/7134 completed (loss: 0.09568679332733154, acc: 0.9828326106071472)
[2025-02-13 03:58:11,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:11,508][root][INFO] - Training Epoch: 2/2, step 3361/7134 completed (loss: 0.03664611652493477, acc: 0.9843400716781616)
[2025-02-13 03:58:11,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:11,901][root][INFO] - Training Epoch: 2/2, step 3362/7134 completed (loss: 0.04312917962670326, acc: 0.9863945841789246)
[2025-02-13 03:58:12,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:12,252][root][INFO] - Training Epoch: 2/2, step 3363/7134 completed (loss: 0.018324699252843857, acc: 0.9953051805496216)
[2025-02-13 03:58:12,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:12,671][root][INFO] - Training Epoch: 2/2, step 3364/7134 completed (loss: 0.07075779139995575, acc: 0.9831029176712036)
[2025-02-13 03:58:12,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:13,078][root][INFO] - Training Epoch: 2/2, step 3365/7134 completed (loss: 0.0627649798989296, acc: 0.983132541179657)
[2025-02-13 03:58:13,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:13,484][root][INFO] - Training Epoch: 2/2, step 3366/7134 completed (loss: 0.022057276219129562, acc: 0.9887892603874207)
[2025-02-13 03:58:13,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:13,886][root][INFO] - Training Epoch: 2/2, step 3367/7134 completed (loss: 0.03183750435709953, acc: 0.9931129217147827)
[2025-02-13 03:58:14,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:14,312][root][INFO] - Training Epoch: 2/2, step 3368/7134 completed (loss: 0.035282202064991, acc: 0.9921996593475342)
[2025-02-13 03:58:14,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:14,763][root][INFO] - Training Epoch: 2/2, step 3369/7134 completed (loss: 0.04039417952299118, acc: 0.9899280667304993)
[2025-02-13 03:58:14,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:15,181][root][INFO] - Training Epoch: 2/2, step 3370/7134 completed (loss: 0.015747644007205963, acc: 0.9971387982368469)
[2025-02-13 03:58:15,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:15,585][root][INFO] - Training Epoch: 2/2, step 3371/7134 completed (loss: 0.016789982095360756, acc: 0.9950658082962036)
[2025-02-13 03:58:15,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:16,003][root][INFO] - Training Epoch: 2/2, step 3372/7134 completed (loss: 0.026117943227291107, acc: 0.9952977895736694)
[2025-02-13 03:58:16,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:16,407][root][INFO] - Training Epoch: 2/2, step 3373/7134 completed (loss: 0.024832889437675476, acc: 0.9922839403152466)
[2025-02-13 03:58:16,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:16,853][root][INFO] - Training Epoch: 2/2, step 3374/7134 completed (loss: 0.015842707827687263, acc: 0.9958217144012451)
[2025-02-13 03:58:16,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:17,296][root][INFO] - Training Epoch: 2/2, step 3375/7134 completed (loss: 0.017154548317193985, acc: 0.9942113161087036)
[2025-02-13 03:58:17,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:17,730][root][INFO] - Training Epoch: 2/2, step 3376/7134 completed (loss: 0.007668609265238047, acc: 0.9974457025527954)
[2025-02-13 03:58:17,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:18,196][root][INFO] - Training Epoch: 2/2, step 3377/7134 completed (loss: 0.021638810634613037, acc: 0.9964243173599243)
[2025-02-13 03:58:18,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:18,624][root][INFO] - Training Epoch: 2/2, step 3378/7134 completed (loss: 0.011845006607472897, acc: 0.99609375)
[2025-02-13 03:58:18,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:19,032][root][INFO] - Training Epoch: 2/2, step 3379/7134 completed (loss: 0.018728801980614662, acc: 0.995488703250885)
[2025-02-13 03:58:19,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:19,438][root][INFO] - Training Epoch: 2/2, step 3380/7134 completed (loss: 0.03771325200796127, acc: 0.9907692074775696)
[2025-02-13 03:58:19,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:19,878][root][INFO] - Training Epoch: 2/2, step 3381/7134 completed (loss: 0.031787946820259094, acc: 0.9961240291595459)
[2025-02-13 03:58:20,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:20,291][root][INFO] - Training Epoch: 2/2, step 3382/7134 completed (loss: 0.013522109016776085, acc: 0.9948096871376038)
[2025-02-13 03:58:20,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:20,733][root][INFO] - Training Epoch: 2/2, step 3383/7134 completed (loss: 0.03477959334850311, acc: 0.9928264021873474)
[2025-02-13 03:58:20,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:21,123][root][INFO] - Training Epoch: 2/2, step 3384/7134 completed (loss: 0.03169461712241173, acc: 0.9877675771713257)
[2025-02-13 03:58:21,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:21,538][root][INFO] - Training Epoch: 2/2, step 3385/7134 completed (loss: 0.029258178547024727, acc: 0.9913669228553772)
[2025-02-13 03:58:21,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:21,955][root][INFO] - Training Epoch: 2/2, step 3386/7134 completed (loss: 0.01486674789339304, acc: 0.9957627058029175)
[2025-02-13 03:58:22,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:22,370][root][INFO] - Training Epoch: 2/2, step 3387/7134 completed (loss: 0.0446065217256546, acc: 0.9918367266654968)
[2025-02-13 03:58:22,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:22,785][root][INFO] - Training Epoch: 2/2, step 3388/7134 completed (loss: 0.027504609897732735, acc: 0.9891501069068909)
[2025-02-13 03:58:22,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:23,199][root][INFO] - Training Epoch: 2/2, step 3389/7134 completed (loss: 0.021290279924869537, acc: 0.9947299361228943)
[2025-02-13 03:58:23,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:23,624][root][INFO] - Training Epoch: 2/2, step 3390/7134 completed (loss: 0.021891804412007332, acc: 0.9935232996940613)
[2025-02-13 03:58:23,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:24,010][root][INFO] - Training Epoch: 2/2, step 3391/7134 completed (loss: 0.003087991615757346, acc: 1.0)
[2025-02-13 03:58:24,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:24,394][root][INFO] - Training Epoch: 2/2, step 3392/7134 completed (loss: 0.010285130701959133, acc: 0.9968553185462952)
[2025-02-13 03:58:24,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:24,790][root][INFO] - Training Epoch: 2/2, step 3393/7134 completed (loss: 0.02894236333668232, acc: 0.9891975522041321)
[2025-02-13 03:58:24,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:25,213][root][INFO] - Training Epoch: 2/2, step 3394/7134 completed (loss: 0.03440236672759056, acc: 0.9905063509941101)
[2025-02-13 03:58:25,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:25,594][root][INFO] - Training Epoch: 2/2, step 3395/7134 completed (loss: 0.06277511268854141, acc: 0.9805194735527039)
[2025-02-13 03:58:25,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:26,033][root][INFO] - Training Epoch: 2/2, step 3396/7134 completed (loss: 0.022022927179932594, acc: 0.9928143620491028)
[2025-02-13 03:58:26,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:26,477][root][INFO] - Training Epoch: 2/2, step 3397/7134 completed (loss: 0.02537619322538376, acc: 0.9938875436782837)
[2025-02-13 03:58:26,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:26,898][root][INFO] - Training Epoch: 2/2, step 3398/7134 completed (loss: 0.07156161963939667, acc: 0.9892473220825195)
[2025-02-13 03:58:27,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:27,303][root][INFO] - Training Epoch: 2/2, step 3399/7134 completed (loss: 0.09058203548192978, acc: 0.9742646813392639)
[2025-02-13 03:58:27,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:27,721][root][INFO] - Training Epoch: 2/2, step 3400/7134 completed (loss: 0.042779210954904556, acc: 0.9797160029411316)
[2025-02-13 03:58:27,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:28,127][root][INFO] - Training Epoch: 2/2, step 3401/7134 completed (loss: 0.038937732577323914, acc: 0.9877622127532959)
[2025-02-13 03:58:28,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:28,616][root][INFO] - Training Epoch: 2/2, step 3402/7134 completed (loss: 0.044290050864219666, acc: 0.990777313709259)
[2025-02-13 03:58:28,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:29,071][root][INFO] - Training Epoch: 2/2, step 3403/7134 completed (loss: 0.07901482284069061, acc: 0.9791666865348816)
[2025-02-13 03:58:29,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:29,481][root][INFO] - Training Epoch: 2/2, step 3404/7134 completed (loss: 0.018530137836933136, acc: 0.9970104694366455)
[2025-02-13 03:58:29,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:29,889][root][INFO] - Training Epoch: 2/2, step 3405/7134 completed (loss: 0.0678916871547699, acc: 0.9838129281997681)
[2025-02-13 03:58:30,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:30,325][root][INFO] - Training Epoch: 2/2, step 3406/7134 completed (loss: 0.03828691691160202, acc: 0.9845361113548279)
[2025-02-13 03:58:30,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:30,700][root][INFO] - Training Epoch: 2/2, step 3407/7134 completed (loss: 0.05204271152615547, acc: 0.9875195026397705)
[2025-02-13 03:58:30,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:31,093][root][INFO] - Training Epoch: 2/2, step 3408/7134 completed (loss: 0.035558685660362244, acc: 0.9861751198768616)
[2025-02-13 03:58:31,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:31,535][root][INFO] - Training Epoch: 2/2, step 3409/7134 completed (loss: 0.03900080546736717, acc: 0.9918808937072754)
[2025-02-13 03:58:31,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:31,948][root][INFO] - Training Epoch: 2/2, step 3410/7134 completed (loss: 0.037338610738515854, acc: 0.9917355179786682)
[2025-02-13 03:58:32,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:32,383][root][INFO] - Training Epoch: 2/2, step 3411/7134 completed (loss: 0.04009552672505379, acc: 0.9909443855285645)
[2025-02-13 03:58:32,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:32,773][root][INFO] - Training Epoch: 2/2, step 3412/7134 completed (loss: 0.04075753316283226, acc: 0.9867674708366394)
[2025-02-13 03:58:32,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:33,182][root][INFO] - Training Epoch: 2/2, step 3413/7134 completed (loss: 0.03565967455506325, acc: 0.9900000095367432)
[2025-02-13 03:58:33,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:33,594][root][INFO] - Training Epoch: 2/2, step 3414/7134 completed (loss: 0.035733021795749664, acc: 0.9916527271270752)
[2025-02-13 03:58:33,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:34,035][root][INFO] - Training Epoch: 2/2, step 3415/7134 completed (loss: 0.036533091217279434, acc: 0.9919871687889099)
[2025-02-13 03:58:34,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:34,493][root][INFO] - Training Epoch: 2/2, step 3416/7134 completed (loss: 0.045592498034238815, acc: 0.981992781162262)
[2025-02-13 03:58:34,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:34,925][root][INFO] - Training Epoch: 2/2, step 3417/7134 completed (loss: 0.020874962210655212, acc: 0.9932523369789124)
[2025-02-13 03:58:35,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:35,343][root][INFO] - Training Epoch: 2/2, step 3418/7134 completed (loss: 0.026360662654042244, acc: 0.9926362037658691)
[2025-02-13 03:58:35,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:35,741][root][INFO] - Training Epoch: 2/2, step 3419/7134 completed (loss: 0.014836663380265236, acc: 0.9956772327423096)
[2025-02-13 03:58:35,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:36,159][root][INFO] - Training Epoch: 2/2, step 3420/7134 completed (loss: 0.016276592388749123, acc: 0.9936808943748474)
[2025-02-13 03:58:36,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:36,581][root][INFO] - Training Epoch: 2/2, step 3421/7134 completed (loss: 0.028541769832372665, acc: 0.996889591217041)
[2025-02-13 03:58:36,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:36,966][root][INFO] - Training Epoch: 2/2, step 3422/7134 completed (loss: 0.008895457722246647, acc: 0.996497392654419)
[2025-02-13 03:58:37,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:37,391][root][INFO] - Training Epoch: 2/2, step 3423/7134 completed (loss: 0.020038584247231483, acc: 0.9939393997192383)
[2025-02-13 03:58:37,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:37,804][root][INFO] - Training Epoch: 2/2, step 3424/7134 completed (loss: 0.05952577665448189, acc: 0.9789156913757324)
[2025-02-13 03:58:37,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:38,213][root][INFO] - Training Epoch: 2/2, step 3425/7134 completed (loss: 0.02192351594567299, acc: 0.9948717951774597)
[2025-02-13 03:58:38,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:38,648][root][INFO] - Training Epoch: 2/2, step 3426/7134 completed (loss: 0.04201946035027504, acc: 0.9944827556610107)
[2025-02-13 03:58:38,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:39,026][root][INFO] - Training Epoch: 2/2, step 3427/7134 completed (loss: 0.03929297998547554, acc: 0.9921011328697205)
[2025-02-13 03:58:39,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:39,458][root][INFO] - Training Epoch: 2/2, step 3428/7134 completed (loss: 0.027574295178055763, acc: 0.9957020282745361)
[2025-02-13 03:58:39,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:39,903][root][INFO] - Training Epoch: 2/2, step 3429/7134 completed (loss: 0.04309890791773796, acc: 0.9870550036430359)
[2025-02-13 03:58:40,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:40,316][root][INFO] - Training Epoch: 2/2, step 3430/7134 completed (loss: 0.07104503363370895, acc: 0.987034022808075)
[2025-02-13 03:58:40,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:40,743][root][INFO] - Training Epoch: 2/2, step 3431/7134 completed (loss: 0.03603959456086159, acc: 0.9898132681846619)
[2025-02-13 03:58:40,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:41,186][root][INFO] - Training Epoch: 2/2, step 3432/7134 completed (loss: 0.032584041357040405, acc: 0.9931412935256958)
[2025-02-13 03:58:41,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:41,636][root][INFO] - Training Epoch: 2/2, step 3433/7134 completed (loss: 0.05522603169083595, acc: 0.9852941036224365)
[2025-02-13 03:58:41,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:42,044][root][INFO] - Training Epoch: 2/2, step 3434/7134 completed (loss: 0.021573495119810104, acc: 0.995230495929718)
[2025-02-13 03:58:42,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:42,460][root][INFO] - Training Epoch: 2/2, step 3435/7134 completed (loss: 0.03943237289786339, acc: 0.9887459874153137)
[2025-02-13 03:58:42,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:42,881][root][INFO] - Training Epoch: 2/2, step 3436/7134 completed (loss: 0.03630049526691437, acc: 0.9902439117431641)
[2025-02-13 03:58:43,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:43,249][root][INFO] - Training Epoch: 2/2, step 3437/7134 completed (loss: 0.012421718798577785, acc: 0.99609375)
[2025-02-13 03:58:43,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:43,660][root][INFO] - Training Epoch: 2/2, step 3438/7134 completed (loss: 0.033026400953531265, acc: 0.9939393997192383)
[2025-02-13 03:58:43,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:44,039][root][INFO] - Training Epoch: 2/2, step 3439/7134 completed (loss: 0.03496013209223747, acc: 0.9909583926200867)
[2025-02-13 03:58:44,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:44,441][root][INFO] - Training Epoch: 2/2, step 3440/7134 completed (loss: 0.056655000895261765, acc: 0.9773662686347961)
[2025-02-13 03:58:44,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:44,844][root][INFO] - Training Epoch: 2/2, step 3441/7134 completed (loss: 0.027620717883110046, acc: 0.9855491518974304)
[2025-02-13 03:58:44,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:45,251][root][INFO] - Training Epoch: 2/2, step 3442/7134 completed (loss: 0.030219554901123047, acc: 0.9945255517959595)
[2025-02-13 03:58:45,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:45,609][root][INFO] - Training Epoch: 2/2, step 3443/7134 completed (loss: 0.06871074438095093, acc: 0.983146071434021)
[2025-02-13 03:58:45,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:46,016][root][INFO] - Training Epoch: 2/2, step 3444/7134 completed (loss: 0.06091121584177017, acc: 0.9889240264892578)
[2025-02-13 03:58:46,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:46,361][root][INFO] - Training Epoch: 2/2, step 3445/7134 completed (loss: 0.09626200795173645, acc: 0.9741935729980469)
[2025-02-13 03:58:46,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:46,782][root][INFO] - Training Epoch: 2/2, step 3446/7134 completed (loss: 0.08914124965667725, acc: 0.9816053509712219)
[2025-02-13 03:58:46,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:47,201][root][INFO] - Training Epoch: 2/2, step 3447/7134 completed (loss: 0.04771050438284874, acc: 0.9853420257568359)
[2025-02-13 03:58:47,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:47,603][root][INFO] - Training Epoch: 2/2, step 3448/7134 completed (loss: 0.02147754281759262, acc: 0.9924924969673157)
[2025-02-13 03:58:47,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:47,992][root][INFO] - Training Epoch: 2/2, step 3449/7134 completed (loss: 0.048195309937000275, acc: 0.9897959232330322)
[2025-02-13 03:58:48,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:48,401][root][INFO] - Training Epoch: 2/2, step 3450/7134 completed (loss: 0.029456382617354393, acc: 0.9873417615890503)
[2025-02-13 03:58:48,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:48,811][root][INFO] - Training Epoch: 2/2, step 3451/7134 completed (loss: 0.03545108810067177, acc: 0.9876325130462646)
[2025-02-13 03:58:48,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:49,213][root][INFO] - Training Epoch: 2/2, step 3452/7134 completed (loss: 0.03692544996738434, acc: 0.989154040813446)
[2025-02-13 03:58:49,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:49,599][root][INFO] - Training Epoch: 2/2, step 3453/7134 completed (loss: 0.04903874546289444, acc: 0.9881154298782349)
[2025-02-13 03:58:49,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:50,027][root][INFO] - Training Epoch: 2/2, step 3454/7134 completed (loss: 0.025208069011569023, acc: 0.9929478168487549)
[2025-02-13 03:58:50,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:50,461][root][INFO] - Training Epoch: 2/2, step 3455/7134 completed (loss: 0.01225098967552185, acc: 0.9971014261245728)
[2025-02-13 03:58:50,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:50,901][root][INFO] - Training Epoch: 2/2, step 3456/7134 completed (loss: 0.028093121945858, acc: 0.9941176176071167)
[2025-02-13 03:58:51,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:51,333][root][INFO] - Training Epoch: 2/2, step 3457/7134 completed (loss: 0.016094839200377464, acc: 0.993220329284668)
[2025-02-13 03:58:51,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:51,779][root][INFO] - Training Epoch: 2/2, step 3458/7134 completed (loss: 0.04586441069841385, acc: 0.9878214001655579)
[2025-02-13 03:58:51,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:52,185][root][INFO] - Training Epoch: 2/2, step 3459/7134 completed (loss: 0.04976950213313103, acc: 0.9878234267234802)
[2025-02-13 03:58:52,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:52,534][root][INFO] - Training Epoch: 2/2, step 3460/7134 completed (loss: 0.12691497802734375, acc: 0.9647436141967773)
[2025-02-13 03:58:52,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:52,837][root][INFO] - Training Epoch: 2/2, step 3461/7134 completed (loss: 0.18738269805908203, acc: 0.9586206674575806)
[2025-02-13 03:58:52,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:53,233][root][INFO] - Training Epoch: 2/2, step 3462/7134 completed (loss: 0.09545566141605377, acc: 0.976452112197876)
[2025-02-13 03:58:53,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:53,680][root][INFO] - Training Epoch: 2/2, step 3463/7134 completed (loss: 0.019570045173168182, acc: 0.993966817855835)
[2025-02-13 03:58:53,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:54,107][root][INFO] - Training Epoch: 2/2, step 3464/7134 completed (loss: 0.04489883780479431, acc: 0.9865951538085938)
[2025-02-13 03:58:54,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:54,495][root][INFO] - Training Epoch: 2/2, step 3465/7134 completed (loss: 0.07954781502485275, acc: 0.9775967597961426)
[2025-02-13 03:58:54,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:54,893][root][INFO] - Training Epoch: 2/2, step 3466/7134 completed (loss: 0.014734609983861446, acc: 0.996830403804779)
[2025-02-13 03:58:55,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:55,296][root][INFO] - Training Epoch: 2/2, step 3467/7134 completed (loss: 0.028920873999595642, acc: 0.987261176109314)
[2025-02-13 03:58:55,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:55,707][root][INFO] - Training Epoch: 2/2, step 3468/7134 completed (loss: 0.008557400666177273, acc: 0.9985527992248535)
[2025-02-13 03:58:55,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:56,112][root][INFO] - Training Epoch: 2/2, step 3469/7134 completed (loss: 0.03411995619535446, acc: 0.9830188751220703)
[2025-02-13 03:58:56,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:56,500][root][INFO] - Training Epoch: 2/2, step 3470/7134 completed (loss: 0.012649359181523323, acc: 0.9953917264938354)
[2025-02-13 03:58:56,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:56,897][root][INFO] - Training Epoch: 2/2, step 3471/7134 completed (loss: 0.059242334216833115, acc: 0.9861878156661987)
[2025-02-13 03:58:57,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:57,321][root][INFO] - Training Epoch: 2/2, step 3472/7134 completed (loss: 0.013227468356490135, acc: 0.997019350528717)
[2025-02-13 03:58:57,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:57,787][root][INFO] - Training Epoch: 2/2, step 3473/7134 completed (loss: 0.02853730134665966, acc: 0.9884105920791626)
[2025-02-13 03:58:57,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:58,169][root][INFO] - Training Epoch: 2/2, step 3474/7134 completed (loss: 0.047447096556425095, acc: 0.9911660552024841)
[2025-02-13 03:58:58,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:58,574][root][INFO] - Training Epoch: 2/2, step 3475/7134 completed (loss: 0.02224770374596119, acc: 0.9946714043617249)
[2025-02-13 03:58:58,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:58,993][root][INFO] - Training Epoch: 2/2, step 3476/7134 completed (loss: 0.04457397386431694, acc: 0.9811643958091736)
[2025-02-13 03:58:59,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:59,402][root][INFO] - Training Epoch: 2/2, step 3477/7134 completed (loss: 0.058892589062452316, acc: 0.9868420958518982)
[2025-02-13 03:58:59,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:59,810][root][INFO] - Training Epoch: 2/2, step 3478/7134 completed (loss: 0.016467444598674774, acc: 0.9955089688301086)
[2025-02-13 03:58:59,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:00,223][root][INFO] - Training Epoch: 2/2, step 3479/7134 completed (loss: 0.005960073322057724, acc: 0.998487114906311)
[2025-02-13 03:59:00,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:00,663][root][INFO] - Training Epoch: 2/2, step 3480/7134 completed (loss: 0.02639356069266796, acc: 0.9931880235671997)
[2025-02-13 03:59:00,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:01,070][root][INFO] - Training Epoch: 2/2, step 3481/7134 completed (loss: 0.02507888153195381, acc: 0.9946808218955994)
[2025-02-13 03:59:01,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:01,465][root][INFO] - Training Epoch: 2/2, step 3482/7134 completed (loss: 0.01989913359284401, acc: 0.9964028596878052)
[2025-02-13 03:59:01,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:01,890][root][INFO] - Training Epoch: 2/2, step 3483/7134 completed (loss: 0.0036681003402918577, acc: 1.0)
[2025-02-13 03:59:02,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:02,282][root][INFO] - Training Epoch: 2/2, step 3484/7134 completed (loss: 0.0245343204587698, acc: 0.9941002726554871)
[2025-02-13 03:59:02,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:02,663][root][INFO] - Training Epoch: 2/2, step 3485/7134 completed (loss: 0.02425439842045307, acc: 0.9927849769592285)
[2025-02-13 03:59:02,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:03,077][root][INFO] - Training Epoch: 2/2, step 3486/7134 completed (loss: 0.01025359332561493, acc: 0.9982014298439026)
[2025-02-13 03:59:03,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:03,492][root][INFO] - Training Epoch: 2/2, step 3487/7134 completed (loss: 0.026204636320471764, acc: 0.9867452383041382)
[2025-02-13 03:59:03,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:03,912][root][INFO] - Training Epoch: 2/2, step 3488/7134 completed (loss: 0.018911629915237427, acc: 0.9950658082962036)
[2025-02-13 03:59:04,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:04,298][root][INFO] - Training Epoch: 2/2, step 3489/7134 completed (loss: 0.05078168585896492, acc: 0.9852670431137085)
[2025-02-13 03:59:04,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:04,747][root][INFO] - Training Epoch: 2/2, step 3490/7134 completed (loss: 0.056285981088876724, acc: 0.9896602630615234)
[2025-02-13 03:59:04,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:05,162][root][INFO] - Training Epoch: 2/2, step 3491/7134 completed (loss: 0.039194051176309586, acc: 0.9914893507957458)
[2025-02-13 03:59:05,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:05,514][root][INFO] - Training Epoch: 2/2, step 3492/7134 completed (loss: 0.027152398601174355, acc: 0.9917864203453064)
[2025-02-13 03:59:05,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:05,940][root][INFO] - Training Epoch: 2/2, step 3493/7134 completed (loss: 0.013003334403038025, acc: 0.9971988797187805)
[2025-02-13 03:59:06,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:06,322][root][INFO] - Training Epoch: 2/2, step 3494/7134 completed (loss: 0.02965734153985977, acc: 0.9931034445762634)
[2025-02-13 03:59:06,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:06,717][root][INFO] - Training Epoch: 2/2, step 3495/7134 completed (loss: 0.009349847212433815, acc: 0.9976019263267517)
[2025-02-13 03:59:06,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:07,093][root][INFO] - Training Epoch: 2/2, step 3496/7134 completed (loss: 0.015754885971546173, acc: 0.9954954981803894)
[2025-02-13 03:59:07,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:07,488][root][INFO] - Training Epoch: 2/2, step 3497/7134 completed (loss: 0.03189801797270775, acc: 0.9896142482757568)
[2025-02-13 03:59:07,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:07,905][root][INFO] - Training Epoch: 2/2, step 3498/7134 completed (loss: 0.02929426170885563, acc: 0.9862805008888245)
[2025-02-13 03:59:08,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:08,315][root][INFO] - Training Epoch: 2/2, step 3499/7134 completed (loss: 0.009783553890883923, acc: 1.0)
[2025-02-13 03:59:08,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:08,729][root][INFO] - Training Epoch: 2/2, step 3500/7134 completed (loss: 0.04769653081893921, acc: 0.98531574010849)
[2025-02-13 03:59:08,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:09,142][root][INFO] - Training Epoch: 2/2, step 3501/7134 completed (loss: 0.08014735579490662, acc: 0.9758551120758057)
[2025-02-13 03:59:09,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:09,534][root][INFO] - Training Epoch: 2/2, step 3502/7134 completed (loss: 0.08868226408958435, acc: 0.9754385948181152)
[2025-02-13 03:59:09,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:09,947][root][INFO] - Training Epoch: 2/2, step 3503/7134 completed (loss: 0.040420956909656525, acc: 0.9861351847648621)
[2025-02-13 03:59:10,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:10,356][root][INFO] - Training Epoch: 2/2, step 3504/7134 completed (loss: 0.008727118372917175, acc: 0.9969834089279175)
[2025-02-13 03:59:10,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:10,774][root][INFO] - Training Epoch: 2/2, step 3505/7134 completed (loss: 0.023411830887198448, acc: 0.9964221715927124)
[2025-02-13 03:59:10,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:11,124][root][INFO] - Training Epoch: 2/2, step 3506/7134 completed (loss: 0.026592975482344627, acc: 0.992438554763794)
[2025-02-13 03:59:11,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:11,549][root][INFO] - Training Epoch: 2/2, step 3507/7134 completed (loss: 0.00818716362118721, acc: 0.996688723564148)
[2025-02-13 03:59:11,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:11,963][root][INFO] - Training Epoch: 2/2, step 3508/7134 completed (loss: 0.02503911592066288, acc: 0.9956140518188477)
[2025-02-13 03:59:12,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:12,389][root][INFO] - Training Epoch: 2/2, step 3509/7134 completed (loss: 0.01667059399187565, acc: 0.9944751262664795)
[2025-02-13 03:59:12,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:12,767][root][INFO] - Training Epoch: 2/2, step 3510/7134 completed (loss: 0.02405916340649128, acc: 0.989708423614502)
[2025-02-13 03:59:12,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:13,184][root][INFO] - Training Epoch: 2/2, step 3511/7134 completed (loss: 0.007388652767986059, acc: 0.9981982111930847)
[2025-02-13 03:59:13,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:13,586][root][INFO] - Training Epoch: 2/2, step 3512/7134 completed (loss: 0.03880750387907028, acc: 0.9841521382331848)
[2025-02-13 03:59:13,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:14,001][root][INFO] - Training Epoch: 2/2, step 3513/7134 completed (loss: 0.01709505170583725, acc: 0.9964601993560791)
[2025-02-13 03:59:14,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:14,447][root][INFO] - Training Epoch: 2/2, step 3514/7134 completed (loss: 0.022676024585962296, acc: 0.9965437650680542)
[2025-02-13 03:59:14,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:14,855][root][INFO] - Training Epoch: 2/2, step 3515/7134 completed (loss: 0.00499491672962904, acc: 1.0)
[2025-02-13 03:59:15,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:15,315][root][INFO] - Training Epoch: 2/2, step 3516/7134 completed (loss: 0.018216047435998917, acc: 0.9945828914642334)
[2025-02-13 03:59:15,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:15,760][root][INFO] - Training Epoch: 2/2, step 3517/7134 completed (loss: 0.005363333038985729, acc: 0.9988080859184265)
[2025-02-13 03:59:15,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:16,200][root][INFO] - Training Epoch: 2/2, step 3518/7134 completed (loss: 0.016582805663347244, acc: 0.9926062822341919)
[2025-02-13 03:59:16,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:16,644][root][INFO] - Training Epoch: 2/2, step 3519/7134 completed (loss: 0.01956961862742901, acc: 0.9943820238113403)
[2025-02-13 03:59:16,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:17,097][root][INFO] - Training Epoch: 2/2, step 3520/7134 completed (loss: 0.05654165521264076, acc: 0.9858823418617249)
[2025-02-13 03:59:17,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:17,538][root][INFO] - Training Epoch: 2/2, step 3521/7134 completed (loss: 0.00878438912332058, acc: 0.9957325458526611)
[2025-02-13 03:59:17,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:17,925][root][INFO] - Training Epoch: 2/2, step 3522/7134 completed (loss: 0.008838697336614132, acc: 0.9973261952400208)
[2025-02-13 03:59:18,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:18,368][root][INFO] - Training Epoch: 2/2, step 3523/7134 completed (loss: 0.004657391458749771, acc: 1.0)
[2025-02-13 03:59:18,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:18,808][root][INFO] - Training Epoch: 2/2, step 3524/7134 completed (loss: 0.03255076706409454, acc: 0.9947916865348816)
[2025-02-13 03:59:18,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:19,243][root][INFO] - Training Epoch: 2/2, step 3525/7134 completed (loss: 0.009242760948836803, acc: 0.9950576424598694)
[2025-02-13 03:59:19,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:19,681][root][INFO] - Training Epoch: 2/2, step 3526/7134 completed (loss: 0.04195994511246681, acc: 0.9923954606056213)
[2025-02-13 03:59:19,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:20,142][root][INFO] - Training Epoch: 2/2, step 3527/7134 completed (loss: 0.02268003113567829, acc: 0.994878351688385)
[2025-02-13 03:59:20,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:20,593][root][INFO] - Training Epoch: 2/2, step 3528/7134 completed (loss: 0.0028135899920016527, acc: 1.0)
[2025-02-13 03:59:20,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:21,020][root][INFO] - Training Epoch: 2/2, step 3529/7134 completed (loss: 0.006872531026601791, acc: 1.0)
[2025-02-13 03:59:21,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:21,490][root][INFO] - Training Epoch: 2/2, step 3530/7134 completed (loss: 0.022103719413280487, acc: 0.9954128265380859)
[2025-02-13 03:59:21,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:21,927][root][INFO] - Training Epoch: 2/2, step 3531/7134 completed (loss: 0.014456531964242458, acc: 0.9954819083213806)
[2025-02-13 03:59:22,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:22,370][root][INFO] - Training Epoch: 2/2, step 3532/7134 completed (loss: 0.003373779123649001, acc: 1.0)
[2025-02-13 03:59:22,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:22,808][root][INFO] - Training Epoch: 2/2, step 3533/7134 completed (loss: 0.011660600081086159, acc: 0.9988358616828918)
[2025-02-13 03:59:22,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:23,230][root][INFO] - Training Epoch: 2/2, step 3534/7134 completed (loss: 0.03887591138482094, acc: 0.9902642369270325)
[2025-02-13 03:59:23,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:23,654][root][INFO] - Training Epoch: 2/2, step 3535/7134 completed (loss: 0.01770576275885105, acc: 0.9953917264938354)
[2025-02-13 03:59:23,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:24,082][root][INFO] - Training Epoch: 2/2, step 3536/7134 completed (loss: 0.0464627742767334, acc: 0.9904371500015259)
[2025-02-13 03:59:24,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:24,474][root][INFO] - Training Epoch: 2/2, step 3537/7134 completed (loss: 0.02111811190843582, acc: 0.994966447353363)
[2025-02-13 03:59:24,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:24,862][root][INFO] - Training Epoch: 2/2, step 3538/7134 completed (loss: 0.04416907951235771, acc: 0.9923954606056213)
[2025-02-13 03:59:24,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:25,184][root][INFO] - Training Epoch: 2/2, step 3539/7134 completed (loss: 0.010568266734480858, acc: 0.9975728392601013)
[2025-02-13 03:59:25,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:25,571][root][INFO] - Training Epoch: 2/2, step 3540/7134 completed (loss: 0.04750819504261017, acc: 0.9833080172538757)
[2025-02-13 03:59:25,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:25,988][root][INFO] - Training Epoch: 2/2, step 3541/7134 completed (loss: 0.022013362497091293, acc: 0.9941262602806091)
[2025-02-13 03:59:26,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:26,394][root][INFO] - Training Epoch: 2/2, step 3542/7134 completed (loss: 0.021962115541100502, acc: 0.9969230890274048)
[2025-02-13 03:59:26,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:26,781][root][INFO] - Training Epoch: 2/2, step 3543/7134 completed (loss: 0.04716096818447113, acc: 0.9894366264343262)
[2025-02-13 03:59:26,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:27,217][root][INFO] - Training Epoch: 2/2, step 3544/7134 completed (loss: 0.030961288139224052, acc: 0.9923076629638672)
[2025-02-13 03:59:27,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:27,663][root][INFO] - Training Epoch: 2/2, step 3545/7134 completed (loss: 0.010037839412689209, acc: 0.9984471797943115)
[2025-02-13 03:59:27,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:28,080][root][INFO] - Training Epoch: 2/2, step 3546/7134 completed (loss: 0.023852292448282242, acc: 0.9933993220329285)
[2025-02-13 03:59:28,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:28,525][root][INFO] - Training Epoch: 2/2, step 3547/7134 completed (loss: 0.04317981004714966, acc: 0.9832167625427246)
[2025-02-13 03:59:28,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:28,945][root][INFO] - Training Epoch: 2/2, step 3548/7134 completed (loss: 0.01943828910589218, acc: 0.9933628439903259)
[2025-02-13 03:59:29,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:29,345][root][INFO] - Training Epoch: 2/2, step 3549/7134 completed (loss: 0.03272606059908867, acc: 0.9894179701805115)
[2025-02-13 03:59:29,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:29,740][root][INFO] - Training Epoch: 2/2, step 3550/7134 completed (loss: 0.018215790390968323, acc: 0.9947460889816284)
[2025-02-13 03:59:29,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:30,142][root][INFO] - Training Epoch: 2/2, step 3551/7134 completed (loss: 0.021327173337340355, acc: 0.9932659864425659)
[2025-02-13 03:59:30,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:30,572][root][INFO] - Training Epoch: 2/2, step 3552/7134 completed (loss: 0.0986253172159195, acc: 0.9700374603271484)
[2025-02-13 03:59:30,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:30,974][root][INFO] - Training Epoch: 2/2, step 3553/7134 completed (loss: 0.06670829653739929, acc: 0.9851973652839661)
[2025-02-13 03:59:31,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:31,384][root][INFO] - Training Epoch: 2/2, step 3554/7134 completed (loss: 0.0440407358109951, acc: 0.9894578456878662)
[2025-02-13 03:59:31,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:31,780][root][INFO] - Training Epoch: 2/2, step 3555/7134 completed (loss: 0.029963204637169838, acc: 0.9921507239341736)
[2025-02-13 03:59:31,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:32,159][root][INFO] - Training Epoch: 2/2, step 3556/7134 completed (loss: 0.03475795313715935, acc: 0.9904030561447144)
[2025-02-13 03:59:32,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:32,601][root][INFO] - Training Epoch: 2/2, step 3557/7134 completed (loss: 0.02179133892059326, acc: 0.996370255947113)
[2025-02-13 03:59:32,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:33,003][root][INFO] - Training Epoch: 2/2, step 3558/7134 completed (loss: 0.04335227981209755, acc: 0.9876288771629333)
[2025-02-13 03:59:33,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:33,414][root][INFO] - Training Epoch: 2/2, step 3559/7134 completed (loss: 0.011806491762399673, acc: 0.9985465407371521)
[2025-02-13 03:59:33,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:33,823][root][INFO] - Training Epoch: 2/2, step 3560/7134 completed (loss: 0.01286003552377224, acc: 0.9972602725028992)
[2025-02-13 03:59:33,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:34,249][root][INFO] - Training Epoch: 2/2, step 3561/7134 completed (loss: 0.045428209006786346, acc: 0.9859943985939026)
[2025-02-13 03:59:34,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:34,648][root][INFO] - Training Epoch: 2/2, step 3562/7134 completed (loss: 0.031116781756281853, acc: 0.9892857074737549)
[2025-02-13 03:59:34,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:35,053][root][INFO] - Training Epoch: 2/2, step 3563/7134 completed (loss: 0.019052857533097267, acc: 0.995230495929718)
[2025-02-13 03:59:35,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:36,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:36,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:36,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:37,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:37,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:37,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:37,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:38,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:38,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:39,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:39,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:39,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:40,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:40,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:40,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:41,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:41,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:41,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:42,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:42,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:42,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:42,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:43,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:43,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:43,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:44,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:44,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:44,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:45,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:45,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:46,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:46,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:46,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:47,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:47,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:47,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:48,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:48,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:49,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:49,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:49,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:50,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:50,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:50,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:51,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:51,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:52,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:52,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:52,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:53,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:53,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:53,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:54,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:54,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:54,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:55,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:55,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:55,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:56,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:56,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:56,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:57,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:57,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:57,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:58,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:58,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:58,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:59,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:59,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:59,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:00,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:00,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:00,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:01,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:01,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:01,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:02,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:02,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:02,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:03,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:03,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:03,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:04,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:04,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:04,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:05,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:05,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:06,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:06,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:06,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:07,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:07,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:07,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:08,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:08,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:08,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:09,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:09,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:09,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:10,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:10,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:10,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:11,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:11,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:11,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:12,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:12,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:12,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:13,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:13,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:13,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:14,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:14,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:15,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:15,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:15,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:15,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:16,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:16,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:16,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:17,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:17,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:17,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:18,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:18,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:19,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:19,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:19,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:20,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:20,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:20,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:21,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:21,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:21,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:22,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:22,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:22,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:23,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:23,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:24,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:24,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:24,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:25,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:25,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:25,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:26,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:26,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:26,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:27,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:27,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:27,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:28,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:28,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:28,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:29,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:29,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:30,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:30,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:30,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:31,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:31,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:31,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:32,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:32,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:33,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:33,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:33,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:34,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:34,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:35,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:35,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:35,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:36,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:36,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:37,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:37,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:37,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:38,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:38,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:38,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:39,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:39,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:39,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:40,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:40,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:40,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:41,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:41,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:41,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:42,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:42,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:42,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:43,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:43,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:43,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:44,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:44,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:44,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:45,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:45,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:46,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:46,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:46,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:47,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:47,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:47,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:48,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:48,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:48,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:49,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:49,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:49,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:50,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:50,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:50,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:51,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:51,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:51,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:52,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:52,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:52,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:53,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:53,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:53,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:54,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:54,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:54,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:55,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:55,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:55,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:56,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:56,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:56,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:57,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:57,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:57,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:58,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:58,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:58,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:59,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:59,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:59,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:00,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:00,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:00,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:01,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:01,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:02,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:02,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:03,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:03,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:03,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:04,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:04,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:04,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:05,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:05,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:05,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:06,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:06,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:06,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:07,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:07,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:07,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:08,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:08,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:09,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:09,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:09,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:10,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:10,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:11,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:11,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:11,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:12,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:12,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:12,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:13,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:13,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:13,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:14,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:14,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:15,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:15,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:15,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:16,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:16,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:17,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:17,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:18,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:18,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:18,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:19,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:19,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:20,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:20,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:20,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:21,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:21,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:21,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:22,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:22,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:22,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:23,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:23,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:23,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:24,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:24,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:24,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:25,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:25,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:25,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:26,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:26,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:26,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:27,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:27,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:27,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:28,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:28,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:28,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:29,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:29,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:29,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:30,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:30,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:30,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:31,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:31,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:32,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:32,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:32,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:33,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:33,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:33,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:34,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:34,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:34,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:35,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:35,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:36,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:36,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:36,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:37,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:37,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:37,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:38,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:38,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:38,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:39,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:39,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:40,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:40,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:40,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:41,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:41,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:41,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:42,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:42,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:43,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:43,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:44,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:44,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:45,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:45,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:46,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:46,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:46,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:47,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:47,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:47,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:48,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:48,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:48,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:49,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:49,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:49,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:50,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:50,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:50,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:51,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:51,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:51,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:52,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:52,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:52,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:53,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:53,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:53,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:54,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:54,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:54,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:55,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:55,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:55,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:56,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:56,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:56,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:57,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:57,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:57,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:58,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:58,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:58,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:58,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:59,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:59,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:59,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:00,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:00,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:00,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:01,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:01,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:02,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:02,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:02,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:03,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:03,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:03,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:03,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:04,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:04,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:04,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:05,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:05,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:05,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:06,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:06,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:06,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:07,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:07,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:07,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:08,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:08,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:08,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:09,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:09,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:09,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:10,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:10,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:11,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:11,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:11,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:12,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:12,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:12,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:13,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:13,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:13,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:14,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:14,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:14,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:15,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:15,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:15,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:16,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:16,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:16,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:17,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:17,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:17,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:18,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:18,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:18,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:19,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:19,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:19,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:20,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:20,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:20,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:21,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:21,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:21,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:22,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:22,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:22,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:23,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:23,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:23,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:24,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:24,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:24,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:25,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:25,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:25,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:26,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:26,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:27,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:27,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:27,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:28,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:28,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:28,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:29,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:29,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:29,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:30,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:30,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:30,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:31,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:31,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:32,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:32,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:32,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:33,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:33,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:33,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:34,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:34,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:34,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:35,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:35,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:35,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:36,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:36,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:36,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:37,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:37,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:37,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:38,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:38,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:38,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:39,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:39,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:39,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:40,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:40,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:41,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:41,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:41,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:42,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:42,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:42,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:43,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:43,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:44,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:44,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:44,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:45,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:45,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:45,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:46,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:46,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:46,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:47,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:47,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:47,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:48,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:48,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:48,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:49,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:49,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:49,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:50,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:50,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:50,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:51,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:51,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:51,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:52,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:52,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:52,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:53,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:53,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:53,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:54,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:54,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:54,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:55,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:55,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:56,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:56,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:56,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:57,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:57,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:57,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:58,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:58,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:58,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:59,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:59,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:00,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:00,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:01,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:01,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:01,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:02,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:02,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:02,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:03,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:03,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:03,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:04,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:04,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:04,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:05,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:05,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:05,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:06,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:06,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:06,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:06,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:07,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:07,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:08,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:08,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:08,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:09,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:09,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:09,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:10,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:10,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:10,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:11,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:11,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:11,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:12,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:12,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:12,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:13,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:13,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:13,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:14,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:14,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:15,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:15,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:16,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:16,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:17,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:17,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:18,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:18,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:19,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:19,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:19,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:20,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:20,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:20,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:21,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:21,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:21,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:22,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:22,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:22,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:23,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:23,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:24,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:24,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:24,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:25,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:25,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:25,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:26,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:26,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:26,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:27,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:27,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:27,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:28,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:28,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:28,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:29,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:29,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:29,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:30,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:30,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:30,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:31,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:31,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:31,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:32,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:32,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:32,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:33,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:33,991][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0486, device='cuda:0') eval_epoch_loss=tensor(0.0474, device='cuda:0') eval_epoch_acc=tensor(0.9867, device='cuda:0')
[2025-02-13 04:03:33,993][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 04:03:33,993][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 04:03:34,324][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_3564_loss_0.04742717742919922/model.pt
[2025-02-13 04:03:34,330][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 04:03:34,330][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.04742717742919922
[2025-02-13 04:03:34,331][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9866829514503479
[2025-02-13 04:03:34,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:34,814][root][INFO] - Training Epoch: 2/2, step 3564/7134 completed (loss: 0.05050092563033104, acc: 0.9924812316894531)
[2025-02-13 04:03:34,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:35,220][root][INFO] - Training Epoch: 2/2, step 3565/7134 completed (loss: 0.07097481936216354, acc: 0.9774436354637146)
[2025-02-13 04:03:35,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:35,626][root][INFO] - Training Epoch: 2/2, step 3566/7134 completed (loss: 0.05096496269106865, acc: 0.9820716977119446)
[2025-02-13 04:03:35,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:36,032][root][INFO] - Training Epoch: 2/2, step 3567/7134 completed (loss: 0.08381090313196182, acc: 0.9818887710571289)
[2025-02-13 04:03:36,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:36,441][root][INFO] - Training Epoch: 2/2, step 3568/7134 completed (loss: 0.08398986607789993, acc: 0.9803600907325745)
[2025-02-13 04:03:36,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:36,797][root][INFO] - Training Epoch: 2/2, step 3569/7134 completed (loss: 0.042781054973602295, acc: 0.9874551892280579)
[2025-02-13 04:03:36,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:37,224][root][INFO] - Training Epoch: 2/2, step 3570/7134 completed (loss: 0.08062272518873215, acc: 0.9851239919662476)
[2025-02-13 04:03:37,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:37,641][root][INFO] - Training Epoch: 2/2, step 3571/7134 completed (loss: 0.06229456141591072, acc: 0.9867374300956726)
[2025-02-13 04:03:37,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:38,092][root][INFO] - Training Epoch: 2/2, step 3572/7134 completed (loss: 0.01576777920126915, acc: 0.9986824989318848)
[2025-02-13 04:03:38,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:38,523][root][INFO] - Training Epoch: 2/2, step 3573/7134 completed (loss: 0.04642920196056366, acc: 0.9866666793823242)
[2025-02-13 04:03:38,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:38,920][root][INFO] - Training Epoch: 2/2, step 3574/7134 completed (loss: 0.05161648243665695, acc: 0.9789271950721741)
[2025-02-13 04:03:39,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:39,327][root][INFO] - Training Epoch: 2/2, step 3575/7134 completed (loss: 0.09023363143205643, acc: 0.9742709994316101)
[2025-02-13 04:03:39,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:39,728][root][INFO] - Training Epoch: 2/2, step 3576/7134 completed (loss: 0.05888393148779869, acc: 0.9834254384040833)
[2025-02-13 04:03:39,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:40,204][root][INFO] - Training Epoch: 2/2, step 3577/7134 completed (loss: 0.03810145705938339, acc: 0.9886220097541809)
[2025-02-13 04:03:40,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:40,639][root][INFO] - Training Epoch: 2/2, step 3578/7134 completed (loss: 0.04066535085439682, acc: 0.9854439496994019)
[2025-02-13 04:03:40,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:41,056][root][INFO] - Training Epoch: 2/2, step 3579/7134 completed (loss: 0.03616499900817871, acc: 0.9866666793823242)
[2025-02-13 04:03:41,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:41,468][root][INFO] - Training Epoch: 2/2, step 3580/7134 completed (loss: 0.06189548596739769, acc: 0.9795275330543518)
[2025-02-13 04:03:41,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:41,888][root][INFO] - Training Epoch: 2/2, step 3581/7134 completed (loss: 0.06972012668848038, acc: 0.9808542132377625)
[2025-02-13 04:03:41,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:42,229][root][INFO] - Training Epoch: 2/2, step 3582/7134 completed (loss: 0.05649630352854729, acc: 0.9810426831245422)
[2025-02-13 04:03:42,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:42,664][root][INFO] - Training Epoch: 2/2, step 3583/7134 completed (loss: 0.06079593300819397, acc: 0.9817629456520081)
[2025-02-13 04:03:42,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:43,000][root][INFO] - Training Epoch: 2/2, step 3584/7134 completed (loss: 0.06324934214353561, acc: 0.9894179701805115)
[2025-02-13 04:03:43,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:43,420][root][INFO] - Training Epoch: 2/2, step 3585/7134 completed (loss: 0.04879986494779587, acc: 0.984375)
[2025-02-13 04:03:43,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:43,824][root][INFO] - Training Epoch: 2/2, step 3586/7134 completed (loss: 0.0432872548699379, acc: 0.9871588945388794)
[2025-02-13 04:03:43,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:44,233][root][INFO] - Training Epoch: 2/2, step 3587/7134 completed (loss: 0.042055822908878326, acc: 0.989847719669342)
[2025-02-13 04:03:44,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:44,680][root][INFO] - Training Epoch: 2/2, step 3588/7134 completed (loss: 0.06156131252646446, acc: 0.9866864085197449)
[2025-02-13 04:03:44,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:45,103][root][INFO] - Training Epoch: 2/2, step 3589/7134 completed (loss: 0.06236536055803299, acc: 0.9909228682518005)
[2025-02-13 04:03:45,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:45,543][root][INFO] - Training Epoch: 2/2, step 3590/7134 completed (loss: 0.02050282619893551, acc: 0.992977499961853)
[2025-02-13 04:03:45,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:45,928][root][INFO] - Training Epoch: 2/2, step 3591/7134 completed (loss: 0.030009040609002113, acc: 0.9894894957542419)
[2025-02-13 04:03:46,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:46,334][root][INFO] - Training Epoch: 2/2, step 3592/7134 completed (loss: 0.016787204891443253, acc: 0.9917355179786682)
[2025-02-13 04:03:46,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:46,767][root][INFO] - Training Epoch: 2/2, step 3593/7134 completed (loss: 0.041960228234529495, acc: 0.9894921183586121)
[2025-02-13 04:03:46,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:47,178][root][INFO] - Training Epoch: 2/2, step 3594/7134 completed (loss: 0.022250209003686905, acc: 0.9918830990791321)
[2025-02-13 04:03:47,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:47,612][root][INFO] - Training Epoch: 2/2, step 3595/7134 completed (loss: 0.038698066025972366, acc: 0.9846860766410828)
[2025-02-13 04:03:47,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:48,028][root][INFO] - Training Epoch: 2/2, step 3596/7134 completed (loss: 0.03927686810493469, acc: 0.9903714060783386)
[2025-02-13 04:03:48,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:48,477][root][INFO] - Training Epoch: 2/2, step 3597/7134 completed (loss: 0.02743903174996376, acc: 0.9928057789802551)
[2025-02-13 04:03:48,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:48,965][root][INFO] - Training Epoch: 2/2, step 3598/7134 completed (loss: 0.049976348876953125, acc: 0.97826087474823)
[2025-02-13 04:03:49,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:49,413][root][INFO] - Training Epoch: 2/2, step 3599/7134 completed (loss: 0.044765666127204895, acc: 0.9886506795883179)
[2025-02-13 04:03:49,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:49,874][root][INFO] - Training Epoch: 2/2, step 3600/7134 completed (loss: 0.04942534491419792, acc: 0.9865319728851318)
[2025-02-13 04:03:50,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:50,314][root][INFO] - Training Epoch: 2/2, step 3601/7134 completed (loss: 0.11103057116270065, acc: 0.9724137783050537)
[2025-02-13 04:03:50,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:50,766][root][INFO] - Training Epoch: 2/2, step 3602/7134 completed (loss: 0.07366305589675903, acc: 0.9766627550125122)
[2025-02-13 04:03:50,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:51,170][root][INFO] - Training Epoch: 2/2, step 3603/7134 completed (loss: 0.02377256378531456, acc: 0.9859374761581421)
[2025-02-13 04:03:51,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:51,541][root][INFO] - Training Epoch: 2/2, step 3604/7134 completed (loss: 0.0343436561524868, acc: 0.9943714737892151)
[2025-02-13 04:03:51,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:52,002][root][INFO] - Training Epoch: 2/2, step 3605/7134 completed (loss: 0.04334370791912079, acc: 0.9896480441093445)
[2025-02-13 04:03:52,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:52,440][root][INFO] - Training Epoch: 2/2, step 3606/7134 completed (loss: 0.04575903341174126, acc: 0.9893428087234497)
[2025-02-13 04:03:52,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:52,875][root][INFO] - Training Epoch: 2/2, step 3607/7134 completed (loss: 0.031026309356093407, acc: 0.9899328947067261)
[2025-02-13 04:03:53,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:53,334][root][INFO] - Training Epoch: 2/2, step 3608/7134 completed (loss: 0.07830328494310379, acc: 0.9811946749687195)
[2025-02-13 04:03:53,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:53,794][root][INFO] - Training Epoch: 2/2, step 3609/7134 completed (loss: 0.0397263839840889, acc: 0.989847719669342)
[2025-02-13 04:03:53,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:54,243][root][INFO] - Training Epoch: 2/2, step 3610/7134 completed (loss: 0.04915875568985939, acc: 0.9818181991577148)
[2025-02-13 04:03:54,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:54,687][root][INFO] - Training Epoch: 2/2, step 3611/7134 completed (loss: 0.04731297120451927, acc: 0.9865591526031494)
[2025-02-13 04:03:54,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:55,170][root][INFO] - Training Epoch: 2/2, step 3612/7134 completed (loss: 0.07802607119083405, acc: 0.9771783947944641)
[2025-02-13 04:03:55,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:55,631][root][INFO] - Training Epoch: 2/2, step 3613/7134 completed (loss: 0.05208626016974449, acc: 0.9809160232543945)
[2025-02-13 04:03:55,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:56,050][root][INFO] - Training Epoch: 2/2, step 3614/7134 completed (loss: 0.03931190073490143, acc: 0.9879518151283264)
[2025-02-13 04:03:56,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:56,487][root][INFO] - Training Epoch: 2/2, step 3615/7134 completed (loss: 0.0216328427195549, acc: 0.9969419240951538)
[2025-02-13 04:03:56,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:56,906][root][INFO] - Training Epoch: 2/2, step 3616/7134 completed (loss: 0.03595692291855812, acc: 0.9858712553977966)
[2025-02-13 04:03:57,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:57,361][root][INFO] - Training Epoch: 2/2, step 3617/7134 completed (loss: 0.03871874138712883, acc: 0.9876237511634827)
[2025-02-13 04:03:57,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:57,767][root][INFO] - Training Epoch: 2/2, step 3618/7134 completed (loss: 0.09279533475637436, acc: 0.9689542651176453)
[2025-02-13 04:03:57,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:58,217][root][INFO] - Training Epoch: 2/2, step 3619/7134 completed (loss: 0.08903531730175018, acc: 0.9702380895614624)
[2025-02-13 04:03:58,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:58,682][root][INFO] - Training Epoch: 2/2, step 3620/7134 completed (loss: 0.04454800486564636, acc: 0.9873417615890503)
[2025-02-13 04:03:58,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:59,094][root][INFO] - Training Epoch: 2/2, step 3621/7134 completed (loss: 0.029714545235037804, acc: 0.9902912378311157)
[2025-02-13 04:03:59,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:59,549][root][INFO] - Training Epoch: 2/2, step 3622/7134 completed (loss: 0.030613910406827927, acc: 0.9909909963607788)
[2025-02-13 04:03:59,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:59,892][root][INFO] - Training Epoch: 2/2, step 3623/7134 completed (loss: 0.014231120236217976, acc: 0.9925650358200073)
[2025-02-13 04:04:00,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:00,312][root][INFO] - Training Epoch: 2/2, step 3624/7134 completed (loss: 0.019290341064333916, acc: 0.996666669845581)
[2025-02-13 04:04:00,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:00,677][root][INFO] - Training Epoch: 2/2, step 3625/7134 completed (loss: 0.07097627967596054, acc: 0.9798792600631714)
[2025-02-13 04:04:00,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:01,035][root][INFO] - Training Epoch: 2/2, step 3626/7134 completed (loss: 0.15009033679962158, acc: 0.9599999785423279)
[2025-02-13 04:04:01,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:01,436][root][INFO] - Training Epoch: 2/2, step 3627/7134 completed (loss: 0.08372657001018524, acc: 0.9810874462127686)
[2025-02-13 04:04:01,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:01,804][root][INFO] - Training Epoch: 2/2, step 3628/7134 completed (loss: 0.12711845338344574, acc: 0.96875)
[2025-02-13 04:04:01,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:02,144][root][INFO] - Training Epoch: 2/2, step 3629/7134 completed (loss: 0.09925603121519089, acc: 0.977642297744751)
[2025-02-13 04:04:02,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:02,492][root][INFO] - Training Epoch: 2/2, step 3630/7134 completed (loss: 0.07914973050355911, acc: 0.9793281555175781)
[2025-02-13 04:04:02,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:02,909][root][INFO] - Training Epoch: 2/2, step 3631/7134 completed (loss: 0.06543571501970291, acc: 0.9819672107696533)
[2025-02-13 04:04:03,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:03,320][root][INFO] - Training Epoch: 2/2, step 3632/7134 completed (loss: 0.03939756378531456, acc: 0.9899396300315857)
[2025-02-13 04:04:03,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:03,728][root][INFO] - Training Epoch: 2/2, step 3633/7134 completed (loss: 0.0856049656867981, acc: 0.9842180609703064)
[2025-02-13 04:04:03,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:04,120][root][INFO] - Training Epoch: 2/2, step 3634/7134 completed (loss: 0.10049837082624435, acc: 0.97773277759552)
[2025-02-13 04:04:04,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:04,525][root][INFO] - Training Epoch: 2/2, step 3635/7134 completed (loss: 0.027557598426938057, acc: 0.9910394549369812)
[2025-02-13 04:04:04,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:04,927][root][INFO] - Training Epoch: 2/2, step 3636/7134 completed (loss: 0.028902120888233185, acc: 0.9901574850082397)
[2025-02-13 04:04:05,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:05,343][root][INFO] - Training Epoch: 2/2, step 3637/7134 completed (loss: 0.034593187272548676, acc: 0.9879275560379028)
[2025-02-13 04:04:05,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:05,734][root][INFO] - Training Epoch: 2/2, step 3638/7134 completed (loss: 0.02734357863664627, acc: 0.9929412007331848)
[2025-02-13 04:04:05,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:06,134][root][INFO] - Training Epoch: 2/2, step 3639/7134 completed (loss: 0.020188825204968452, acc: 0.9909909963607788)
[2025-02-13 04:04:06,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:06,557][root][INFO] - Training Epoch: 2/2, step 3640/7134 completed (loss: 0.03761384263634682, acc: 0.9904580116271973)
[2025-02-13 04:04:06,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:06,911][root][INFO] - Training Epoch: 2/2, step 3641/7134 completed (loss: 0.0275124479085207, acc: 0.994163453578949)
[2025-02-13 04:04:07,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:07,329][root][INFO] - Training Epoch: 2/2, step 3642/7134 completed (loss: 0.02601894736289978, acc: 0.9938176274299622)
[2025-02-13 04:04:07,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:07,736][root][INFO] - Training Epoch: 2/2, step 3643/7134 completed (loss: 0.03185954689979553, acc: 0.9948453903198242)
[2025-02-13 04:04:07,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:08,145][root][INFO] - Training Epoch: 2/2, step 3644/7134 completed (loss: 0.06523191183805466, acc: 0.9821138381958008)
[2025-02-13 04:04:08,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:08,566][root][INFO] - Training Epoch: 2/2, step 3645/7134 completed (loss: 0.048475973308086395, acc: 0.98591548204422)
[2025-02-13 04:04:08,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:08,965][root][INFO] - Training Epoch: 2/2, step 3646/7134 completed (loss: 0.10427944362163544, acc: 0.9739583134651184)
[2025-02-13 04:04:09,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:09,358][root][INFO] - Training Epoch: 2/2, step 3647/7134 completed (loss: 0.01777038350701332, acc: 0.9922480583190918)
[2025-02-13 04:04:09,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:09,805][root][INFO] - Training Epoch: 2/2, step 3648/7134 completed (loss: 0.06092432141304016, acc: 0.9807956218719482)
[2025-02-13 04:04:09,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:10,235][root][INFO] - Training Epoch: 2/2, step 3649/7134 completed (loss: 0.06587854772806168, acc: 0.981792688369751)
[2025-02-13 04:04:10,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:10,679][root][INFO] - Training Epoch: 2/2, step 3650/7134 completed (loss: 0.045515093952417374, acc: 0.9843137264251709)
[2025-02-13 04:04:10,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:11,119][root][INFO] - Training Epoch: 2/2, step 3651/7134 completed (loss: 0.06931597739458084, acc: 0.9825737476348877)
[2025-02-13 04:04:11,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:11,528][root][INFO] - Training Epoch: 2/2, step 3652/7134 completed (loss: 0.06209472194314003, acc: 0.9788838624954224)
[2025-02-13 04:04:11,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:11,949][root][INFO] - Training Epoch: 2/2, step 3653/7134 completed (loss: 0.08795849233865738, acc: 0.9743589758872986)
[2025-02-13 04:04:12,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:12,366][root][INFO] - Training Epoch: 2/2, step 3654/7134 completed (loss: 0.022222882136702538, acc: 0.9930915236473083)
[2025-02-13 04:04:12,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:12,790][root][INFO] - Training Epoch: 2/2, step 3655/7134 completed (loss: 0.05603982135653496, acc: 0.9890109896659851)
[2025-02-13 04:04:12,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:13,145][root][INFO] - Training Epoch: 2/2, step 3656/7134 completed (loss: 0.09300211071968079, acc: 0.9727272987365723)
[2025-02-13 04:04:13,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:13,563][root][INFO] - Training Epoch: 2/2, step 3657/7134 completed (loss: 0.015447304584085941, acc: 0.995192289352417)
[2025-02-13 04:04:13,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:13,984][root][INFO] - Training Epoch: 2/2, step 3658/7134 completed (loss: 0.04413947835564613, acc: 0.9922118186950684)
[2025-02-13 04:04:14,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:14,401][root][INFO] - Training Epoch: 2/2, step 3659/7134 completed (loss: 0.027852462604641914, acc: 0.9925261735916138)
[2025-02-13 04:04:14,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:14,667][root][INFO] - Training Epoch: 2/2, step 3660/7134 completed (loss: 0.05370756611227989, acc: 0.9868852496147156)
[2025-02-13 04:04:14,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:14,935][root][INFO] - Training Epoch: 2/2, step 3661/7134 completed (loss: 0.04214074835181236, acc: 0.9857142567634583)
[2025-02-13 04:04:15,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:15,384][root][INFO] - Training Epoch: 2/2, step 3662/7134 completed (loss: 0.08354926109313965, acc: 0.9756097793579102)
[2025-02-13 04:04:15,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:15,794][root][INFO] - Training Epoch: 2/2, step 3663/7134 completed (loss: 0.03390410915017128, acc: 0.9890109896659851)
[2025-02-13 04:04:15,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:16,212][root][INFO] - Training Epoch: 2/2, step 3664/7134 completed (loss: 0.054665010422468185, acc: 0.9893617033958435)
[2025-02-13 04:04:16,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:16,625][root][INFO] - Training Epoch: 2/2, step 3665/7134 completed (loss: 0.03347726911306381, acc: 0.9934318661689758)
[2025-02-13 04:04:16,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:17,030][root][INFO] - Training Epoch: 2/2, step 3666/7134 completed (loss: 0.04936600103974342, acc: 0.9817517995834351)
[2025-02-13 04:04:17,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:17,397][root][INFO] - Training Epoch: 2/2, step 3667/7134 completed (loss: 0.14946797490119934, acc: 0.965831458568573)
[2025-02-13 04:04:17,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:17,794][root][INFO] - Training Epoch: 2/2, step 3668/7134 completed (loss: 0.047976814210414886, acc: 0.9895287752151489)
[2025-02-13 04:04:17,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:18,196][root][INFO] - Training Epoch: 2/2, step 3669/7134 completed (loss: 0.01806349866092205, acc: 0.996219277381897)
[2025-02-13 04:04:18,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:18,607][root][INFO] - Training Epoch: 2/2, step 3670/7134 completed (loss: 0.03484649583697319, acc: 0.9931318759918213)
[2025-02-13 04:04:18,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:18,999][root][INFO] - Training Epoch: 2/2, step 3671/7134 completed (loss: 0.01870698854327202, acc: 0.9927745461463928)
[2025-02-13 04:04:19,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:19,450][root][INFO] - Training Epoch: 2/2, step 3672/7134 completed (loss: 0.02527049370110035, acc: 0.9949495196342468)
[2025-02-13 04:04:19,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:19,853][root][INFO] - Training Epoch: 2/2, step 3673/7134 completed (loss: 0.03889463469386101, acc: 0.9882943034172058)
[2025-02-13 04:04:19,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:20,273][root][INFO] - Training Epoch: 2/2, step 3674/7134 completed (loss: 0.013123561628162861, acc: 0.9968503713607788)
[2025-02-13 04:04:20,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:20,707][root][INFO] - Training Epoch: 2/2, step 3675/7134 completed (loss: 0.012415104545652866, acc: 0.9974489808082581)
[2025-02-13 04:04:20,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:21,123][root][INFO] - Training Epoch: 2/2, step 3676/7134 completed (loss: 0.019309107214212418, acc: 0.9941691160202026)
[2025-02-13 04:04:21,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:21,566][root][INFO] - Training Epoch: 2/2, step 3677/7134 completed (loss: 0.036060456186532974, acc: 0.9863013625144958)
[2025-02-13 04:04:21,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:21,987][root][INFO] - Training Epoch: 2/2, step 3678/7134 completed (loss: 0.02851567044854164, acc: 0.9905533194541931)
[2025-02-13 04:04:22,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:22,440][root][INFO] - Training Epoch: 2/2, step 3679/7134 completed (loss: 0.01607055589556694, acc: 0.9966942071914673)
[2025-02-13 04:04:22,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:22,869][root][INFO] - Training Epoch: 2/2, step 3680/7134 completed (loss: 0.03554455190896988, acc: 0.9886363744735718)
[2025-02-13 04:04:23,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:23,275][root][INFO] - Training Epoch: 2/2, step 3681/7134 completed (loss: 0.024272888898849487, acc: 0.9916201233863831)
[2025-02-13 04:04:23,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:23,700][root][INFO] - Training Epoch: 2/2, step 3682/7134 completed (loss: 0.01899554207921028, acc: 0.9947299361228943)
[2025-02-13 04:04:23,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:24,152][root][INFO] - Training Epoch: 2/2, step 3683/7134 completed (loss: 0.01608339138329029, acc: 0.995275616645813)
[2025-02-13 04:04:24,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:24,557][root][INFO] - Training Epoch: 2/2, step 3684/7134 completed (loss: 0.02852349542081356, acc: 0.9927849769592285)
[2025-02-13 04:04:24,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:24,955][root][INFO] - Training Epoch: 2/2, step 3685/7134 completed (loss: 0.01481443177908659, acc: 0.9922928810119629)
[2025-02-13 04:04:25,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:25,362][root][INFO] - Training Epoch: 2/2, step 3686/7134 completed (loss: 0.02442094497382641, acc: 0.9925037622451782)
[2025-02-13 04:04:25,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:25,776][root][INFO] - Training Epoch: 2/2, step 3687/7134 completed (loss: 0.011085158213973045, acc: 0.9953846335411072)
[2025-02-13 04:04:25,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:26,180][root][INFO] - Training Epoch: 2/2, step 3688/7134 completed (loss: 0.009300080128014088, acc: 0.9979715943336487)
[2025-02-13 04:04:26,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:26,598][root][INFO] - Training Epoch: 2/2, step 3689/7134 completed (loss: 0.02213752269744873, acc: 0.9934640526771545)
[2025-02-13 04:04:26,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:27,041][root][INFO] - Training Epoch: 2/2, step 3690/7134 completed (loss: 0.012138335034251213, acc: 0.9971387982368469)
[2025-02-13 04:04:27,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:27,433][root][INFO] - Training Epoch: 2/2, step 3691/7134 completed (loss: 0.025960378348827362, acc: 0.9950371980667114)
[2025-02-13 04:04:27,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:27,848][root][INFO] - Training Epoch: 2/2, step 3692/7134 completed (loss: 0.03864622116088867, acc: 0.9893454909324646)
[2025-02-13 04:04:27,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:28,269][root][INFO] - Training Epoch: 2/2, step 3693/7134 completed (loss: 0.011014861986041069, acc: 0.9952681660652161)
[2025-02-13 04:04:28,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:28,689][root][INFO] - Training Epoch: 2/2, step 3694/7134 completed (loss: 0.021407125517725945, acc: 0.9957325458526611)
[2025-02-13 04:04:28,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:29,120][root][INFO] - Training Epoch: 2/2, step 3695/7134 completed (loss: 0.022413654252886772, acc: 0.9929178357124329)
[2025-02-13 04:04:29,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:29,571][root][INFO] - Training Epoch: 2/2, step 3696/7134 completed (loss: 0.022162389010190964, acc: 0.9922879338264465)
[2025-02-13 04:04:29,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:29,967][root][INFO] - Training Epoch: 2/2, step 3697/7134 completed (loss: 0.013631039299070835, acc: 0.9956268072128296)
[2025-02-13 04:04:30,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:30,360][root][INFO] - Training Epoch: 2/2, step 3698/7134 completed (loss: 0.017016369849443436, acc: 0.9919742941856384)
[2025-02-13 04:04:30,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:30,761][root][INFO] - Training Epoch: 2/2, step 3699/7134 completed (loss: 0.004120846278965473, acc: 0.9983108043670654)
[2025-02-13 04:04:30,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:31,186][root][INFO] - Training Epoch: 2/2, step 3700/7134 completed (loss: 0.014271781779825687, acc: 0.9941691160202026)
[2025-02-13 04:04:31,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:31,631][root][INFO] - Training Epoch: 2/2, step 3701/7134 completed (loss: 0.029552994295954704, acc: 0.9933775067329407)
[2025-02-13 04:04:31,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:32,040][root][INFO] - Training Epoch: 2/2, step 3702/7134 completed (loss: 0.027707159519195557, acc: 0.994490385055542)
[2025-02-13 04:04:32,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:32,483][root][INFO] - Training Epoch: 2/2, step 3703/7134 completed (loss: 0.02099870704114437, acc: 0.9961685538291931)
[2025-02-13 04:04:32,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:32,887][root][INFO] - Training Epoch: 2/2, step 3704/7134 completed (loss: 0.004283297341316938, acc: 1.0)
[2025-02-13 04:04:33,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:33,289][root][INFO] - Training Epoch: 2/2, step 3705/7134 completed (loss: 0.05344086512923241, acc: 0.9840764403343201)
[2025-02-13 04:04:33,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:33,702][root][INFO] - Training Epoch: 2/2, step 3706/7134 completed (loss: 0.07756839692592621, acc: 0.9803197979927063)
[2025-02-13 04:04:33,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:34,129][root][INFO] - Training Epoch: 2/2, step 3707/7134 completed (loss: 0.019987361505627632, acc: 0.9929873943328857)
[2025-02-13 04:04:34,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:34,581][root][INFO] - Training Epoch: 2/2, step 3708/7134 completed (loss: 0.04169797524809837, acc: 0.9857142567634583)
[2025-02-13 04:04:34,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:35,065][root][INFO] - Training Epoch: 2/2, step 3709/7134 completed (loss: 0.019081035628914833, acc: 0.993914783000946)
[2025-02-13 04:04:35,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:35,544][root][INFO] - Training Epoch: 2/2, step 3710/7134 completed (loss: 0.03227291628718376, acc: 0.9866803288459778)
[2025-02-13 04:04:35,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:35,960][root][INFO] - Training Epoch: 2/2, step 3711/7134 completed (loss: 0.026131199672818184, acc: 0.9937965273857117)
[2025-02-13 04:04:36,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:36,423][root][INFO] - Training Epoch: 2/2, step 3712/7134 completed (loss: 0.028333744034171104, acc: 0.9940263032913208)
[2025-02-13 04:04:36,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:36,852][root][INFO] - Training Epoch: 2/2, step 3713/7134 completed (loss: 0.04124028608202934, acc: 0.9917898178100586)
[2025-02-13 04:04:36,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:37,283][root][INFO] - Training Epoch: 2/2, step 3714/7134 completed (loss: 0.03004000522196293, acc: 0.9922651648521423)
[2025-02-13 04:04:37,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:37,730][root][INFO] - Training Epoch: 2/2, step 3715/7134 completed (loss: 0.021520819514989853, acc: 0.996372401714325)
[2025-02-13 04:04:37,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:38,188][root][INFO] - Training Epoch: 2/2, step 3716/7134 completed (loss: 0.03705736994743347, acc: 0.9864364862442017)
[2025-02-13 04:04:38,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:38,624][root][INFO] - Training Epoch: 2/2, step 3717/7134 completed (loss: 0.024359729140996933, acc: 0.9959016442298889)
[2025-02-13 04:04:38,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:39,081][root][INFO] - Training Epoch: 2/2, step 3718/7134 completed (loss: 0.03957238420844078, acc: 0.9894490242004395)
[2025-02-13 04:04:39,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:39,530][root][INFO] - Training Epoch: 2/2, step 3719/7134 completed (loss: 0.05071934685111046, acc: 0.9852941036224365)
[2025-02-13 04:04:39,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:39,920][root][INFO] - Training Epoch: 2/2, step 3720/7134 completed (loss: 0.015498381108045578, acc: 0.9941349029541016)
[2025-02-13 04:04:40,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:40,372][root][INFO] - Training Epoch: 2/2, step 3721/7134 completed (loss: 0.023724470287561417, acc: 0.9920106530189514)
[2025-02-13 04:04:40,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:40,806][root][INFO] - Training Epoch: 2/2, step 3722/7134 completed (loss: 0.0169219933450222, acc: 0.993842363357544)
[2025-02-13 04:04:40,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:41,220][root][INFO] - Training Epoch: 2/2, step 3723/7134 completed (loss: 0.043839577585458755, acc: 0.9884892106056213)
[2025-02-13 04:04:41,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:41,651][root][INFO] - Training Epoch: 2/2, step 3724/7134 completed (loss: 0.033187177032232285, acc: 0.9899425506591797)
[2025-02-13 04:04:41,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:42,098][root][INFO] - Training Epoch: 2/2, step 3725/7134 completed (loss: 0.09105624258518219, acc: 0.974926233291626)
[2025-02-13 04:04:42,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:42,557][root][INFO] - Training Epoch: 2/2, step 3726/7134 completed (loss: 0.03187035024166107, acc: 0.9925611019134521)
[2025-02-13 04:04:42,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:42,981][root][INFO] - Training Epoch: 2/2, step 3727/7134 completed (loss: 0.024216556921601295, acc: 0.9946595430374146)
[2025-02-13 04:04:43,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:43,425][root][INFO] - Training Epoch: 2/2, step 3728/7134 completed (loss: 0.010800696909427643, acc: 0.9977452158927917)
[2025-02-13 04:04:43,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:43,870][root][INFO] - Training Epoch: 2/2, step 3729/7134 completed (loss: 0.023035962134599686, acc: 0.9920454621315002)
[2025-02-13 04:04:44,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:44,310][root][INFO] - Training Epoch: 2/2, step 3730/7134 completed (loss: 0.018616747111082077, acc: 0.9932157397270203)
[2025-02-13 04:04:44,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:44,748][root][INFO] - Training Epoch: 2/2, step 3731/7134 completed (loss: 0.019582495093345642, acc: 0.9950186610221863)
[2025-02-13 04:04:44,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:45,189][root][INFO] - Training Epoch: 2/2, step 3732/7134 completed (loss: 0.01663854531943798, acc: 0.9954596757888794)
[2025-02-13 04:04:45,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:45,613][root][INFO] - Training Epoch: 2/2, step 3733/7134 completed (loss: 0.05265573784708977, acc: 0.9859747290611267)
[2025-02-13 04:04:45,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:46,013][root][INFO] - Training Epoch: 2/2, step 3734/7134 completed (loss: 0.04044255241751671, acc: 0.9886731505393982)
[2025-02-13 04:04:46,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:46,418][root][INFO] - Training Epoch: 2/2, step 3735/7134 completed (loss: 0.044743895530700684, acc: 0.9879518151283264)
[2025-02-13 04:04:46,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:46,855][root][INFO] - Training Epoch: 2/2, step 3736/7134 completed (loss: 0.0949544906616211, acc: 0.9791304469108582)
[2025-02-13 04:04:46,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:47,268][root][INFO] - Training Epoch: 2/2, step 3737/7134 completed (loss: 0.05609304457902908, acc: 0.9819004535675049)
[2025-02-13 04:04:47,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:47,664][root][INFO] - Training Epoch: 2/2, step 3738/7134 completed (loss: 0.036242205649614334, acc: 0.9933993220329285)
[2025-02-13 04:04:47,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:48,011][root][INFO] - Training Epoch: 2/2, step 3739/7134 completed (loss: 0.051147717982530594, acc: 0.9859550595283508)
[2025-02-13 04:04:48,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:48,406][root][INFO] - Training Epoch: 2/2, step 3740/7134 completed (loss: 0.033602580428123474, acc: 0.9918830990791321)
[2025-02-13 04:04:48,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:48,820][root][INFO] - Training Epoch: 2/2, step 3741/7134 completed (loss: 0.04457767307758331, acc: 0.9900142550468445)
[2025-02-13 04:04:48,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:49,181][root][INFO] - Training Epoch: 2/2, step 3742/7134 completed (loss: 0.0459301583468914, acc: 0.9863247871398926)
[2025-02-13 04:04:49,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:49,594][root][INFO] - Training Epoch: 2/2, step 3743/7134 completed (loss: 0.0516992025077343, acc: 0.9867256879806519)
[2025-02-13 04:04:49,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:50,031][root][INFO] - Training Epoch: 2/2, step 3744/7134 completed (loss: 0.0568699985742569, acc: 0.9841897487640381)
[2025-02-13 04:04:50,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:50,415][root][INFO] - Training Epoch: 2/2, step 3745/7134 completed (loss: 0.0678270012140274, acc: 0.9814385175704956)
[2025-02-13 04:04:50,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:50,851][root][INFO] - Training Epoch: 2/2, step 3746/7134 completed (loss: 0.04975370690226555, acc: 0.9869109988212585)
[2025-02-13 04:04:51,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:51,281][root][INFO] - Training Epoch: 2/2, step 3747/7134 completed (loss: 0.04074052348732948, acc: 0.9897959232330322)
[2025-02-13 04:04:51,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:51,630][root][INFO] - Training Epoch: 2/2, step 3748/7134 completed (loss: 0.06594729423522949, acc: 0.9823943376541138)
[2025-02-13 04:04:51,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:52,033][root][INFO] - Training Epoch: 2/2, step 3749/7134 completed (loss: 0.07842917740345001, acc: 0.9805097579956055)
[2025-02-13 04:04:52,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:52,452][root][INFO] - Training Epoch: 2/2, step 3750/7134 completed (loss: 0.02886766381561756, acc: 0.991631805896759)
[2025-02-13 04:04:52,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:52,873][root][INFO] - Training Epoch: 2/2, step 3751/7134 completed (loss: 0.05631810054183006, acc: 0.9772403836250305)
[2025-02-13 04:04:53,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:53,296][root][INFO] - Training Epoch: 2/2, step 3752/7134 completed (loss: 0.040695954114198685, acc: 0.9850746393203735)
[2025-02-13 04:04:53,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:53,716][root][INFO] - Training Epoch: 2/2, step 3753/7134 completed (loss: 0.06348750740289688, acc: 0.980567991733551)
[2025-02-13 04:04:53,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:54,116][root][INFO] - Training Epoch: 2/2, step 3754/7134 completed (loss: 0.017066573724150658, acc: 0.9954476356506348)
[2025-02-13 04:04:54,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:54,519][root][INFO] - Training Epoch: 2/2, step 3755/7134 completed (loss: 0.03909630328416824, acc: 0.9888682961463928)
[2025-02-13 04:04:54,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:54,935][root][INFO] - Training Epoch: 2/2, step 3756/7134 completed (loss: 0.0468587726354599, acc: 0.9916897416114807)
[2025-02-13 04:04:55,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:55,354][root][INFO] - Training Epoch: 2/2, step 3757/7134 completed (loss: 0.07883311063051224, acc: 0.9737274050712585)
[2025-02-13 04:04:55,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:55,731][root][INFO] - Training Epoch: 2/2, step 3758/7134 completed (loss: 0.033747803419828415, acc: 0.9909909963607788)
[2025-02-13 04:04:55,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:56,134][root][INFO] - Training Epoch: 2/2, step 3759/7134 completed (loss: 0.0556834377348423, acc: 0.9814189076423645)
[2025-02-13 04:04:56,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:56,582][root][INFO] - Training Epoch: 2/2, step 3760/7134 completed (loss: 0.057583075016736984, acc: 0.9901130199432373)
[2025-02-13 04:04:56,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:56,998][root][INFO] - Training Epoch: 2/2, step 3761/7134 completed (loss: 0.019794274121522903, acc: 0.9953632354736328)
[2025-02-13 04:04:57,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:57,430][root][INFO] - Training Epoch: 2/2, step 3762/7134 completed (loss: 0.03141475468873978, acc: 0.9889958500862122)
[2025-02-13 04:04:57,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:57,836][root][INFO] - Training Epoch: 2/2, step 3763/7134 completed (loss: 0.021471889689564705, acc: 0.9922839403152466)
[2025-02-13 04:04:57,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:58,257][root][INFO] - Training Epoch: 2/2, step 3764/7134 completed (loss: 0.054050493985414505, acc: 0.9822161197662354)
[2025-02-13 04:04:58,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:58,731][root][INFO] - Training Epoch: 2/2, step 3765/7134 completed (loss: 0.022153226658701897, acc: 0.9915966391563416)
[2025-02-13 04:04:58,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:59,118][root][INFO] - Training Epoch: 2/2, step 3766/7134 completed (loss: 0.06297991424798965, acc: 0.9871794581413269)
[2025-02-13 04:04:59,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:59,553][root][INFO] - Training Epoch: 2/2, step 3767/7134 completed (loss: 0.012660793960094452, acc: 0.996259331703186)
[2025-02-13 04:04:59,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:59,967][root][INFO] - Training Epoch: 2/2, step 3768/7134 completed (loss: 0.012904058210551739, acc: 0.9958506226539612)
[2025-02-13 04:05:00,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:00,414][root][INFO] - Training Epoch: 2/2, step 3769/7134 completed (loss: 0.03512643277645111, acc: 0.989924430847168)
[2025-02-13 04:05:00,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:00,827][root][INFO] - Training Epoch: 2/2, step 3770/7134 completed (loss: 0.028525611385703087, acc: 0.9929378628730774)
[2025-02-13 04:05:00,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:01,274][root][INFO] - Training Epoch: 2/2, step 3771/7134 completed (loss: 0.027248011901974678, acc: 0.9948052167892456)
[2025-02-13 04:05:01,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:01,704][root][INFO] - Training Epoch: 2/2, step 3772/7134 completed (loss: 0.05682586133480072, acc: 0.9835575222969055)
[2025-02-13 04:05:01,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:02,161][root][INFO] - Training Epoch: 2/2, step 3773/7134 completed (loss: 0.03147607296705246, acc: 0.9921875)
[2025-02-13 04:05:02,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:02,611][root][INFO] - Training Epoch: 2/2, step 3774/7134 completed (loss: 0.025785578414797783, acc: 0.9930394291877747)
[2025-02-13 04:05:02,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:03,039][root][INFO] - Training Epoch: 2/2, step 3775/7134 completed (loss: 0.0284590907394886, acc: 0.9928366541862488)
[2025-02-13 04:05:03,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:03,478][root][INFO] - Training Epoch: 2/2, step 3776/7134 completed (loss: 0.0356491394340992, acc: 0.9881481528282166)
[2025-02-13 04:05:03,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:03,911][root][INFO] - Training Epoch: 2/2, step 3777/7134 completed (loss: 0.03341596946120262, acc: 0.9924812316894531)
[2025-02-13 04:05:04,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:04,331][root][INFO] - Training Epoch: 2/2, step 3778/7134 completed (loss: 0.05524663254618645, acc: 0.9843527674674988)
[2025-02-13 04:05:04,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:04,733][root][INFO] - Training Epoch: 2/2, step 3779/7134 completed (loss: 0.031529370695352554, acc: 0.9900285005569458)
[2025-02-13 04:05:04,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:05,152][root][INFO] - Training Epoch: 2/2, step 3780/7134 completed (loss: 0.019137175753712654, acc: 0.9899135231971741)
[2025-02-13 04:05:05,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:05,564][root][INFO] - Training Epoch: 2/2, step 3781/7134 completed (loss: 0.05537537485361099, acc: 0.9862825870513916)
[2025-02-13 04:05:05,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:05,955][root][INFO] - Training Epoch: 2/2, step 3782/7134 completed (loss: 0.031688906252384186, acc: 0.9871794581413269)
[2025-02-13 04:05:06,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:06,380][root][INFO] - Training Epoch: 2/2, step 3783/7134 completed (loss: 0.04046071320772171, acc: 0.9858064651489258)
[2025-02-13 04:05:06,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:06,784][root][INFO] - Training Epoch: 2/2, step 3784/7134 completed (loss: 0.05544344708323479, acc: 0.9860896468162537)
[2025-02-13 04:05:06,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:07,197][root][INFO] - Training Epoch: 2/2, step 3785/7134 completed (loss: 0.010443096980452538, acc: 0.9968798756599426)
[2025-02-13 04:05:07,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:07,603][root][INFO] - Training Epoch: 2/2, step 3786/7134 completed (loss: 0.03846060484647751, acc: 0.9857697486877441)
[2025-02-13 04:05:07,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:07,974][root][INFO] - Training Epoch: 2/2, step 3787/7134 completed (loss: 0.03596580773591995, acc: 0.9872408509254456)
[2025-02-13 04:05:08,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:08,432][root][INFO] - Training Epoch: 2/2, step 3788/7134 completed (loss: 0.03212806209921837, acc: 0.9905533194541931)
[2025-02-13 04:05:08,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:08,870][root][INFO] - Training Epoch: 2/2, step 3789/7134 completed (loss: 0.03615060821175575, acc: 0.9873417615890503)
[2025-02-13 04:05:09,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:09,313][root][INFO] - Training Epoch: 2/2, step 3790/7134 completed (loss: 0.09120392054319382, acc: 0.980988621711731)
[2025-02-13 04:05:09,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:09,722][root][INFO] - Training Epoch: 2/2, step 3791/7134 completed (loss: 0.015383237041532993, acc: 0.9974193572998047)
[2025-02-13 04:05:09,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:10,133][root][INFO] - Training Epoch: 2/2, step 3792/7134 completed (loss: 0.024086784571409225, acc: 0.9934318661689758)
[2025-02-13 04:05:10,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:10,563][root][INFO] - Training Epoch: 2/2, step 3793/7134 completed (loss: 0.03894602879881859, acc: 0.9878378510475159)
[2025-02-13 04:05:10,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:10,987][root][INFO] - Training Epoch: 2/2, step 3794/7134 completed (loss: 0.026807527989149094, acc: 0.9896238446235657)
[2025-02-13 04:05:11,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:11,443][root][INFO] - Training Epoch: 2/2, step 3795/7134 completed (loss: 0.032239362597465515, acc: 0.9891566038131714)
[2025-02-13 04:05:11,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:11,881][root][INFO] - Training Epoch: 2/2, step 3796/7134 completed (loss: 0.030976222828030586, acc: 0.9912060499191284)
[2025-02-13 04:05:12,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:12,294][root][INFO] - Training Epoch: 2/2, step 3797/7134 completed (loss: 0.017909590154886246, acc: 0.9921773076057434)
[2025-02-13 04:05:12,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:12,708][root][INFO] - Training Epoch: 2/2, step 3798/7134 completed (loss: 0.04557296261191368, acc: 0.9885057210922241)
[2025-02-13 04:05:12,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:13,130][root][INFO] - Training Epoch: 2/2, step 3799/7134 completed (loss: 0.0421181321144104, acc: 0.9871299862861633)
[2025-02-13 04:05:13,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:13,548][root][INFO] - Training Epoch: 2/2, step 3800/7134 completed (loss: 0.03794444352388382, acc: 0.9908257126808167)
[2025-02-13 04:05:13,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:13,996][root][INFO] - Training Epoch: 2/2, step 3801/7134 completed (loss: 0.018090680241584778, acc: 0.9950617551803589)
[2025-02-13 04:05:14,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:14,437][root][INFO] - Training Epoch: 2/2, step 3802/7134 completed (loss: 0.027363283559679985, acc: 0.9929078221321106)
[2025-02-13 04:05:14,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:14,850][root][INFO] - Training Epoch: 2/2, step 3803/7134 completed (loss: 0.030314546078443527, acc: 0.9901960492134094)
[2025-02-13 04:05:14,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:15,299][root][INFO] - Training Epoch: 2/2, step 3804/7134 completed (loss: 0.026174770668148994, acc: 0.9922239780426025)
[2025-02-13 04:05:15,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:15,735][root][INFO] - Training Epoch: 2/2, step 3805/7134 completed (loss: 0.014193630777299404, acc: 0.9947643876075745)
[2025-02-13 04:05:15,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:16,125][root][INFO] - Training Epoch: 2/2, step 3806/7134 completed (loss: 0.03701987490057945, acc: 0.9921104311943054)
[2025-02-13 04:05:16,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:16,547][root][INFO] - Training Epoch: 2/2, step 3807/7134 completed (loss: 0.010953333228826523, acc: 0.9970760345458984)
[2025-02-13 04:05:16,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:17,001][root][INFO] - Training Epoch: 2/2, step 3808/7134 completed (loss: 0.03664674237370491, acc: 0.9924050569534302)
[2025-02-13 04:05:17,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:17,418][root][INFO] - Training Epoch: 2/2, step 3809/7134 completed (loss: 0.05442536994814873, acc: 0.9809644818305969)
[2025-02-13 04:05:17,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:17,826][root][INFO] - Training Epoch: 2/2, step 3810/7134 completed (loss: 0.02323242276906967, acc: 0.9910714030265808)
[2025-02-13 04:05:17,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:18,258][root][INFO] - Training Epoch: 2/2, step 3811/7134 completed (loss: 0.04175474867224693, acc: 0.9875862002372742)
[2025-02-13 04:05:18,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:18,686][root][INFO] - Training Epoch: 2/2, step 3812/7134 completed (loss: 0.05502030625939369, acc: 0.9777015447616577)
[2025-02-13 04:05:18,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:19,128][root][INFO] - Training Epoch: 2/2, step 3813/7134 completed (loss: 0.05076327919960022, acc: 0.9854881167411804)
[2025-02-13 04:05:19,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:19,564][root][INFO] - Training Epoch: 2/2, step 3814/7134 completed (loss: 0.024267280474305153, acc: 0.9919571280479431)
[2025-02-13 04:05:19,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:19,973][root][INFO] - Training Epoch: 2/2, step 3815/7134 completed (loss: 0.0319078154861927, acc: 0.9866666793823242)
[2025-02-13 04:05:20,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:20,425][root][INFO] - Training Epoch: 2/2, step 3816/7134 completed (loss: 0.024480069056153297, acc: 0.9936908483505249)
[2025-02-13 04:05:20,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:20,829][root][INFO] - Training Epoch: 2/2, step 3817/7134 completed (loss: 0.04309965670108795, acc: 0.9880095720291138)
[2025-02-13 04:05:20,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:21,178][root][INFO] - Training Epoch: 2/2, step 3818/7134 completed (loss: 0.07175153493881226, acc: 0.9858585596084595)
[2025-02-13 04:05:21,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:21,525][root][INFO] - Training Epoch: 2/2, step 3819/7134 completed (loss: 0.05738409236073494, acc: 0.9759036302566528)
[2025-02-13 04:05:21,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:21,922][root][INFO] - Training Epoch: 2/2, step 3820/7134 completed (loss: 0.03394545242190361, acc: 0.9857482314109802)
[2025-02-13 04:05:22,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:22,315][root][INFO] - Training Epoch: 2/2, step 3821/7134 completed (loss: 0.02224431373178959, acc: 0.989847719669342)
[2025-02-13 04:05:22,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:22,711][root][INFO] - Training Epoch: 2/2, step 3822/7134 completed (loss: 0.03731325641274452, acc: 0.9882121682167053)
[2025-02-13 04:05:22,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:23,144][root][INFO] - Training Epoch: 2/2, step 3823/7134 completed (loss: 0.014080798253417015, acc: 0.9972337484359741)
[2025-02-13 04:05:23,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:23,538][root][INFO] - Training Epoch: 2/2, step 3824/7134 completed (loss: 0.06413496285676956, acc: 0.9848229289054871)
[2025-02-13 04:05:23,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:23,955][root][INFO] - Training Epoch: 2/2, step 3825/7134 completed (loss: 0.023110080510377884, acc: 0.9940119981765747)
[2025-02-13 04:05:24,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:24,390][root][INFO] - Training Epoch: 2/2, step 3826/7134 completed (loss: 0.04973237216472626, acc: 0.9838235378265381)
[2025-02-13 04:05:24,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:24,828][root][INFO] - Training Epoch: 2/2, step 3827/7134 completed (loss: 0.025592461228370667, acc: 0.994397759437561)
[2025-02-13 04:05:24,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:25,186][root][INFO] - Training Epoch: 2/2, step 3828/7134 completed (loss: 0.03359002247452736, acc: 0.9893898963928223)
[2025-02-13 04:05:25,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:25,636][root][INFO] - Training Epoch: 2/2, step 3829/7134 completed (loss: 0.03141709417104721, acc: 0.993966817855835)
[2025-02-13 04:05:25,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:26,056][root][INFO] - Training Epoch: 2/2, step 3830/7134 completed (loss: 0.04700116440653801, acc: 0.9882746934890747)
[2025-02-13 04:05:26,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:26,490][root][INFO] - Training Epoch: 2/2, step 3831/7134 completed (loss: 0.03682634234428406, acc: 0.9909909963607788)
[2025-02-13 04:05:26,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:26,862][root][INFO] - Training Epoch: 2/2, step 3832/7134 completed (loss: 0.030060116201639175, acc: 0.994106113910675)
[2025-02-13 04:05:26,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:27,291][root][INFO] - Training Epoch: 2/2, step 3833/7134 completed (loss: 0.014736311510205269, acc: 0.9938119053840637)
[2025-02-13 04:05:27,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:27,710][root][INFO] - Training Epoch: 2/2, step 3834/7134 completed (loss: 0.008223491720855236, acc: 0.998701274394989)
[2025-02-13 04:05:27,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:28,118][root][INFO] - Training Epoch: 2/2, step 3835/7134 completed (loss: 0.010600765235722065, acc: 0.9951456189155579)
[2025-02-13 04:05:28,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:28,435][root][INFO] - Training Epoch: 2/2, step 3836/7134 completed (loss: 0.047323692589998245, acc: 0.991304337978363)
[2025-02-13 04:05:28,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:28,832][root][INFO] - Training Epoch: 2/2, step 3837/7134 completed (loss: 0.028979580849409103, acc: 0.9956896305084229)
[2025-02-13 04:05:28,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:29,261][root][INFO] - Training Epoch: 2/2, step 3838/7134 completed (loss: 0.006704631261527538, acc: 0.9986720085144043)
[2025-02-13 04:05:29,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:29,718][root][INFO] - Training Epoch: 2/2, step 3839/7134 completed (loss: 0.0306924507021904, acc: 0.9930939078330994)
[2025-02-13 04:05:29,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:30,112][root][INFO] - Training Epoch: 2/2, step 3840/7134 completed (loss: 0.014320222660899162, acc: 0.9946452379226685)
[2025-02-13 04:05:30,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:30,569][root][INFO] - Training Epoch: 2/2, step 3841/7134 completed (loss: 0.04151763767004013, acc: 0.9877913594245911)
[2025-02-13 04:05:30,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:30,888][root][INFO] - Training Epoch: 2/2, step 3842/7134 completed (loss: 0.0036464182194322348, acc: 1.0)
[2025-02-13 04:05:31,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:31,372][root][INFO] - Training Epoch: 2/2, step 3843/7134 completed (loss: 0.021323274821043015, acc: 0.9937434792518616)
[2025-02-13 04:05:31,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:31,816][root][INFO] - Training Epoch: 2/2, step 3844/7134 completed (loss: 0.022142628207802773, acc: 0.9961880445480347)
[2025-02-13 04:05:31,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:32,213][root][INFO] - Training Epoch: 2/2, step 3845/7134 completed (loss: 0.03349422663450241, acc: 0.9915825128555298)
[2025-02-13 04:05:32,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:32,591][root][INFO] - Training Epoch: 2/2, step 3846/7134 completed (loss: 0.024349398910999298, acc: 0.9966044425964355)
[2025-02-13 04:05:32,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:32,989][root][INFO] - Training Epoch: 2/2, step 3847/7134 completed (loss: 0.016475653275847435, acc: 0.9968000054359436)
[2025-02-13 04:05:33,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:33,446][root][INFO] - Training Epoch: 2/2, step 3848/7134 completed (loss: 0.023495616391301155, acc: 0.9974554777145386)
[2025-02-13 04:05:33,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:33,849][root][INFO] - Training Epoch: 2/2, step 3849/7134 completed (loss: 0.05520032346248627, acc: 0.987522304058075)
[2025-02-13 04:05:33,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:34,264][root][INFO] - Training Epoch: 2/2, step 3850/7134 completed (loss: 0.06101808324456215, acc: 0.9877750873565674)
[2025-02-13 04:05:34,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:34,721][root][INFO] - Training Epoch: 2/2, step 3851/7134 completed (loss: 0.04476584121584892, acc: 0.9874476790428162)
[2025-02-13 04:05:34,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:35,134][root][INFO] - Training Epoch: 2/2, step 3852/7134 completed (loss: 0.018888121470808983, acc: 0.9935691356658936)
[2025-02-13 04:05:35,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:35,546][root][INFO] - Training Epoch: 2/2, step 3853/7134 completed (loss: 0.04540984332561493, acc: 0.9859353303909302)
[2025-02-13 04:05:35,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:35,969][root][INFO] - Training Epoch: 2/2, step 3854/7134 completed (loss: 0.0165353175252676, acc: 0.9957864880561829)
[2025-02-13 04:05:36,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:36,410][root][INFO] - Training Epoch: 2/2, step 3855/7134 completed (loss: 0.03910830616950989, acc: 0.9912717938423157)
[2025-02-13 04:05:36,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:36,816][root][INFO] - Training Epoch: 2/2, step 3856/7134 completed (loss: 0.04162289947271347, acc: 0.9888424277305603)
[2025-02-13 04:05:36,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:37,197][root][INFO] - Training Epoch: 2/2, step 3857/7134 completed (loss: 0.025698808953166008, acc: 0.9919614195823669)
[2025-02-13 04:05:37,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:37,626][root][INFO] - Training Epoch: 2/2, step 3858/7134 completed (loss: 0.05949537083506584, acc: 0.9858356714248657)
[2025-02-13 04:05:37,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:38,062][root][INFO] - Training Epoch: 2/2, step 3859/7134 completed (loss: 0.039811909198760986, acc: 0.9923664331436157)
[2025-02-13 04:05:38,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:38,509][root][INFO] - Training Epoch: 2/2, step 3860/7134 completed (loss: 0.03287115320563316, acc: 0.9910614490509033)
[2025-02-13 04:05:38,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:38,940][root][INFO] - Training Epoch: 2/2, step 3861/7134 completed (loss: 0.03037281334400177, acc: 0.9877551198005676)
[2025-02-13 04:05:39,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:39,320][root][INFO] - Training Epoch: 2/2, step 3862/7134 completed (loss: 0.04209394007921219, acc: 0.9901477694511414)
[2025-02-13 04:05:39,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:39,788][root][INFO] - Training Epoch: 2/2, step 3863/7134 completed (loss: 0.018632732331752777, acc: 0.9938271641731262)
[2025-02-13 04:05:39,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:40,227][root][INFO] - Training Epoch: 2/2, step 3864/7134 completed (loss: 0.0289909765124321, acc: 0.9960835576057434)
[2025-02-13 04:05:40,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:40,657][root][INFO] - Training Epoch: 2/2, step 3865/7134 completed (loss: 0.02618476003408432, acc: 0.9920318722724915)
[2025-02-13 04:05:40,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:41,076][root][INFO] - Training Epoch: 2/2, step 3866/7134 completed (loss: 0.04714567959308624, acc: 0.9877899885177612)
[2025-02-13 04:05:41,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:41,518][root][INFO] - Training Epoch: 2/2, step 3867/7134 completed (loss: 0.03771314397454262, acc: 0.9867424368858337)
[2025-02-13 04:05:41,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:41,926][root][INFO] - Training Epoch: 2/2, step 3868/7134 completed (loss: 0.01953708752989769, acc: 0.9966158866882324)
[2025-02-13 04:05:42,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:42,344][root][INFO] - Training Epoch: 2/2, step 3869/7134 completed (loss: 0.025115132331848145, acc: 0.9878970980644226)
[2025-02-13 04:05:42,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:42,761][root][INFO] - Training Epoch: 2/2, step 3870/7134 completed (loss: 0.0649200975894928, acc: 0.9861111044883728)
[2025-02-13 04:05:42,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:43,187][root][INFO] - Training Epoch: 2/2, step 3871/7134 completed (loss: 0.03811916708946228, acc: 0.9862843155860901)
[2025-02-13 04:05:43,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:43,664][root][INFO] - Training Epoch: 2/2, step 3872/7134 completed (loss: 0.04267086088657379, acc: 0.9903640151023865)
[2025-02-13 04:05:43,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:44,108][root][INFO] - Training Epoch: 2/2, step 3873/7134 completed (loss: 0.026996081694960594, acc: 0.9923273921012878)
[2025-02-13 04:05:44,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:44,526][root][INFO] - Training Epoch: 2/2, step 3874/7134 completed (loss: 0.04341394826769829, acc: 0.9889349937438965)
[2025-02-13 04:05:44,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:44,981][root][INFO] - Training Epoch: 2/2, step 3875/7134 completed (loss: 0.05541738495230675, acc: 0.9827127456665039)
[2025-02-13 04:05:45,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:45,385][root][INFO] - Training Epoch: 2/2, step 3876/7134 completed (loss: 0.01634257473051548, acc: 0.9924952983856201)
[2025-02-13 04:05:45,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:45,798][root][INFO] - Training Epoch: 2/2, step 3877/7134 completed (loss: 0.055165160447359085, acc: 0.9895561337471008)
[2025-02-13 04:05:45,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:46,207][root][INFO] - Training Epoch: 2/2, step 3878/7134 completed (loss: 0.07700688391923904, acc: 0.9878706336021423)
[2025-02-13 04:05:46,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:46,641][root][INFO] - Training Epoch: 2/2, step 3879/7134 completed (loss: 0.02007180266082287, acc: 0.9944547414779663)
[2025-02-13 04:05:46,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:47,081][root][INFO] - Training Epoch: 2/2, step 3880/7134 completed (loss: 0.10199137032032013, acc: 0.9794420003890991)
[2025-02-13 04:05:47,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:47,498][root][INFO] - Training Epoch: 2/2, step 3881/7134 completed (loss: 0.09836553782224655, acc: 0.9760000109672546)
[2025-02-13 04:05:47,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:47,935][root][INFO] - Training Epoch: 2/2, step 3882/7134 completed (loss: 0.05205236002802849, acc: 0.9850000143051147)
[2025-02-13 04:05:48,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:48,369][root][INFO] - Training Epoch: 2/2, step 3883/7134 completed (loss: 0.011605296283960342, acc: 0.9986013770103455)
[2025-02-13 04:05:48,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:48,787][root][INFO] - Training Epoch: 2/2, step 3884/7134 completed (loss: 0.019788261502981186, acc: 0.9931972622871399)
[2025-02-13 04:05:48,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:49,194][root][INFO] - Training Epoch: 2/2, step 3885/7134 completed (loss: 0.025983572006225586, acc: 0.9921671152114868)
[2025-02-13 04:05:49,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:49,632][root][INFO] - Training Epoch: 2/2, step 3886/7134 completed (loss: 0.030503658577799797, acc: 0.990338146686554)
[2025-02-13 04:05:49,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:50,040][root][INFO] - Training Epoch: 2/2, step 3887/7134 completed (loss: 0.03090623952448368, acc: 0.9954751133918762)
[2025-02-13 04:05:50,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:50,472][root][INFO] - Training Epoch: 2/2, step 3888/7134 completed (loss: 0.028402607887983322, acc: 0.9887499809265137)
[2025-02-13 04:05:50,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:50,936][root][INFO] - Training Epoch: 2/2, step 3889/7134 completed (loss: 0.02199527435004711, acc: 0.992337167263031)
[2025-02-13 04:05:51,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:51,381][root][INFO] - Training Epoch: 2/2, step 3890/7134 completed (loss: 0.019129043444991112, acc: 0.9939831495285034)
[2025-02-13 04:05:51,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:51,827][root][INFO] - Training Epoch: 2/2, step 3891/7134 completed (loss: 0.012216457165777683, acc: 0.9973856210708618)
[2025-02-13 04:05:51,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:52,260][root][INFO] - Training Epoch: 2/2, step 3892/7134 completed (loss: 0.023080220445990562, acc: 0.9907692074775696)
[2025-02-13 04:05:52,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:52,676][root][INFO] - Training Epoch: 2/2, step 3893/7134 completed (loss: 0.06917177140712738, acc: 0.9780927896499634)
[2025-02-13 04:05:52,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:53,078][root][INFO] - Training Epoch: 2/2, step 3894/7134 completed (loss: 0.012517708353698254, acc: 0.9985954761505127)
[2025-02-13 04:05:53,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:53,520][root][INFO] - Training Epoch: 2/2, step 3895/7134 completed (loss: 0.024521855637431145, acc: 0.9922580718994141)
[2025-02-13 04:05:53,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:53,981][root][INFO] - Training Epoch: 2/2, step 3896/7134 completed (loss: 0.012626857496798038, acc: 0.9977426528930664)
[2025-02-13 04:05:54,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:54,433][root][INFO] - Training Epoch: 2/2, step 3897/7134 completed (loss: 0.05256974697113037, acc: 0.98591548204422)
[2025-02-13 04:05:54,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:54,870][root][INFO] - Training Epoch: 2/2, step 3898/7134 completed (loss: 0.032339613884687424, acc: 0.9885495901107788)
[2025-02-13 04:05:55,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:55,315][root][INFO] - Training Epoch: 2/2, step 3899/7134 completed (loss: 0.02310427464544773, acc: 0.9909443855285645)
[2025-02-13 04:05:55,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:55,786][root][INFO] - Training Epoch: 2/2, step 3900/7134 completed (loss: 0.036862749606370926, acc: 0.9837775230407715)
[2025-02-13 04:05:55,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:56,199][root][INFO] - Training Epoch: 2/2, step 3901/7134 completed (loss: 0.08077497035264969, acc: 0.9811320900917053)
[2025-02-13 04:05:56,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:56,620][root][INFO] - Training Epoch: 2/2, step 3902/7134 completed (loss: 0.08011356741189957, acc: 0.9827814698219299)
[2025-02-13 04:05:56,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:57,079][root][INFO] - Training Epoch: 2/2, step 3903/7134 completed (loss: 0.08140178769826889, acc: 0.9758713245391846)
[2025-02-13 04:05:57,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:57,485][root][INFO] - Training Epoch: 2/2, step 3904/7134 completed (loss: 0.033982351422309875, acc: 0.9893454909324646)
[2025-02-13 04:05:57,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:57,929][root][INFO] - Training Epoch: 2/2, step 3905/7134 completed (loss: 0.020176950842142105, acc: 0.9929478168487549)
[2025-02-13 04:05:58,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:58,370][root][INFO] - Training Epoch: 2/2, step 3906/7134 completed (loss: 0.011493697762489319, acc: 0.9985632300376892)
[2025-02-13 04:05:58,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:58,783][root][INFO] - Training Epoch: 2/2, step 3907/7134 completed (loss: 0.03201805427670479, acc: 0.9922879338264465)
[2025-02-13 04:05:58,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:59,216][root][INFO] - Training Epoch: 2/2, step 3908/7134 completed (loss: 0.059232015162706375, acc: 0.9816232919692993)
[2025-02-13 04:05:59,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:59,631][root][INFO] - Training Epoch: 2/2, step 3909/7134 completed (loss: 0.03728692978620529, acc: 0.9886914491653442)
[2025-02-13 04:05:59,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:00,049][root][INFO] - Training Epoch: 2/2, step 3910/7134 completed (loss: 0.045736342668533325, acc: 0.9847009778022766)
[2025-02-13 04:06:00,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:00,491][root][INFO] - Training Epoch: 2/2, step 3911/7134 completed (loss: 0.03588911145925522, acc: 0.9902912378311157)
[2025-02-13 04:06:00,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:00,916][root][INFO] - Training Epoch: 2/2, step 3912/7134 completed (loss: 0.012068180367350578, acc: 0.9940476417541504)
[2025-02-13 04:06:01,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:01,345][root][INFO] - Training Epoch: 2/2, step 3913/7134 completed (loss: 0.03996410593390465, acc: 0.9910256266593933)
[2025-02-13 04:06:01,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:01,750][root][INFO] - Training Epoch: 2/2, step 3914/7134 completed (loss: 0.026289697736501694, acc: 0.9932659864425659)
[2025-02-13 04:06:01,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:02,197][root][INFO] - Training Epoch: 2/2, step 3915/7134 completed (loss: 0.017104703933000565, acc: 0.994358241558075)
[2025-02-13 04:06:02,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:02,637][root][INFO] - Training Epoch: 2/2, step 3916/7134 completed (loss: 0.02041848935186863, acc: 0.993630588054657)
[2025-02-13 04:06:02,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:03,079][root][INFO] - Training Epoch: 2/2, step 3917/7134 completed (loss: 0.011167739517986774, acc: 0.9975932836532593)
[2025-02-13 04:06:03,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:03,512][root][INFO] - Training Epoch: 2/2, step 3918/7134 completed (loss: 0.0275635477155447, acc: 0.9947916865348816)
[2025-02-13 04:06:03,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:03,949][root][INFO] - Training Epoch: 2/2, step 3919/7134 completed (loss: 0.07267250120639801, acc: 0.9869706630706787)
[2025-02-13 04:06:04,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:04,394][root][INFO] - Training Epoch: 2/2, step 3920/7134 completed (loss: 0.025315698236227036, acc: 0.9890965819358826)
[2025-02-13 04:06:04,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:04,810][root][INFO] - Training Epoch: 2/2, step 3921/7134 completed (loss: 0.0336441844701767, acc: 0.9879999756813049)
[2025-02-13 04:06:04,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:05,200][root][INFO] - Training Epoch: 2/2, step 3922/7134 completed (loss: 0.07539742439985275, acc: 0.9769392013549805)
[2025-02-13 04:06:05,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:05,601][root][INFO] - Training Epoch: 2/2, step 3923/7134 completed (loss: 0.07780783623456955, acc: 0.9845890402793884)
[2025-02-13 04:06:05,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:06,039][root][INFO] - Training Epoch: 2/2, step 3924/7134 completed (loss: 0.0363052673637867, acc: 0.9870689511299133)
[2025-02-13 04:06:06,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:06,460][root][INFO] - Training Epoch: 2/2, step 3925/7134 completed (loss: 0.0383966900408268, acc: 0.985358715057373)
[2025-02-13 04:06:06,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:06,860][root][INFO] - Training Epoch: 2/2, step 3926/7134 completed (loss: 0.04665437713265419, acc: 0.9884892106056213)
[2025-02-13 04:06:07,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:07,282][root][INFO] - Training Epoch: 2/2, step 3927/7134 completed (loss: 0.031185444444417953, acc: 0.9876203536987305)
[2025-02-13 04:06:07,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:07,714][root][INFO] - Training Epoch: 2/2, step 3928/7134 completed (loss: 0.05831127241253853, acc: 0.982758641242981)
[2025-02-13 04:06:07,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:08,101][root][INFO] - Training Epoch: 2/2, step 3929/7134 completed (loss: 0.028652813285589218, acc: 0.9936407208442688)
[2025-02-13 04:06:08,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:08,519][root][INFO] - Training Epoch: 2/2, step 3930/7134 completed (loss: 0.03392139822244644, acc: 0.9873949289321899)
[2025-02-13 04:06:08,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:08,928][root][INFO] - Training Epoch: 2/2, step 3931/7134 completed (loss: 0.01961309276521206, acc: 0.9925261735916138)
[2025-02-13 04:06:09,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:09,356][root][INFO] - Training Epoch: 2/2, step 3932/7134 completed (loss: 0.03766774758696556, acc: 0.9841897487640381)
[2025-02-13 04:06:09,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:09,715][root][INFO] - Training Epoch: 2/2, step 3933/7134 completed (loss: 0.03184162825345993, acc: 0.9901315569877625)
[2025-02-13 04:06:09,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:10,111][root][INFO] - Training Epoch: 2/2, step 3934/7134 completed (loss: 0.019373901188373566, acc: 0.9929742217063904)
[2025-02-13 04:06:10,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:10,501][root][INFO] - Training Epoch: 2/2, step 3935/7134 completed (loss: 0.006940222345292568, acc: 0.9964664578437805)
[2025-02-13 04:06:10,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:10,904][root][INFO] - Training Epoch: 2/2, step 3936/7134 completed (loss: 0.04335923492908478, acc: 0.9886792302131653)
[2025-02-13 04:06:11,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:11,313][root][INFO] - Training Epoch: 2/2, step 3937/7134 completed (loss: 0.0340004488825798, acc: 0.9877551198005676)
[2025-02-13 04:06:11,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:11,691][root][INFO] - Training Epoch: 2/2, step 3938/7134 completed (loss: 0.0158369243144989, acc: 0.9954751133918762)
[2025-02-13 04:06:11,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:12,082][root][INFO] - Training Epoch: 2/2, step 3939/7134 completed (loss: 0.05888986960053444, acc: 0.9813874959945679)
[2025-02-13 04:06:12,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:12,411][root][INFO] - Training Epoch: 2/2, step 3940/7134 completed (loss: 0.03529008850455284, acc: 0.9899497628211975)
[2025-02-13 04:06:12,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:12,735][root][INFO] - Training Epoch: 2/2, step 3941/7134 completed (loss: 0.03118755668401718, acc: 0.9908536672592163)
[2025-02-13 04:06:12,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:13,118][root][INFO] - Training Epoch: 2/2, step 3942/7134 completed (loss: 0.025199759751558304, acc: 0.9918032884597778)
[2025-02-13 04:06:13,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:13,373][root][INFO] - Training Epoch: 2/2, step 3943/7134 completed (loss: 0.012658313848078251, acc: 0.9961389899253845)
[2025-02-13 04:06:13,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:13,789][root][INFO] - Training Epoch: 2/2, step 3944/7134 completed (loss: 0.023887870833277702, acc: 0.9944649338722229)
[2025-02-13 04:06:13,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:14,194][root][INFO] - Training Epoch: 2/2, step 3945/7134 completed (loss: 0.023653538897633553, acc: 0.9908536672592163)
[2025-02-13 04:06:14,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:14,556][root][INFO] - Training Epoch: 2/2, step 3946/7134 completed (loss: 0.020406346768140793, acc: 0.9958246350288391)
[2025-02-13 04:06:14,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:14,931][root][INFO] - Training Epoch: 2/2, step 3947/7134 completed (loss: 0.018922986462712288, acc: 0.9948186278343201)
[2025-02-13 04:06:15,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:15,323][root][INFO] - Training Epoch: 2/2, step 3948/7134 completed (loss: 0.02301221340894699, acc: 0.9899497628211975)
[2025-02-13 04:06:15,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:15,724][root][INFO] - Training Epoch: 2/2, step 3949/7134 completed (loss: 0.031690601259469986, acc: 0.9906716346740723)
[2025-02-13 04:06:15,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:16,064][root][INFO] - Training Epoch: 2/2, step 3950/7134 completed (loss: 0.03825177997350693, acc: 0.9910714030265808)
[2025-02-13 04:06:16,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:16,430][root][INFO] - Training Epoch: 2/2, step 3951/7134 completed (loss: 0.022127758711576462, acc: 0.9924812316894531)
[2025-02-13 04:06:16,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:16,821][root][INFO] - Training Epoch: 2/2, step 3952/7134 completed (loss: 0.015331706032156944, acc: 0.9948717951774597)
[2025-02-13 04:06:16,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:17,191][root][INFO] - Training Epoch: 2/2, step 3953/7134 completed (loss: 0.03422720730304718, acc: 0.992409884929657)
[2025-02-13 04:06:17,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:17,561][root][INFO] - Training Epoch: 2/2, step 3954/7134 completed (loss: 0.04368825629353523, acc: 0.9896907210350037)
[2025-02-13 04:06:17,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:17,955][root][INFO] - Training Epoch: 2/2, step 3955/7134 completed (loss: 0.030824679881334305, acc: 0.9898819327354431)
[2025-02-13 04:06:18,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:18,358][root][INFO] - Training Epoch: 2/2, step 3956/7134 completed (loss: 0.016578055918216705, acc: 0.9968000054359436)
[2025-02-13 04:06:18,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:18,753][root][INFO] - Training Epoch: 2/2, step 3957/7134 completed (loss: 0.023459451273083687, acc: 0.9887387156486511)
[2025-02-13 04:06:18,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:19,107][root][INFO] - Training Epoch: 2/2, step 3958/7134 completed (loss: 0.05500931665301323, acc: 0.9807692170143127)
[2025-02-13 04:06:19,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:19,514][root][INFO] - Training Epoch: 2/2, step 3959/7134 completed (loss: 0.012329401448369026, acc: 0.997802197933197)
[2025-02-13 04:06:19,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:19,948][root][INFO] - Training Epoch: 2/2, step 3960/7134 completed (loss: 0.027398081496357918, acc: 0.9914529919624329)
[2025-02-13 04:06:20,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:20,366][root][INFO] - Training Epoch: 2/2, step 3961/7134 completed (loss: 0.008573300205171108, acc: 0.995502233505249)
[2025-02-13 04:06:20,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:20,771][root][INFO] - Training Epoch: 2/2, step 3962/7134 completed (loss: 0.016958607360720634, acc: 0.9935275316238403)
[2025-02-13 04:06:20,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:21,191][root][INFO] - Training Epoch: 2/2, step 3963/7134 completed (loss: 0.06370587646961212, acc: 0.9897360801696777)
[2025-02-13 04:06:21,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:21,587][root][INFO] - Training Epoch: 2/2, step 3964/7134 completed (loss: 0.048054587095975876, acc: 0.9888888597488403)
[2025-02-13 04:06:21,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:21,995][root][INFO] - Training Epoch: 2/2, step 3965/7134 completed (loss: 0.03250043839216232, acc: 0.995708167552948)
[2025-02-13 04:06:22,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:22,397][root][INFO] - Training Epoch: 2/2, step 3966/7134 completed (loss: 0.030557915568351746, acc: 0.987364649772644)
[2025-02-13 04:06:22,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:22,829][root][INFO] - Training Epoch: 2/2, step 3967/7134 completed (loss: 0.09144774079322815, acc: 0.9757365584373474)
[2025-02-13 04:06:22,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:23,270][root][INFO] - Training Epoch: 2/2, step 3968/7134 completed (loss: 0.019960524514317513, acc: 0.9960988163948059)
[2025-02-13 04:06:23,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:23,597][root][INFO] - Training Epoch: 2/2, step 3969/7134 completed (loss: 0.04028775542974472, acc: 0.981675386428833)
[2025-02-13 04:06:23,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:24,002][root][INFO] - Training Epoch: 2/2, step 3970/7134 completed (loss: 0.05060102045536041, acc: 0.9899280667304993)
[2025-02-13 04:06:24,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:24,444][root][INFO] - Training Epoch: 2/2, step 3971/7134 completed (loss: 0.11314569413661957, acc: 0.9744245409965515)
[2025-02-13 04:06:24,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:24,851][root][INFO] - Training Epoch: 2/2, step 3972/7134 completed (loss: 0.1423034965991974, acc: 0.969072163105011)
[2025-02-13 04:06:24,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:25,271][root][INFO] - Training Epoch: 2/2, step 3973/7134 completed (loss: 0.041759610176086426, acc: 0.9879102110862732)
[2025-02-13 04:06:25,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:25,704][root][INFO] - Training Epoch: 2/2, step 3974/7134 completed (loss: 0.08483096212148666, acc: 0.9783197641372681)
[2025-02-13 04:06:25,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:26,121][root][INFO] - Training Epoch: 2/2, step 3975/7134 completed (loss: 0.026675615459680557, acc: 0.9939576983451843)
[2025-02-13 04:06:26,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:26,566][root][INFO] - Training Epoch: 2/2, step 3976/7134 completed (loss: 0.11846067756414413, acc: 0.976401150226593)
[2025-02-13 04:06:26,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:26,993][root][INFO] - Training Epoch: 2/2, step 3977/7134 completed (loss: 0.018798254430294037, acc: 0.9965217113494873)
[2025-02-13 04:06:27,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:27,407][root][INFO] - Training Epoch: 2/2, step 3978/7134 completed (loss: 0.023505987599492073, acc: 0.9948253631591797)
[2025-02-13 04:06:27,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:27,843][root][INFO] - Training Epoch: 2/2, step 3979/7134 completed (loss: 0.04270942881703377, acc: 0.9872495532035828)
[2025-02-13 04:06:27,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:28,257][root][INFO] - Training Epoch: 2/2, step 3980/7134 completed (loss: 0.029409855604171753, acc: 0.9893292784690857)
[2025-02-13 04:06:28,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:28,664][root][INFO] - Training Epoch: 2/2, step 3981/7134 completed (loss: 0.06849268078804016, acc: 0.9785932898521423)
[2025-02-13 04:06:28,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:29,083][root][INFO] - Training Epoch: 2/2, step 3982/7134 completed (loss: 0.08082078397274017, acc: 0.9784792065620422)
[2025-02-13 04:06:29,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:29,484][root][INFO] - Training Epoch: 2/2, step 3983/7134 completed (loss: 0.23664672672748566, acc: 0.9489361643791199)
[2025-02-13 04:06:29,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:29,911][root][INFO] - Training Epoch: 2/2, step 3984/7134 completed (loss: 0.07412252575159073, acc: 0.9818181991577148)
[2025-02-13 04:06:30,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:30,306][root][INFO] - Training Epoch: 2/2, step 3985/7134 completed (loss: 0.025374574586749077, acc: 0.9964349269866943)
[2025-02-13 04:06:30,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:30,736][root][INFO] - Training Epoch: 2/2, step 3986/7134 completed (loss: 0.15513180196285248, acc: 0.9636363387107849)
[2025-02-13 04:06:30,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:31,161][root][INFO] - Training Epoch: 2/2, step 3987/7134 completed (loss: 0.0576186366379261, acc: 0.9865642786026001)
[2025-02-13 04:06:31,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:31,599][root][INFO] - Training Epoch: 2/2, step 3988/7134 completed (loss: 0.0594908706843853, acc: 0.983565092086792)
[2025-02-13 04:06:31,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:31,930][root][INFO] - Training Epoch: 2/2, step 3989/7134 completed (loss: 0.1551949679851532, acc: 0.9658848643302917)
[2025-02-13 04:06:32,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:32,349][root][INFO] - Training Epoch: 2/2, step 3990/7134 completed (loss: 0.034591082483530045, acc: 0.9900497794151306)
[2025-02-13 04:06:32,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:32,684][root][INFO] - Training Epoch: 2/2, step 3991/7134 completed (loss: 0.03780073672533035, acc: 0.9933993220329285)
[2025-02-13 04:06:32,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:33,075][root][INFO] - Training Epoch: 2/2, step 3992/7134 completed (loss: 0.05552074685692787, acc: 0.9864864945411682)
[2025-02-13 04:06:33,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:33,449][root][INFO] - Training Epoch: 2/2, step 3993/7134 completed (loss: 0.05563891679048538, acc: 0.9816513657569885)
[2025-02-13 04:06:33,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:33,814][root][INFO] - Training Epoch: 2/2, step 3994/7134 completed (loss: 0.053461283445358276, acc: 0.9904761910438538)
[2025-02-13 04:06:33,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:34,185][root][INFO] - Training Epoch: 2/2, step 3995/7134 completed (loss: 0.10551555454730988, acc: 0.9806201457977295)
[2025-02-13 04:06:34,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:34,547][root][INFO] - Training Epoch: 2/2, step 3996/7134 completed (loss: 0.04864040017127991, acc: 0.9856262803077698)
[2025-02-13 04:06:34,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:34,924][root][INFO] - Training Epoch: 2/2, step 3997/7134 completed (loss: 0.11969154328107834, acc: 0.9736263751983643)
[2025-02-13 04:06:35,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:35,269][root][INFO] - Training Epoch: 2/2, step 3998/7134 completed (loss: 0.07863976806402206, acc: 0.9759299755096436)
[2025-02-13 04:06:35,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:35,629][root][INFO] - Training Epoch: 2/2, step 3999/7134 completed (loss: 0.04518893361091614, acc: 0.9840319156646729)
[2025-02-13 04:06:35,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:36,027][root][INFO] - Training Epoch: 2/2, step 4000/7134 completed (loss: 0.03782355785369873, acc: 0.9903475046157837)
[2025-02-13 04:06:36,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:36,419][root][INFO] - Training Epoch: 2/2, step 4001/7134 completed (loss: 0.07309841364622116, acc: 0.9781312346458435)
[2025-02-13 04:06:36,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:36,810][root][INFO] - Training Epoch: 2/2, step 4002/7134 completed (loss: 0.08952835202217102, acc: 0.983132541179657)
[2025-02-13 04:06:36,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:37,228][root][INFO] - Training Epoch: 2/2, step 4003/7134 completed (loss: 0.09754204005002975, acc: 0.9760147333145142)
[2025-02-13 04:06:37,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:37,584][root][INFO] - Training Epoch: 2/2, step 4004/7134 completed (loss: 0.03229182958602905, acc: 0.9930070042610168)
[2025-02-13 04:06:37,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:37,984][root][INFO] - Training Epoch: 2/2, step 4005/7134 completed (loss: 0.028328606858849525, acc: 0.9937205910682678)
[2025-02-13 04:06:38,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:38,446][root][INFO] - Training Epoch: 2/2, step 4006/7134 completed (loss: 0.02361341379582882, acc: 0.9916753172874451)
[2025-02-13 04:06:38,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:38,870][root][INFO] - Training Epoch: 2/2, step 4007/7134 completed (loss: 0.028234615921974182, acc: 0.9925037622451782)
[2025-02-13 04:06:39,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:39,326][root][INFO] - Training Epoch: 2/2, step 4008/7134 completed (loss: 0.01519104279577732, acc: 0.9957716464996338)
[2025-02-13 04:06:39,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:39,761][root][INFO] - Training Epoch: 2/2, step 4009/7134 completed (loss: 0.013012059032917023, acc: 0.9960052967071533)
[2025-02-13 04:06:39,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:40,217][root][INFO] - Training Epoch: 2/2, step 4010/7134 completed (loss: 0.010741475969552994, acc: 1.0)
[2025-02-13 04:06:40,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:40,661][root][INFO] - Training Epoch: 2/2, step 4011/7134 completed (loss: 0.025289366021752357, acc: 0.9946879148483276)
[2025-02-13 04:06:40,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:41,067][root][INFO] - Training Epoch: 2/2, step 4012/7134 completed (loss: 0.008322334848344326, acc: 0.9985875487327576)
[2025-02-13 04:06:41,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:41,501][root][INFO] - Training Epoch: 2/2, step 4013/7134 completed (loss: 0.01637245900928974, acc: 0.9951768517494202)
[2025-02-13 04:06:41,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:41,947][root][INFO] - Training Epoch: 2/2, step 4014/7134 completed (loss: 0.008604884147644043, acc: 0.9968652129173279)
[2025-02-13 04:06:42,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:42,363][root][INFO] - Training Epoch: 2/2, step 4015/7134 completed (loss: 0.01787855289876461, acc: 0.9938555955886841)
[2025-02-13 04:06:42,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:42,829][root][INFO] - Training Epoch: 2/2, step 4016/7134 completed (loss: 0.02875811792910099, acc: 0.9910714030265808)
[2025-02-13 04:06:42,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:43,282][root][INFO] - Training Epoch: 2/2, step 4017/7134 completed (loss: 0.02326279878616333, acc: 0.9953810572624207)
[2025-02-13 04:06:43,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:43,724][root][INFO] - Training Epoch: 2/2, step 4018/7134 completed (loss: 0.022431667894124985, acc: 0.996363639831543)
[2025-02-13 04:06:43,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:44,153][root][INFO] - Training Epoch: 2/2, step 4019/7134 completed (loss: 0.047936685383319855, acc: 0.9873096346855164)
[2025-02-13 04:06:44,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:44,553][root][INFO] - Training Epoch: 2/2, step 4020/7134 completed (loss: 0.036368753761053085, acc: 0.9822404384613037)
[2025-02-13 04:06:44,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:44,927][root][INFO] - Training Epoch: 2/2, step 4021/7134 completed (loss: 0.01285109668970108, acc: 0.9946523904800415)
[2025-02-13 04:06:45,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:45,367][root][INFO] - Training Epoch: 2/2, step 4022/7134 completed (loss: 0.02219581790268421, acc: 0.991411030292511)
[2025-02-13 04:06:45,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:45,828][root][INFO] - Training Epoch: 2/2, step 4023/7134 completed (loss: 0.04118923470377922, acc: 0.9899371266365051)
[2025-02-13 04:06:45,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:46,261][root][INFO] - Training Epoch: 2/2, step 4024/7134 completed (loss: 0.031117746606469154, acc: 0.9895012974739075)
[2025-02-13 04:06:46,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:46,697][root][INFO] - Training Epoch: 2/2, step 4025/7134 completed (loss: 0.05091214179992676, acc: 0.987542450428009)
[2025-02-13 04:06:46,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:47,103][root][INFO] - Training Epoch: 2/2, step 4026/7134 completed (loss: 0.060053322464227676, acc: 0.9882006049156189)
[2025-02-13 04:06:47,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:47,533][root][INFO] - Training Epoch: 2/2, step 4027/7134 completed (loss: 0.03668772429227829, acc: 0.9880095720291138)
[2025-02-13 04:06:47,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:47,943][root][INFO] - Training Epoch: 2/2, step 4028/7134 completed (loss: 0.06137554347515106, acc: 0.9845094680786133)
[2025-02-13 04:06:48,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:48,388][root][INFO] - Training Epoch: 2/2, step 4029/7134 completed (loss: 0.01951250061392784, acc: 0.9938499331474304)
[2025-02-13 04:06:48,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:48,818][root][INFO] - Training Epoch: 2/2, step 4030/7134 completed (loss: 0.007570735644549131, acc: 0.9979466199874878)
[2025-02-13 04:06:48,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:49,225][root][INFO] - Training Epoch: 2/2, step 4031/7134 completed (loss: 0.01775176450610161, acc: 0.9941349029541016)
[2025-02-13 04:06:49,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:49,653][root][INFO] - Training Epoch: 2/2, step 4032/7134 completed (loss: 0.05010226368904114, acc: 0.9919678568840027)
[2025-02-13 04:06:49,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:50,052][root][INFO] - Training Epoch: 2/2, step 4033/7134 completed (loss: 0.016516372561454773, acc: 0.9911949634552002)
[2025-02-13 04:06:50,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:50,487][root][INFO] - Training Epoch: 2/2, step 4034/7134 completed (loss: 0.005334609653800726, acc: 0.9987819790840149)
[2025-02-13 04:06:50,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:50,997][root][INFO] - Training Epoch: 2/2, step 4035/7134 completed (loss: 0.03981102630496025, acc: 0.9881936311721802)
[2025-02-13 04:06:51,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:51,403][root][INFO] - Training Epoch: 2/2, step 4036/7134 completed (loss: 0.032711900770664215, acc: 0.9890109896659851)
[2025-02-13 04:06:51,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:51,860][root][INFO] - Training Epoch: 2/2, step 4037/7134 completed (loss: 0.020838182419538498, acc: 0.9927797913551331)
[2025-02-13 04:06:51,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:52,284][root][INFO] - Training Epoch: 2/2, step 4038/7134 completed (loss: 0.029381785541772842, acc: 0.9944979548454285)
[2025-02-13 04:06:52,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:52,682][root][INFO] - Training Epoch: 2/2, step 4039/7134 completed (loss: 0.03480237349867821, acc: 0.9886524677276611)
[2025-02-13 04:06:52,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:53,106][root][INFO] - Training Epoch: 2/2, step 4040/7134 completed (loss: 0.012911762110888958, acc: 0.993842363357544)
[2025-02-13 04:06:53,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:53,557][root][INFO] - Training Epoch: 2/2, step 4041/7134 completed (loss: 0.034437816590070724, acc: 0.9919354915618896)
[2025-02-13 04:06:53,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:53,979][root][INFO] - Training Epoch: 2/2, step 4042/7134 completed (loss: 0.02738307975232601, acc: 0.9897058606147766)
[2025-02-13 04:06:54,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:54,366][root][INFO] - Training Epoch: 2/2, step 4043/7134 completed (loss: 0.013005798682570457, acc: 0.9926578402519226)
[2025-02-13 04:06:54,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:54,794][root][INFO] - Training Epoch: 2/2, step 4044/7134 completed (loss: 0.008048987947404385, acc: 0.9987373948097229)
[2025-02-13 04:06:54,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:55,240][root][INFO] - Training Epoch: 2/2, step 4045/7134 completed (loss: 0.047694217413663864, acc: 0.987500011920929)
[2025-02-13 04:06:55,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:55,670][root][INFO] - Training Epoch: 2/2, step 4046/7134 completed (loss: 0.03258305788040161, acc: 0.9917582273483276)
[2025-02-13 04:06:55,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:56,117][root][INFO] - Training Epoch: 2/2, step 4047/7134 completed (loss: 0.049213506281375885, acc: 0.9892601370811462)
[2025-02-13 04:06:56,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:56,570][root][INFO] - Training Epoch: 2/2, step 4048/7134 completed (loss: 0.023422492668032646, acc: 0.9941725134849548)
[2025-02-13 04:06:56,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:57,040][root][INFO] - Training Epoch: 2/2, step 4049/7134 completed (loss: 0.027381232008337975, acc: 0.9931034445762634)
[2025-02-13 04:06:57,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:57,459][root][INFO] - Training Epoch: 2/2, step 4050/7134 completed (loss: 0.04081931337714195, acc: 0.9872340559959412)
[2025-02-13 04:06:57,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:57,900][root][INFO] - Training Epoch: 2/2, step 4051/7134 completed (loss: 0.05437818169593811, acc: 0.9916267991065979)
[2025-02-13 04:06:58,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:58,294][root][INFO] - Training Epoch: 2/2, step 4052/7134 completed (loss: 0.024097977206110954, acc: 0.9935400485992432)
[2025-02-13 04:06:58,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:58,728][root][INFO] - Training Epoch: 2/2, step 4053/7134 completed (loss: 0.024040859192609787, acc: 0.9867374300956726)
[2025-02-13 04:06:58,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:59,167][root][INFO] - Training Epoch: 2/2, step 4054/7134 completed (loss: 0.04317130893468857, acc: 0.9940000176429749)
[2025-02-13 04:06:59,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:59,623][root][INFO] - Training Epoch: 2/2, step 4055/7134 completed (loss: 0.020791180431842804, acc: 0.9902912378311157)
[2025-02-13 04:06:59,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:00,071][root][INFO] - Training Epoch: 2/2, step 4056/7134 completed (loss: 0.022392742335796356, acc: 0.9915151596069336)
[2025-02-13 04:07:00,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:00,513][root][INFO] - Training Epoch: 2/2, step 4057/7134 completed (loss: 0.053096748888492584, acc: 0.9888734221458435)
[2025-02-13 04:07:00,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:00,855][root][INFO] - Training Epoch: 2/2, step 4058/7134 completed (loss: 0.059230297803878784, acc: 0.9789674878120422)
[2025-02-13 04:07:00,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:01,254][root][INFO] - Training Epoch: 2/2, step 4059/7134 completed (loss: 0.027576006948947906, acc: 0.9927007555961609)
[2025-02-13 04:07:01,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:01,646][root][INFO] - Training Epoch: 2/2, step 4060/7134 completed (loss: 0.020217686891555786, acc: 0.9938176274299622)
[2025-02-13 04:07:01,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:02,070][root][INFO] - Training Epoch: 2/2, step 4061/7134 completed (loss: 0.018021170049905777, acc: 0.994301974773407)
[2025-02-13 04:07:02,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:02,477][root][INFO] - Training Epoch: 2/2, step 4062/7134 completed (loss: 0.007038980722427368, acc: 0.9968503713607788)
[2025-02-13 04:07:02,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:02,830][root][INFO] - Training Epoch: 2/2, step 4063/7134 completed (loss: 0.01438350323587656, acc: 0.9929328560829163)
[2025-02-13 04:07:02,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:03,255][root][INFO] - Training Epoch: 2/2, step 4064/7134 completed (loss: 0.008623950183391571, acc: 0.9969742894172668)
[2025-02-13 04:07:03,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:03,686][root][INFO] - Training Epoch: 2/2, step 4065/7134 completed (loss: 0.015034504234790802, acc: 0.9944827556610107)
[2025-02-13 04:07:03,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:04,127][root][INFO] - Training Epoch: 2/2, step 4066/7134 completed (loss: 0.008168285712599754, acc: 0.9972222447395325)
[2025-02-13 04:07:04,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:04,529][root][INFO] - Training Epoch: 2/2, step 4067/7134 completed (loss: 0.031556349247694016, acc: 0.9901960492134094)
[2025-02-13 04:07:04,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:04,932][root][INFO] - Training Epoch: 2/2, step 4068/7134 completed (loss: 0.040381886065006256, acc: 0.9888888597488403)
[2025-02-13 04:07:05,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:05,338][root][INFO] - Training Epoch: 2/2, step 4069/7134 completed (loss: 0.02070220746099949, acc: 0.9968701004981995)
[2025-02-13 04:07:05,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:05,746][root][INFO] - Training Epoch: 2/2, step 4070/7134 completed (loss: 0.02521427534520626, acc: 0.9893454909324646)
[2025-02-13 04:07:05,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:06,141][root][INFO] - Training Epoch: 2/2, step 4071/7134 completed (loss: 0.02599584497511387, acc: 0.9901153445243835)
[2025-02-13 04:07:06,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:06,560][root][INFO] - Training Epoch: 2/2, step 4072/7134 completed (loss: 0.011730809696018696, acc: 0.9971140027046204)
[2025-02-13 04:07:06,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:06,983][root][INFO] - Training Epoch: 2/2, step 4073/7134 completed (loss: 0.01800539530813694, acc: 0.9913793206214905)
[2025-02-13 04:07:07,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:07,405][root][INFO] - Training Epoch: 2/2, step 4074/7134 completed (loss: 0.02445434033870697, acc: 0.995708167552948)
[2025-02-13 04:07:07,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:07,830][root][INFO] - Training Epoch: 2/2, step 4075/7134 completed (loss: 0.01230858825147152, acc: 0.995502233505249)
[2025-02-13 04:07:07,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:08,272][root][INFO] - Training Epoch: 2/2, step 4076/7134 completed (loss: 0.01135334838181734, acc: 0.9956584572792053)
[2025-02-13 04:07:08,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:08,666][root][INFO] - Training Epoch: 2/2, step 4077/7134 completed (loss: 0.03809826448559761, acc: 0.989847719669342)
[2025-02-13 04:07:08,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:09,078][root][INFO] - Training Epoch: 2/2, step 4078/7134 completed (loss: 0.014238792471587658, acc: 0.995726466178894)
[2025-02-13 04:07:09,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:09,418][root][INFO] - Training Epoch: 2/2, step 4079/7134 completed (loss: 0.018937237560749054, acc: 0.9946523904800415)
[2025-02-13 04:07:09,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:09,800][root][INFO] - Training Epoch: 2/2, step 4080/7134 completed (loss: 0.008127129636704922, acc: 0.996835470199585)
[2025-02-13 04:07:09,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:10,183][root][INFO] - Training Epoch: 2/2, step 4081/7134 completed (loss: 0.005176446866244078, acc: 0.9983498454093933)
[2025-02-13 04:07:10,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:10,574][root][INFO] - Training Epoch: 2/2, step 4082/7134 completed (loss: 0.03893812373280525, acc: 0.9815157055854797)
[2025-02-13 04:07:10,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:10,981][root][INFO] - Training Epoch: 2/2, step 4083/7134 completed (loss: 0.04078809544444084, acc: 0.9895209670066833)
[2025-02-13 04:07:11,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:11,380][root][INFO] - Training Epoch: 2/2, step 4084/7134 completed (loss: 0.0783718079328537, acc: 0.9725490212440491)
[2025-02-13 04:07:11,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:11,801][root][INFO] - Training Epoch: 2/2, step 4085/7134 completed (loss: 0.03242877870798111, acc: 0.9905660152435303)
[2025-02-13 04:07:11,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:12,206][root][INFO] - Training Epoch: 2/2, step 4086/7134 completed (loss: 0.06227382645010948, acc: 0.9836333990097046)
[2025-02-13 04:07:12,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:12,645][root][INFO] - Training Epoch: 2/2, step 4087/7134 completed (loss: 0.026984697207808495, acc: 0.9961240291595459)
[2025-02-13 04:07:12,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:13,076][root][INFO] - Training Epoch: 2/2, step 4088/7134 completed (loss: 0.06244142726063728, acc: 0.9838308691978455)
[2025-02-13 04:07:13,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:13,478][root][INFO] - Training Epoch: 2/2, step 4089/7134 completed (loss: 0.05076010897755623, acc: 0.9840142130851746)
[2025-02-13 04:07:13,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:13,889][root][INFO] - Training Epoch: 2/2, step 4090/7134 completed (loss: 0.094234399497509, acc: 0.9775725603103638)
[2025-02-13 04:07:14,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:14,293][root][INFO] - Training Epoch: 2/2, step 4091/7134 completed (loss: 0.1185954362154007, acc: 0.9713321924209595)
[2025-02-13 04:07:14,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:14,736][root][INFO] - Training Epoch: 2/2, step 4092/7134 completed (loss: 0.047212865203619, acc: 0.9897040128707886)
[2025-02-13 04:07:14,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:15,112][root][INFO] - Training Epoch: 2/2, step 4093/7134 completed (loss: 0.019017715007066727, acc: 0.9963570237159729)
[2025-02-13 04:07:15,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:15,523][root][INFO] - Training Epoch: 2/2, step 4094/7134 completed (loss: 0.0588611401617527, acc: 0.9864864945411682)
[2025-02-13 04:07:15,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:15,969][root][INFO] - Training Epoch: 2/2, step 4095/7134 completed (loss: 0.020073048770427704, acc: 0.9971056580543518)
[2025-02-13 04:07:16,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:16,331][root][INFO] - Training Epoch: 2/2, step 4096/7134 completed (loss: 0.07821694761514664, acc: 0.9850746393203735)
[2025-02-13 04:07:16,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:16,782][root][INFO] - Training Epoch: 2/2, step 4097/7134 completed (loss: 0.0468464158475399, acc: 0.9903225898742676)
[2025-02-13 04:07:16,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:17,200][root][INFO] - Training Epoch: 2/2, step 4098/7134 completed (loss: 0.035564739257097244, acc: 0.9918699264526367)
[2025-02-13 04:07:17,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:17,603][root][INFO] - Training Epoch: 2/2, step 4099/7134 completed (loss: 0.03426317870616913, acc: 0.9894366264343262)
[2025-02-13 04:07:17,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:18,054][root][INFO] - Training Epoch: 2/2, step 4100/7134 completed (loss: 0.008310269564390182, acc: 0.998487114906311)
[2025-02-13 04:07:18,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:18,451][root][INFO] - Training Epoch: 2/2, step 4101/7134 completed (loss: 0.01981908641755581, acc: 0.9942965507507324)
[2025-02-13 04:07:18,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:18,864][root][INFO] - Training Epoch: 2/2, step 4102/7134 completed (loss: 0.028700148686766624, acc: 0.9926874041557312)
[2025-02-13 04:07:19,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:19,279][root][INFO] - Training Epoch: 2/2, step 4103/7134 completed (loss: 0.0038044077809900045, acc: 1.0)
[2025-02-13 04:07:19,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:19,688][root][INFO] - Training Epoch: 2/2, step 4104/7134 completed (loss: 0.008775508962571621, acc: 0.9983079433441162)
[2025-02-13 04:07:19,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:20,103][root][INFO] - Training Epoch: 2/2, step 4105/7134 completed (loss: 0.022724436596035957, acc: 0.995468258857727)
[2025-02-13 04:07:20,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:20,511][root][INFO] - Training Epoch: 2/2, step 4106/7134 completed (loss: 0.02057008445262909, acc: 0.9938650131225586)
[2025-02-13 04:07:20,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:20,897][root][INFO] - Training Epoch: 2/2, step 4107/7134 completed (loss: 0.022037696093320847, acc: 0.9966386556625366)
[2025-02-13 04:07:21,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:21,279][root][INFO] - Training Epoch: 2/2, step 4108/7134 completed (loss: 0.024042071774601936, acc: 0.9935275316238403)
[2025-02-13 04:07:21,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:21,682][root][INFO] - Training Epoch: 2/2, step 4109/7134 completed (loss: 0.013321277685463428, acc: 0.9983948469161987)
[2025-02-13 04:07:21,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:22,084][root][INFO] - Training Epoch: 2/2, step 4110/7134 completed (loss: 0.002868782030418515, acc: 1.0)
[2025-02-13 04:07:22,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:22,508][root][INFO] - Training Epoch: 2/2, step 4111/7134 completed (loss: 0.013263548724353313, acc: 0.9968454241752625)
[2025-02-13 04:07:22,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:22,935][root][INFO] - Training Epoch: 2/2, step 4112/7134 completed (loss: 0.029606958851218224, acc: 0.9892473220825195)
[2025-02-13 04:07:23,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:23,358][root][INFO] - Training Epoch: 2/2, step 4113/7134 completed (loss: 0.03514450043439865, acc: 0.9906976819038391)
[2025-02-13 04:07:23,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:23,752][root][INFO] - Training Epoch: 2/2, step 4114/7134 completed (loss: 0.09154021739959717, acc: 0.9695122241973877)
[2025-02-13 04:07:23,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:24,150][root][INFO] - Training Epoch: 2/2, step 4115/7134 completed (loss: 0.038033489137887955, acc: 0.9850746393203735)
[2025-02-13 04:07:24,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:24,582][root][INFO] - Training Epoch: 2/2, step 4116/7134 completed (loss: 0.007940898649394512, acc: 0.9983713626861572)
[2025-02-13 04:07:24,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:24,989][root][INFO] - Training Epoch: 2/2, step 4117/7134 completed (loss: 0.017930177971720695, acc: 0.9936908483505249)
[2025-02-13 04:07:25,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:25,334][root][INFO] - Training Epoch: 2/2, step 4118/7134 completed (loss: 0.012935221195220947, acc: 0.9950860142707825)
[2025-02-13 04:07:25,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:25,736][root][INFO] - Training Epoch: 2/2, step 4119/7134 completed (loss: 0.04045700281858444, acc: 0.9903069734573364)
[2025-02-13 04:07:25,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:26,106][root][INFO] - Training Epoch: 2/2, step 4120/7134 completed (loss: 0.03515439108014107, acc: 0.9934102296829224)
[2025-02-13 04:07:26,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:26,517][root][INFO] - Training Epoch: 2/2, step 4121/7134 completed (loss: 0.04686799272894859, acc: 0.9914772510528564)
[2025-02-13 04:07:26,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:26,919][root][INFO] - Training Epoch: 2/2, step 4122/7134 completed (loss: 0.022542236372828484, acc: 0.9923664331436157)
[2025-02-13 04:07:27,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:27,323][root][INFO] - Training Epoch: 2/2, step 4123/7134 completed (loss: 0.030768983066082, acc: 0.9909502267837524)
[2025-02-13 04:07:27,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:27,746][root][INFO] - Training Epoch: 2/2, step 4124/7134 completed (loss: 0.03107827715575695, acc: 0.9915134310722351)
[2025-02-13 04:07:27,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:28,152][root][INFO] - Training Epoch: 2/2, step 4125/7134 completed (loss: 0.018876278772950172, acc: 0.9941176176071167)
[2025-02-13 04:07:28,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:28,560][root][INFO] - Training Epoch: 2/2, step 4126/7134 completed (loss: 0.015084413811564445, acc: 0.9957507252693176)
[2025-02-13 04:07:28,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:28,952][root][INFO] - Training Epoch: 2/2, step 4127/7134 completed (loss: 0.03577710688114166, acc: 0.9906250238418579)
[2025-02-13 04:07:29,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:29,349][root][INFO] - Training Epoch: 2/2, step 4128/7134 completed (loss: 0.025010762736201286, acc: 0.9906103014945984)
[2025-02-13 04:07:29,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:29,795][root][INFO] - Training Epoch: 2/2, step 4129/7134 completed (loss: 0.011312177404761314, acc: 0.9940652847290039)
[2025-02-13 04:07:29,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:30,186][root][INFO] - Training Epoch: 2/2, step 4130/7134 completed (loss: 0.01974763348698616, acc: 0.9922928810119629)
[2025-02-13 04:07:30,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:30,583][root][INFO] - Training Epoch: 2/2, step 4131/7134 completed (loss: 0.012896179221570492, acc: 0.9944547414779663)
[2025-02-13 04:07:30,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:30,976][root][INFO] - Training Epoch: 2/2, step 4132/7134 completed (loss: 0.025786785408854485, acc: 0.9933035969734192)
[2025-02-13 04:07:31,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:31,342][root][INFO] - Training Epoch: 2/2, step 4133/7134 completed (loss: 0.04328024014830589, acc: 0.9833333492279053)
[2025-02-13 04:07:31,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:31,709][root][INFO] - Training Epoch: 2/2, step 4134/7134 completed (loss: 0.0526871494948864, acc: 0.9810426831245422)
[2025-02-13 04:07:31,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:32,114][root][INFO] - Training Epoch: 2/2, step 4135/7134 completed (loss: 0.01941343955695629, acc: 0.9946714043617249)
[2025-02-13 04:07:32,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:32,546][root][INFO] - Training Epoch: 2/2, step 4136/7134 completed (loss: 0.023518895730376244, acc: 0.9964601993560791)
[2025-02-13 04:07:32,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:32,953][root][INFO] - Training Epoch: 2/2, step 4137/7134 completed (loss: 0.025834716856479645, acc: 0.990234375)
[2025-02-13 04:07:33,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:33,354][root][INFO] - Training Epoch: 2/2, step 4138/7134 completed (loss: 0.027346745133399963, acc: 0.9890282154083252)
[2025-02-13 04:07:33,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:33,746][root][INFO] - Training Epoch: 2/2, step 4139/7134 completed (loss: 0.023001838475465775, acc: 0.995708167552948)
[2025-02-13 04:07:33,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:34,154][root][INFO] - Training Epoch: 2/2, step 4140/7134 completed (loss: 0.04276224225759506, acc: 0.9889655113220215)
[2025-02-13 04:07:34,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:34,625][root][INFO] - Training Epoch: 2/2, step 4141/7134 completed (loss: 0.062224604189395905, acc: 0.982807993888855)
[2025-02-13 04:07:34,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:35,075][root][INFO] - Training Epoch: 2/2, step 4142/7134 completed (loss: 0.07819227874279022, acc: 0.9805492162704468)
[2025-02-13 04:07:35,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:35,550][root][INFO] - Training Epoch: 2/2, step 4143/7134 completed (loss: 0.04369845613837242, acc: 0.9918367266654968)
[2025-02-13 04:07:35,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:36,023][root][INFO] - Training Epoch: 2/2, step 4144/7134 completed (loss: 0.06473343074321747, acc: 0.9829156994819641)
[2025-02-13 04:07:36,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:36,454][root][INFO] - Training Epoch: 2/2, step 4145/7134 completed (loss: 0.030468087643384933, acc: 0.9909747242927551)
[2025-02-13 04:07:36,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:36,805][root][INFO] - Training Epoch: 2/2, step 4146/7134 completed (loss: 0.07576625049114227, acc: 0.9858585596084595)
[2025-02-13 04:07:36,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:37,217][root][INFO] - Training Epoch: 2/2, step 4147/7134 completed (loss: 0.06976041197776794, acc: 0.9885057210922241)
[2025-02-13 04:07:37,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:37,619][root][INFO] - Training Epoch: 2/2, step 4148/7134 completed (loss: 0.12357663363218307, acc: 0.969111979007721)
[2025-02-13 04:07:37,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:37,940][root][INFO] - Training Epoch: 2/2, step 4149/7134 completed (loss: 0.11068861931562424, acc: 0.9706666469573975)
[2025-02-13 04:07:38,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:38,343][root][INFO] - Training Epoch: 2/2, step 4150/7134 completed (loss: 0.11608990281820297, acc: 0.9678068161010742)
[2025-02-13 04:07:38,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:38,806][root][INFO] - Training Epoch: 2/2, step 4151/7134 completed (loss: 0.14672090113162994, acc: 0.9764492511749268)
[2025-02-13 04:07:38,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:39,236][root][INFO] - Training Epoch: 2/2, step 4152/7134 completed (loss: 0.05261597782373428, acc: 0.9860896468162537)
[2025-02-13 04:07:39,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:39,669][root][INFO] - Training Epoch: 2/2, step 4153/7134 completed (loss: 0.08556962013244629, acc: 0.9831288456916809)
[2025-02-13 04:07:39,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:40,129][root][INFO] - Training Epoch: 2/2, step 4154/7134 completed (loss: 0.051791492849588394, acc: 0.9849537014961243)
[2025-02-13 04:07:40,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:40,515][root][INFO] - Training Epoch: 2/2, step 4155/7134 completed (loss: 0.05175461620092392, acc: 0.9845626354217529)
[2025-02-13 04:07:40,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:40,957][root][INFO] - Training Epoch: 2/2, step 4156/7134 completed (loss: 0.05721202865242958, acc: 0.9833101630210876)
[2025-02-13 04:07:41,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:41,356][root][INFO] - Training Epoch: 2/2, step 4157/7134 completed (loss: 0.045120567083358765, acc: 0.9880239367485046)
[2025-02-13 04:07:41,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:41,713][root][INFO] - Training Epoch: 2/2, step 4158/7134 completed (loss: 0.12326254695653915, acc: 0.9691516757011414)
[2025-02-13 04:07:41,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:42,147][root][INFO] - Training Epoch: 2/2, step 4159/7134 completed (loss: 0.04792160913348198, acc: 0.9847198724746704)
[2025-02-13 04:07:42,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:42,596][root][INFO] - Training Epoch: 2/2, step 4160/7134 completed (loss: 0.07565045356750488, acc: 0.9792453050613403)
[2025-02-13 04:07:42,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:43,057][root][INFO] - Training Epoch: 2/2, step 4161/7134 completed (loss: 0.049734313040971756, acc: 0.9899598360061646)
[2025-02-13 04:07:43,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:43,473][root][INFO] - Training Epoch: 2/2, step 4162/7134 completed (loss: 0.05948353931307793, acc: 0.9836065769195557)
[2025-02-13 04:07:43,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:43,900][root][INFO] - Training Epoch: 2/2, step 4163/7134 completed (loss: 0.055766016244888306, acc: 0.9878970980644226)
[2025-02-13 04:07:44,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:44,319][root][INFO] - Training Epoch: 2/2, step 4164/7134 completed (loss: 0.04915214702486992, acc: 0.9840348362922668)
[2025-02-13 04:07:44,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:44,781][root][INFO] - Training Epoch: 2/2, step 4165/7134 completed (loss: 0.05853728577494621, acc: 0.9850187301635742)
[2025-02-13 04:07:44,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:45,223][root][INFO] - Training Epoch: 2/2, step 4166/7134 completed (loss: 0.049922019243240356, acc: 0.9838472604751587)
[2025-02-13 04:07:45,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:45,616][root][INFO] - Training Epoch: 2/2, step 4167/7134 completed (loss: 0.01177566684782505, acc: 0.9982638955116272)
[2025-02-13 04:07:45,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:46,051][root][INFO] - Training Epoch: 2/2, step 4168/7134 completed (loss: 0.03629249706864357, acc: 0.9930555820465088)
[2025-02-13 04:07:46,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:46,523][root][INFO] - Training Epoch: 2/2, step 4169/7134 completed (loss: 0.055712223052978516, acc: 0.9915522933006287)
[2025-02-13 04:07:46,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:47,003][root][INFO] - Training Epoch: 2/2, step 4170/7134 completed (loss: 0.07975228875875473, acc: 0.9766454100608826)
[2025-02-13 04:07:47,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:47,500][root][INFO] - Training Epoch: 2/2, step 4171/7134 completed (loss: 0.020171822980046272, acc: 0.9942330121994019)
[2025-02-13 04:07:47,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:47,928][root][INFO] - Training Epoch: 2/2, step 4172/7134 completed (loss: 0.05118393898010254, acc: 0.9862843155860901)
[2025-02-13 04:07:48,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:48,309][root][INFO] - Training Epoch: 2/2, step 4173/7134 completed (loss: 0.04604794830083847, acc: 0.9849785566329956)
[2025-02-13 04:07:48,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:48,706][root][INFO] - Training Epoch: 2/2, step 4174/7134 completed (loss: 0.0052902596071362495, acc: 1.0)
[2025-02-13 04:07:48,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:49,110][root][INFO] - Training Epoch: 2/2, step 4175/7134 completed (loss: 0.03149793669581413, acc: 0.9914089441299438)
[2025-02-13 04:07:49,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:49,543][root][INFO] - Training Epoch: 2/2, step 4176/7134 completed (loss: 0.016408191993832588, acc: 0.9940387606620789)
[2025-02-13 04:07:49,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:50,013][root][INFO] - Training Epoch: 2/2, step 4177/7134 completed (loss: 0.018438449129462242, acc: 0.9934297204017639)
[2025-02-13 04:07:50,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:50,457][root][INFO] - Training Epoch: 2/2, step 4178/7134 completed (loss: 0.017850713804364204, acc: 0.9924471378326416)
[2025-02-13 04:07:50,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:50,930][root][INFO] - Training Epoch: 2/2, step 4179/7134 completed (loss: 0.010506636463105679, acc: 0.9960629940032959)
[2025-02-13 04:07:51,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:51,374][root][INFO] - Training Epoch: 2/2, step 4180/7134 completed (loss: 0.01089775562286377, acc: 0.9985875487327576)
[2025-02-13 04:07:51,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:51,797][root][INFO] - Training Epoch: 2/2, step 4181/7134 completed (loss: 0.00982697680592537, acc: 0.995106041431427)
[2025-02-13 04:07:51,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:52,229][root][INFO] - Training Epoch: 2/2, step 4182/7134 completed (loss: 0.004928643349558115, acc: 1.0)
[2025-02-13 04:07:52,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:52,694][root][INFO] - Training Epoch: 2/2, step 4183/7134 completed (loss: 0.013384438119828701, acc: 0.9958275556564331)
[2025-02-13 04:07:52,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:53,107][root][INFO] - Training Epoch: 2/2, step 4184/7134 completed (loss: 0.028195954859256744, acc: 0.9920424222946167)
[2025-02-13 04:07:53,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:53,549][root][INFO] - Training Epoch: 2/2, step 4185/7134 completed (loss: 0.007848490960896015, acc: 0.9977169036865234)
[2025-02-13 04:07:53,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:54,010][root][INFO] - Training Epoch: 2/2, step 4186/7134 completed (loss: 0.0630684345960617, acc: 0.9903730154037476)
[2025-02-13 04:07:54,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:54,412][root][INFO] - Training Epoch: 2/2, step 4187/7134 completed (loss: 0.05150816962122917, acc: 0.9921875)
[2025-02-13 04:07:54,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:54,855][root][INFO] - Training Epoch: 2/2, step 4188/7134 completed (loss: 0.009929461404681206, acc: 0.9950000047683716)
[2025-02-13 04:07:54,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:55,297][root][INFO] - Training Epoch: 2/2, step 4189/7134 completed (loss: 0.016984159126877785, acc: 0.9972028136253357)
[2025-02-13 04:07:55,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:55,727][root][INFO] - Training Epoch: 2/2, step 4190/7134 completed (loss: 0.021474063396453857, acc: 0.9957386255264282)
[2025-02-13 04:07:55,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:56,051][root][INFO] - Training Epoch: 2/2, step 4191/7134 completed (loss: 0.028533540666103363, acc: 0.9953917264938354)
[2025-02-13 04:07:56,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:56,481][root][INFO] - Training Epoch: 2/2, step 4192/7134 completed (loss: 0.00952002964913845, acc: 0.9968051314353943)
[2025-02-13 04:07:56,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:56,924][root][INFO] - Training Epoch: 2/2, step 4193/7134 completed (loss: 0.02087341994047165, acc: 0.9959893226623535)
[2025-02-13 04:07:57,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:57,365][root][INFO] - Training Epoch: 2/2, step 4194/7134 completed (loss: 0.010053738951683044, acc: 0.9969742894172668)
[2025-02-13 04:07:57,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:57,774][root][INFO] - Training Epoch: 2/2, step 4195/7134 completed (loss: 0.03616863861680031, acc: 0.989159882068634)
[2025-02-13 04:07:57,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:58,165][root][INFO] - Training Epoch: 2/2, step 4196/7134 completed (loss: 0.006927973590791225, acc: 0.9965397715568542)
[2025-02-13 04:07:58,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:58,618][root][INFO] - Training Epoch: 2/2, step 4197/7134 completed (loss: 0.032917387783527374, acc: 0.9896507263183594)
[2025-02-13 04:07:58,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:59,054][root][INFO] - Training Epoch: 2/2, step 4198/7134 completed (loss: 0.01848568581044674, acc: 0.9944444298744202)
[2025-02-13 04:07:59,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:59,446][root][INFO] - Training Epoch: 2/2, step 4199/7134 completed (loss: 0.02444491721689701, acc: 0.9927007555961609)
[2025-02-13 04:07:59,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:59,846][root][INFO] - Training Epoch: 2/2, step 4200/7134 completed (loss: 0.035322900861501694, acc: 0.9892473220825195)
[2025-02-13 04:07:59,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:00,264][root][INFO] - Training Epoch: 2/2, step 4201/7134 completed (loss: 0.012782640755176544, acc: 0.9982876777648926)
[2025-02-13 04:08:00,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:00,677][root][INFO] - Training Epoch: 2/2, step 4202/7134 completed (loss: 0.04715731367468834, acc: 0.9883720874786377)
[2025-02-13 04:08:00,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:01,131][root][INFO] - Training Epoch: 2/2, step 4203/7134 completed (loss: 0.04793454334139824, acc: 0.9843993782997131)
[2025-02-13 04:08:01,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:01,539][root][INFO] - Training Epoch: 2/2, step 4204/7134 completed (loss: 0.08242902904748917, acc: 0.9809941649436951)
[2025-02-13 04:08:01,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:01,935][root][INFO] - Training Epoch: 2/2, step 4205/7134 completed (loss: 0.03857363387942314, acc: 0.9913669228553772)
[2025-02-13 04:08:02,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:02,378][root][INFO] - Training Epoch: 2/2, step 4206/7134 completed (loss: 0.060706209391355515, acc: 0.9873060584068298)
[2025-02-13 04:08:02,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:02,794][root][INFO] - Training Epoch: 2/2, step 4207/7134 completed (loss: 0.047538209706544876, acc: 0.9888535141944885)
[2025-02-13 04:08:02,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:03,187][root][INFO] - Training Epoch: 2/2, step 4208/7134 completed (loss: 0.057698749005794525, acc: 0.9872408509254456)
[2025-02-13 04:08:03,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:03,602][root][INFO] - Training Epoch: 2/2, step 4209/7134 completed (loss: 0.05359038710594177, acc: 0.9858356714248657)
[2025-02-13 04:08:03,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:04,033][root][INFO] - Training Epoch: 2/2, step 4210/7134 completed (loss: 0.007056018803268671, acc: 1.0)
[2025-02-13 04:08:04,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:04,304][root][INFO] - Training Epoch: 2/2, step 4211/7134 completed (loss: 0.027417132630944252, acc: 0.9861751198768616)
[2025-02-13 04:08:04,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:04,679][root][INFO] - Training Epoch: 2/2, step 4212/7134 completed (loss: 0.030218014493584633, acc: 0.9934533834457397)
[2025-02-13 04:08:04,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:05,117][root][INFO] - Training Epoch: 2/2, step 4213/7134 completed (loss: 0.011341770179569721, acc: 0.9971387982368469)
[2025-02-13 04:08:05,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:05,560][root][INFO] - Training Epoch: 2/2, step 4214/7134 completed (loss: 0.06333786994218826, acc: 0.9820895791053772)
[2025-02-13 04:08:05,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:05,934][root][INFO] - Training Epoch: 2/2, step 4215/7134 completed (loss: 0.045506205409765244, acc: 0.9868131875991821)
[2025-02-13 04:08:06,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:06,332][root][INFO] - Training Epoch: 2/2, step 4216/7134 completed (loss: 0.04151281714439392, acc: 0.9890310764312744)
[2025-02-13 04:08:06,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:06,715][root][INFO] - Training Epoch: 2/2, step 4217/7134 completed (loss: 0.023024125024676323, acc: 0.9902098178863525)
[2025-02-13 04:08:06,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:07,073][root][INFO] - Training Epoch: 2/2, step 4218/7134 completed (loss: 0.03706871345639229, acc: 0.9896907210350037)
[2025-02-13 04:08:07,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:07,367][root][INFO] - Training Epoch: 2/2, step 4219/7134 completed (loss: 0.024063019081950188, acc: 0.9919678568840027)
[2025-02-13 04:08:07,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:07,805][root][INFO] - Training Epoch: 2/2, step 4220/7134 completed (loss: 0.052606336772441864, acc: 0.9856114983558655)
[2025-02-13 04:08:07,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:08,246][root][INFO] - Training Epoch: 2/2, step 4221/7134 completed (loss: 0.035373736172914505, acc: 0.9929203391075134)
[2025-02-13 04:08:08,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:08,626][root][INFO] - Training Epoch: 2/2, step 4222/7134 completed (loss: 0.10847844928503036, acc: 0.9755555391311646)
[2025-02-13 04:08:08,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:09,030][root][INFO] - Training Epoch: 2/2, step 4223/7134 completed (loss: 0.035162899643182755, acc: 0.9880794882774353)
[2025-02-13 04:08:09,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:09,426][root][INFO] - Training Epoch: 2/2, step 4224/7134 completed (loss: 0.062376655638217926, acc: 0.9848812222480774)
[2025-02-13 04:08:09,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:09,852][root][INFO] - Training Epoch: 2/2, step 4225/7134 completed (loss: 0.09625904262065887, acc: 0.979626476764679)
[2025-02-13 04:08:09,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:10,290][root][INFO] - Training Epoch: 2/2, step 4226/7134 completed (loss: 0.05418160557746887, acc: 0.9872286319732666)
[2025-02-13 04:08:10,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:10,649][root][INFO] - Training Epoch: 2/2, step 4227/7134 completed (loss: 0.09482639282941818, acc: 0.9877150058746338)
[2025-02-13 04:08:10,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:11,056][root][INFO] - Training Epoch: 2/2, step 4228/7134 completed (loss: 0.10705338418483734, acc: 0.9758620858192444)
[2025-02-13 04:08:11,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:11,458][root][INFO] - Training Epoch: 2/2, step 4229/7134 completed (loss: 0.08884161710739136, acc: 0.9790356159210205)
[2025-02-13 04:08:11,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:11,778][root][INFO] - Training Epoch: 2/2, step 4230/7134 completed (loss: 0.06478678435087204, acc: 0.9852941036224365)
[2025-02-13 04:08:11,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:12,204][root][INFO] - Training Epoch: 2/2, step 4231/7134 completed (loss: 0.12714911997318268, acc: 0.970534086227417)
[2025-02-13 04:08:12,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:12,619][root][INFO] - Training Epoch: 2/2, step 4232/7134 completed (loss: 0.08258340507745743, acc: 0.9740458130836487)
[2025-02-13 04:08:12,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:13,034][root][INFO] - Training Epoch: 2/2, step 4233/7134 completed (loss: 0.09514661878347397, acc: 0.9799666404724121)
[2025-02-13 04:08:13,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:13,441][root][INFO] - Training Epoch: 2/2, step 4234/7134 completed (loss: 0.06107231602072716, acc: 0.9888712167739868)
[2025-02-13 04:08:13,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:13,879][root][INFO] - Training Epoch: 2/2, step 4235/7134 completed (loss: 0.03594035282731056, acc: 0.9889415502548218)
[2025-02-13 04:08:14,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:14,268][root][INFO] - Training Epoch: 2/2, step 4236/7134 completed (loss: 0.017250625416636467, acc: 0.997474730014801)
[2025-02-13 04:08:14,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:14,670][root][INFO] - Training Epoch: 2/2, step 4237/7134 completed (loss: 0.03177588805556297, acc: 0.9896907210350037)
[2025-02-13 04:08:14,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:15,089][root][INFO] - Training Epoch: 2/2, step 4238/7134 completed (loss: 0.041317641735076904, acc: 0.9918166995048523)
[2025-02-13 04:08:15,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:15,485][root][INFO] - Training Epoch: 2/2, step 4239/7134 completed (loss: 0.027006573975086212, acc: 0.9919999837875366)
[2025-02-13 04:08:15,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:15,893][root][INFO] - Training Epoch: 2/2, step 4240/7134 completed (loss: 0.05075881630182266, acc: 0.9854604005813599)
[2025-02-13 04:08:16,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:16,307][root][INFO] - Training Epoch: 2/2, step 4241/7134 completed (loss: 0.015741148963570595, acc: 0.9934924244880676)
[2025-02-13 04:08:16,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:16,682][root][INFO] - Training Epoch: 2/2, step 4242/7134 completed (loss: 0.03611792251467705, acc: 0.9888641238212585)
[2025-02-13 04:08:16,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:17,109][root][INFO] - Training Epoch: 2/2, step 4243/7134 completed (loss: 0.06387673318386078, acc: 0.9873417615890503)
[2025-02-13 04:08:17,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:17,459][root][INFO] - Training Epoch: 2/2, step 4244/7134 completed (loss: 0.035181675106287, acc: 0.9911308288574219)
[2025-02-13 04:08:17,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:17,803][root][INFO] - Training Epoch: 2/2, step 4245/7134 completed (loss: 0.03206726163625717, acc: 0.9926650524139404)
[2025-02-13 04:08:17,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:18,218][root][INFO] - Training Epoch: 2/2, step 4246/7134 completed (loss: 0.04004964232444763, acc: 0.9936608672142029)
[2025-02-13 04:08:18,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:18,613][root][INFO] - Training Epoch: 2/2, step 4247/7134 completed (loss: 0.029598413035273552, acc: 0.987522304058075)
[2025-02-13 04:08:18,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:19,027][root][INFO] - Training Epoch: 2/2, step 4248/7134 completed (loss: 0.05404696613550186, acc: 0.9807692170143127)
[2025-02-13 04:08:19,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:19,493][root][INFO] - Training Epoch: 2/2, step 4249/7134 completed (loss: 0.053878143429756165, acc: 0.9888888597488403)
[2025-02-13 04:08:19,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:19,832][root][INFO] - Training Epoch: 2/2, step 4250/7134 completed (loss: 0.06700749695301056, acc: 0.9851379990577698)
[2025-02-13 04:08:19,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:20,234][root][INFO] - Training Epoch: 2/2, step 4251/7134 completed (loss: 0.012442399747669697, acc: 0.9967105388641357)
[2025-02-13 04:08:20,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:20,639][root][INFO] - Training Epoch: 2/2, step 4252/7134 completed (loss: 0.03740997239947319, acc: 0.9888712167739868)
[2025-02-13 04:08:20,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:21,051][root][INFO] - Training Epoch: 2/2, step 4253/7134 completed (loss: 0.04177644103765488, acc: 0.9913232326507568)
[2025-02-13 04:08:21,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:21,430][root][INFO] - Training Epoch: 2/2, step 4254/7134 completed (loss: 0.01923995092511177, acc: 0.9925187230110168)
[2025-02-13 04:08:21,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:21,864][root][INFO] - Training Epoch: 2/2, step 4255/7134 completed (loss: 0.034147825092077255, acc: 0.9931507110595703)
[2025-02-13 04:08:22,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:22,262][root][INFO] - Training Epoch: 2/2, step 4256/7134 completed (loss: 0.03092670440673828, acc: 0.9909502267837524)
[2025-02-13 04:08:22,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:22,673][root][INFO] - Training Epoch: 2/2, step 4257/7134 completed (loss: 0.016487669199705124, acc: 0.9946523904800415)
[2025-02-13 04:08:22,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:23,085][root][INFO] - Training Epoch: 2/2, step 4258/7134 completed (loss: 0.010134904645383358, acc: 0.9980915784835815)
[2025-02-13 04:08:23,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:23,523][root][INFO] - Training Epoch: 2/2, step 4259/7134 completed (loss: 0.07508236169815063, acc: 0.9813753366470337)
[2025-02-13 04:08:23,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:23,946][root][INFO] - Training Epoch: 2/2, step 4260/7134 completed (loss: 0.014425650238990784, acc: 0.9937888383865356)
[2025-02-13 04:08:24,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:24,346][root][INFO] - Training Epoch: 2/2, step 4261/7134 completed (loss: 0.02130972594022751, acc: 0.990439772605896)
[2025-02-13 04:08:24,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:24,770][root][INFO] - Training Epoch: 2/2, step 4262/7134 completed (loss: 0.014601195231080055, acc: 0.9968701004981995)
[2025-02-13 04:08:24,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:25,182][root][INFO] - Training Epoch: 2/2, step 4263/7134 completed (loss: 0.06709480285644531, acc: 0.9915540814399719)
[2025-02-13 04:08:25,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:25,597][root][INFO] - Training Epoch: 2/2, step 4264/7134 completed (loss: 0.04967754706740379, acc: 0.978723406791687)
[2025-02-13 04:08:25,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:26,033][root][INFO] - Training Epoch: 2/2, step 4265/7134 completed (loss: 0.025736995041370392, acc: 0.9886363744735718)
[2025-02-13 04:08:26,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:26,492][root][INFO] - Training Epoch: 2/2, step 4266/7134 completed (loss: 0.02554069086909294, acc: 0.9928143620491028)
[2025-02-13 04:08:26,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:26,954][root][INFO] - Training Epoch: 2/2, step 4267/7134 completed (loss: 0.029623981565237045, acc: 0.9903030395507812)
[2025-02-13 04:08:27,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:27,381][root][INFO] - Training Epoch: 2/2, step 4268/7134 completed (loss: 0.06627938151359558, acc: 0.9799599051475525)
[2025-02-13 04:08:27,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:27,846][root][INFO] - Training Epoch: 2/2, step 4269/7134 completed (loss: 0.100062794983387, acc: 0.9697842001914978)
[2025-02-13 04:08:27,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:28,283][root][INFO] - Training Epoch: 2/2, step 4270/7134 completed (loss: 0.04698297753930092, acc: 0.9856770634651184)
[2025-02-13 04:08:28,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:28,728][root][INFO] - Training Epoch: 2/2, step 4271/7134 completed (loss: 0.031729213893413544, acc: 0.9863013625144958)
[2025-02-13 04:08:28,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:29,191][root][INFO] - Training Epoch: 2/2, step 4272/7134 completed (loss: 0.027124224230647087, acc: 0.9894366264343262)
[2025-02-13 04:08:29,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:29,638][root][INFO] - Training Epoch: 2/2, step 4273/7134 completed (loss: 0.026575705036520958, acc: 0.9912170767784119)
[2025-02-13 04:08:29,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:30,100][root][INFO] - Training Epoch: 2/2, step 4274/7134 completed (loss: 0.06052188202738762, acc: 0.9824766516685486)
[2025-02-13 04:08:30,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:30,530][root][INFO] - Training Epoch: 2/2, step 4275/7134 completed (loss: 0.017661752179265022, acc: 0.993686854839325)
[2025-02-13 04:08:30,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:30,972][root][INFO] - Training Epoch: 2/2, step 4276/7134 completed (loss: 0.04771529883146286, acc: 0.9896907210350037)
[2025-02-13 04:08:31,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:31,373][root][INFO] - Training Epoch: 2/2, step 4277/7134 completed (loss: 0.023610712960362434, acc: 0.9920634627342224)
[2025-02-13 04:08:31,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:31,814][root][INFO] - Training Epoch: 2/2, step 4278/7134 completed (loss: 0.03822288662195206, acc: 0.9888888597488403)
[2025-02-13 04:08:31,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:32,233][root][INFO] - Training Epoch: 2/2, step 4279/7134 completed (loss: 0.025443414226174355, acc: 0.992559552192688)
[2025-02-13 04:08:32,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:32,678][root][INFO] - Training Epoch: 2/2, step 4280/7134 completed (loss: 0.007995675317943096, acc: 1.0)
[2025-02-13 04:08:32,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:33,144][root][INFO] - Training Epoch: 2/2, step 4281/7134 completed (loss: 0.033048875629901886, acc: 0.9876237511634827)
[2025-02-13 04:08:33,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:33,579][root][INFO] - Training Epoch: 2/2, step 4282/7134 completed (loss: 0.010953932069242, acc: 0.9981481432914734)
[2025-02-13 04:08:33,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:34,039][root][INFO] - Training Epoch: 2/2, step 4283/7134 completed (loss: 0.020194627344608307, acc: 0.9942660331726074)
[2025-02-13 04:08:34,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:34,411][root][INFO] - Training Epoch: 2/2, step 4284/7134 completed (loss: 0.04714592173695564, acc: 0.9818840622901917)
[2025-02-13 04:08:34,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:34,842][root][INFO] - Training Epoch: 2/2, step 4285/7134 completed (loss: 0.012986903078854084, acc: 0.9982876777648926)
[2025-02-13 04:08:34,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:35,245][root][INFO] - Training Epoch: 2/2, step 4286/7134 completed (loss: 0.04438451677560806, acc: 0.9874100685119629)
[2025-02-13 04:08:35,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:35,693][root][INFO] - Training Epoch: 2/2, step 4287/7134 completed (loss: 0.047040682286024094, acc: 0.9871134161949158)
[2025-02-13 04:08:35,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:36,145][root][INFO] - Training Epoch: 2/2, step 4288/7134 completed (loss: 0.04316413030028343, acc: 0.9858155846595764)
[2025-02-13 04:08:36,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:36,608][root][INFO] - Training Epoch: 2/2, step 4289/7134 completed (loss: 0.03896314650774002, acc: 0.9879699349403381)
[2025-02-13 04:08:36,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:37,052][root][INFO] - Training Epoch: 2/2, step 4290/7134 completed (loss: 0.022588102146983147, acc: 0.9905277490615845)
[2025-02-13 04:08:37,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:37,492][root][INFO] - Training Epoch: 2/2, step 4291/7134 completed (loss: 0.02722763642668724, acc: 0.9904648661613464)
[2025-02-13 04:08:37,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:37,911][root][INFO] - Training Epoch: 2/2, step 4292/7134 completed (loss: 0.022646024823188782, acc: 0.9939758777618408)
[2025-02-13 04:08:38,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:38,319][root][INFO] - Training Epoch: 2/2, step 4293/7134 completed (loss: 0.010783637873828411, acc: 0.9979838728904724)
[2025-02-13 04:08:38,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:38,734][root][INFO] - Training Epoch: 2/2, step 4294/7134 completed (loss: 0.014833186753094196, acc: 0.9927219748497009)
[2025-02-13 04:08:38,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:39,165][root][INFO] - Training Epoch: 2/2, step 4295/7134 completed (loss: 0.023870190605521202, acc: 0.9908758997917175)
[2025-02-13 04:08:39,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:39,575][root][INFO] - Training Epoch: 2/2, step 4296/7134 completed (loss: 0.019379256293177605, acc: 0.9975669384002686)
[2025-02-13 04:08:39,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:39,990][root][INFO] - Training Epoch: 2/2, step 4297/7134 completed (loss: 0.013305357657372952, acc: 0.9953051805496216)
[2025-02-13 04:08:40,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:40,414][root][INFO] - Training Epoch: 2/2, step 4298/7134 completed (loss: 0.03208012878894806, acc: 0.9903714060783386)
[2025-02-13 04:08:40,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:40,813][root][INFO] - Training Epoch: 2/2, step 4299/7134 completed (loss: 0.028626056388020515, acc: 0.9914039969444275)
[2025-02-13 04:08:40,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:41,212][root][INFO] - Training Epoch: 2/2, step 4300/7134 completed (loss: 0.010730229318141937, acc: 0.9978070259094238)
[2025-02-13 04:08:41,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:41,624][root][INFO] - Training Epoch: 2/2, step 4301/7134 completed (loss: 0.028066612780094147, acc: 0.9918032884597778)
[2025-02-13 04:08:41,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:42,041][root][INFO] - Training Epoch: 2/2, step 4302/7134 completed (loss: 0.014066963456571102, acc: 0.9955223798751831)
[2025-02-13 04:08:42,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:42,479][root][INFO] - Training Epoch: 2/2, step 4303/7134 completed (loss: 0.04829197749495506, acc: 0.9832935333251953)
[2025-02-13 04:08:42,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:42,920][root][INFO] - Training Epoch: 2/2, step 4304/7134 completed (loss: 0.02368256263434887, acc: 0.9925705790519714)
[2025-02-13 04:08:43,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:43,381][root][INFO] - Training Epoch: 2/2, step 4305/7134 completed (loss: 0.024914218112826347, acc: 0.9917840361595154)
[2025-02-13 04:08:43,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:43,825][root][INFO] - Training Epoch: 2/2, step 4306/7134 completed (loss: 0.06304483115673065, acc: 0.9855453372001648)
[2025-02-13 04:08:43,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:44,258][root][INFO] - Training Epoch: 2/2, step 4307/7134 completed (loss: 0.05566553771495819, acc: 0.9823434948921204)
[2025-02-13 04:08:44,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:44,714][root][INFO] - Training Epoch: 2/2, step 4308/7134 completed (loss: 0.04315274953842163, acc: 0.988399088382721)
[2025-02-13 04:08:44,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:45,163][root][INFO] - Training Epoch: 2/2, step 4309/7134 completed (loss: 0.031011400744318962, acc: 0.992601752281189)
[2025-02-13 04:08:45,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:45,604][root][INFO] - Training Epoch: 2/2, step 4310/7134 completed (loss: 0.018323281779885292, acc: 0.9941245317459106)
[2025-02-13 04:08:45,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:46,015][root][INFO] - Training Epoch: 2/2, step 4311/7134 completed (loss: 0.014031150378286839, acc: 0.9942029118537903)
[2025-02-13 04:08:46,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:46,422][root][INFO] - Training Epoch: 2/2, step 4312/7134 completed (loss: 0.020447801798582077, acc: 0.9918256402015686)
[2025-02-13 04:08:46,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:46,837][root][INFO] - Training Epoch: 2/2, step 4313/7134 completed (loss: 0.014306505210697651, acc: 0.9948052167892456)
[2025-02-13 04:08:46,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:47,254][root][INFO] - Training Epoch: 2/2, step 4314/7134 completed (loss: 0.02614646591246128, acc: 0.9879153966903687)
[2025-02-13 04:08:47,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:47,690][root][INFO] - Training Epoch: 2/2, step 4315/7134 completed (loss: 0.02821618691086769, acc: 0.9921568632125854)
[2025-02-13 04:08:47,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:48,133][root][INFO] - Training Epoch: 2/2, step 4316/7134 completed (loss: 0.029078038409352303, acc: 0.9910714030265808)
[2025-02-13 04:08:48,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:48,576][root][INFO] - Training Epoch: 2/2, step 4317/7134 completed (loss: 0.04295286163687706, acc: 0.9888476133346558)
[2025-02-13 04:08:48,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:49,013][root][INFO] - Training Epoch: 2/2, step 4318/7134 completed (loss: 0.025336990132927895, acc: 0.9910314083099365)
[2025-02-13 04:08:49,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:49,481][root][INFO] - Training Epoch: 2/2, step 4319/7134 completed (loss: 0.011981823481619358, acc: 0.9988851547241211)
[2025-02-13 04:08:49,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:49,891][root][INFO] - Training Epoch: 2/2, step 4320/7134 completed (loss: 0.020234916359186172, acc: 0.995945930480957)
[2025-02-13 04:08:50,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:50,247][root][INFO] - Training Epoch: 2/2, step 4321/7134 completed (loss: 0.03845040127635002, acc: 0.9907621145248413)
[2025-02-13 04:08:50,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:50,682][root][INFO] - Training Epoch: 2/2, step 4322/7134 completed (loss: 0.028194177895784378, acc: 0.9905660152435303)
[2025-02-13 04:08:50,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:51,078][root][INFO] - Training Epoch: 2/2, step 4323/7134 completed (loss: 0.05324535071849823, acc: 0.9845857620239258)
[2025-02-13 04:08:51,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:51,463][root][INFO] - Training Epoch: 2/2, step 4324/7134 completed (loss: 0.0158047117292881, acc: 0.9961240291595459)
[2025-02-13 04:08:51,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:51,876][root][INFO] - Training Epoch: 2/2, step 4325/7134 completed (loss: 0.025429617613554, acc: 0.9920254945755005)
[2025-02-13 04:08:52,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:52,292][root][INFO] - Training Epoch: 2/2, step 4326/7134 completed (loss: 0.01942419819533825, acc: 0.9929577708244324)
[2025-02-13 04:08:52,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:52,684][root][INFO] - Training Epoch: 2/2, step 4327/7134 completed (loss: 0.015767419710755348, acc: 0.9930070042610168)
[2025-02-13 04:08:52,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:53,086][root][INFO] - Training Epoch: 2/2, step 4328/7134 completed (loss: 0.03708259016275406, acc: 0.9906014800071716)
[2025-02-13 04:08:53,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:53,532][root][INFO] - Training Epoch: 2/2, step 4329/7134 completed (loss: 0.03237135708332062, acc: 0.9947575330734253)
[2025-02-13 04:08:53,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:53,872][root][INFO] - Training Epoch: 2/2, step 4330/7134 completed (loss: 0.03792129456996918, acc: 0.9850746393203735)
[2025-02-13 04:08:54,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:54,245][root][INFO] - Training Epoch: 2/2, step 4331/7134 completed (loss: 0.005521082319319248, acc: 1.0)
[2025-02-13 04:08:54,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:54,653][root][INFO] - Training Epoch: 2/2, step 4332/7134 completed (loss: 0.03863747790455818, acc: 0.9885931611061096)
[2025-02-13 04:08:54,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:55,065][root][INFO] - Training Epoch: 2/2, step 4333/7134 completed (loss: 0.027553411200642586, acc: 0.9893778562545776)
[2025-02-13 04:08:55,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:55,508][root][INFO] - Training Epoch: 2/2, step 4334/7134 completed (loss: 0.03316253423690796, acc: 0.9880239367485046)
[2025-02-13 04:08:55,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:55,912][root][INFO] - Training Epoch: 2/2, step 4335/7134 completed (loss: 0.04110192880034447, acc: 0.9899193644523621)
[2025-02-13 04:08:56,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:56,324][root][INFO] - Training Epoch: 2/2, step 4336/7134 completed (loss: 0.004441750701516867, acc: 1.0)
[2025-02-13 04:08:56,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:56,731][root][INFO] - Training Epoch: 2/2, step 4337/7134 completed (loss: 0.016096126288175583, acc: 0.9952531456947327)
[2025-02-13 04:08:56,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:57,135][root][INFO] - Training Epoch: 2/2, step 4338/7134 completed (loss: 0.012063134461641312, acc: 0.998420238494873)
[2025-02-13 04:08:57,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:57,547][root][INFO] - Training Epoch: 2/2, step 4339/7134 completed (loss: 0.031584639102220535, acc: 0.9883381724357605)
[2025-02-13 04:08:57,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:57,997][root][INFO] - Training Epoch: 2/2, step 4340/7134 completed (loss: 0.02558458037674427, acc: 0.9966216087341309)
[2025-02-13 04:08:58,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:58,413][root][INFO] - Training Epoch: 2/2, step 4341/7134 completed (loss: 0.03630882501602173, acc: 0.9940564632415771)
[2025-02-13 04:08:58,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:58,865][root][INFO] - Training Epoch: 2/2, step 4342/7134 completed (loss: 0.03614550828933716, acc: 0.9919871687889099)
[2025-02-13 04:08:59,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:59,287][root][INFO] - Training Epoch: 2/2, step 4343/7134 completed (loss: 0.020543072372674942, acc: 0.9950082898139954)
[2025-02-13 04:08:59,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:59,654][root][INFO] - Training Epoch: 2/2, step 4344/7134 completed (loss: 0.03256765380501747, acc: 0.9937629699707031)
[2025-02-13 04:08:59,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:00,014][root][INFO] - Training Epoch: 2/2, step 4345/7134 completed (loss: 0.025449132546782494, acc: 0.9917184114456177)
[2025-02-13 04:09:00,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:00,381][root][INFO] - Training Epoch: 2/2, step 4346/7134 completed (loss: 0.0074339210987091064, acc: 0.9979296326637268)
[2025-02-13 04:09:00,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:00,789][root][INFO] - Training Epoch: 2/2, step 4347/7134 completed (loss: 0.048165712505578995, acc: 0.9900990128517151)
[2025-02-13 04:09:00,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:01,176][root][INFO] - Training Epoch: 2/2, step 4348/7134 completed (loss: 0.016845280304551125, acc: 0.9968000054359436)
[2025-02-13 04:09:01,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:01,525][root][INFO] - Training Epoch: 2/2, step 4349/7134 completed (loss: 0.008022183552384377, acc: 0.9976905584335327)
[2025-02-13 04:09:01,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:01,959][root][INFO] - Training Epoch: 2/2, step 4350/7134 completed (loss: 0.0359431691467762, acc: 0.9908376932144165)
[2025-02-13 04:09:02,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:02,374][root][INFO] - Training Epoch: 2/2, step 4351/7134 completed (loss: 0.05901763215661049, acc: 0.9807956218719482)
[2025-02-13 04:09:02,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:02,811][root][INFO] - Training Epoch: 2/2, step 4352/7134 completed (loss: 0.042974088340997696, acc: 0.9873257279396057)
[2025-02-13 04:09:02,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:03,252][root][INFO] - Training Epoch: 2/2, step 4353/7134 completed (loss: 0.03343362361192703, acc: 0.990728497505188)
[2025-02-13 04:09:03,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:03,680][root][INFO] - Training Epoch: 2/2, step 4354/7134 completed (loss: 0.03015856444835663, acc: 0.9893333315849304)
[2025-02-13 04:09:03,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:04,090][root][INFO] - Training Epoch: 2/2, step 4355/7134 completed (loss: 0.029470741748809814, acc: 0.9887359142303467)
[2025-02-13 04:09:04,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:04,530][root][INFO] - Training Epoch: 2/2, step 4356/7134 completed (loss: 0.020772097632288933, acc: 0.9916368126869202)
[2025-02-13 04:09:04,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:04,963][root][INFO] - Training Epoch: 2/2, step 4357/7134 completed (loss: 0.04391659423708916, acc: 0.9889763593673706)
[2025-02-13 04:09:05,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:05,416][root][INFO] - Training Epoch: 2/2, step 4358/7134 completed (loss: 0.032773301005363464, acc: 0.9904191493988037)
[2025-02-13 04:09:05,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:05,834][root][INFO] - Training Epoch: 2/2, step 4359/7134 completed (loss: 0.03597017005085945, acc: 0.9873217344284058)
[2025-02-13 04:09:05,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:06,220][root][INFO] - Training Epoch: 2/2, step 4360/7134 completed (loss: 0.04895719513297081, acc: 0.9846368432044983)
[2025-02-13 04:09:06,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:06,648][root][INFO] - Training Epoch: 2/2, step 4361/7134 completed (loss: 0.021651865914463997, acc: 0.9913294911384583)
[2025-02-13 04:09:06,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:07,094][root][INFO] - Training Epoch: 2/2, step 4362/7134 completed (loss: 0.026804152876138687, acc: 0.9901356101036072)
[2025-02-13 04:09:07,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:07,532][root][INFO] - Training Epoch: 2/2, step 4363/7134 completed (loss: 0.023178581148386, acc: 0.9917355179786682)
[2025-02-13 04:09:07,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:07,929][root][INFO] - Training Epoch: 2/2, step 4364/7134 completed (loss: 0.015010538510978222, acc: 0.995312511920929)
[2025-02-13 04:09:08,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:08,352][root][INFO] - Training Epoch: 2/2, step 4365/7134 completed (loss: 0.021407991647720337, acc: 0.9905660152435303)
[2025-02-13 04:09:08,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:08,799][root][INFO] - Training Epoch: 2/2, step 4366/7134 completed (loss: 0.059663016349077225, acc: 0.9806700944900513)
[2025-02-13 04:09:08,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:09,231][root][INFO] - Training Epoch: 2/2, step 4367/7134 completed (loss: 0.02559537999331951, acc: 0.9899713397026062)
[2025-02-13 04:09:09,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:09,683][root][INFO] - Training Epoch: 2/2, step 4368/7134 completed (loss: 0.014120712876319885, acc: 0.992443323135376)
[2025-02-13 04:09:09,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:10,101][root][INFO] - Training Epoch: 2/2, step 4369/7134 completed (loss: 0.01622077077627182, acc: 0.9950000047683716)
[2025-02-13 04:09:10,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:10,542][root][INFO] - Training Epoch: 2/2, step 4370/7134 completed (loss: 0.041663020849227905, acc: 0.9911727905273438)
[2025-02-13 04:09:10,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:10,985][root][INFO] - Training Epoch: 2/2, step 4371/7134 completed (loss: 0.037953175604343414, acc: 0.9868074059486389)
[2025-02-13 04:09:11,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:11,396][root][INFO] - Training Epoch: 2/2, step 4372/7134 completed (loss: 0.03302896022796631, acc: 0.9913941621780396)
[2025-02-13 04:09:11,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:11,825][root][INFO] - Training Epoch: 2/2, step 4373/7134 completed (loss: 0.03191080316901207, acc: 0.9892984628677368)
[2025-02-13 04:09:11,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:12,247][root][INFO] - Training Epoch: 2/2, step 4374/7134 completed (loss: 0.037526581436395645, acc: 0.9856114983558655)
[2025-02-13 04:09:12,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:12,687][root][INFO] - Training Epoch: 2/2, step 4375/7134 completed (loss: 0.021591003984212875, acc: 0.992277979850769)
[2025-02-13 04:09:12,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:13,080][root][INFO] - Training Epoch: 2/2, step 4376/7134 completed (loss: 0.03217897191643715, acc: 0.9909502267837524)
[2025-02-13 04:09:13,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:13,519][root][INFO] - Training Epoch: 2/2, step 4377/7134 completed (loss: 0.04831656813621521, acc: 0.9925834536552429)
[2025-02-13 04:09:13,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:13,945][root][INFO] - Training Epoch: 2/2, step 4378/7134 completed (loss: 0.026560038328170776, acc: 0.9901960492134094)
[2025-02-13 04:09:14,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:14,400][root][INFO] - Training Epoch: 2/2, step 4379/7134 completed (loss: 0.0754363015294075, acc: 0.9753788113594055)
[2025-02-13 04:09:14,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:14,788][root][INFO] - Training Epoch: 2/2, step 4380/7134 completed (loss: 0.045895736664533615, acc: 0.9894921183586121)
[2025-02-13 04:09:14,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:15,230][root][INFO] - Training Epoch: 2/2, step 4381/7134 completed (loss: 0.016349362209439278, acc: 0.9944827556610107)
[2025-02-13 04:09:15,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:15,619][root][INFO] - Training Epoch: 2/2, step 4382/7134 completed (loss: 0.007639545947313309, acc: 1.0)
[2025-02-13 04:09:15,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:16,050][root][INFO] - Training Epoch: 2/2, step 4383/7134 completed (loss: 0.025235192850232124, acc: 0.9876203536987305)
[2025-02-13 04:09:16,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:16,454][root][INFO] - Training Epoch: 2/2, step 4384/7134 completed (loss: 0.0166302677243948, acc: 0.9937185645103455)
[2025-02-13 04:09:16,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:16,879][root][INFO] - Training Epoch: 2/2, step 4385/7134 completed (loss: 0.05078956484794617, acc: 0.9843546152114868)
[2025-02-13 04:09:17,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:17,314][root][INFO] - Training Epoch: 2/2, step 4386/7134 completed (loss: 0.03995088115334511, acc: 0.9869668483734131)
[2025-02-13 04:09:17,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:17,755][root][INFO] - Training Epoch: 2/2, step 4387/7134 completed (loss: 0.045444127172231674, acc: 0.9855072498321533)
[2025-02-13 04:09:17,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:18,164][root][INFO] - Training Epoch: 2/2, step 4388/7134 completed (loss: 0.025638991966843605, acc: 0.9906832575798035)
[2025-02-13 04:09:18,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:18,572][root][INFO] - Training Epoch: 2/2, step 4389/7134 completed (loss: 0.0556144043803215, acc: 0.985318124294281)
[2025-02-13 04:09:18,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:19,018][root][INFO] - Training Epoch: 2/2, step 4390/7134 completed (loss: 0.07305776327848434, acc: 0.9761570692062378)
[2025-02-13 04:09:19,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:19,427][root][INFO] - Training Epoch: 2/2, step 4391/7134 completed (loss: 0.052460722625255585, acc: 0.9853420257568359)
[2025-02-13 04:09:19,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:19,857][root][INFO] - Training Epoch: 2/2, step 4392/7134 completed (loss: 0.023317519575357437, acc: 0.9950980544090271)
[2025-02-13 04:09:20,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:20,300][root][INFO] - Training Epoch: 2/2, step 4393/7134 completed (loss: 0.020839514210820198, acc: 0.9943740963935852)
[2025-02-13 04:09:20,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:20,742][root][INFO] - Training Epoch: 2/2, step 4394/7134 completed (loss: 0.02694951742887497, acc: 0.9929478168487549)
[2025-02-13 04:09:20,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:21,145][root][INFO] - Training Epoch: 2/2, step 4395/7134 completed (loss: 0.04331420361995697, acc: 0.9811617136001587)
[2025-02-13 04:09:21,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:21,545][root][INFO] - Training Epoch: 2/2, step 4396/7134 completed (loss: 0.06836166232824326, acc: 0.9796748161315918)
[2025-02-13 04:09:21,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:21,942][root][INFO] - Training Epoch: 2/2, step 4397/7134 completed (loss: 0.0434725247323513, acc: 0.9835293889045715)
[2025-02-13 04:09:22,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:22,367][root][INFO] - Training Epoch: 2/2, step 4398/7134 completed (loss: 0.009333017282187939, acc: 0.997474730014801)
[2025-02-13 04:09:22,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:22,778][root][INFO] - Training Epoch: 2/2, step 4399/7134 completed (loss: 0.040532320737838745, acc: 0.9919614195823669)
[2025-02-13 04:09:22,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:23,233][root][INFO] - Training Epoch: 2/2, step 4400/7134 completed (loss: 0.06401976197957993, acc: 0.9851301312446594)
[2025-02-13 04:09:23,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:23,626][root][INFO] - Training Epoch: 2/2, step 4401/7134 completed (loss: 0.06108812987804413, acc: 0.9855072498321533)
[2025-02-13 04:09:23,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:24,049][root][INFO] - Training Epoch: 2/2, step 4402/7134 completed (loss: 0.09506645798683167, acc: 0.9680232405662537)
[2025-02-13 04:09:24,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:24,489][root][INFO] - Training Epoch: 2/2, step 4403/7134 completed (loss: 0.12504184246063232, acc: 0.9663212299346924)
[2025-02-13 04:09:24,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:24,902][root][INFO] - Training Epoch: 2/2, step 4404/7134 completed (loss: 0.12448260933160782, acc: 0.9707446694374084)
[2025-02-13 04:09:25,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:25,372][root][INFO] - Training Epoch: 2/2, step 4405/7134 completed (loss: 0.02266453206539154, acc: 0.9935204982757568)
[2025-02-13 04:09:25,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:25,854][root][INFO] - Training Epoch: 2/2, step 4406/7134 completed (loss: 0.02752382680773735, acc: 0.9910913109779358)
[2025-02-13 04:09:25,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:26,316][root][INFO] - Training Epoch: 2/2, step 4407/7134 completed (loss: 0.043539948761463165, acc: 0.9861271381378174)
[2025-02-13 04:09:26,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:26,759][root][INFO] - Training Epoch: 2/2, step 4408/7134 completed (loss: 0.07124911993741989, acc: 0.9825378060340881)
[2025-02-13 04:09:26,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:27,197][root][INFO] - Training Epoch: 2/2, step 4409/7134 completed (loss: 0.04939889907836914, acc: 0.9853300452232361)
[2025-02-13 04:09:27,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:27,590][root][INFO] - Training Epoch: 2/2, step 4410/7134 completed (loss: 0.053139578551054, acc: 0.9856630563735962)
[2025-02-13 04:09:27,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:28,016][root][INFO] - Training Epoch: 2/2, step 4411/7134 completed (loss: 0.10724420100450516, acc: 0.9741007089614868)
[2025-02-13 04:09:28,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:28,432][root][INFO] - Training Epoch: 2/2, step 4412/7134 completed (loss: 0.12820793688297272, acc: 0.9595808386802673)
[2025-02-13 04:09:28,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:28,852][root][INFO] - Training Epoch: 2/2, step 4413/7134 completed (loss: 0.08224605023860931, acc: 0.9795918464660645)
[2025-02-13 04:09:28,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:29,263][root][INFO] - Training Epoch: 2/2, step 4414/7134 completed (loss: 0.03020239621400833, acc: 0.9893364906311035)
[2025-02-13 04:09:29,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:29,703][root][INFO] - Training Epoch: 2/2, step 4415/7134 completed (loss: 0.057808417826890945, acc: 0.9838107228279114)
[2025-02-13 04:09:29,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:30,161][root][INFO] - Training Epoch: 2/2, step 4416/7134 completed (loss: 0.05180218815803528, acc: 0.9871645569801331)
[2025-02-13 04:09:30,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:30,600][root][INFO] - Training Epoch: 2/2, step 4417/7134 completed (loss: 0.07349590957164764, acc: 0.9855769276618958)
[2025-02-13 04:09:30,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:31,039][root][INFO] - Training Epoch: 2/2, step 4418/7134 completed (loss: 0.06021709367632866, acc: 0.9830729365348816)
[2025-02-13 04:09:31,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:31,450][root][INFO] - Training Epoch: 2/2, step 4419/7134 completed (loss: 0.07378872483968735, acc: 0.9771634340286255)
[2025-02-13 04:09:31,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:31,892][root][INFO] - Training Epoch: 2/2, step 4420/7134 completed (loss: 0.0699247419834137, acc: 0.97648686170578)
[2025-02-13 04:09:32,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:32,335][root][INFO] - Training Epoch: 2/2, step 4421/7134 completed (loss: 0.04595469683408737, acc: 0.9887514114379883)
[2025-02-13 04:09:32,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:32,771][root][INFO] - Training Epoch: 2/2, step 4422/7134 completed (loss: 0.06587297469377518, acc: 0.9748822450637817)
[2025-02-13 04:09:32,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:33,217][root][INFO] - Training Epoch: 2/2, step 4423/7134 completed (loss: 0.03928491100668907, acc: 0.9885844588279724)
[2025-02-13 04:09:33,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:33,665][root][INFO] - Training Epoch: 2/2, step 4424/7134 completed (loss: 0.03073117323219776, acc: 0.9894894957542419)
[2025-02-13 04:09:33,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:34,114][root][INFO] - Training Epoch: 2/2, step 4425/7134 completed (loss: 0.027761494740843773, acc: 0.9894737005233765)
[2025-02-13 04:09:34,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:34,584][root][INFO] - Training Epoch: 2/2, step 4426/7134 completed (loss: 0.04911000654101372, acc: 0.9844881296157837)
[2025-02-13 04:09:34,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:34,939][root][INFO] - Training Epoch: 2/2, step 4427/7134 completed (loss: 0.07921064645051956, acc: 0.9710843563079834)
[2025-02-13 04:09:35,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:35,390][root][INFO] - Training Epoch: 2/2, step 4428/7134 completed (loss: 0.023191526532173157, acc: 0.9929577708244324)
[2025-02-13 04:09:35,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:35,686][root][INFO] - Training Epoch: 2/2, step 4429/7134 completed (loss: 0.03352843225002289, acc: 0.9845361113548279)
[2025-02-13 04:09:35,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:36,103][root][INFO] - Training Epoch: 2/2, step 4430/7134 completed (loss: 0.04107385501265526, acc: 0.9878048896789551)
[2025-02-13 04:09:36,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:36,500][root][INFO] - Training Epoch: 2/2, step 4431/7134 completed (loss: 0.05233655497431755, acc: 0.9875583052635193)
[2025-02-13 04:09:36,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:36,896][root][INFO] - Training Epoch: 2/2, step 4432/7134 completed (loss: 0.01254010945558548, acc: 0.996927797794342)
[2025-02-13 04:09:37,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:37,355][root][INFO] - Training Epoch: 2/2, step 4433/7134 completed (loss: 0.03496776148676872, acc: 0.9918414950370789)
[2025-02-13 04:09:37,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:37,786][root][INFO] - Training Epoch: 2/2, step 4434/7134 completed (loss: 0.02261471003293991, acc: 0.9909909963607788)
[2025-02-13 04:09:37,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:38,180][root][INFO] - Training Epoch: 2/2, step 4435/7134 completed (loss: 0.018243838101625443, acc: 0.9907578825950623)
[2025-02-13 04:09:38,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:38,643][root][INFO] - Training Epoch: 2/2, step 4436/7134 completed (loss: 0.059437524527311325, acc: 0.9790502786636353)
[2025-02-13 04:09:38,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:39,062][root][INFO] - Training Epoch: 2/2, step 4437/7134 completed (loss: 0.015508984215557575, acc: 0.9947506785392761)
[2025-02-13 04:09:39,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:39,477][root][INFO] - Training Epoch: 2/2, step 4438/7134 completed (loss: 0.02929418906569481, acc: 0.993220329284668)
[2025-02-13 04:09:39,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:39,890][root][INFO] - Training Epoch: 2/2, step 4439/7134 completed (loss: 0.010349971242249012, acc: 0.995121955871582)
[2025-02-13 04:09:40,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:40,293][root][INFO] - Training Epoch: 2/2, step 4440/7134 completed (loss: 0.026530176401138306, acc: 0.9903314709663391)
[2025-02-13 04:09:40,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:40,763][root][INFO] - Training Epoch: 2/2, step 4441/7134 completed (loss: 0.02349584735929966, acc: 0.9953917264938354)
[2025-02-13 04:09:40,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:41,200][root][INFO] - Training Epoch: 2/2, step 4442/7134 completed (loss: 0.050485968589782715, acc: 0.9884169697761536)
[2025-02-13 04:09:41,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:41,646][root][INFO] - Training Epoch: 2/2, step 4443/7134 completed (loss: 0.043129634112119675, acc: 0.9884615540504456)
[2025-02-13 04:09:41,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:42,030][root][INFO] - Training Epoch: 2/2, step 4444/7134 completed (loss: 0.023787183687090874, acc: 0.9918434023857117)
[2025-02-13 04:09:42,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:42,428][root][INFO] - Training Epoch: 2/2, step 4445/7134 completed (loss: 0.03412152826786041, acc: 0.9874301552772522)
[2025-02-13 04:09:42,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:42,837][root][INFO] - Training Epoch: 2/2, step 4446/7134 completed (loss: 0.013296335004270077, acc: 0.9951691031455994)
[2025-02-13 04:09:42,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:43,293][root][INFO] - Training Epoch: 2/2, step 4447/7134 completed (loss: 0.025527488440275192, acc: 0.9941037893295288)
[2025-02-13 04:09:43,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:43,690][root][INFO] - Training Epoch: 2/2, step 4448/7134 completed (loss: 0.042628929018974304, acc: 0.9872029423713684)
[2025-02-13 04:09:43,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:44,075][root][INFO] - Training Epoch: 2/2, step 4449/7134 completed (loss: 0.04320535063743591, acc: 0.9795538783073425)
[2025-02-13 04:09:44,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:44,506][root][INFO] - Training Epoch: 2/2, step 4450/7134 completed (loss: 0.02899223379790783, acc: 0.9881481528282166)
[2025-02-13 04:09:44,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:44,902][root][INFO] - Training Epoch: 2/2, step 4451/7134 completed (loss: 0.08961275219917297, acc: 0.9817578792572021)
[2025-02-13 04:09:45,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:45,333][root][INFO] - Training Epoch: 2/2, step 4452/7134 completed (loss: 0.013027516193687916, acc: 0.9919999837875366)
[2025-02-13 04:09:45,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:45,743][root][INFO] - Training Epoch: 2/2, step 4453/7134 completed (loss: 0.047733791172504425, acc: 0.9904371500015259)
[2025-02-13 04:09:45,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:46,159][root][INFO] - Training Epoch: 2/2, step 4454/7134 completed (loss: 0.03101382590830326, acc: 0.9943342804908752)
[2025-02-13 04:09:46,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:46,531][root][INFO] - Training Epoch: 2/2, step 4455/7134 completed (loss: 0.1062002032995224, acc: 0.9834254384040833)
[2025-02-13 04:09:46,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:46,921][root][INFO] - Training Epoch: 2/2, step 4456/7134 completed (loss: 0.013742315582931042, acc: 0.9967637658119202)
[2025-02-13 04:09:47,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:47,265][root][INFO] - Training Epoch: 2/2, step 4457/7134 completed (loss: 0.021040091291069984, acc: 0.9925093650817871)
[2025-02-13 04:09:47,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:47,676][root][INFO] - Training Epoch: 2/2, step 4458/7134 completed (loss: 0.008748463355004787, acc: 0.9967266917228699)
[2025-02-13 04:09:47,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:48,103][root][INFO] - Training Epoch: 2/2, step 4459/7134 completed (loss: 0.021108059212565422, acc: 0.9917582273483276)
[2025-02-13 04:09:48,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:48,527][root][INFO] - Training Epoch: 2/2, step 4460/7134 completed (loss: 0.02295740693807602, acc: 0.9914425611495972)
[2025-02-13 04:09:48,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:48,955][root][INFO] - Training Epoch: 2/2, step 4461/7134 completed (loss: 0.04201731085777283, acc: 0.9901599287986755)
[2025-02-13 04:09:49,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:49,387][root][INFO] - Training Epoch: 2/2, step 4462/7134 completed (loss: 0.007030311040580273, acc: 0.9972752332687378)
[2025-02-13 04:09:49,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:49,822][root][INFO] - Training Epoch: 2/2, step 4463/7134 completed (loss: 0.016033511608839035, acc: 0.99622642993927)
[2025-02-13 04:09:49,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:50,273][root][INFO] - Training Epoch: 2/2, step 4464/7134 completed (loss: 0.031375542283058167, acc: 0.9915682673454285)
[2025-02-13 04:09:50,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:50,713][root][INFO] - Training Epoch: 2/2, step 4465/7134 completed (loss: 0.009588537737727165, acc: 0.9961340427398682)
[2025-02-13 04:09:50,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:51,158][root][INFO] - Training Epoch: 2/2, step 4466/7134 completed (loss: 0.013245724141597748, acc: 0.9950617551803589)
[2025-02-13 04:09:51,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:51,605][root][INFO] - Training Epoch: 2/2, step 4467/7134 completed (loss: 0.022169865667819977, acc: 0.9924623370170593)
[2025-02-13 04:09:51,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:52,034][root][INFO] - Training Epoch: 2/2, step 4468/7134 completed (loss: 0.008040380664169788, acc: 0.9988109469413757)
[2025-02-13 04:09:52,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:52,477][root][INFO] - Training Epoch: 2/2, step 4469/7134 completed (loss: 0.019187793135643005, acc: 0.9942196607589722)
[2025-02-13 04:09:52,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:52,944][root][INFO] - Training Epoch: 2/2, step 4470/7134 completed (loss: 0.01322153490036726, acc: 0.9966850876808167)
[2025-02-13 04:09:53,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:53,373][root][INFO] - Training Epoch: 2/2, step 4471/7134 completed (loss: 0.00865598302334547, acc: 0.9975429773330688)
[2025-02-13 04:09:53,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:53,782][root][INFO] - Training Epoch: 2/2, step 4472/7134 completed (loss: 0.03501901030540466, acc: 0.9934123754501343)
[2025-02-13 04:09:53,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:54,201][root][INFO] - Training Epoch: 2/2, step 4473/7134 completed (loss: 0.06305921077728271, acc: 0.9831932783126831)
[2025-02-13 04:09:54,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:54,618][root][INFO] - Training Epoch: 2/2, step 4474/7134 completed (loss: 0.056552667170763016, acc: 0.9846583008766174)
[2025-02-13 04:09:54,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:55,059][root][INFO] - Training Epoch: 2/2, step 4475/7134 completed (loss: 0.09541745483875275, acc: 0.9702233076095581)
[2025-02-13 04:09:55,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:55,497][root][INFO] - Training Epoch: 2/2, step 4476/7134 completed (loss: 0.043617989867925644, acc: 0.9849108457565308)
[2025-02-13 04:09:55,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:55,934][root][INFO] - Training Epoch: 2/2, step 4477/7134 completed (loss: 0.006045651622116566, acc: 0.9975550174713135)
[2025-02-13 04:09:56,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:56,376][root][INFO] - Training Epoch: 2/2, step 4478/7134 completed (loss: 0.01775144599378109, acc: 0.9975278377532959)
[2025-02-13 04:09:56,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:56,794][root][INFO] - Training Epoch: 2/2, step 4479/7134 completed (loss: 0.014194129034876823, acc: 0.9973045587539673)
[2025-02-13 04:09:56,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:57,244][root][INFO] - Training Epoch: 2/2, step 4480/7134 completed (loss: 0.024326948449015617, acc: 0.9929906725883484)
[2025-02-13 04:09:57,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:57,695][root][INFO] - Training Epoch: 2/2, step 4481/7134 completed (loss: 0.03348618373274803, acc: 0.9929906725883484)
[2025-02-13 04:09:57,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:58,138][root][INFO] - Training Epoch: 2/2, step 4482/7134 completed (loss: 0.011644262820482254, acc: 0.9964028596878052)
[2025-02-13 04:09:58,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:58,539][root][INFO] - Training Epoch: 2/2, step 4483/7134 completed (loss: 0.026467982679605484, acc: 0.9925705790519714)
[2025-02-13 04:09:58,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:58,991][root][INFO] - Training Epoch: 2/2, step 4484/7134 completed (loss: 0.011679479852318764, acc: 0.9973545074462891)
[2025-02-13 04:09:59,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:59,399][root][INFO] - Training Epoch: 2/2, step 4485/7134 completed (loss: 0.010002554394304752, acc: 0.9969372153282166)
[2025-02-13 04:09:59,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:59,789][root][INFO] - Training Epoch: 2/2, step 4486/7134 completed (loss: 0.00979609228670597, acc: 0.9985074400901794)
[2025-02-13 04:09:59,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:00,209][root][INFO] - Training Epoch: 2/2, step 4487/7134 completed (loss: 0.005320937838405371, acc: 0.9986206889152527)
[2025-02-13 04:10:00,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:00,559][root][INFO] - Training Epoch: 2/2, step 4488/7134 completed (loss: 0.00938801746815443, acc: 0.9964028596878052)
[2025-02-13 04:10:00,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:00,981][root][INFO] - Training Epoch: 2/2, step 4489/7134 completed (loss: 0.010564401745796204, acc: 0.9982817769050598)
[2025-02-13 04:10:01,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:01,404][root][INFO] - Training Epoch: 2/2, step 4490/7134 completed (loss: 0.011426957324147224, acc: 0.9962962865829468)
[2025-02-13 04:10:01,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:01,809][root][INFO] - Training Epoch: 2/2, step 4491/7134 completed (loss: 0.002211964223533869, acc: 1.0)
[2025-02-13 04:10:01,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:02,207][root][INFO] - Training Epoch: 2/2, step 4492/7134 completed (loss: 0.010331987403333187, acc: 0.9944853186607361)
[2025-02-13 04:10:02,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:02,612][root][INFO] - Training Epoch: 2/2, step 4493/7134 completed (loss: 0.006514390464872122, acc: 0.9986245036125183)
[2025-02-13 04:10:02,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:03,039][root][INFO] - Training Epoch: 2/2, step 4494/7134 completed (loss: 0.02288568764925003, acc: 0.9942113161087036)
[2025-02-13 04:10:03,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:03,475][root][INFO] - Training Epoch: 2/2, step 4495/7134 completed (loss: 0.01244195643812418, acc: 0.9974457025527954)
[2025-02-13 04:10:03,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:03,886][root][INFO] - Training Epoch: 2/2, step 4496/7134 completed (loss: 0.01771320402622223, acc: 0.9914529919624329)
[2025-02-13 04:10:04,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:04,293][root][INFO] - Training Epoch: 2/2, step 4497/7134 completed (loss: 0.013629346154630184, acc: 0.9973509907722473)
[2025-02-13 04:10:04,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:04,720][root][INFO] - Training Epoch: 2/2, step 4498/7134 completed (loss: 0.04547742381691933, acc: 0.9927431344985962)
[2025-02-13 04:10:04,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:05,064][root][INFO] - Training Epoch: 2/2, step 4499/7134 completed (loss: 0.02567131444811821, acc: 0.9930434823036194)
[2025-02-13 04:10:05,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:05,499][root][INFO] - Training Epoch: 2/2, step 4500/7134 completed (loss: 0.027062302455306053, acc: 0.9930070042610168)
[2025-02-13 04:10:05,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:05,909][root][INFO] - Training Epoch: 2/2, step 4501/7134 completed (loss: 0.017949819564819336, acc: 0.99589604139328)
[2025-02-13 04:10:06,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:06,294][root][INFO] - Training Epoch: 2/2, step 4502/7134 completed (loss: 0.011675572022795677, acc: 0.9970015287399292)
[2025-02-13 04:10:06,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:06,663][root][INFO] - Training Epoch: 2/2, step 4503/7134 completed (loss: 0.011993983760476112, acc: 0.9967897534370422)
[2025-02-13 04:10:06,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:07,070][root][INFO] - Training Epoch: 2/2, step 4504/7134 completed (loss: 0.026214664801955223, acc: 0.9895366430282593)
[2025-02-13 04:10:07,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:07,515][root][INFO] - Training Epoch: 2/2, step 4505/7134 completed (loss: 0.023910565301775932, acc: 0.9901719689369202)
[2025-02-13 04:10:07,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:07,923][root][INFO] - Training Epoch: 2/2, step 4506/7134 completed (loss: 0.02005605958402157, acc: 0.9897040128707886)
[2025-02-13 04:10:08,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:08,324][root][INFO] - Training Epoch: 2/2, step 4507/7134 completed (loss: 0.017885372042655945, acc: 0.9943100810050964)
[2025-02-13 04:10:08,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:08,733][root][INFO] - Training Epoch: 2/2, step 4508/7134 completed (loss: 0.040309470146894455, acc: 0.9886040091514587)
[2025-02-13 04:10:08,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:09,183][root][INFO] - Training Epoch: 2/2, step 4509/7134 completed (loss: 0.031867582350969315, acc: 0.991946280002594)
[2025-02-13 04:10:09,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:09,623][root][INFO] - Training Epoch: 2/2, step 4510/7134 completed (loss: 0.04265682399272919, acc: 0.9858956336975098)
[2025-02-13 04:10:09,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:10,067][root][INFO] - Training Epoch: 2/2, step 4511/7134 completed (loss: 0.05673115327954292, acc: 0.9831223487854004)
[2025-02-13 04:10:10,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:10,467][root][INFO] - Training Epoch: 2/2, step 4512/7134 completed (loss: 0.025390803813934326, acc: 0.9910394549369812)
[2025-02-13 04:10:10,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:10,911][root][INFO] - Training Epoch: 2/2, step 4513/7134 completed (loss: 0.028161874040961266, acc: 0.9943820238113403)
[2025-02-13 04:10:11,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:11,338][root][INFO] - Training Epoch: 2/2, step 4514/7134 completed (loss: 0.0762074738740921, acc: 0.9810218811035156)
[2025-02-13 04:10:11,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:11,775][root][INFO] - Training Epoch: 2/2, step 4515/7134 completed (loss: 0.021387286484241486, acc: 0.9940298795700073)
[2025-02-13 04:10:11,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:12,205][root][INFO] - Training Epoch: 2/2, step 4516/7134 completed (loss: 0.01696091517806053, acc: 0.9926560521125793)
[2025-02-13 04:10:12,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:12,639][root][INFO] - Training Epoch: 2/2, step 4517/7134 completed (loss: 0.033035025000572205, acc: 0.991037130355835)
[2025-02-13 04:10:12,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:13,083][root][INFO] - Training Epoch: 2/2, step 4518/7134 completed (loss: 0.04215407744050026, acc: 0.9897959232330322)
[2025-02-13 04:10:13,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:13,526][root][INFO] - Training Epoch: 2/2, step 4519/7134 completed (loss: 0.025553707033395767, acc: 0.993954062461853)
[2025-02-13 04:10:13,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:13,960][root][INFO] - Training Epoch: 2/2, step 4520/7134 completed (loss: 0.018525728955864906, acc: 0.9957627058029175)
[2025-02-13 04:10:14,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:14,423][root][INFO] - Training Epoch: 2/2, step 4521/7134 completed (loss: 0.05752785503864288, acc: 0.9855595827102661)
[2025-02-13 04:10:14,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:14,868][root][INFO] - Training Epoch: 2/2, step 4522/7134 completed (loss: 0.030108798295259476, acc: 0.989924430847168)
[2025-02-13 04:10:15,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:15,299][root][INFO] - Training Epoch: 2/2, step 4523/7134 completed (loss: 0.026386110112071037, acc: 0.9881656765937805)
[2025-02-13 04:10:15,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:15,758][root][INFO] - Training Epoch: 2/2, step 4524/7134 completed (loss: 0.044251974672079086, acc: 0.9851552248001099)
[2025-02-13 04:10:15,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:16,191][root][INFO] - Training Epoch: 2/2, step 4525/7134 completed (loss: 0.0231774915009737, acc: 0.9933333396911621)
[2025-02-13 04:10:16,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:16,597][root][INFO] - Training Epoch: 2/2, step 4526/7134 completed (loss: 0.03002971038222313, acc: 0.9896774291992188)
[2025-02-13 04:10:16,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:17,075][root][INFO] - Training Epoch: 2/2, step 4527/7134 completed (loss: 0.02135084755718708, acc: 0.9910813570022583)
[2025-02-13 04:10:17,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:17,519][root][INFO] - Training Epoch: 2/2, step 4528/7134 completed (loss: 0.0437544584274292, acc: 0.9858611822128296)
[2025-02-13 04:10:17,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:17,914][root][INFO] - Training Epoch: 2/2, step 4529/7134 completed (loss: 0.05789938196539879, acc: 0.9883419871330261)
[2025-02-13 04:10:18,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:18,309][root][INFO] - Training Epoch: 2/2, step 4530/7134 completed (loss: 0.018863197416067123, acc: 0.9929971694946289)
[2025-02-13 04:10:18,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:18,752][root][INFO] - Training Epoch: 2/2, step 4531/7134 completed (loss: 0.024452053010463715, acc: 0.9928186535835266)
[2025-02-13 04:10:18,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:19,207][root][INFO] - Training Epoch: 2/2, step 4532/7134 completed (loss: 0.03496966511011124, acc: 0.9910358786582947)
[2025-02-13 04:10:19,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:19,636][root][INFO] - Training Epoch: 2/2, step 4533/7134 completed (loss: 0.020523738116025925, acc: 0.9940688014030457)
[2025-02-13 04:10:19,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:20,068][root][INFO] - Training Epoch: 2/2, step 4534/7134 completed (loss: 0.012012086808681488, acc: 0.9964788556098938)
[2025-02-13 04:10:20,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:20,495][root][INFO] - Training Epoch: 2/2, step 4535/7134 completed (loss: 0.02301453799009323, acc: 0.9943052530288696)
[2025-02-13 04:10:20,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:20,971][root][INFO] - Training Epoch: 2/2, step 4536/7134 completed (loss: 0.027187295258045197, acc: 0.9913793206214905)
[2025-02-13 04:10:21,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:21,432][root][INFO] - Training Epoch: 2/2, step 4537/7134 completed (loss: 0.03151227533817291, acc: 0.9932960867881775)
[2025-02-13 04:10:21,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:21,892][root][INFO] - Training Epoch: 2/2, step 4538/7134 completed (loss: 0.03153606131672859, acc: 0.9908046126365662)
[2025-02-13 04:10:22,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:22,352][root][INFO] - Training Epoch: 2/2, step 4539/7134 completed (loss: 0.030310912057757378, acc: 0.9892125129699707)
[2025-02-13 04:10:22,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:22,803][root][INFO] - Training Epoch: 2/2, step 4540/7134 completed (loss: 0.030641866847872734, acc: 0.9913294911384583)
[2025-02-13 04:10:22,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:23,163][root][INFO] - Training Epoch: 2/2, step 4541/7134 completed (loss: 0.03912609815597534, acc: 0.9856262803077698)
[2025-02-13 04:10:23,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:23,529][root][INFO] - Training Epoch: 2/2, step 4542/7134 completed (loss: 0.04452621564269066, acc: 0.9891774654388428)
[2025-02-13 04:10:23,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:23,902][root][INFO] - Training Epoch: 2/2, step 4543/7134 completed (loss: 0.05936737358570099, acc: 0.9821746945381165)
[2025-02-13 04:10:24,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:24,324][root][INFO] - Training Epoch: 2/2, step 4544/7134 completed (loss: 0.030348720028996468, acc: 0.9923780560493469)
[2025-02-13 04:10:24,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:24,726][root][INFO] - Training Epoch: 2/2, step 4545/7134 completed (loss: 0.06009093299508095, acc: 0.9865125417709351)
[2025-02-13 04:10:24,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:25,125][root][INFO] - Training Epoch: 2/2, step 4546/7134 completed (loss: 0.04805869236588478, acc: 0.9932773113250732)
[2025-02-13 04:10:25,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:25,523][root][INFO] - Training Epoch: 2/2, step 4547/7134 completed (loss: 0.02889188379049301, acc: 0.9861591458320618)
[2025-02-13 04:10:25,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:25,954][root][INFO] - Training Epoch: 2/2, step 4548/7134 completed (loss: 0.0336884930729866, acc: 0.9867330193519592)
[2025-02-13 04:10:26,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:26,309][root][INFO] - Training Epoch: 2/2, step 4549/7134 completed (loss: 0.04921134561300278, acc: 0.9849785566329956)
[2025-02-13 04:10:26,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:26,743][root][INFO] - Training Epoch: 2/2, step 4550/7134 completed (loss: 0.047918397933244705, acc: 0.9932998418807983)
[2025-02-13 04:10:26,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:27,144][root][INFO] - Training Epoch: 2/2, step 4551/7134 completed (loss: 0.03396540880203247, acc: 0.9928571581840515)
[2025-02-13 04:10:27,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:27,533][root][INFO] - Training Epoch: 2/2, step 4552/7134 completed (loss: 0.028256261721253395, acc: 0.9904761910438538)
[2025-02-13 04:10:27,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:27,920][root][INFO] - Training Epoch: 2/2, step 4553/7134 completed (loss: 0.03411386162042618, acc: 0.9881556630134583)
[2025-02-13 04:10:28,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:28,304][root][INFO] - Training Epoch: 2/2, step 4554/7134 completed (loss: 0.04673835262656212, acc: 0.9837837815284729)
[2025-02-13 04:10:28,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:28,704][root][INFO] - Training Epoch: 2/2, step 4555/7134 completed (loss: 0.030166514217853546, acc: 0.9916897416114807)
[2025-02-13 04:10:28,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:29,149][root][INFO] - Training Epoch: 2/2, step 4556/7134 completed (loss: 0.06247451901435852, acc: 0.9834586381912231)
[2025-02-13 04:10:29,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:29,488][root][INFO] - Training Epoch: 2/2, step 4557/7134 completed (loss: 0.05267161875963211, acc: 0.985401451587677)
[2025-02-13 04:10:29,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:29,885][root][INFO] - Training Epoch: 2/2, step 4558/7134 completed (loss: 0.043758612126111984, acc: 0.9831649661064148)
[2025-02-13 04:10:30,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:30,297][root][INFO] - Training Epoch: 2/2, step 4559/7134 completed (loss: 0.04300238564610481, acc: 0.9831932783126831)
[2025-02-13 04:10:30,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:30,693][root][INFO] - Training Epoch: 2/2, step 4560/7134 completed (loss: 0.025418788194656372, acc: 0.9945651888847351)
[2025-02-13 04:10:30,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:31,052][root][INFO] - Training Epoch: 2/2, step 4561/7134 completed (loss: 0.04715424031019211, acc: 0.9862744808197021)
[2025-02-13 04:10:31,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:31,445][root][INFO] - Training Epoch: 2/2, step 4562/7134 completed (loss: 0.04039694368839264, acc: 0.9834254384040833)
[2025-02-13 04:10:31,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:31,873][root][INFO] - Training Epoch: 2/2, step 4563/7134 completed (loss: 0.030551616102457047, acc: 0.9908397197723389)
[2025-02-13 04:10:32,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:32,319][root][INFO] - Training Epoch: 2/2, step 4564/7134 completed (loss: 0.04314993694424629, acc: 0.9844852089881897)
[2025-02-13 04:10:32,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:32,699][root][INFO] - Training Epoch: 2/2, step 4565/7134 completed (loss: 0.0245184525847435, acc: 0.990439772605896)
[2025-02-13 04:10:32,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:33,109][root][INFO] - Training Epoch: 2/2, step 4566/7134 completed (loss: 0.029645003378391266, acc: 0.9928315281867981)
[2025-02-13 04:10:33,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:33,488][root][INFO] - Training Epoch: 2/2, step 4567/7134 completed (loss: 0.012658840976655483, acc: 0.9963235259056091)
[2025-02-13 04:10:33,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:33,859][root][INFO] - Training Epoch: 2/2, step 4568/7134 completed (loss: 0.052566882222890854, acc: 0.9862385392189026)
[2025-02-13 04:10:33,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:34,262][root][INFO] - Training Epoch: 2/2, step 4569/7134 completed (loss: 0.013788496144115925, acc: 0.9941176176071167)
[2025-02-13 04:10:34,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:34,670][root][INFO] - Training Epoch: 2/2, step 4570/7134 completed (loss: 0.0202163252979517, acc: 0.9927113652229309)
[2025-02-13 04:10:34,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:35,098][root][INFO] - Training Epoch: 2/2, step 4571/7134 completed (loss: 0.02221149392426014, acc: 0.9935815334320068)
[2025-02-13 04:10:35,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:35,517][root][INFO] - Training Epoch: 2/2, step 4572/7134 completed (loss: 0.030051065608859062, acc: 0.9922118186950684)
[2025-02-13 04:10:35,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:35,947][root][INFO] - Training Epoch: 2/2, step 4573/7134 completed (loss: 0.028412694111466408, acc: 0.9929971694946289)
[2025-02-13 04:10:36,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:36,364][root][INFO] - Training Epoch: 2/2, step 4574/7134 completed (loss: 0.011142703704535961, acc: 0.9946996569633484)
[2025-02-13 04:10:36,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:36,792][root][INFO] - Training Epoch: 2/2, step 4575/7134 completed (loss: 0.03611097112298012, acc: 0.9893617033958435)
[2025-02-13 04:10:36,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:37,219][root][INFO] - Training Epoch: 2/2, step 4576/7134 completed (loss: 0.009078689850866795, acc: 0.9962359070777893)
[2025-02-13 04:10:37,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:37,638][root][INFO] - Training Epoch: 2/2, step 4577/7134 completed (loss: 0.02687220834195614, acc: 0.992977499961853)
[2025-02-13 04:10:37,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:38,049][root][INFO] - Training Epoch: 2/2, step 4578/7134 completed (loss: 0.01308402605354786, acc: 0.9956331849098206)
[2025-02-13 04:10:38,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:38,449][root][INFO] - Training Epoch: 2/2, step 4579/7134 completed (loss: 0.04629812389612198, acc: 0.9904912710189819)
[2025-02-13 04:10:38,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:38,860][root][INFO] - Training Epoch: 2/2, step 4580/7134 completed (loss: 0.018965639173984528, acc: 0.9925373196601868)
[2025-02-13 04:10:38,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:39,271][root][INFO] - Training Epoch: 2/2, step 4581/7134 completed (loss: 0.019289422780275345, acc: 0.9929453134536743)
[2025-02-13 04:10:39,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:39,687][root][INFO] - Training Epoch: 2/2, step 4582/7134 completed (loss: 0.01580791175365448, acc: 0.9917762875556946)
[2025-02-13 04:10:39,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:40,074][root][INFO] - Training Epoch: 2/2, step 4583/7134 completed (loss: 0.008389392867684364, acc: 0.9950980544090271)
[2025-02-13 04:10:40,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:40,482][root][INFO] - Training Epoch: 2/2, step 4584/7134 completed (loss: 0.011706524528563023, acc: 0.9951691031455994)
[2025-02-13 04:10:40,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:40,903][root][INFO] - Training Epoch: 2/2, step 4585/7134 completed (loss: 0.008320614695549011, acc: 0.9983818531036377)
[2025-02-13 04:10:41,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:41,355][root][INFO] - Training Epoch: 2/2, step 4586/7134 completed (loss: 0.02029687538743019, acc: 0.9921976327896118)
[2025-02-13 04:10:41,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:41,783][root][INFO] - Training Epoch: 2/2, step 4587/7134 completed (loss: 0.022652562707662582, acc: 0.9945205450057983)
[2025-02-13 04:10:41,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:42,211][root][INFO] - Training Epoch: 2/2, step 4588/7134 completed (loss: 0.042550407350063324, acc: 0.9840116500854492)
[2025-02-13 04:10:42,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:42,606][root][INFO] - Training Epoch: 2/2, step 4589/7134 completed (loss: 0.04631125181913376, acc: 0.9865269660949707)
[2025-02-13 04:10:42,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:43,033][root][INFO] - Training Epoch: 2/2, step 4590/7134 completed (loss: 0.0314343087375164, acc: 0.9917126893997192)
[2025-02-13 04:10:43,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:43,461][root][INFO] - Training Epoch: 2/2, step 4591/7134 completed (loss: 0.023888517171144485, acc: 0.9881266355514526)
[2025-02-13 04:10:43,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:43,860][root][INFO] - Training Epoch: 2/2, step 4592/7134 completed (loss: 0.032296303659677505, acc: 0.9883889555931091)
[2025-02-13 04:10:43,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:44,271][root][INFO] - Training Epoch: 2/2, step 4593/7134 completed (loss: 0.03683702275156975, acc: 0.9925925731658936)
[2025-02-13 04:10:44,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:44,720][root][INFO] - Training Epoch: 2/2, step 4594/7134 completed (loss: 0.037349849939346313, acc: 0.9893333315849304)
[2025-02-13 04:10:44,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:45,147][root][INFO] - Training Epoch: 2/2, step 4595/7134 completed (loss: 0.022904694080352783, acc: 0.9925705790519714)
[2025-02-13 04:10:45,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:45,562][root][INFO] - Training Epoch: 2/2, step 4596/7134 completed (loss: 0.03044789843261242, acc: 0.9873015880584717)
[2025-02-13 04:10:45,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:45,998][root][INFO] - Training Epoch: 2/2, step 4597/7134 completed (loss: 0.016121651977300644, acc: 0.9963548183441162)
[2025-02-13 04:10:46,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:46,442][root][INFO] - Training Epoch: 2/2, step 4598/7134 completed (loss: 0.04971563443541527, acc: 0.9865030646324158)
[2025-02-13 04:10:46,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:46,832][root][INFO] - Training Epoch: 2/2, step 4599/7134 completed (loss: 0.01954854279756546, acc: 0.9920508861541748)
[2025-02-13 04:10:46,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:47,252][root][INFO] - Training Epoch: 2/2, step 4600/7134 completed (loss: 0.01754157990217209, acc: 0.9959404468536377)
[2025-02-13 04:10:47,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:47,715][root][INFO] - Training Epoch: 2/2, step 4601/7134 completed (loss: 0.03976019471883774, acc: 0.9843546152114868)
[2025-02-13 04:10:47,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:48,147][root][INFO] - Training Epoch: 2/2, step 4602/7134 completed (loss: 0.027701187878847122, acc: 0.9904631972312927)
[2025-02-13 04:10:48,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:48,574][root][INFO] - Training Epoch: 2/2, step 4603/7134 completed (loss: 0.02961447276175022, acc: 0.9944827556610107)
[2025-02-13 04:10:48,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:48,984][root][INFO] - Training Epoch: 2/2, step 4604/7134 completed (loss: 0.018259380012750626, acc: 0.9887640476226807)
[2025-02-13 04:10:49,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:49,401][root][INFO] - Training Epoch: 2/2, step 4605/7134 completed (loss: 0.037707190960645676, acc: 0.9847161769866943)
[2025-02-13 04:10:49,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:49,829][root][INFO] - Training Epoch: 2/2, step 4606/7134 completed (loss: 0.014396521262824535, acc: 0.9954128265380859)
[2025-02-13 04:10:49,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:50,198][root][INFO] - Training Epoch: 2/2, step 4607/7134 completed (loss: 0.012644555419683456, acc: 0.9915825128555298)
[2025-02-13 04:10:50,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:50,617][root][INFO] - Training Epoch: 2/2, step 4608/7134 completed (loss: 0.009767537005245686, acc: 0.9970370531082153)
[2025-02-13 04:10:50,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:51,030][root][INFO] - Training Epoch: 2/2, step 4609/7134 completed (loss: 0.017950832843780518, acc: 0.9944953918457031)
[2025-02-13 04:10:51,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:51,454][root][INFO] - Training Epoch: 2/2, step 4610/7134 completed (loss: 0.018945282325148582, acc: 0.9907038807868958)
[2025-02-13 04:10:51,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:51,871][root][INFO] - Training Epoch: 2/2, step 4611/7134 completed (loss: 0.00891829188913107, acc: 0.9968701004981995)
[2025-02-13 04:10:52,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:52,235][root][INFO] - Training Epoch: 2/2, step 4612/7134 completed (loss: 0.019722359254956245, acc: 0.9944238066673279)
[2025-02-13 04:10:52,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:52,634][root][INFO] - Training Epoch: 2/2, step 4613/7134 completed (loss: 0.017996056005358696, acc: 0.9947916865348816)
[2025-02-13 04:10:52,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:53,026][root][INFO] - Training Epoch: 2/2, step 4614/7134 completed (loss: 0.019759133458137512, acc: 0.9924242496490479)
[2025-02-13 04:10:53,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:53,472][root][INFO] - Training Epoch: 2/2, step 4615/7134 completed (loss: 0.03577835485339165, acc: 0.9864314794540405)
[2025-02-13 04:10:53,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:53,887][root][INFO] - Training Epoch: 2/2, step 4616/7134 completed (loss: 0.04001057147979736, acc: 0.9881656765937805)
[2025-02-13 04:10:54,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:54,265][root][INFO] - Training Epoch: 2/2, step 4617/7134 completed (loss: 0.02328258752822876, acc: 0.9894894957542419)
[2025-02-13 04:10:54,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:54,662][root][INFO] - Training Epoch: 2/2, step 4618/7134 completed (loss: 0.02987334504723549, acc: 0.9883494973182678)
[2025-02-13 04:10:54,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:55,099][root][INFO] - Training Epoch: 2/2, step 4619/7134 completed (loss: 0.010068521834909916, acc: 1.0)
[2025-02-13 04:10:55,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:55,523][root][INFO] - Training Epoch: 2/2, step 4620/7134 completed (loss: 0.013607207685709, acc: 0.9942029118537903)
[2025-02-13 04:10:55,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:55,924][root][INFO] - Training Epoch: 2/2, step 4621/7134 completed (loss: 0.014424550347030163, acc: 0.9922839403152466)
[2025-02-13 04:10:56,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:56,347][root][INFO] - Training Epoch: 2/2, step 4622/7134 completed (loss: 0.025111550465226173, acc: 0.995006263256073)
[2025-02-13 04:10:56,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:56,758][root][INFO] - Training Epoch: 2/2, step 4623/7134 completed (loss: 0.039247699081897736, acc: 0.9869281053543091)
[2025-02-13 04:10:56,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:57,246][root][INFO] - Training Epoch: 2/2, step 4624/7134 completed (loss: 0.054461635649204254, acc: 0.9835391044616699)
[2025-02-13 04:10:57,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:57,649][root][INFO] - Training Epoch: 2/2, step 4625/7134 completed (loss: 0.09609386324882507, acc: 0.9688013195991516)
[2025-02-13 04:10:57,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:58,088][root][INFO] - Training Epoch: 2/2, step 4626/7134 completed (loss: 0.04545021802186966, acc: 0.9892617464065552)
[2025-02-13 04:10:58,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:58,499][root][INFO] - Training Epoch: 2/2, step 4627/7134 completed (loss: 0.06636357307434082, acc: 0.9787557125091553)
[2025-02-13 04:10:58,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:58,948][root][INFO] - Training Epoch: 2/2, step 4628/7134 completed (loss: 0.04649445414543152, acc: 0.9826086759567261)
[2025-02-13 04:10:59,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:59,378][root][INFO] - Training Epoch: 2/2, step 4629/7134 completed (loss: 0.058818548917770386, acc: 0.9794871807098389)
[2025-02-13 04:10:59,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:59,832][root][INFO] - Training Epoch: 2/2, step 4630/7134 completed (loss: 0.040259990841150284, acc: 0.9894319772720337)
[2025-02-13 04:10:59,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:00,278][root][INFO] - Training Epoch: 2/2, step 4631/7134 completed (loss: 0.03907593712210655, acc: 0.9878318309783936)
[2025-02-13 04:11:00,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:00,752][root][INFO] - Training Epoch: 2/2, step 4632/7134 completed (loss: 0.03761071711778641, acc: 0.9897058606147766)
[2025-02-13 04:11:00,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:01,187][root][INFO] - Training Epoch: 2/2, step 4633/7134 completed (loss: 0.042227186262607574, acc: 0.9872832298278809)
[2025-02-13 04:11:01,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:01,602][root][INFO] - Training Epoch: 2/2, step 4634/7134 completed (loss: 0.028202762827277184, acc: 0.9902912378311157)
[2025-02-13 04:11:01,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:01,972][root][INFO] - Training Epoch: 2/2, step 4635/7134 completed (loss: 0.05629580095410347, acc: 0.9893292784690857)
[2025-02-13 04:11:02,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:02,254][root][INFO] - Training Epoch: 2/2, step 4636/7134 completed (loss: 0.011902078986167908, acc: 0.9974226951599121)
[2025-02-13 04:11:02,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:02,683][root][INFO] - Training Epoch: 2/2, step 4637/7134 completed (loss: 0.02398935705423355, acc: 0.9919354915618896)
[2025-02-13 04:11:02,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:03,109][root][INFO] - Training Epoch: 2/2, step 4638/7134 completed (loss: 0.036233287304639816, acc: 0.9894319772720337)
[2025-02-13 04:11:03,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:03,538][root][INFO] - Training Epoch: 2/2, step 4639/7134 completed (loss: 0.03411915898323059, acc: 0.9939172863960266)
[2025-02-13 04:11:03,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:03,997][root][INFO] - Training Epoch: 2/2, step 4640/7134 completed (loss: 0.041134994477033615, acc: 0.9884615540504456)
[2025-02-13 04:11:04,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:04,421][root][INFO] - Training Epoch: 2/2, step 4641/7134 completed (loss: 0.017847711220383644, acc: 0.9971590638160706)
[2025-02-13 04:11:04,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:04,855][root][INFO] - Training Epoch: 2/2, step 4642/7134 completed (loss: 0.014459120109677315, acc: 0.9920634627342224)
[2025-02-13 04:11:05,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:05,310][root][INFO] - Training Epoch: 2/2, step 4643/7134 completed (loss: 0.02303755283355713, acc: 0.9952662587165833)
[2025-02-13 04:11:05,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:05,768][root][INFO] - Training Epoch: 2/2, step 4644/7134 completed (loss: 0.017902052029967308, acc: 0.997633159160614)
[2025-02-13 04:11:05,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:06,210][root][INFO] - Training Epoch: 2/2, step 4645/7134 completed (loss: 0.01662592589855194, acc: 0.9944367408752441)
[2025-02-13 04:11:06,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:06,647][root][INFO] - Training Epoch: 2/2, step 4646/7134 completed (loss: 0.015184752643108368, acc: 0.9937205910682678)
[2025-02-13 04:11:06,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:07,107][root][INFO] - Training Epoch: 2/2, step 4647/7134 completed (loss: 0.06208052113652229, acc: 0.9895833134651184)
[2025-02-13 04:11:07,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:07,541][root][INFO] - Training Epoch: 2/2, step 4648/7134 completed (loss: 0.018241384997963905, acc: 0.9945130348205566)
[2025-02-13 04:11:07,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:07,982][root][INFO] - Training Epoch: 2/2, step 4649/7134 completed (loss: 0.027652475982904434, acc: 0.9929947257041931)
[2025-02-13 04:11:08,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:08,437][root][INFO] - Training Epoch: 2/2, step 4650/7134 completed (loss: 0.03787973150610924, acc: 0.988095223903656)
[2025-02-13 04:11:08,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:08,857][root][INFO] - Training Epoch: 2/2, step 4651/7134 completed (loss: 0.06930246204137802, acc: 0.9771863222122192)
[2025-02-13 04:11:08,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:09,285][root][INFO] - Training Epoch: 2/2, step 4652/7134 completed (loss: 0.030781729146838188, acc: 0.9923780560493469)
[2025-02-13 04:11:09,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:09,746][root][INFO] - Training Epoch: 2/2, step 4653/7134 completed (loss: 0.04217143356800079, acc: 0.9860215187072754)
[2025-02-13 04:11:09,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:10,192][root][INFO] - Training Epoch: 2/2, step 4654/7134 completed (loss: 0.0603976771235466, acc: 0.9805115461349487)
[2025-02-13 04:11:10,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:10,628][root][INFO] - Training Epoch: 2/2, step 4655/7134 completed (loss: 0.04096687585115433, acc: 0.9904240965843201)
[2025-02-13 04:11:10,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:11,078][root][INFO] - Training Epoch: 2/2, step 4656/7134 completed (loss: 0.03352326899766922, acc: 0.9924242496490479)
[2025-02-13 04:11:11,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:11,486][root][INFO] - Training Epoch: 2/2, step 4657/7134 completed (loss: 0.04520253464579582, acc: 0.9915013909339905)
[2025-02-13 04:11:11,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:11,898][root][INFO] - Training Epoch: 2/2, step 4658/7134 completed (loss: 0.04323435202240944, acc: 0.9851149916648865)
[2025-02-13 04:11:12,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:12,342][root][INFO] - Training Epoch: 2/2, step 4659/7134 completed (loss: 0.06560665369033813, acc: 0.9845758080482483)
[2025-02-13 04:11:12,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:12,778][root][INFO] - Training Epoch: 2/2, step 4660/7134 completed (loss: 0.07289785146713257, acc: 0.9811912178993225)
[2025-02-13 04:11:12,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:13,163][root][INFO] - Training Epoch: 2/2, step 4661/7134 completed (loss: 0.03265893831849098, acc: 0.9923780560493469)
[2025-02-13 04:11:13,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:13,552][root][INFO] - Training Epoch: 2/2, step 4662/7134 completed (loss: 0.06606447696685791, acc: 0.9829931855201721)
[2025-02-13 04:11:13,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:13,996][root][INFO] - Training Epoch: 2/2, step 4663/7134 completed (loss: 0.057816632091999054, acc: 0.9866310358047485)
[2025-02-13 04:11:14,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:14,439][root][INFO] - Training Epoch: 2/2, step 4664/7134 completed (loss: 0.03469904884696007, acc: 0.9933949708938599)
[2025-02-13 04:11:14,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:14,853][root][INFO] - Training Epoch: 2/2, step 4665/7134 completed (loss: 0.023839987814426422, acc: 0.9972714781761169)
[2025-02-13 04:11:15,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:15,308][root][INFO] - Training Epoch: 2/2, step 4666/7134 completed (loss: 0.03331436216831207, acc: 0.9921773076057434)
[2025-02-13 04:11:15,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:15,698][root][INFO] - Training Epoch: 2/2, step 4667/7134 completed (loss: 0.05368628725409508, acc: 0.9827883243560791)
[2025-02-13 04:11:15,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:16,103][root][INFO] - Training Epoch: 2/2, step 4668/7134 completed (loss: 0.017520247027277946, acc: 0.9946236610412598)
[2025-02-13 04:11:16,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:16,521][root][INFO] - Training Epoch: 2/2, step 4669/7134 completed (loss: 0.02425350993871689, acc: 0.9911242723464966)
[2025-02-13 04:11:16,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:16,916][root][INFO] - Training Epoch: 2/2, step 4670/7134 completed (loss: 0.06392600387334824, acc: 0.9824561476707458)
[2025-02-13 04:11:17,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:17,324][root][INFO] - Training Epoch: 2/2, step 4671/7134 completed (loss: 0.03443522751331329, acc: 0.9901269674301147)
[2025-02-13 04:11:17,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:17,722][root][INFO] - Training Epoch: 2/2, step 4672/7134 completed (loss: 0.045214418321847916, acc: 0.9878048896789551)
[2025-02-13 04:11:17,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:18,127][root][INFO] - Training Epoch: 2/2, step 4673/7134 completed (loss: 0.04092717170715332, acc: 0.9828392863273621)
[2025-02-13 04:11:18,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:18,541][root][INFO] - Training Epoch: 2/2, step 4674/7134 completed (loss: 0.03396254777908325, acc: 0.9920760989189148)
[2025-02-13 04:11:18,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:18,980][root][INFO] - Training Epoch: 2/2, step 4675/7134 completed (loss: 0.03520180657505989, acc: 0.9923567175865173)
[2025-02-13 04:11:19,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:19,387][root][INFO] - Training Epoch: 2/2, step 4676/7134 completed (loss: 0.01599201001226902, acc: 0.9954819083213806)
[2025-02-13 04:11:19,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:19,772][root][INFO] - Training Epoch: 2/2, step 4677/7134 completed (loss: 0.025993945077061653, acc: 0.9934210777282715)
[2025-02-13 04:11:19,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:20,184][root][INFO] - Training Epoch: 2/2, step 4678/7134 completed (loss: 0.04852769523859024, acc: 0.9866270422935486)
[2025-02-13 04:11:20,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:20,610][root][INFO] - Training Epoch: 2/2, step 4679/7134 completed (loss: 0.03297828510403633, acc: 0.9889655113220215)
[2025-02-13 04:11:20,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:20,997][root][INFO] - Training Epoch: 2/2, step 4680/7134 completed (loss: 0.05086512118577957, acc: 0.985981285572052)
[2025-02-13 04:11:21,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:21,401][root][INFO] - Training Epoch: 2/2, step 4681/7134 completed (loss: 0.03173629567027092, acc: 0.989230751991272)
[2025-02-13 04:11:21,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:21,877][root][INFO] - Training Epoch: 2/2, step 4682/7134 completed (loss: 0.019046612083911896, acc: 0.9963503479957581)
[2025-02-13 04:11:22,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:22,279][root][INFO] - Training Epoch: 2/2, step 4683/7134 completed (loss: 0.03013533353805542, acc: 0.9900142550468445)
[2025-02-13 04:11:22,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:22,740][root][INFO] - Training Epoch: 2/2, step 4684/7134 completed (loss: 0.05401317775249481, acc: 0.9821802973747253)
[2025-02-13 04:11:22,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:23,194][root][INFO] - Training Epoch: 2/2, step 4685/7134 completed (loss: 0.03617218881845474, acc: 0.9866803288459778)
[2025-02-13 04:11:23,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:23,666][root][INFO] - Training Epoch: 2/2, step 4686/7134 completed (loss: 0.028752995654940605, acc: 0.9930151104927063)
[2025-02-13 04:11:23,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:23,992][root][INFO] - Training Epoch: 2/2, step 4687/7134 completed (loss: 0.09466049820184708, acc: 0.9693251252174377)
[2025-02-13 04:11:24,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:24,434][root][INFO] - Training Epoch: 2/2, step 4688/7134 completed (loss: 0.13562345504760742, acc: 0.9589322209358215)
[2025-02-13 04:11:24,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:24,895][root][INFO] - Training Epoch: 2/2, step 4689/7134 completed (loss: 0.10359915345907211, acc: 0.9749631881713867)
[2025-02-13 04:11:25,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:25,342][root][INFO] - Training Epoch: 2/2, step 4690/7134 completed (loss: 0.06544621288776398, acc: 0.9853249192237854)
[2025-02-13 04:11:25,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:25,767][root][INFO] - Training Epoch: 2/2, step 4691/7134 completed (loss: 0.030401501804590225, acc: 0.9863945841789246)
[2025-02-13 04:11:25,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:26,086][root][INFO] - Training Epoch: 2/2, step 4692/7134 completed (loss: 0.11144529283046722, acc: 0.9723991751670837)
[2025-02-13 04:11:26,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:26,576][root][INFO] - Training Epoch: 2/2, step 4693/7134 completed (loss: 0.1401969939470291, acc: 0.965413510799408)
[2025-02-13 04:11:26,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:26,973][root][INFO] - Training Epoch: 2/2, step 4694/7134 completed (loss: 0.1393749862909317, acc: 0.9756592512130737)
[2025-02-13 04:11:27,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:27,393][root][INFO] - Training Epoch: 2/2, step 4695/7134 completed (loss: 0.05478539690375328, acc: 0.9819375872612)
[2025-02-13 04:11:27,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:27,825][root][INFO] - Training Epoch: 2/2, step 4696/7134 completed (loss: 0.02134002558887005, acc: 0.9925705790519714)
[2025-02-13 04:11:27,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:28,153][root][INFO] - Training Epoch: 2/2, step 4697/7134 completed (loss: 0.02842407114803791, acc: 0.9906014800071716)
[2025-02-13 04:11:28,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:28,546][root][INFO] - Training Epoch: 2/2, step 4698/7134 completed (loss: 0.09366418421268463, acc: 0.9751908183097839)
[2025-02-13 04:11:28,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:28,995][root][INFO] - Training Epoch: 2/2, step 4699/7134 completed (loss: 0.053836338222026825, acc: 0.9908592104911804)
[2025-02-13 04:11:29,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:29,431][root][INFO] - Training Epoch: 2/2, step 4700/7134 completed (loss: 0.0531449168920517, acc: 0.9821162223815918)
[2025-02-13 04:11:29,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:29,762][root][INFO] - Training Epoch: 2/2, step 4701/7134 completed (loss: 0.04754181206226349, acc: 0.9872204661369324)
[2025-02-13 04:11:29,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:30,201][root][INFO] - Training Epoch: 2/2, step 4702/7134 completed (loss: 0.04352334141731262, acc: 0.987261176109314)
[2025-02-13 04:11:30,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:30,648][root][INFO] - Training Epoch: 2/2, step 4703/7134 completed (loss: 0.03620237112045288, acc: 0.9891892075538635)
[2025-02-13 04:11:30,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:31,062][root][INFO] - Training Epoch: 2/2, step 4704/7134 completed (loss: 0.020369380712509155, acc: 0.9941383600234985)
[2025-02-13 04:11:31,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:31,505][root][INFO] - Training Epoch: 2/2, step 4705/7134 completed (loss: 0.030569808557629585, acc: 0.9871345162391663)
[2025-02-13 04:11:31,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:31,966][root][INFO] - Training Epoch: 2/2, step 4706/7134 completed (loss: 0.08884429931640625, acc: 0.9781931638717651)
[2025-02-13 04:11:32,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:32,379][root][INFO] - Training Epoch: 2/2, step 4707/7134 completed (loss: 0.14561323821544647, acc: 0.9710884094238281)
[2025-02-13 04:11:32,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:32,787][root][INFO] - Training Epoch: 2/2, step 4708/7134 completed (loss: 0.0166719239205122, acc: 0.9925187230110168)
[2025-02-13 04:11:32,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:33,231][root][INFO] - Training Epoch: 2/2, step 4709/7134 completed (loss: 0.021212896332144737, acc: 0.9934123754501343)
[2025-02-13 04:11:33,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:33,666][root][INFO] - Training Epoch: 2/2, step 4710/7134 completed (loss: 0.034509409219026566, acc: 0.9912663698196411)
[2025-02-13 04:11:33,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:34,128][root][INFO] - Training Epoch: 2/2, step 4711/7134 completed (loss: 0.054889678955078125, acc: 0.9830220937728882)
[2025-02-13 04:11:34,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:34,537][root][INFO] - Training Epoch: 2/2, step 4712/7134 completed (loss: 0.06654514372348785, acc: 0.9818181991577148)
[2025-02-13 04:11:34,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:34,959][root][INFO] - Training Epoch: 2/2, step 4713/7134 completed (loss: 0.016740061342716217, acc: 0.9964285492897034)
[2025-02-13 04:11:35,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:35,312][root][INFO] - Training Epoch: 2/2, step 4714/7134 completed (loss: 0.04664275050163269, acc: 0.9831775426864624)
[2025-02-13 04:11:35,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:35,713][root][INFO] - Training Epoch: 2/2, step 4715/7134 completed (loss: 0.08683472871780396, acc: 0.9765458703041077)
[2025-02-13 04:11:35,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:36,106][root][INFO] - Training Epoch: 2/2, step 4716/7134 completed (loss: 0.007894158363342285, acc: 0.9969512224197388)
[2025-02-13 04:11:36,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:36,504][root][INFO] - Training Epoch: 2/2, step 4717/7134 completed (loss: 0.030654868111014366, acc: 0.995555579662323)
[2025-02-13 04:11:36,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:36,904][root][INFO] - Training Epoch: 2/2, step 4718/7134 completed (loss: 0.031008874997496605, acc: 0.9900662302970886)
[2025-02-13 04:11:37,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:37,324][root][INFO] - Training Epoch: 2/2, step 4719/7134 completed (loss: 0.02309565059840679, acc: 0.9941002726554871)
[2025-02-13 04:11:37,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:37,749][root][INFO] - Training Epoch: 2/2, step 4720/7134 completed (loss: 0.005184610839933157, acc: 0.9985975027084351)
[2025-02-13 04:11:37,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:38,143][root][INFO] - Training Epoch: 2/2, step 4721/7134 completed (loss: 0.0070572467520833015, acc: 0.9956772327423096)
[2025-02-13 04:11:38,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:38,543][root][INFO] - Training Epoch: 2/2, step 4722/7134 completed (loss: 0.016311101615428925, acc: 0.9911242723464966)
[2025-02-13 04:11:38,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:38,940][root][INFO] - Training Epoch: 2/2, step 4723/7134 completed (loss: 0.008276489563286304, acc: 0.9969512224197388)
[2025-02-13 04:11:39,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:39,395][root][INFO] - Training Epoch: 2/2, step 4724/7134 completed (loss: 0.026112567633390427, acc: 0.9911242723464966)
[2025-02-13 04:11:39,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:39,820][root][INFO] - Training Epoch: 2/2, step 4725/7134 completed (loss: 0.011678637936711311, acc: 0.9973614811897278)
[2025-02-13 04:11:39,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:40,235][root][INFO] - Training Epoch: 2/2, step 4726/7134 completed (loss: 0.04137585312128067, acc: 0.9872340559959412)
[2025-02-13 04:11:40,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:40,640][root][INFO] - Training Epoch: 2/2, step 4727/7134 completed (loss: 0.03860774263739586, acc: 0.9890282154083252)
[2025-02-13 04:11:40,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:41,043][root][INFO] - Training Epoch: 2/2, step 4728/7134 completed (loss: 0.03035586141049862, acc: 0.9868203997612)
[2025-02-13 04:11:41,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:41,448][root][INFO] - Training Epoch: 2/2, step 4729/7134 completed (loss: 0.020177381113171577, acc: 0.9943661689758301)
[2025-02-13 04:11:41,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:41,863][root][INFO] - Training Epoch: 2/2, step 4730/7134 completed (loss: 0.019307194277644157, acc: 0.9938744306564331)
[2025-02-13 04:11:41,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:42,265][root][INFO] - Training Epoch: 2/2, step 4731/7134 completed (loss: 0.029111014679074287, acc: 0.9923664331436157)
[2025-02-13 04:11:42,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:42,661][root][INFO] - Training Epoch: 2/2, step 4732/7134 completed (loss: 0.050524402409791946, acc: 0.9836660623550415)
[2025-02-13 04:11:42,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:43,074][root][INFO] - Training Epoch: 2/2, step 4733/7134 completed (loss: 0.007134812884032726, acc: 0.9967690110206604)
[2025-02-13 04:11:43,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:43,489][root][INFO] - Training Epoch: 2/2, step 4734/7134 completed (loss: 0.038219254463911057, acc: 0.9856915473937988)
[2025-02-13 04:11:43,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:43,915][root][INFO] - Training Epoch: 2/2, step 4735/7134 completed (loss: 0.018873203545808792, acc: 0.9954954981803894)
[2025-02-13 04:11:44,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:44,278][root][INFO] - Training Epoch: 2/2, step 4736/7134 completed (loss: 0.053887754678726196, acc: 0.990138053894043)
[2025-02-13 04:11:44,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:44,693][root][INFO] - Training Epoch: 2/2, step 4737/7134 completed (loss: 0.023592544719576836, acc: 0.9952977895736694)
[2025-02-13 04:11:44,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:45,089][root][INFO] - Training Epoch: 2/2, step 4738/7134 completed (loss: 0.0160298440605402, acc: 0.9939302206039429)
[2025-02-13 04:11:45,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:45,487][root][INFO] - Training Epoch: 2/2, step 4739/7134 completed (loss: 0.08153443038463593, acc: 0.9712460041046143)
[2025-02-13 04:11:45,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:45,906][root][INFO] - Training Epoch: 2/2, step 4740/7134 completed (loss: 0.06463296711444855, acc: 0.9805389046669006)
[2025-02-13 04:11:46,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:46,253][root][INFO] - Training Epoch: 2/2, step 4741/7134 completed (loss: 0.04579876735806465, acc: 0.9870610237121582)
[2025-02-13 04:11:46,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:46,657][root][INFO] - Training Epoch: 2/2, step 4742/7134 completed (loss: 0.07165984064340591, acc: 0.982300877571106)
[2025-02-13 04:11:46,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:47,055][root][INFO] - Training Epoch: 2/2, step 4743/7134 completed (loss: 0.042284052819013596, acc: 0.9839857816696167)
[2025-02-13 04:11:47,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:47,388][root][INFO] - Training Epoch: 2/2, step 4744/7134 completed (loss: 0.031158503144979477, acc: 0.9917355179786682)
[2025-02-13 04:11:47,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:47,707][root][INFO] - Training Epoch: 2/2, step 4745/7134 completed (loss: 0.031504370272159576, acc: 0.9899665713310242)
[2025-02-13 04:11:47,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:48,112][root][INFO] - Training Epoch: 2/2, step 4746/7134 completed (loss: 0.028328701853752136, acc: 0.998161792755127)
[2025-02-13 04:11:48,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:48,544][root][INFO] - Training Epoch: 2/2, step 4747/7134 completed (loss: 0.020390313118696213, acc: 0.9946019053459167)
[2025-02-13 04:11:48,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:48,975][root][INFO] - Training Epoch: 2/2, step 4748/7134 completed (loss: 0.05783487856388092, acc: 0.9808259606361389)
[2025-02-13 04:11:49,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:49,412][root][INFO] - Training Epoch: 2/2, step 4749/7134 completed (loss: 0.04188309237360954, acc: 0.9933333396911621)
[2025-02-13 04:11:49,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:49,836][root][INFO] - Training Epoch: 2/2, step 4750/7134 completed (loss: 0.052163757383823395, acc: 0.9807322025299072)
[2025-02-13 04:11:49,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:50,295][root][INFO] - Training Epoch: 2/2, step 4751/7134 completed (loss: 0.019419267773628235, acc: 0.9943422675132751)
[2025-02-13 04:11:50,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:50,741][root][INFO] - Training Epoch: 2/2, step 4752/7134 completed (loss: 0.04631049558520317, acc: 0.9923195242881775)
[2025-02-13 04:11:50,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:51,150][root][INFO] - Training Epoch: 2/2, step 4753/7134 completed (loss: 0.07623592764139175, acc: 0.9804216623306274)
[2025-02-13 04:11:51,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:51,569][root][INFO] - Training Epoch: 2/2, step 4754/7134 completed (loss: 0.01666681468486786, acc: 0.997063159942627)
[2025-02-13 04:11:51,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:52,001][root][INFO] - Training Epoch: 2/2, step 4755/7134 completed (loss: 0.054544128477573395, acc: 0.9861303567886353)
[2025-02-13 04:11:52,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:52,391][root][INFO] - Training Epoch: 2/2, step 4756/7134 completed (loss: 0.062160391360521317, acc: 0.9895397424697876)
[2025-02-13 04:11:52,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:52,860][root][INFO] - Training Epoch: 2/2, step 4757/7134 completed (loss: 0.025027522817254066, acc: 0.9942594766616821)
[2025-02-13 04:11:53,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:53,287][root][INFO] - Training Epoch: 2/2, step 4758/7134 completed (loss: 0.02716662921011448, acc: 0.9915966391563416)
[2025-02-13 04:11:53,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:53,721][root][INFO] - Training Epoch: 2/2, step 4759/7134 completed (loss: 0.011912131682038307, acc: 0.998275876045227)
[2025-02-13 04:11:53,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:54,125][root][INFO] - Training Epoch: 2/2, step 4760/7134 completed (loss: 0.030331280082464218, acc: 0.9937888383865356)
[2025-02-13 04:11:54,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:54,530][root][INFO] - Training Epoch: 2/2, step 4761/7134 completed (loss: 0.06415338814258575, acc: 0.989313006401062)
[2025-02-13 04:11:54,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:54,953][root][INFO] - Training Epoch: 2/2, step 4762/7134 completed (loss: 0.0534525029361248, acc: 0.9891892075538635)
[2025-02-13 04:11:55,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:55,364][root][INFO] - Training Epoch: 2/2, step 4763/7134 completed (loss: 0.05184260010719299, acc: 0.9863221645355225)
[2025-02-13 04:11:55,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:55,800][root][INFO] - Training Epoch: 2/2, step 4764/7134 completed (loss: 0.0323091484606266, acc: 0.988875150680542)
[2025-02-13 04:11:55,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:56,218][root][INFO] - Training Epoch: 2/2, step 4765/7134 completed (loss: 0.05356666445732117, acc: 0.9870503544807434)
[2025-02-13 04:11:56,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:56,655][root][INFO] - Training Epoch: 2/2, step 4766/7134 completed (loss: 0.04163181781768799, acc: 0.9897040128707886)
[2025-02-13 04:11:56,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:56,980][root][INFO] - Training Epoch: 2/2, step 4767/7134 completed (loss: 0.05488366261124611, acc: 0.9796437621116638)
[2025-02-13 04:11:57,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:57,441][root][INFO] - Training Epoch: 2/2, step 4768/7134 completed (loss: 0.05259491503238678, acc: 0.9823848009109497)
[2025-02-13 04:11:57,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:57,829][root][INFO] - Training Epoch: 2/2, step 4769/7134 completed (loss: 0.039886415004730225, acc: 0.9903692007064819)
[2025-02-13 04:11:57,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:58,219][root][INFO] - Training Epoch: 2/2, step 4770/7134 completed (loss: 0.026189077645540237, acc: 0.991919219493866)
[2025-02-13 04:11:58,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:58,612][root][INFO] - Training Epoch: 2/2, step 4771/7134 completed (loss: 0.06254716217517853, acc: 0.9869186282157898)
[2025-02-13 04:11:58,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:59,014][root][INFO] - Training Epoch: 2/2, step 4772/7134 completed (loss: 0.04116057604551315, acc: 0.9920254945755005)
[2025-02-13 04:11:59,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:59,415][root][INFO] - Training Epoch: 2/2, step 4773/7134 completed (loss: 0.04365599527955055, acc: 0.9863013625144958)
[2025-02-13 04:11:59,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:59,798][root][INFO] - Training Epoch: 2/2, step 4774/7134 completed (loss: 0.031155524775385857, acc: 0.992977499961853)
[2025-02-13 04:11:59,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:00,208][root][INFO] - Training Epoch: 2/2, step 4775/7134 completed (loss: 0.0429014228284359, acc: 0.988727867603302)
[2025-02-13 04:12:00,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:00,633][root][INFO] - Training Epoch: 2/2, step 4776/7134 completed (loss: 0.04538111761212349, acc: 0.9871588945388794)
[2025-02-13 04:12:00,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:01,077][root][INFO] - Training Epoch: 2/2, step 4777/7134 completed (loss: 0.014967397786676884, acc: 0.9966611266136169)
[2025-02-13 04:12:01,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:01,481][root][INFO] - Training Epoch: 2/2, step 4778/7134 completed (loss: 0.029096834361553192, acc: 0.9934354424476624)
[2025-02-13 04:12:01,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:01,925][root][INFO] - Training Epoch: 2/2, step 4779/7134 completed (loss: 0.017818879336118698, acc: 0.9963167309761047)
[2025-02-13 04:12:02,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:02,349][root][INFO] - Training Epoch: 2/2, step 4780/7134 completed (loss: 0.004697935190051794, acc: 0.9984423518180847)
[2025-02-13 04:12:02,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:02,764][root][INFO] - Training Epoch: 2/2, step 4781/7134 completed (loss: 0.018084552139043808, acc: 0.9921721816062927)
[2025-02-13 04:12:02,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:03,191][root][INFO] - Training Epoch: 2/2, step 4782/7134 completed (loss: 0.010740368627011776, acc: 0.9951377511024475)
[2025-02-13 04:12:03,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:03,598][root][INFO] - Training Epoch: 2/2, step 4783/7134 completed (loss: 0.05457185581326485, acc: 0.9875444769859314)
[2025-02-13 04:12:03,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:04,044][root][INFO] - Training Epoch: 2/2, step 4784/7134 completed (loss: 0.02130304090678692, acc: 0.9945429563522339)
[2025-02-13 04:12:04,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:04,445][root][INFO] - Training Epoch: 2/2, step 4785/7134 completed (loss: 0.04363613575696945, acc: 0.9906367063522339)
[2025-02-13 04:12:04,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:04,857][root][INFO] - Training Epoch: 2/2, step 4786/7134 completed (loss: 0.036916885524988174, acc: 0.9922360181808472)
[2025-02-13 04:12:05,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:05,269][root][INFO] - Training Epoch: 2/2, step 4787/7134 completed (loss: 0.00664999894797802, acc: 0.9984177350997925)
[2025-02-13 04:12:05,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:05,676][root][INFO] - Training Epoch: 2/2, step 4788/7134 completed (loss: 0.01009219791740179, acc: 0.9964538812637329)
[2025-02-13 04:12:05,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:06,081][root][INFO] - Training Epoch: 2/2, step 4789/7134 completed (loss: 0.023519782349467278, acc: 0.993630588054657)
[2025-02-13 04:12:06,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:06,517][root][INFO] - Training Epoch: 2/2, step 4790/7134 completed (loss: 0.014725526794791222, acc: 0.9945651888847351)
[2025-02-13 04:12:06,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:06,924][root][INFO] - Training Epoch: 2/2, step 4791/7134 completed (loss: 0.012868612073361874, acc: 0.9962756037712097)
[2025-02-13 04:12:07,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:07,346][root][INFO] - Training Epoch: 2/2, step 4792/7134 completed (loss: 0.0063133640214800835, acc: 0.9983277320861816)
[2025-02-13 04:12:07,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:07,703][root][INFO] - Training Epoch: 2/2, step 4793/7134 completed (loss: 0.06219841539859772, acc: 0.983561635017395)
[2025-02-13 04:12:07,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:08,116][root][INFO] - Training Epoch: 2/2, step 4794/7134 completed (loss: 0.05023429915308952, acc: 0.9828571677207947)
[2025-02-13 04:12:08,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:08,550][root][INFO] - Training Epoch: 2/2, step 4795/7134 completed (loss: 0.018284952268004417, acc: 0.991525411605835)
[2025-02-13 04:12:08,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:08,947][root][INFO] - Training Epoch: 2/2, step 4796/7134 completed (loss: 0.020932769402861595, acc: 0.9905660152435303)
[2025-02-13 04:12:09,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:09,385][root][INFO] - Training Epoch: 2/2, step 4797/7134 completed (loss: 0.018512384966015816, acc: 0.9937888383865356)
[2025-02-13 04:12:09,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:09,800][root][INFO] - Training Epoch: 2/2, step 4798/7134 completed (loss: 0.01560299564152956, acc: 0.9920106530189514)
[2025-02-13 04:12:09,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:10,248][root][INFO] - Training Epoch: 2/2, step 4799/7134 completed (loss: 0.020492812618613243, acc: 0.9929278492927551)
[2025-02-13 04:12:10,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:10,658][root][INFO] - Training Epoch: 2/2, step 4800/7134 completed (loss: 0.01826641708612442, acc: 0.9940387606620789)
[2025-02-13 04:12:10,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:11,100][root][INFO] - Training Epoch: 2/2, step 4801/7134 completed (loss: 0.014753120951354504, acc: 0.9977375268936157)
[2025-02-13 04:12:11,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:11,545][root][INFO] - Training Epoch: 2/2, step 4802/7134 completed (loss: 0.0360492505133152, acc: 0.9869109988212585)
[2025-02-13 04:12:11,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:11,937][root][INFO] - Training Epoch: 2/2, step 4803/7134 completed (loss: 0.04676487296819687, acc: 0.9867647290229797)
[2025-02-13 04:12:12,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:12,370][root][INFO] - Training Epoch: 2/2, step 4804/7134 completed (loss: 0.03611854836344719, acc: 0.9846153855323792)
[2025-02-13 04:12:12,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:12,802][root][INFO] - Training Epoch: 2/2, step 4805/7134 completed (loss: 0.03512856736779213, acc: 0.991909384727478)
[2025-02-13 04:12:12,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:13,228][root][INFO] - Training Epoch: 2/2, step 4806/7134 completed (loss: 0.02880493737757206, acc: 0.9933444261550903)
[2025-02-13 04:12:13,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:13,608][root][INFO] - Training Epoch: 2/2, step 4807/7134 completed (loss: 0.03852693364024162, acc: 0.9939209818840027)
[2025-02-13 04:12:13,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:14,001][root][INFO] - Training Epoch: 2/2, step 4808/7134 completed (loss: 0.03191598504781723, acc: 0.9946808218955994)
[2025-02-13 04:12:14,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:14,426][root][INFO] - Training Epoch: 2/2, step 4809/7134 completed (loss: 0.019838489592075348, acc: 0.9968253970146179)
[2025-02-13 04:12:14,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:14,877][root][INFO] - Training Epoch: 2/2, step 4810/7134 completed (loss: 0.03847439959645271, acc: 0.995720386505127)
[2025-02-13 04:12:15,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:15,273][root][INFO] - Training Epoch: 2/2, step 4811/7134 completed (loss: 0.04333607107400894, acc: 0.9903100728988647)
[2025-02-13 04:12:15,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:15,696][root][INFO] - Training Epoch: 2/2, step 4812/7134 completed (loss: 0.0988043025135994, acc: 0.9819193482398987)
[2025-02-13 04:12:15,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:16,027][root][INFO] - Training Epoch: 2/2, step 4813/7134 completed (loss: 0.017048684880137444, acc: 0.9948052167892456)
[2025-02-13 04:12:16,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:16,446][root][INFO] - Training Epoch: 2/2, step 4814/7134 completed (loss: 0.025335827842354774, acc: 0.9948979616165161)
[2025-02-13 04:12:16,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:16,763][root][INFO] - Training Epoch: 2/2, step 4815/7134 completed (loss: 0.0021069967187941074, acc: 1.0)
[2025-02-13 04:12:16,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:17,180][root][INFO] - Training Epoch: 2/2, step 4816/7134 completed (loss: 0.004468670580536127, acc: 0.9975247383117676)
[2025-02-13 04:12:17,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:17,582][root][INFO] - Training Epoch: 2/2, step 4817/7134 completed (loss: 0.008735175244510174, acc: 0.9975903630256653)
[2025-02-13 04:12:17,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:18,054][root][INFO] - Training Epoch: 2/2, step 4818/7134 completed (loss: 0.02148216962814331, acc: 0.9969230890274048)
[2025-02-13 04:12:18,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:18,459][root][INFO] - Training Epoch: 2/2, step 4819/7134 completed (loss: 0.020837442949414253, acc: 0.9959431886672974)
[2025-02-13 04:12:18,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:18,902][root][INFO] - Training Epoch: 2/2, step 4820/7134 completed (loss: 0.016504380851984024, acc: 0.9974259734153748)
[2025-02-13 04:12:19,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:19,341][root][INFO] - Training Epoch: 2/2, step 4821/7134 completed (loss: 0.021771235391497612, acc: 0.99301677942276)
[2025-02-13 04:12:19,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:19,756][root][INFO] - Training Epoch: 2/2, step 4822/7134 completed (loss: 0.02401762828230858, acc: 0.996219277381897)
[2025-02-13 04:12:19,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:20,188][root][INFO] - Training Epoch: 2/2, step 4823/7134 completed (loss: 0.05691705644130707, acc: 0.9873617887496948)
[2025-02-13 04:12:20,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:20,560][root][INFO] - Training Epoch: 2/2, step 4824/7134 completed (loss: 0.031610071659088135, acc: 0.9885844588279724)
[2025-02-13 04:12:20,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:20,966][root][INFO] - Training Epoch: 2/2, step 4825/7134 completed (loss: 0.00945256557315588, acc: 0.9963503479957581)
[2025-02-13 04:12:21,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:21,420][root][INFO] - Training Epoch: 2/2, step 4826/7134 completed (loss: 0.02462843246757984, acc: 0.991416335105896)
[2025-02-13 04:12:21,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:21,812][root][INFO] - Training Epoch: 2/2, step 4827/7134 completed (loss: 0.023466059938073158, acc: 0.9948096871376038)
[2025-02-13 04:12:21,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:22,245][root][INFO] - Training Epoch: 2/2, step 4828/7134 completed (loss: 0.009253737516701221, acc: 0.9987849593162537)
[2025-02-13 04:12:22,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:22,695][root][INFO] - Training Epoch: 2/2, step 4829/7134 completed (loss: 0.008574220351874828, acc: 0.9961538314819336)
[2025-02-13 04:12:22,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:23,137][root][INFO] - Training Epoch: 2/2, step 4830/7134 completed (loss: 0.011468357406556606, acc: 0.9955621361732483)
[2025-02-13 04:12:23,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:23,574][root][INFO] - Training Epoch: 2/2, step 4831/7134 completed (loss: 0.007831508293747902, acc: 0.9970760345458984)
[2025-02-13 04:12:23,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:23,992][root][INFO] - Training Epoch: 2/2, step 4832/7134 completed (loss: 0.026077590882778168, acc: 0.9934853315353394)
[2025-02-13 04:12:24,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:24,445][root][INFO] - Training Epoch: 2/2, step 4833/7134 completed (loss: 0.008937553502619267, acc: 0.9974586963653564)
[2025-02-13 04:12:24,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:24,883][root][INFO] - Training Epoch: 2/2, step 4834/7134 completed (loss: 0.023511668667197227, acc: 0.9940476417541504)
[2025-02-13 04:12:25,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:25,307][root][INFO] - Training Epoch: 2/2, step 4835/7134 completed (loss: 0.013756346888840199, acc: 0.9945945739746094)
[2025-02-13 04:12:25,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:25,738][root][INFO] - Training Epoch: 2/2, step 4836/7134 completed (loss: 0.00955286342650652, acc: 0.9969465732574463)
[2025-02-13 04:12:25,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:26,133][root][INFO] - Training Epoch: 2/2, step 4837/7134 completed (loss: 0.05143490433692932, acc: 0.9889705777168274)
[2025-02-13 04:12:26,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:26,550][root][INFO] - Training Epoch: 2/2, step 4838/7134 completed (loss: 0.01654018834233284, acc: 0.9967793822288513)
[2025-02-13 04:12:26,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:26,936][root][INFO] - Training Epoch: 2/2, step 4839/7134 completed (loss: 0.04511215537786484, acc: 0.9913941621780396)
[2025-02-13 04:12:27,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:27,359][root][INFO] - Training Epoch: 2/2, step 4840/7134 completed (loss: 0.03051498532295227, acc: 0.9913294911384583)
[2025-02-13 04:12:27,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:27,751][root][INFO] - Training Epoch: 2/2, step 4841/7134 completed (loss: 0.0303829126060009, acc: 0.992548406124115)
[2025-02-13 04:12:27,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:28,191][root][INFO] - Training Epoch: 2/2, step 4842/7134 completed (loss: 0.04512565955519676, acc: 0.9885203838348389)
[2025-02-13 04:12:28,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:28,634][root][INFO] - Training Epoch: 2/2, step 4843/7134 completed (loss: 0.021177692338824272, acc: 0.9940564632415771)
[2025-02-13 04:12:28,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:29,008][root][INFO] - Training Epoch: 2/2, step 4844/7134 completed (loss: 0.02138804830610752, acc: 0.993537962436676)
[2025-02-13 04:12:29,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:29,448][root][INFO] - Training Epoch: 2/2, step 4845/7134 completed (loss: 0.024876952171325684, acc: 0.9931694269180298)
[2025-02-13 04:12:29,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:29,869][root][INFO] - Training Epoch: 2/2, step 4846/7134 completed (loss: 0.02347615733742714, acc: 0.9918919205665588)
[2025-02-13 04:12:29,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:30,266][root][INFO] - Training Epoch: 2/2, step 4847/7134 completed (loss: 0.02213493175804615, acc: 0.9946091771125793)
[2025-02-13 04:12:30,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:30,670][root][INFO] - Training Epoch: 2/2, step 4848/7134 completed (loss: 0.023414522409439087, acc: 0.9972375631332397)
[2025-02-13 04:12:30,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:31,054][root][INFO] - Training Epoch: 2/2, step 4849/7134 completed (loss: 0.019205357879400253, acc: 0.9955423474311829)
[2025-02-13 04:12:31,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:31,488][root][INFO] - Training Epoch: 2/2, step 4850/7134 completed (loss: 0.03452812135219574, acc: 0.9926560521125793)
[2025-02-13 04:12:31,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:31,938][root][INFO] - Training Epoch: 2/2, step 4851/7134 completed (loss: 0.027729790657758713, acc: 0.9894859790802002)
[2025-02-13 04:12:32,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:32,375][root][INFO] - Training Epoch: 2/2, step 4852/7134 completed (loss: 0.027687616646289825, acc: 0.989347517490387)
[2025-02-13 04:12:32,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:32,777][root][INFO] - Training Epoch: 2/2, step 4853/7134 completed (loss: 0.06471128016710281, acc: 0.9885877370834351)
[2025-02-13 04:12:32,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:33,163][root][INFO] - Training Epoch: 2/2, step 4854/7134 completed (loss: 0.03084411285817623, acc: 0.9946666955947876)
[2025-02-13 04:12:33,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:33,598][root][INFO] - Training Epoch: 2/2, step 4855/7134 completed (loss: 0.025728339329361916, acc: 0.9927448630332947)
[2025-02-13 04:12:33,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:34,054][root][INFO] - Training Epoch: 2/2, step 4856/7134 completed (loss: 0.03013312816619873, acc: 0.9923664331436157)
[2025-02-13 04:12:34,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:34,455][root][INFO] - Training Epoch: 2/2, step 4857/7134 completed (loss: 0.009505797177553177, acc: 0.9971222877502441)
[2025-02-13 04:12:34,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:34,883][root][INFO] - Training Epoch: 2/2, step 4858/7134 completed (loss: 0.02177443355321884, acc: 0.994301974773407)
[2025-02-13 04:12:35,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:35,318][root][INFO] - Training Epoch: 2/2, step 4859/7134 completed (loss: 0.015918347984552383, acc: 0.9948520064353943)
[2025-02-13 04:12:35,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:35,737][root][INFO] - Training Epoch: 2/2, step 4860/7134 completed (loss: 0.011402699165046215, acc: 0.9972752332687378)
[2025-02-13 04:12:35,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:36,157][root][INFO] - Training Epoch: 2/2, step 4861/7134 completed (loss: 0.014539184048771858, acc: 0.9946091771125793)
[2025-02-13 04:12:36,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:36,570][root][INFO] - Training Epoch: 2/2, step 4862/7134 completed (loss: 0.02364458702504635, acc: 0.995708167552948)
[2025-02-13 04:12:36,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:36,984][root][INFO] - Training Epoch: 2/2, step 4863/7134 completed (loss: 0.02129662036895752, acc: 0.9963503479957581)
[2025-02-13 04:12:37,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:37,390][root][INFO] - Training Epoch: 2/2, step 4864/7134 completed (loss: 0.01794668287038803, acc: 0.9923076629638672)
[2025-02-13 04:12:37,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:37,819][root][INFO] - Training Epoch: 2/2, step 4865/7134 completed (loss: 0.028634510934352875, acc: 0.9938949942588806)
[2025-02-13 04:12:37,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:38,276][root][INFO] - Training Epoch: 2/2, step 4866/7134 completed (loss: 0.029847638681530952, acc: 0.993514895439148)
[2025-02-13 04:12:38,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:38,715][root][INFO] - Training Epoch: 2/2, step 4867/7134 completed (loss: 0.04292002320289612, acc: 0.9894179701805115)
[2025-02-13 04:12:38,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:39,109][root][INFO] - Training Epoch: 2/2, step 4868/7134 completed (loss: 0.04480762779712677, acc: 0.9903100728988647)
[2025-02-13 04:12:39,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:39,504][root][INFO] - Training Epoch: 2/2, step 4869/7134 completed (loss: 0.04001249000430107, acc: 0.9911110997200012)
[2025-02-13 04:12:39,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:39,941][root][INFO] - Training Epoch: 2/2, step 4870/7134 completed (loss: 0.02139933593571186, acc: 0.9953051805496216)
[2025-02-13 04:12:40,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:40,328][root][INFO] - Training Epoch: 2/2, step 4871/7134 completed (loss: 0.03389957919716835, acc: 0.9938271641731262)
[2025-02-13 04:12:40,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:40,738][root][INFO] - Training Epoch: 2/2, step 4872/7134 completed (loss: 0.012169650755822659, acc: 0.9958275556564331)
[2025-02-13 04:12:40,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:41,189][root][INFO] - Training Epoch: 2/2, step 4873/7134 completed (loss: 0.021964237093925476, acc: 0.9908854365348816)
[2025-02-13 04:12:41,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:41,633][root][INFO] - Training Epoch: 2/2, step 4874/7134 completed (loss: 0.017916373908519745, acc: 0.99314284324646)
[2025-02-13 04:12:41,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:42,089][root][INFO] - Training Epoch: 2/2, step 4875/7134 completed (loss: 0.022985728457570076, acc: 0.9916782379150391)
[2025-02-13 04:12:42,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:42,419][root][INFO] - Training Epoch: 2/2, step 4876/7134 completed (loss: 0.054236967116594315, acc: 0.985200822353363)
[2025-02-13 04:12:42,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:42,846][root][INFO] - Training Epoch: 2/2, step 4877/7134 completed (loss: 0.03450220450758934, acc: 0.9870967864990234)
[2025-02-13 04:12:42,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:43,250][root][INFO] - Training Epoch: 2/2, step 4878/7134 completed (loss: 0.02482076920568943, acc: 0.9896907210350037)
[2025-02-13 04:12:43,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:43,692][root][INFO] - Training Epoch: 2/2, step 4879/7134 completed (loss: 0.051923006772994995, acc: 0.9849397540092468)
[2025-02-13 04:12:43,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:44,120][root][INFO] - Training Epoch: 2/2, step 4880/7134 completed (loss: 0.008538725785911083, acc: 0.9987030029296875)
[2025-02-13 04:12:44,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:44,562][root][INFO] - Training Epoch: 2/2, step 4881/7134 completed (loss: 0.039471566677093506, acc: 0.9868612885475159)
[2025-02-13 04:12:44,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:44,965][root][INFO] - Training Epoch: 2/2, step 4882/7134 completed (loss: 0.03587585687637329, acc: 0.9819079041481018)
[2025-02-13 04:12:45,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:45,375][root][INFO] - Training Epoch: 2/2, step 4883/7134 completed (loss: 0.02692871168255806, acc: 0.9868612885475159)
[2025-02-13 04:12:45,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:45,809][root][INFO] - Training Epoch: 2/2, step 4884/7134 completed (loss: 0.009089646860957146, acc: 0.9953917264938354)
[2025-02-13 04:12:45,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:46,202][root][INFO] - Training Epoch: 2/2, step 4885/7134 completed (loss: 0.010174735449254513, acc: 0.9973614811897278)
[2025-02-13 04:12:46,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:46,604][root][INFO] - Training Epoch: 2/2, step 4886/7134 completed (loss: 0.025998873636126518, acc: 0.9923076629638672)
[2025-02-13 04:12:46,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:47,058][root][INFO] - Training Epoch: 2/2, step 4887/7134 completed (loss: 0.00653200875967741, acc: 0.9988052845001221)
[2025-02-13 04:12:47,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:47,527][root][INFO] - Training Epoch: 2/2, step 4888/7134 completed (loss: 0.03948470950126648, acc: 0.9931318759918213)
[2025-02-13 04:12:47,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:47,969][root][INFO] - Training Epoch: 2/2, step 4889/7134 completed (loss: 0.005893524736166, acc: 0.9987966418266296)
[2025-02-13 04:12:48,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:48,345][root][INFO] - Training Epoch: 2/2, step 4890/7134 completed (loss: 0.03425551950931549, acc: 0.9952380657196045)
[2025-02-13 04:12:48,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:48,778][root][INFO] - Training Epoch: 2/2, step 4891/7134 completed (loss: 0.04801960289478302, acc: 0.9863184094429016)
[2025-02-13 04:12:48,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:49,182][root][INFO] - Training Epoch: 2/2, step 4892/7134 completed (loss: 0.03431379422545433, acc: 0.9916107654571533)
[2025-02-13 04:12:49,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:49,601][root][INFO] - Training Epoch: 2/2, step 4893/7134 completed (loss: 0.12821312248706818, acc: 0.9630606770515442)
[2025-02-13 04:12:49,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:50,033][root][INFO] - Training Epoch: 2/2, step 4894/7134 completed (loss: 0.00946764089167118, acc: 0.9985590577125549)
[2025-02-13 04:12:50,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:50,473][root][INFO] - Training Epoch: 2/2, step 4895/7134 completed (loss: 0.008895042352378368, acc: 0.9987499713897705)
[2025-02-13 04:12:50,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:50,873][root][INFO] - Training Epoch: 2/2, step 4896/7134 completed (loss: 0.009676122106611729, acc: 0.9985954761505127)
[2025-02-13 04:12:51,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:51,318][root][INFO] - Training Epoch: 2/2, step 4897/7134 completed (loss: 0.0385143980383873, acc: 0.9894737005233765)
[2025-02-13 04:12:51,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:51,776][root][INFO] - Training Epoch: 2/2, step 4898/7134 completed (loss: 0.016847187653183937, acc: 0.99622642993927)
[2025-02-13 04:12:51,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:52,210][root][INFO] - Training Epoch: 2/2, step 4899/7134 completed (loss: 0.020004263147711754, acc: 0.9941775798797607)
[2025-02-13 04:12:52,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:52,639][root][INFO] - Training Epoch: 2/2, step 4900/7134 completed (loss: 0.023223653435707092, acc: 0.9947368502616882)
[2025-02-13 04:12:52,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:53,071][root][INFO] - Training Epoch: 2/2, step 4901/7134 completed (loss: 0.0178709514439106, acc: 0.9939849376678467)
[2025-02-13 04:12:53,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:53,514][root][INFO] - Training Epoch: 2/2, step 4902/7134 completed (loss: 0.01299224328249693, acc: 0.9960212111473083)
[2025-02-13 04:12:53,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:53,935][root][INFO] - Training Epoch: 2/2, step 4903/7134 completed (loss: 0.005921453237533569, acc: 0.9982078671455383)
[2025-02-13 04:12:54,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:54,349][root][INFO] - Training Epoch: 2/2, step 4904/7134 completed (loss: 0.015047091990709305, acc: 0.9940298795700073)
[2025-02-13 04:12:54,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:54,763][root][INFO] - Training Epoch: 2/2, step 4905/7134 completed (loss: 0.0030624172650277615, acc: 1.0)
[2025-02-13 04:12:54,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:55,167][root][INFO] - Training Epoch: 2/2, step 4906/7134 completed (loss: 0.010838665068149567, acc: 0.9983136653900146)
[2025-02-13 04:12:55,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:55,581][root][INFO] - Training Epoch: 2/2, step 4907/7134 completed (loss: 0.01237462554126978, acc: 0.996835470199585)
[2025-02-13 04:12:55,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:56,002][root][INFO] - Training Epoch: 2/2, step 4908/7134 completed (loss: 0.005348076578229666, acc: 0.9983999729156494)
[2025-02-13 04:12:56,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:56,441][root][INFO] - Training Epoch: 2/2, step 4909/7134 completed (loss: 0.021710751578211784, acc: 0.9967585206031799)
[2025-02-13 04:12:56,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:56,853][root][INFO] - Training Epoch: 2/2, step 4910/7134 completed (loss: 0.019872797653079033, acc: 0.9921630024909973)
[2025-02-13 04:12:56,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:57,289][root][INFO] - Training Epoch: 2/2, step 4911/7134 completed (loss: 0.02186751551926136, acc: 0.9934036731719971)
[2025-02-13 04:12:57,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:57,718][root][INFO] - Training Epoch: 2/2, step 4912/7134 completed (loss: 0.02331022173166275, acc: 0.9927361011505127)
[2025-02-13 04:12:57,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:58,134][root][INFO] - Training Epoch: 2/2, step 4913/7134 completed (loss: 0.020232507959008217, acc: 0.9932432174682617)
[2025-02-13 04:12:58,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:58,552][root][INFO] - Training Epoch: 2/2, step 4914/7134 completed (loss: 0.021955816075205803, acc: 0.9954128265380859)
[2025-02-13 04:12:58,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:58,982][root][INFO] - Training Epoch: 2/2, step 4915/7134 completed (loss: 0.0030519769061356783, acc: 1.0)
[2025-02-13 04:12:59,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:59,306][root][INFO] - Training Epoch: 2/2, step 4916/7134 completed (loss: 0.039735570549964905, acc: 0.987864077091217)
[2025-02-13 04:12:59,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:59,739][root][INFO] - Training Epoch: 2/2, step 4917/7134 completed (loss: 0.04438852146267891, acc: 0.9888734221458435)
[2025-02-13 04:12:59,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:00,148][root][INFO] - Training Epoch: 2/2, step 4918/7134 completed (loss: 0.0037758781109005213, acc: 0.998633861541748)
[2025-02-13 04:13:00,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:00,623][root][INFO] - Training Epoch: 2/2, step 4919/7134 completed (loss: 0.03450834006071091, acc: 0.99210524559021)
[2025-02-13 04:13:00,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:01,070][root][INFO] - Training Epoch: 2/2, step 4920/7134 completed (loss: 0.05930037423968315, acc: 0.9909560680389404)
[2025-02-13 04:13:01,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:01,481][root][INFO] - Training Epoch: 2/2, step 4921/7134 completed (loss: 0.028967445716261864, acc: 0.9956958293914795)
[2025-02-13 04:13:01,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:01,916][root][INFO] - Training Epoch: 2/2, step 4922/7134 completed (loss: 0.020670277997851372, acc: 0.994557797908783)
[2025-02-13 04:13:02,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:02,355][root][INFO] - Training Epoch: 2/2, step 4923/7134 completed (loss: 0.01192568615078926, acc: 0.9955947399139404)
[2025-02-13 04:13:02,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:02,749][root][INFO] - Training Epoch: 2/2, step 4924/7134 completed (loss: 0.01757080666720867, acc: 0.9980000257492065)
[2025-02-13 04:13:02,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:03,156][root][INFO] - Training Epoch: 2/2, step 4925/7134 completed (loss: 0.02310912311077118, acc: 0.9885550737380981)
[2025-02-13 04:13:03,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:03,557][root][INFO] - Training Epoch: 2/2, step 4926/7134 completed (loss: 0.01308300532400608, acc: 0.9942611455917358)
[2025-02-13 04:13:03,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:03,953][root][INFO] - Training Epoch: 2/2, step 4927/7134 completed (loss: 0.02482340857386589, acc: 0.9901960492134094)
[2025-02-13 04:13:04,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:04,376][root][INFO] - Training Epoch: 2/2, step 4928/7134 completed (loss: 0.009972997941076756, acc: 0.997682511806488)
[2025-02-13 04:13:04,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:04,767][root][INFO] - Training Epoch: 2/2, step 4929/7134 completed (loss: 0.02046164683997631, acc: 0.9937984347343445)
[2025-02-13 04:13:04,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:05,169][root][INFO] - Training Epoch: 2/2, step 4930/7134 completed (loss: 0.03204313665628433, acc: 0.9954751133918762)
[2025-02-13 04:13:05,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:05,575][root][INFO] - Training Epoch: 2/2, step 4931/7134 completed (loss: 0.08220674097537994, acc: 0.9878234267234802)
[2025-02-13 04:13:05,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:06,011][root][INFO] - Training Epoch: 2/2, step 4932/7134 completed (loss: 0.01612073741853237, acc: 0.9932523369789124)
[2025-02-13 04:13:06,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:06,443][root][INFO] - Training Epoch: 2/2, step 4933/7134 completed (loss: 0.02841905876994133, acc: 0.993819534778595)
[2025-02-13 04:13:06,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:06,884][root][INFO] - Training Epoch: 2/2, step 4934/7134 completed (loss: 0.005899668671190739, acc: 0.9972936511039734)
[2025-02-13 04:13:07,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:07,325][root][INFO] - Training Epoch: 2/2, step 4935/7134 completed (loss: 0.011009089648723602, acc: 0.99726402759552)
[2025-02-13 04:13:07,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:07,732][root][INFO] - Training Epoch: 2/2, step 4936/7134 completed (loss: 0.02115473710000515, acc: 0.9907407164573669)
[2025-02-13 04:13:07,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:08,143][root][INFO] - Training Epoch: 2/2, step 4937/7134 completed (loss: 0.01733056828379631, acc: 0.9925261735916138)
[2025-02-13 04:13:08,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:08,557][root][INFO] - Training Epoch: 2/2, step 4938/7134 completed (loss: 0.01973380334675312, acc: 0.9919028282165527)
[2025-02-13 04:13:08,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:08,973][root][INFO] - Training Epoch: 2/2, step 4939/7134 completed (loss: 0.019761603325605392, acc: 0.9885807633399963)
[2025-02-13 04:13:09,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:09,394][root][INFO] - Training Epoch: 2/2, step 4940/7134 completed (loss: 0.03001265414059162, acc: 0.9954545497894287)
[2025-02-13 04:13:09,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:09,821][root][INFO] - Training Epoch: 2/2, step 4941/7134 completed (loss: 0.02247096225619316, acc: 0.9946164488792419)
[2025-02-13 04:13:09,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:10,252][root][INFO] - Training Epoch: 2/2, step 4942/7134 completed (loss: 0.03087385930120945, acc: 0.9906166195869446)
[2025-02-13 04:13:10,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:10,673][root][INFO] - Training Epoch: 2/2, step 4943/7134 completed (loss: 0.0350114107131958, acc: 0.9890561103820801)
[2025-02-13 04:13:10,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:11,083][root][INFO] - Training Epoch: 2/2, step 4944/7134 completed (loss: 0.027426637709140778, acc: 0.9894737005233765)
[2025-02-13 04:13:11,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:11,453][root][INFO] - Training Epoch: 2/2, step 4945/7134 completed (loss: 0.02279609441757202, acc: 0.9919354915618896)
[2025-02-13 04:13:11,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:11,849][root][INFO] - Training Epoch: 2/2, step 4946/7134 completed (loss: 0.05389149487018585, acc: 0.9802197813987732)
[2025-02-13 04:13:11,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:12,259][root][INFO] - Training Epoch: 2/2, step 4947/7134 completed (loss: 0.022169899195432663, acc: 0.9933481216430664)
[2025-02-13 04:13:12,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:12,624][root][INFO] - Training Epoch: 2/2, step 4948/7134 completed (loss: 0.026459714397788048, acc: 0.9946523904800415)
[2025-02-13 04:13:12,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:12,939][root][INFO] - Training Epoch: 2/2, step 4949/7134 completed (loss: 0.02074052207171917, acc: 0.9968253970146179)
[2025-02-13 04:13:13,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:13,348][root][INFO] - Training Epoch: 2/2, step 4950/7134 completed (loss: 0.011233964934945107, acc: 0.9982486963272095)
[2025-02-13 04:13:13,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:13,764][root][INFO] - Training Epoch: 2/2, step 4951/7134 completed (loss: 0.015037285163998604, acc: 0.9971510171890259)
[2025-02-13 04:13:13,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:14,207][root][INFO] - Training Epoch: 2/2, step 4952/7134 completed (loss: 0.02097718045115471, acc: 0.9950330853462219)
[2025-02-13 04:13:14,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:14,560][root][INFO] - Training Epoch: 2/2, step 4953/7134 completed (loss: 0.009162422269582748, acc: 0.9956427216529846)
[2025-02-13 04:13:14,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:14,956][root][INFO] - Training Epoch: 2/2, step 4954/7134 completed (loss: 0.009754401631653309, acc: 0.9942857027053833)
[2025-02-13 04:13:15,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:15,301][root][INFO] - Training Epoch: 2/2, step 4955/7134 completed (loss: 0.019546430557966232, acc: 0.9953161478042603)
[2025-02-13 04:13:15,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:15,708][root][INFO] - Training Epoch: 2/2, step 4956/7134 completed (loss: 0.013733035884797573, acc: 0.9957325458526611)
[2025-02-13 04:13:15,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:16,106][root][INFO] - Training Epoch: 2/2, step 4957/7134 completed (loss: 0.02260916493833065, acc: 0.9894179701805115)
[2025-02-13 04:13:16,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:16,516][root][INFO] - Training Epoch: 2/2, step 4958/7134 completed (loss: 0.00927529763430357, acc: 0.9956834316253662)
[2025-02-13 04:13:16,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:16,928][root][INFO] - Training Epoch: 2/2, step 4959/7134 completed (loss: 0.006622715387493372, acc: 0.9970588088035583)
[2025-02-13 04:13:17,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:17,331][root][INFO] - Training Epoch: 2/2, step 4960/7134 completed (loss: 0.0091167027130723, acc: 0.998275876045227)
[2025-02-13 04:13:17,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:17,739][root][INFO] - Training Epoch: 2/2, step 4961/7134 completed (loss: 0.008763223886489868, acc: 0.9963570237159729)
[2025-02-13 04:13:17,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:18,142][root][INFO] - Training Epoch: 2/2, step 4962/7134 completed (loss: 0.003922197036445141, acc: 1.0)
[2025-02-13 04:13:18,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:18,533][root][INFO] - Training Epoch: 2/2, step 4963/7134 completed (loss: 0.021835029125213623, acc: 0.9918830990791321)
[2025-02-13 04:13:18,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:18,933][root][INFO] - Training Epoch: 2/2, step 4964/7134 completed (loss: 0.015251166187226772, acc: 0.995121955871582)
[2025-02-13 04:13:19,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:19,330][root][INFO] - Training Epoch: 2/2, step 4965/7134 completed (loss: 0.031887732446193695, acc: 0.9915966391563416)
[2025-02-13 04:13:19,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:19,719][root][INFO] - Training Epoch: 2/2, step 4966/7134 completed (loss: 0.052794020622968674, acc: 0.9861496090888977)
[2025-02-13 04:13:19,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:20,120][root][INFO] - Training Epoch: 2/2, step 4967/7134 completed (loss: 0.030912119895219803, acc: 0.990234375)
[2025-02-13 04:13:20,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:20,542][root][INFO] - Training Epoch: 2/2, step 4968/7134 completed (loss: 0.06273500621318817, acc: 0.9768683314323425)
[2025-02-13 04:13:20,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:20,973][root][INFO] - Training Epoch: 2/2, step 4969/7134 completed (loss: 0.05547342076897621, acc: 0.9845361113548279)
[2025-02-13 04:13:21,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:21,369][root][INFO] - Training Epoch: 2/2, step 4970/7134 completed (loss: 0.016010163351893425, acc: 0.9899665713310242)
[2025-02-13 04:13:21,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:21,780][root][INFO] - Training Epoch: 2/2, step 4971/7134 completed (loss: 0.016952715814113617, acc: 0.993220329284668)
[2025-02-13 04:13:21,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:22,205][root][INFO] - Training Epoch: 2/2, step 4972/7134 completed (loss: 0.010108143091201782, acc: 0.9971791505813599)
[2025-02-13 04:13:22,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:22,611][root][INFO] - Training Epoch: 2/2, step 4973/7134 completed (loss: 0.03839350491762161, acc: 0.991769552230835)
[2025-02-13 04:13:22,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:22,976][root][INFO] - Training Epoch: 2/2, step 4974/7134 completed (loss: 0.034263238310813904, acc: 0.9879759550094604)
[2025-02-13 04:13:23,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:23,385][root][INFO] - Training Epoch: 2/2, step 4975/7134 completed (loss: 0.04969125986099243, acc: 0.9872204661369324)
[2025-02-13 04:13:23,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:23,709][root][INFO] - Training Epoch: 2/2, step 4976/7134 completed (loss: 0.022587604820728302, acc: 0.9906322956085205)
[2025-02-13 04:13:23,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:24,132][root][INFO] - Training Epoch: 2/2, step 4977/7134 completed (loss: 0.06690734624862671, acc: 0.9784946441650391)
[2025-02-13 04:13:24,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:24,545][root][INFO] - Training Epoch: 2/2, step 4978/7134 completed (loss: 0.015236393548548222, acc: 0.9950310587882996)
[2025-02-13 04:13:24,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:24,934][root][INFO] - Training Epoch: 2/2, step 4979/7134 completed (loss: 0.09204196184873581, acc: 0.9802306294441223)
[2025-02-13 04:13:25,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:25,375][root][INFO] - Training Epoch: 2/2, step 4980/7134 completed (loss: 0.05104217305779457, acc: 0.988727867603302)
[2025-02-13 04:13:25,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:25,822][root][INFO] - Training Epoch: 2/2, step 4981/7134 completed (loss: 0.031682997941970825, acc: 0.9872408509254456)
[2025-02-13 04:13:25,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:26,223][root][INFO] - Training Epoch: 2/2, step 4982/7134 completed (loss: 0.05062789097428322, acc: 0.9858299493789673)
[2025-02-13 04:13:26,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:26,657][root][INFO] - Training Epoch: 2/2, step 4983/7134 completed (loss: 0.04878303036093712, acc: 0.9875389337539673)
[2025-02-13 04:13:26,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:26,908][root][INFO] - Training Epoch: 2/2, step 4984/7134 completed (loss: 0.041647784411907196, acc: 0.987500011920929)
[2025-02-13 04:13:27,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:27,250][root][INFO] - Training Epoch: 2/2, step 4985/7134 completed (loss: 0.023212719708681107, acc: 0.9938837885856628)
[2025-02-13 04:13:27,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:27,652][root][INFO] - Training Epoch: 2/2, step 4986/7134 completed (loss: 0.049046095460653305, acc: 0.9851411581039429)
[2025-02-13 04:13:27,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:28,029][root][INFO] - Training Epoch: 2/2, step 4987/7134 completed (loss: 0.026966990903019905, acc: 0.9920477271080017)
[2025-02-13 04:13:28,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:28,446][root][INFO] - Training Epoch: 2/2, step 4988/7134 completed (loss: 0.013816174119710922, acc: 0.9964664578437805)
[2025-02-13 04:13:28,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:28,831][root][INFO] - Training Epoch: 2/2, step 4989/7134 completed (loss: 0.019839566200971603, acc: 0.9925742745399475)
[2025-02-13 04:13:28,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:29,231][root][INFO] - Training Epoch: 2/2, step 4990/7134 completed (loss: 0.032557740807533264, acc: 0.9932432174682617)
[2025-02-13 04:13:29,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:29,643][root][INFO] - Training Epoch: 2/2, step 4991/7134 completed (loss: 0.015222902409732342, acc: 0.9932998418807983)
[2025-02-13 04:13:29,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:30,068][root][INFO] - Training Epoch: 2/2, step 4992/7134 completed (loss: 0.1056666225194931, acc: 0.9737417697906494)
[2025-02-13 04:13:30,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:30,323][root][INFO] - Training Epoch: 2/2, step 4993/7134 completed (loss: 0.007660834118723869, acc: 1.0)
[2025-02-13 04:13:30,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:30,780][root][INFO] - Training Epoch: 2/2, step 4994/7134 completed (loss: 0.027021927759051323, acc: 0.9957447052001953)
[2025-02-13 04:13:30,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:31,040][root][INFO] - Training Epoch: 2/2, step 4995/7134 completed (loss: 0.035816870629787445, acc: 0.9877551198005676)
[2025-02-13 04:13:31,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:31,435][root][INFO] - Training Epoch: 2/2, step 4996/7134 completed (loss: 0.0185112152248621, acc: 0.9926199316978455)
[2025-02-13 04:13:31,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:31,887][root][INFO] - Training Epoch: 2/2, step 4997/7134 completed (loss: 0.030518466606736183, acc: 0.9948630332946777)
[2025-02-13 04:13:31,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:32,195][root][INFO] - Training Epoch: 2/2, step 4998/7134 completed (loss: 0.08207781612873077, acc: 0.9836065769195557)
[2025-02-13 04:13:32,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:32,596][root][INFO] - Training Epoch: 2/2, step 4999/7134 completed (loss: 0.029231112450361252, acc: 0.9931153059005737)
[2025-02-13 04:13:32,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:32,962][root][INFO] - Training Epoch: 2/2, step 5000/7134 completed (loss: 0.021751992404460907, acc: 0.9930232763290405)
[2025-02-13 04:13:33,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:33,403][root][INFO] - Training Epoch: 2/2, step 5001/7134 completed (loss: 0.043104443699121475, acc: 0.9898256063461304)
[2025-02-13 04:13:33,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:33,815][root][INFO] - Training Epoch: 2/2, step 5002/7134 completed (loss: 0.035023946315050125, acc: 0.9962825179100037)
[2025-02-13 04:13:33,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:34,211][root][INFO] - Training Epoch: 2/2, step 5003/7134 completed (loss: 0.021344570443034172, acc: 0.9936000108718872)
[2025-02-13 04:13:34,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:34,528][root][INFO] - Training Epoch: 2/2, step 5004/7134 completed (loss: 0.017746375873684883, acc: 0.9933775067329407)
[2025-02-13 04:13:34,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:34,893][root][INFO] - Training Epoch: 2/2, step 5005/7134 completed (loss: 0.030516665428876877, acc: 0.992682933807373)
[2025-02-13 04:13:35,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:35,325][root][INFO] - Training Epoch: 2/2, step 5006/7134 completed (loss: 0.02522295154631138, acc: 0.9941262602806091)
[2025-02-13 04:13:35,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:35,721][root][INFO] - Training Epoch: 2/2, step 5007/7134 completed (loss: 0.012445728294551373, acc: 0.9971751570701599)
[2025-02-13 04:13:35,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:36,179][root][INFO] - Training Epoch: 2/2, step 5008/7134 completed (loss: 0.038591060787439346, acc: 0.9901574850082397)
[2025-02-13 04:13:36,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:36,579][root][INFO] - Training Epoch: 2/2, step 5009/7134 completed (loss: 0.04148624464869499, acc: 0.9950371980667114)
[2025-02-13 04:13:36,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:36,962][root][INFO] - Training Epoch: 2/2, step 5010/7134 completed (loss: 0.024934785440564156, acc: 0.9959514141082764)
[2025-02-13 04:13:37,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:37,423][root][INFO] - Training Epoch: 2/2, step 5011/7134 completed (loss: 0.034749869257211685, acc: 0.9869918823242188)
[2025-02-13 04:13:37,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:37,693][root][INFO] - Training Epoch: 2/2, step 5012/7134 completed (loss: 0.05457324534654617, acc: 0.982758641242981)
[2025-02-13 04:13:37,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:38,075][root][INFO] - Training Epoch: 2/2, step 5013/7134 completed (loss: 0.006213256623595953, acc: 0.9979959726333618)
[2025-02-13 04:13:38,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:38,537][root][INFO] - Training Epoch: 2/2, step 5014/7134 completed (loss: 0.03517073765397072, acc: 0.9918032884597778)
[2025-02-13 04:13:38,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:38,969][root][INFO] - Training Epoch: 2/2, step 5015/7134 completed (loss: 0.07858961075544357, acc: 0.9875862002372742)
[2025-02-13 04:13:39,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:39,351][root][INFO] - Training Epoch: 2/2, step 5016/7134 completed (loss: 0.022240912541747093, acc: 0.9909583926200867)
[2025-02-13 04:13:39,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:39,747][root][INFO] - Training Epoch: 2/2, step 5017/7134 completed (loss: 0.01457980740815401, acc: 0.9945945739746094)
[2025-02-13 04:13:39,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:40,147][root][INFO] - Training Epoch: 2/2, step 5018/7134 completed (loss: 0.022470060735940933, acc: 0.9946808218955994)
[2025-02-13 04:13:40,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:40,568][root][INFO] - Training Epoch: 2/2, step 5019/7134 completed (loss: 0.02417570725083351, acc: 0.9928774833679199)
[2025-02-13 04:13:40,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:41,027][root][INFO] - Training Epoch: 2/2, step 5020/7134 completed (loss: 0.029316013678908348, acc: 0.9912499785423279)
[2025-02-13 04:13:41,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:41,433][root][INFO] - Training Epoch: 2/2, step 5021/7134 completed (loss: 0.0346798300743103, acc: 0.986270010471344)
[2025-02-13 04:13:41,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:41,851][root][INFO] - Training Epoch: 2/2, step 5022/7134 completed (loss: 0.021797966212034225, acc: 0.9969135522842407)
[2025-02-13 04:13:41,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:42,302][root][INFO] - Training Epoch: 2/2, step 5023/7134 completed (loss: 0.03467698395252228, acc: 0.9893617033958435)
[2025-02-13 04:13:42,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:42,743][root][INFO] - Training Epoch: 2/2, step 5024/7134 completed (loss: 0.023809069767594337, acc: 0.9961389899253845)
[2025-02-13 04:13:42,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:43,153][root][INFO] - Training Epoch: 2/2, step 5025/7134 completed (loss: 0.02430623769760132, acc: 0.9927536249160767)
[2025-02-13 04:13:43,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:43,540][root][INFO] - Training Epoch: 2/2, step 5026/7134 completed (loss: 0.03856397047638893, acc: 0.9894921183586121)
[2025-02-13 04:13:43,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:43,931][root][INFO] - Training Epoch: 2/2, step 5027/7134 completed (loss: 0.018261581659317017, acc: 0.9927140474319458)
[2025-02-13 04:13:44,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:44,323][root][INFO] - Training Epoch: 2/2, step 5028/7134 completed (loss: 0.013808013871312141, acc: 0.9968652129173279)
[2025-02-13 04:13:44,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:44,710][root][INFO] - Training Epoch: 2/2, step 5029/7134 completed (loss: 0.01598133146762848, acc: 0.9924242496490479)
[2025-02-13 04:13:44,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:45,143][root][INFO] - Training Epoch: 2/2, step 5030/7134 completed (loss: 0.014517560601234436, acc: 0.9959072470664978)
[2025-02-13 04:13:45,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:45,548][root][INFO] - Training Epoch: 2/2, step 5031/7134 completed (loss: 0.07090209424495697, acc: 0.988095223903656)
[2025-02-13 04:13:45,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:45,952][root][INFO] - Training Epoch: 2/2, step 5032/7134 completed (loss: 0.020459601655602455, acc: 0.9983766078948975)
[2025-02-13 04:13:46,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:46,354][root][INFO] - Training Epoch: 2/2, step 5033/7134 completed (loss: 0.006724723614752293, acc: 0.9983974099159241)
[2025-02-13 04:13:46,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:46,745][root][INFO] - Training Epoch: 2/2, step 5034/7134 completed (loss: 0.012746904045343399, acc: 0.9964538812637329)
[2025-02-13 04:13:46,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:47,144][root][INFO] - Training Epoch: 2/2, step 5035/7134 completed (loss: 0.003116088453680277, acc: 1.0)
[2025-02-13 04:13:47,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:47,561][root][INFO] - Training Epoch: 2/2, step 5036/7134 completed (loss: 0.04049813002347946, acc: 0.9888392686843872)
[2025-02-13 04:13:47,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:47,978][root][INFO] - Training Epoch: 2/2, step 5037/7134 completed (loss: 0.056940708309412, acc: 0.989276111125946)
[2025-02-13 04:13:48,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:48,350][root][INFO] - Training Epoch: 2/2, step 5038/7134 completed (loss: 0.029297197237610817, acc: 0.9955456852912903)
[2025-02-13 04:13:48,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:48,728][root][INFO] - Training Epoch: 2/2, step 5039/7134 completed (loss: 0.03514361009001732, acc: 0.9905063509941101)
[2025-02-13 04:13:48,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:49,062][root][INFO] - Training Epoch: 2/2, step 5040/7134 completed (loss: 0.009326943196356297, acc: 0.9961758852005005)
[2025-02-13 04:13:49,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:49,492][root][INFO] - Training Epoch: 2/2, step 5041/7134 completed (loss: 0.008147857151925564, acc: 0.9946949481964111)
[2025-02-13 04:13:49,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:49,857][root][INFO] - Training Epoch: 2/2, step 5042/7134 completed (loss: 0.03771783411502838, acc: 0.9906759858131409)
[2025-02-13 04:13:49,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:50,204][root][INFO] - Training Epoch: 2/2, step 5043/7134 completed (loss: 0.029067808762192726, acc: 0.9881423115730286)
[2025-02-13 04:13:50,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:50,628][root][INFO] - Training Epoch: 2/2, step 5044/7134 completed (loss: 0.024016935378313065, acc: 0.9912280440330505)
[2025-02-13 04:13:50,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:51,067][root][INFO] - Training Epoch: 2/2, step 5045/7134 completed (loss: 0.04675870016217232, acc: 0.982503354549408)
[2025-02-13 04:13:51,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:51,406][root][INFO] - Training Epoch: 2/2, step 5046/7134 completed (loss: 0.05018473044037819, acc: 0.9877192974090576)
[2025-02-13 04:13:51,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:51,743][root][INFO] - Training Epoch: 2/2, step 5047/7134 completed (loss: 0.052059829235076904, acc: 0.9834710955619812)
[2025-02-13 04:13:51,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:52,184][root][INFO] - Training Epoch: 2/2, step 5048/7134 completed (loss: 0.031712617725133896, acc: 0.9923896789550781)
[2025-02-13 04:13:52,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:52,576][root][INFO] - Training Epoch: 2/2, step 5049/7134 completed (loss: 0.06672985851764679, acc: 0.9836512207984924)
[2025-02-13 04:13:52,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:52,991][root][INFO] - Training Epoch: 2/2, step 5050/7134 completed (loss: 0.04300396516919136, acc: 0.9915611743927002)
[2025-02-13 04:13:53,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:53,409][root][INFO] - Training Epoch: 2/2, step 5051/7134 completed (loss: 0.040212616324424744, acc: 0.9898550510406494)
[2025-02-13 04:13:53,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:53,810][root][INFO] - Training Epoch: 2/2, step 5052/7134 completed (loss: 0.024452853947877884, acc: 0.9909090995788574)
[2025-02-13 04:13:53,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:54,217][root][INFO] - Training Epoch: 2/2, step 5053/7134 completed (loss: 0.06968169659376144, acc: 0.9830188751220703)
[2025-02-13 04:13:54,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:54,665][root][INFO] - Training Epoch: 2/2, step 5054/7134 completed (loss: 0.05671434476971626, acc: 0.9810725450515747)
[2025-02-13 04:13:54,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:54,998][root][INFO] - Training Epoch: 2/2, step 5055/7134 completed (loss: 0.015917355194687843, acc: 0.9959183931350708)
[2025-02-13 04:13:55,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:55,415][root][INFO] - Training Epoch: 2/2, step 5056/7134 completed (loss: 0.05288640037178993, acc: 0.9857142567634583)
[2025-02-13 04:13:55,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:55,833][root][INFO] - Training Epoch: 2/2, step 5057/7134 completed (loss: 0.0761043131351471, acc: 0.9829303026199341)
[2025-02-13 04:13:55,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:56,243][root][INFO] - Training Epoch: 2/2, step 5058/7134 completed (loss: 0.06571974605321884, acc: 0.9840579628944397)
[2025-02-13 04:13:56,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:56,687][root][INFO] - Training Epoch: 2/2, step 5059/7134 completed (loss: 0.03416069597005844, acc: 0.9908257126808167)
[2025-02-13 04:13:56,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:57,113][root][INFO] - Training Epoch: 2/2, step 5060/7134 completed (loss: 0.019723670557141304, acc: 0.9955423474311829)
[2025-02-13 04:13:57,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:57,491][root][INFO] - Training Epoch: 2/2, step 5061/7134 completed (loss: 0.037333860993385315, acc: 0.9848812222480774)
[2025-02-13 04:13:57,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:57,906][root][INFO] - Training Epoch: 2/2, step 5062/7134 completed (loss: 0.05053310841321945, acc: 0.9828473329544067)
[2025-02-13 04:13:58,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:58,331][root][INFO] - Training Epoch: 2/2, step 5063/7134 completed (loss: 0.03582190349698067, acc: 0.9883138537406921)
[2025-02-13 04:13:58,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:58,770][root][INFO] - Training Epoch: 2/2, step 5064/7134 completed (loss: 0.02361319214105606, acc: 0.9948630332946777)
[2025-02-13 04:13:58,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:59,217][root][INFO] - Training Epoch: 2/2, step 5065/7134 completed (loss: 0.038504283875226974, acc: 0.995768666267395)
[2025-02-13 04:13:59,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:59,634][root][INFO] - Training Epoch: 2/2, step 5066/7134 completed (loss: 0.037554703652858734, acc: 0.9878970980644226)
[2025-02-13 04:13:59,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:00,015][root][INFO] - Training Epoch: 2/2, step 5067/7134 completed (loss: 0.017979811877012253, acc: 0.9961013793945312)
[2025-02-13 04:14:00,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:00,420][root][INFO] - Training Epoch: 2/2, step 5068/7134 completed (loss: 0.0334111712872982, acc: 0.9889415502548218)
[2025-02-13 04:14:00,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:00,836][root][INFO] - Training Epoch: 2/2, step 5069/7134 completed (loss: 0.02208767458796501, acc: 0.9906250238418579)
[2025-02-13 04:14:00,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:01,185][root][INFO] - Training Epoch: 2/2, step 5070/7134 completed (loss: 0.021575836464762688, acc: 0.9942528605461121)
[2025-02-13 04:14:01,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:01,594][root][INFO] - Training Epoch: 2/2, step 5071/7134 completed (loss: 0.0494479201734066, acc: 0.9873772859573364)
[2025-02-13 04:14:01,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:02,022][root][INFO] - Training Epoch: 2/2, step 5072/7134 completed (loss: 0.05222534388303757, acc: 0.9849435091018677)
[2025-02-13 04:14:02,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:02,430][root][INFO] - Training Epoch: 2/2, step 5073/7134 completed (loss: 0.0468393936753273, acc: 0.9864603281021118)
[2025-02-13 04:14:02,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:02,870][root][INFO] - Training Epoch: 2/2, step 5074/7134 completed (loss: 0.03962651640176773, acc: 0.9883913993835449)
[2025-02-13 04:14:02,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:03,269][root][INFO] - Training Epoch: 2/2, step 5075/7134 completed (loss: 0.03428380563855171, acc: 0.9941860437393188)
[2025-02-13 04:14:03,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:03,721][root][INFO] - Training Epoch: 2/2, step 5076/7134 completed (loss: 0.03645145520567894, acc: 0.9904305934906006)
[2025-02-13 04:14:03,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:04,162][root][INFO] - Training Epoch: 2/2, step 5077/7134 completed (loss: 0.021732816472649574, acc: 0.9920886158943176)
[2025-02-13 04:14:04,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:04,573][root][INFO] - Training Epoch: 2/2, step 5078/7134 completed (loss: 0.06922488659620285, acc: 0.9853556752204895)
[2025-02-13 04:14:04,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:04,981][root][INFO] - Training Epoch: 2/2, step 5079/7134 completed (loss: 0.022221939638257027, acc: 0.9918367266654968)
[2025-02-13 04:14:05,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:05,433][root][INFO] - Training Epoch: 2/2, step 5080/7134 completed (loss: 0.015682458877563477, acc: 0.9940898418426514)
[2025-02-13 04:14:05,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:05,858][root][INFO] - Training Epoch: 2/2, step 5081/7134 completed (loss: 0.009055177681148052, acc: 0.9945651888847351)
[2025-02-13 04:14:06,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:06,301][root][INFO] - Training Epoch: 2/2, step 5082/7134 completed (loss: 0.015188204124569893, acc: 0.9965986609458923)
[2025-02-13 04:14:06,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:06,716][root][INFO] - Training Epoch: 2/2, step 5083/7134 completed (loss: 0.0482010655105114, acc: 0.9943740963935852)
[2025-02-13 04:14:06,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:07,176][root][INFO] - Training Epoch: 2/2, step 5084/7134 completed (loss: 0.01315052155405283, acc: 0.9942129850387573)
[2025-02-13 04:14:07,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:07,635][root][INFO] - Training Epoch: 2/2, step 5085/7134 completed (loss: 0.0180419459939003, acc: 0.9940828680992126)
[2025-02-13 04:14:07,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:08,048][root][INFO] - Training Epoch: 2/2, step 5086/7134 completed (loss: 0.032421283423900604, acc: 0.9911054372787476)
[2025-02-13 04:14:08,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:08,483][root][INFO] - Training Epoch: 2/2, step 5087/7134 completed (loss: 0.014233815483748913, acc: 0.9959839582443237)
[2025-02-13 04:14:08,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:08,896][root][INFO] - Training Epoch: 2/2, step 5088/7134 completed (loss: 0.00861620157957077, acc: 0.9985358715057373)
[2025-02-13 04:14:09,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:09,336][root][INFO] - Training Epoch: 2/2, step 5089/7134 completed (loss: 0.01293663028627634, acc: 0.9975062608718872)
[2025-02-13 04:14:09,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:09,776][root][INFO] - Training Epoch: 2/2, step 5090/7134 completed (loss: 0.011426558718085289, acc: 0.9964994192123413)
[2025-02-13 04:14:09,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:10,193][root][INFO] - Training Epoch: 2/2, step 5091/7134 completed (loss: 0.00853646732866764, acc: 0.9986824989318848)
[2025-02-13 04:14:10,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:10,608][root][INFO] - Training Epoch: 2/2, step 5092/7134 completed (loss: 0.02906317636370659, acc: 0.9944674968719482)
[2025-02-13 04:14:10,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:11,020][root][INFO] - Training Epoch: 2/2, step 5093/7134 completed (loss: 0.023189136758446693, acc: 0.9917126893997192)
[2025-02-13 04:14:11,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:11,438][root][INFO] - Training Epoch: 2/2, step 5094/7134 completed (loss: 0.03876786306500435, acc: 0.9909090995788574)
[2025-02-13 04:14:11,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:11,840][root][INFO] - Training Epoch: 2/2, step 5095/7134 completed (loss: 0.07557053118944168, acc: 0.9812792539596558)
[2025-02-13 04:14:11,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:12,283][root][INFO] - Training Epoch: 2/2, step 5096/7134 completed (loss: 0.03513277322053909, acc: 0.9907514452934265)
[2025-02-13 04:14:12,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:12,726][root][INFO] - Training Epoch: 2/2, step 5097/7134 completed (loss: 0.027614109218120575, acc: 0.990231990814209)
[2025-02-13 04:14:12,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:13,167][root][INFO] - Training Epoch: 2/2, step 5098/7134 completed (loss: 0.019479598850011826, acc: 0.9921259880065918)
[2025-02-13 04:14:13,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:13,624][root][INFO] - Training Epoch: 2/2, step 5099/7134 completed (loss: 0.027950750663876534, acc: 0.9896073937416077)
[2025-02-13 04:14:13,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:14,076][root][INFO] - Training Epoch: 2/2, step 5100/7134 completed (loss: 0.029853571206331253, acc: 0.992290735244751)
[2025-02-13 04:14:14,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:14,510][root][INFO] - Training Epoch: 2/2, step 5101/7134 completed (loss: 0.01147141121327877, acc: 0.996503472328186)
[2025-02-13 04:14:14,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:14,918][root][INFO] - Training Epoch: 2/2, step 5102/7134 completed (loss: 0.031684063374996185, acc: 0.9904502034187317)
[2025-02-13 04:14:15,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:15,385][root][INFO] - Training Epoch: 2/2, step 5103/7134 completed (loss: 0.02908528409898281, acc: 0.9908362030982971)
[2025-02-13 04:14:15,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:15,850][root][INFO] - Training Epoch: 2/2, step 5104/7134 completed (loss: 0.019620567560195923, acc: 0.9955703020095825)
[2025-02-13 04:14:15,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:16,240][root][INFO] - Training Epoch: 2/2, step 5105/7134 completed (loss: 0.02657787688076496, acc: 0.992548406124115)
[2025-02-13 04:14:16,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:16,670][root][INFO] - Training Epoch: 2/2, step 5106/7134 completed (loss: 0.01767132803797722, acc: 0.9925925731658936)
[2025-02-13 04:14:16,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:17,090][root][INFO] - Training Epoch: 2/2, step 5107/7134 completed (loss: 0.08082566410303116, acc: 0.9841521382331848)
[2025-02-13 04:14:17,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:17,491][root][INFO] - Training Epoch: 2/2, step 5108/7134 completed (loss: 0.044164497405290604, acc: 0.9860627055168152)
[2025-02-13 04:14:17,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:17,954][root][INFO] - Training Epoch: 2/2, step 5109/7134 completed (loss: 0.054530251771211624, acc: 0.9867256879806519)
[2025-02-13 04:14:18,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:18,383][root][INFO] - Training Epoch: 2/2, step 5110/7134 completed (loss: 0.05645119026303291, acc: 0.9846153855323792)
[2025-02-13 04:14:18,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:18,795][root][INFO] - Training Epoch: 2/2, step 5111/7134 completed (loss: 0.021907638758420944, acc: 0.9931507110595703)
[2025-02-13 04:14:18,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:19,234][root][INFO] - Training Epoch: 2/2, step 5112/7134 completed (loss: 0.05947379022836685, acc: 0.9887920022010803)
[2025-02-13 04:14:19,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:19,623][root][INFO] - Training Epoch: 2/2, step 5113/7134 completed (loss: 0.051306404173374176, acc: 0.9850746393203735)
[2025-02-13 04:14:19,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:20,081][root][INFO] - Training Epoch: 2/2, step 5114/7134 completed (loss: 0.02542678266763687, acc: 0.9944367408752441)
[2025-02-13 04:14:20,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:20,503][root][INFO] - Training Epoch: 2/2, step 5115/7134 completed (loss: 0.015894263982772827, acc: 0.9948387145996094)
[2025-02-13 04:14:20,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:20,956][root][INFO] - Training Epoch: 2/2, step 5116/7134 completed (loss: 0.012002517469227314, acc: 0.9961038827896118)
[2025-02-13 04:14:21,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:21,371][root][INFO] - Training Epoch: 2/2, step 5117/7134 completed (loss: 0.022648800164461136, acc: 0.9930264949798584)
[2025-02-13 04:14:21,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:21,752][root][INFO] - Training Epoch: 2/2, step 5118/7134 completed (loss: 0.04804941266775131, acc: 0.9908952713012695)
[2025-02-13 04:14:21,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:22,179][root][INFO] - Training Epoch: 2/2, step 5119/7134 completed (loss: 0.02679038979113102, acc: 0.993122398853302)
[2025-02-13 04:14:22,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:22,589][root][INFO] - Training Epoch: 2/2, step 5120/7134 completed (loss: 0.06312531232833862, acc: 0.985602080821991)
[2025-02-13 04:14:22,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:23,006][root][INFO] - Training Epoch: 2/2, step 5121/7134 completed (loss: 0.021471792832016945, acc: 0.994452178478241)
[2025-02-13 04:14:23,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:23,421][root][INFO] - Training Epoch: 2/2, step 5122/7134 completed (loss: 0.039131999015808105, acc: 0.9859353303909302)
[2025-02-13 04:14:23,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:23,864][root][INFO] - Training Epoch: 2/2, step 5123/7134 completed (loss: 0.026149064302444458, acc: 0.9927623867988586)
[2025-02-13 04:14:23,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:24,297][root][INFO] - Training Epoch: 2/2, step 5124/7134 completed (loss: 0.03591648489236832, acc: 0.9888198971748352)
[2025-02-13 04:14:24,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:24,720][root][INFO] - Training Epoch: 2/2, step 5125/7134 completed (loss: 0.02802959643304348, acc: 0.9968404173851013)
[2025-02-13 04:14:24,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:25,155][root][INFO] - Training Epoch: 2/2, step 5126/7134 completed (loss: 0.017092429101467133, acc: 0.9948717951774597)
[2025-02-13 04:14:25,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:25,567][root][INFO] - Training Epoch: 2/2, step 5127/7134 completed (loss: 0.026505881920456886, acc: 0.9930070042610168)
[2025-02-13 04:14:25,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:26,004][root][INFO] - Training Epoch: 2/2, step 5128/7134 completed (loss: 0.01299720536917448, acc: 0.995604395866394)
[2025-02-13 04:14:26,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:26,431][root][INFO] - Training Epoch: 2/2, step 5129/7134 completed (loss: 0.044753264635801315, acc: 0.9823529124259949)
[2025-02-13 04:14:26,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:26,845][root][INFO] - Training Epoch: 2/2, step 5130/7134 completed (loss: 0.013566629029810429, acc: 0.9937597513198853)
[2025-02-13 04:14:26,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:27,286][root][INFO] - Training Epoch: 2/2, step 5131/7134 completed (loss: 0.031089095398783684, acc: 0.9918830990791321)
[2025-02-13 04:14:27,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:27,667][root][INFO] - Training Epoch: 2/2, step 5132/7134 completed (loss: 0.009942321106791496, acc: 0.998019814491272)
[2025-02-13 04:14:27,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:28,086][root][INFO] - Training Epoch: 2/2, step 5133/7134 completed (loss: 0.04581481218338013, acc: 0.9829192757606506)
[2025-02-13 04:14:28,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:28,520][root][INFO] - Training Epoch: 2/2, step 5134/7134 completed (loss: 0.015026348643004894, acc: 0.998123824596405)
[2025-02-13 04:14:28,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:28,896][root][INFO] - Training Epoch: 2/2, step 5135/7134 completed (loss: 0.04967036098241806, acc: 0.991428554058075)
[2025-02-13 04:14:29,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:29,330][root][INFO] - Training Epoch: 2/2, step 5136/7134 completed (loss: 0.02823237143456936, acc: 0.9890710115432739)
[2025-02-13 04:14:29,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:29,759][root][INFO] - Training Epoch: 2/2, step 5137/7134 completed (loss: 0.015174368396401405, acc: 0.9963302612304688)
[2025-02-13 04:14:29,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:30,204][root][INFO] - Training Epoch: 2/2, step 5138/7134 completed (loss: 0.014618463814258575, acc: 0.9962499737739563)
[2025-02-13 04:14:30,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:30,595][root][INFO] - Training Epoch: 2/2, step 5139/7134 completed (loss: 0.013195428997278214, acc: 0.9971387982368469)
[2025-02-13 04:14:30,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:31,041][root][INFO] - Training Epoch: 2/2, step 5140/7134 completed (loss: 0.022826118394732475, acc: 0.9946977496147156)
[2025-02-13 04:14:31,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:31,295][root][INFO] - Training Epoch: 2/2, step 5141/7134 completed (loss: 0.024523738771677017, acc: 0.9961389899253845)
[2025-02-13 04:14:31,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:31,734][root][INFO] - Training Epoch: 2/2, step 5142/7134 completed (loss: 0.012093923054635525, acc: 0.9952940940856934)
[2025-02-13 04:14:31,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:32,181][root][INFO] - Training Epoch: 2/2, step 5143/7134 completed (loss: 0.01871771179139614, acc: 0.9923858046531677)
[2025-02-13 04:14:32,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:32,613][root][INFO] - Training Epoch: 2/2, step 5144/7134 completed (loss: 0.016547303646802902, acc: 0.9941657185554504)
[2025-02-13 04:14:32,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:33,074][root][INFO] - Training Epoch: 2/2, step 5145/7134 completed (loss: 0.009801900945603848, acc: 0.997952938079834)
[2025-02-13 04:14:33,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:33,529][root][INFO] - Training Epoch: 2/2, step 5146/7134 completed (loss: 0.04681970924139023, acc: 0.9843924045562744)
[2025-02-13 04:14:33,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:33,916][root][INFO] - Training Epoch: 2/2, step 5147/7134 completed (loss: 0.019132668152451515, acc: 0.9918032884597778)
[2025-02-13 04:14:34,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:34,325][root][INFO] - Training Epoch: 2/2, step 5148/7134 completed (loss: 0.06327493488788605, acc: 0.9853528738021851)
[2025-02-13 04:14:34,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:34,791][root][INFO] - Training Epoch: 2/2, step 5149/7134 completed (loss: 0.028235694393515587, acc: 0.9900497794151306)
[2025-02-13 04:14:34,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:35,216][root][INFO] - Training Epoch: 2/2, step 5150/7134 completed (loss: 0.049624308943748474, acc: 0.9830508232116699)
[2025-02-13 04:14:35,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:35,636][root][INFO] - Training Epoch: 2/2, step 5151/7134 completed (loss: 0.038777489215135574, acc: 0.9888613820075989)
[2025-02-13 04:14:35,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:36,073][root][INFO] - Training Epoch: 2/2, step 5152/7134 completed (loss: 0.02410568855702877, acc: 0.9906166195869446)
[2025-02-13 04:14:36,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:36,515][root][INFO] - Training Epoch: 2/2, step 5153/7134 completed (loss: 0.013794886879622936, acc: 0.9958847761154175)
[2025-02-13 04:14:36,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:36,982][root][INFO] - Training Epoch: 2/2, step 5154/7134 completed (loss: 0.018651682883501053, acc: 0.9931740760803223)
[2025-02-13 04:14:37,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:37,412][root][INFO] - Training Epoch: 2/2, step 5155/7134 completed (loss: 0.03963969275355339, acc: 0.9954819083213806)
[2025-02-13 04:14:37,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:37,895][root][INFO] - Training Epoch: 2/2, step 5156/7134 completed (loss: 0.0184677354991436, acc: 0.9940416812896729)
[2025-02-13 04:14:38,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:38,334][root][INFO] - Training Epoch: 2/2, step 5157/7134 completed (loss: 0.04398873448371887, acc: 0.9889975786209106)
[2025-02-13 04:14:38,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:38,774][root][INFO] - Training Epoch: 2/2, step 5158/7134 completed (loss: 0.07069169729948044, acc: 0.9792284965515137)
[2025-02-13 04:14:38,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:39,200][root][INFO] - Training Epoch: 2/2, step 5159/7134 completed (loss: 0.01915106177330017, acc: 0.9934980273246765)
[2025-02-13 04:14:39,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:39,619][root][INFO] - Training Epoch: 2/2, step 5160/7134 completed (loss: 0.048731960356235504, acc: 0.9855907559394836)
[2025-02-13 04:14:39,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:40,044][root][INFO] - Training Epoch: 2/2, step 5161/7134 completed (loss: 0.06705785542726517, acc: 0.9790209531784058)
[2025-02-13 04:14:40,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:40,448][root][INFO] - Training Epoch: 2/2, step 5162/7134 completed (loss: 0.01619727350771427, acc: 0.9935170412063599)
[2025-02-13 04:14:40,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:40,850][root][INFO] - Training Epoch: 2/2, step 5163/7134 completed (loss: 0.01772436499595642, acc: 0.9943342804908752)
[2025-02-13 04:14:40,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:41,258][root][INFO] - Training Epoch: 2/2, step 5164/7134 completed (loss: 0.01568908989429474, acc: 0.9966386556625366)
[2025-02-13 04:14:41,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:41,714][root][INFO] - Training Epoch: 2/2, step 5165/7134 completed (loss: 0.03182009607553482, acc: 0.9924699068069458)
[2025-02-13 04:14:41,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:42,130][root][INFO] - Training Epoch: 2/2, step 5166/7134 completed (loss: 0.007058838848024607, acc: 1.0)
[2025-02-13 04:14:42,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:42,537][root][INFO] - Training Epoch: 2/2, step 5167/7134 completed (loss: 0.04675309732556343, acc: 0.9775640964508057)
[2025-02-13 04:14:42,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:42,943][root][INFO] - Training Epoch: 2/2, step 5168/7134 completed (loss: 0.03216642141342163, acc: 0.9875776171684265)
[2025-02-13 04:14:43,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:43,370][root][INFO] - Training Epoch: 2/2, step 5169/7134 completed (loss: 0.0420260913670063, acc: 0.9844236969947815)
[2025-02-13 04:14:43,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:43,803][root][INFO] - Training Epoch: 2/2, step 5170/7134 completed (loss: 0.03208136558532715, acc: 0.9894039630889893)
[2025-02-13 04:14:43,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:44,146][root][INFO] - Training Epoch: 2/2, step 5171/7134 completed (loss: 0.01811942271888256, acc: 0.997863233089447)
[2025-02-13 04:14:44,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:44,566][root][INFO] - Training Epoch: 2/2, step 5172/7134 completed (loss: 0.065533347427845, acc: 0.9829642176628113)
[2025-02-13 04:14:44,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:44,959][root][INFO] - Training Epoch: 2/2, step 5173/7134 completed (loss: 0.04660605266690254, acc: 0.9858267903327942)
[2025-02-13 04:14:45,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:45,397][root][INFO] - Training Epoch: 2/2, step 5174/7134 completed (loss: 0.015909038484096527, acc: 0.9974457025527954)
[2025-02-13 04:14:45,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:45,838][root][INFO] - Training Epoch: 2/2, step 5175/7134 completed (loss: 0.010616699233651161, acc: 0.9958563446998596)
[2025-02-13 04:14:45,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:46,278][root][INFO] - Training Epoch: 2/2, step 5176/7134 completed (loss: 0.011763762682676315, acc: 0.9969651103019714)
[2025-02-13 04:14:46,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:46,716][root][INFO] - Training Epoch: 2/2, step 5177/7134 completed (loss: 0.04896879568696022, acc: 0.9895651936531067)
[2025-02-13 04:14:46,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:47,127][root][INFO] - Training Epoch: 2/2, step 5178/7134 completed (loss: 0.08666262775659561, acc: 0.9756097793579102)
[2025-02-13 04:14:47,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:47,542][root][INFO] - Training Epoch: 2/2, step 5179/7134 completed (loss: 0.020916901528835297, acc: 0.9940828680992126)
[2025-02-13 04:14:47,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:47,916][root][INFO] - Training Epoch: 2/2, step 5180/7134 completed (loss: 0.05198952928185463, acc: 0.9831144213676453)
[2025-02-13 04:14:48,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:48,246][root][INFO] - Training Epoch: 2/2, step 5181/7134 completed (loss: 0.05769214779138565, acc: 0.9867021441459656)
[2025-02-13 04:14:48,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:48,629][root][INFO] - Training Epoch: 2/2, step 5182/7134 completed (loss: 0.12980833649635315, acc: 0.9666666388511658)
[2025-02-13 04:14:48,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:49,043][root][INFO] - Training Epoch: 2/2, step 5183/7134 completed (loss: 0.046366289258003235, acc: 0.9900990128517151)
[2025-02-13 04:14:49,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:49,429][root][INFO] - Training Epoch: 2/2, step 5184/7134 completed (loss: 0.02206259034574032, acc: 0.9926650524139404)
[2025-02-13 04:14:49,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:49,835][root][INFO] - Training Epoch: 2/2, step 5185/7134 completed (loss: 0.04263124614953995, acc: 0.9879518151283264)
[2025-02-13 04:14:49,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:50,242][root][INFO] - Training Epoch: 2/2, step 5186/7134 completed (loss: 0.038451455533504486, acc: 0.9857434034347534)
[2025-02-13 04:14:50,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:50,645][root][INFO] - Training Epoch: 2/2, step 5187/7134 completed (loss: 0.020305082201957703, acc: 0.9942857027053833)
[2025-02-13 04:14:50,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:51,085][root][INFO] - Training Epoch: 2/2, step 5188/7134 completed (loss: 0.10339827835559845, acc: 0.9727272987365723)
[2025-02-13 04:14:51,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:51,375][root][INFO] - Training Epoch: 2/2, step 5189/7134 completed (loss: 0.06035899743437767, acc: 0.9872449040412903)
[2025-02-13 04:14:51,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:51,717][root][INFO] - Training Epoch: 2/2, step 5190/7134 completed (loss: 0.03823874890804291, acc: 0.9904988408088684)
[2025-02-13 04:14:51,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:52,171][root][INFO] - Training Epoch: 2/2, step 5191/7134 completed (loss: 0.0999208316206932, acc: 0.9707174301147461)
[2025-02-13 04:14:52,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:52,610][root][INFO] - Training Epoch: 2/2, step 5192/7134 completed (loss: 0.04529563710093498, acc: 0.9916765689849854)
[2025-02-13 04:14:52,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:53,035][root][INFO] - Training Epoch: 2/2, step 5193/7134 completed (loss: 0.015249490737915039, acc: 0.9960629940032959)
[2025-02-13 04:14:53,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:53,445][root][INFO] - Training Epoch: 2/2, step 5194/7134 completed (loss: 0.056378960609436035, acc: 0.9821782112121582)
[2025-02-13 04:14:53,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:53,780][root][INFO] - Training Epoch: 2/2, step 5195/7134 completed (loss: 0.04564955085515976, acc: 0.9890350699424744)
[2025-02-13 04:14:53,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:54,122][root][INFO] - Training Epoch: 2/2, step 5196/7134 completed (loss: 0.0381154902279377, acc: 0.9880383014678955)
[2025-02-13 04:14:54,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:54,520][root][INFO] - Training Epoch: 2/2, step 5197/7134 completed (loss: 0.02790193073451519, acc: 0.991769552230835)
[2025-02-13 04:14:54,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:54,900][root][INFO] - Training Epoch: 2/2, step 5198/7134 completed (loss: 0.022734809666872025, acc: 0.9916840195655823)
[2025-02-13 04:14:55,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:55,365][root][INFO] - Training Epoch: 2/2, step 5199/7134 completed (loss: 0.07250957936048508, acc: 0.9825581312179565)
[2025-02-13 04:14:55,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:55,778][root][INFO] - Training Epoch: 2/2, step 5200/7134 completed (loss: 0.020806143060326576, acc: 0.9921414256095886)
[2025-02-13 04:14:55,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:56,223][root][INFO] - Training Epoch: 2/2, step 5201/7134 completed (loss: 0.046844542026519775, acc: 0.9915561079978943)
[2025-02-13 04:14:56,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:56,633][root][INFO] - Training Epoch: 2/2, step 5202/7134 completed (loss: 0.061877306550741196, acc: 0.9862637519836426)
[2025-02-13 04:14:56,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:57,068][root][INFO] - Training Epoch: 2/2, step 5203/7134 completed (loss: 0.048695776611566544, acc: 0.9863945841789246)
[2025-02-13 04:14:57,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:57,459][root][INFO] - Training Epoch: 2/2, step 5204/7134 completed (loss: 0.018510673195123672, acc: 0.9938144087791443)
[2025-02-13 04:14:57,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:57,714][root][INFO] - Training Epoch: 2/2, step 5205/7134 completed (loss: 0.030406594276428223, acc: 0.9887640476226807)
[2025-02-13 04:14:57,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:58,160][root][INFO] - Training Epoch: 2/2, step 5206/7134 completed (loss: 0.043155573308467865, acc: 0.9935794472694397)
[2025-02-13 04:14:58,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:58,541][root][INFO] - Training Epoch: 2/2, step 5207/7134 completed (loss: 0.057769227772951126, acc: 0.9839034080505371)
[2025-02-13 04:14:58,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:59,030][root][INFO] - Training Epoch: 2/2, step 5208/7134 completed (loss: 0.019655508920550346, acc: 0.9947229623794556)
[2025-02-13 04:14:59,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:59,437][root][INFO] - Training Epoch: 2/2, step 5209/7134 completed (loss: 0.027775725349783897, acc: 0.9879518151283264)
[2025-02-13 04:14:59,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:59,902][root][INFO] - Training Epoch: 2/2, step 5210/7134 completed (loss: 0.02448081225156784, acc: 0.9933244585990906)
[2025-02-13 04:15:00,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:00,342][root][INFO] - Training Epoch: 2/2, step 5211/7134 completed (loss: 0.03414009511470795, acc: 0.9861111044883728)
[2025-02-13 04:15:00,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:00,681][root][INFO] - Training Epoch: 2/2, step 5212/7134 completed (loss: 0.05995487421751022, acc: 0.979522168636322)
[2025-02-13 04:15:00,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:01,092][root][INFO] - Training Epoch: 2/2, step 5213/7134 completed (loss: 0.050905365496873856, acc: 0.9910072088241577)
[2025-02-13 04:15:01,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:01,483][root][INFO] - Training Epoch: 2/2, step 5214/7134 completed (loss: 0.08665426820516586, acc: 0.9721029996871948)
[2025-02-13 04:15:01,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:01,944][root][INFO] - Training Epoch: 2/2, step 5215/7134 completed (loss: 0.05911344662308693, acc: 0.9795657992362976)
[2025-02-13 04:15:02,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:02,196][root][INFO] - Training Epoch: 2/2, step 5216/7134 completed (loss: 0.04553651064634323, acc: 0.9847908616065979)
[2025-02-13 04:15:02,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:02,499][root][INFO] - Training Epoch: 2/2, step 5217/7134 completed (loss: 0.05504997819662094, acc: 0.9817073345184326)
[2025-02-13 04:15:02,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:02,862][root][INFO] - Training Epoch: 2/2, step 5218/7134 completed (loss: 0.02660512365400791, acc: 0.9955056309700012)
[2025-02-13 04:15:02,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:03,242][root][INFO] - Training Epoch: 2/2, step 5219/7134 completed (loss: 0.05622771382331848, acc: 0.9878934621810913)
[2025-02-13 04:15:03,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:03,569][root][INFO] - Training Epoch: 2/2, step 5220/7134 completed (loss: 0.0554678849875927, acc: 0.9821109175682068)
[2025-02-13 04:15:03,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:03,989][root][INFO] - Training Epoch: 2/2, step 5221/7134 completed (loss: 0.05464516580104828, acc: 0.9890965819358826)
[2025-02-13 04:15:04,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:04,252][root][INFO] - Training Epoch: 2/2, step 5222/7134 completed (loss: 0.023557744920253754, acc: 0.9937694668769836)
[2025-02-13 04:15:04,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:04,627][root][INFO] - Training Epoch: 2/2, step 5223/7134 completed (loss: 0.025286400690674782, acc: 0.9907833933830261)
[2025-02-13 04:15:04,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:05,026][root][INFO] - Training Epoch: 2/2, step 5224/7134 completed (loss: 0.05789889767765999, acc: 0.9841827750205994)
[2025-02-13 04:15:05,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:05,458][root][INFO] - Training Epoch: 2/2, step 5225/7134 completed (loss: 0.060751792043447495, acc: 0.9854604005813599)
[2025-02-13 04:15:05,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:05,882][root][INFO] - Training Epoch: 2/2, step 5226/7134 completed (loss: 0.05237477645277977, acc: 0.9868203997612)
[2025-02-13 04:15:06,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:06,331][root][INFO] - Training Epoch: 2/2, step 5227/7134 completed (loss: 0.040379393845796585, acc: 0.9961038827896118)
[2025-02-13 04:15:06,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:06,734][root][INFO] - Training Epoch: 2/2, step 5228/7134 completed (loss: 0.054667674005031586, acc: 0.9898256063461304)
[2025-02-13 04:15:06,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:07,157][root][INFO] - Training Epoch: 2/2, step 5229/7134 completed (loss: 0.030869564041495323, acc: 0.98959881067276)
[2025-02-13 04:15:07,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:07,583][root][INFO] - Training Epoch: 2/2, step 5230/7134 completed (loss: 0.023553516715765, acc: 0.993261456489563)
[2025-02-13 04:15:07,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:08,029][root][INFO] - Training Epoch: 2/2, step 5231/7134 completed (loss: 0.039535362273454666, acc: 0.9936708807945251)
[2025-02-13 04:15:08,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:08,451][root][INFO] - Training Epoch: 2/2, step 5232/7134 completed (loss: 0.020173154771327972, acc: 0.9910314083099365)
[2025-02-13 04:15:08,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:08,917][root][INFO] - Training Epoch: 2/2, step 5233/7134 completed (loss: 0.024239428341388702, acc: 0.994369387626648)
[2025-02-13 04:15:09,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:09,369][root][INFO] - Training Epoch: 2/2, step 5234/7134 completed (loss: 0.028771966695785522, acc: 0.9963280558586121)
[2025-02-13 04:15:09,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:09,768][root][INFO] - Training Epoch: 2/2, step 5235/7134 completed (loss: 0.015316533856093884, acc: 0.9949579834938049)
[2025-02-13 04:15:09,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:10,175][root][INFO] - Training Epoch: 2/2, step 5236/7134 completed (loss: 0.007959284819662571, acc: 0.9982638955116272)
[2025-02-13 04:15:10,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:10,578][root][INFO] - Training Epoch: 2/2, step 5237/7134 completed (loss: 0.007438951171934605, acc: 0.9964157938957214)
[2025-02-13 04:15:10,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:11,002][root][INFO] - Training Epoch: 2/2, step 5238/7134 completed (loss: 0.013517050072550774, acc: 0.9964788556098938)
[2025-02-13 04:15:11,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:11,353][root][INFO] - Training Epoch: 2/2, step 5239/7134 completed (loss: 0.013506353832781315, acc: 0.9942085146903992)
[2025-02-13 04:15:11,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:11,771][root][INFO] - Training Epoch: 2/2, step 5240/7134 completed (loss: 0.02098996378481388, acc: 0.9958506226539612)
[2025-02-13 04:15:11,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:12,190][root][INFO] - Training Epoch: 2/2, step 5241/7134 completed (loss: 0.03706939518451691, acc: 0.9890282154083252)
[2025-02-13 04:15:12,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:12,641][root][INFO] - Training Epoch: 2/2, step 5242/7134 completed (loss: 0.010801928117871284, acc: 0.9966942071914673)
[2025-02-13 04:15:12,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:13,045][root][INFO] - Training Epoch: 2/2, step 5243/7134 completed (loss: 0.02496599778532982, acc: 0.9948275685310364)
[2025-02-13 04:15:13,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:13,438][root][INFO] - Training Epoch: 2/2, step 5244/7134 completed (loss: 0.016139870509505272, acc: 0.9931034445762634)
[2025-02-13 04:15:13,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:13,923][root][INFO] - Training Epoch: 2/2, step 5245/7134 completed (loss: 0.013064991682767868, acc: 0.9965096116065979)
[2025-02-13 04:15:14,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:14,328][root][INFO] - Training Epoch: 2/2, step 5246/7134 completed (loss: 0.01854376681149006, acc: 0.9907975196838379)
[2025-02-13 04:15:14,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:14,732][root][INFO] - Training Epoch: 2/2, step 5247/7134 completed (loss: 0.01466423086822033, acc: 0.9944444298744202)
[2025-02-13 04:15:14,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:15,138][root][INFO] - Training Epoch: 2/2, step 5248/7134 completed (loss: 0.05038265883922577, acc: 0.9868913888931274)
[2025-02-13 04:15:15,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:15,549][root][INFO] - Training Epoch: 2/2, step 5249/7134 completed (loss: 0.006516703404486179, acc: 0.99842768907547)
[2025-02-13 04:15:15,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:15,995][root][INFO] - Training Epoch: 2/2, step 5250/7134 completed (loss: 0.025418737903237343, acc: 0.9936948418617249)
[2025-02-13 04:15:16,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:16,432][root][INFO] - Training Epoch: 2/2, step 5251/7134 completed (loss: 0.02300451323390007, acc: 0.9915151596069336)
[2025-02-13 04:15:16,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:16,815][root][INFO] - Training Epoch: 2/2, step 5252/7134 completed (loss: 0.0458163321018219, acc: 0.9836448431015015)
[2025-02-13 04:15:16,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:17,240][root][INFO] - Training Epoch: 2/2, step 5253/7134 completed (loss: 0.014813925139605999, acc: 0.9934640526771545)
[2025-02-13 04:15:17,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:17,674][root][INFO] - Training Epoch: 2/2, step 5254/7134 completed (loss: 0.05086136609315872, acc: 0.9864176511764526)
[2025-02-13 04:15:17,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:18,112][root][INFO] - Training Epoch: 2/2, step 5255/7134 completed (loss: 0.03393709287047386, acc: 0.9890410900115967)
[2025-02-13 04:15:18,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:18,552][root][INFO] - Training Epoch: 2/2, step 5256/7134 completed (loss: 0.04863521456718445, acc: 0.9860896468162537)
[2025-02-13 04:15:18,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:18,975][root][INFO] - Training Epoch: 2/2, step 5257/7134 completed (loss: 0.05367876961827278, acc: 0.9847792983055115)
[2025-02-13 04:15:19,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:19,414][root][INFO] - Training Epoch: 2/2, step 5258/7134 completed (loss: 0.022606998682022095, acc: 0.9872449040412903)
[2025-02-13 04:15:19,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:19,830][root][INFO] - Training Epoch: 2/2, step 5259/7134 completed (loss: 0.03832351043820381, acc: 0.9936708807945251)
[2025-02-13 04:15:19,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:20,267][root][INFO] - Training Epoch: 2/2, step 5260/7134 completed (loss: 0.02526293508708477, acc: 0.992682933807373)
[2025-02-13 04:15:20,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:20,672][root][INFO] - Training Epoch: 2/2, step 5261/7134 completed (loss: 0.0624585822224617, acc: 0.9884615540504456)
[2025-02-13 04:15:20,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:21,126][root][INFO] - Training Epoch: 2/2, step 5262/7134 completed (loss: 0.015452336519956589, acc: 0.9931034445762634)
[2025-02-13 04:15:21,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:21,530][root][INFO] - Training Epoch: 2/2, step 5263/7134 completed (loss: 0.040389854460954666, acc: 0.9859648942947388)
[2025-02-13 04:15:21,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:21,989][root][INFO] - Training Epoch: 2/2, step 5264/7134 completed (loss: 0.02183833159506321, acc: 0.9914945363998413)
[2025-02-13 04:15:22,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:22,430][root][INFO] - Training Epoch: 2/2, step 5265/7134 completed (loss: 0.030505238100886345, acc: 0.9936467409133911)
[2025-02-13 04:15:22,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:22,843][root][INFO] - Training Epoch: 2/2, step 5266/7134 completed (loss: 0.020349707454442978, acc: 0.9945205450057983)
[2025-02-13 04:15:22,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:23,234][root][INFO] - Training Epoch: 2/2, step 5267/7134 completed (loss: 0.07238908857107162, acc: 0.9797822833061218)
[2025-02-13 04:15:23,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:23,638][root][INFO] - Training Epoch: 2/2, step 5268/7134 completed (loss: 0.03913343325257301, acc: 0.9827255010604858)
[2025-02-13 04:15:23,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:24,050][root][INFO] - Training Epoch: 2/2, step 5269/7134 completed (loss: 0.06380271911621094, acc: 0.9767441749572754)
[2025-02-13 04:15:24,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:24,502][root][INFO] - Training Epoch: 2/2, step 5270/7134 completed (loss: 0.023649735376238823, acc: 0.9919785857200623)
[2025-02-13 04:15:24,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:24,932][root][INFO] - Training Epoch: 2/2, step 5271/7134 completed (loss: 0.05005139857530594, acc: 0.9823633432388306)
[2025-02-13 04:15:25,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:25,347][root][INFO] - Training Epoch: 2/2, step 5272/7134 completed (loss: 0.018339872360229492, acc: 0.9941349029541016)
[2025-02-13 04:15:25,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:25,776][root][INFO] - Training Epoch: 2/2, step 5273/7134 completed (loss: 0.0196759682148695, acc: 0.9949066042900085)
[2025-02-13 04:15:25,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:26,188][root][INFO] - Training Epoch: 2/2, step 5274/7134 completed (loss: 0.036876410245895386, acc: 0.9880596995353699)
[2025-02-13 04:15:26,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:26,615][root][INFO] - Training Epoch: 2/2, step 5275/7134 completed (loss: 0.015456072054803371, acc: 0.9926874041557312)
[2025-02-13 04:15:26,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:27,016][root][INFO] - Training Epoch: 2/2, step 5276/7134 completed (loss: 0.03385041281580925, acc: 0.9865471124649048)
[2025-02-13 04:15:27,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:27,407][root][INFO] - Training Epoch: 2/2, step 5277/7134 completed (loss: 0.02246139571070671, acc: 0.9927954077720642)
[2025-02-13 04:15:27,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:27,823][root][INFO] - Training Epoch: 2/2, step 5278/7134 completed (loss: 0.032809413969516754, acc: 0.9908257126808167)
[2025-02-13 04:15:27,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:28,241][root][INFO] - Training Epoch: 2/2, step 5279/7134 completed (loss: 0.008806316182017326, acc: 0.9980545043945312)
[2025-02-13 04:15:28,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:28,652][root][INFO] - Training Epoch: 2/2, step 5280/7134 completed (loss: 0.0299459807574749, acc: 0.9933664798736572)
[2025-02-13 04:15:28,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:29,062][root][INFO] - Training Epoch: 2/2, step 5281/7134 completed (loss: 0.015387049876153469, acc: 0.9966722130775452)
[2025-02-13 04:15:29,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:29,456][root][INFO] - Training Epoch: 2/2, step 5282/7134 completed (loss: 0.01761954464018345, acc: 0.9942938685417175)
[2025-02-13 04:15:29,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:29,868][root][INFO] - Training Epoch: 2/2, step 5283/7134 completed (loss: 0.01772579364478588, acc: 0.9931600689888)
[2025-02-13 04:15:30,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:30,280][root][INFO] - Training Epoch: 2/2, step 5284/7134 completed (loss: 0.031491756439208984, acc: 0.9944674968719482)
[2025-02-13 04:15:30,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:30,731][root][INFO] - Training Epoch: 2/2, step 5285/7134 completed (loss: 0.03185702860355377, acc: 0.9893491268157959)
[2025-02-13 04:15:30,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:31,173][root][INFO] - Training Epoch: 2/2, step 5286/7134 completed (loss: 0.032216593623161316, acc: 0.9941176176071167)
[2025-02-13 04:15:31,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:31,610][root][INFO] - Training Epoch: 2/2, step 5287/7134 completed (loss: 0.01910797692835331, acc: 0.9932249188423157)
[2025-02-13 04:15:31,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:32,020][root][INFO] - Training Epoch: 2/2, step 5288/7134 completed (loss: 0.010678436607122421, acc: 0.995398759841919)
[2025-02-13 04:15:32,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:32,461][root][INFO] - Training Epoch: 2/2, step 5289/7134 completed (loss: 0.034265272319316864, acc: 0.9929178357124329)
[2025-02-13 04:15:32,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:32,905][root][INFO] - Training Epoch: 2/2, step 5290/7134 completed (loss: 0.02693057991564274, acc: 0.9911392331123352)
[2025-02-13 04:15:33,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:33,352][root][INFO] - Training Epoch: 2/2, step 5291/7134 completed (loss: 0.02375911921262741, acc: 0.9951515197753906)
[2025-02-13 04:15:33,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:33,782][root][INFO] - Training Epoch: 2/2, step 5292/7134 completed (loss: 0.010474901646375656, acc: 0.9974554777145386)
[2025-02-13 04:15:33,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:34,215][root][INFO] - Training Epoch: 2/2, step 5293/7134 completed (loss: 0.023554805666208267, acc: 0.9927007555961609)
[2025-02-13 04:15:34,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:34,653][root][INFO] - Training Epoch: 2/2, step 5294/7134 completed (loss: 0.023677023127675056, acc: 0.9944211840629578)
[2025-02-13 04:15:34,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:35,044][root][INFO] - Training Epoch: 2/2, step 5295/7134 completed (loss: 0.010105820372700691, acc: 0.9963235259056091)
[2025-02-13 04:15:35,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:35,465][root][INFO] - Training Epoch: 2/2, step 5296/7134 completed (loss: 0.013468174263834953, acc: 0.9958275556564331)
[2025-02-13 04:15:35,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:35,877][root][INFO] - Training Epoch: 2/2, step 5297/7134 completed (loss: 0.03223870322108269, acc: 0.9899425506591797)
[2025-02-13 04:15:36,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:36,355][root][INFO] - Training Epoch: 2/2, step 5298/7134 completed (loss: 0.019765548408031464, acc: 0.9965811967849731)
[2025-02-13 04:15:36,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:36,756][root][INFO] - Training Epoch: 2/2, step 5299/7134 completed (loss: 0.02175910770893097, acc: 0.9967897534370422)
[2025-02-13 04:15:36,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:37,171][root][INFO] - Training Epoch: 2/2, step 5300/7134 completed (loss: 0.01808248460292816, acc: 0.9926578402519226)
[2025-02-13 04:15:37,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:37,576][root][INFO] - Training Epoch: 2/2, step 5301/7134 completed (loss: 0.012803628109395504, acc: 0.9955947399139404)
[2025-02-13 04:15:37,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:37,986][root][INFO] - Training Epoch: 2/2, step 5302/7134 completed (loss: 0.06649579107761383, acc: 0.9848024249076843)
[2025-02-13 04:15:38,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:38,397][root][INFO] - Training Epoch: 2/2, step 5303/7134 completed (loss: 0.04489646852016449, acc: 0.991150438785553)
[2025-02-13 04:15:38,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:38,807][root][INFO] - Training Epoch: 2/2, step 5304/7134 completed (loss: 0.06754590570926666, acc: 0.9866888523101807)
[2025-02-13 04:15:38,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:39,210][root][INFO] - Training Epoch: 2/2, step 5305/7134 completed (loss: 0.05726487189531326, acc: 0.9839416146278381)
[2025-02-13 04:15:39,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:39,623][root][INFO] - Training Epoch: 2/2, step 5306/7134 completed (loss: 0.012274805456399918, acc: 0.9986110925674438)
[2025-02-13 04:15:39,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:40,034][root][INFO] - Training Epoch: 2/2, step 5307/7134 completed (loss: 0.05252633988857269, acc: 0.9845678806304932)
[2025-02-13 04:15:40,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:40,433][root][INFO] - Training Epoch: 2/2, step 5308/7134 completed (loss: 0.04623087868094444, acc: 0.9909365773200989)
[2025-02-13 04:15:40,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:40,837][root][INFO] - Training Epoch: 2/2, step 5309/7134 completed (loss: 0.055708229541778564, acc: 0.980322003364563)
[2025-02-13 04:15:40,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:41,267][root][INFO] - Training Epoch: 2/2, step 5310/7134 completed (loss: 0.046903517097234726, acc: 0.9832636117935181)
[2025-02-13 04:15:41,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:41,690][root][INFO] - Training Epoch: 2/2, step 5311/7134 completed (loss: 0.05711640417575836, acc: 0.9807383418083191)
[2025-02-13 04:15:41,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:42,076][root][INFO] - Training Epoch: 2/2, step 5312/7134 completed (loss: 0.020044485107064247, acc: 0.9943714737892151)
[2025-02-13 04:15:42,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:42,498][root][INFO] - Training Epoch: 2/2, step 5313/7134 completed (loss: 0.028007101267576218, acc: 0.9908925294876099)
[2025-02-13 04:15:42,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:42,901][root][INFO] - Training Epoch: 2/2, step 5314/7134 completed (loss: 0.030239271000027657, acc: 0.9912891983985901)
[2025-02-13 04:15:43,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:43,326][root][INFO] - Training Epoch: 2/2, step 5315/7134 completed (loss: 0.02707197330892086, acc: 0.9885433912277222)
[2025-02-13 04:15:43,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:43,730][root][INFO] - Training Epoch: 2/2, step 5316/7134 completed (loss: 0.04465284198522568, acc: 0.9899159669876099)
[2025-02-13 04:15:43,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:44,098][root][INFO] - Training Epoch: 2/2, step 5317/7134 completed (loss: 0.02664455957710743, acc: 0.9910394549369812)
[2025-02-13 04:15:44,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:44,487][root][INFO] - Training Epoch: 2/2, step 5318/7134 completed (loss: 0.02176244743168354, acc: 0.9913644194602966)
[2025-02-13 04:15:44,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:44,887][root][INFO] - Training Epoch: 2/2, step 5319/7134 completed (loss: 0.040158290416002274, acc: 0.9908758997917175)
[2025-02-13 04:15:45,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:45,308][root][INFO] - Training Epoch: 2/2, step 5320/7134 completed (loss: 0.036561671644449234, acc: 0.9953415989875793)
[2025-02-13 04:15:45,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:45,716][root][INFO] - Training Epoch: 2/2, step 5321/7134 completed (loss: 0.016174878925085068, acc: 0.9937984347343445)
[2025-02-13 04:15:45,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:46,130][root][INFO] - Training Epoch: 2/2, step 5322/7134 completed (loss: 0.016857990995049477, acc: 0.9924585223197937)
[2025-02-13 04:15:46,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:46,563][root][INFO] - Training Epoch: 2/2, step 5323/7134 completed (loss: 0.03415362164378166, acc: 0.989393949508667)
[2025-02-13 04:15:46,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:46,973][root][INFO] - Training Epoch: 2/2, step 5324/7134 completed (loss: 0.014206181280314922, acc: 0.9953632354736328)
[2025-02-13 04:15:47,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:47,394][root][INFO] - Training Epoch: 2/2, step 5325/7134 completed (loss: 0.027941767126321793, acc: 0.991482138633728)
[2025-02-13 04:15:47,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:47,812][root][INFO] - Training Epoch: 2/2, step 5326/7134 completed (loss: 0.020517103374004364, acc: 0.9935483932495117)
[2025-02-13 04:15:47,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:48,224][root][INFO] - Training Epoch: 2/2, step 5327/7134 completed (loss: 0.026886899024248123, acc: 0.9927431344985962)
[2025-02-13 04:15:48,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:48,621][root][INFO] - Training Epoch: 2/2, step 5328/7134 completed (loss: 0.014560114592313766, acc: 0.9944953918457031)
[2025-02-13 04:15:48,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:49,028][root][INFO] - Training Epoch: 2/2, step 5329/7134 completed (loss: 0.03186359256505966, acc: 0.990275502204895)
[2025-02-13 04:15:49,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:49,436][root][INFO] - Training Epoch: 2/2, step 5330/7134 completed (loss: 0.027327705174684525, acc: 0.9906250238418579)
[2025-02-13 04:15:49,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:49,836][root][INFO] - Training Epoch: 2/2, step 5331/7134 completed (loss: 0.030623972415924072, acc: 0.9929947257041931)
[2025-02-13 04:15:49,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:50,253][root][INFO] - Training Epoch: 2/2, step 5332/7134 completed (loss: 0.03462783247232437, acc: 0.995121955871582)
[2025-02-13 04:15:50,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:50,654][root][INFO] - Training Epoch: 2/2, step 5333/7134 completed (loss: 0.013519464060664177, acc: 0.9968847632408142)
[2025-02-13 04:15:50,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:51,057][root][INFO] - Training Epoch: 2/2, step 5334/7134 completed (loss: 0.04463714733719826, acc: 0.992343008518219)
[2025-02-13 04:15:51,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:51,453][root][INFO] - Training Epoch: 2/2, step 5335/7134 completed (loss: 0.03335264325141907, acc: 0.9936102032661438)
[2025-02-13 04:15:51,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:51,882][root][INFO] - Training Epoch: 2/2, step 5336/7134 completed (loss: 0.027462437748908997, acc: 0.989062488079071)
[2025-02-13 04:15:52,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:52,337][root][INFO] - Training Epoch: 2/2, step 5337/7134 completed (loss: 0.02104017697274685, acc: 0.9899777173995972)
[2025-02-13 04:15:52,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:52,827][root][INFO] - Training Epoch: 2/2, step 5338/7134 completed (loss: 0.015360691584646702, acc: 0.9965753555297852)
[2025-02-13 04:15:52,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:53,232][root][INFO] - Training Epoch: 2/2, step 5339/7134 completed (loss: 0.029704662039875984, acc: 0.9922279715538025)
[2025-02-13 04:15:53,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:53,673][root][INFO] - Training Epoch: 2/2, step 5340/7134 completed (loss: 0.04915476590394974, acc: 0.9809358716011047)
[2025-02-13 04:15:53,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:54,107][root][INFO] - Training Epoch: 2/2, step 5341/7134 completed (loss: 0.010287275537848473, acc: 0.996610164642334)
[2025-02-13 04:15:54,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:54,558][root][INFO] - Training Epoch: 2/2, step 5342/7134 completed (loss: 0.03755618631839752, acc: 0.9866844415664673)
[2025-02-13 04:15:54,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:54,996][root][INFO] - Training Epoch: 2/2, step 5343/7134 completed (loss: 0.04241025447845459, acc: 0.9868074059486389)
[2025-02-13 04:15:55,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:55,445][root][INFO] - Training Epoch: 2/2, step 5344/7134 completed (loss: 0.02979659102857113, acc: 0.9894737005233765)
[2025-02-13 04:15:55,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:55,878][root][INFO] - Training Epoch: 2/2, step 5345/7134 completed (loss: 0.025253966450691223, acc: 0.9908046126365662)
[2025-02-13 04:15:56,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:56,316][root][INFO] - Training Epoch: 2/2, step 5346/7134 completed (loss: 0.036469966173172, acc: 0.9874081611633301)
[2025-02-13 04:15:56,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:57,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:57,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:58,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:58,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:58,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:59,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:59,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:59,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:00,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:00,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:00,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:01,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:01,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:01,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:02,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:02,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:02,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:03,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:03,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:03,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:04,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:04,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:04,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:05,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:05,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:05,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:06,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:06,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:06,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:07,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:07,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:08,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:08,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:08,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:09,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:09,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:09,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:10,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:10,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:10,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:11,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:11,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:11,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:12,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:12,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:13,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:13,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:13,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:14,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:14,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:14,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:15,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:15,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:15,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:16,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:16,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:16,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:17,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:17,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:17,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:18,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:18,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:18,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:19,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:19,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:19,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:20,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:20,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:20,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:21,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:21,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:21,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:22,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:22,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:23,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:23,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:23,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:24,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:24,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:24,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:25,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:25,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:25,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:26,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:26,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:26,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:27,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:27,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:27,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:28,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:28,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:28,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:29,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:29,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:29,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:30,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:30,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:30,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:31,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:31,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:32,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:32,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:32,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:32,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:33,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:33,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:34,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:34,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:34,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:35,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:35,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:35,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:36,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:36,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:36,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:37,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:37,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:37,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:38,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:38,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:38,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:39,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:39,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:39,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:40,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:40,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:40,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:41,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:41,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:41,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:42,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:42,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:42,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:43,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:43,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:43,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:44,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:44,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:45,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:45,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:45,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:46,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:46,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:46,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:47,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:47,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:47,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:48,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:48,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:49,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:49,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:49,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:50,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:50,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:50,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:51,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:51,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:51,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:52,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:52,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:52,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:53,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:53,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:54,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:54,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:54,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:55,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:55,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:56,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:56,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:56,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:57,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:57,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:57,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:58,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:58,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:59,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:59,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:59,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:59,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:00,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:00,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:01,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:01,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:01,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:02,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:02,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:02,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:03,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:03,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:03,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:04,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:04,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:04,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:05,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:05,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:05,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:06,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:06,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:06,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:07,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:07,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:07,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:08,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:08,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:08,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:09,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:09,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:09,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:10,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:10,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:11,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:11,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:11,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:11,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:12,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:12,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:12,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:13,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:13,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:13,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:14,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:14,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:14,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:15,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:15,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:15,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:16,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:16,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:16,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:17,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:17,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:17,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:18,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:18,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:18,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:19,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:19,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:19,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:20,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:20,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:20,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:21,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:21,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:21,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:22,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:23,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:23,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:23,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:24,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:24,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:24,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:25,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:25,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:25,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:26,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:26,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:26,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:27,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:27,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:27,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:28,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:28,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:29,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:29,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:29,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:30,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:30,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:30,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:31,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:31,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:31,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:32,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:32,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:33,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:33,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:33,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:34,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:34,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:34,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:35,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:35,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:36,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:36,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:37,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:37,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:38,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:38,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:38,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:39,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:39,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:40,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:40,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:40,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:41,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:41,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:42,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:42,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:42,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:43,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:43,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:43,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:44,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:44,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:44,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:44,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:45,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:45,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:45,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:46,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:46,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:47,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:47,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:47,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:48,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:48,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:48,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:49,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:49,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:49,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:50,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:50,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:50,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:51,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:51,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:52,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:52,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:52,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:53,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:53,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:54,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:54,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:54,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:55,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:55,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:55,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:56,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:56,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:56,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:57,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:57,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:58,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:58,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:58,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:59,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:59,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:00,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:00,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:00,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:01,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:01,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:01,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:02,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:02,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:02,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:03,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:03,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:03,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:04,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:05,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:05,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:06,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:06,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:07,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:07,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:07,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:08,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:08,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:08,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:09,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:09,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:10,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:10,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:10,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:11,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:11,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:11,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:11,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:12,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:12,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:12,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:13,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:13,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:14,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:14,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:14,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:15,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:15,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:15,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:16,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:16,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:16,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:17,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:17,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:17,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:18,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:18,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:18,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:19,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:19,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:19,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:20,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:20,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:20,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:21,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:21,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:21,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:22,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:22,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:22,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:23,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:23,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:24,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:24,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:24,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:25,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:25,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:25,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:26,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:26,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:26,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:27,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:27,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:27,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:28,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:28,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:28,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:29,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:29,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:29,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:30,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:30,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:31,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:31,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:31,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:32,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:32,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:32,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:33,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:33,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:33,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:34,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:34,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:34,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:35,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:35,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:35,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:36,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:36,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:36,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:37,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:37,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:37,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:38,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:38,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:39,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:39,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:39,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:40,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:40,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:40,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:41,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:41,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:41,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:42,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:42,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:42,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:43,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:43,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:43,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:44,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:44,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:44,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:45,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:45,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:45,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:46,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:46,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:46,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:47,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:47,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:48,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:48,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:49,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:49,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:49,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:50,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:50,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:50,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:51,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:51,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:52,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:52,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:52,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:53,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:53,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:53,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:54,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:54,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:54,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:55,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:55,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:56,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:56,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:56,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:57,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:57,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:57,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:58,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:58,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:58,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:59,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:59,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:00,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:00,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:00,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:01,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:01,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:01,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:02,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:02,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:02,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:03,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:03,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:03,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:04,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:04,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:04,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:05,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:05,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:05,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:06,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:06,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:06,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:07,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:07,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:08,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:08,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:08,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:09,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:09,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:09,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:10,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:10,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:10,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:11,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:11,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:11,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:12,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:12,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:13,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:13,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:13,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:14,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:14,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:14,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:15,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:15,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:16,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:16,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:16,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:17,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:17,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:17,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:18,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:18,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:19,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:19,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:19,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:20,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:20,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:20,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:21,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:21,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:22,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:22,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:23,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:23,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:23,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:24,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:24,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:25,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:25,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:26,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:26,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:26,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:27,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:27,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:27,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:28,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:28,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:28,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:29,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:29,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:29,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:30,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:30,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:30,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:31,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:31,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:31,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:32,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:32,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:32,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:33,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:33,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:34,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:34,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:34,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:35,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:35,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:35,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:36,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:36,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:36,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:37,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:37,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:37,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:38,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:38,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:38,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:39,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:39,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:40,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:40,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:41,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:42,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:42,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:42,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:43,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:43,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:44,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:44,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:45,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:45,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:45,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:46,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:46,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:47,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:47,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:47,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:47,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:48,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:48,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:49,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:49,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:49,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:50,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:50,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:52,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:52,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:53,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:53,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:53,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:54,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:54,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:54,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:55,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:55,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:55,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:56,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:56,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:57,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:57,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:57,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:58,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:58,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:58,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:59,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:59,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:00,352][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0512, device='cuda:0') eval_epoch_loss=tensor(0.0500, device='cuda:0') eval_epoch_acc=tensor(0.9866, device='cuda:0')
[2025-02-13 04:20:00,354][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 04:20:00,355][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 04:20:00,643][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_5347_loss_0.04996589198708534/model.pt
[2025-02-13 04:20:00,648][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 04:20:00,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:01,113][root][INFO] - Training Epoch: 2/2, step 5347/7134 completed (loss: 0.021252481266856194, acc: 0.9920424222946167)
[2025-02-13 04:20:01,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:01,571][root][INFO] - Training Epoch: 2/2, step 5348/7134 completed (loss: 0.035203590989112854, acc: 0.990304708480835)
[2025-02-13 04:20:01,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:02,003][root][INFO] - Training Epoch: 2/2, step 5349/7134 completed (loss: 0.03827524930238724, acc: 0.9870800971984863)
[2025-02-13 04:20:02,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:02,438][root][INFO] - Training Epoch: 2/2, step 5350/7134 completed (loss: 0.029483210295438766, acc: 0.9854689836502075)
[2025-02-13 04:20:02,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:02,853][root][INFO] - Training Epoch: 2/2, step 5351/7134 completed (loss: 0.030672116205096245, acc: 0.9900744557380676)
[2025-02-13 04:20:02,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:03,256][root][INFO] - Training Epoch: 2/2, step 5352/7134 completed (loss: 0.028970306739211082, acc: 0.9935317039489746)
[2025-02-13 04:20:03,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:03,707][root][INFO] - Training Epoch: 2/2, step 5353/7134 completed (loss: 0.010798760689795017, acc: 0.9977452158927917)
[2025-02-13 04:20:03,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:04,162][root][INFO] - Training Epoch: 2/2, step 5354/7134 completed (loss: 0.034038152545690536, acc: 0.9889867901802063)
[2025-02-13 04:20:04,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:04,601][root][INFO] - Training Epoch: 2/2, step 5355/7134 completed (loss: 0.06476586312055588, acc: 0.9846335649490356)
[2025-02-13 04:20:04,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:05,046][root][INFO] - Training Epoch: 2/2, step 5356/7134 completed (loss: 0.04944872856140137, acc: 0.9850560426712036)
[2025-02-13 04:20:05,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:05,453][root][INFO] - Training Epoch: 2/2, step 5357/7134 completed (loss: 0.04761485755443573, acc: 0.9843205809593201)
[2025-02-13 04:20:05,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:05,905][root][INFO] - Training Epoch: 2/2, step 5358/7134 completed (loss: 0.03812843933701515, acc: 0.9903448224067688)
[2025-02-13 04:20:06,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:06,337][root][INFO] - Training Epoch: 2/2, step 5359/7134 completed (loss: 0.010769917629659176, acc: 0.997668981552124)
[2025-02-13 04:20:06,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:06,766][root][INFO] - Training Epoch: 2/2, step 5360/7134 completed (loss: 0.010023741982877254, acc: 0.9958791136741638)
[2025-02-13 04:20:06,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:07,219][root][INFO] - Training Epoch: 2/2, step 5361/7134 completed (loss: 0.03022339940071106, acc: 0.9904191493988037)
[2025-02-13 04:20:07,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:07,668][root][INFO] - Training Epoch: 2/2, step 5362/7134 completed (loss: 0.01831861212849617, acc: 0.9925373196601868)
[2025-02-13 04:20:07,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:08,086][root][INFO] - Training Epoch: 2/2, step 5363/7134 completed (loss: 0.046376168727874756, acc: 0.9840116500854492)
[2025-02-13 04:20:08,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:08,512][root][INFO] - Training Epoch: 2/2, step 5364/7134 completed (loss: 0.019742794334888458, acc: 0.9922077655792236)
[2025-02-13 04:20:08,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:08,945][root][INFO] - Training Epoch: 2/2, step 5365/7134 completed (loss: 0.013003677129745483, acc: 0.9964726567268372)
[2025-02-13 04:20:09,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:09,332][root][INFO] - Training Epoch: 2/2, step 5366/7134 completed (loss: 0.050381358712911606, acc: 0.9826388955116272)
[2025-02-13 04:20:09,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:09,772][root][INFO] - Training Epoch: 2/2, step 5367/7134 completed (loss: 0.03162072226405144, acc: 0.9924623370170593)
[2025-02-13 04:20:09,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:10,210][root][INFO] - Training Epoch: 2/2, step 5368/7134 completed (loss: 0.030331548303365707, acc: 0.9866220951080322)
[2025-02-13 04:20:10,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:10,597][root][INFO] - Training Epoch: 2/2, step 5369/7134 completed (loss: 0.02043556049466133, acc: 0.993630588054657)
[2025-02-13 04:20:10,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:11,025][root][INFO] - Training Epoch: 2/2, step 5370/7134 completed (loss: 0.052221689373254776, acc: 0.9890109896659851)
[2025-02-13 04:20:11,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:11,461][root][INFO] - Training Epoch: 2/2, step 5371/7134 completed (loss: 0.07619793713092804, acc: 0.9817276000976562)
[2025-02-13 04:20:11,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:11,899][root][INFO] - Training Epoch: 2/2, step 5372/7134 completed (loss: 0.007037673145532608, acc: 0.9985975027084351)
[2025-02-13 04:20:12,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:12,298][root][INFO] - Training Epoch: 2/2, step 5373/7134 completed (loss: 0.05994628369808197, acc: 0.9874476790428162)
[2025-02-13 04:20:12,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:12,622][root][INFO] - Training Epoch: 2/2, step 5374/7134 completed (loss: 0.0281869787722826, acc: 0.9935691356658936)
[2025-02-13 04:20:12,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:13,026][root][INFO] - Training Epoch: 2/2, step 5375/7134 completed (loss: 0.03944580629467964, acc: 0.9865900278091431)
[2025-02-13 04:20:13,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:13,421][root][INFO] - Training Epoch: 2/2, step 5376/7134 completed (loss: 0.021374130621552467, acc: 0.99303138256073)
[2025-02-13 04:20:13,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:13,858][root][INFO] - Training Epoch: 2/2, step 5377/7134 completed (loss: 0.03386635705828667, acc: 0.9924127459526062)
[2025-02-13 04:20:13,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:14,295][root][INFO] - Training Epoch: 2/2, step 5378/7134 completed (loss: 0.01720944233238697, acc: 0.9939320683479309)
[2025-02-13 04:20:14,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:14,728][root][INFO] - Training Epoch: 2/2, step 5379/7134 completed (loss: 0.027879958972334862, acc: 0.9948119521141052)
[2025-02-13 04:20:14,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:15,137][root][INFO] - Training Epoch: 2/2, step 5380/7134 completed (loss: 0.03293541446328163, acc: 0.9911816716194153)
[2025-02-13 04:20:15,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:15,540][root][INFO] - Training Epoch: 2/2, step 5381/7134 completed (loss: 0.04866265133023262, acc: 0.9888424277305603)
[2025-02-13 04:20:15,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:15,941][root][INFO] - Training Epoch: 2/2, step 5382/7134 completed (loss: 0.014932974241673946, acc: 0.9965870380401611)
[2025-02-13 04:20:16,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:16,375][root][INFO] - Training Epoch: 2/2, step 5383/7134 completed (loss: 0.017410660162568092, acc: 0.9943181872367859)
[2025-02-13 04:20:16,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:16,834][root][INFO] - Training Epoch: 2/2, step 5384/7134 completed (loss: 0.009219532832503319, acc: 0.9952606558799744)
[2025-02-13 04:20:16,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:17,257][root][INFO] - Training Epoch: 2/2, step 5385/7134 completed (loss: 0.021594516932964325, acc: 0.9920739531517029)
[2025-02-13 04:20:17,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:17,689][root][INFO] - Training Epoch: 2/2, step 5386/7134 completed (loss: 0.009932426735758781, acc: 0.9971910119056702)
[2025-02-13 04:20:17,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:18,134][root][INFO] - Training Epoch: 2/2, step 5387/7134 completed (loss: 0.01358766295015812, acc: 0.9961904883384705)
[2025-02-13 04:20:18,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:18,576][root][INFO] - Training Epoch: 2/2, step 5388/7134 completed (loss: 0.02002355456352234, acc: 0.9924242496490479)
[2025-02-13 04:20:18,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:18,981][root][INFO] - Training Epoch: 2/2, step 5389/7134 completed (loss: 0.022592077031731606, acc: 0.995207667350769)
[2025-02-13 04:20:19,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:19,405][root][INFO] - Training Epoch: 2/2, step 5390/7134 completed (loss: 0.04137727618217468, acc: 0.9904912710189819)
[2025-02-13 04:20:19,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:19,773][root][INFO] - Training Epoch: 2/2, step 5391/7134 completed (loss: 0.05310339480638504, acc: 0.9862778782844543)
[2025-02-13 04:20:19,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:20,174][root][INFO] - Training Epoch: 2/2, step 5392/7134 completed (loss: 0.04621174558997154, acc: 0.9847561120986938)
[2025-02-13 04:20:20,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:20,573][root][INFO] - Training Epoch: 2/2, step 5393/7134 completed (loss: 0.017362793907523155, acc: 0.9921875)
[2025-02-13 04:20:20,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:20,943][root][INFO] - Training Epoch: 2/2, step 5394/7134 completed (loss: 0.03541036695241928, acc: 0.988095223903656)
[2025-02-13 04:20:21,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:21,375][root][INFO] - Training Epoch: 2/2, step 5395/7134 completed (loss: 0.018841248005628586, acc: 0.9924585223197937)
[2025-02-13 04:20:21,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:21,798][root][INFO] - Training Epoch: 2/2, step 5396/7134 completed (loss: 0.036183685064315796, acc: 0.9894737005233765)
[2025-02-13 04:20:21,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:22,217][root][INFO] - Training Epoch: 2/2, step 5397/7134 completed (loss: 0.017255499958992004, acc: 0.9932318329811096)
[2025-02-13 04:20:22,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:22,635][root][INFO] - Training Epoch: 2/2, step 5398/7134 completed (loss: 0.02379555068910122, acc: 0.99262535572052)
[2025-02-13 04:20:22,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:23,052][root][INFO] - Training Epoch: 2/2, step 5399/7134 completed (loss: 0.022597825154662132, acc: 0.9904240965843201)
[2025-02-13 04:20:23,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:23,475][root][INFO] - Training Epoch: 2/2, step 5400/7134 completed (loss: 0.09862761944532394, acc: 0.9749216437339783)
[2025-02-13 04:20:23,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:23,894][root][INFO] - Training Epoch: 2/2, step 5401/7134 completed (loss: 0.026009656488895416, acc: 0.995184600353241)
[2025-02-13 04:20:24,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:24,314][root][INFO] - Training Epoch: 2/2, step 5402/7134 completed (loss: 0.015340332873165607, acc: 0.9955947399139404)
[2025-02-13 04:20:24,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:24,686][root][INFO] - Training Epoch: 2/2, step 5403/7134 completed (loss: 0.04648207128047943, acc: 0.9918434023857117)
[2025-02-13 04:20:24,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:25,121][root][INFO] - Training Epoch: 2/2, step 5404/7134 completed (loss: 0.060578010976314545, acc: 0.9838926196098328)
[2025-02-13 04:20:25,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:25,567][root][INFO] - Training Epoch: 2/2, step 5405/7134 completed (loss: 0.018200842663645744, acc: 0.9947643876075745)
[2025-02-13 04:20:25,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:25,998][root][INFO] - Training Epoch: 2/2, step 5406/7134 completed (loss: 0.014637453481554985, acc: 0.9961013793945312)
[2025-02-13 04:20:26,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:26,380][root][INFO] - Training Epoch: 2/2, step 5407/7134 completed (loss: 0.019113104790449142, acc: 0.9951691031455994)
[2025-02-13 04:20:26,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:26,790][root][INFO] - Training Epoch: 2/2, step 5408/7134 completed (loss: 0.05022495985031128, acc: 0.9875195026397705)
[2025-02-13 04:20:26,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:27,208][root][INFO] - Training Epoch: 2/2, step 5409/7134 completed (loss: 0.08994224667549133, acc: 0.9863842725753784)
[2025-02-13 04:20:27,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:27,626][root][INFO] - Training Epoch: 2/2, step 5410/7134 completed (loss: 0.040857356041669846, acc: 0.9868766665458679)
[2025-02-13 04:20:27,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:28,050][root][INFO] - Training Epoch: 2/2, step 5411/7134 completed (loss: 0.023709731176495552, acc: 0.9956458806991577)
[2025-02-13 04:20:28,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:28,492][root][INFO] - Training Epoch: 2/2, step 5412/7134 completed (loss: 0.06530018150806427, acc: 0.9884615540504456)
[2025-02-13 04:20:28,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:28,953][root][INFO] - Training Epoch: 2/2, step 5413/7134 completed (loss: 0.061060965061187744, acc: 0.9785714149475098)
[2025-02-13 04:20:29,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:29,410][root][INFO] - Training Epoch: 2/2, step 5414/7134 completed (loss: 0.0821111872792244, acc: 0.9794344305992126)
[2025-02-13 04:20:29,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:29,832][root][INFO] - Training Epoch: 2/2, step 5415/7134 completed (loss: 0.04606292396783829, acc: 0.9856733679771423)
[2025-02-13 04:20:29,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:30,288][root][INFO] - Training Epoch: 2/2, step 5416/7134 completed (loss: 0.05214393511414528, acc: 0.9802555441856384)
[2025-02-13 04:20:30,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:30,738][root][INFO] - Training Epoch: 2/2, step 5417/7134 completed (loss: 0.042656052857637405, acc: 0.9912609457969666)
[2025-02-13 04:20:30,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:31,148][root][INFO] - Training Epoch: 2/2, step 5418/7134 completed (loss: 0.03142224997282028, acc: 0.9881831407546997)
[2025-02-13 04:20:31,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:31,561][root][INFO] - Training Epoch: 2/2, step 5419/7134 completed (loss: 0.08239823579788208, acc: 0.9762712121009827)
[2025-02-13 04:20:31,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:31,975][root][INFO] - Training Epoch: 2/2, step 5420/7134 completed (loss: 0.0626058429479599, acc: 0.9825174808502197)
[2025-02-13 04:20:32,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:32,410][root][INFO] - Training Epoch: 2/2, step 5421/7134 completed (loss: 0.09193583577871323, acc: 0.9765990376472473)
[2025-02-13 04:20:32,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:32,854][root][INFO] - Training Epoch: 2/2, step 5422/7134 completed (loss: 0.034497469663619995, acc: 0.9911242723464966)
[2025-02-13 04:20:32,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:33,168][root][INFO] - Training Epoch: 2/2, step 5423/7134 completed (loss: 0.06523510068655014, acc: 0.9824047088623047)
[2025-02-13 04:20:33,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:33,566][root][INFO] - Training Epoch: 2/2, step 5424/7134 completed (loss: 0.026978228241205215, acc: 0.9894366264343262)
[2025-02-13 04:20:33,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:33,924][root][INFO] - Training Epoch: 2/2, step 5425/7134 completed (loss: 0.02403401769697666, acc: 0.9941747784614563)
[2025-02-13 04:20:34,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:34,304][root][INFO] - Training Epoch: 2/2, step 5426/7134 completed (loss: 0.04951644688844681, acc: 0.9879518151283264)
[2025-02-13 04:20:34,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:34,713][root][INFO] - Training Epoch: 2/2, step 5427/7134 completed (loss: 0.027322540059685707, acc: 0.9905213117599487)
[2025-02-13 04:20:34,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:35,116][root][INFO] - Training Epoch: 2/2, step 5428/7134 completed (loss: 0.03424244746565819, acc: 0.991482138633728)
[2025-02-13 04:20:35,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:35,493][root][INFO] - Training Epoch: 2/2, step 5429/7134 completed (loss: 0.057102251797914505, acc: 0.9855595827102661)
[2025-02-13 04:20:35,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:35,907][root][INFO] - Training Epoch: 2/2, step 5430/7134 completed (loss: 0.024643227458000183, acc: 0.9937304258346558)
[2025-02-13 04:20:36,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:36,271][root][INFO] - Training Epoch: 2/2, step 5431/7134 completed (loss: 0.022810587659478188, acc: 0.9943289160728455)
[2025-02-13 04:20:36,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:36,671][root][INFO] - Training Epoch: 2/2, step 5432/7134 completed (loss: 0.01696380227804184, acc: 0.9962335228919983)
[2025-02-13 04:20:36,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:37,023][root][INFO] - Training Epoch: 2/2, step 5433/7134 completed (loss: 0.04436248540878296, acc: 0.991919219493866)
[2025-02-13 04:20:37,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:37,414][root][INFO] - Training Epoch: 2/2, step 5434/7134 completed (loss: 0.02504807896912098, acc: 0.9939637780189514)
[2025-02-13 04:20:37,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:37,778][root][INFO] - Training Epoch: 2/2, step 5435/7134 completed (loss: 0.02272310107946396, acc: 0.9937238693237305)
[2025-02-13 04:20:37,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:38,178][root][INFO] - Training Epoch: 2/2, step 5436/7134 completed (loss: 0.028493162244558334, acc: 0.9925816059112549)
[2025-02-13 04:20:38,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:38,584][root][INFO] - Training Epoch: 2/2, step 5437/7134 completed (loss: 0.019650066271424294, acc: 0.9965096116065979)
[2025-02-13 04:20:38,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:39,030][root][INFO] - Training Epoch: 2/2, step 5438/7134 completed (loss: 0.02429273910820484, acc: 0.9922480583190918)
[2025-02-13 04:20:39,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:39,452][root][INFO] - Training Epoch: 2/2, step 5439/7134 completed (loss: 0.03788048401474953, acc: 0.9840116500854492)
[2025-02-13 04:20:39,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:39,888][root][INFO] - Training Epoch: 2/2, step 5440/7134 completed (loss: 0.039737097918987274, acc: 0.9910600185394287)
[2025-02-13 04:20:40,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:40,322][root][INFO] - Training Epoch: 2/2, step 5441/7134 completed (loss: 0.03561409190297127, acc: 0.9914320707321167)
[2025-02-13 04:20:40,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:40,739][root][INFO] - Training Epoch: 2/2, step 5442/7134 completed (loss: 0.019635360687971115, acc: 0.9959404468536377)
[2025-02-13 04:20:40,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:41,132][root][INFO] - Training Epoch: 2/2, step 5443/7134 completed (loss: 0.030607672408223152, acc: 0.9900568127632141)
[2025-02-13 04:20:41,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:41,552][root][INFO] - Training Epoch: 2/2, step 5444/7134 completed (loss: 0.025683937594294548, acc: 0.989051103591919)
[2025-02-13 04:20:41,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:41,976][root][INFO] - Training Epoch: 2/2, step 5445/7134 completed (loss: 0.027949223294854164, acc: 0.9908592104911804)
[2025-02-13 04:20:42,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:42,397][root][INFO] - Training Epoch: 2/2, step 5446/7134 completed (loss: 0.022436026483774185, acc: 0.9957982897758484)
[2025-02-13 04:20:42,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:42,793][root][INFO] - Training Epoch: 2/2, step 5447/7134 completed (loss: 0.024680335074663162, acc: 0.9919484853744507)
[2025-02-13 04:20:42,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:43,250][root][INFO] - Training Epoch: 2/2, step 5448/7134 completed (loss: 0.03663177415728569, acc: 0.9846368432044983)
[2025-02-13 04:20:43,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:43,655][root][INFO] - Training Epoch: 2/2, step 5449/7134 completed (loss: 0.01584162376821041, acc: 0.9964912533760071)
[2025-02-13 04:20:43,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:44,076][root][INFO] - Training Epoch: 2/2, step 5450/7134 completed (loss: 0.012271113693714142, acc: 0.9961389899253845)
[2025-02-13 04:20:44,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:44,488][root][INFO] - Training Epoch: 2/2, step 5451/7134 completed (loss: 0.031137103214859962, acc: 0.9908257126808167)
[2025-02-13 04:20:44,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:44,903][root][INFO] - Training Epoch: 2/2, step 5452/7134 completed (loss: 0.05272822454571724, acc: 0.9896640777587891)
[2025-02-13 04:20:45,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:45,346][root][INFO] - Training Epoch: 2/2, step 5453/7134 completed (loss: 0.042909637093544006, acc: 0.9926380515098572)
[2025-02-13 04:20:45,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:45,792][root][INFO] - Training Epoch: 2/2, step 5454/7134 completed (loss: 0.022295212373137474, acc: 0.9921773076057434)
[2025-02-13 04:20:45,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:46,234][root][INFO] - Training Epoch: 2/2, step 5455/7134 completed (loss: 0.07627430558204651, acc: 0.9797979593276978)
[2025-02-13 04:20:46,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:46,675][root][INFO] - Training Epoch: 2/2, step 5456/7134 completed (loss: 0.03594471886754036, acc: 0.989924430847168)
[2025-02-13 04:20:46,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:47,128][root][INFO] - Training Epoch: 2/2, step 5457/7134 completed (loss: 0.0469539500772953, acc: 0.990604043006897)
[2025-02-13 04:20:47,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:47,565][root][INFO] - Training Epoch: 2/2, step 5458/7134 completed (loss: 0.02396535873413086, acc: 0.9910827875137329)
[2025-02-13 04:20:47,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:47,984][root][INFO] - Training Epoch: 2/2, step 5459/7134 completed (loss: 0.007152874954044819, acc: 0.9985097050666809)
[2025-02-13 04:20:48,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:48,393][root][INFO] - Training Epoch: 2/2, step 5460/7134 completed (loss: 0.023888984695076942, acc: 0.9944367408752441)
[2025-02-13 04:20:48,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:48,796][root][INFO] - Training Epoch: 2/2, step 5461/7134 completed (loss: 0.03183748945593834, acc: 0.9911699891090393)
[2025-02-13 04:20:48,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:49,238][root][INFO] - Training Epoch: 2/2, step 5462/7134 completed (loss: 0.0069425348192453384, acc: 0.9985693693161011)
[2025-02-13 04:20:49,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:49,655][root][INFO] - Training Epoch: 2/2, step 5463/7134 completed (loss: 0.02497166022658348, acc: 0.9923664331436157)
[2025-02-13 04:20:49,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:50,085][root][INFO] - Training Epoch: 2/2, step 5464/7134 completed (loss: 0.028646228834986687, acc: 0.9894319772720337)
[2025-02-13 04:20:50,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:50,522][root][INFO] - Training Epoch: 2/2, step 5465/7134 completed (loss: 0.0403413288295269, acc: 0.9925261735916138)
[2025-02-13 04:20:50,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:50,969][root][INFO] - Training Epoch: 2/2, step 5466/7134 completed (loss: 0.016729064285755157, acc: 0.9953970313072205)
[2025-02-13 04:20:51,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:51,435][root][INFO] - Training Epoch: 2/2, step 5467/7134 completed (loss: 0.014498820528388023, acc: 0.9953051805496216)
[2025-02-13 04:20:51,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:51,880][root][INFO] - Training Epoch: 2/2, step 5468/7134 completed (loss: 0.027820654213428497, acc: 0.9918793439865112)
[2025-02-13 04:20:52,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:52,320][root][INFO] - Training Epoch: 2/2, step 5469/7134 completed (loss: 0.016037296503782272, acc: 0.9950920343399048)
[2025-02-13 04:20:52,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:52,766][root][INFO] - Training Epoch: 2/2, step 5470/7134 completed (loss: 0.005407707300037146, acc: 1.0)
[2025-02-13 04:20:52,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:53,198][root][INFO] - Training Epoch: 2/2, step 5471/7134 completed (loss: 0.01961115188896656, acc: 0.9945945739746094)
[2025-02-13 04:20:53,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:53,634][root][INFO] - Training Epoch: 2/2, step 5472/7134 completed (loss: 0.020031506195664406, acc: 0.996259331703186)
[2025-02-13 04:20:53,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:54,079][root][INFO] - Training Epoch: 2/2, step 5473/7134 completed (loss: 0.011686992831528187, acc: 0.9963189959526062)
[2025-02-13 04:20:54,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:54,497][root][INFO] - Training Epoch: 2/2, step 5474/7134 completed (loss: 0.02315532974898815, acc: 0.9928571581840515)
[2025-02-13 04:20:54,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:54,932][root][INFO] - Training Epoch: 2/2, step 5475/7134 completed (loss: 0.014611871913075447, acc: 0.9957325458526611)
[2025-02-13 04:20:55,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:55,337][root][INFO] - Training Epoch: 2/2, step 5476/7134 completed (loss: 0.0094226049259305, acc: 0.9971056580543518)
[2025-02-13 04:20:55,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:55,778][root][INFO] - Training Epoch: 2/2, step 5477/7134 completed (loss: 0.01659337431192398, acc: 0.9975369572639465)
[2025-02-13 04:20:55,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:56,247][root][INFO] - Training Epoch: 2/2, step 5478/7134 completed (loss: 0.04987721890211105, acc: 0.9862778782844543)
[2025-02-13 04:20:56,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:56,693][root][INFO] - Training Epoch: 2/2, step 5479/7134 completed (loss: 0.0373033806681633, acc: 0.9850543737411499)
[2025-02-13 04:20:56,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:57,136][root][INFO] - Training Epoch: 2/2, step 5480/7134 completed (loss: 0.03859531134366989, acc: 0.9894737005233765)
[2025-02-13 04:20:57,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:57,535][root][INFO] - Training Epoch: 2/2, step 5481/7134 completed (loss: 0.021740010008215904, acc: 0.9898989796638489)
[2025-02-13 04:20:57,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:58,261][root][INFO] - Training Epoch: 2/2, step 5482/7134 completed (loss: 0.036500487476587296, acc: 0.9891172647476196)
[2025-02-13 04:20:58,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:58,761][root][INFO] - Training Epoch: 2/2, step 5483/7134 completed (loss: 0.03843831643462181, acc: 0.9860724210739136)
[2025-02-13 04:20:58,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:59,203][root][INFO] - Training Epoch: 2/2, step 5484/7134 completed (loss: 0.045210275799036026, acc: 0.9833101630210876)
[2025-02-13 04:20:59,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:59,608][root][INFO] - Training Epoch: 2/2, step 5485/7134 completed (loss: 0.03849918022751808, acc: 0.9891107082366943)
[2025-02-13 04:20:59,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:00,049][root][INFO] - Training Epoch: 2/2, step 5486/7134 completed (loss: 0.0484532006084919, acc: 0.9861591458320618)
[2025-02-13 04:21:00,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:00,448][root][INFO] - Training Epoch: 2/2, step 5487/7134 completed (loss: 0.019039498642086983, acc: 0.9934959411621094)
[2025-02-13 04:21:00,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:00,918][root][INFO] - Training Epoch: 2/2, step 5488/7134 completed (loss: 0.050223611295223236, acc: 0.9855263233184814)
[2025-02-13 04:21:01,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:01,375][root][INFO] - Training Epoch: 2/2, step 5489/7134 completed (loss: 0.03164149448275566, acc: 0.9938080310821533)
[2025-02-13 04:21:01,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:01,684][root][INFO] - Training Epoch: 2/2, step 5490/7134 completed (loss: 0.022914914414286613, acc: 0.9908883571624756)
[2025-02-13 04:21:01,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:02,083][root][INFO] - Training Epoch: 2/2, step 5491/7134 completed (loss: 0.021150516346096992, acc: 0.992668628692627)
[2025-02-13 04:21:02,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:02,486][root][INFO] - Training Epoch: 2/2, step 5492/7134 completed (loss: 0.03141328692436218, acc: 0.9915397763252258)
[2025-02-13 04:21:02,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:02,889][root][INFO] - Training Epoch: 2/2, step 5493/7134 completed (loss: 0.03335697576403618, acc: 0.9864130616188049)
[2025-02-13 04:21:03,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:03,343][root][INFO] - Training Epoch: 2/2, step 5494/7134 completed (loss: 0.01726621575653553, acc: 0.9928876161575317)
[2025-02-13 04:21:03,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:03,707][root][INFO] - Training Epoch: 2/2, step 5495/7134 completed (loss: 0.020006529986858368, acc: 0.9964157938957214)
[2025-02-13 04:21:03,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:03,996][root][INFO] - Training Epoch: 2/2, step 5496/7134 completed (loss: 0.04136138781905174, acc: 0.9883268475532532)
[2025-02-13 04:21:04,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:04,397][root][INFO] - Training Epoch: 2/2, step 5497/7134 completed (loss: 0.05997898802161217, acc: 0.980966329574585)
[2025-02-13 04:21:04,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:04,823][root][INFO] - Training Epoch: 2/2, step 5498/7134 completed (loss: 0.020128324627876282, acc: 0.9907161593437195)
[2025-02-13 04:21:04,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:05,225][root][INFO] - Training Epoch: 2/2, step 5499/7134 completed (loss: 0.02937476895749569, acc: 0.9899497628211975)
[2025-02-13 04:21:05,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:05,644][root][INFO] - Training Epoch: 2/2, step 5500/7134 completed (loss: 0.029971066862344742, acc: 0.9918144345283508)
[2025-02-13 04:21:05,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:06,080][root][INFO] - Training Epoch: 2/2, step 5501/7134 completed (loss: 0.01606723852455616, acc: 0.9950617551803589)
[2025-02-13 04:21:06,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:06,522][root][INFO] - Training Epoch: 2/2, step 5502/7134 completed (loss: 0.03186999633908272, acc: 0.9838056564331055)
[2025-02-13 04:21:06,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:06,920][root][INFO] - Training Epoch: 2/2, step 5503/7134 completed (loss: 0.03769364953041077, acc: 0.9880715608596802)
[2025-02-13 04:21:07,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:07,360][root][INFO] - Training Epoch: 2/2, step 5504/7134 completed (loss: 0.016156017780303955, acc: 0.9951573610305786)
[2025-02-13 04:21:07,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:07,776][root][INFO] - Training Epoch: 2/2, step 5505/7134 completed (loss: 0.02842569909989834, acc: 0.9843971729278564)
[2025-02-13 04:21:07,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:08,210][root][INFO] - Training Epoch: 2/2, step 5506/7134 completed (loss: 0.02713906578719616, acc: 0.9896193742752075)
[2025-02-13 04:21:08,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:08,637][root][INFO] - Training Epoch: 2/2, step 5507/7134 completed (loss: 0.036756481975317, acc: 0.9845938086509705)
[2025-02-13 04:21:08,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:09,076][root][INFO] - Training Epoch: 2/2, step 5508/7134 completed (loss: 0.017310569062829018, acc: 0.9941588640213013)
[2025-02-13 04:21:09,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:09,532][root][INFO] - Training Epoch: 2/2, step 5509/7134 completed (loss: 0.027653768658638, acc: 0.9941520690917969)
[2025-02-13 04:21:09,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:09,974][root][INFO] - Training Epoch: 2/2, step 5510/7134 completed (loss: 0.03858558461070061, acc: 0.9871465563774109)
[2025-02-13 04:21:10,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:10,450][root][INFO] - Training Epoch: 2/2, step 5511/7134 completed (loss: 0.040443673729896545, acc: 0.9886877536773682)
[2025-02-13 04:21:10,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:10,888][root][INFO] - Training Epoch: 2/2, step 5512/7134 completed (loss: 0.010413000360131264, acc: 0.9947299361228943)
[2025-02-13 04:21:11,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:11,296][root][INFO] - Training Epoch: 2/2, step 5513/7134 completed (loss: 0.028974177315831184, acc: 0.99048912525177)
[2025-02-13 04:21:11,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:11,739][root][INFO] - Training Epoch: 2/2, step 5514/7134 completed (loss: 0.09135778993368149, acc: 0.9763681888580322)
[2025-02-13 04:21:11,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:12,150][root][INFO] - Training Epoch: 2/2, step 5515/7134 completed (loss: 0.032075993716716766, acc: 0.9871612191200256)
[2025-02-13 04:21:12,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:12,543][root][INFO] - Training Epoch: 2/2, step 5516/7134 completed (loss: 0.027709048241376877, acc: 0.9929676651954651)
[2025-02-13 04:21:12,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:12,946][root][INFO] - Training Epoch: 2/2, step 5517/7134 completed (loss: 0.03196254000067711, acc: 0.9906914830207825)
[2025-02-13 04:21:13,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:13,387][root][INFO] - Training Epoch: 2/2, step 5518/7134 completed (loss: 0.03307630494236946, acc: 0.989130437374115)
[2025-02-13 04:21:13,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:13,826][root][INFO] - Training Epoch: 2/2, step 5519/7134 completed (loss: 0.008945551700890064, acc: 0.9972413778305054)
[2025-02-13 04:21:13,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:14,266][root][INFO] - Training Epoch: 2/2, step 5520/7134 completed (loss: 0.019598007202148438, acc: 0.9932050108909607)
[2025-02-13 04:21:14,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:14,707][root][INFO] - Training Epoch: 2/2, step 5521/7134 completed (loss: 0.015273131430149078, acc: 0.9937185645103455)
[2025-02-13 04:21:14,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:15,139][root][INFO] - Training Epoch: 2/2, step 5522/7134 completed (loss: 0.03760417923331261, acc: 0.98562091588974)
[2025-02-13 04:21:15,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:15,519][root][INFO] - Training Epoch: 2/2, step 5523/7134 completed (loss: 0.04069659858942032, acc: 0.992977499961853)
[2025-02-13 04:21:15,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:15,920][root][INFO] - Training Epoch: 2/2, step 5524/7134 completed (loss: 0.03764038532972336, acc: 0.9876161217689514)
[2025-02-13 04:21:16,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:16,359][root][INFO] - Training Epoch: 2/2, step 5525/7134 completed (loss: 0.03558250144124031, acc: 0.9882869720458984)
[2025-02-13 04:21:16,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:16,771][root][INFO] - Training Epoch: 2/2, step 5526/7134 completed (loss: 0.028411777690052986, acc: 0.9926362037658691)
[2025-02-13 04:21:16,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:17,184][root][INFO] - Training Epoch: 2/2, step 5527/7134 completed (loss: 0.024336373433470726, acc: 0.9915966391563416)
[2025-02-13 04:21:17,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:17,622][root][INFO] - Training Epoch: 2/2, step 5528/7134 completed (loss: 0.027425851672887802, acc: 0.9928994178771973)
[2025-02-13 04:21:17,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:18,064][root][INFO] - Training Epoch: 2/2, step 5529/7134 completed (loss: 0.05173391103744507, acc: 0.9858155846595764)
[2025-02-13 04:21:18,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:18,501][root][INFO] - Training Epoch: 2/2, step 5530/7134 completed (loss: 0.018227137625217438, acc: 0.9940387606620789)
[2025-02-13 04:21:18,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:18,944][root][INFO] - Training Epoch: 2/2, step 5531/7134 completed (loss: 0.03474031388759613, acc: 0.9925280213356018)
[2025-02-13 04:21:19,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:19,402][root][INFO] - Training Epoch: 2/2, step 5532/7134 completed (loss: 0.060139626264572144, acc: 0.982694685459137)
[2025-02-13 04:21:19,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:19,866][root][INFO] - Training Epoch: 2/2, step 5533/7134 completed (loss: 0.010791026055812836, acc: 0.9965986609458923)
[2025-02-13 04:21:20,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:20,329][root][INFO] - Training Epoch: 2/2, step 5534/7134 completed (loss: 0.035678569227457047, acc: 0.9894859790802002)
[2025-02-13 04:21:20,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:20,780][root][INFO] - Training Epoch: 2/2, step 5535/7134 completed (loss: 0.025980373844504356, acc: 0.9902067184448242)
[2025-02-13 04:21:20,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:21,212][root][INFO] - Training Epoch: 2/2, step 5536/7134 completed (loss: 0.027045561000704765, acc: 0.9918604493141174)
[2025-02-13 04:21:21,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:21,641][root][INFO] - Training Epoch: 2/2, step 5537/7134 completed (loss: 0.02957688271999359, acc: 0.9894982576370239)
[2025-02-13 04:21:21,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:22,088][root][INFO] - Training Epoch: 2/2, step 5538/7134 completed (loss: 0.02129773609340191, acc: 0.9965870380401611)
[2025-02-13 04:21:22,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:22,552][root][INFO] - Training Epoch: 2/2, step 5539/7134 completed (loss: 0.02712906152009964, acc: 0.9932432174682617)
[2025-02-13 04:21:22,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:22,999][root][INFO] - Training Epoch: 2/2, step 5540/7134 completed (loss: 0.044588856399059296, acc: 0.9894737005233765)
[2025-02-13 04:21:23,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:23,454][root][INFO] - Training Epoch: 2/2, step 5541/7134 completed (loss: 0.02236744947731495, acc: 0.9933510422706604)
[2025-02-13 04:21:23,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:23,876][root][INFO] - Training Epoch: 2/2, step 5542/7134 completed (loss: 0.007789555471390486, acc: 0.9971469044685364)
[2025-02-13 04:21:24,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:24,335][root][INFO] - Training Epoch: 2/2, step 5543/7134 completed (loss: 0.026965143159031868, acc: 0.9944567680358887)
[2025-02-13 04:21:24,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:24,789][root][INFO] - Training Epoch: 2/2, step 5544/7134 completed (loss: 0.013062105514109135, acc: 0.9967741966247559)
[2025-02-13 04:21:24,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:25,244][root][INFO] - Training Epoch: 2/2, step 5545/7134 completed (loss: 0.03482416644692421, acc: 0.9890710115432739)
[2025-02-13 04:21:25,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:25,680][root][INFO] - Training Epoch: 2/2, step 5546/7134 completed (loss: 0.028792526572942734, acc: 0.9900332093238831)
[2025-02-13 04:21:25,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:26,137][root][INFO] - Training Epoch: 2/2, step 5547/7134 completed (loss: 0.020204728469252586, acc: 0.9907833933830261)
[2025-02-13 04:21:26,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:26,565][root][INFO] - Training Epoch: 2/2, step 5548/7134 completed (loss: 0.017790086567401886, acc: 0.9953488111495972)
[2025-02-13 04:21:26,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:26,952][root][INFO] - Training Epoch: 2/2, step 5549/7134 completed (loss: 0.018138999119400978, acc: 0.9942857027053833)
[2025-02-13 04:21:27,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:27,391][root][INFO] - Training Epoch: 2/2, step 5550/7134 completed (loss: 0.011051409877836704, acc: 0.9952996373176575)
[2025-02-13 04:21:27,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:27,791][root][INFO] - Training Epoch: 2/2, step 5551/7134 completed (loss: 0.04640386253595352, acc: 0.9851149916648865)
[2025-02-13 04:21:27,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:28,229][root][INFO] - Training Epoch: 2/2, step 5552/7134 completed (loss: 0.008327736519277096, acc: 0.9964328408241272)
[2025-02-13 04:21:28,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:28,662][root][INFO] - Training Epoch: 2/2, step 5553/7134 completed (loss: 0.03094683587551117, acc: 0.993819534778595)
[2025-02-13 04:21:28,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:29,089][root][INFO] - Training Epoch: 2/2, step 5554/7134 completed (loss: 0.010657521896064281, acc: 0.9963325262069702)
[2025-02-13 04:21:29,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:29,513][root][INFO] - Training Epoch: 2/2, step 5555/7134 completed (loss: 0.025362158194184303, acc: 0.990641713142395)
[2025-02-13 04:21:29,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:29,909][root][INFO] - Training Epoch: 2/2, step 5556/7134 completed (loss: 0.02830657549202442, acc: 0.990920901298523)
[2025-02-13 04:21:30,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:30,353][root][INFO] - Training Epoch: 2/2, step 5557/7134 completed (loss: 0.019156597554683685, acc: 0.9971988797187805)
[2025-02-13 04:21:30,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:30,793][root][INFO] - Training Epoch: 2/2, step 5558/7134 completed (loss: 0.014503837563097477, acc: 0.9950310587882996)
[2025-02-13 04:21:30,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:31,225][root][INFO] - Training Epoch: 2/2, step 5559/7134 completed (loss: 0.02833073027431965, acc: 0.9950310587882996)
[2025-02-13 04:21:31,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:31,648][root][INFO] - Training Epoch: 2/2, step 5560/7134 completed (loss: 0.042919598519802094, acc: 0.9909326434135437)
[2025-02-13 04:21:31,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:32,018][root][INFO] - Training Epoch: 2/2, step 5561/7134 completed (loss: 0.030757831409573555, acc: 0.9943289160728455)
[2025-02-13 04:21:32,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:32,452][root][INFO] - Training Epoch: 2/2, step 5562/7134 completed (loss: 0.036814264953136444, acc: 0.9948805570602417)
[2025-02-13 04:21:32,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:32,890][root][INFO] - Training Epoch: 2/2, step 5563/7134 completed (loss: 0.031248262152075768, acc: 0.9924924969673157)
[2025-02-13 04:21:33,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:33,294][root][INFO] - Training Epoch: 2/2, step 5564/7134 completed (loss: 0.019799193367362022, acc: 0.9972106218338013)
[2025-02-13 04:21:33,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:33,730][root][INFO] - Training Epoch: 2/2, step 5565/7134 completed (loss: 0.005428014788776636, acc: 0.9984639286994934)
[2025-02-13 04:21:33,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:34,185][root][INFO] - Training Epoch: 2/2, step 5566/7134 completed (loss: 0.012474487535655499, acc: 0.9977037906646729)
[2025-02-13 04:21:34,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:34,652][root][INFO] - Training Epoch: 2/2, step 5567/7134 completed (loss: 0.017417892813682556, acc: 0.9920091032981873)
[2025-02-13 04:21:34,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:35,050][root][INFO] - Training Epoch: 2/2, step 5568/7134 completed (loss: 0.01911275088787079, acc: 0.9961340427398682)
[2025-02-13 04:21:35,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:35,513][root][INFO] - Training Epoch: 2/2, step 5569/7134 completed (loss: 0.007453080266714096, acc: 0.9977169036865234)
[2025-02-13 04:21:35,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:35,947][root][INFO] - Training Epoch: 2/2, step 5570/7134 completed (loss: 0.031924448907375336, acc: 0.991304337978363)
[2025-02-13 04:21:36,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:36,349][root][INFO] - Training Epoch: 2/2, step 5571/7134 completed (loss: 0.023163730278611183, acc: 0.9940564632415771)
[2025-02-13 04:21:36,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:36,769][root][INFO] - Training Epoch: 2/2, step 5572/7134 completed (loss: 0.03771332651376724, acc: 0.9931318759918213)
[2025-02-13 04:21:36,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:37,230][root][INFO] - Training Epoch: 2/2, step 5573/7134 completed (loss: 0.026765698567032814, acc: 0.9927219748497009)
[2025-02-13 04:21:37,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:37,624][root][INFO] - Training Epoch: 2/2, step 5574/7134 completed (loss: 0.005346922669559717, acc: 0.9984126687049866)
[2025-02-13 04:21:37,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:38,057][root][INFO] - Training Epoch: 2/2, step 5575/7134 completed (loss: 0.01744646392762661, acc: 0.9952152967453003)
[2025-02-13 04:21:38,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:38,457][root][INFO] - Training Epoch: 2/2, step 5576/7134 completed (loss: 0.028448566794395447, acc: 0.9914039969444275)
[2025-02-13 04:21:38,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:38,874][root][INFO] - Training Epoch: 2/2, step 5577/7134 completed (loss: 0.034993819892406464, acc: 0.9912739992141724)
[2025-02-13 04:21:39,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:39,310][root][INFO] - Training Epoch: 2/2, step 5578/7134 completed (loss: 0.014618382789194584, acc: 0.9942611455917358)
[2025-02-13 04:21:39,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:39,747][root][INFO] - Training Epoch: 2/2, step 5579/7134 completed (loss: 0.023961693048477173, acc: 0.9875518679618835)
[2025-02-13 04:21:39,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:40,151][root][INFO] - Training Epoch: 2/2, step 5580/7134 completed (loss: 0.009255013428628445, acc: 0.9986357688903809)
[2025-02-13 04:21:40,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:40,584][root][INFO] - Training Epoch: 2/2, step 5581/7134 completed (loss: 0.013721671886742115, acc: 0.9933510422706604)
[2025-02-13 04:21:40,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:41,031][root][INFO] - Training Epoch: 2/2, step 5582/7134 completed (loss: 0.014798654243350029, acc: 0.9947848916053772)
[2025-02-13 04:21:41,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:41,480][root][INFO] - Training Epoch: 2/2, step 5583/7134 completed (loss: 0.021541181951761246, acc: 0.99370276927948)
[2025-02-13 04:21:41,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:41,894][root][INFO] - Training Epoch: 2/2, step 5584/7134 completed (loss: 0.01883392594754696, acc: 0.9955621361732483)
[2025-02-13 04:21:42,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:42,349][root][INFO] - Training Epoch: 2/2, step 5585/7134 completed (loss: 0.047838397324085236, acc: 0.9890909194946289)
[2025-02-13 04:21:42,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:42,732][root][INFO] - Training Epoch: 2/2, step 5586/7134 completed (loss: 0.015065276063978672, acc: 0.9946714043617249)
[2025-02-13 04:21:42,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:43,169][root][INFO] - Training Epoch: 2/2, step 5587/7134 completed (loss: 0.00933439563959837, acc: 0.9972260594367981)
[2025-02-13 04:21:43,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:43,611][root][INFO] - Training Epoch: 2/2, step 5588/7134 completed (loss: 0.009214550256729126, acc: 0.998487114906311)
[2025-02-13 04:21:43,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:44,026][root][INFO] - Training Epoch: 2/2, step 5589/7134 completed (loss: 0.02538297511637211, acc: 0.9968454241752625)
[2025-02-13 04:21:44,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:44,430][root][INFO] - Training Epoch: 2/2, step 5590/7134 completed (loss: 0.020402194932103157, acc: 0.9929577708244324)
[2025-02-13 04:21:44,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:44,835][root][INFO] - Training Epoch: 2/2, step 5591/7134 completed (loss: 0.010537873022258282, acc: 0.9973822236061096)
[2025-02-13 04:21:44,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:45,236][root][INFO] - Training Epoch: 2/2, step 5592/7134 completed (loss: 0.012929136864840984, acc: 0.9972565174102783)
[2025-02-13 04:21:45,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:45,670][root][INFO] - Training Epoch: 2/2, step 5593/7134 completed (loss: 0.016820289194583893, acc: 0.9935232996940613)
[2025-02-13 04:21:45,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:46,102][root][INFO] - Training Epoch: 2/2, step 5594/7134 completed (loss: 0.007151127327233553, acc: 0.9986522793769836)
[2025-02-13 04:21:46,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:46,522][root][INFO] - Training Epoch: 2/2, step 5595/7134 completed (loss: 0.026722906157374382, acc: 0.9919571280479431)
[2025-02-13 04:21:46,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:46,927][root][INFO] - Training Epoch: 2/2, step 5596/7134 completed (loss: 0.00872121937572956, acc: 0.9959677457809448)
[2025-02-13 04:21:47,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:47,348][root][INFO] - Training Epoch: 2/2, step 5597/7134 completed (loss: 0.03494721278548241, acc: 0.9889807105064392)
[2025-02-13 04:21:47,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:47,760][root][INFO] - Training Epoch: 2/2, step 5598/7134 completed (loss: 0.01856471598148346, acc: 0.994413435459137)
[2025-02-13 04:21:47,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:48,188][root][INFO] - Training Epoch: 2/2, step 5599/7134 completed (loss: 0.05698035657405853, acc: 0.9877049326896667)
[2025-02-13 04:21:48,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:48,629][root][INFO] - Training Epoch: 2/2, step 5600/7134 completed (loss: 0.01574726589024067, acc: 0.9957020282745361)
[2025-02-13 04:21:48,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:49,015][root][INFO] - Training Epoch: 2/2, step 5601/7134 completed (loss: 0.0236924197524786, acc: 0.991304337978363)
[2025-02-13 04:21:49,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:49,424][root][INFO] - Training Epoch: 2/2, step 5602/7134 completed (loss: 0.0368901789188385, acc: 0.9855072498321533)
[2025-02-13 04:21:49,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:49,783][root][INFO] - Training Epoch: 2/2, step 5603/7134 completed (loss: 0.006826780270785093, acc: 1.0)
[2025-02-13 04:21:49,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:50,164][root][INFO] - Training Epoch: 2/2, step 5604/7134 completed (loss: 0.041988879442214966, acc: 0.9823151230812073)
[2025-02-13 04:21:50,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:50,557][root][INFO] - Training Epoch: 2/2, step 5605/7134 completed (loss: 0.025230592116713524, acc: 0.9932088255882263)
[2025-02-13 04:21:50,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:50,950][root][INFO] - Training Epoch: 2/2, step 5606/7134 completed (loss: 0.03500780463218689, acc: 0.9871794581413269)
[2025-02-13 04:21:51,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:51,346][root][INFO] - Training Epoch: 2/2, step 5607/7134 completed (loss: 0.016593074426054955, acc: 0.995488703250885)
[2025-02-13 04:21:51,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:51,740][root][INFO] - Training Epoch: 2/2, step 5608/7134 completed (loss: 0.02162807621061802, acc: 0.9934318661689758)
[2025-02-13 04:21:51,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:52,130][root][INFO] - Training Epoch: 2/2, step 5609/7134 completed (loss: 0.037468135356903076, acc: 0.9895651936531067)
[2025-02-13 04:21:52,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:52,489][root][INFO] - Training Epoch: 2/2, step 5610/7134 completed (loss: 0.016578735783696175, acc: 0.9939637780189514)
[2025-02-13 04:21:52,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:52,943][root][INFO] - Training Epoch: 2/2, step 5611/7134 completed (loss: 0.022977041080594063, acc: 0.9936708807945251)
[2025-02-13 04:21:53,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:53,372][root][INFO] - Training Epoch: 2/2, step 5612/7134 completed (loss: 0.03265887498855591, acc: 0.9907975196838379)
[2025-02-13 04:21:53,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:53,770][root][INFO] - Training Epoch: 2/2, step 5613/7134 completed (loss: 0.034009505063295364, acc: 0.991055428981781)
[2025-02-13 04:21:53,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:54,173][root][INFO] - Training Epoch: 2/2, step 5614/7134 completed (loss: 0.04494516924023628, acc: 0.9871428608894348)
[2025-02-13 04:21:54,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:54,597][root][INFO] - Training Epoch: 2/2, step 5615/7134 completed (loss: 0.03319967910647392, acc: 0.982758641242981)
[2025-02-13 04:21:54,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:55,000][root][INFO] - Training Epoch: 2/2, step 5616/7134 completed (loss: 0.012417996302247047, acc: 0.9970282316207886)
[2025-02-13 04:21:55,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:55,393][root][INFO] - Training Epoch: 2/2, step 5617/7134 completed (loss: 0.008708483539521694, acc: 0.9983766078948975)
[2025-02-13 04:21:55,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:55,814][root][INFO] - Training Epoch: 2/2, step 5618/7134 completed (loss: 0.04793248325586319, acc: 0.9788618087768555)
[2025-02-13 04:21:55,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:56,192][root][INFO] - Training Epoch: 2/2, step 5619/7134 completed (loss: 0.04218175262212753, acc: 0.9858012199401855)
[2025-02-13 04:21:56,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:56,589][root][INFO] - Training Epoch: 2/2, step 5620/7134 completed (loss: 0.027941934764385223, acc: 0.9906542301177979)
[2025-02-13 04:21:56,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:57,047][root][INFO] - Training Epoch: 2/2, step 5621/7134 completed (loss: 0.03066098503768444, acc: 0.9923760890960693)
[2025-02-13 04:21:57,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:57,471][root][INFO] - Training Epoch: 2/2, step 5622/7134 completed (loss: 0.04955526813864708, acc: 0.9918256402015686)
[2025-02-13 04:21:57,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:57,845][root][INFO] - Training Epoch: 2/2, step 5623/7134 completed (loss: 0.024215299636125565, acc: 0.9912891983985901)
[2025-02-13 04:21:57,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:58,262][root][INFO] - Training Epoch: 2/2, step 5624/7134 completed (loss: 0.03568316623568535, acc: 0.9918166995048523)
[2025-02-13 04:21:58,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:58,642][root][INFO] - Training Epoch: 2/2, step 5625/7134 completed (loss: 0.006563074886798859, acc: 0.9976470470428467)
[2025-02-13 04:21:58,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:59,046][root][INFO] - Training Epoch: 2/2, step 5626/7134 completed (loss: 0.009460010565817356, acc: 0.9983079433441162)
[2025-02-13 04:21:59,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:59,452][root][INFO] - Training Epoch: 2/2, step 5627/7134 completed (loss: 0.09704695641994476, acc: 0.9758812785148621)
[2025-02-13 04:21:59,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:59,894][root][INFO] - Training Epoch: 2/2, step 5628/7134 completed (loss: 0.03699270263314247, acc: 0.9872495532035828)
[2025-02-13 04:22:00,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:00,314][root][INFO] - Training Epoch: 2/2, step 5629/7134 completed (loss: 0.03325631842017174, acc: 0.9841827750205994)
[2025-02-13 04:22:00,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:00,718][root][INFO] - Training Epoch: 2/2, step 5630/7134 completed (loss: 0.041973844170570374, acc: 0.9835729002952576)
[2025-02-13 04:22:00,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:01,116][root][INFO] - Training Epoch: 2/2, step 5631/7134 completed (loss: 0.05406687408685684, acc: 0.9817578792572021)
[2025-02-13 04:22:01,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:01,531][root][INFO] - Training Epoch: 2/2, step 5632/7134 completed (loss: 0.02722812257707119, acc: 0.9927404522895813)
[2025-02-13 04:22:01,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:01,931][root][INFO] - Training Epoch: 2/2, step 5633/7134 completed (loss: 0.02424759417772293, acc: 0.9934853315353394)
[2025-02-13 04:22:02,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:02,336][root][INFO] - Training Epoch: 2/2, step 5634/7134 completed (loss: 0.03800386190414429, acc: 0.9930192232131958)
[2025-02-13 04:22:02,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:02,746][root][INFO] - Training Epoch: 2/2, step 5635/7134 completed (loss: 0.017672382295131683, acc: 0.9966216087341309)
[2025-02-13 04:22:02,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:03,164][root][INFO] - Training Epoch: 2/2, step 5636/7134 completed (loss: 0.03211692348122597, acc: 0.9936102032661438)
[2025-02-13 04:22:03,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:03,566][root][INFO] - Training Epoch: 2/2, step 5637/7134 completed (loss: 0.034610483795404434, acc: 0.9881955981254578)
[2025-02-13 04:22:03,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:03,922][root][INFO] - Training Epoch: 2/2, step 5638/7134 completed (loss: 0.028292838484048843, acc: 0.994140625)
[2025-02-13 04:22:04,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:04,342][root][INFO] - Training Epoch: 2/2, step 5639/7134 completed (loss: 0.018550412729382515, acc: 0.995726466178894)
[2025-02-13 04:22:04,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:04,738][root][INFO] - Training Epoch: 2/2, step 5640/7134 completed (loss: 0.03943336755037308, acc: 0.9886178970336914)
[2025-02-13 04:22:04,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:05,070][root][INFO] - Training Epoch: 2/2, step 5641/7134 completed (loss: 0.03514707833528519, acc: 0.9924242496490479)
[2025-02-13 04:22:05,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:05,456][root][INFO] - Training Epoch: 2/2, step 5642/7134 completed (loss: 0.03195338323712349, acc: 0.9909090995788574)
[2025-02-13 04:22:05,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:05,840][root][INFO] - Training Epoch: 2/2, step 5643/7134 completed (loss: 0.04357191175222397, acc: 0.98893803358078)
[2025-02-13 04:22:05,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:06,260][root][INFO] - Training Epoch: 2/2, step 5644/7134 completed (loss: 0.03392920270562172, acc: 0.9878048896789551)
[2025-02-13 04:22:06,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:06,662][root][INFO] - Training Epoch: 2/2, step 5645/7134 completed (loss: 0.01428877841681242, acc: 0.9955817461013794)
[2025-02-13 04:22:06,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:07,055][root][INFO] - Training Epoch: 2/2, step 5646/7134 completed (loss: 0.03377147763967514, acc: 0.9888535141944885)
[2025-02-13 04:22:07,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:07,467][root][INFO] - Training Epoch: 2/2, step 5647/7134 completed (loss: 0.036107055842876434, acc: 0.9906976819038391)
[2025-02-13 04:22:07,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:07,884][root][INFO] - Training Epoch: 2/2, step 5648/7134 completed (loss: 0.07502733170986176, acc: 0.9850746393203735)
[2025-02-13 04:22:08,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:08,275][root][INFO] - Training Epoch: 2/2, step 5649/7134 completed (loss: 0.04134044051170349, acc: 0.9877675771713257)
[2025-02-13 04:22:08,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:08,690][root][INFO] - Training Epoch: 2/2, step 5650/7134 completed (loss: 0.026179932057857513, acc: 0.994854211807251)
[2025-02-13 04:22:08,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:09,112][root][INFO] - Training Epoch: 2/2, step 5651/7134 completed (loss: 0.016905272379517555, acc: 0.9926035404205322)
[2025-02-13 04:22:09,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:09,556][root][INFO] - Training Epoch: 2/2, step 5652/7134 completed (loss: 0.030562592670321465, acc: 0.9887820482254028)
[2025-02-13 04:22:09,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:09,886][root][INFO] - Training Epoch: 2/2, step 5653/7134 completed (loss: 0.0638054683804512, acc: 0.9765625)
[2025-02-13 04:22:10,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:10,287][root][INFO] - Training Epoch: 2/2, step 5654/7134 completed (loss: 0.01925824210047722, acc: 0.9939939975738525)
[2025-02-13 04:22:10,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:10,698][root][INFO] - Training Epoch: 2/2, step 5655/7134 completed (loss: 0.040163010358810425, acc: 0.988095223903656)
[2025-02-13 04:22:10,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:11,083][root][INFO] - Training Epoch: 2/2, step 5656/7134 completed (loss: 0.02298644930124283, acc: 0.9904761910438538)
[2025-02-13 04:22:11,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:11,482][root][INFO] - Training Epoch: 2/2, step 5657/7134 completed (loss: 0.03409464657306671, acc: 0.9912152290344238)
[2025-02-13 04:22:11,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:11,923][root][INFO] - Training Epoch: 2/2, step 5658/7134 completed (loss: 0.034015629440546036, acc: 0.9875690340995789)
[2025-02-13 04:22:12,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:12,333][root][INFO] - Training Epoch: 2/2, step 5659/7134 completed (loss: 0.024017633870244026, acc: 0.9929078221321106)
[2025-02-13 04:22:12,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:12,740][root][INFO] - Training Epoch: 2/2, step 5660/7134 completed (loss: 0.01659935899078846, acc: 0.9933884143829346)
[2025-02-13 04:22:12,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:13,178][root][INFO] - Training Epoch: 2/2, step 5661/7134 completed (loss: 0.004860741551965475, acc: 1.0)
[2025-02-13 04:22:13,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:13,610][root][INFO] - Training Epoch: 2/2, step 5662/7134 completed (loss: 0.01155779417604208, acc: 0.9967948794364929)
[2025-02-13 04:22:13,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:14,013][root][INFO] - Training Epoch: 2/2, step 5663/7134 completed (loss: 0.00970590952783823, acc: 0.9969183206558228)
[2025-02-13 04:22:14,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:14,414][root][INFO] - Training Epoch: 2/2, step 5664/7134 completed (loss: 0.06953616440296173, acc: 0.9866270422935486)
[2025-02-13 04:22:14,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:14,825][root][INFO] - Training Epoch: 2/2, step 5665/7134 completed (loss: 0.01498501654714346, acc: 0.9942938685417175)
[2025-02-13 04:22:14,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:15,259][root][INFO] - Training Epoch: 2/2, step 5666/7134 completed (loss: 0.0064263371750712395, acc: 0.9972413778305054)
[2025-02-13 04:22:15,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:15,644][root][INFO] - Training Epoch: 2/2, step 5667/7134 completed (loss: 0.028623173013329506, acc: 0.9945454597473145)
[2025-02-13 04:22:15,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:16,036][root][INFO] - Training Epoch: 2/2, step 5668/7134 completed (loss: 0.026967477053403854, acc: 0.9933554530143738)
[2025-02-13 04:22:16,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:16,440][root][INFO] - Training Epoch: 2/2, step 5669/7134 completed (loss: 0.011279696598649025, acc: 0.9970674514770508)
[2025-02-13 04:22:16,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:16,848][root][INFO] - Training Epoch: 2/2, step 5670/7134 completed (loss: 0.019977331161499023, acc: 0.9916666746139526)
[2025-02-13 04:22:16,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:17,301][root][INFO] - Training Epoch: 2/2, step 5671/7134 completed (loss: 0.01400755625218153, acc: 0.9963099360466003)
[2025-02-13 04:22:17,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:17,745][root][INFO] - Training Epoch: 2/2, step 5672/7134 completed (loss: 0.01850413903594017, acc: 0.9968454241752625)
[2025-02-13 04:22:17,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:18,123][root][INFO] - Training Epoch: 2/2, step 5673/7134 completed (loss: 0.012557008303701878, acc: 0.9985228776931763)
[2025-02-13 04:22:18,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:18,564][root][INFO] - Training Epoch: 2/2, step 5674/7134 completed (loss: 0.018725810572504997, acc: 0.9938837885856628)
[2025-02-13 04:22:18,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:18,961][root][INFO] - Training Epoch: 2/2, step 5675/7134 completed (loss: 0.026462554931640625, acc: 0.9950248599052429)
[2025-02-13 04:22:19,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:19,374][root][INFO] - Training Epoch: 2/2, step 5676/7134 completed (loss: 0.07689821720123291, acc: 0.9852070808410645)
[2025-02-13 04:22:19,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:19,789][root][INFO] - Training Epoch: 2/2, step 5677/7134 completed (loss: 0.04059381037950516, acc: 0.984375)
[2025-02-13 04:22:19,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:20,197][root][INFO] - Training Epoch: 2/2, step 5678/7134 completed (loss: 0.027084609493613243, acc: 0.9926470518112183)
[2025-02-13 04:22:20,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:20,642][root][INFO] - Training Epoch: 2/2, step 5679/7134 completed (loss: 0.06590937077999115, acc: 0.9751098155975342)
[2025-02-13 04:22:20,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:21,057][root][INFO] - Training Epoch: 2/2, step 5680/7134 completed (loss: 0.0766834169626236, acc: 0.9746646881103516)
[2025-02-13 04:22:21,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:21,478][root][INFO] - Training Epoch: 2/2, step 5681/7134 completed (loss: 0.0415581613779068, acc: 0.9864253401756287)
[2025-02-13 04:22:21,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:21,889][root][INFO] - Training Epoch: 2/2, step 5682/7134 completed (loss: 0.02879699319601059, acc: 0.9884560108184814)
[2025-02-13 04:22:22,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:22,308][root][INFO] - Training Epoch: 2/2, step 5683/7134 completed (loss: 0.045627411454916, acc: 0.9795022010803223)
[2025-02-13 04:22:22,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:22,759][root][INFO] - Training Epoch: 2/2, step 5684/7134 completed (loss: 0.05559315159916878, acc: 0.9865092635154724)
[2025-02-13 04:22:22,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:23,162][root][INFO] - Training Epoch: 2/2, step 5685/7134 completed (loss: 0.037740711122751236, acc: 0.9889705777168274)
[2025-02-13 04:22:23,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:23,576][root][INFO] - Training Epoch: 2/2, step 5686/7134 completed (loss: 0.059143610298633575, acc: 0.9830795526504517)
[2025-02-13 04:22:23,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:23,950][root][INFO] - Training Epoch: 2/2, step 5687/7134 completed (loss: 0.054878149181604385, acc: 0.9871794581413269)
[2025-02-13 04:22:24,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:24,388][root][INFO] - Training Epoch: 2/2, step 5688/7134 completed (loss: 0.05615696683526039, acc: 0.9853137731552124)
[2025-02-13 04:22:24,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:24,823][root][INFO] - Training Epoch: 2/2, step 5689/7134 completed (loss: 0.05146634578704834, acc: 0.9878234267234802)
[2025-02-13 04:22:24,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:25,285][root][INFO] - Training Epoch: 2/2, step 5690/7134 completed (loss: 0.02947801537811756, acc: 0.9929245114326477)
[2025-02-13 04:22:25,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:25,691][root][INFO] - Training Epoch: 2/2, step 5691/7134 completed (loss: 0.037892840802669525, acc: 0.9888198971748352)
[2025-02-13 04:22:25,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:26,141][root][INFO] - Training Epoch: 2/2, step 5692/7134 completed (loss: 0.060428436845541, acc: 0.9879356622695923)
[2025-02-13 04:22:26,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:26,546][root][INFO] - Training Epoch: 2/2, step 5693/7134 completed (loss: 0.019295139238238335, acc: 0.995502233505249)
[2025-02-13 04:22:26,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:26,941][root][INFO] - Training Epoch: 2/2, step 5694/7134 completed (loss: 0.05033904314041138, acc: 0.9884225726127625)
[2025-02-13 04:22:27,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:27,378][root][INFO] - Training Epoch: 2/2, step 5695/7134 completed (loss: 0.01608496904373169, acc: 0.9957020282745361)
[2025-02-13 04:22:27,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:27,819][root][INFO] - Training Epoch: 2/2, step 5696/7134 completed (loss: 0.031527888029813766, acc: 0.9894117712974548)
[2025-02-13 04:22:27,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:28,251][root][INFO] - Training Epoch: 2/2, step 5697/7134 completed (loss: 0.03161146864295006, acc: 0.995768666267395)
[2025-02-13 04:22:28,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:28,680][root][INFO] - Training Epoch: 2/2, step 5698/7134 completed (loss: 0.03743589296936989, acc: 0.9918367266654968)
[2025-02-13 04:22:28,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:29,126][root][INFO] - Training Epoch: 2/2, step 5699/7134 completed (loss: 0.03924873098731041, acc: 0.987261176109314)
[2025-02-13 04:22:29,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:29,563][root][INFO] - Training Epoch: 2/2, step 5700/7134 completed (loss: 0.019406791776418686, acc: 0.9914893507957458)
[2025-02-13 04:22:29,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:30,027][root][INFO] - Training Epoch: 2/2, step 5701/7134 completed (loss: 0.031202901154756546, acc: 0.9962453246116638)
[2025-02-13 04:22:30,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:30,460][root][INFO] - Training Epoch: 2/2, step 5702/7134 completed (loss: 0.034790877252817154, acc: 0.9945873022079468)
[2025-02-13 04:22:30,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:30,885][root][INFO] - Training Epoch: 2/2, step 5703/7134 completed (loss: 0.02209484949707985, acc: 0.9943740963935852)
[2025-02-13 04:22:31,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:31,289][root][INFO] - Training Epoch: 2/2, step 5704/7134 completed (loss: 0.03908565640449524, acc: 0.9885877370834351)
[2025-02-13 04:22:31,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:31,549][root][INFO] - Training Epoch: 2/2, step 5705/7134 completed (loss: 0.014348733238875866, acc: 0.9964285492897034)
[2025-02-13 04:22:31,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:32,017][root][INFO] - Training Epoch: 2/2, step 5706/7134 completed (loss: 0.01138093788176775, acc: 0.9977628588676453)
[2025-02-13 04:22:32,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:32,422][root][INFO] - Training Epoch: 2/2, step 5707/7134 completed (loss: 0.027657493948936462, acc: 0.98591548204422)
[2025-02-13 04:22:32,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:32,832][root][INFO] - Training Epoch: 2/2, step 5708/7134 completed (loss: 0.01715875416994095, acc: 0.995555579662323)
[2025-02-13 04:22:32,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:33,258][root][INFO] - Training Epoch: 2/2, step 5709/7134 completed (loss: 0.04745756834745407, acc: 0.9917920827865601)
[2025-02-13 04:22:33,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:33,698][root][INFO] - Training Epoch: 2/2, step 5710/7134 completed (loss: 0.02550390362739563, acc: 0.9898219108581543)
[2025-02-13 04:22:33,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:34,111][root][INFO] - Training Epoch: 2/2, step 5711/7134 completed (loss: 0.04295256361365318, acc: 0.9882199168205261)
[2025-02-13 04:22:34,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:34,535][root][INFO] - Training Epoch: 2/2, step 5712/7134 completed (loss: 0.04199489578604698, acc: 0.984000027179718)
[2025-02-13 04:22:34,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:34,995][root][INFO] - Training Epoch: 2/2, step 5713/7134 completed (loss: 0.02686011977493763, acc: 0.9885641932487488)
[2025-02-13 04:22:35,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:35,439][root][INFO] - Training Epoch: 2/2, step 5714/7134 completed (loss: 0.02379053458571434, acc: 0.9950186610221863)
[2025-02-13 04:22:35,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:35,841][root][INFO] - Training Epoch: 2/2, step 5715/7134 completed (loss: 0.06250421702861786, acc: 0.9888268113136292)
[2025-02-13 04:22:35,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:36,235][root][INFO] - Training Epoch: 2/2, step 5716/7134 completed (loss: 0.08492353558540344, acc: 0.9822379946708679)
[2025-02-13 04:22:36,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:36,637][root][INFO] - Training Epoch: 2/2, step 5717/7134 completed (loss: 0.06274856626987457, acc: 0.9828926920890808)
[2025-02-13 04:22:36,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:37,025][root][INFO] - Training Epoch: 2/2, step 5718/7134 completed (loss: 0.026399333029985428, acc: 0.9905362725257874)
[2025-02-13 04:22:37,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:37,425][root][INFO] - Training Epoch: 2/2, step 5719/7134 completed (loss: 0.01870521903038025, acc: 0.995502233505249)
[2025-02-13 04:22:37,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:37,824][root][INFO] - Training Epoch: 2/2, step 5720/7134 completed (loss: 0.032746821641922, acc: 0.9861830472946167)
[2025-02-13 04:22:37,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:38,228][root][INFO] - Training Epoch: 2/2, step 5721/7134 completed (loss: 0.014706475660204887, acc: 0.9947368502616882)
[2025-02-13 04:22:38,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:38,566][root][INFO] - Training Epoch: 2/2, step 5722/7134 completed (loss: 0.01521544624119997, acc: 0.9955654144287109)
[2025-02-13 04:22:38,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:38,988][root][INFO] - Training Epoch: 2/2, step 5723/7134 completed (loss: 0.032163891941308975, acc: 0.9909228682518005)
[2025-02-13 04:22:39,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:39,380][root][INFO] - Training Epoch: 2/2, step 5724/7134 completed (loss: 0.021591706201434135, acc: 0.9904000163078308)
[2025-02-13 04:22:39,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:39,807][root][INFO] - Training Epoch: 2/2, step 5725/7134 completed (loss: 0.010842842049896717, acc: 0.9985714554786682)
[2025-02-13 04:22:39,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:40,205][root][INFO] - Training Epoch: 2/2, step 5726/7134 completed (loss: 0.043246712535619736, acc: 0.9923547506332397)
[2025-02-13 04:22:40,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:40,615][root][INFO] - Training Epoch: 2/2, step 5727/7134 completed (loss: 0.011895962990820408, acc: 0.9986631274223328)
[2025-02-13 04:22:40,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:40,989][root][INFO] - Training Epoch: 2/2, step 5728/7134 completed (loss: 0.007717258762568235, acc: 1.0)
[2025-02-13 04:22:41,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:41,374][root][INFO] - Training Epoch: 2/2, step 5729/7134 completed (loss: 0.010016167536377907, acc: 0.996666669845581)
[2025-02-13 04:22:41,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:41,762][root][INFO] - Training Epoch: 2/2, step 5730/7134 completed (loss: 0.05667232722043991, acc: 0.9858585596084595)
[2025-02-13 04:22:41,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:42,191][root][INFO] - Training Epoch: 2/2, step 5731/7134 completed (loss: 0.022894226014614105, acc: 0.9933554530143738)
[2025-02-13 04:22:42,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:42,583][root][INFO] - Training Epoch: 2/2, step 5732/7134 completed (loss: 0.0895029753446579, acc: 0.9786407947540283)
[2025-02-13 04:22:42,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:42,983][root][INFO] - Training Epoch: 2/2, step 5733/7134 completed (loss: 0.024117939174175262, acc: 0.9927140474319458)
[2025-02-13 04:22:43,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:43,384][root][INFO] - Training Epoch: 2/2, step 5734/7134 completed (loss: 0.010241899639368057, acc: 1.0)
[2025-02-13 04:22:43,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:43,780][root][INFO] - Training Epoch: 2/2, step 5735/7134 completed (loss: 0.038591668009757996, acc: 0.9874551892280579)
[2025-02-13 04:22:43,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:44,228][root][INFO] - Training Epoch: 2/2, step 5736/7134 completed (loss: 0.03474106267094612, acc: 0.9864661693572998)
[2025-02-13 04:22:44,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:44,638][root][INFO] - Training Epoch: 2/2, step 5737/7134 completed (loss: 0.03002035617828369, acc: 0.9894259572029114)
[2025-02-13 04:22:44,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:45,081][root][INFO] - Training Epoch: 2/2, step 5738/7134 completed (loss: 0.026627082377672195, acc: 0.9915356636047363)
[2025-02-13 04:22:45,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:45,503][root][INFO] - Training Epoch: 2/2, step 5739/7134 completed (loss: 0.03495173156261444, acc: 0.9831578731536865)
[2025-02-13 04:22:45,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:45,915][root][INFO] - Training Epoch: 2/2, step 5740/7134 completed (loss: 0.02705460786819458, acc: 0.9910485744476318)
[2025-02-13 04:22:46,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:46,332][root][INFO] - Training Epoch: 2/2, step 5741/7134 completed (loss: 0.03425664082169533, acc: 0.9910141229629517)
[2025-02-13 04:22:46,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:46,744][root][INFO] - Training Epoch: 2/2, step 5742/7134 completed (loss: 0.031273189932107925, acc: 0.9881734848022461)
[2025-02-13 04:22:46,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:47,188][root][INFO] - Training Epoch: 2/2, step 5743/7134 completed (loss: 0.01739358715713024, acc: 0.9950980544090271)
[2025-02-13 04:22:47,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:47,558][root][INFO] - Training Epoch: 2/2, step 5744/7134 completed (loss: 0.009092236869037151, acc: 0.9979079365730286)
[2025-02-13 04:22:47,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:48,000][root][INFO] - Training Epoch: 2/2, step 5745/7134 completed (loss: 0.03218566253781319, acc: 0.9896373152732849)
[2025-02-13 04:22:48,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:48,392][root][INFO] - Training Epoch: 2/2, step 5746/7134 completed (loss: 0.05090557038784027, acc: 0.9897058606147766)
[2025-02-13 04:22:48,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:48,806][root][INFO] - Training Epoch: 2/2, step 5747/7134 completed (loss: 0.016757028177380562, acc: 0.9930747747421265)
[2025-02-13 04:22:48,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:49,241][root][INFO] - Training Epoch: 2/2, step 5748/7134 completed (loss: 0.011350351385772228, acc: 0.9945155382156372)
[2025-02-13 04:22:49,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:49,681][root][INFO] - Training Epoch: 2/2, step 5749/7134 completed (loss: 0.029670996591448784, acc: 0.9921466112136841)
[2025-02-13 04:22:49,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:50,103][root][INFO] - Training Epoch: 2/2, step 5750/7134 completed (loss: 0.03576064482331276, acc: 0.9900990128517151)
[2025-02-13 04:22:50,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:50,520][root][INFO] - Training Epoch: 2/2, step 5751/7134 completed (loss: 0.025468196719884872, acc: 0.9884467124938965)
[2025-02-13 04:22:50,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:50,936][root][INFO] - Training Epoch: 2/2, step 5752/7134 completed (loss: 0.057437002658843994, acc: 0.988399088382721)
[2025-02-13 04:22:51,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:51,336][root][INFO] - Training Epoch: 2/2, step 5753/7134 completed (loss: 0.05306325480341911, acc: 0.987500011920929)
[2025-02-13 04:22:51,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:51,800][root][INFO] - Training Epoch: 2/2, step 5754/7134 completed (loss: 0.04288094863295555, acc: 0.9821656346321106)
[2025-02-13 04:22:51,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:52,212][root][INFO] - Training Epoch: 2/2, step 5755/7134 completed (loss: 0.030942311510443687, acc: 0.9882869720458984)
[2025-02-13 04:22:52,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:52,621][root][INFO] - Training Epoch: 2/2, step 5756/7134 completed (loss: 0.03594040870666504, acc: 0.9906914830207825)
[2025-02-13 04:22:52,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:53,054][root][INFO] - Training Epoch: 2/2, step 5757/7134 completed (loss: 0.016569368541240692, acc: 0.9936143159866333)
[2025-02-13 04:22:53,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:53,434][root][INFO] - Training Epoch: 2/2, step 5758/7134 completed (loss: 0.034762781113386154, acc: 0.9882352948188782)
[2025-02-13 04:22:53,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:53,799][root][INFO] - Training Epoch: 2/2, step 5759/7134 completed (loss: 0.02932434156537056, acc: 0.9838274717330933)
[2025-02-13 04:22:53,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:54,226][root][INFO] - Training Epoch: 2/2, step 5760/7134 completed (loss: 0.021869122982025146, acc: 0.9954198598861694)
[2025-02-13 04:22:54,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:54,670][root][INFO] - Training Epoch: 2/2, step 5761/7134 completed (loss: 0.023779183626174927, acc: 0.9926739931106567)
[2025-02-13 04:22:54,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:55,072][root][INFO] - Training Epoch: 2/2, step 5762/7134 completed (loss: 0.05910229682922363, acc: 0.9835766553878784)
[2025-02-13 04:22:55,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:55,501][root][INFO] - Training Epoch: 2/2, step 5763/7134 completed (loss: 0.025525851175189018, acc: 0.9949811697006226)
[2025-02-13 04:22:55,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:55,882][root][INFO] - Training Epoch: 2/2, step 5764/7134 completed (loss: 0.013451329432427883, acc: 0.9948717951774597)
[2025-02-13 04:22:56,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:56,300][root][INFO] - Training Epoch: 2/2, step 5765/7134 completed (loss: 0.012058406136929989, acc: 0.9971988797187805)
[2025-02-13 04:22:56,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:56,712][root][INFO] - Training Epoch: 2/2, step 5766/7134 completed (loss: 0.01872039958834648, acc: 0.9970104694366455)
[2025-02-13 04:22:56,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:57,115][root][INFO] - Training Epoch: 2/2, step 5767/7134 completed (loss: 0.010456056334078312, acc: 0.9958275556564331)
[2025-02-13 04:22:57,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:57,538][root][INFO] - Training Epoch: 2/2, step 5768/7134 completed (loss: 0.01395975612103939, acc: 0.992514967918396)
[2025-02-13 04:22:57,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:57,979][root][INFO] - Training Epoch: 2/2, step 5769/7134 completed (loss: 0.007380844559520483, acc: 0.9967266917228699)
[2025-02-13 04:22:58,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:58,425][root][INFO] - Training Epoch: 2/2, step 5770/7134 completed (loss: 0.031085746362805367, acc: 0.9914320707321167)
[2025-02-13 04:22:58,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:58,843][root][INFO] - Training Epoch: 2/2, step 5771/7134 completed (loss: 0.03695822134613991, acc: 0.98777174949646)
[2025-02-13 04:22:58,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:59,272][root][INFO] - Training Epoch: 2/2, step 5772/7134 completed (loss: 0.04742187634110451, acc: 0.9878261089324951)
[2025-02-13 04:22:59,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:59,706][root][INFO] - Training Epoch: 2/2, step 5773/7134 completed (loss: 0.01461057923734188, acc: 0.9949832558631897)
[2025-02-13 04:22:59,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:00,100][root][INFO] - Training Epoch: 2/2, step 5774/7134 completed (loss: 0.006748704705387354, acc: 0.998516321182251)
[2025-02-13 04:23:00,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:00,500][root][INFO] - Training Epoch: 2/2, step 5775/7134 completed (loss: 0.011458156630396843, acc: 0.9967051148414612)
[2025-02-13 04:23:00,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:00,913][root][INFO] - Training Epoch: 2/2, step 5776/7134 completed (loss: 0.03726363927125931, acc: 0.983146071434021)
[2025-02-13 04:23:01,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:01,352][root][INFO] - Training Epoch: 2/2, step 5777/7134 completed (loss: 0.05512909218668938, acc: 0.9888613820075989)
[2025-02-13 04:23:01,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:01,799][root][INFO] - Training Epoch: 2/2, step 5778/7134 completed (loss: 0.07380715757608414, acc: 0.9807692170143127)
[2025-02-13 04:23:01,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:02,246][root][INFO] - Training Epoch: 2/2, step 5779/7134 completed (loss: 0.03488803654909134, acc: 0.9917469024658203)
[2025-02-13 04:23:02,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:02,702][root][INFO] - Training Epoch: 2/2, step 5780/7134 completed (loss: 0.025261513888835907, acc: 0.9916467666625977)
[2025-02-13 04:23:02,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:03,134][root][INFO] - Training Epoch: 2/2, step 5781/7134 completed (loss: 0.02531472034752369, acc: 0.9930939078330994)
[2025-02-13 04:23:03,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:03,571][root][INFO] - Training Epoch: 2/2, step 5782/7134 completed (loss: 0.010803647339344025, acc: 0.9974093437194824)
[2025-02-13 04:23:03,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:04,008][root][INFO] - Training Epoch: 2/2, step 5783/7134 completed (loss: 0.008231058716773987, acc: 0.9973439574241638)
[2025-02-13 04:23:04,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:04,423][root][INFO] - Training Epoch: 2/2, step 5784/7134 completed (loss: 0.02132343128323555, acc: 0.9946164488792419)
[2025-02-13 04:23:04,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:04,830][root][INFO] - Training Epoch: 2/2, step 5785/7134 completed (loss: 0.022392038255929947, acc: 0.9919137358665466)
[2025-02-13 04:23:04,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:05,220][root][INFO] - Training Epoch: 2/2, step 5786/7134 completed (loss: 0.019303984940052032, acc: 0.9918032884597778)
[2025-02-13 04:23:05,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:05,651][root][INFO] - Training Epoch: 2/2, step 5787/7134 completed (loss: 0.0159672349691391, acc: 0.996268630027771)
[2025-02-13 04:23:05,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:06,044][root][INFO] - Training Epoch: 2/2, step 5788/7134 completed (loss: 0.007335323840379715, acc: 0.9969230890274048)
[2025-02-13 04:23:06,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:06,480][root][INFO] - Training Epoch: 2/2, step 5789/7134 completed (loss: 0.015804655849933624, acc: 0.9937965273857117)
[2025-02-13 04:23:06,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:06,909][root][INFO] - Training Epoch: 2/2, step 5790/7134 completed (loss: 0.025416366755962372, acc: 0.9862637519836426)
[2025-02-13 04:23:07,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:07,330][root][INFO] - Training Epoch: 2/2, step 5791/7134 completed (loss: 0.029414989054203033, acc: 0.9914089441299438)
[2025-02-13 04:23:07,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:07,749][root][INFO] - Training Epoch: 2/2, step 5792/7134 completed (loss: 0.058847274631261826, acc: 0.9864176511764526)
[2025-02-13 04:23:07,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:08,154][root][INFO] - Training Epoch: 2/2, step 5793/7134 completed (loss: 0.06279538571834564, acc: 0.9827044010162354)
[2025-02-13 04:23:08,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:08,556][root][INFO] - Training Epoch: 2/2, step 5794/7134 completed (loss: 0.035313572734594345, acc: 0.9878787994384766)
[2025-02-13 04:23:08,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:08,958][root][INFO] - Training Epoch: 2/2, step 5795/7134 completed (loss: 0.09660089761018753, acc: 0.9747899174690247)
[2025-02-13 04:23:09,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:09,364][root][INFO] - Training Epoch: 2/2, step 5796/7134 completed (loss: 0.025727620348334312, acc: 0.9915397763252258)
[2025-02-13 04:23:09,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:09,776][root][INFO] - Training Epoch: 2/2, step 5797/7134 completed (loss: 0.022604897618293762, acc: 0.9919999837875366)
[2025-02-13 04:23:09,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:10,175][root][INFO] - Training Epoch: 2/2, step 5798/7134 completed (loss: 0.19762735068798065, acc: 0.9599999785423279)
[2025-02-13 04:23:10,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:10,569][root][INFO] - Training Epoch: 2/2, step 5799/7134 completed (loss: 0.07983472943305969, acc: 0.9783037304878235)
[2025-02-13 04:23:10,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:10,970][root][INFO] - Training Epoch: 2/2, step 5800/7134 completed (loss: 0.042046308517456055, acc: 0.9852941036224365)
[2025-02-13 04:23:11,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:11,387][root][INFO] - Training Epoch: 2/2, step 5801/7134 completed (loss: 0.0398281067609787, acc: 0.9878048896789551)
[2025-02-13 04:23:11,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:11,793][root][INFO] - Training Epoch: 2/2, step 5802/7134 completed (loss: 0.02113686501979828, acc: 0.9944547414779663)
[2025-02-13 04:23:11,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:12,236][root][INFO] - Training Epoch: 2/2, step 5803/7134 completed (loss: 0.020184185355901718, acc: 0.9923469424247742)
[2025-02-13 04:23:12,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:12,646][root][INFO] - Training Epoch: 2/2, step 5804/7134 completed (loss: 0.06045146659016609, acc: 0.9797022938728333)
[2025-02-13 04:23:12,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:13,061][root][INFO] - Training Epoch: 2/2, step 5805/7134 completed (loss: 0.03764480724930763, acc: 0.9913669228553772)
[2025-02-13 04:23:13,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:13,487][root][INFO] - Training Epoch: 2/2, step 5806/7134 completed (loss: 0.02870332822203636, acc: 0.9904371500015259)
[2025-02-13 04:23:13,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:13,901][root][INFO] - Training Epoch: 2/2, step 5807/7134 completed (loss: 0.04519083350896835, acc: 0.9863842725753784)
[2025-02-13 04:23:14,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:14,347][root][INFO] - Training Epoch: 2/2, step 5808/7134 completed (loss: 0.021534068509936333, acc: 0.9943740963935852)
[2025-02-13 04:23:14,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:14,742][root][INFO] - Training Epoch: 2/2, step 5809/7134 completed (loss: 0.03111124224960804, acc: 0.9934210777282715)
[2025-02-13 04:23:14,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:15,156][root][INFO] - Training Epoch: 2/2, step 5810/7134 completed (loss: 0.019763199612498283, acc: 0.9898580312728882)
[2025-02-13 04:23:15,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:15,464][root][INFO] - Training Epoch: 2/2, step 5811/7134 completed (loss: 0.03390007093548775, acc: 0.983132541179657)
[2025-02-13 04:23:15,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:15,848][root][INFO] - Training Epoch: 2/2, step 5812/7134 completed (loss: 0.012317797169089317, acc: 0.9966996908187866)
[2025-02-13 04:23:15,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:16,241][root][INFO] - Training Epoch: 2/2, step 5813/7134 completed (loss: 0.017025193199515343, acc: 0.994339644908905)
[2025-02-13 04:23:16,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:16,707][root][INFO] - Training Epoch: 2/2, step 5814/7134 completed (loss: 0.010805507190525532, acc: 0.9966158866882324)
[2025-02-13 04:23:16,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:17,116][root][INFO] - Training Epoch: 2/2, step 5815/7134 completed (loss: 0.019820794463157654, acc: 0.9962335228919983)
[2025-02-13 04:23:17,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:17,550][root][INFO] - Training Epoch: 2/2, step 5816/7134 completed (loss: 0.007043363526463509, acc: 0.9975062608718872)
[2025-02-13 04:23:17,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:17,948][root][INFO] - Training Epoch: 2/2, step 5817/7134 completed (loss: 0.044113192707300186, acc: 0.989924430847168)
[2025-02-13 04:23:18,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:18,359][root][INFO] - Training Epoch: 2/2, step 5818/7134 completed (loss: 0.030018487945199013, acc: 0.9866666793823242)
[2025-02-13 04:23:18,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:18,749][root][INFO] - Training Epoch: 2/2, step 5819/7134 completed (loss: 0.022599201649427414, acc: 0.9948275685310364)
[2025-02-13 04:23:18,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:19,120][root][INFO] - Training Epoch: 2/2, step 5820/7134 completed (loss: 0.03812922537326813, acc: 0.9876033067703247)
[2025-02-13 04:23:19,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:19,511][root][INFO] - Training Epoch: 2/2, step 5821/7134 completed (loss: 0.01872166059911251, acc: 0.9938367009162903)
[2025-02-13 04:23:19,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:19,913][root][INFO] - Training Epoch: 2/2, step 5822/7134 completed (loss: 0.024703269824385643, acc: 0.9919354915618896)
[2025-02-13 04:23:20,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:20,285][root][INFO] - Training Epoch: 2/2, step 5823/7134 completed (loss: 0.018283164128661156, acc: 0.9922239780426025)
[2025-02-13 04:23:20,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:20,709][root][INFO] - Training Epoch: 2/2, step 5824/7134 completed (loss: 0.012950742617249489, acc: 0.9984447956085205)
[2025-02-13 04:23:20,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:21,126][root][INFO] - Training Epoch: 2/2, step 5825/7134 completed (loss: 0.04419935122132301, acc: 0.991349458694458)
[2025-02-13 04:23:21,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:21,539][root][INFO] - Training Epoch: 2/2, step 5826/7134 completed (loss: 0.018831821158528328, acc: 0.9929078221321106)
[2025-02-13 04:23:21,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:21,926][root][INFO] - Training Epoch: 2/2, step 5827/7134 completed (loss: 0.009150141850113869, acc: 0.9961538314819336)
[2025-02-13 04:23:22,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:22,338][root][INFO] - Training Epoch: 2/2, step 5828/7134 completed (loss: 0.024027667939662933, acc: 0.9910846948623657)
[2025-02-13 04:23:22,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:22,761][root][INFO] - Training Epoch: 2/2, step 5829/7134 completed (loss: 0.02014761045575142, acc: 0.9934123754501343)
[2025-02-13 04:23:22,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:23,132][root][INFO] - Training Epoch: 2/2, step 5830/7134 completed (loss: 0.018824689090251923, acc: 0.996219277381897)
[2025-02-13 04:23:23,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:23,516][root][INFO] - Training Epoch: 2/2, step 5831/7134 completed (loss: 0.012704158201813698, acc: 0.9976744055747986)
[2025-02-13 04:23:23,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:23,932][root][INFO] - Training Epoch: 2/2, step 5832/7134 completed (loss: 0.047431424260139465, acc: 0.9898648858070374)
[2025-02-13 04:23:24,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:24,308][root][INFO] - Training Epoch: 2/2, step 5833/7134 completed (loss: 0.01812705770134926, acc: 0.9928315281867981)
[2025-02-13 04:23:24,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:24,720][root][INFO] - Training Epoch: 2/2, step 5834/7134 completed (loss: 0.021675121039152145, acc: 0.9958333373069763)
[2025-02-13 04:23:24,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:25,132][root][INFO] - Training Epoch: 2/2, step 5835/7134 completed (loss: 0.01032042596489191, acc: 0.9971098303794861)
[2025-02-13 04:23:25,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:25,548][root][INFO] - Training Epoch: 2/2, step 5836/7134 completed (loss: 0.02144927717745304, acc: 0.9927954077720642)
[2025-02-13 04:23:25,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:25,954][root][INFO] - Training Epoch: 2/2, step 5837/7134 completed (loss: 0.05347442999482155, acc: 0.9915540814399719)
[2025-02-13 04:23:26,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:26,370][root][INFO] - Training Epoch: 2/2, step 5838/7134 completed (loss: 0.03739301860332489, acc: 0.9934102296829224)
[2025-02-13 04:23:26,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:26,775][root][INFO] - Training Epoch: 2/2, step 5839/7134 completed (loss: 0.01747557520866394, acc: 0.9928571581840515)
[2025-02-13 04:23:26,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:27,174][root][INFO] - Training Epoch: 2/2, step 5840/7134 completed (loss: 0.013608042150735855, acc: 0.995230495929718)
[2025-02-13 04:23:27,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:27,583][root][INFO] - Training Epoch: 2/2, step 5841/7134 completed (loss: 0.026025988161563873, acc: 0.9888357520103455)
[2025-02-13 04:23:27,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:27,983][root][INFO] - Training Epoch: 2/2, step 5842/7134 completed (loss: 0.016110669821500778, acc: 0.9927536249160767)
[2025-02-13 04:23:28,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:28,418][root][INFO] - Training Epoch: 2/2, step 5843/7134 completed (loss: 0.029291324317455292, acc: 0.993306577205658)
[2025-02-13 04:23:28,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:28,837][root][INFO] - Training Epoch: 2/2, step 5844/7134 completed (loss: 0.009241453371942043, acc: 0.9985775351524353)
[2025-02-13 04:23:28,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:29,244][root][INFO] - Training Epoch: 2/2, step 5845/7134 completed (loss: 0.01571241207420826, acc: 0.995726466178894)
[2025-02-13 04:23:29,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:29,662][root][INFO] - Training Epoch: 2/2, step 5846/7134 completed (loss: 0.029049070551991463, acc: 0.9917920827865601)
[2025-02-13 04:23:29,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:30,087][root][INFO] - Training Epoch: 2/2, step 5847/7134 completed (loss: 0.009101797826588154, acc: 0.9972527623176575)
[2025-02-13 04:23:30,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:30,483][root][INFO] - Training Epoch: 2/2, step 5848/7134 completed (loss: 0.018838271498680115, acc: 0.9910714030265808)
[2025-02-13 04:23:30,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:30,874][root][INFO] - Training Epoch: 2/2, step 5849/7134 completed (loss: 0.031203968450427055, acc: 0.9896755218505859)
[2025-02-13 04:23:31,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:31,268][root][INFO] - Training Epoch: 2/2, step 5850/7134 completed (loss: 0.013684304431080818, acc: 0.9938931465148926)
[2025-02-13 04:23:31,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:31,652][root][INFO] - Training Epoch: 2/2, step 5851/7134 completed (loss: 0.020566027611494064, acc: 0.9934210777282715)
[2025-02-13 04:23:31,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:32,059][root][INFO] - Training Epoch: 2/2, step 5852/7134 completed (loss: 0.010490010492503643, acc: 0.9956204295158386)
[2025-02-13 04:23:32,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:32,477][root][INFO] - Training Epoch: 2/2, step 5853/7134 completed (loss: 0.03831794112920761, acc: 0.9918808937072754)
[2025-02-13 04:23:32,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:32,876][root][INFO] - Training Epoch: 2/2, step 5854/7134 completed (loss: 0.01673380471765995, acc: 0.991150438785553)
[2025-02-13 04:23:32,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:33,259][root][INFO] - Training Epoch: 2/2, step 5855/7134 completed (loss: 0.032196491956710815, acc: 0.9896373152732849)
[2025-02-13 04:23:33,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:33,673][root][INFO] - Training Epoch: 2/2, step 5856/7134 completed (loss: 0.05301491171121597, acc: 0.9833837151527405)
[2025-02-13 04:23:33,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:34,119][root][INFO] - Training Epoch: 2/2, step 5857/7134 completed (loss: 0.021376147866249084, acc: 0.9921011328697205)
[2025-02-13 04:23:34,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:34,505][root][INFO] - Training Epoch: 2/2, step 5858/7134 completed (loss: 0.011543326079845428, acc: 0.9981516003608704)
[2025-02-13 04:23:34,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:34,910][root][INFO] - Training Epoch: 2/2, step 5859/7134 completed (loss: 0.027723677456378937, acc: 0.994140625)
[2025-02-13 04:23:35,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:35,299][root][INFO] - Training Epoch: 2/2, step 5860/7134 completed (loss: 0.025160040706396103, acc: 0.9889807105064392)
[2025-02-13 04:23:35,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:35,680][root][INFO] - Training Epoch: 2/2, step 5861/7134 completed (loss: 0.01332424208521843, acc: 0.9959431886672974)
[2025-02-13 04:23:35,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:36,088][root][INFO] - Training Epoch: 2/2, step 5862/7134 completed (loss: 0.015562399290502071, acc: 0.9935622215270996)
[2025-02-13 04:23:36,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:36,420][root][INFO] - Training Epoch: 2/2, step 5863/7134 completed (loss: 0.03717225417494774, acc: 0.9924242496490479)
[2025-02-13 04:23:36,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:36,835][root][INFO] - Training Epoch: 2/2, step 5864/7134 completed (loss: 0.05324126034975052, acc: 0.985029935836792)
[2025-02-13 04:23:36,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:37,233][root][INFO] - Training Epoch: 2/2, step 5865/7134 completed (loss: 0.0410696305334568, acc: 0.9904761910438538)
[2025-02-13 04:23:37,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:37,681][root][INFO] - Training Epoch: 2/2, step 5866/7134 completed (loss: 0.013977430760860443, acc: 0.9933862686157227)
[2025-02-13 04:23:37,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:38,089][root][INFO] - Training Epoch: 2/2, step 5867/7134 completed (loss: 0.008353758603334427, acc: 1.0)
[2025-02-13 04:23:38,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:38,505][root][INFO] - Training Epoch: 2/2, step 5868/7134 completed (loss: 0.033490508794784546, acc: 0.9879310131072998)
[2025-02-13 04:23:38,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:38,897][root][INFO] - Training Epoch: 2/2, step 5869/7134 completed (loss: 0.04443112015724182, acc: 0.9895522594451904)
[2025-02-13 04:23:39,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:39,305][root][INFO] - Training Epoch: 2/2, step 5870/7134 completed (loss: 0.05712268128991127, acc: 0.9895397424697876)
[2025-02-13 04:23:39,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:39,685][root][INFO] - Training Epoch: 2/2, step 5871/7134 completed (loss: 0.02300211787223816, acc: 0.9915074110031128)
[2025-02-13 04:23:39,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:40,087][root][INFO] - Training Epoch: 2/2, step 5872/7134 completed (loss: 0.028735823929309845, acc: 0.9878234267234802)
[2025-02-13 04:23:40,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:40,579][root][INFO] - Training Epoch: 2/2, step 5873/7134 completed (loss: 0.03144676610827446, acc: 0.9906666874885559)
[2025-02-13 04:23:40,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:40,912][root][INFO] - Training Epoch: 2/2, step 5874/7134 completed (loss: 0.010008284822106361, acc: 0.9959839582443237)
[2025-02-13 04:23:41,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:41,252][root][INFO] - Training Epoch: 2/2, step 5875/7134 completed (loss: 0.036733172833919525, acc: 0.9840707778930664)
[2025-02-13 04:23:41,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:41,683][root][INFO] - Training Epoch: 2/2, step 5876/7134 completed (loss: 0.007614989299327135, acc: 0.9975728392601013)
[2025-02-13 04:23:41,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:42,095][root][INFO] - Training Epoch: 2/2, step 5877/7134 completed (loss: 0.030726296827197075, acc: 0.9911167621612549)
[2025-02-13 04:23:42,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:42,515][root][INFO] - Training Epoch: 2/2, step 5878/7134 completed (loss: 0.016750048846006393, acc: 0.9956140518188477)
[2025-02-13 04:23:42,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:42,906][root][INFO] - Training Epoch: 2/2, step 5879/7134 completed (loss: 0.006192687898874283, acc: 1.0)
[2025-02-13 04:23:42,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:43,158][root][INFO] - Training Epoch: 2/2, step 5880/7134 completed (loss: 0.008854378946125507, acc: 0.995555579662323)
[2025-02-13 04:23:43,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:43,561][root][INFO] - Training Epoch: 2/2, step 5881/7134 completed (loss: 0.008797132410109043, acc: 0.9980806112289429)
[2025-02-13 04:23:43,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:43,850][root][INFO] - Training Epoch: 2/2, step 5882/7134 completed (loss: 0.013670274056494236, acc: 0.9976470470428467)
[2025-02-13 04:23:43,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:44,182][root][INFO] - Training Epoch: 2/2, step 5883/7134 completed (loss: 0.022978391498327255, acc: 0.9906367063522339)
[2025-02-13 04:23:44,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:44,590][root][INFO] - Training Epoch: 2/2, step 5884/7134 completed (loss: 0.04090367257595062, acc: 0.9817517995834351)
[2025-02-13 04:23:44,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:44,971][root][INFO] - Training Epoch: 2/2, step 5885/7134 completed (loss: 0.06840000301599503, acc: 0.98097825050354)
[2025-02-13 04:23:45,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:45,298][root][INFO] - Training Epoch: 2/2, step 5886/7134 completed (loss: 0.010410779155790806, acc: 0.9971428513526917)
[2025-02-13 04:23:45,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:45,690][root][INFO] - Training Epoch: 2/2, step 5887/7134 completed (loss: 0.026951296254992485, acc: 0.9915074110031128)
[2025-02-13 04:23:45,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:46,094][root][INFO] - Training Epoch: 2/2, step 5888/7134 completed (loss: 0.05437570810317993, acc: 0.9864864945411682)
[2025-02-13 04:23:46,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:46,313][root][INFO] - Training Epoch: 2/2, step 5889/7134 completed (loss: 0.012084313668310642, acc: 0.9955357313156128)
[2025-02-13 04:23:46,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:46,744][root][INFO] - Training Epoch: 2/2, step 5890/7134 completed (loss: 0.014953126199543476, acc: 0.9947552680969238)
[2025-02-13 04:23:46,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:47,161][root][INFO] - Training Epoch: 2/2, step 5891/7134 completed (loss: 0.029599342495203018, acc: 0.9871086478233337)
[2025-02-13 04:23:47,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:47,604][root][INFO] - Training Epoch: 2/2, step 5892/7134 completed (loss: 0.04495837166905403, acc: 0.9927113652229309)
[2025-02-13 04:23:47,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:48,014][root][INFO] - Training Epoch: 2/2, step 5893/7134 completed (loss: 0.03917212784290314, acc: 0.9889763593673706)
[2025-02-13 04:23:48,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:48,344][root][INFO] - Training Epoch: 2/2, step 5894/7134 completed (loss: 0.003550437279045582, acc: 1.0)
[2025-02-13 04:23:48,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:48,739][root][INFO] - Training Epoch: 2/2, step 5895/7134 completed (loss: 0.012739084661006927, acc: 0.9952380657196045)
[2025-02-13 04:23:48,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:49,150][root][INFO] - Training Epoch: 2/2, step 5896/7134 completed (loss: 0.03546057268977165, acc: 0.988095223903656)
[2025-02-13 04:23:49,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:49,583][root][INFO] - Training Epoch: 2/2, step 5897/7134 completed (loss: 0.010734041221439838, acc: 0.9958677887916565)
[2025-02-13 04:23:49,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:49,983][root][INFO] - Training Epoch: 2/2, step 5898/7134 completed (loss: 0.006240937393158674, acc: 1.0)
[2025-02-13 04:23:50,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:50,236][root][INFO] - Training Epoch: 2/2, step 5899/7134 completed (loss: 0.01165834441781044, acc: 1.0)
[2025-02-13 04:23:50,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:50,526][root][INFO] - Training Epoch: 2/2, step 5900/7134 completed (loss: 0.011529333889484406, acc: 0.9940476417541504)
[2025-02-13 04:23:50,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:50,839][root][INFO] - Training Epoch: 2/2, step 5901/7134 completed (loss: 0.03784044459462166, acc: 0.9900000095367432)
[2025-02-13 04:23:50,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:51,265][root][INFO] - Training Epoch: 2/2, step 5902/7134 completed (loss: 0.042454905807971954, acc: 0.9793814420700073)
[2025-02-13 04:23:51,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:51,684][root][INFO] - Training Epoch: 2/2, step 5903/7134 completed (loss: 0.031025517731904984, acc: 0.9908758997917175)
[2025-02-13 04:23:51,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:52,126][root][INFO] - Training Epoch: 2/2, step 5904/7134 completed (loss: 0.028465567156672478, acc: 0.9904371500015259)
[2025-02-13 04:23:52,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:52,518][root][INFO] - Training Epoch: 2/2, step 5905/7134 completed (loss: 0.02729869820177555, acc: 0.9916550517082214)
[2025-02-13 04:23:52,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:52,966][root][INFO] - Training Epoch: 2/2, step 5906/7134 completed (loss: 0.05301971361041069, acc: 0.9828009605407715)
[2025-02-13 04:23:53,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:53,439][root][INFO] - Training Epoch: 2/2, step 5907/7134 completed (loss: 0.039290349930524826, acc: 0.9870634078979492)
[2025-02-13 04:23:53,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:53,909][root][INFO] - Training Epoch: 2/2, step 5908/7134 completed (loss: 0.020074276253581047, acc: 0.9936061501502991)
[2025-02-13 04:23:54,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:54,483][root][INFO] - Training Epoch: 2/2, step 5909/7134 completed (loss: 0.025189192965626717, acc: 0.9870316982269287)
[2025-02-13 04:23:54,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:54,970][root][INFO] - Training Epoch: 2/2, step 5910/7134 completed (loss: 0.04300997406244278, acc: 0.9833564758300781)
[2025-02-13 04:23:55,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:55,384][root][INFO] - Training Epoch: 2/2, step 5911/7134 completed (loss: 0.03526252135634422, acc: 0.9933884143829346)
[2025-02-13 04:23:55,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:55,769][root][INFO] - Training Epoch: 2/2, step 5912/7134 completed (loss: 0.020711589604616165, acc: 0.9887482523918152)
[2025-02-13 04:23:55,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:56,224][root][INFO] - Training Epoch: 2/2, step 5913/7134 completed (loss: 0.07462568581104279, acc: 0.979619562625885)
[2025-02-13 04:23:56,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:56,607][root][INFO] - Training Epoch: 2/2, step 5914/7134 completed (loss: 0.017860379070043564, acc: 0.9967897534370422)
[2025-02-13 04:23:56,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:57,056][root][INFO] - Training Epoch: 2/2, step 5915/7134 completed (loss: 0.024310356006026268, acc: 0.9953488111495972)
[2025-02-13 04:23:57,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:57,510][root][INFO] - Training Epoch: 2/2, step 5916/7134 completed (loss: 0.034047842025756836, acc: 0.9921362996101379)
[2025-02-13 04:23:57,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:57,884][root][INFO] - Training Epoch: 2/2, step 5917/7134 completed (loss: 0.035720568150281906, acc: 0.9913344979286194)
[2025-02-13 04:23:58,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:58,329][root][INFO] - Training Epoch: 2/2, step 5918/7134 completed (loss: 0.07804016768932343, acc: 0.9796807169914246)
[2025-02-13 04:23:58,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:58,741][root][INFO] - Training Epoch: 2/2, step 5919/7134 completed (loss: 0.03236501291394234, acc: 0.9924012422561646)
[2025-02-13 04:23:58,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:59,144][root][INFO] - Training Epoch: 2/2, step 5920/7134 completed (loss: 0.024968380108475685, acc: 0.9950082898139954)
[2025-02-13 04:23:59,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:59,549][root][INFO] - Training Epoch: 2/2, step 5921/7134 completed (loss: 0.017411252483725548, acc: 0.9937369227409363)
[2025-02-13 04:23:59,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:59,967][root][INFO] - Training Epoch: 2/2, step 5922/7134 completed (loss: 0.12969185411930084, acc: 0.9769503474235535)
[2025-02-13 04:24:00,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:00,414][root][INFO] - Training Epoch: 2/2, step 5923/7134 completed (loss: 0.05614839494228363, acc: 0.9897511005401611)
[2025-02-13 04:24:00,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:00,822][root][INFO] - Training Epoch: 2/2, step 5924/7134 completed (loss: 0.03356501832604408, acc: 0.9880668520927429)
[2025-02-13 04:24:00,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:01,162][root][INFO] - Training Epoch: 2/2, step 5925/7134 completed (loss: 0.15740495920181274, acc: 0.9677419066429138)
[2025-02-13 04:24:01,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:01,560][root][INFO] - Training Epoch: 2/2, step 5926/7134 completed (loss: 0.06386227160692215, acc: 0.9783333539962769)
[2025-02-13 04:24:01,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:01,952][root][INFO] - Training Epoch: 2/2, step 5927/7134 completed (loss: 0.06699817627668381, acc: 0.9799554347991943)
[2025-02-13 04:24:02,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:02,257][root][INFO] - Training Epoch: 2/2, step 5928/7134 completed (loss: 0.15432921051979065, acc: 0.9554794430732727)
[2025-02-13 04:24:02,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:02,666][root][INFO] - Training Epoch: 2/2, step 5929/7134 completed (loss: 0.08492319285869598, acc: 0.9776119589805603)
[2025-02-13 04:24:02,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:03,027][root][INFO] - Training Epoch: 2/2, step 5930/7134 completed (loss: 0.05796302109956741, acc: 0.9888641238212585)
[2025-02-13 04:24:03,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:03,420][root][INFO] - Training Epoch: 2/2, step 5931/7134 completed (loss: 0.03180629014968872, acc: 0.9913606643676758)
[2025-02-13 04:24:03,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:03,815][root][INFO] - Training Epoch: 2/2, step 5932/7134 completed (loss: 0.1005912721157074, acc: 0.9685184955596924)
[2025-02-13 04:24:03,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:04,162][root][INFO] - Training Epoch: 2/2, step 5933/7134 completed (loss: 0.09885936975479126, acc: 0.9748427867889404)
[2025-02-13 04:24:04,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:04,562][root][INFO] - Training Epoch: 2/2, step 5934/7134 completed (loss: 0.04487404227256775, acc: 0.9875665903091431)
[2025-02-13 04:24:04,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:05,036][root][INFO] - Training Epoch: 2/2, step 5935/7134 completed (loss: 0.05001504719257355, acc: 0.9864077568054199)
[2025-02-13 04:24:05,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:05,418][root][INFO] - Training Epoch: 2/2, step 5936/7134 completed (loss: 0.054542507976293564, acc: 0.9846860766410828)
[2025-02-13 04:24:05,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:05,828][root][INFO] - Training Epoch: 2/2, step 5937/7134 completed (loss: 0.022823719307780266, acc: 0.9952152967453003)
[2025-02-13 04:24:05,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:06,227][root][INFO] - Training Epoch: 2/2, step 5938/7134 completed (loss: 0.08715575933456421, acc: 0.9770554304122925)
[2025-02-13 04:24:06,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:06,582][root][INFO] - Training Epoch: 2/2, step 5939/7134 completed (loss: 0.08278045803308487, acc: 0.9773755669593811)
[2025-02-13 04:24:06,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:07,031][root][INFO] - Training Epoch: 2/2, step 5940/7134 completed (loss: 0.04294956102967262, acc: 0.9897959232330322)
[2025-02-13 04:24:07,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:07,395][root][INFO] - Training Epoch: 2/2, step 5941/7134 completed (loss: 0.04953126236796379, acc: 0.9906250238418579)
[2025-02-13 04:24:07,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:07,796][root][INFO] - Training Epoch: 2/2, step 5942/7134 completed (loss: 0.013908599503338337, acc: 0.9961758852005005)
[2025-02-13 04:24:07,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:08,200][root][INFO] - Training Epoch: 2/2, step 5943/7134 completed (loss: 0.01218909677118063, acc: 1.0)
[2025-02-13 04:24:08,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:08,644][root][INFO] - Training Epoch: 2/2, step 5944/7134 completed (loss: 0.04510863125324249, acc: 0.9829843044281006)
[2025-02-13 04:24:08,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:09,097][root][INFO] - Training Epoch: 2/2, step 5945/7134 completed (loss: 0.03410938009619713, acc: 0.9879879951477051)
[2025-02-13 04:24:09,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:09,499][root][INFO] - Training Epoch: 2/2, step 5946/7134 completed (loss: 0.02859441563487053, acc: 0.9927536249160767)
[2025-02-13 04:24:09,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:09,934][root][INFO] - Training Epoch: 2/2, step 5947/7134 completed (loss: 0.02929222397506237, acc: 0.9900867342948914)
[2025-02-13 04:24:10,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:10,349][root][INFO] - Training Epoch: 2/2, step 5948/7134 completed (loss: 0.060740575194358826, acc: 0.9760000109672546)
[2025-02-13 04:24:10,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:10,780][root][INFO] - Training Epoch: 2/2, step 5949/7134 completed (loss: 0.00998308788985014, acc: 0.998745322227478)
[2025-02-13 04:24:10,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:11,181][root][INFO] - Training Epoch: 2/2, step 5950/7134 completed (loss: 0.07467019557952881, acc: 0.978622317314148)
[2025-02-13 04:24:11,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:11,593][root][INFO] - Training Epoch: 2/2, step 5951/7134 completed (loss: 0.04022430628538132, acc: 0.9856528043746948)
[2025-02-13 04:24:11,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:12,046][root][INFO] - Training Epoch: 2/2, step 5952/7134 completed (loss: 0.15521949529647827, acc: 0.9671875238418579)
[2025-02-13 04:24:12,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:12,464][root][INFO] - Training Epoch: 2/2, step 5953/7134 completed (loss: 0.03002876788377762, acc: 0.9917355179786682)
[2025-02-13 04:24:12,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:12,865][root][INFO] - Training Epoch: 2/2, step 5954/7134 completed (loss: 0.022849271073937416, acc: 0.9923858046531677)
[2025-02-13 04:24:12,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:13,107][root][INFO] - Training Epoch: 2/2, step 5955/7134 completed (loss: 0.03185727447271347, acc: 0.9952380657196045)
[2025-02-13 04:24:13,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:13,492][root][INFO] - Training Epoch: 2/2, step 5956/7134 completed (loss: 0.047869134694337845, acc: 0.9862744808197021)
[2025-02-13 04:24:13,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:13,875][root][INFO] - Training Epoch: 2/2, step 5957/7134 completed (loss: 0.051159877330064774, acc: 0.9880059957504272)
[2025-02-13 04:24:14,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:14,297][root][INFO] - Training Epoch: 2/2, step 5958/7134 completed (loss: 0.029627008363604546, acc: 0.990604043006897)
[2025-02-13 04:24:14,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:14,732][root][INFO] - Training Epoch: 2/2, step 5959/7134 completed (loss: 0.05501844733953476, acc: 0.9879840016365051)
[2025-02-13 04:24:14,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:15,166][root][INFO] - Training Epoch: 2/2, step 5960/7134 completed (loss: 0.010324235074222088, acc: 0.9957864880561829)
[2025-02-13 04:24:15,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:15,517][root][INFO] - Training Epoch: 2/2, step 5961/7134 completed (loss: 0.027947822585701942, acc: 0.9959016442298889)
[2025-02-13 04:24:15,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:15,929][root][INFO] - Training Epoch: 2/2, step 5962/7134 completed (loss: 0.006950421258807182, acc: 0.9983739852905273)
[2025-02-13 04:24:16,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:16,342][root][INFO] - Training Epoch: 2/2, step 5963/7134 completed (loss: 0.021284397691488266, acc: 0.99301677942276)
[2025-02-13 04:24:16,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:16,740][root][INFO] - Training Epoch: 2/2, step 5964/7134 completed (loss: 0.009446796029806137, acc: 0.996303141117096)
[2025-02-13 04:24:16,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:17,156][root][INFO] - Training Epoch: 2/2, step 5965/7134 completed (loss: 0.02437475509941578, acc: 0.9940029978752136)
[2025-02-13 04:24:17,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:17,582][root][INFO] - Training Epoch: 2/2, step 5966/7134 completed (loss: 0.03929679095745087, acc: 0.9889042973518372)
[2025-02-13 04:24:17,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:18,009][root][INFO] - Training Epoch: 2/2, step 5967/7134 completed (loss: 0.019492559134960175, acc: 0.9925373196601868)
[2025-02-13 04:24:18,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:18,408][root][INFO] - Training Epoch: 2/2, step 5968/7134 completed (loss: 0.04182201996445656, acc: 0.9885844588279724)
[2025-02-13 04:24:18,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:18,812][root][INFO] - Training Epoch: 2/2, step 5969/7134 completed (loss: 0.025175318121910095, acc: 0.995356023311615)
[2025-02-13 04:24:18,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:19,216][root][INFO] - Training Epoch: 2/2, step 5970/7134 completed (loss: 0.04847213625907898, acc: 0.993852436542511)
[2025-02-13 04:24:19,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:19,646][root][INFO] - Training Epoch: 2/2, step 5971/7134 completed (loss: 0.01284104771912098, acc: 0.9962406158447266)
[2025-02-13 04:24:19,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:20,072][root][INFO] - Training Epoch: 2/2, step 5972/7134 completed (loss: 0.06921173632144928, acc: 0.988252580165863)
[2025-02-13 04:24:20,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:20,537][root][INFO] - Training Epoch: 2/2, step 5973/7134 completed (loss: 0.06972047686576843, acc: 0.9798561334609985)
[2025-02-13 04:24:20,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:20,992][root][INFO] - Training Epoch: 2/2, step 5974/7134 completed (loss: 0.016085246577858925, acc: 0.9930675625801086)
[2025-02-13 04:24:21,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:21,374][root][INFO] - Training Epoch: 2/2, step 5975/7134 completed (loss: 0.00931225810199976, acc: 0.9970501661300659)
[2025-02-13 04:24:21,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:21,812][root][INFO] - Training Epoch: 2/2, step 5976/7134 completed (loss: 0.014411192387342453, acc: 0.9938650131225586)
[2025-02-13 04:24:21,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:22,253][root][INFO] - Training Epoch: 2/2, step 5977/7134 completed (loss: 0.014514326117932796, acc: 0.9962825179100037)
[2025-02-13 04:24:22,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:22,687][root][INFO] - Training Epoch: 2/2, step 5978/7134 completed (loss: 0.020908523350954056, acc: 0.9967948794364929)
[2025-02-13 04:24:22,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:23,002][root][INFO] - Training Epoch: 2/2, step 5979/7134 completed (loss: 0.022701501846313477, acc: 0.9925925731658936)
[2025-02-13 04:24:23,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:23,434][root][INFO] - Training Epoch: 2/2, step 5980/7134 completed (loss: 0.013753730803728104, acc: 0.99615877866745)
[2025-02-13 04:24:23,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:23,885][root][INFO] - Training Epoch: 2/2, step 5981/7134 completed (loss: 0.013022775761783123, acc: 0.9976878762245178)
[2025-02-13 04:24:24,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:24,274][root][INFO] - Training Epoch: 2/2, step 5982/7134 completed (loss: 0.03014860674738884, acc: 0.9920477271080017)
[2025-02-13 04:24:24,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:24,695][root][INFO] - Training Epoch: 2/2, step 5983/7134 completed (loss: 0.038164086639881134, acc: 0.987500011920929)
[2025-02-13 04:24:24,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:25,127][root][INFO] - Training Epoch: 2/2, step 5984/7134 completed (loss: 0.025197455659508705, acc: 0.9914893507957458)
[2025-02-13 04:24:25,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:25,573][root][INFO] - Training Epoch: 2/2, step 5985/7134 completed (loss: 0.027359534054994583, acc: 0.9949937462806702)
[2025-02-13 04:24:25,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:26,032][root][INFO] - Training Epoch: 2/2, step 5986/7134 completed (loss: 0.03845968842506409, acc: 0.9885350465774536)
[2025-02-13 04:24:26,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:26,479][root][INFO] - Training Epoch: 2/2, step 5987/7134 completed (loss: 0.04578445851802826, acc: 0.9860140085220337)
[2025-02-13 04:24:26,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:26,900][root][INFO] - Training Epoch: 2/2, step 5988/7134 completed (loss: 0.02127453312277794, acc: 0.9933333396911621)
[2025-02-13 04:24:27,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:27,347][root][INFO] - Training Epoch: 2/2, step 5989/7134 completed (loss: 0.030839532613754272, acc: 0.9889298677444458)
[2025-02-13 04:24:27,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:27,811][root][INFO] - Training Epoch: 2/2, step 5990/7134 completed (loss: 0.022934671491384506, acc: 0.9931507110595703)
[2025-02-13 04:24:27,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:28,080][root][INFO] - Training Epoch: 2/2, step 5991/7134 completed (loss: 0.11304152011871338, acc: 0.9804469347000122)
[2025-02-13 04:24:28,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:28,562][root][INFO] - Training Epoch: 2/2, step 5992/7134 completed (loss: 0.01507414598017931, acc: 0.996515691280365)
[2025-02-13 04:24:28,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:28,979][root][INFO] - Training Epoch: 2/2, step 5993/7134 completed (loss: 0.03217807039618492, acc: 0.9902912378311157)
[2025-02-13 04:24:29,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:29,361][root][INFO] - Training Epoch: 2/2, step 5994/7134 completed (loss: 0.018807969987392426, acc: 0.9942965507507324)
[2025-02-13 04:24:29,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:29,814][root][INFO] - Training Epoch: 2/2, step 5995/7134 completed (loss: 0.006394159514456987, acc: 0.9985915422439575)
[2025-02-13 04:24:29,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:30,248][root][INFO] - Training Epoch: 2/2, step 5996/7134 completed (loss: 0.048022542148828506, acc: 0.9890453815460205)
[2025-02-13 04:24:30,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:30,680][root][INFO] - Training Epoch: 2/2, step 5997/7134 completed (loss: 0.037858862429857254, acc: 0.9933244585990906)
[2025-02-13 04:24:30,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:31,115][root][INFO] - Training Epoch: 2/2, step 5998/7134 completed (loss: 0.028120899572968483, acc: 0.9937655925750732)
[2025-02-13 04:24:31,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:31,514][root][INFO] - Training Epoch: 2/2, step 5999/7134 completed (loss: 0.022881530225276947, acc: 0.9938042163848877)
[2025-02-13 04:24:31,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:31,905][root][INFO] - Training Epoch: 2/2, step 6000/7134 completed (loss: 0.022050591185688972, acc: 0.9900166392326355)
[2025-02-13 04:24:32,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:32,343][root][INFO] - Training Epoch: 2/2, step 6001/7134 completed (loss: 0.00855981931090355, acc: 1.0)
[2025-02-13 04:24:32,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:32,754][root][INFO] - Training Epoch: 2/2, step 6002/7134 completed (loss: 0.04267486557364464, acc: 0.9863547682762146)
[2025-02-13 04:24:32,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:33,210][root][INFO] - Training Epoch: 2/2, step 6003/7134 completed (loss: 0.022363383322954178, acc: 0.9911894202232361)
[2025-02-13 04:24:33,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:33,632][root][INFO] - Training Epoch: 2/2, step 6004/7134 completed (loss: 0.03480665385723114, acc: 0.9916550517082214)
[2025-02-13 04:24:33,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:34,031][root][INFO] - Training Epoch: 2/2, step 6005/7134 completed (loss: 0.024716738611459732, acc: 0.9893454909324646)
[2025-02-13 04:24:34,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:34,445][root][INFO] - Training Epoch: 2/2, step 6006/7134 completed (loss: 0.039153803139925, acc: 0.9864603281021118)
[2025-02-13 04:24:34,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:34,888][root][INFO] - Training Epoch: 2/2, step 6007/7134 completed (loss: 0.01654931902885437, acc: 0.9966611266136169)
[2025-02-13 04:24:35,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:35,319][root][INFO] - Training Epoch: 2/2, step 6008/7134 completed (loss: 0.03304154798388481, acc: 0.9851632118225098)
[2025-02-13 04:24:35,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:35,723][root][INFO] - Training Epoch: 2/2, step 6009/7134 completed (loss: 0.03592385724186897, acc: 0.9906832575798035)
[2025-02-13 04:24:35,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:36,153][root][INFO] - Training Epoch: 2/2, step 6010/7134 completed (loss: 0.052266158163547516, acc: 0.9850948452949524)
[2025-02-13 04:24:36,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:36,649][root][INFO] - Training Epoch: 2/2, step 6011/7134 completed (loss: 0.03683256730437279, acc: 0.9883419871330261)
[2025-02-13 04:24:36,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:37,068][root][INFO] - Training Epoch: 2/2, step 6012/7134 completed (loss: 0.03486187756061554, acc: 0.9850746393203735)
[2025-02-13 04:24:37,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:37,485][root][INFO] - Training Epoch: 2/2, step 6013/7134 completed (loss: 0.04594210907816887, acc: 0.9852941036224365)
[2025-02-13 04:24:37,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:37,894][root][INFO] - Training Epoch: 2/2, step 6014/7134 completed (loss: 0.02497146837413311, acc: 0.9909256100654602)
[2025-02-13 04:24:38,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:38,325][root][INFO] - Training Epoch: 2/2, step 6015/7134 completed (loss: 0.05358206480741501, acc: 0.9876033067703247)
[2025-02-13 04:24:38,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:38,720][root][INFO] - Training Epoch: 2/2, step 6016/7134 completed (loss: 0.010771612636744976, acc: 0.9964157938957214)
[2025-02-13 04:24:38,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:38,968][root][INFO] - Training Epoch: 2/2, step 6017/7134 completed (loss: 0.018316954374313354, acc: 0.9950494766235352)
[2025-02-13 04:24:39,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:39,236][root][INFO] - Training Epoch: 2/2, step 6018/7134 completed (loss: 0.010583067312836647, acc: 1.0)
[2025-02-13 04:24:39,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:39,480][root][INFO] - Training Epoch: 2/2, step 6019/7134 completed (loss: 0.02868196927011013, acc: 0.9913793206214905)
[2025-02-13 04:24:39,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:39,758][root][INFO] - Training Epoch: 2/2, step 6020/7134 completed (loss: 0.002999926218762994, acc: 1.0)
[2025-02-13 04:24:39,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:39,996][root][INFO] - Training Epoch: 2/2, step 6021/7134 completed (loss: 0.0049417526461184025, acc: 1.0)
[2025-02-13 04:24:40,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:40,300][root][INFO] - Training Epoch: 2/2, step 6022/7134 completed (loss: 0.023742495104670525, acc: 0.9889298677444458)
[2025-02-13 04:24:40,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:40,609][root][INFO] - Training Epoch: 2/2, step 6023/7134 completed (loss: 0.021108325570821762, acc: 0.9945799708366394)
[2025-02-13 04:24:40,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:40,864][root][INFO] - Training Epoch: 2/2, step 6024/7134 completed (loss: 0.008432101458311081, acc: 1.0)
[2025-02-13 04:24:40,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:41,263][root][INFO] - Training Epoch: 2/2, step 6025/7134 completed (loss: 0.013662592507898808, acc: 0.9979079365730286)
[2025-02-13 04:24:41,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:41,510][root][INFO] - Training Epoch: 2/2, step 6026/7134 completed (loss: 0.028030332177877426, acc: 0.9885714054107666)
[2025-02-13 04:24:41,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:41,822][root][INFO] - Training Epoch: 2/2, step 6027/7134 completed (loss: 0.016942381858825684, acc: 0.9974293112754822)
[2025-02-13 04:24:41,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:42,138][root][INFO] - Training Epoch: 2/2, step 6028/7134 completed (loss: 0.008379035629332066, acc: 1.0)
[2025-02-13 04:24:42,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:42,643][root][INFO] - Training Epoch: 2/2, step 6029/7134 completed (loss: 0.028750833123922348, acc: 0.9927797913551331)
[2025-02-13 04:24:42,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:42,951][root][INFO] - Training Epoch: 2/2, step 6030/7134 completed (loss: 0.007760188542306423, acc: 0.9961685538291931)
[2025-02-13 04:24:43,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:43,408][root][INFO] - Training Epoch: 2/2, step 6031/7134 completed (loss: 0.015035239979624748, acc: 0.9970760345458984)
[2025-02-13 04:24:43,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:43,754][root][INFO] - Training Epoch: 2/2, step 6032/7134 completed (loss: 0.01681254245340824, acc: 0.9974683523178101)
[2025-02-13 04:24:43,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:44,071][root][INFO] - Training Epoch: 2/2, step 6033/7134 completed (loss: 0.07319048792123795, acc: 0.9836065769195557)
[2025-02-13 04:24:44,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:44,322][root][INFO] - Training Epoch: 2/2, step 6034/7134 completed (loss: 0.09245197474956512, acc: 0.9754098653793335)
[2025-02-13 04:24:44,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:44,798][root][INFO] - Training Epoch: 2/2, step 6035/7134 completed (loss: 0.07014058530330658, acc: 0.9766082167625427)
[2025-02-13 04:24:44,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:45,270][root][INFO] - Training Epoch: 2/2, step 6036/7134 completed (loss: 0.04419255629181862, acc: 0.9874125719070435)
[2025-02-13 04:24:45,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:45,703][root][INFO] - Training Epoch: 2/2, step 6037/7134 completed (loss: 0.02781461551785469, acc: 0.989830493927002)
[2025-02-13 04:24:45,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:46,163][root][INFO] - Training Epoch: 2/2, step 6038/7134 completed (loss: 0.02444901503622532, acc: 0.991051435470581)
[2025-02-13 04:24:46,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:46,613][root][INFO] - Training Epoch: 2/2, step 6039/7134 completed (loss: 0.03355962410569191, acc: 0.990510106086731)
[2025-02-13 04:24:46,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:47,066][root][INFO] - Training Epoch: 2/2, step 6040/7134 completed (loss: 0.06408195197582245, acc: 0.9833679795265198)
[2025-02-13 04:24:47,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:47,473][root][INFO] - Training Epoch: 2/2, step 6041/7134 completed (loss: 0.036602653563022614, acc: 0.9895150661468506)
[2025-02-13 04:24:47,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:47,919][root][INFO] - Training Epoch: 2/2, step 6042/7134 completed (loss: 0.018105316907167435, acc: 0.9940000176429749)
[2025-02-13 04:24:48,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:48,356][root][INFO] - Training Epoch: 2/2, step 6043/7134 completed (loss: 0.024340633302927017, acc: 0.9928057789802551)
[2025-02-13 04:24:48,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:48,800][root][INFO] - Training Epoch: 2/2, step 6044/7134 completed (loss: 0.05696416273713112, acc: 0.9876543283462524)
[2025-02-13 04:24:48,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:49,216][root][INFO] - Training Epoch: 2/2, step 6045/7134 completed (loss: 0.039255160838365555, acc: 0.9893491268157959)
[2025-02-13 04:24:49,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:49,655][root][INFO] - Training Epoch: 2/2, step 6046/7134 completed (loss: 0.03479258343577385, acc: 0.9861351847648621)
[2025-02-13 04:24:49,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:50,127][root][INFO] - Training Epoch: 2/2, step 6047/7134 completed (loss: 0.09944283962249756, acc: 0.9763912558555603)
[2025-02-13 04:24:50,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:50,563][root][INFO] - Training Epoch: 2/2, step 6048/7134 completed (loss: 0.047311607748270035, acc: 0.9839679598808289)
[2025-02-13 04:24:50,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:50,963][root][INFO] - Training Epoch: 2/2, step 6049/7134 completed (loss: 0.009531037881970406, acc: 0.9982935190200806)
[2025-02-13 04:24:51,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:51,366][root][INFO] - Training Epoch: 2/2, step 6050/7134 completed (loss: 0.0561661571264267, acc: 0.9794801473617554)
[2025-02-13 04:24:51,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:51,825][root][INFO] - Training Epoch: 2/2, step 6051/7134 completed (loss: 0.07030412554740906, acc: 0.9793205261230469)
[2025-02-13 04:24:51,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:52,283][root][INFO] - Training Epoch: 2/2, step 6052/7134 completed (loss: 0.06060849130153656, acc: 0.9811617136001587)
[2025-02-13 04:24:52,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:52,740][root][INFO] - Training Epoch: 2/2, step 6053/7134 completed (loss: 0.06621811538934708, acc: 0.9802631735801697)
[2025-02-13 04:24:52,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:53,176][root][INFO] - Training Epoch: 2/2, step 6054/7134 completed (loss: 0.03481985256075859, acc: 0.9927536249160767)
[2025-02-13 04:24:53,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:53,583][root][INFO] - Training Epoch: 2/2, step 6055/7134 completed (loss: 0.044422879815101624, acc: 0.9857819676399231)
[2025-02-13 04:24:53,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:53,954][root][INFO] - Training Epoch: 2/2, step 6056/7134 completed (loss: 0.04704819247126579, acc: 0.9884792566299438)
[2025-02-13 04:24:54,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:54,310][root][INFO] - Training Epoch: 2/2, step 6057/7134 completed (loss: 0.03283737227320671, acc: 0.9901315569877625)
[2025-02-13 04:24:54,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:54,665][root][INFO] - Training Epoch: 2/2, step 6058/7134 completed (loss: 0.010940429754555225, acc: 0.9954954981803894)
[2025-02-13 04:24:54,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:55,010][root][INFO] - Training Epoch: 2/2, step 6059/7134 completed (loss: 0.01473105326294899, acc: 0.9893993139266968)
[2025-02-13 04:24:55,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:55,453][root][INFO] - Training Epoch: 2/2, step 6060/7134 completed (loss: 0.02607165090739727, acc: 0.9894737005233765)
[2025-02-13 04:24:55,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:55,853][root][INFO] - Training Epoch: 2/2, step 6061/7134 completed (loss: 0.06471718847751617, acc: 0.9815100431442261)
[2025-02-13 04:24:55,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:56,307][root][INFO] - Training Epoch: 2/2, step 6062/7134 completed (loss: 0.052961550652980804, acc: 0.9844961166381836)
[2025-02-13 04:24:56,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:56,724][root][INFO] - Training Epoch: 2/2, step 6063/7134 completed (loss: 0.067728690803051, acc: 0.9819168448448181)
[2025-02-13 04:24:57,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:57,447][root][INFO] - Training Epoch: 2/2, step 6064/7134 completed (loss: 0.02716708742082119, acc: 0.9927641153335571)
[2025-02-13 04:24:57,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:57,904][root][INFO] - Training Epoch: 2/2, step 6065/7134 completed (loss: 0.04451087489724159, acc: 0.9878493547439575)
[2025-02-13 04:24:58,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:58,308][root][INFO] - Training Epoch: 2/2, step 6066/7134 completed (loss: 0.06526504456996918, acc: 0.9736408591270447)
[2025-02-13 04:24:58,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:58,715][root][INFO] - Training Epoch: 2/2, step 6067/7134 completed (loss: 0.04453582316637039, acc: 0.9855700135231018)
[2025-02-13 04:24:58,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:59,194][root][INFO] - Training Epoch: 2/2, step 6068/7134 completed (loss: 0.07304422557353973, acc: 0.9862328171730042)
[2025-02-13 04:24:59,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:59,591][root][INFO] - Training Epoch: 2/2, step 6069/7134 completed (loss: 0.057514503598213196, acc: 0.9810218811035156)
[2025-02-13 04:24:59,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:00,043][root][INFO] - Training Epoch: 2/2, step 6070/7134 completed (loss: 0.07220304012298584, acc: 0.9773635268211365)
[2025-02-13 04:25:00,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:00,443][root][INFO] - Training Epoch: 2/2, step 6071/7134 completed (loss: 0.04414502531290054, acc: 0.9919246435165405)
[2025-02-13 04:25:00,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:00,773][root][INFO] - Training Epoch: 2/2, step 6072/7134 completed (loss: 0.09055909514427185, acc: 0.9768907427787781)
[2025-02-13 04:25:00,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:01,192][root][INFO] - Training Epoch: 2/2, step 6073/7134 completed (loss: 0.04791100695729256, acc: 0.9832167625427246)
[2025-02-13 04:25:01,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:01,592][root][INFO] - Training Epoch: 2/2, step 6074/7134 completed (loss: 0.04042767733335495, acc: 0.9897058606147766)
[2025-02-13 04:25:01,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:02,032][root][INFO] - Training Epoch: 2/2, step 6075/7134 completed (loss: 0.0444556400179863, acc: 0.9892086386680603)
[2025-02-13 04:25:02,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:02,445][root][INFO] - Training Epoch: 2/2, step 6076/7134 completed (loss: 0.05538692697882652, acc: 0.982758641242981)
[2025-02-13 04:25:02,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:02,854][root][INFO] - Training Epoch: 2/2, step 6077/7134 completed (loss: 0.031584665179252625, acc: 0.9904153347015381)
[2025-02-13 04:25:02,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:03,271][root][INFO] - Training Epoch: 2/2, step 6078/7134 completed (loss: 0.05957058072090149, acc: 0.9873239398002625)
[2025-02-13 04:25:03,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:03,719][root][INFO] - Training Epoch: 2/2, step 6079/7134 completed (loss: 0.02836701273918152, acc: 0.9908758997917175)
[2025-02-13 04:25:03,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:04,120][root][INFO] - Training Epoch: 2/2, step 6080/7134 completed (loss: 0.02354332059621811, acc: 0.9915134310722351)
[2025-02-13 04:25:04,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:04,539][root][INFO] - Training Epoch: 2/2, step 6081/7134 completed (loss: 0.022530503571033478, acc: 0.9923664331436157)
[2025-02-13 04:25:04,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:04,936][root][INFO] - Training Epoch: 2/2, step 6082/7134 completed (loss: 0.033586882054805756, acc: 0.993630588054657)
[2025-02-13 04:25:05,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:05,383][root][INFO] - Training Epoch: 2/2, step 6083/7134 completed (loss: 0.011916402727365494, acc: 0.9950860142707825)
[2025-02-13 04:25:05,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:05,783][root][INFO] - Training Epoch: 2/2, step 6084/7134 completed (loss: 0.039409786462783813, acc: 0.9921383857727051)
[2025-02-13 04:25:05,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:06,230][root][INFO] - Training Epoch: 2/2, step 6085/7134 completed (loss: 0.019284697249531746, acc: 0.9982110857963562)
[2025-02-13 04:25:06,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:06,701][root][INFO] - Training Epoch: 2/2, step 6086/7134 completed (loss: 0.01815456710755825, acc: 0.9957864880561829)
[2025-02-13 04:25:06,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:07,154][root][INFO] - Training Epoch: 2/2, step 6087/7134 completed (loss: 0.05288168415427208, acc: 0.9811320900917053)
[2025-02-13 04:25:07,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:07,563][root][INFO] - Training Epoch: 2/2, step 6088/7134 completed (loss: 0.029575670138001442, acc: 0.9906166195869446)
[2025-02-13 04:25:07,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:08,022][root][INFO] - Training Epoch: 2/2, step 6089/7134 completed (loss: 0.0348329097032547, acc: 0.9889655113220215)
[2025-02-13 04:25:08,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:08,445][root][INFO] - Training Epoch: 2/2, step 6090/7134 completed (loss: 0.0217753816395998, acc: 0.9958847761154175)
[2025-02-13 04:25:08,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:08,843][root][INFO] - Training Epoch: 2/2, step 6091/7134 completed (loss: 0.037591446191072464, acc: 0.9904458522796631)
[2025-02-13 04:25:08,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:09,236][root][INFO] - Training Epoch: 2/2, step 6092/7134 completed (loss: 0.04415982961654663, acc: 0.9841269850730896)
[2025-02-13 04:25:09,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:09,641][root][INFO] - Training Epoch: 2/2, step 6093/7134 completed (loss: 0.05254549905657768, acc: 0.9851577281951904)
[2025-02-13 04:25:09,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:10,036][root][INFO] - Training Epoch: 2/2, step 6094/7134 completed (loss: 0.034144096076488495, acc: 0.9901960492134094)
[2025-02-13 04:25:10,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:10,411][root][INFO] - Training Epoch: 2/2, step 6095/7134 completed (loss: 0.019652334973216057, acc: 0.9956896305084229)
[2025-02-13 04:25:10,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:10,861][root][INFO] - Training Epoch: 2/2, step 6096/7134 completed (loss: 0.04152049869298935, acc: 0.9868735074996948)
[2025-02-13 04:25:11,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:11,280][root][INFO] - Training Epoch: 2/2, step 6097/7134 completed (loss: 0.03681468963623047, acc: 0.9889196753501892)
[2025-02-13 04:25:11,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:11,701][root][INFO] - Training Epoch: 2/2, step 6098/7134 completed (loss: 0.02957511693239212, acc: 0.992277979850769)
[2025-02-13 04:25:11,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:12,135][root][INFO] - Training Epoch: 2/2, step 6099/7134 completed (loss: 0.0547141507267952, acc: 0.9892966151237488)
[2025-02-13 04:25:12,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:12,447][root][INFO] - Training Epoch: 2/2, step 6100/7134 completed (loss: 0.06562476605176926, acc: 0.9847561120986938)
[2025-02-13 04:25:12,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:12,828][root][INFO] - Training Epoch: 2/2, step 6101/7134 completed (loss: 0.0163713488727808, acc: 0.9968051314353943)
[2025-02-13 04:25:12,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:13,227][root][INFO] - Training Epoch: 2/2, step 6102/7134 completed (loss: 0.03745397552847862, acc: 0.9911971688270569)
[2025-02-13 04:25:13,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:13,652][root][INFO] - Training Epoch: 2/2, step 6103/7134 completed (loss: 0.0409650020301342, acc: 0.9833333492279053)
[2025-02-13 04:25:13,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:14,053][root][INFO] - Training Epoch: 2/2, step 6104/7134 completed (loss: 0.07114072144031525, acc: 0.9733333587646484)
[2025-02-13 04:25:14,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:14,478][root][INFO] - Training Epoch: 2/2, step 6105/7134 completed (loss: 0.06273166090250015, acc: 0.9836309552192688)
[2025-02-13 04:25:14,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:14,827][root][INFO] - Training Epoch: 2/2, step 6106/7134 completed (loss: 0.03933551535010338, acc: 0.9856887459754944)
[2025-02-13 04:25:14,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:15,236][root][INFO] - Training Epoch: 2/2, step 6107/7134 completed (loss: 0.05433721840381622, acc: 0.9810924530029297)
[2025-02-13 04:25:15,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:15,629][root][INFO] - Training Epoch: 2/2, step 6108/7134 completed (loss: 0.028180621564388275, acc: 0.9872495532035828)
[2025-02-13 04:25:15,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:16,008][root][INFO] - Training Epoch: 2/2, step 6109/7134 completed (loss: 0.013801325112581253, acc: 0.9957627058029175)
[2025-02-13 04:25:16,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:16,415][root][INFO] - Training Epoch: 2/2, step 6110/7134 completed (loss: 0.07871069759130478, acc: 0.9764957427978516)
[2025-02-13 04:25:16,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:16,794][root][INFO] - Training Epoch: 2/2, step 6111/7134 completed (loss: 0.06554565578699112, acc: 0.9812332391738892)
[2025-02-13 04:25:16,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:17,205][root][INFO] - Training Epoch: 2/2, step 6112/7134 completed (loss: 0.025273632258176804, acc: 0.9899665713310242)
[2025-02-13 04:25:17,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:17,622][root][INFO] - Training Epoch: 2/2, step 6113/7134 completed (loss: 0.021284619346261024, acc: 0.9924127459526062)
[2025-02-13 04:25:17,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:18,011][root][INFO] - Training Epoch: 2/2, step 6114/7134 completed (loss: 0.0660814642906189, acc: 0.9818181991577148)
[2025-02-13 04:25:18,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:18,369][root][INFO] - Training Epoch: 2/2, step 6115/7134 completed (loss: 0.04406684264540672, acc: 0.9885057210922241)
[2025-02-13 04:25:18,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:18,698][root][INFO] - Training Epoch: 2/2, step 6116/7134 completed (loss: 0.05572891607880592, acc: 0.9881423115730286)
[2025-02-13 04:25:18,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:19,120][root][INFO] - Training Epoch: 2/2, step 6117/7134 completed (loss: 0.08262919634580612, acc: 0.9836065769195557)
[2025-02-13 04:25:19,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:19,604][root][INFO] - Training Epoch: 2/2, step 6118/7134 completed (loss: 0.045495837926864624, acc: 0.9889705777168274)
[2025-02-13 04:25:19,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:20,045][root][INFO] - Training Epoch: 2/2, step 6119/7134 completed (loss: 0.03387869521975517, acc: 0.9879952073097229)
[2025-02-13 04:25:20,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:20,453][root][INFO] - Training Epoch: 2/2, step 6120/7134 completed (loss: 0.02936154045164585, acc: 0.9928443431854248)
[2025-02-13 04:25:20,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:20,888][root][INFO] - Training Epoch: 2/2, step 6121/7134 completed (loss: 0.048488833010196686, acc: 0.9874100685119629)
[2025-02-13 04:25:21,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:21,322][root][INFO] - Training Epoch: 2/2, step 6122/7134 completed (loss: 0.023744840174913406, acc: 0.9905660152435303)
[2025-02-13 04:25:21,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:21,735][root][INFO] - Training Epoch: 2/2, step 6123/7134 completed (loss: 0.07711227238178253, acc: 0.9792099595069885)
[2025-02-13 04:25:21,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:22,126][root][INFO] - Training Epoch: 2/2, step 6124/7134 completed (loss: 0.02015519328415394, acc: 0.9923076629638672)
[2025-02-13 04:25:22,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:22,573][root][INFO] - Training Epoch: 2/2, step 6125/7134 completed (loss: 0.08822812139987946, acc: 0.9783132672309875)
[2025-02-13 04:25:22,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:22,978][root][INFO] - Training Epoch: 2/2, step 6126/7134 completed (loss: 0.051505912095308304, acc: 0.9874607920646667)
[2025-02-13 04:25:23,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:23,237][root][INFO] - Training Epoch: 2/2, step 6127/7134 completed (loss: 0.022459357976913452, acc: 0.9952380657196045)
[2025-02-13 04:25:23,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:23,641][root][INFO] - Training Epoch: 2/2, step 6128/7134 completed (loss: 0.03006567992269993, acc: 0.9878542423248291)
[2025-02-13 04:25:23,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:24,060][root][INFO] - Training Epoch: 2/2, step 6129/7134 completed (loss: 0.055795952677726746, acc: 0.9803094267845154)
[2025-02-13 04:25:24,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:24,446][root][INFO] - Training Epoch: 2/2, step 6130/7134 completed (loss: 0.017390359193086624, acc: 0.9928315281867981)
[2025-02-13 04:25:24,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:24,888][root][INFO] - Training Epoch: 2/2, step 6131/7134 completed (loss: 0.03513850271701813, acc: 0.9916167855262756)
[2025-02-13 04:25:25,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:25,289][root][INFO] - Training Epoch: 2/2, step 6132/7134 completed (loss: 0.03606782481074333, acc: 0.989062488079071)
[2025-02-13 04:25:25,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:25,743][root][INFO] - Training Epoch: 2/2, step 6133/7134 completed (loss: 0.0513172522187233, acc: 0.9866504669189453)
[2025-02-13 04:25:25,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:26,124][root][INFO] - Training Epoch: 2/2, step 6134/7134 completed (loss: 0.06548979878425598, acc: 0.9800918698310852)
[2025-02-13 04:25:26,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:26,523][root][INFO] - Training Epoch: 2/2, step 6135/7134 completed (loss: 0.011978445574641228, acc: 0.9982578158378601)
[2025-02-13 04:25:26,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:26,931][root][INFO] - Training Epoch: 2/2, step 6136/7134 completed (loss: 0.05344703420996666, acc: 0.994397759437561)
[2025-02-13 04:25:27,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:27,326][root][INFO] - Training Epoch: 2/2, step 6137/7134 completed (loss: 0.06234673410654068, acc: 0.9875195026397705)
[2025-02-13 04:25:27,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:27,723][root][INFO] - Training Epoch: 2/2, step 6138/7134 completed (loss: 0.04841824993491173, acc: 0.9861751198768616)
[2025-02-13 04:25:27,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:28,106][root][INFO] - Training Epoch: 2/2, step 6139/7134 completed (loss: 0.04437081515789032, acc: 0.9870370626449585)
[2025-02-13 04:25:28,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:28,564][root][INFO] - Training Epoch: 2/2, step 6140/7134 completed (loss: 0.0313357375562191, acc: 0.9946996569633484)
[2025-02-13 04:25:28,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:28,880][root][INFO] - Training Epoch: 2/2, step 6141/7134 completed (loss: 0.07255120575428009, acc: 0.9877675771713257)
[2025-02-13 04:25:29,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:29,295][root][INFO] - Training Epoch: 2/2, step 6142/7134 completed (loss: 0.025163494050502777, acc: 0.9884892106056213)
[2025-02-13 04:25:29,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:29,725][root][INFO] - Training Epoch: 2/2, step 6143/7134 completed (loss: 0.02394871786236763, acc: 0.995945930480957)
[2025-02-13 04:25:29,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:30,116][root][INFO] - Training Epoch: 2/2, step 6144/7134 completed (loss: 0.02300608530640602, acc: 0.9915966391563416)
[2025-02-13 04:25:30,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:30,558][root][INFO] - Training Epoch: 2/2, step 6145/7134 completed (loss: 0.057514090090990067, acc: 0.9867899417877197)
[2025-02-13 04:25:30,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:30,950][root][INFO] - Training Epoch: 2/2, step 6146/7134 completed (loss: 0.0536905936896801, acc: 0.9849170446395874)
[2025-02-13 04:25:31,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:31,347][root][INFO] - Training Epoch: 2/2, step 6147/7134 completed (loss: 0.03917417675256729, acc: 0.9900793433189392)
[2025-02-13 04:25:31,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:31,789][root][INFO] - Training Epoch: 2/2, step 6148/7134 completed (loss: 0.03029595874249935, acc: 0.9865996837615967)
[2025-02-13 04:25:31,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:32,197][root][INFO] - Training Epoch: 2/2, step 6149/7134 completed (loss: 0.036481693387031555, acc: 0.9871382713317871)
[2025-02-13 04:25:32,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:32,611][root][INFO] - Training Epoch: 2/2, step 6150/7134 completed (loss: 0.025483107194304466, acc: 0.9912280440330505)
[2025-02-13 04:25:32,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:32,977][root][INFO] - Training Epoch: 2/2, step 6151/7134 completed (loss: 0.02639017440378666, acc: 0.9905808568000793)
[2025-02-13 04:25:33,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:33,397][root][INFO] - Training Epoch: 2/2, step 6152/7134 completed (loss: 0.03623807802796364, acc: 0.9879336357116699)
[2025-02-13 04:25:33,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:33,838][root][INFO] - Training Epoch: 2/2, step 6153/7134 completed (loss: 0.014411336742341518, acc: 0.9971056580543518)
[2025-02-13 04:25:33,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:34,278][root][INFO] - Training Epoch: 2/2, step 6154/7134 completed (loss: 0.03988736495375633, acc: 0.9902371168136597)
[2025-02-13 04:25:34,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:34,708][root][INFO] - Training Epoch: 2/2, step 6155/7134 completed (loss: 0.0360560342669487, acc: 0.9881305694580078)
[2025-02-13 04:25:34,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:35,094][root][INFO] - Training Epoch: 2/2, step 6156/7134 completed (loss: 0.023643607273697853, acc: 0.9936608672142029)
[2025-02-13 04:25:35,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:35,519][root][INFO] - Training Epoch: 2/2, step 6157/7134 completed (loss: 0.023092638701200485, acc: 0.9921630024909973)
[2025-02-13 04:25:35,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:35,937][root][INFO] - Training Epoch: 2/2, step 6158/7134 completed (loss: 0.06277456134557724, acc: 0.9891501069068909)
[2025-02-13 04:25:36,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:36,377][root][INFO] - Training Epoch: 2/2, step 6159/7134 completed (loss: 0.03396762162446976, acc: 0.9891435503959656)
[2025-02-13 04:25:36,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:36,790][root][INFO] - Training Epoch: 2/2, step 6160/7134 completed (loss: 0.06723003089427948, acc: 0.9803030490875244)
[2025-02-13 04:25:36,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:37,239][root][INFO] - Training Epoch: 2/2, step 6161/7134 completed (loss: 0.05406412482261658, acc: 0.9912126660346985)
[2025-02-13 04:25:37,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:37,673][root][INFO] - Training Epoch: 2/2, step 6162/7134 completed (loss: 0.043256498873233795, acc: 0.9885433912277222)
[2025-02-13 04:25:37,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:38,086][root][INFO] - Training Epoch: 2/2, step 6163/7134 completed (loss: 0.09623391181230545, acc: 0.9839285612106323)
[2025-02-13 04:25:38,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:38,526][root][INFO] - Training Epoch: 2/2, step 6164/7134 completed (loss: 0.06121859699487686, acc: 0.980169951915741)
[2025-02-13 04:25:38,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:38,904][root][INFO] - Training Epoch: 2/2, step 6165/7134 completed (loss: 0.050741519778966904, acc: 0.9889298677444458)
[2025-02-13 04:25:39,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:39,304][root][INFO] - Training Epoch: 2/2, step 6166/7134 completed (loss: 0.03253205493092537, acc: 0.994727611541748)
[2025-02-13 04:25:39,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:39,711][root][INFO] - Training Epoch: 2/2, step 6167/7134 completed (loss: 0.028336642310023308, acc: 0.9929278492927551)
[2025-02-13 04:25:39,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:40,146][root][INFO] - Training Epoch: 2/2, step 6168/7134 completed (loss: 0.04746314510703087, acc: 0.9858430027961731)
[2025-02-13 04:25:40,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:40,541][root][INFO] - Training Epoch: 2/2, step 6169/7134 completed (loss: 0.04659532755613327, acc: 0.991253674030304)
[2025-02-13 04:25:40,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:40,930][root][INFO] - Training Epoch: 2/2, step 6170/7134 completed (loss: 0.029753651469945908, acc: 0.9943289160728455)
[2025-02-13 04:25:41,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:41,355][root][INFO] - Training Epoch: 2/2, step 6171/7134 completed (loss: 0.010862503200769424, acc: 0.9947437644004822)
[2025-02-13 04:25:41,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:41,733][root][INFO] - Training Epoch: 2/2, step 6172/7134 completed (loss: 0.014012430794537067, acc: 0.9959920048713684)
[2025-02-13 04:25:41,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:42,124][root][INFO] - Training Epoch: 2/2, step 6173/7134 completed (loss: 0.012713509611785412, acc: 0.9926362037658691)
[2025-02-13 04:25:42,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:42,535][root][INFO] - Training Epoch: 2/2, step 6174/7134 completed (loss: 0.005073453765362501, acc: 0.9985337257385254)
[2025-02-13 04:25:42,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:42,968][root][INFO] - Training Epoch: 2/2, step 6175/7134 completed (loss: 0.021637890487909317, acc: 0.9926035404205322)
[2025-02-13 04:25:43,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:43,361][root][INFO] - Training Epoch: 2/2, step 6176/7134 completed (loss: 0.033049892634153366, acc: 0.986975371837616)
[2025-02-13 04:25:43,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:43,779][root][INFO] - Training Epoch: 2/2, step 6177/7134 completed (loss: 0.024826327338814735, acc: 0.9897698163986206)
[2025-02-13 04:25:43,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:44,180][root][INFO] - Training Epoch: 2/2, step 6178/7134 completed (loss: 0.018530208617448807, acc: 0.9970501661300659)
[2025-02-13 04:25:44,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:44,584][root][INFO] - Training Epoch: 2/2, step 6179/7134 completed (loss: 0.03147554025053978, acc: 0.9922360181808472)
[2025-02-13 04:25:44,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:44,930][root][INFO] - Training Epoch: 2/2, step 6180/7134 completed (loss: 0.11636032164096832, acc: 0.9681528806686401)
[2025-02-13 04:25:45,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:45,313][root][INFO] - Training Epoch: 2/2, step 6181/7134 completed (loss: 0.22376951575279236, acc: 0.9462810158729553)
[2025-02-13 04:25:45,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:45,731][root][INFO] - Training Epoch: 2/2, step 6182/7134 completed (loss: 0.029231606051325798, acc: 0.9888682961463928)
[2025-02-13 04:25:45,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:46,149][root][INFO] - Training Epoch: 2/2, step 6183/7134 completed (loss: 0.050210755318403244, acc: 0.9892802238464355)
[2025-02-13 04:25:46,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:46,544][root][INFO] - Training Epoch: 2/2, step 6184/7134 completed (loss: 0.08479958772659302, acc: 0.9697580933570862)
[2025-02-13 04:25:46,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:46,948][root][INFO] - Training Epoch: 2/2, step 6185/7134 completed (loss: 0.035698167979717255, acc: 0.9852941036224365)
[2025-02-13 04:25:47,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:47,347][root][INFO] - Training Epoch: 2/2, step 6186/7134 completed (loss: 0.030693484470248222, acc: 0.9860140085220337)
[2025-02-13 04:25:47,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:47,793][root][INFO] - Training Epoch: 2/2, step 6187/7134 completed (loss: 0.03433910384774208, acc: 0.9894179701805115)
[2025-02-13 04:25:47,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:48,222][root][INFO] - Training Epoch: 2/2, step 6188/7134 completed (loss: 0.0485810786485672, acc: 0.9816642999649048)
[2025-02-13 04:25:48,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:48,670][root][INFO] - Training Epoch: 2/2, step 6189/7134 completed (loss: 0.08003833144903183, acc: 0.9826839566230774)
[2025-02-13 04:25:48,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:49,076][root][INFO] - Training Epoch: 2/2, step 6190/7134 completed (loss: 0.04923325777053833, acc: 0.985049843788147)
[2025-02-13 04:25:49,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:49,516][root][INFO] - Training Epoch: 2/2, step 6191/7134 completed (loss: 0.03991255164146423, acc: 0.9860140085220337)
[2025-02-13 04:25:49,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:49,977][root][INFO] - Training Epoch: 2/2, step 6192/7134 completed (loss: 0.03556806966662407, acc: 0.986066460609436)
[2025-02-13 04:25:50,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:50,412][root][INFO] - Training Epoch: 2/2, step 6193/7134 completed (loss: 0.04375900328159332, acc: 0.9869203567504883)
[2025-02-13 04:25:50,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:50,843][root][INFO] - Training Epoch: 2/2, step 6194/7134 completed (loss: 0.03922724723815918, acc: 0.9805447459220886)
[2025-02-13 04:25:50,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:51,232][root][INFO] - Training Epoch: 2/2, step 6195/7134 completed (loss: 0.04688958451151848, acc: 0.9856459498405457)
[2025-02-13 04:25:51,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:51,661][root][INFO] - Training Epoch: 2/2, step 6196/7134 completed (loss: 0.033298738300800323, acc: 0.9862259030342102)
[2025-02-13 04:25:51,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:52,094][root][INFO] - Training Epoch: 2/2, step 6197/7134 completed (loss: 0.07269559800624847, acc: 0.9793103337287903)
[2025-02-13 04:25:52,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:52,490][root][INFO] - Training Epoch: 2/2, step 6198/7134 completed (loss: 0.027737688273191452, acc: 0.9896265268325806)
[2025-02-13 04:25:52,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:52,928][root][INFO] - Training Epoch: 2/2, step 6199/7134 completed (loss: 0.04605231061577797, acc: 0.9895591735839844)
[2025-02-13 04:25:53,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:53,360][root][INFO] - Training Epoch: 2/2, step 6200/7134 completed (loss: 0.029938513413071632, acc: 0.9902439117431641)
[2025-02-13 04:25:53,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:53,812][root][INFO] - Training Epoch: 2/2, step 6201/7134 completed (loss: 0.03336166962981224, acc: 0.9933110475540161)
[2025-02-13 04:25:53,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:54,206][root][INFO] - Training Epoch: 2/2, step 6202/7134 completed (loss: 0.0349021777510643, acc: 0.989154040813446)
[2025-02-13 04:25:54,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:54,639][root][INFO] - Training Epoch: 2/2, step 6203/7134 completed (loss: 0.0181571114808321, acc: 0.9961734414100647)
[2025-02-13 04:25:54,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:55,058][root][INFO] - Training Epoch: 2/2, step 6204/7134 completed (loss: 0.00991908460855484, acc: 1.0)
[2025-02-13 04:25:55,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:55,467][root][INFO] - Training Epoch: 2/2, step 6205/7134 completed (loss: 0.02666986547410488, acc: 0.9931740760803223)
[2025-02-13 04:25:55,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:55,855][root][INFO] - Training Epoch: 2/2, step 6206/7134 completed (loss: 0.025446448475122452, acc: 0.9884225726127625)
[2025-02-13 04:25:55,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:56,300][root][INFO] - Training Epoch: 2/2, step 6207/7134 completed (loss: 0.11001909524202347, acc: 0.9719626307487488)
[2025-02-13 04:25:56,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:56,722][root][INFO] - Training Epoch: 2/2, step 6208/7134 completed (loss: 0.034320201724767685, acc: 0.9872685074806213)
[2025-02-13 04:25:56,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:57,160][root][INFO] - Training Epoch: 2/2, step 6209/7134 completed (loss: 0.04211768880486488, acc: 0.9844760894775391)
[2025-02-13 04:25:57,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:57,622][root][INFO] - Training Epoch: 2/2, step 6210/7134 completed (loss: 0.024700237438082695, acc: 0.9912087917327881)
[2025-02-13 04:25:57,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:58,067][root][INFO] - Training Epoch: 2/2, step 6211/7134 completed (loss: 0.0373108945786953, acc: 0.9891745448112488)
[2025-02-13 04:25:58,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:58,468][root][INFO] - Training Epoch: 2/2, step 6212/7134 completed (loss: 0.024777112528681755, acc: 0.9964912533760071)
[2025-02-13 04:25:58,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:58,853][root][INFO] - Training Epoch: 2/2, step 6213/7134 completed (loss: 0.01748381368815899, acc: 0.9937629699707031)
[2025-02-13 04:25:58,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:59,285][root][INFO] - Training Epoch: 2/2, step 6214/7134 completed (loss: 0.016617029905319214, acc: 0.9948652386665344)
[2025-02-13 04:25:59,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:59,730][root][INFO] - Training Epoch: 2/2, step 6215/7134 completed (loss: 0.03307413309812546, acc: 0.9884792566299438)
[2025-02-13 04:25:59,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:00,038][root][INFO] - Training Epoch: 2/2, step 6216/7134 completed (loss: 0.01349595282226801, acc: 0.9978723526000977)
[2025-02-13 04:26:00,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:00,471][root][INFO] - Training Epoch: 2/2, step 6217/7134 completed (loss: 0.033527590334415436, acc: 0.9932885766029358)
[2025-02-13 04:26:00,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:00,919][root][INFO] - Training Epoch: 2/2, step 6218/7134 completed (loss: 0.025418588891625404, acc: 0.9893292784690857)
[2025-02-13 04:26:01,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:01,347][root][INFO] - Training Epoch: 2/2, step 6219/7134 completed (loss: 0.019375918433070183, acc: 0.9943740963935852)
[2025-02-13 04:26:01,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:01,724][root][INFO] - Training Epoch: 2/2, step 6220/7134 completed (loss: 0.041595932096242905, acc: 0.9833333492279053)
[2025-02-13 04:26:01,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:02,136][root][INFO] - Training Epoch: 2/2, step 6221/7134 completed (loss: 0.007687130477279425, acc: 0.9964912533760071)
[2025-02-13 04:26:02,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:02,516][root][INFO] - Training Epoch: 2/2, step 6222/7134 completed (loss: 0.04274782910943031, acc: 0.9890710115432739)
[2025-02-13 04:26:02,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:02,947][root][INFO] - Training Epoch: 2/2, step 6223/7134 completed (loss: 0.03324558585882187, acc: 0.9914529919624329)
[2025-02-13 04:26:03,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:03,361][root][INFO] - Training Epoch: 2/2, step 6224/7134 completed (loss: 0.006721996236592531, acc: 0.9985486268997192)
[2025-02-13 04:26:03,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:03,803][root][INFO] - Training Epoch: 2/2, step 6225/7134 completed (loss: 0.020536724478006363, acc: 0.9936224222183228)
[2025-02-13 04:26:03,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:04,211][root][INFO] - Training Epoch: 2/2, step 6226/7134 completed (loss: 0.03197813779115677, acc: 0.9855832457542419)
[2025-02-13 04:26:04,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:04,644][root][INFO] - Training Epoch: 2/2, step 6227/7134 completed (loss: 0.007829824462532997, acc: 0.9973226189613342)
[2025-02-13 04:26:04,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:05,080][root][INFO] - Training Epoch: 2/2, step 6228/7134 completed (loss: 0.021347902715206146, acc: 0.9901719689369202)
[2025-02-13 04:26:05,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:05,520][root][INFO] - Training Epoch: 2/2, step 6229/7134 completed (loss: 0.06586574763059616, acc: 0.9837586879730225)
[2025-02-13 04:26:05,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:05,962][root][INFO] - Training Epoch: 2/2, step 6230/7134 completed (loss: 0.027086148038506508, acc: 0.9900249242782593)
[2025-02-13 04:26:06,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:06,352][root][INFO] - Training Epoch: 2/2, step 6231/7134 completed (loss: 0.013799281790852547, acc: 0.9974160194396973)
[2025-02-13 04:26:06,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:06,801][root][INFO] - Training Epoch: 2/2, step 6232/7134 completed (loss: 0.016658242791891098, acc: 0.9949495196342468)
[2025-02-13 04:26:06,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:07,235][root][INFO] - Training Epoch: 2/2, step 6233/7134 completed (loss: 0.022174622863531113, acc: 0.9906166195869446)
[2025-02-13 04:26:07,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:07,673][root][INFO] - Training Epoch: 2/2, step 6234/7134 completed (loss: 0.04239005222916603, acc: 0.9826202988624573)
[2025-02-13 04:26:07,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:08,080][root][INFO] - Training Epoch: 2/2, step 6235/7134 completed (loss: 0.021358177065849304, acc: 0.9934810996055603)
[2025-02-13 04:26:08,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:08,514][root][INFO] - Training Epoch: 2/2, step 6236/7134 completed (loss: 0.013893834315240383, acc: 0.9959072470664978)
[2025-02-13 04:26:08,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:08,938][root][INFO] - Training Epoch: 2/2, step 6237/7134 completed (loss: 0.023230280727148056, acc: 0.9923547506332397)
[2025-02-13 04:26:09,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:09,333][root][INFO] - Training Epoch: 2/2, step 6238/7134 completed (loss: 0.006174084730446339, acc: 1.0)
[2025-02-13 04:26:09,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:09,778][root][INFO] - Training Epoch: 2/2, step 6239/7134 completed (loss: 0.03312677890062332, acc: 0.9928315281867981)
[2025-02-13 04:26:09,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:10,215][root][INFO] - Training Epoch: 2/2, step 6240/7134 completed (loss: 0.003883151337504387, acc: 0.9985895752906799)
[2025-02-13 04:26:10,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:10,633][root][INFO] - Training Epoch: 2/2, step 6241/7134 completed (loss: 0.024725381284952164, acc: 0.9936708807945251)
[2025-02-13 04:26:10,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:11,048][root][INFO] - Training Epoch: 2/2, step 6242/7134 completed (loss: 0.010374198667705059, acc: 0.9965986609458923)
[2025-02-13 04:26:11,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:11,452][root][INFO] - Training Epoch: 2/2, step 6243/7134 completed (loss: 0.017088860273361206, acc: 0.993630588054657)
[2025-02-13 04:26:11,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:11,862][root][INFO] - Training Epoch: 2/2, step 6244/7134 completed (loss: 0.011276413686573505, acc: 0.9986282587051392)
[2025-02-13 04:26:11,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:12,275][root][INFO] - Training Epoch: 2/2, step 6245/7134 completed (loss: 0.019014356657862663, acc: 0.99452805519104)
[2025-02-13 04:26:12,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:12,708][root][INFO] - Training Epoch: 2/2, step 6246/7134 completed (loss: 0.017799818888306618, acc: 0.992094874382019)
[2025-02-13 04:26:12,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:13,153][root][INFO] - Training Epoch: 2/2, step 6247/7134 completed (loss: 0.023931629955768585, acc: 0.9891566038131714)
[2025-02-13 04:26:13,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:13,573][root][INFO] - Training Epoch: 2/2, step 6248/7134 completed (loss: 0.03863900154829025, acc: 0.9875389337539673)
[2025-02-13 04:26:13,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:13,961][root][INFO] - Training Epoch: 2/2, step 6249/7134 completed (loss: 0.011105109937489033, acc: 0.9952830076217651)
[2025-02-13 04:26:14,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:14,360][root][INFO] - Training Epoch: 2/2, step 6250/7134 completed (loss: 0.002468930324539542, acc: 1.0)
[2025-02-13 04:26:14,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:14,776][root][INFO] - Training Epoch: 2/2, step 6251/7134 completed (loss: 0.01738807000219822, acc: 0.995398759841919)
[2025-02-13 04:26:14,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:15,180][root][INFO] - Training Epoch: 2/2, step 6252/7134 completed (loss: 0.010477891191840172, acc: 0.9983818531036377)
[2025-02-13 04:26:15,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:15,585][root][INFO] - Training Epoch: 2/2, step 6253/7134 completed (loss: 0.015455617569386959, acc: 0.9965517520904541)
[2025-02-13 04:26:15,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:15,981][root][INFO] - Training Epoch: 2/2, step 6254/7134 completed (loss: 0.024070559069514275, acc: 0.9948096871376038)
[2025-02-13 04:26:16,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:16,397][root][INFO] - Training Epoch: 2/2, step 6255/7134 completed (loss: 0.015314129181206226, acc: 0.9945799708366394)
[2025-02-13 04:26:16,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:16,797][root][INFO] - Training Epoch: 2/2, step 6256/7134 completed (loss: 0.006954109761863947, acc: 0.9971631169319153)
[2025-02-13 04:26:16,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:17,212][root][INFO] - Training Epoch: 2/2, step 6257/7134 completed (loss: 0.02041046880185604, acc: 0.9956834316253662)
[2025-02-13 04:26:17,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:17,594][root][INFO] - Training Epoch: 2/2, step 6258/7134 completed (loss: 0.009625720791518688, acc: 0.9968000054359436)
[2025-02-13 04:26:17,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:18,013][root][INFO] - Training Epoch: 2/2, step 6259/7134 completed (loss: 0.013351642526686192, acc: 0.9939939975738525)
[2025-02-13 04:26:18,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:18,405][root][INFO] - Training Epoch: 2/2, step 6260/7134 completed (loss: 0.023350294679403305, acc: 0.9916107654571533)
[2025-02-13 04:26:18,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:18,834][root][INFO] - Training Epoch: 2/2, step 6261/7134 completed (loss: 0.018942201510071754, acc: 0.9954268336296082)
[2025-02-13 04:26:18,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:19,234][root][INFO] - Training Epoch: 2/2, step 6262/7134 completed (loss: 0.007877119816839695, acc: 0.9971305727958679)
[2025-02-13 04:26:19,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:19,630][root][INFO] - Training Epoch: 2/2, step 6263/7134 completed (loss: 0.002355156699195504, acc: 1.0)
[2025-02-13 04:26:19,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:20,038][root][INFO] - Training Epoch: 2/2, step 6264/7134 completed (loss: 0.005748264025896788, acc: 0.9967159032821655)
[2025-02-13 04:26:20,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:20,468][root][INFO] - Training Epoch: 2/2, step 6265/7134 completed (loss: 0.008058596402406693, acc: 0.9985207319259644)
[2025-02-13 04:26:20,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:20,864][root][INFO] - Training Epoch: 2/2, step 6266/7134 completed (loss: 0.008323081769049168, acc: 0.9947552680969238)
[2025-02-13 04:26:21,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:21,266][root][INFO] - Training Epoch: 2/2, step 6267/7134 completed (loss: 0.019021030515432358, acc: 0.9951140284538269)
[2025-02-13 04:26:21,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:21,691][root][INFO] - Training Epoch: 2/2, step 6268/7134 completed (loss: 0.016749931499361992, acc: 0.9985119104385376)
[2025-02-13 04:26:21,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:22,097][root][INFO] - Training Epoch: 2/2, step 6269/7134 completed (loss: 0.016035208478569984, acc: 0.9957355856895447)
[2025-02-13 04:26:22,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:22,506][root][INFO] - Training Epoch: 2/2, step 6270/7134 completed (loss: 0.003583001671358943, acc: 0.9984662532806396)
[2025-02-13 04:26:22,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:22,926][root][INFO] - Training Epoch: 2/2, step 6271/7134 completed (loss: 0.017583714798092842, acc: 0.9973261952400208)
[2025-02-13 04:26:23,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:23,379][root][INFO] - Training Epoch: 2/2, step 6272/7134 completed (loss: 0.006308054085820913, acc: 0.9985315799713135)
[2025-02-13 04:26:23,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:23,820][root][INFO] - Training Epoch: 2/2, step 6273/7134 completed (loss: 0.007907673716545105, acc: 0.99863201379776)
[2025-02-13 04:26:23,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:24,227][root][INFO] - Training Epoch: 2/2, step 6274/7134 completed (loss: 0.012462439015507698, acc: 0.9971469044685364)
[2025-02-13 04:26:24,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:24,640][root][INFO] - Training Epoch: 2/2, step 6275/7134 completed (loss: 0.03032318875193596, acc: 0.9939117431640625)
[2025-02-13 04:26:24,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:25,003][root][INFO] - Training Epoch: 2/2, step 6276/7134 completed (loss: 0.02052241563796997, acc: 0.9947460889816284)
[2025-02-13 04:26:25,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:25,369][root][INFO] - Training Epoch: 2/2, step 6277/7134 completed (loss: 0.04893502965569496, acc: 0.9841628670692444)
[2025-02-13 04:26:25,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:25,767][root][INFO] - Training Epoch: 2/2, step 6278/7134 completed (loss: 0.004020349122583866, acc: 1.0)
[2025-02-13 04:26:25,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:26,162][root][INFO] - Training Epoch: 2/2, step 6279/7134 completed (loss: 0.008044998161494732, acc: 0.9982876777648926)
[2025-02-13 04:26:26,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:26,556][root][INFO] - Training Epoch: 2/2, step 6280/7134 completed (loss: 0.018085341900587082, acc: 0.9895287752151489)
[2025-02-13 04:26:26,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:26,955][root][INFO] - Training Epoch: 2/2, step 6281/7134 completed (loss: 0.013980680145323277, acc: 0.9965338110923767)
[2025-02-13 04:26:27,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:27,371][root][INFO] - Training Epoch: 2/2, step 6282/7134 completed (loss: 0.00658661313354969, acc: 0.9968553185462952)
[2025-02-13 04:26:27,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:27,777][root][INFO] - Training Epoch: 2/2, step 6283/7134 completed (loss: 0.03271874040365219, acc: 0.9912663698196411)
[2025-02-13 04:26:27,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:28,170][root][INFO] - Training Epoch: 2/2, step 6284/7134 completed (loss: 0.04198772460222244, acc: 0.9824945330619812)
[2025-02-13 04:26:28,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:28,525][root][INFO] - Training Epoch: 2/2, step 6285/7134 completed (loss: 0.015405896119773388, acc: 0.9947506785392761)
[2025-02-13 04:26:28,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:28,917][root][INFO] - Training Epoch: 2/2, step 6286/7134 completed (loss: 0.007620569784194231, acc: 0.9967105388641357)
[2025-02-13 04:26:29,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:29,325][root][INFO] - Training Epoch: 2/2, step 6287/7134 completed (loss: 0.0241011343896389, acc: 0.9926470518112183)
[2025-02-13 04:26:29,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:29,725][root][INFO] - Training Epoch: 2/2, step 6288/7134 completed (loss: 0.006910381838679314, acc: 0.9967319965362549)
[2025-02-13 04:26:29,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:30,099][root][INFO] - Training Epoch: 2/2, step 6289/7134 completed (loss: 0.022946452721953392, acc: 0.9945945739746094)
[2025-02-13 04:26:30,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:30,510][root][INFO] - Training Epoch: 2/2, step 6290/7134 completed (loss: 0.01220074761658907, acc: 0.9983999729156494)
[2025-02-13 04:26:30,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:30,883][root][INFO] - Training Epoch: 2/2, step 6291/7134 completed (loss: 0.008968334645032883, acc: 0.996363639831543)
[2025-02-13 04:26:31,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:31,232][root][INFO] - Training Epoch: 2/2, step 6292/7134 completed (loss: 0.026752755045890808, acc: 0.9908883571624756)
[2025-02-13 04:26:31,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:31,651][root][INFO] - Training Epoch: 2/2, step 6293/7134 completed (loss: 0.047537077218294144, acc: 0.988054633140564)
[2025-02-13 04:26:31,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:32,069][root][INFO] - Training Epoch: 2/2, step 6294/7134 completed (loss: 0.022327808663249016, acc: 0.9935170412063599)
[2025-02-13 04:26:32,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:32,467][root][INFO] - Training Epoch: 2/2, step 6295/7134 completed (loss: 0.034943051636219025, acc: 0.9873417615890503)
[2025-02-13 04:26:32,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:32,830][root][INFO] - Training Epoch: 2/2, step 6296/7134 completed (loss: 0.04920971766114235, acc: 0.9862204790115356)
[2025-02-13 04:26:32,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:33,234][root][INFO] - Training Epoch: 2/2, step 6297/7134 completed (loss: 0.024736564606428146, acc: 0.9901639223098755)
[2025-02-13 04:26:33,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:33,636][root][INFO] - Training Epoch: 2/2, step 6298/7134 completed (loss: 0.030690934509038925, acc: 0.9862542748451233)
[2025-02-13 04:26:33,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:34,049][root][INFO] - Training Epoch: 2/2, step 6299/7134 completed (loss: 0.05600309371948242, acc: 0.9824281334877014)
[2025-02-13 04:26:34,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:34,456][root][INFO] - Training Epoch: 2/2, step 6300/7134 completed (loss: 0.03529735654592514, acc: 0.9882746934890747)
[2025-02-13 04:26:34,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:34,906][root][INFO] - Training Epoch: 2/2, step 6301/7134 completed (loss: 0.010018154978752136, acc: 0.9965517520904541)
[2025-02-13 04:26:35,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:35,334][root][INFO] - Training Epoch: 2/2, step 6302/7134 completed (loss: 0.0069381012581288815, acc: 0.9972375631332397)
[2025-02-13 04:26:35,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:35,803][root][INFO] - Training Epoch: 2/2, step 6303/7134 completed (loss: 0.009353927336633205, acc: 0.9989350438117981)
[2025-02-13 04:26:35,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:36,239][root][INFO] - Training Epoch: 2/2, step 6304/7134 completed (loss: 0.02429029904305935, acc: 0.9891892075538635)
[2025-02-13 04:26:36,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:36,668][root][INFO] - Training Epoch: 2/2, step 6305/7134 completed (loss: 0.007729758974164724, acc: 0.9983633160591125)
[2025-02-13 04:26:36,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:37,111][root][INFO] - Training Epoch: 2/2, step 6306/7134 completed (loss: 0.03687620162963867, acc: 0.9903961420059204)
[2025-02-13 04:26:37,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:37,547][root][INFO] - Training Epoch: 2/2, step 6307/7134 completed (loss: 0.04084886237978935, acc: 0.9947090148925781)
[2025-02-13 04:26:37,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:37,980][root][INFO] - Training Epoch: 2/2, step 6308/7134 completed (loss: 0.01796380802989006, acc: 0.9950860142707825)
[2025-02-13 04:26:38,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:38,387][root][INFO] - Training Epoch: 2/2, step 6309/7134 completed (loss: 0.011832904070615768, acc: 0.995199978351593)
[2025-02-13 04:26:38,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:38,765][root][INFO] - Training Epoch: 2/2, step 6310/7134 completed (loss: 0.028761940076947212, acc: 0.9890109896659851)
[2025-02-13 04:26:38,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:39,201][root][INFO] - Training Epoch: 2/2, step 6311/7134 completed (loss: 0.019702546298503876, acc: 0.9963099360466003)
[2025-02-13 04:26:39,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:39,606][root][INFO] - Training Epoch: 2/2, step 6312/7134 completed (loss: 0.021881770342588425, acc: 0.9937106966972351)
[2025-02-13 04:26:39,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:40,039][root][INFO] - Training Epoch: 2/2, step 6313/7134 completed (loss: 0.014148307032883167, acc: 0.9950920343399048)
[2025-02-13 04:26:40,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:40,489][root][INFO] - Training Epoch: 2/2, step 6314/7134 completed (loss: 0.013326470740139484, acc: 0.9987389445304871)
[2025-02-13 04:26:40,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:40,914][root][INFO] - Training Epoch: 2/2, step 6315/7134 completed (loss: 0.0406268872320652, acc: 0.9918032884597778)
[2025-02-13 04:26:41,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:41,342][root][INFO] - Training Epoch: 2/2, step 6316/7134 completed (loss: 0.020016593858599663, acc: 0.9967585206031799)
[2025-02-13 04:26:41,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:41,734][root][INFO] - Training Epoch: 2/2, step 6317/7134 completed (loss: 0.00759587436914444, acc: 1.0)
[2025-02-13 04:26:41,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:42,140][root][INFO] - Training Epoch: 2/2, step 6318/7134 completed (loss: 0.008889660239219666, acc: 0.995230495929718)
[2025-02-13 04:26:42,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:42,594][root][INFO] - Training Epoch: 2/2, step 6319/7134 completed (loss: 0.009794789366424084, acc: 0.9969135522842407)
[2025-02-13 04:26:42,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:43,059][root][INFO] - Training Epoch: 2/2, step 6320/7134 completed (loss: 0.014780459925532341, acc: 0.9936808943748474)
[2025-02-13 04:26:43,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:43,501][root][INFO] - Training Epoch: 2/2, step 6321/7134 completed (loss: 0.0071501159109175205, acc: 0.9966517686843872)
[2025-02-13 04:26:43,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:43,941][root][INFO] - Training Epoch: 2/2, step 6322/7134 completed (loss: 0.02901303768157959, acc: 0.992732584476471)
[2025-02-13 04:26:44,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:44,390][root][INFO] - Training Epoch: 2/2, step 6323/7134 completed (loss: 0.013724388554692268, acc: 0.9954954981803894)
[2025-02-13 04:26:44,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:44,824][root][INFO] - Training Epoch: 2/2, step 6324/7134 completed (loss: 0.013819477520883083, acc: 0.9946737885475159)
[2025-02-13 04:26:44,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:45,212][root][INFO] - Training Epoch: 2/2, step 6325/7134 completed (loss: 0.013795791193842888, acc: 0.99245285987854)
[2025-02-13 04:26:45,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:45,623][root][INFO] - Training Epoch: 2/2, step 6326/7134 completed (loss: 0.026955392211675644, acc: 0.9919742941856384)
[2025-02-13 04:26:45,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:46,025][root][INFO] - Training Epoch: 2/2, step 6327/7134 completed (loss: 0.017943818122148514, acc: 0.9920844435691833)
[2025-02-13 04:26:46,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:46,438][root][INFO] - Training Epoch: 2/2, step 6328/7134 completed (loss: 0.02011192962527275, acc: 0.9920254945755005)
[2025-02-13 04:26:46,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:46,847][root][INFO] - Training Epoch: 2/2, step 6329/7134 completed (loss: 0.016381550580263138, acc: 0.9954545497894287)
[2025-02-13 04:26:46,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:47,246][root][INFO] - Training Epoch: 2/2, step 6330/7134 completed (loss: 0.011171934194862843, acc: 0.9950371980667114)
[2025-02-13 04:26:47,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:47,654][root][INFO] - Training Epoch: 2/2, step 6331/7134 completed (loss: 0.0122739989310503, acc: 0.9946996569633484)
[2025-02-13 04:26:47,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:48,077][root][INFO] - Training Epoch: 2/2, step 6332/7134 completed (loss: 0.04692074656486511, acc: 0.9851301312446594)
[2025-02-13 04:26:48,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:48,481][root][INFO] - Training Epoch: 2/2, step 6333/7134 completed (loss: 0.03187226504087448, acc: 0.9888888597488403)
[2025-02-13 04:26:48,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:48,834][root][INFO] - Training Epoch: 2/2, step 6334/7134 completed (loss: 0.0288643017411232, acc: 0.9905303120613098)
[2025-02-13 04:26:48,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:49,235][root][INFO] - Training Epoch: 2/2, step 6335/7134 completed (loss: 0.008250122889876366, acc: 0.994966447353363)
[2025-02-13 04:26:49,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:49,629][root][INFO] - Training Epoch: 2/2, step 6336/7134 completed (loss: 0.02280012145638466, acc: 0.9908257126808167)
[2025-02-13 04:26:49,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:50,050][root][INFO] - Training Epoch: 2/2, step 6337/7134 completed (loss: 0.00828301627188921, acc: 0.9981752038002014)
[2025-02-13 04:26:50,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:50,468][root][INFO] - Training Epoch: 2/2, step 6338/7134 completed (loss: 0.019925272092223167, acc: 0.9921011328697205)
[2025-02-13 04:26:50,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:50,873][root][INFO] - Training Epoch: 2/2, step 6339/7134 completed (loss: 0.013796770013868809, acc: 0.9948096871376038)
[2025-02-13 04:26:51,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:51,287][root][INFO] - Training Epoch: 2/2, step 6340/7134 completed (loss: 0.015486722812056541, acc: 0.9941349029541016)
[2025-02-13 04:26:51,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:51,683][root][INFO] - Training Epoch: 2/2, step 6341/7134 completed (loss: 0.008136818185448647, acc: 0.9981024861335754)
[2025-02-13 04:26:51,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:52,131][root][INFO] - Training Epoch: 2/2, step 6342/7134 completed (loss: 0.021125294268131256, acc: 0.9905787110328674)
[2025-02-13 04:26:52,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:52,564][root][INFO] - Training Epoch: 2/2, step 6343/7134 completed (loss: 0.029505150392651558, acc: 0.9948365092277527)
[2025-02-13 04:26:52,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:53,015][root][INFO] - Training Epoch: 2/2, step 6344/7134 completed (loss: 0.010756237432360649, acc: 0.99726402759552)
[2025-02-13 04:26:53,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:53,444][root][INFO] - Training Epoch: 2/2, step 6345/7134 completed (loss: 0.01710464060306549, acc: 0.9918032884597778)
[2025-02-13 04:26:53,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:53,853][root][INFO] - Training Epoch: 2/2, step 6346/7134 completed (loss: 0.05121269449591637, acc: 0.9860681295394897)
[2025-02-13 04:26:53,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:54,197][root][INFO] - Training Epoch: 2/2, step 6347/7134 completed (loss: 0.019971659407019615, acc: 0.9978355169296265)
[2025-02-13 04:26:54,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:54,609][root][INFO] - Training Epoch: 2/2, step 6348/7134 completed (loss: 0.03623310849070549, acc: 0.9913194179534912)
[2025-02-13 04:26:54,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:55,009][root][INFO] - Training Epoch: 2/2, step 6349/7134 completed (loss: 0.028130099177360535, acc: 0.9890710115432739)
[2025-02-13 04:26:55,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:55,429][root][INFO] - Training Epoch: 2/2, step 6350/7134 completed (loss: 0.04162263497710228, acc: 0.9904761910438538)
[2025-02-13 04:26:55,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:55,866][root][INFO] - Training Epoch: 2/2, step 6351/7134 completed (loss: 0.0384005606174469, acc: 0.9895522594451904)
[2025-02-13 04:26:56,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:56,283][root][INFO] - Training Epoch: 2/2, step 6352/7134 completed (loss: 0.10844049602746964, acc: 0.9747399687767029)
[2025-02-13 04:26:56,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:56,755][root][INFO] - Training Epoch: 2/2, step 6353/7134 completed (loss: 0.018886037170886993, acc: 0.9926793575286865)
[2025-02-13 04:26:56,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:57,181][root][INFO] - Training Epoch: 2/2, step 6354/7134 completed (loss: 0.0154586061835289, acc: 0.9944598078727722)
[2025-02-13 04:26:57,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:57,591][root][INFO] - Training Epoch: 2/2, step 6355/7134 completed (loss: 0.005677244625985622, acc: 0.9983818531036377)
[2025-02-13 04:26:57,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:57,996][root][INFO] - Training Epoch: 2/2, step 6356/7134 completed (loss: 0.021698039025068283, acc: 0.9910314083099365)
[2025-02-13 04:26:58,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:58,418][root][INFO] - Training Epoch: 2/2, step 6357/7134 completed (loss: 0.007236808072775602, acc: 0.9958333373069763)
[2025-02-13 04:26:58,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:58,871][root][INFO] - Training Epoch: 2/2, step 6358/7134 completed (loss: 0.019596518948674202, acc: 0.9943661689758301)
[2025-02-13 04:26:59,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:59,274][root][INFO] - Training Epoch: 2/2, step 6359/7134 completed (loss: 0.03743361681699753, acc: 0.9915966391563416)
[2025-02-13 04:26:59,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:59,672][root][INFO] - Training Epoch: 2/2, step 6360/7134 completed (loss: 0.01625794917345047, acc: 0.9952229261398315)
[2025-02-13 04:26:59,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:00,077][root][INFO] - Training Epoch: 2/2, step 6361/7134 completed (loss: 0.04470004513859749, acc: 0.9872262477874756)
[2025-02-13 04:27:00,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:00,518][root][INFO] - Training Epoch: 2/2, step 6362/7134 completed (loss: 0.018795793876051903, acc: 0.9954268336296082)
[2025-02-13 04:27:00,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:00,938][root][INFO] - Training Epoch: 2/2, step 6363/7134 completed (loss: 0.025125721469521523, acc: 0.9941349029541016)
[2025-02-13 04:27:01,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:01,352][root][INFO] - Training Epoch: 2/2, step 6364/7134 completed (loss: 0.008418502286076546, acc: 0.9985693693161011)
[2025-02-13 04:27:01,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:01,757][root][INFO] - Training Epoch: 2/2, step 6365/7134 completed (loss: 0.014437953941524029, acc: 0.9918699264526367)
[2025-02-13 04:27:01,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:02,183][root][INFO] - Training Epoch: 2/2, step 6366/7134 completed (loss: 0.023518767207860947, acc: 0.9947159886360168)
[2025-02-13 04:27:02,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:02,577][root][INFO] - Training Epoch: 2/2, step 6367/7134 completed (loss: 0.03789478540420532, acc: 0.9905481934547424)
[2025-02-13 04:27:02,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:02,994][root][INFO] - Training Epoch: 2/2, step 6368/7134 completed (loss: 0.06464484333992004, acc: 0.9844632744789124)
[2025-02-13 04:27:03,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:03,435][root][INFO] - Training Epoch: 2/2, step 6369/7134 completed (loss: 0.032748207449913025, acc: 0.9908015727996826)
[2025-02-13 04:27:03,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:03,885][root][INFO] - Training Epoch: 2/2, step 6370/7134 completed (loss: 0.09323328733444214, acc: 0.9749340415000916)
[2025-02-13 04:27:04,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:04,299][root][INFO] - Training Epoch: 2/2, step 6371/7134 completed (loss: 0.039557717740535736, acc: 0.9875346422195435)
[2025-02-13 04:27:04,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:04,691][root][INFO] - Training Epoch: 2/2, step 6372/7134 completed (loss: 0.09528354555368423, acc: 0.9750480055809021)
[2025-02-13 04:27:04,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:05,066][root][INFO] - Training Epoch: 2/2, step 6373/7134 completed (loss: 0.04375794529914856, acc: 0.9836065769195557)
[2025-02-13 04:27:05,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:05,489][root][INFO] - Training Epoch: 2/2, step 6374/7134 completed (loss: 0.01215589139610529, acc: 0.9958506226539612)
[2025-02-13 04:27:05,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:05,923][root][INFO] - Training Epoch: 2/2, step 6375/7134 completed (loss: 0.04463644698262215, acc: 0.9858793616294861)
[2025-02-13 04:27:06,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:06,312][root][INFO] - Training Epoch: 2/2, step 6376/7134 completed (loss: 0.01727323792874813, acc: 0.9926199316978455)
[2025-02-13 04:27:06,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:06,696][root][INFO] - Training Epoch: 2/2, step 6377/7134 completed (loss: 0.03227395564317703, acc: 0.9908424615859985)
[2025-02-13 04:27:06,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:07,158][root][INFO] - Training Epoch: 2/2, step 6378/7134 completed (loss: 0.04770948737859726, acc: 0.984994649887085)
[2025-02-13 04:27:07,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:07,554][root][INFO] - Training Epoch: 2/2, step 6379/7134 completed (loss: 0.02025494910776615, acc: 0.9949238300323486)
[2025-02-13 04:27:07,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:08,010][root][INFO] - Training Epoch: 2/2, step 6380/7134 completed (loss: 0.039566680788993835, acc: 0.9910179376602173)
[2025-02-13 04:27:08,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:08,435][root][INFO] - Training Epoch: 2/2, step 6381/7134 completed (loss: 0.024051718413829803, acc: 0.9938931465148926)
[2025-02-13 04:27:08,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:08,881][root][INFO] - Training Epoch: 2/2, step 6382/7134 completed (loss: 0.0041788555681705475, acc: 1.0)
[2025-02-13 04:27:09,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:09,316][root][INFO] - Training Epoch: 2/2, step 6383/7134 completed (loss: 0.06264010071754456, acc: 0.9891451597213745)
[2025-02-13 04:27:09,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:09,754][root][INFO] - Training Epoch: 2/2, step 6384/7134 completed (loss: 0.024077314883470535, acc: 0.994020938873291)
[2025-02-13 04:27:09,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:10,179][root][INFO] - Training Epoch: 2/2, step 6385/7134 completed (loss: 0.07044297456741333, acc: 0.9868263602256775)
[2025-02-13 04:27:10,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:10,649][root][INFO] - Training Epoch: 2/2, step 6386/7134 completed (loss: 0.008089973591268063, acc: 0.9987789988517761)
[2025-02-13 04:27:10,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:11,078][root][INFO] - Training Epoch: 2/2, step 6387/7134 completed (loss: 0.03983457013964653, acc: 0.9903225898742676)
[2025-02-13 04:27:11,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:11,489][root][INFO] - Training Epoch: 2/2, step 6388/7134 completed (loss: 0.041303884238004684, acc: 0.9880239367485046)
[2025-02-13 04:27:11,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:11,946][root][INFO] - Training Epoch: 2/2, step 6389/7134 completed (loss: 0.05558760464191437, acc: 0.9841269850730896)
[2025-02-13 04:27:12,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:12,426][root][INFO] - Training Epoch: 2/2, step 6390/7134 completed (loss: 0.05396227166056633, acc: 0.9848178029060364)
[2025-02-13 04:27:12,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:12,854][root][INFO] - Training Epoch: 2/2, step 6391/7134 completed (loss: 0.07429493218660355, acc: 0.9820742607116699)
[2025-02-13 04:27:12,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:13,301][root][INFO] - Training Epoch: 2/2, step 6392/7134 completed (loss: 0.05198125168681145, acc: 0.9846153855323792)
[2025-02-13 04:27:13,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:13,756][root][INFO] - Training Epoch: 2/2, step 6393/7134 completed (loss: 0.017671843990683556, acc: 0.9943116903305054)
[2025-02-13 04:27:13,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:14,201][root][INFO] - Training Epoch: 2/2, step 6394/7134 completed (loss: 0.05652706325054169, acc: 0.9847009778022766)
[2025-02-13 04:27:14,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:14,665][root][INFO] - Training Epoch: 2/2, step 6395/7134 completed (loss: 0.02728705108165741, acc: 0.9950799345970154)
[2025-02-13 04:27:14,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:15,104][root][INFO] - Training Epoch: 2/2, step 6396/7134 completed (loss: 0.05244091525673866, acc: 0.9873853325843811)
[2025-02-13 04:27:15,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:15,565][root][INFO] - Training Epoch: 2/2, step 6397/7134 completed (loss: 0.022136522457003593, acc: 0.9925558567047119)
[2025-02-13 04:27:15,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:16,006][root][INFO] - Training Epoch: 2/2, step 6398/7134 completed (loss: 0.025994278490543365, acc: 0.9887482523918152)
[2025-02-13 04:27:16,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:16,454][root][INFO] - Training Epoch: 2/2, step 6399/7134 completed (loss: 0.05162499099969864, acc: 0.9849624037742615)
[2025-02-13 04:27:16,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:16,880][root][INFO] - Training Epoch: 2/2, step 6400/7134 completed (loss: 0.021399976685643196, acc: 0.995726466178894)
[2025-02-13 04:27:17,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:17,342][root][INFO] - Training Epoch: 2/2, step 6401/7134 completed (loss: 0.033717185258865356, acc: 0.9913138151168823)
[2025-02-13 04:27:17,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:17,782][root][INFO] - Training Epoch: 2/2, step 6402/7134 completed (loss: 0.042766496539115906, acc: 0.9909194111824036)
[2025-02-13 04:27:17,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:18,179][root][INFO] - Training Epoch: 2/2, step 6403/7134 completed (loss: 0.03220918029546738, acc: 0.9897959232330322)
[2025-02-13 04:27:18,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:18,610][root][INFO] - Training Epoch: 2/2, step 6404/7134 completed (loss: 0.03201715648174286, acc: 0.9930459260940552)
[2025-02-13 04:27:18,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:19,098][root][INFO] - Training Epoch: 2/2, step 6405/7134 completed (loss: 0.016541535034775734, acc: 0.9946236610412598)
[2025-02-13 04:27:19,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:19,560][root][INFO] - Training Epoch: 2/2, step 6406/7134 completed (loss: 0.030244912952184677, acc: 0.9920182228088379)
[2025-02-13 04:27:19,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:20,018][root][INFO] - Training Epoch: 2/2, step 6407/7134 completed (loss: 0.022414647042751312, acc: 0.9934297204017639)
[2025-02-13 04:27:20,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:20,428][root][INFO] - Training Epoch: 2/2, step 6408/7134 completed (loss: 0.05187042057514191, acc: 0.9881831407546997)
[2025-02-13 04:27:20,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:20,849][root][INFO] - Training Epoch: 2/2, step 6409/7134 completed (loss: 0.03185208886861801, acc: 0.9947368502616882)
[2025-02-13 04:27:20,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:21,258][root][INFO] - Training Epoch: 2/2, step 6410/7134 completed (loss: 0.01629028096795082, acc: 0.9908088445663452)
[2025-02-13 04:27:21,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:21,647][root][INFO] - Training Epoch: 2/2, step 6411/7134 completed (loss: 0.031821198761463165, acc: 0.9899497628211975)
[2025-02-13 04:27:21,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:22,051][root][INFO] - Training Epoch: 2/2, step 6412/7134 completed (loss: 0.009155631996691227, acc: 0.9973333477973938)
[2025-02-13 04:27:22,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:22,496][root][INFO] - Training Epoch: 2/2, step 6413/7134 completed (loss: 0.044803496450185776, acc: 0.9863013625144958)
[2025-02-13 04:27:22,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:22,888][root][INFO] - Training Epoch: 2/2, step 6414/7134 completed (loss: 0.008726615458726883, acc: 0.9984894394874573)
[2025-02-13 04:27:23,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:23,306][root][INFO] - Training Epoch: 2/2, step 6415/7134 completed (loss: 0.02134620025753975, acc: 0.9910141229629517)
[2025-02-13 04:27:23,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:23,718][root][INFO] - Training Epoch: 2/2, step 6416/7134 completed (loss: 0.03209520876407623, acc: 0.9886147975921631)
[2025-02-13 04:27:23,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:24,133][root][INFO] - Training Epoch: 2/2, step 6417/7134 completed (loss: 0.06176779419183731, acc: 0.982425332069397)
[2025-02-13 04:27:24,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:24,568][root][INFO] - Training Epoch: 2/2, step 6418/7134 completed (loss: 0.08414675295352936, acc: 0.976123571395874)
[2025-02-13 04:27:24,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:24,955][root][INFO] - Training Epoch: 2/2, step 6419/7134 completed (loss: 0.050880398601293564, acc: 0.9869494438171387)
[2025-02-13 04:27:25,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:25,388][root][INFO] - Training Epoch: 2/2, step 6420/7134 completed (loss: 0.02989240549504757, acc: 0.9899135231971741)
[2025-02-13 04:27:25,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:25,794][root][INFO] - Training Epoch: 2/2, step 6421/7134 completed (loss: 0.028136201202869415, acc: 0.9910846948623657)
[2025-02-13 04:27:25,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:26,224][root][INFO] - Training Epoch: 2/2, step 6422/7134 completed (loss: 0.033694107085466385, acc: 0.9890561103820801)
[2025-02-13 04:27:26,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:26,645][root][INFO] - Training Epoch: 2/2, step 6423/7134 completed (loss: 0.04932323470711708, acc: 0.9912023544311523)
[2025-02-13 04:27:26,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:27,091][root][INFO] - Training Epoch: 2/2, step 6424/7134 completed (loss: 0.0361650176346302, acc: 0.9861286282539368)
[2025-02-13 04:27:27,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:27,546][root][INFO] - Training Epoch: 2/2, step 6425/7134 completed (loss: 0.02817661501467228, acc: 0.9895697236061096)
[2025-02-13 04:27:27,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:28,004][root][INFO] - Training Epoch: 2/2, step 6426/7134 completed (loss: 0.03041740693151951, acc: 0.9907651543617249)
[2025-02-13 04:27:28,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:28,432][root][INFO] - Training Epoch: 2/2, step 6427/7134 completed (loss: 0.04807319492101669, acc: 0.9872159361839294)
[2025-02-13 04:27:28,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:28,875][root][INFO] - Training Epoch: 2/2, step 6428/7134 completed (loss: 0.01951027102768421, acc: 0.9916201233863831)
[2025-02-13 04:27:29,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:29,288][root][INFO] - Training Epoch: 2/2, step 6429/7134 completed (loss: 0.013479134067893028, acc: 0.9939393997192383)
[2025-02-13 04:27:29,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:29,730][root][INFO] - Training Epoch: 2/2, step 6430/7134 completed (loss: 0.04681658744812012, acc: 0.9841726422309875)
[2025-02-13 04:27:29,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:30,135][root][INFO] - Training Epoch: 2/2, step 6431/7134 completed (loss: 0.05687427148222923, acc: 0.9868420958518982)
[2025-02-13 04:27:30,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:30,537][root][INFO] - Training Epoch: 2/2, step 6432/7134 completed (loss: 0.1434166580438614, acc: 0.9696969985961914)
[2025-02-13 04:27:30,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:30,962][root][INFO] - Training Epoch: 2/2, step 6433/7134 completed (loss: 0.03192271664738655, acc: 0.9904371500015259)
[2025-02-13 04:27:31,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:31,362][root][INFO] - Training Epoch: 2/2, step 6434/7134 completed (loss: 0.006171346642076969, acc: 1.0)
[2025-02-13 04:27:31,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:31,776][root][INFO] - Training Epoch: 2/2, step 6435/7134 completed (loss: 0.04207273945212364, acc: 0.9878683090209961)
[2025-02-13 04:27:31,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:32,210][root][INFO] - Training Epoch: 2/2, step 6436/7134 completed (loss: 0.01160958968102932, acc: 0.9953632354736328)
[2025-02-13 04:27:32,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:32,643][root][INFO] - Training Epoch: 2/2, step 6437/7134 completed (loss: 0.032231081277132034, acc: 0.9916666746139526)
[2025-02-13 04:27:32,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:33,099][root][INFO] - Training Epoch: 2/2, step 6438/7134 completed (loss: 0.032926592975854874, acc: 0.990111231803894)
[2025-02-13 04:27:33,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:33,513][root][INFO] - Training Epoch: 2/2, step 6439/7134 completed (loss: 0.03868551924824715, acc: 0.98740154504776)
[2025-02-13 04:27:33,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:33,978][root][INFO] - Training Epoch: 2/2, step 6440/7134 completed (loss: 0.018381234258413315, acc: 0.9952210187911987)
[2025-02-13 04:27:34,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:34,400][root][INFO] - Training Epoch: 2/2, step 6441/7134 completed (loss: 0.016650421544909477, acc: 0.9955686926841736)
[2025-02-13 04:27:34,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:34,871][root][INFO] - Training Epoch: 2/2, step 6442/7134 completed (loss: 0.025325417518615723, acc: 0.9954596757888794)
[2025-02-13 04:27:35,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:35,314][root][INFO] - Training Epoch: 2/2, step 6443/7134 completed (loss: 0.016489116474986076, acc: 0.9950494766235352)
[2025-02-13 04:27:35,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:35,768][root][INFO] - Training Epoch: 2/2, step 6444/7134 completed (loss: 0.027191825211048126, acc: 0.9916201233863831)
[2025-02-13 04:27:35,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:36,208][root][INFO] - Training Epoch: 2/2, step 6445/7134 completed (loss: 0.0438726432621479, acc: 0.9889025688171387)
[2025-02-13 04:27:36,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:36,655][root][INFO] - Training Epoch: 2/2, step 6446/7134 completed (loss: 0.018654363229870796, acc: 0.9964243173599243)
[2025-02-13 04:27:36,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:37,129][root][INFO] - Training Epoch: 2/2, step 6447/7134 completed (loss: 0.028216000646352768, acc: 0.9907940030097961)
[2025-02-13 04:27:37,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:37,557][root][INFO] - Training Epoch: 2/2, step 6448/7134 completed (loss: 0.08040868490934372, acc: 0.9828660488128662)
[2025-02-13 04:27:37,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:37,995][root][INFO] - Training Epoch: 2/2, step 6449/7134 completed (loss: 0.01748006045818329, acc: 0.9971014261245728)
[2025-02-13 04:27:38,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:38,448][root][INFO] - Training Epoch: 2/2, step 6450/7134 completed (loss: 0.020153861492872238, acc: 0.9940476417541504)
[2025-02-13 04:27:38,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:38,964][root][INFO] - Training Epoch: 2/2, step 6451/7134 completed (loss: 0.03635656088590622, acc: 0.9882467985153198)
[2025-02-13 04:27:39,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:39,396][root][INFO] - Training Epoch: 2/2, step 6452/7134 completed (loss: 0.013660815544426441, acc: 0.9955406785011292)
[2025-02-13 04:27:39,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:39,830][root][INFO] - Training Epoch: 2/2, step 6453/7134 completed (loss: 0.016369551420211792, acc: 0.9946236610412598)
[2025-02-13 04:27:39,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:40,310][root][INFO] - Training Epoch: 2/2, step 6454/7134 completed (loss: 0.03036162070930004, acc: 0.9952885508537292)
[2025-02-13 04:27:40,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:40,734][root][INFO] - Training Epoch: 2/2, step 6455/7134 completed (loss: 0.0166765246540308, acc: 0.9966044425964355)
[2025-02-13 04:27:40,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:41,143][root][INFO] - Training Epoch: 2/2, step 6456/7134 completed (loss: 0.022528717294335365, acc: 0.9938650131225586)
[2025-02-13 04:27:41,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:41,571][root][INFO] - Training Epoch: 2/2, step 6457/7134 completed (loss: 0.00799653772264719, acc: 0.9977973699569702)
[2025-02-13 04:27:41,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:41,957][root][INFO] - Training Epoch: 2/2, step 6458/7134 completed (loss: 0.015541906468570232, acc: 0.9949579834938049)
[2025-02-13 04:27:42,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:42,378][root][INFO] - Training Epoch: 2/2, step 6459/7134 completed (loss: 0.001342636183835566, acc: 1.0)
[2025-02-13 04:27:42,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:42,843][root][INFO] - Training Epoch: 2/2, step 6460/7134 completed (loss: 0.005072919651865959, acc: 0.9976387023925781)
[2025-02-13 04:27:42,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:43,266][root][INFO] - Training Epoch: 2/2, step 6461/7134 completed (loss: 0.00957176461815834, acc: 0.9973718523979187)
[2025-02-13 04:27:43,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:43,689][root][INFO] - Training Epoch: 2/2, step 6462/7134 completed (loss: 0.012210684828460217, acc: 0.9975399971008301)
[2025-02-13 04:27:43,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:44,155][root][INFO] - Training Epoch: 2/2, step 6463/7134 completed (loss: 0.015989400446414948, acc: 0.9954441785812378)
[2025-02-13 04:27:44,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:44,581][root][INFO] - Training Epoch: 2/2, step 6464/7134 completed (loss: 0.015412507578730583, acc: 0.9947984218597412)
[2025-02-13 04:27:44,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:45,000][root][INFO] - Training Epoch: 2/2, step 6465/7134 completed (loss: 0.016870029270648956, acc: 0.9961439371109009)
[2025-02-13 04:27:45,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:45,413][root][INFO] - Training Epoch: 2/2, step 6466/7134 completed (loss: 0.013034248724579811, acc: 0.9960370063781738)
[2025-02-13 04:27:45,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:45,839][root][INFO] - Training Epoch: 2/2, step 6467/7134 completed (loss: 0.028775181621313095, acc: 0.9923664331436157)
[2025-02-13 04:27:45,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:46,270][root][INFO] - Training Epoch: 2/2, step 6468/7134 completed (loss: 0.0194683987647295, acc: 0.9906191229820251)
[2025-02-13 04:27:46,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:46,733][root][INFO] - Training Epoch: 2/2, step 6469/7134 completed (loss: 0.03022024780511856, acc: 0.9914737939834595)
[2025-02-13 04:27:46,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:47,104][root][INFO] - Training Epoch: 2/2, step 6470/7134 completed (loss: 0.052607908844947815, acc: 0.9853300452232361)
[2025-02-13 04:27:47,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:47,544][root][INFO] - Training Epoch: 2/2, step 6471/7134 completed (loss: 0.011841967701911926, acc: 0.9951140284538269)
[2025-02-13 04:27:47,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:48,014][root][INFO] - Training Epoch: 2/2, step 6472/7134 completed (loss: 0.05855241045355797, acc: 0.9893617033958435)
[2025-02-13 04:27:48,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:48,450][root][INFO] - Training Epoch: 2/2, step 6473/7134 completed (loss: 0.010361145250499249, acc: 0.9959785342216492)
[2025-02-13 04:27:48,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:48,862][root][INFO] - Training Epoch: 2/2, step 6474/7134 completed (loss: 0.012642978690564632, acc: 0.9959623217582703)
[2025-02-13 04:27:48,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:49,240][root][INFO] - Training Epoch: 2/2, step 6475/7134 completed (loss: 0.024075714871287346, acc: 0.9950000047683716)
[2025-02-13 04:27:49,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:49,586][root][INFO] - Training Epoch: 2/2, step 6476/7134 completed (loss: 0.04555632174015045, acc: 0.9933993220329285)
[2025-02-13 04:27:49,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:49,968][root][INFO] - Training Epoch: 2/2, step 6477/7134 completed (loss: 0.02942681685090065, acc: 0.9936440587043762)
[2025-02-13 04:27:50,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:50,374][root][INFO] - Training Epoch: 2/2, step 6478/7134 completed (loss: 0.012679321691393852, acc: 0.9944649338722229)
[2025-02-13 04:27:50,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:50,797][root][INFO] - Training Epoch: 2/2, step 6479/7134 completed (loss: 0.011410431005060673, acc: 0.9966499209403992)
[2025-02-13 04:27:50,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:51,194][root][INFO] - Training Epoch: 2/2, step 6480/7134 completed (loss: 0.041881293058395386, acc: 0.9845361113548279)
[2025-02-13 04:27:51,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:51,602][root][INFO] - Training Epoch: 2/2, step 6481/7134 completed (loss: 0.009151384234428406, acc: 0.9982993006706238)
[2025-02-13 04:27:51,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:52,002][root][INFO] - Training Epoch: 2/2, step 6482/7134 completed (loss: 0.012446019798517227, acc: 0.9950000047683716)
[2025-02-13 04:27:52,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:52,417][root][INFO] - Training Epoch: 2/2, step 6483/7134 completed (loss: 0.04790746048092842, acc: 0.9909747242927551)
[2025-02-13 04:27:52,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:52,825][root][INFO] - Training Epoch: 2/2, step 6484/7134 completed (loss: 0.026358747854828835, acc: 0.992668628692627)
[2025-02-13 04:27:52,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:53,286][root][INFO] - Training Epoch: 2/2, step 6485/7134 completed (loss: 0.009837224148213863, acc: 0.9980236887931824)
[2025-02-13 04:27:53,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:53,687][root][INFO] - Training Epoch: 2/2, step 6486/7134 completed (loss: 0.08616713434457779, acc: 0.9818181991577148)
[2025-02-13 04:27:53,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:54,018][root][INFO] - Training Epoch: 2/2, step 6487/7134 completed (loss: 0.0984826311469078, acc: 0.97773277759552)
[2025-02-13 04:27:54,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:54,400][root][INFO] - Training Epoch: 2/2, step 6488/7134 completed (loss: 0.06694163382053375, acc: 0.9758551120758057)
[2025-02-13 04:27:54,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:54,801][root][INFO] - Training Epoch: 2/2, step 6489/7134 completed (loss: 0.07841498404741287, acc: 0.9813432693481445)
[2025-02-13 04:27:54,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:55,169][root][INFO] - Training Epoch: 2/2, step 6490/7134 completed (loss: 0.04166239872574806, acc: 0.9931389093399048)
[2025-02-13 04:27:55,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:55,604][root][INFO] - Training Epoch: 2/2, step 6491/7134 completed (loss: 0.060745470225811005, acc: 0.9865047335624695)
[2025-02-13 04:27:55,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:56,022][root][INFO] - Training Epoch: 2/2, step 6492/7134 completed (loss: 0.01688913255929947, acc: 0.9961190223693848)
[2025-02-13 04:27:56,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:56,482][root][INFO] - Training Epoch: 2/2, step 6493/7134 completed (loss: 0.05414612218737602, acc: 0.9872773289680481)
[2025-02-13 04:27:56,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:56,869][root][INFO] - Training Epoch: 2/2, step 6494/7134 completed (loss: 0.025980588048696518, acc: 0.9963099360466003)
[2025-02-13 04:27:56,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:57,293][root][INFO] - Training Epoch: 2/2, step 6495/7134 completed (loss: 0.036566440016031265, acc: 0.9859747290611267)
[2025-02-13 04:27:57,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:57,739][root][INFO] - Training Epoch: 2/2, step 6496/7134 completed (loss: 0.02809540182352066, acc: 0.99370276927948)
[2025-02-13 04:27:57,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:58,157][root][INFO] - Training Epoch: 2/2, step 6497/7134 completed (loss: 0.05236188322305679, acc: 0.9813374876976013)
[2025-02-13 04:27:58,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:58,587][root][INFO] - Training Epoch: 2/2, step 6498/7134 completed (loss: 0.01433639507740736, acc: 0.995121955871582)
[2025-02-13 04:27:58,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:59,016][root][INFO] - Training Epoch: 2/2, step 6499/7134 completed (loss: 0.023630859330296516, acc: 0.9900373816490173)
[2025-02-13 04:27:59,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:59,475][root][INFO] - Training Epoch: 2/2, step 6500/7134 completed (loss: 0.055817216634750366, acc: 0.9831932783126831)
[2025-02-13 04:27:59,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:59,904][root][INFO] - Training Epoch: 2/2, step 6501/7134 completed (loss: 0.08276265859603882, acc: 0.9769821166992188)
[2025-02-13 04:28:00,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:00,409][root][INFO] - Training Epoch: 2/2, step 6502/7134 completed (loss: 0.022743260487914085, acc: 0.9940617680549622)
[2025-02-13 04:28:00,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:00,815][root][INFO] - Training Epoch: 2/2, step 6503/7134 completed (loss: 0.03609517961740494, acc: 0.9882698059082031)
[2025-02-13 04:28:00,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:01,203][root][INFO] - Training Epoch: 2/2, step 6504/7134 completed (loss: 0.037225835025310516, acc: 0.9905213117599487)
[2025-02-13 04:28:01,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:01,634][root][INFO] - Training Epoch: 2/2, step 6505/7134 completed (loss: 0.058658499270677567, acc: 0.9826086759567261)
[2025-02-13 04:28:01,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:02,098][root][INFO] - Training Epoch: 2/2, step 6506/7134 completed (loss: 0.05186828225851059, acc: 0.9837133288383484)
[2025-02-13 04:28:02,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:02,540][root][INFO] - Training Epoch: 2/2, step 6507/7134 completed (loss: 0.056936804205179214, acc: 0.9848713874816895)
[2025-02-13 04:28:02,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:02,992][root][INFO] - Training Epoch: 2/2, step 6508/7134 completed (loss: 0.05454812943935394, acc: 0.9906103014945984)
[2025-02-13 04:28:03,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:03,436][root][INFO] - Training Epoch: 2/2, step 6509/7134 completed (loss: 0.0285366028547287, acc: 0.9900332093238831)
[2025-02-13 04:28:03,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:03,926][root][INFO] - Training Epoch: 2/2, step 6510/7134 completed (loss: 0.03751759231090546, acc: 0.9883720874786377)
[2025-02-13 04:28:04,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:04,355][root][INFO] - Training Epoch: 2/2, step 6511/7134 completed (loss: 0.04252662509679794, acc: 0.9879699349403381)
[2025-02-13 04:28:04,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:04,619][root][INFO] - Training Epoch: 2/2, step 6512/7134 completed (loss: 0.015877028927206993, acc: 0.9961977005004883)
[2025-02-13 04:28:04,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:05,032][root][INFO] - Training Epoch: 2/2, step 6513/7134 completed (loss: 0.036035940051078796, acc: 0.9891975522041321)
[2025-02-13 04:28:05,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:05,435][root][INFO] - Training Epoch: 2/2, step 6514/7134 completed (loss: 0.04402058944106102, acc: 0.9886363744735718)
[2025-02-13 04:28:05,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:05,897][root][INFO] - Training Epoch: 2/2, step 6515/7134 completed (loss: 0.032336048781871796, acc: 0.9885786771774292)
[2025-02-13 04:28:06,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:06,348][root][INFO] - Training Epoch: 2/2, step 6516/7134 completed (loss: 0.06370166689157486, acc: 0.9840849041938782)
[2025-02-13 04:28:06,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:06,785][root][INFO] - Training Epoch: 2/2, step 6517/7134 completed (loss: 0.022087406367063522, acc: 0.9947368502616882)
[2025-02-13 04:28:06,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:07,224][root][INFO] - Training Epoch: 2/2, step 6518/7134 completed (loss: 0.01753990724682808, acc: 0.9944827556610107)
[2025-02-13 04:28:07,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:07,640][root][INFO] - Training Epoch: 2/2, step 6519/7134 completed (loss: 0.025335902348160744, acc: 0.9929078221321106)
[2025-02-13 04:28:07,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:08,038][root][INFO] - Training Epoch: 2/2, step 6520/7134 completed (loss: 0.011531827040016651, acc: 0.9984447956085205)
[2025-02-13 04:28:08,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:08,444][root][INFO] - Training Epoch: 2/2, step 6521/7134 completed (loss: 0.014183237217366695, acc: 0.9947916865348816)
[2025-02-13 04:28:08,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:08,835][root][INFO] - Training Epoch: 2/2, step 6522/7134 completed (loss: 0.03462034836411476, acc: 0.9914236664772034)
[2025-02-13 04:28:08,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:09,224][root][INFO] - Training Epoch: 2/2, step 6523/7134 completed (loss: 0.06868179142475128, acc: 0.9818511605262756)
[2025-02-13 04:28:09,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:09,658][root][INFO] - Training Epoch: 2/2, step 6524/7134 completed (loss: 0.0625949427485466, acc: 0.9858611822128296)
[2025-02-13 04:28:09,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:10,073][root][INFO] - Training Epoch: 2/2, step 6525/7134 completed (loss: 0.011826340109109879, acc: 0.9939758777618408)
[2025-02-13 04:28:10,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:10,516][root][INFO] - Training Epoch: 2/2, step 6526/7134 completed (loss: 0.05851699039340019, acc: 0.9923175573348999)
[2025-02-13 04:28:10,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:10,929][root][INFO] - Training Epoch: 2/2, step 6527/7134 completed (loss: 0.03347023203969002, acc: 0.9955489635467529)
[2025-02-13 04:28:11,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:11,314][root][INFO] - Training Epoch: 2/2, step 6528/7134 completed (loss: 0.055118851363658905, acc: 0.9891696572303772)
[2025-02-13 04:28:11,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:11,699][root][INFO] - Training Epoch: 2/2, step 6529/7134 completed (loss: 0.08455932140350342, acc: 0.981203019618988)
[2025-02-13 04:28:11,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:12,120][root][INFO] - Training Epoch: 2/2, step 6530/7134 completed (loss: 0.02679951675236225, acc: 0.9914407730102539)
[2025-02-13 04:28:12,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:12,535][root][INFO] - Training Epoch: 2/2, step 6531/7134 completed (loss: 0.01154467836022377, acc: 0.9956647157669067)
[2025-02-13 04:28:12,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:12,971][root][INFO] - Training Epoch: 2/2, step 6532/7134 completed (loss: 0.05359729006886482, acc: 0.9887359142303467)
[2025-02-13 04:28:13,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:13,393][root][INFO] - Training Epoch: 2/2, step 6533/7134 completed (loss: 0.015416081063449383, acc: 0.9955223798751831)
[2025-02-13 04:28:13,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:13,789][root][INFO] - Training Epoch: 2/2, step 6534/7134 completed (loss: 0.05902579426765442, acc: 0.989276111125946)
[2025-02-13 04:28:13,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:14,180][root][INFO] - Training Epoch: 2/2, step 6535/7134 completed (loss: 0.014755581505596638, acc: 1.0)
[2025-02-13 04:28:14,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:14,580][root][INFO] - Training Epoch: 2/2, step 6536/7134 completed (loss: 0.026232954114675522, acc: 0.9900793433189392)
[2025-02-13 04:28:14,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:14,962][root][INFO] - Training Epoch: 2/2, step 6537/7134 completed (loss: 0.07311902195215225, acc: 0.971238911151886)
[2025-02-13 04:28:15,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:15,371][root][INFO] - Training Epoch: 2/2, step 6538/7134 completed (loss: 0.061359718441963196, acc: 0.980461835861206)
[2025-02-13 04:28:15,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:15,656][root][INFO] - Training Epoch: 2/2, step 6539/7134 completed (loss: 0.0064498321153223515, acc: 0.9969879388809204)
[2025-02-13 04:28:15,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:15,995][root][INFO] - Training Epoch: 2/2, step 6540/7134 completed (loss: 0.18794021010398865, acc: 0.9562841653823853)
[2025-02-13 04:28:16,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:16,453][root][INFO] - Training Epoch: 2/2, step 6541/7134 completed (loss: 0.033025894314050674, acc: 0.9877451062202454)
[2025-02-13 04:28:16,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:16,902][root][INFO] - Training Epoch: 2/2, step 6542/7134 completed (loss: 0.02885623276233673, acc: 0.991525411605835)
[2025-02-13 04:28:17,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:17,312][root][INFO] - Training Epoch: 2/2, step 6543/7134 completed (loss: 0.008282365277409554, acc: 0.99790358543396)
[2025-02-13 04:28:17,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:17,748][root][INFO] - Training Epoch: 2/2, step 6544/7134 completed (loss: 0.050966255366802216, acc: 0.9880715608596802)
[2025-02-13 04:28:17,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:18,152][root][INFO] - Training Epoch: 2/2, step 6545/7134 completed (loss: 0.013858424499630928, acc: 0.9962825179100037)
[2025-02-13 04:28:18,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:18,534][root][INFO] - Training Epoch: 2/2, step 6546/7134 completed (loss: 0.02848462015390396, acc: 0.990227997303009)
[2025-02-13 04:28:18,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:18,939][root][INFO] - Training Epoch: 2/2, step 6547/7134 completed (loss: 0.09524571150541306, acc: 0.9812606573104858)
[2025-02-13 04:28:19,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:19,275][root][INFO] - Training Epoch: 2/2, step 6548/7134 completed (loss: 0.011732777580618858, acc: 0.9960317611694336)
[2025-02-13 04:28:19,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:19,682][root][INFO] - Training Epoch: 2/2, step 6549/7134 completed (loss: 0.07796359062194824, acc: 0.9864077568054199)
[2025-02-13 04:28:19,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:20,085][root][INFO] - Training Epoch: 2/2, step 6550/7134 completed (loss: 0.0623125396668911, acc: 0.9800000190734863)
[2025-02-13 04:28:20,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:20,537][root][INFO] - Training Epoch: 2/2, step 6551/7134 completed (loss: 0.011774001643061638, acc: 0.9965831637382507)
[2025-02-13 04:28:20,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:20,998][root][INFO] - Training Epoch: 2/2, step 6552/7134 completed (loss: 0.0349406898021698, acc: 0.9920318722724915)
[2025-02-13 04:28:21,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:21,454][root][INFO] - Training Epoch: 2/2, step 6553/7134 completed (loss: 0.017882486805319786, acc: 0.9936386942863464)
[2025-02-13 04:28:21,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:21,914][root][INFO] - Training Epoch: 2/2, step 6554/7134 completed (loss: 0.02782673016190529, acc: 0.9956756830215454)
[2025-02-13 04:28:22,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:22,387][root][INFO] - Training Epoch: 2/2, step 6555/7134 completed (loss: 0.04078302159905434, acc: 0.9909456968307495)
[2025-02-13 04:28:22,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:22,818][root][INFO] - Training Epoch: 2/2, step 6556/7134 completed (loss: 0.034809064120054245, acc: 0.9905263185501099)
[2025-02-13 04:28:22,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:23,289][root][INFO] - Training Epoch: 2/2, step 6557/7134 completed (loss: 0.025874750688672066, acc: 0.9939516186714172)
[2025-02-13 04:28:23,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:23,768][root][INFO] - Training Epoch: 2/2, step 6558/7134 completed (loss: 0.03127564862370491, acc: 0.991304337978363)
[2025-02-13 04:28:23,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:24,202][root][INFO] - Training Epoch: 2/2, step 6559/7134 completed (loss: 0.022536035627126694, acc: 0.9972260594367981)
[2025-02-13 04:28:24,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:24,730][root][INFO] - Training Epoch: 2/2, step 6560/7134 completed (loss: 0.01844080351293087, acc: 0.9940546751022339)
[2025-02-13 04:28:24,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:25,159][root][INFO] - Training Epoch: 2/2, step 6561/7134 completed (loss: 0.015278195030987263, acc: 0.9929824471473694)
[2025-02-13 04:28:25,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:25,588][root][INFO] - Training Epoch: 2/2, step 6562/7134 completed (loss: 0.02965833619236946, acc: 0.9910314083099365)
[2025-02-13 04:28:25,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:26,045][root][INFO] - Training Epoch: 2/2, step 6563/7134 completed (loss: 0.03373101353645325, acc: 0.9883585572242737)
[2025-02-13 04:28:26,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:26,485][root][INFO] - Training Epoch: 2/2, step 6564/7134 completed (loss: 0.039882637560367584, acc: 0.9892037510871887)
[2025-02-13 04:28:26,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:26,949][root][INFO] - Training Epoch: 2/2, step 6565/7134 completed (loss: 0.033206161111593246, acc: 0.9894291758537292)
[2025-02-13 04:28:27,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:27,439][root][INFO] - Training Epoch: 2/2, step 6566/7134 completed (loss: 0.01794215850532055, acc: 0.9962013363838196)
[2025-02-13 04:28:27,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:27,922][root][INFO] - Training Epoch: 2/2, step 6567/7134 completed (loss: 0.012470557354390621, acc: 0.9959473013877869)
[2025-02-13 04:28:28,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:28,381][root][INFO] - Training Epoch: 2/2, step 6568/7134 completed (loss: 0.017140399664640427, acc: 0.9936948418617249)
[2025-02-13 04:28:28,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:28,821][root][INFO] - Training Epoch: 2/2, step 6569/7134 completed (loss: 0.010229011066257954, acc: 0.997802197933197)
[2025-02-13 04:28:28,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:29,266][root][INFO] - Training Epoch: 2/2, step 6570/7134 completed (loss: 0.012537708505988121, acc: 0.9976771473884583)
[2025-02-13 04:28:29,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:29,662][root][INFO] - Training Epoch: 2/2, step 6571/7134 completed (loss: 0.011071240529417992, acc: 0.9973822236061096)
[2025-02-13 04:28:29,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:30,098][root][INFO] - Training Epoch: 2/2, step 6572/7134 completed (loss: 0.0391206368803978, acc: 0.9900221824645996)
[2025-02-13 04:28:30,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:30,554][root][INFO] - Training Epoch: 2/2, step 6573/7134 completed (loss: 0.019934438169002533, acc: 0.9920993447303772)
[2025-02-13 04:28:30,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:31,020][root][INFO] - Training Epoch: 2/2, step 6574/7134 completed (loss: 0.0138443848118186, acc: 0.9948875308036804)
[2025-02-13 04:28:31,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:31,510][root][INFO] - Training Epoch: 2/2, step 6575/7134 completed (loss: 0.025484666228294373, acc: 0.9922651648521423)
[2025-02-13 04:28:31,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:31,958][root][INFO] - Training Epoch: 2/2, step 6576/7134 completed (loss: 0.010847450233995914, acc: 0.9968487620353699)
[2025-02-13 04:28:32,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:32,402][root][INFO] - Training Epoch: 2/2, step 6577/7134 completed (loss: 0.032140325754880905, acc: 0.9929971694946289)
[2025-02-13 04:28:32,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:33,035][root][INFO] - Training Epoch: 2/2, step 6578/7134 completed (loss: 0.014362109825015068, acc: 0.9973309636116028)
[2025-02-13 04:28:33,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:33,420][root][INFO] - Training Epoch: 2/2, step 6579/7134 completed (loss: 0.028806794434785843, acc: 0.9945553541183472)
[2025-02-13 04:28:33,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:33,873][root][INFO] - Training Epoch: 2/2, step 6580/7134 completed (loss: 0.010643327608704567, acc: 0.9941176176071167)
[2025-02-13 04:28:34,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:34,307][root][INFO] - Training Epoch: 2/2, step 6581/7134 completed (loss: 0.022860519587993622, acc: 0.9926199316978455)
[2025-02-13 04:28:34,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:34,704][root][INFO] - Training Epoch: 2/2, step 6582/7134 completed (loss: 0.03547241911292076, acc: 0.9884892106056213)
[2025-02-13 04:28:34,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:35,155][root][INFO] - Training Epoch: 2/2, step 6583/7134 completed (loss: 0.007513448130339384, acc: 0.9977116584777832)
[2025-02-13 04:28:35,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:35,603][root][INFO] - Training Epoch: 2/2, step 6584/7134 completed (loss: 0.03341669216752052, acc: 0.990208089351654)
[2025-02-13 04:28:35,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:36,037][root][INFO] - Training Epoch: 2/2, step 6585/7134 completed (loss: 0.020783182233572006, acc: 0.9901315569877625)
[2025-02-13 04:28:36,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:36,467][root][INFO] - Training Epoch: 2/2, step 6586/7134 completed (loss: 0.01801997795701027, acc: 0.9935232996940613)
[2025-02-13 04:28:36,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:36,913][root][INFO] - Training Epoch: 2/2, step 6587/7134 completed (loss: 0.06374676525592804, acc: 0.9846698045730591)
[2025-02-13 04:28:37,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:37,323][root][INFO] - Training Epoch: 2/2, step 6588/7134 completed (loss: 0.03756791725754738, acc: 0.9911949634552002)
[2025-02-13 04:28:37,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:37,753][root][INFO] - Training Epoch: 2/2, step 6589/7134 completed (loss: 0.034467484802007675, acc: 0.9917012453079224)
[2025-02-13 04:28:37,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:38,210][root][INFO] - Training Epoch: 2/2, step 6590/7134 completed (loss: 0.031045518815517426, acc: 0.9930151104927063)
[2025-02-13 04:28:38,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:38,668][root][INFO] - Training Epoch: 2/2, step 6591/7134 completed (loss: 0.04275622218847275, acc: 0.9876084327697754)
[2025-02-13 04:28:38,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:39,120][root][INFO] - Training Epoch: 2/2, step 6592/7134 completed (loss: 0.05107789859175682, acc: 0.9852440357208252)
[2025-02-13 04:28:39,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:39,532][root][INFO] - Training Epoch: 2/2, step 6593/7134 completed (loss: 0.03496132418513298, acc: 0.9893454909324646)
[2025-02-13 04:28:39,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:39,973][root][INFO] - Training Epoch: 2/2, step 6594/7134 completed (loss: 0.02734588272869587, acc: 0.9917840361595154)
[2025-02-13 04:28:40,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:40,402][root][INFO] - Training Epoch: 2/2, step 6595/7134 completed (loss: 0.052975431084632874, acc: 0.9881305694580078)
[2025-02-13 04:28:40,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:40,797][root][INFO] - Training Epoch: 2/2, step 6596/7134 completed (loss: 0.021440697833895683, acc: 0.9939302206039429)
[2025-02-13 04:28:40,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:41,236][root][INFO] - Training Epoch: 2/2, step 6597/7134 completed (loss: 0.023147232830524445, acc: 0.9917452931404114)
[2025-02-13 04:28:41,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:41,669][root][INFO] - Training Epoch: 2/2, step 6598/7134 completed (loss: 0.024205144494771957, acc: 0.9879194498062134)
[2025-02-13 04:28:41,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:42,128][root][INFO] - Training Epoch: 2/2, step 6599/7134 completed (loss: 0.031255729496479034, acc: 0.987922728061676)
[2025-02-13 04:28:42,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:42,534][root][INFO] - Training Epoch: 2/2, step 6600/7134 completed (loss: 0.038335200399160385, acc: 0.9889065027236938)
[2025-02-13 04:28:42,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:42,990][root][INFO] - Training Epoch: 2/2, step 6601/7134 completed (loss: 0.04397307336330414, acc: 0.9894366264343262)
[2025-02-13 04:28:43,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:43,434][root][INFO] - Training Epoch: 2/2, step 6602/7134 completed (loss: 0.016334977000951767, acc: 0.9963898658752441)
[2025-02-13 04:28:43,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:43,879][root][INFO] - Training Epoch: 2/2, step 6603/7134 completed (loss: 0.025525862351059914, acc: 0.9901823401451111)
[2025-02-13 04:28:43,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:44,249][root][INFO] - Training Epoch: 2/2, step 6604/7134 completed (loss: 0.028242431581020355, acc: 0.9971264600753784)
[2025-02-13 04:28:44,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:44,657][root][INFO] - Training Epoch: 2/2, step 6605/7134 completed (loss: 0.08112717419862747, acc: 0.9737206101417542)
[2025-02-13 04:28:44,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:45,104][root][INFO] - Training Epoch: 2/2, step 6606/7134 completed (loss: 0.02990058995783329, acc: 0.9924242496490479)
[2025-02-13 04:28:45,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:45,556][root][INFO] - Training Epoch: 2/2, step 6607/7134 completed (loss: 0.07392540574073792, acc: 0.9853723645210266)
[2025-02-13 04:28:45,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:46,004][root][INFO] - Training Epoch: 2/2, step 6608/7134 completed (loss: 0.023899106308817863, acc: 0.9946879148483276)
[2025-02-13 04:28:46,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:46,437][root][INFO] - Training Epoch: 2/2, step 6609/7134 completed (loss: 0.03450637310743332, acc: 0.9853300452232361)
[2025-02-13 04:28:46,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:46,848][root][INFO] - Training Epoch: 2/2, step 6610/7134 completed (loss: 0.06552338600158691, acc: 0.9811320900917053)
[2025-02-13 04:28:46,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:47,242][root][INFO] - Training Epoch: 2/2, step 6611/7134 completed (loss: 0.08193734288215637, acc: 0.9723076820373535)
[2025-02-13 04:28:47,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:47,708][root][INFO] - Training Epoch: 2/2, step 6612/7134 completed (loss: 0.04683004319667816, acc: 0.9836639165878296)
[2025-02-13 04:28:47,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:48,180][root][INFO] - Training Epoch: 2/2, step 6613/7134 completed (loss: 0.03443056344985962, acc: 0.988095223903656)
[2025-02-13 04:28:48,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:48,613][root][INFO] - Training Epoch: 2/2, step 6614/7134 completed (loss: 0.05061502382159233, acc: 0.9900990128517151)
[2025-02-13 04:28:48,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:49,042][root][INFO] - Training Epoch: 2/2, step 6615/7134 completed (loss: 0.019575342535972595, acc: 0.9943422675132751)
[2025-02-13 04:28:49,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:49,496][root][INFO] - Training Epoch: 2/2, step 6616/7134 completed (loss: 0.03297927975654602, acc: 0.9941314458847046)
[2025-02-13 04:28:49,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:49,891][root][INFO] - Training Epoch: 2/2, step 6617/7134 completed (loss: 0.026081979274749756, acc: 0.9942362904548645)
[2025-02-13 04:28:50,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:50,304][root][INFO] - Training Epoch: 2/2, step 6618/7134 completed (loss: 0.022167587652802467, acc: 0.9940564632415771)
[2025-02-13 04:28:50,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:50,766][root][INFO] - Training Epoch: 2/2, step 6619/7134 completed (loss: 0.03231289982795715, acc: 0.9899103045463562)
[2025-02-13 04:28:50,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:51,219][root][INFO] - Training Epoch: 2/2, step 6620/7134 completed (loss: 0.02170313335955143, acc: 0.9917469024658203)
[2025-02-13 04:28:51,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:51,653][root][INFO] - Training Epoch: 2/2, step 6621/7134 completed (loss: 0.03534504771232605, acc: 0.9872832298278809)
[2025-02-13 04:28:51,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:52,085][root][INFO] - Training Epoch: 2/2, step 6622/7134 completed (loss: 0.04071749001741409, acc: 0.9904371500015259)
[2025-02-13 04:28:52,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:52,520][root][INFO] - Training Epoch: 2/2, step 6623/7134 completed (loss: 0.017671067267656326, acc: 0.9949685335159302)
[2025-02-13 04:28:52,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:52,947][root][INFO] - Training Epoch: 2/2, step 6624/7134 completed (loss: 0.01701251044869423, acc: 0.99609375)
[2025-02-13 04:28:53,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:53,371][root][INFO] - Training Epoch: 2/2, step 6625/7134 completed (loss: 0.023438092321157455, acc: 0.9944979548454285)
[2025-02-13 04:28:53,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:53,822][root][INFO] - Training Epoch: 2/2, step 6626/7134 completed (loss: 0.012812543660402298, acc: 0.996363639831543)
[2025-02-13 04:28:53,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:54,287][root][INFO] - Training Epoch: 2/2, step 6627/7134 completed (loss: 0.027066783979535103, acc: 0.9913513660430908)
[2025-02-13 04:28:54,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:54,742][root][INFO] - Training Epoch: 2/2, step 6628/7134 completed (loss: 0.04429425299167633, acc: 0.9881656765937805)
[2025-02-13 04:28:54,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:55,130][root][INFO] - Training Epoch: 2/2, step 6629/7134 completed (loss: 0.0503263957798481, acc: 0.9884467124938965)
[2025-02-13 04:28:55,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:55,624][root][INFO] - Training Epoch: 2/2, step 6630/7134 completed (loss: 0.030383072793483734, acc: 0.9899665713310242)
[2025-02-13 04:28:55,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:56,059][root][INFO] - Training Epoch: 2/2, step 6631/7134 completed (loss: 0.02770802564918995, acc: 0.9908257126808167)
[2025-02-13 04:28:56,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:56,550][root][INFO] - Training Epoch: 2/2, step 6632/7134 completed (loss: 0.023419780656695366, acc: 0.9943757057189941)
[2025-02-13 04:28:56,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:56,977][root][INFO] - Training Epoch: 2/2, step 6633/7134 completed (loss: 0.030132515355944633, acc: 0.9939024448394775)
[2025-02-13 04:28:57,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:57,388][root][INFO] - Training Epoch: 2/2, step 6634/7134 completed (loss: 0.02537899650633335, acc: 0.991055428981781)
[2025-02-13 04:28:57,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:57,774][root][INFO] - Training Epoch: 2/2, step 6635/7134 completed (loss: 0.07498563081026077, acc: 0.9820972084999084)
[2025-02-13 04:28:57,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:58,228][root][INFO] - Training Epoch: 2/2, step 6636/7134 completed (loss: 0.03152035176753998, acc: 0.9885844588279724)
[2025-02-13 04:28:58,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:58,656][root][INFO] - Training Epoch: 2/2, step 6637/7134 completed (loss: 0.020291198045015335, acc: 0.9927219748497009)
[2025-02-13 04:28:58,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:59,088][root][INFO] - Training Epoch: 2/2, step 6638/7134 completed (loss: 0.016425589099526405, acc: 0.9949173927307129)
[2025-02-13 04:28:59,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:59,533][root][INFO] - Training Epoch: 2/2, step 6639/7134 completed (loss: 0.012901187874376774, acc: 0.9985097050666809)
[2025-02-13 04:28:59,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:59,980][root][INFO] - Training Epoch: 2/2, step 6640/7134 completed (loss: 0.016548078507184982, acc: 0.9938744306564331)
[2025-02-13 04:29:00,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:00,428][root][INFO] - Training Epoch: 2/2, step 6641/7134 completed (loss: 0.05901346355676651, acc: 0.9847596883773804)
[2025-02-13 04:29:00,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:00,866][root][INFO] - Training Epoch: 2/2, step 6642/7134 completed (loss: 0.03164132684469223, acc: 0.990963876247406)
[2025-02-13 04:29:01,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:01,329][root][INFO] - Training Epoch: 2/2, step 6643/7134 completed (loss: 0.017411988228559494, acc: 0.9960370063781738)
[2025-02-13 04:29:01,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:01,785][root][INFO] - Training Epoch: 2/2, step 6644/7134 completed (loss: 0.045704830437898636, acc: 0.9888097643852234)
[2025-02-13 04:29:01,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:02,216][root][INFO] - Training Epoch: 2/2, step 6645/7134 completed (loss: 0.019833985716104507, acc: 0.9938575029373169)
[2025-02-13 04:29:02,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:02,693][root][INFO] - Training Epoch: 2/2, step 6646/7134 completed (loss: 0.02382063679397106, acc: 0.9933920502662659)
[2025-02-13 04:29:02,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:03,125][root][INFO] - Training Epoch: 2/2, step 6647/7134 completed (loss: 0.013034912757575512, acc: 0.9945651888847351)
[2025-02-13 04:29:03,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:03,572][root][INFO] - Training Epoch: 2/2, step 6648/7134 completed (loss: 0.05239607021212578, acc: 0.9807999730110168)
[2025-02-13 04:29:03,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:04,039][root][INFO] - Training Epoch: 2/2, step 6649/7134 completed (loss: 0.03865731507539749, acc: 0.990554928779602)
[2025-02-13 04:29:04,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:04,525][root][INFO] - Training Epoch: 2/2, step 6650/7134 completed (loss: 0.007897092029452324, acc: 0.9966517686843872)
[2025-02-13 04:29:04,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:05,008][root][INFO] - Training Epoch: 2/2, step 6651/7134 completed (loss: 0.022418668493628502, acc: 0.9931787252426147)
[2025-02-13 04:29:05,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:05,456][root][INFO] - Training Epoch: 2/2, step 6652/7134 completed (loss: 0.028247805312275887, acc: 0.9915459156036377)
[2025-02-13 04:29:05,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:05,916][root][INFO] - Training Epoch: 2/2, step 6653/7134 completed (loss: 0.03519934043288231, acc: 0.9868565201759338)
[2025-02-13 04:29:06,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:06,343][root][INFO] - Training Epoch: 2/2, step 6654/7134 completed (loss: 0.05600367859005928, acc: 0.9844961166381836)
[2025-02-13 04:29:06,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:06,763][root][INFO] - Training Epoch: 2/2, step 6655/7134 completed (loss: 0.034930624067783356, acc: 0.9900744557380676)
[2025-02-13 04:29:06,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:07,179][root][INFO] - Training Epoch: 2/2, step 6656/7134 completed (loss: 0.044059351086616516, acc: 0.9860050678253174)
[2025-02-13 04:29:07,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:07,613][root][INFO] - Training Epoch: 2/2, step 6657/7134 completed (loss: 0.02496122010052204, acc: 0.9932249188423157)
[2025-02-13 04:29:07,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:08,070][root][INFO] - Training Epoch: 2/2, step 6658/7134 completed (loss: 0.007815925404429436, acc: 1.0)
[2025-02-13 04:29:08,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:08,546][root][INFO] - Training Epoch: 2/2, step 6659/7134 completed (loss: 0.021589480340480804, acc: 0.99609375)
[2025-02-13 04:29:08,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:08,998][root][INFO] - Training Epoch: 2/2, step 6660/7134 completed (loss: 0.027319174259901047, acc: 0.995039701461792)
[2025-02-13 04:29:09,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:09,481][root][INFO] - Training Epoch: 2/2, step 6661/7134 completed (loss: 0.020056281238794327, acc: 0.9938119053840637)
[2025-02-13 04:29:09,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:09,895][root][INFO] - Training Epoch: 2/2, step 6662/7134 completed (loss: 0.04493359476327896, acc: 0.9806867241859436)
[2025-02-13 04:29:10,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:10,342][root][INFO] - Training Epoch: 2/2, step 6663/7134 completed (loss: 0.027361076325178146, acc: 0.9923497438430786)
[2025-02-13 04:29:10,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:10,738][root][INFO] - Training Epoch: 2/2, step 6664/7134 completed (loss: 0.030327437445521355, acc: 0.9870848655700684)
[2025-02-13 04:29:10,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:11,132][root][INFO] - Training Epoch: 2/2, step 6665/7134 completed (loss: 0.06206227466464043, acc: 0.9707792401313782)
[2025-02-13 04:29:11,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:11,577][root][INFO] - Training Epoch: 2/2, step 6666/7134 completed (loss: 0.020913081243634224, acc: 0.9936548471450806)
[2025-02-13 04:29:11,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:11,991][root][INFO] - Training Epoch: 2/2, step 6667/7134 completed (loss: 0.03943789377808571, acc: 0.9858657121658325)
[2025-02-13 04:29:12,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:12,379][root][INFO] - Training Epoch: 2/2, step 6668/7134 completed (loss: 0.008987289853394032, acc: 0.9965338110923767)
[2025-02-13 04:29:12,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:12,768][root][INFO] - Training Epoch: 2/2, step 6669/7134 completed (loss: 0.022085580974817276, acc: 0.9916107654571533)
[2025-02-13 04:29:12,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:13,150][root][INFO] - Training Epoch: 2/2, step 6670/7134 completed (loss: 0.023278748616576195, acc: 0.9949832558631897)
[2025-02-13 04:29:13,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:13,572][root][INFO] - Training Epoch: 2/2, step 6671/7134 completed (loss: 0.012241472490131855, acc: 0.9941349029541016)
[2025-02-13 04:29:13,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:13,986][root][INFO] - Training Epoch: 2/2, step 6672/7134 completed (loss: 0.015349588356912136, acc: 0.9943714737892151)
[2025-02-13 04:29:14,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:14,412][root][INFO] - Training Epoch: 2/2, step 6673/7134 completed (loss: 0.018745312467217445, acc: 0.9926578402519226)
[2025-02-13 04:29:14,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:14,808][root][INFO] - Training Epoch: 2/2, step 6674/7134 completed (loss: 0.007410528603941202, acc: 1.0)
[2025-02-13 04:29:14,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:15,210][root][INFO] - Training Epoch: 2/2, step 6675/7134 completed (loss: 0.014069506898522377, acc: 0.9950617551803589)
[2025-02-13 04:29:15,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:15,621][root][INFO] - Training Epoch: 2/2, step 6676/7134 completed (loss: 0.01950087398290634, acc: 0.9928401112556458)
[2025-02-13 04:29:15,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:15,997][root][INFO] - Training Epoch: 2/2, step 6677/7134 completed (loss: 0.04701978340744972, acc: 0.9885495901107788)
[2025-02-13 04:29:16,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:16,402][root][INFO] - Training Epoch: 2/2, step 6678/7134 completed (loss: 0.01367136463522911, acc: 0.9946428537368774)
[2025-02-13 04:29:16,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:16,764][root][INFO] - Training Epoch: 2/2, step 6679/7134 completed (loss: 0.00913360994309187, acc: 0.9956331849098206)
[2025-02-13 04:29:16,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:17,120][root][INFO] - Training Epoch: 2/2, step 6680/7134 completed (loss: 0.014436233788728714, acc: 0.9972972869873047)
[2025-02-13 04:29:17,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:17,530][root][INFO] - Training Epoch: 2/2, step 6681/7134 completed (loss: 0.03171224892139435, acc: 0.9937629699707031)
[2025-02-13 04:29:17,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:17,928][root][INFO] - Training Epoch: 2/2, step 6682/7134 completed (loss: 0.04335900396108627, acc: 0.9953271150588989)
[2025-02-13 04:29:18,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:18,283][root][INFO] - Training Epoch: 2/2, step 6683/7134 completed (loss: 0.01658029481768608, acc: 0.9945054650306702)
[2025-02-13 04:29:18,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:18,684][root][INFO] - Training Epoch: 2/2, step 6684/7134 completed (loss: 0.03323536738753319, acc: 0.9928571581840515)
[2025-02-13 04:29:18,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:19,079][root][INFO] - Training Epoch: 2/2, step 6685/7134 completed (loss: 0.016149381175637245, acc: 0.9923664331436157)
[2025-02-13 04:29:19,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:19,488][root][INFO] - Training Epoch: 2/2, step 6686/7134 completed (loss: 0.04889730364084244, acc: 0.9917355179786682)
[2025-02-13 04:29:19,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:19,847][root][INFO] - Training Epoch: 2/2, step 6687/7134 completed (loss: 0.051457781344652176, acc: 0.9814814925193787)
[2025-02-13 04:29:19,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:20,297][root][INFO] - Training Epoch: 2/2, step 6688/7134 completed (loss: 0.034940704703330994, acc: 0.9828178882598877)
[2025-02-13 04:29:20,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:20,711][root][INFO] - Training Epoch: 2/2, step 6689/7134 completed (loss: 0.03908253461122513, acc: 0.9900990128517151)
[2025-02-13 04:29:20,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:21,091][root][INFO] - Training Epoch: 2/2, step 6690/7134 completed (loss: 0.07264531403779984, acc: 0.9733656048774719)
[2025-02-13 04:29:21,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:21,504][root][INFO] - Training Epoch: 2/2, step 6691/7134 completed (loss: 0.01352525595575571, acc: 0.9967266917228699)
[2025-02-13 04:29:21,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:21,851][root][INFO] - Training Epoch: 2/2, step 6692/7134 completed (loss: 0.025882603600621223, acc: 0.9936440587043762)
[2025-02-13 04:29:21,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:22,235][root][INFO] - Training Epoch: 2/2, step 6693/7134 completed (loss: 0.01791144162416458, acc: 0.9950494766235352)
[2025-02-13 04:29:22,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:22,654][root][INFO] - Training Epoch: 2/2, step 6694/7134 completed (loss: 0.018894001841545105, acc: 0.9938837885856628)
[2025-02-13 04:29:22,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:23,070][root][INFO] - Training Epoch: 2/2, step 6695/7134 completed (loss: 0.03910934925079346, acc: 0.9849520921707153)
[2025-02-13 04:29:23,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:23,489][root][INFO] - Training Epoch: 2/2, step 6696/7134 completed (loss: 0.05004866421222687, acc: 0.9860681295394897)
[2025-02-13 04:29:23,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:23,893][root][INFO] - Training Epoch: 2/2, step 6697/7134 completed (loss: 0.045372121036052704, acc: 0.9885246157646179)
[2025-02-13 04:29:24,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:24,296][root][INFO] - Training Epoch: 2/2, step 6698/7134 completed (loss: 0.014932215213775635, acc: 0.9940652847290039)
[2025-02-13 04:29:24,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:24,709][root][INFO] - Training Epoch: 2/2, step 6699/7134 completed (loss: 0.027199864387512207, acc: 0.9908883571624756)
[2025-02-13 04:29:24,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:25,126][root][INFO] - Training Epoch: 2/2, step 6700/7134 completed (loss: 0.04573812335729599, acc: 0.9834558963775635)
[2025-02-13 04:29:25,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:25,514][root][INFO] - Training Epoch: 2/2, step 6701/7134 completed (loss: 0.09124399721622467, acc: 0.9722222089767456)
[2025-02-13 04:29:25,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:25,899][root][INFO] - Training Epoch: 2/2, step 6702/7134 completed (loss: 0.023974889889359474, acc: 0.9882121682167053)
[2025-02-13 04:29:26,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:26,299][root][INFO] - Training Epoch: 2/2, step 6703/7134 completed (loss: 0.06942737102508545, acc: 0.9819079041481018)
[2025-02-13 04:29:26,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:26,722][root][INFO] - Training Epoch: 2/2, step 6704/7134 completed (loss: 0.08300061523914337, acc: 0.9754204154014587)
[2025-02-13 04:29:26,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:27,143][root][INFO] - Training Epoch: 2/2, step 6705/7134 completed (loss: 0.03765389695763588, acc: 0.9869961142539978)
[2025-02-13 04:29:27,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:27,495][root][INFO] - Training Epoch: 2/2, step 6706/7134 completed (loss: 0.02745695412158966, acc: 0.9917355179786682)
[2025-02-13 04:29:27,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:27,899][root][INFO] - Training Epoch: 2/2, step 6707/7134 completed (loss: 0.040415938943624496, acc: 0.9881656765937805)
[2025-02-13 04:29:28,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:28,333][root][INFO] - Training Epoch: 2/2, step 6708/7134 completed (loss: 0.03408612683415413, acc: 0.9900990128517151)
[2025-02-13 04:29:28,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:28,750][root][INFO] - Training Epoch: 2/2, step 6709/7134 completed (loss: 0.02008228935301304, acc: 0.9940119981765747)
[2025-02-13 04:29:28,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:29,158][root][INFO] - Training Epoch: 2/2, step 6710/7134 completed (loss: 0.06844163686037064, acc: 0.9853747487068176)
[2025-02-13 04:29:29,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:29,602][root][INFO] - Training Epoch: 2/2, step 6711/7134 completed (loss: 0.02154085971415043, acc: 0.9920634627342224)
[2025-02-13 04:29:29,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:30,003][root][INFO] - Training Epoch: 2/2, step 6712/7134 completed (loss: 0.0408068485558033, acc: 0.9848713874816895)
[2025-02-13 04:29:30,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:30,517][root][INFO] - Training Epoch: 2/2, step 6713/7134 completed (loss: 0.051307354122400284, acc: 0.9855282306671143)
[2025-02-13 04:29:30,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:30,940][root][INFO] - Training Epoch: 2/2, step 6714/7134 completed (loss: 0.02792769856750965, acc: 0.9881129264831543)
[2025-02-13 04:29:31,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:31,346][root][INFO] - Training Epoch: 2/2, step 6715/7134 completed (loss: 0.06155921891331673, acc: 0.9795918464660645)
[2025-02-13 04:29:31,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:31,765][root][INFO] - Training Epoch: 2/2, step 6716/7134 completed (loss: 0.01377277635037899, acc: 0.992514967918396)
[2025-02-13 04:29:31,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:32,170][root][INFO] - Training Epoch: 2/2, step 6717/7134 completed (loss: 0.022284455597400665, acc: 0.990338146686554)
[2025-02-13 04:29:32,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:32,577][root][INFO] - Training Epoch: 2/2, step 6718/7134 completed (loss: 0.05643164739012718, acc: 0.9833333492279053)
[2025-02-13 04:29:32,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:32,981][root][INFO] - Training Epoch: 2/2, step 6719/7134 completed (loss: 0.02438933774828911, acc: 0.9885621070861816)
[2025-02-13 04:29:33,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:33,397][root][INFO] - Training Epoch: 2/2, step 6720/7134 completed (loss: 0.054165713489055634, acc: 0.9874301552772522)
[2025-02-13 04:29:33,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:33,777][root][INFO] - Training Epoch: 2/2, step 6721/7134 completed (loss: 0.046469613909721375, acc: 0.9875195026397705)
[2025-02-13 04:29:33,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:34,108][root][INFO] - Training Epoch: 2/2, step 6722/7134 completed (loss: 0.05956199765205383, acc: 0.9912126660346985)
[2025-02-13 04:29:34,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:34,526][root][INFO] - Training Epoch: 2/2, step 6723/7134 completed (loss: 0.022801782935857773, acc: 0.9916666746139526)
[2025-02-13 04:29:34,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:34,930][root][INFO] - Training Epoch: 2/2, step 6724/7134 completed (loss: 0.027660377323627472, acc: 0.9941434860229492)
[2025-02-13 04:29:35,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:35,300][root][INFO] - Training Epoch: 2/2, step 6725/7134 completed (loss: 0.023642322048544884, acc: 0.9897611141204834)
[2025-02-13 04:29:35,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:35,679][root][INFO] - Training Epoch: 2/2, step 6726/7134 completed (loss: 0.05200796574354172, acc: 0.9919678568840027)
[2025-02-13 04:29:35,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:36,074][root][INFO] - Training Epoch: 2/2, step 6727/7134 completed (loss: 0.05421062186360359, acc: 0.9841549396514893)
[2025-02-13 04:29:36,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:36,502][root][INFO] - Training Epoch: 2/2, step 6728/7134 completed (loss: 0.04182079806923866, acc: 0.9923195242881775)
[2025-02-13 04:29:36,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:36,946][root][INFO] - Training Epoch: 2/2, step 6729/7134 completed (loss: 0.014510232023894787, acc: 0.9972826242446899)
[2025-02-13 04:29:37,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:37,388][root][INFO] - Training Epoch: 2/2, step 6730/7134 completed (loss: 0.0286676287651062, acc: 0.9905808568000793)
[2025-02-13 04:29:37,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:37,823][root][INFO] - Training Epoch: 2/2, step 6731/7134 completed (loss: 0.020230300724506378, acc: 0.993966817855835)
[2025-02-13 04:29:37,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:38,222][root][INFO] - Training Epoch: 2/2, step 6732/7134 completed (loss: 0.02997272089123726, acc: 0.9889937043190002)
[2025-02-13 04:29:38,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:38,613][root][INFO] - Training Epoch: 2/2, step 6733/7134 completed (loss: 0.03553337603807449, acc: 0.9886147975921631)
[2025-02-13 04:29:38,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:38,997][root][INFO] - Training Epoch: 2/2, step 6734/7134 completed (loss: 0.046602699905633926, acc: 0.9906542301177979)
[2025-02-13 04:29:39,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:39,393][root][INFO] - Training Epoch: 2/2, step 6735/7134 completed (loss: 0.04397367686033249, acc: 0.9907833933830261)
[2025-02-13 04:29:39,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:39,848][root][INFO] - Training Epoch: 2/2, step 6736/7134 completed (loss: 0.035037439316511154, acc: 0.9930362105369568)
[2025-02-13 04:29:40,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:40,282][root][INFO] - Training Epoch: 2/2, step 6737/7134 completed (loss: 0.03983161970973015, acc: 0.987034022808075)
[2025-02-13 04:29:40,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:40,683][root][INFO] - Training Epoch: 2/2, step 6738/7134 completed (loss: 0.025247320532798767, acc: 0.996268630027771)
[2025-02-13 04:29:40,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:41,010][root][INFO] - Training Epoch: 2/2, step 6739/7134 completed (loss: 0.028407562524080276, acc: 0.9908088445663452)
[2025-02-13 04:29:41,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:41,425][root][INFO] - Training Epoch: 2/2, step 6740/7134 completed (loss: 0.04590178653597832, acc: 0.9881154298782349)
[2025-02-13 04:29:41,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:41,832][root][INFO] - Training Epoch: 2/2, step 6741/7134 completed (loss: 0.024416295811533928, acc: 0.9898403286933899)
[2025-02-13 04:29:41,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:42,211][root][INFO] - Training Epoch: 2/2, step 6742/7134 completed (loss: 0.02036985568702221, acc: 0.9913420081138611)
[2025-02-13 04:29:42,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:42,666][root][INFO] - Training Epoch: 2/2, step 6743/7134 completed (loss: 0.014772425405681133, acc: 0.9959127902984619)
[2025-02-13 04:29:42,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:43,073][root][INFO] - Training Epoch: 2/2, step 6744/7134 completed (loss: 0.018526926636695862, acc: 0.9967948794364929)
[2025-02-13 04:29:43,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:43,476][root][INFO] - Training Epoch: 2/2, step 6745/7134 completed (loss: 0.009779082611203194, acc: 0.9986245036125183)
[2025-02-13 04:29:43,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:43,866][root][INFO] - Training Epoch: 2/2, step 6746/7134 completed (loss: 0.010069634765386581, acc: 0.998236358165741)
[2025-02-13 04:29:43,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:44,277][root][INFO] - Training Epoch: 2/2, step 6747/7134 completed (loss: 0.0229205209761858, acc: 0.9937888383865356)
[2025-02-13 04:29:44,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:44,754][root][INFO] - Training Epoch: 2/2, step 6748/7134 completed (loss: 0.021561389788985252, acc: 0.9977452158927917)
[2025-02-13 04:29:44,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:45,225][root][INFO] - Training Epoch: 2/2, step 6749/7134 completed (loss: 0.020832588896155357, acc: 0.9934640526771545)
[2025-02-13 04:29:45,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:45,682][root][INFO] - Training Epoch: 2/2, step 6750/7134 completed (loss: 0.01300605945289135, acc: 0.9988465905189514)
[2025-02-13 04:29:45,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:46,146][root][INFO] - Training Epoch: 2/2, step 6751/7134 completed (loss: 0.01175433024764061, acc: 0.9965753555297852)
[2025-02-13 04:29:46,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:46,563][root][INFO] - Training Epoch: 2/2, step 6752/7134 completed (loss: 0.01324098277837038, acc: 0.9970972537994385)
[2025-02-13 04:29:46,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:47,016][root][INFO] - Training Epoch: 2/2, step 6753/7134 completed (loss: 0.014771933667361736, acc: 0.997706413269043)
[2025-02-13 04:29:47,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:47,438][root][INFO] - Training Epoch: 2/2, step 6754/7134 completed (loss: 0.003528211498633027, acc: 1.0)
[2025-02-13 04:29:47,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:47,880][root][INFO] - Training Epoch: 2/2, step 6755/7134 completed (loss: 0.012926317751407623, acc: 0.9975369572639465)
[2025-02-13 04:29:48,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:48,311][root][INFO] - Training Epoch: 2/2, step 6756/7134 completed (loss: 0.005577158182859421, acc: 1.0)
[2025-02-13 04:29:48,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:48,716][root][INFO] - Training Epoch: 2/2, step 6757/7134 completed (loss: 0.008072678931057453, acc: 0.9954268336296082)
[2025-02-13 04:29:48,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:49,144][root][INFO] - Training Epoch: 2/2, step 6758/7134 completed (loss: 0.01302344724535942, acc: 0.9961190223693848)
[2025-02-13 04:29:49,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:49,611][root][INFO] - Training Epoch: 2/2, step 6759/7134 completed (loss: 0.02440538816154003, acc: 0.9932885766029358)
[2025-02-13 04:29:49,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:50,051][root][INFO] - Training Epoch: 2/2, step 6760/7134 completed (loss: 0.020035291090607643, acc: 0.9934383034706116)
[2025-02-13 04:29:50,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:50,481][root][INFO] - Training Epoch: 2/2, step 6761/7134 completed (loss: 0.021580472588539124, acc: 0.9903614521026611)
[2025-02-13 04:29:50,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:50,923][root][INFO] - Training Epoch: 2/2, step 6762/7134 completed (loss: 0.044812362641096115, acc: 0.9877451062202454)
[2025-02-13 04:29:51,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:51,373][root][INFO] - Training Epoch: 2/2, step 6763/7134 completed (loss: 0.017037294805049896, acc: 0.9962359070777893)
[2025-02-13 04:29:51,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:51,778][root][INFO] - Training Epoch: 2/2, step 6764/7134 completed (loss: 0.024661585688591003, acc: 0.9928160905838013)
[2025-02-13 04:29:51,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:52,257][root][INFO] - Training Epoch: 2/2, step 6765/7134 completed (loss: 0.015164395794272423, acc: 0.9973368644714355)
[2025-02-13 04:29:52,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:52,661][root][INFO] - Training Epoch: 2/2, step 6766/7134 completed (loss: 0.004069410730153322, acc: 1.0)
[2025-02-13 04:29:52,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:52,984][root][INFO] - Training Epoch: 2/2, step 6767/7134 completed (loss: 0.016233688220381737, acc: 0.9977477192878723)
[2025-02-13 04:29:53,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:53,418][root][INFO] - Training Epoch: 2/2, step 6768/7134 completed (loss: 0.005849374923855066, acc: 1.0)
[2025-02-13 04:29:53,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:53,822][root][INFO] - Training Epoch: 2/2, step 6769/7134 completed (loss: 0.019807064905762672, acc: 0.9922839403152466)
[2025-02-13 04:29:53,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:54,252][root][INFO] - Training Epoch: 2/2, step 6770/7134 completed (loss: 0.027548890560865402, acc: 0.9879194498062134)
[2025-02-13 04:29:54,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:54,686][root][INFO] - Training Epoch: 2/2, step 6771/7134 completed (loss: 0.020671583712100983, acc: 0.9952681660652161)
[2025-02-13 04:29:54,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:55,113][root][INFO] - Training Epoch: 2/2, step 6772/7134 completed (loss: 0.07191634923219681, acc: 0.9857397675514221)
[2025-02-13 04:29:55,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:55,512][root][INFO] - Training Epoch: 2/2, step 6773/7134 completed (loss: 0.03203408047556877, acc: 0.9889240264892578)
[2025-02-13 04:29:55,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:55,933][root][INFO] - Training Epoch: 2/2, step 6774/7134 completed (loss: 0.03099077381193638, acc: 0.993630588054657)
[2025-02-13 04:29:56,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:56,335][root][INFO] - Training Epoch: 2/2, step 6775/7134 completed (loss: 0.009204905480146408, acc: 0.9981096386909485)
[2025-02-13 04:29:56,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:56,745][root][INFO] - Training Epoch: 2/2, step 6776/7134 completed (loss: 0.030350055545568466, acc: 0.9893617033958435)
[2025-02-13 04:29:56,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:57,164][root][INFO] - Training Epoch: 2/2, step 6777/7134 completed (loss: 0.021814141422510147, acc: 0.9932885766029358)
[2025-02-13 04:29:57,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:57,604][root][INFO] - Training Epoch: 2/2, step 6778/7134 completed (loss: 0.027139581739902496, acc: 0.9955423474311829)
[2025-02-13 04:29:57,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:58,025][root][INFO] - Training Epoch: 2/2, step 6779/7134 completed (loss: 0.009000048041343689, acc: 1.0)
[2025-02-13 04:29:58,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:58,446][root][INFO] - Training Epoch: 2/2, step 6780/7134 completed (loss: 0.017376627773046494, acc: 0.9970104694366455)
[2025-02-13 04:29:58,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:58,884][root][INFO] - Training Epoch: 2/2, step 6781/7134 completed (loss: 0.01621376909315586, acc: 0.9940119981765747)
[2025-02-13 04:29:59,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:59,287][root][INFO] - Training Epoch: 2/2, step 6782/7134 completed (loss: 0.023168327286839485, acc: 0.9947916865348816)
[2025-02-13 04:29:59,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:59,638][root][INFO] - Training Epoch: 2/2, step 6783/7134 completed (loss: 0.03493620455265045, acc: 0.9884836673736572)
[2025-02-13 04:29:59,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:00,061][root][INFO] - Training Epoch: 2/2, step 6784/7134 completed (loss: 0.014783781953155994, acc: 0.9950082898139954)
[2025-02-13 04:30:00,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:00,479][root][INFO] - Training Epoch: 2/2, step 6785/7134 completed (loss: 0.016661901026964188, acc: 0.9937106966972351)
[2025-02-13 04:30:00,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:00,926][root][INFO] - Training Epoch: 2/2, step 6786/7134 completed (loss: 0.013264026492834091, acc: 0.9956834316253662)
[2025-02-13 04:30:01,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:01,366][root][INFO] - Training Epoch: 2/2, step 6787/7134 completed (loss: 0.024772673845291138, acc: 0.99210524559021)
[2025-02-13 04:30:01,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:01,778][root][INFO] - Training Epoch: 2/2, step 6788/7134 completed (loss: 0.010197893716394901, acc: 0.9964028596878052)
[2025-02-13 04:30:01,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:02,168][root][INFO] - Training Epoch: 2/2, step 6789/7134 completed (loss: 0.014413250610232353, acc: 0.9961089491844177)
[2025-02-13 04:30:02,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:02,597][root][INFO] - Training Epoch: 2/2, step 6790/7134 completed (loss: 0.034239016473293304, acc: 0.9899425506591797)
[2025-02-13 04:30:02,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:03,005][root][INFO] - Training Epoch: 2/2, step 6791/7134 completed (loss: 0.02212253026664257, acc: 0.991482138633728)
[2025-02-13 04:30:03,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:03,421][root][INFO] - Training Epoch: 2/2, step 6792/7134 completed (loss: 0.012866413220763206, acc: 0.9944953918457031)
[2025-02-13 04:30:03,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:03,861][root][INFO] - Training Epoch: 2/2, step 6793/7134 completed (loss: 0.010407734662294388, acc: 0.997890293598175)
[2025-02-13 04:30:04,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:04,279][root][INFO] - Training Epoch: 2/2, step 6794/7134 completed (loss: 0.006775537971407175, acc: 0.9983579516410828)
[2025-02-13 04:30:04,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:04,710][root][INFO] - Training Epoch: 2/2, step 6795/7134 completed (loss: 0.0050662001594901085, acc: 1.0)
[2025-02-13 04:30:04,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:05,125][root][INFO] - Training Epoch: 2/2, step 6796/7134 completed (loss: 0.00843215174973011, acc: 0.9970674514770508)
[2025-02-13 04:30:05,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:05,542][root][INFO] - Training Epoch: 2/2, step 6797/7134 completed (loss: 0.04910784587264061, acc: 0.9854439496994019)
[2025-02-13 04:30:05,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:05,955][root][INFO] - Training Epoch: 2/2, step 6798/7134 completed (loss: 0.022275112569332123, acc: 0.9889807105064392)
[2025-02-13 04:30:06,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:06,372][root][INFO] - Training Epoch: 2/2, step 6799/7134 completed (loss: 0.012158998288214207, acc: 0.99726402759552)
[2025-02-13 04:30:06,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:06,775][root][INFO] - Training Epoch: 2/2, step 6800/7134 completed (loss: 0.026505229994654655, acc: 0.988950252532959)
[2025-02-13 04:30:06,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:07,227][root][INFO] - Training Epoch: 2/2, step 6801/7134 completed (loss: 0.023579468950629234, acc: 0.9905511736869812)
[2025-02-13 04:30:07,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:07,631][root][INFO] - Training Epoch: 2/2, step 6802/7134 completed (loss: 0.015089809894561768, acc: 0.9956076145172119)
[2025-02-13 04:30:07,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:08,062][root][INFO] - Training Epoch: 2/2, step 6803/7134 completed (loss: 0.013615588657557964, acc: 0.9949685335159302)
[2025-02-13 04:30:08,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:08,463][root][INFO] - Training Epoch: 2/2, step 6804/7134 completed (loss: 0.02408425882458687, acc: 0.9937499761581421)
[2025-02-13 04:30:08,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:08,908][root][INFO] - Training Epoch: 2/2, step 6805/7134 completed (loss: 0.020527034997940063, acc: 0.9967948794364929)
[2025-02-13 04:30:09,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:09,340][root][INFO] - Training Epoch: 2/2, step 6806/7134 completed (loss: 0.03508978337049484, acc: 0.9936708807945251)
[2025-02-13 04:30:09,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:09,800][root][INFO] - Training Epoch: 2/2, step 6807/7134 completed (loss: 0.016819151118397713, acc: 0.9942528605461121)
[2025-02-13 04:30:09,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:10,219][root][INFO] - Training Epoch: 2/2, step 6808/7134 completed (loss: 0.04133504256606102, acc: 0.9888357520103455)
[2025-02-13 04:30:10,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:10,663][root][INFO] - Training Epoch: 2/2, step 6809/7134 completed (loss: 0.026147596538066864, acc: 0.9905149340629578)
[2025-02-13 04:30:10,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:11,064][root][INFO] - Training Epoch: 2/2, step 6810/7134 completed (loss: 0.018075238913297653, acc: 0.99210524559021)
[2025-02-13 04:30:11,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:11,514][root][INFO] - Training Epoch: 2/2, step 6811/7134 completed (loss: 0.037208203226327896, acc: 0.993446946144104)
[2025-02-13 04:30:11,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:11,921][root][INFO] - Training Epoch: 2/2, step 6812/7134 completed (loss: 0.014237366616725922, acc: 0.9951377511024475)
[2025-02-13 04:30:12,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:12,339][root][INFO] - Training Epoch: 2/2, step 6813/7134 completed (loss: 0.007405215408653021, acc: 0.9985915422439575)
[2025-02-13 04:30:12,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:12,758][root][INFO] - Training Epoch: 2/2, step 6814/7134 completed (loss: 0.01660391129553318, acc: 0.9973118305206299)
[2025-02-13 04:30:12,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:13,186][root][INFO] - Training Epoch: 2/2, step 6815/7134 completed (loss: 0.028583811596035957, acc: 0.9908015727996826)
[2025-02-13 04:30:13,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:13,607][root][INFO] - Training Epoch: 2/2, step 6816/7134 completed (loss: 0.020115086808800697, acc: 0.9949832558631897)
[2025-02-13 04:30:13,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:14,046][root][INFO] - Training Epoch: 2/2, step 6817/7134 completed (loss: 0.05069083720445633, acc: 0.9897435903549194)
[2025-02-13 04:30:14,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:14,467][root][INFO] - Training Epoch: 2/2, step 6818/7134 completed (loss: 0.011608578264713287, acc: 0.9940029978752136)
[2025-02-13 04:30:14,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:14,894][root][INFO] - Training Epoch: 2/2, step 6819/7134 completed (loss: 0.03768574818968773, acc: 0.9892086386680603)
[2025-02-13 04:30:15,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:15,300][root][INFO] - Training Epoch: 2/2, step 6820/7134 completed (loss: 0.01716284081339836, acc: 0.9945945739746094)
[2025-02-13 04:30:15,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:15,651][root][INFO] - Training Epoch: 2/2, step 6821/7134 completed (loss: 0.015478646382689476, acc: 0.9898989796638489)
[2025-02-13 04:30:15,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:16,069][root][INFO] - Training Epoch: 2/2, step 6822/7134 completed (loss: 0.046357955783605576, acc: 0.9877836108207703)
[2025-02-13 04:30:16,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:16,490][root][INFO] - Training Epoch: 2/2, step 6823/7134 completed (loss: 0.029982000589370728, acc: 0.9899857044219971)
[2025-02-13 04:30:16,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:16,828][root][INFO] - Training Epoch: 2/2, step 6824/7134 completed (loss: 0.0637831762433052, acc: 0.9838709831237793)
[2025-02-13 04:30:16,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:17,216][root][INFO] - Training Epoch: 2/2, step 6825/7134 completed (loss: 0.008393351919949055, acc: 0.997560977935791)
[2025-02-13 04:30:17,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:17,606][root][INFO] - Training Epoch: 2/2, step 6826/7134 completed (loss: 0.03419872745871544, acc: 0.9848484992980957)
[2025-02-13 04:30:17,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:18,029][root][INFO] - Training Epoch: 2/2, step 6827/7134 completed (loss: 0.11053533852100372, acc: 0.9716494679450989)
[2025-02-13 04:30:18,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:18,450][root][INFO] - Training Epoch: 2/2, step 6828/7134 completed (loss: 0.029797857627272606, acc: 0.9872262477874756)
[2025-02-13 04:30:18,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:18,876][root][INFO] - Training Epoch: 2/2, step 6829/7134 completed (loss: 0.043192651122808456, acc: 0.9851632118225098)
[2025-02-13 04:30:19,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:19,268][root][INFO] - Training Epoch: 2/2, step 6830/7134 completed (loss: 0.02296416088938713, acc: 0.9950576424598694)
[2025-02-13 04:30:19,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:19,688][root][INFO] - Training Epoch: 2/2, step 6831/7134 completed (loss: 0.05048486217856407, acc: 0.9865591526031494)
[2025-02-13 04:30:19,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:20,096][root][INFO] - Training Epoch: 2/2, step 6832/7134 completed (loss: 0.02315455861389637, acc: 0.996303141117096)
[2025-02-13 04:30:20,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:20,503][root][INFO] - Training Epoch: 2/2, step 6833/7134 completed (loss: 0.0433228500187397, acc: 0.9839228391647339)
[2025-02-13 04:30:20,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:20,850][root][INFO] - Training Epoch: 2/2, step 6834/7134 completed (loss: 0.03292747959494591, acc: 0.9897785186767578)
[2025-02-13 04:30:20,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:21,242][root][INFO] - Training Epoch: 2/2, step 6835/7134 completed (loss: 0.018121302127838135, acc: 0.9935064911842346)
[2025-02-13 04:30:21,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:21,640][root][INFO] - Training Epoch: 2/2, step 6836/7134 completed (loss: 0.03848796710371971, acc: 0.9861809015274048)
[2025-02-13 04:30:21,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:22,087][root][INFO] - Training Epoch: 2/2, step 6837/7134 completed (loss: 0.032815784215927124, acc: 0.991411030292511)
[2025-02-13 04:30:22,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:22,489][root][INFO] - Training Epoch: 2/2, step 6838/7134 completed (loss: 0.062140174210071564, acc: 0.9814814925193787)
[2025-02-13 04:30:22,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:22,871][root][INFO] - Training Epoch: 2/2, step 6839/7134 completed (loss: 0.08565328270196915, acc: 0.9702970385551453)
[2025-02-13 04:30:23,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:23,274][root][INFO] - Training Epoch: 2/2, step 6840/7134 completed (loss: 0.06770553439855576, acc: 0.9813753366470337)
[2025-02-13 04:30:23,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:23,677][root][INFO] - Training Epoch: 2/2, step 6841/7134 completed (loss: 0.027941280975937843, acc: 0.9884560108184814)
[2025-02-13 04:30:23,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:24,127][root][INFO] - Training Epoch: 2/2, step 6842/7134 completed (loss: 0.022613750770688057, acc: 0.9912280440330505)
[2025-02-13 04:30:24,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:24,579][root][INFO] - Training Epoch: 2/2, step 6843/7134 completed (loss: 0.044474925845861435, acc: 0.9852034449577332)
[2025-02-13 04:30:24,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:24,992][root][INFO] - Training Epoch: 2/2, step 6844/7134 completed (loss: 0.013081925921142101, acc: 0.9948186278343201)
[2025-02-13 04:30:25,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:25,435][root][INFO] - Training Epoch: 2/2, step 6845/7134 completed (loss: 0.017547408118844032, acc: 0.9931972622871399)
[2025-02-13 04:30:25,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:25,841][root][INFO] - Training Epoch: 2/2, step 6846/7134 completed (loss: 0.02497410774230957, acc: 0.9928366541862488)
[2025-02-13 04:30:25,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:26,264][root][INFO] - Training Epoch: 2/2, step 6847/7134 completed (loss: 0.02853401005268097, acc: 0.989393949508667)
[2025-02-13 04:30:26,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:26,658][root][INFO] - Training Epoch: 2/2, step 6848/7134 completed (loss: 0.014193283393979073, acc: 0.9972972869873047)
[2025-02-13 04:30:26,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:27,073][root][INFO] - Training Epoch: 2/2, step 6849/7134 completed (loss: 0.022824538871645927, acc: 0.9965635538101196)
[2025-02-13 04:30:27,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:27,468][root][INFO] - Training Epoch: 2/2, step 6850/7134 completed (loss: 0.01067365426570177, acc: 0.9977116584777832)
[2025-02-13 04:30:27,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:27,895][root][INFO] - Training Epoch: 2/2, step 6851/7134 completed (loss: 0.054138634353876114, acc: 0.9844852089881897)
[2025-02-13 04:30:28,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:28,325][root][INFO] - Training Epoch: 2/2, step 6852/7134 completed (loss: 0.01538512110710144, acc: 0.9947552680969238)
[2025-02-13 04:30:28,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:28,792][root][INFO] - Training Epoch: 2/2, step 6853/7134 completed (loss: 0.02659945748746395, acc: 0.9930843710899353)
[2025-02-13 04:30:28,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:29,143][root][INFO] - Training Epoch: 2/2, step 6854/7134 completed (loss: 0.02940673567354679, acc: 0.9913232326507568)
[2025-02-13 04:30:29,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:29,534][root][INFO] - Training Epoch: 2/2, step 6855/7134 completed (loss: 0.012225687503814697, acc: 0.9929245114326477)
[2025-02-13 04:30:29,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:29,963][root][INFO] - Training Epoch: 2/2, step 6856/7134 completed (loss: 0.005137493368238211, acc: 0.9985693693161011)
[2025-02-13 04:30:30,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:30,405][root][INFO] - Training Epoch: 2/2, step 6857/7134 completed (loss: 0.01619713194668293, acc: 0.9970674514770508)
[2025-02-13 04:30:30,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:30,680][root][INFO] - Training Epoch: 2/2, step 6858/7134 completed (loss: 0.005744973197579384, acc: 1.0)
[2025-02-13 04:30:30,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:31,126][root][INFO] - Training Epoch: 2/2, step 6859/7134 completed (loss: 0.003989534918218851, acc: 0.9974392056465149)
[2025-02-13 04:30:31,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:31,544][root][INFO] - Training Epoch: 2/2, step 6860/7134 completed (loss: 0.01196912582963705, acc: 0.9982269406318665)
[2025-02-13 04:30:31,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:31,930][root][INFO] - Training Epoch: 2/2, step 6861/7134 completed (loss: 0.010975140146911144, acc: 0.9982699155807495)
[2025-02-13 04:30:32,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:32,375][root][INFO] - Training Epoch: 2/2, step 6862/7134 completed (loss: 0.026519279927015305, acc: 0.9923809766769409)
[2025-02-13 04:30:32,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:32,790][root][INFO] - Training Epoch: 2/2, step 6863/7134 completed (loss: 0.008692638948559761, acc: 0.9985875487327576)
[2025-02-13 04:30:32,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:33,233][root][INFO] - Training Epoch: 2/2, step 6864/7134 completed (loss: 0.008616169914603233, acc: 0.9966722130775452)
[2025-02-13 04:30:33,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:33,648][root][INFO] - Training Epoch: 2/2, step 6865/7134 completed (loss: 0.024601634591817856, acc: 0.9896907210350037)
[2025-02-13 04:30:33,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:34,050][root][INFO] - Training Epoch: 2/2, step 6866/7134 completed (loss: 0.008723633363842964, acc: 0.9984802603721619)
[2025-02-13 04:30:34,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:34,467][root][INFO] - Training Epoch: 2/2, step 6867/7134 completed (loss: 0.0031106271781027317, acc: 1.0)
[2025-02-13 04:30:34,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:34,794][root][INFO] - Training Epoch: 2/2, step 6868/7134 completed (loss: 0.00363279995508492, acc: 1.0)
[2025-02-13 04:30:34,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:35,142][root][INFO] - Training Epoch: 2/2, step 6869/7134 completed (loss: 0.007768701296299696, acc: 0.9957173466682434)
[2025-02-13 04:30:35,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:35,579][root][INFO] - Training Epoch: 2/2, step 6870/7134 completed (loss: 0.015821974724531174, acc: 0.9967426657676697)
[2025-02-13 04:30:35,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:36,018][root][INFO] - Training Epoch: 2/2, step 6871/7134 completed (loss: 0.0031649444717913866, acc: 1.0)
[2025-02-13 04:30:36,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:36,421][root][INFO] - Training Epoch: 2/2, step 6872/7134 completed (loss: 0.013066614046692848, acc: 0.9986149668693542)
[2025-02-13 04:30:36,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:36,853][root][INFO] - Training Epoch: 2/2, step 6873/7134 completed (loss: 0.008413262665271759, acc: 0.9957982897758484)
[2025-02-13 04:30:36,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:37,260][root][INFO] - Training Epoch: 2/2, step 6874/7134 completed (loss: 0.009417546913027763, acc: 0.9982269406318665)
[2025-02-13 04:30:37,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:37,678][root][INFO] - Training Epoch: 2/2, step 6875/7134 completed (loss: 0.008080822415649891, acc: 0.9953632354736328)
[2025-02-13 04:30:37,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:38,081][root][INFO] - Training Epoch: 2/2, step 6876/7134 completed (loss: 0.009693902917206287, acc: 0.9949238300323486)
[2025-02-13 04:30:38,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:38,518][root][INFO] - Training Epoch: 2/2, step 6877/7134 completed (loss: 0.013891947455704212, acc: 0.9971910119056702)
[2025-02-13 04:30:38,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:38,936][root][INFO] - Training Epoch: 2/2, step 6878/7134 completed (loss: 0.009999004192650318, acc: 0.9970015287399292)
[2025-02-13 04:30:39,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:39,274][root][INFO] - Training Epoch: 2/2, step 6879/7134 completed (loss: 0.005439986940473318, acc: 1.0)
[2025-02-13 04:30:39,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:39,735][root][INFO] - Training Epoch: 2/2, step 6880/7134 completed (loss: 0.0048377905040979385, acc: 0.9976717233657837)
[2025-02-13 04:30:39,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:40,180][root][INFO] - Training Epoch: 2/2, step 6881/7134 completed (loss: 0.04051120951771736, acc: 0.9939246773719788)
[2025-02-13 04:30:40,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:40,558][root][INFO] - Training Epoch: 2/2, step 6882/7134 completed (loss: 0.03435711935162544, acc: 0.9906014800071716)
[2025-02-13 04:30:40,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:40,960][root][INFO] - Training Epoch: 2/2, step 6883/7134 completed (loss: 0.006359927821904421, acc: 0.9983498454093933)
[2025-02-13 04:30:41,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:41,393][root][INFO] - Training Epoch: 2/2, step 6884/7134 completed (loss: 0.04881330206990242, acc: 0.992443323135376)
[2025-02-13 04:30:41,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:41,774][root][INFO] - Training Epoch: 2/2, step 6885/7134 completed (loss: 0.04064745083451271, acc: 0.9910714030265808)
[2025-02-13 04:30:41,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:42,183][root][INFO] - Training Epoch: 2/2, step 6886/7134 completed (loss: 0.015507861040532589, acc: 0.995245635509491)
[2025-02-13 04:30:42,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:42,573][root][INFO] - Training Epoch: 2/2, step 6887/7134 completed (loss: 0.01889178901910782, acc: 0.9938398599624634)
[2025-02-13 04:30:42,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:43,010][root][INFO] - Training Epoch: 2/2, step 6888/7134 completed (loss: 0.02216404117643833, acc: 0.9950166344642639)
[2025-02-13 04:30:43,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:43,442][root][INFO] - Training Epoch: 2/2, step 6889/7134 completed (loss: 0.004805587697774172, acc: 0.9984685778617859)
[2025-02-13 04:30:43,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:43,881][root][INFO] - Training Epoch: 2/2, step 6890/7134 completed (loss: 0.02338205836713314, acc: 0.9933949708938599)
[2025-02-13 04:30:44,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:44,288][root][INFO] - Training Epoch: 2/2, step 6891/7134 completed (loss: 0.015842445194721222, acc: 0.9940740466117859)
[2025-02-13 04:30:44,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:44,729][root][INFO] - Training Epoch: 2/2, step 6892/7134 completed (loss: 0.012355831451714039, acc: 0.9973958134651184)
[2025-02-13 04:30:44,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:45,162][root][INFO] - Training Epoch: 2/2, step 6893/7134 completed (loss: 0.03290710225701332, acc: 0.9894859790802002)
[2025-02-13 04:30:45,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:45,598][root][INFO] - Training Epoch: 2/2, step 6894/7134 completed (loss: 0.032651081681251526, acc: 0.99210524559021)
[2025-02-13 04:30:45,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:46,041][root][INFO] - Training Epoch: 2/2, step 6895/7134 completed (loss: 0.034549325704574585, acc: 0.9892086386680603)
[2025-02-13 04:30:46,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:46,430][root][INFO] - Training Epoch: 2/2, step 6896/7134 completed (loss: 0.01914161816239357, acc: 0.991304337978363)
[2025-02-13 04:30:46,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:46,866][root][INFO] - Training Epoch: 2/2, step 6897/7134 completed (loss: 0.011701501905918121, acc: 0.9963189959526062)
[2025-02-13 04:30:46,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:47,268][root][INFO] - Training Epoch: 2/2, step 6898/7134 completed (loss: 0.010148811154067516, acc: 0.9972527623176575)
[2025-02-13 04:30:47,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:47,705][root][INFO] - Training Epoch: 2/2, step 6899/7134 completed (loss: 0.02984865941107273, acc: 0.9924623370170593)
[2025-02-13 04:30:47,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:48,144][root][INFO] - Training Epoch: 2/2, step 6900/7134 completed (loss: 0.024981055408716202, acc: 0.9957567453384399)
[2025-02-13 04:30:48,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:48,587][root][INFO] - Training Epoch: 2/2, step 6901/7134 completed (loss: 0.0296455267816782, acc: 0.9907529950141907)
[2025-02-13 04:30:48,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:49,045][root][INFO] - Training Epoch: 2/2, step 6902/7134 completed (loss: 0.018644437193870544, acc: 0.9916782379150391)
[2025-02-13 04:30:49,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:49,481][root][INFO] - Training Epoch: 2/2, step 6903/7134 completed (loss: 0.015419559553265572, acc: 0.9961977005004883)
[2025-02-13 04:30:49,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:49,926][root][INFO] - Training Epoch: 2/2, step 6904/7134 completed (loss: 0.010824809782207012, acc: 0.9950248599052429)
[2025-02-13 04:30:50,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:50,374][root][INFO] - Training Epoch: 2/2, step 6905/7134 completed (loss: 0.024967404082417488, acc: 0.9936061501502991)
[2025-02-13 04:30:50,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:50,835][root][INFO] - Training Epoch: 2/2, step 6906/7134 completed (loss: 0.03516243025660515, acc: 0.9940119981765747)
[2025-02-13 04:30:50,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:51,281][root][INFO] - Training Epoch: 2/2, step 6907/7134 completed (loss: 0.03489234670996666, acc: 0.9937965273857117)
[2025-02-13 04:30:51,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:51,716][root][INFO] - Training Epoch: 2/2, step 6908/7134 completed (loss: 0.018179522827267647, acc: 0.9948186278343201)
[2025-02-13 04:30:51,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:52,154][root][INFO] - Training Epoch: 2/2, step 6909/7134 completed (loss: 0.016896655783057213, acc: 0.996052622795105)
[2025-02-13 04:30:52,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:52,589][root][INFO] - Training Epoch: 2/2, step 6910/7134 completed (loss: 0.020010987296700478, acc: 0.9961783289909363)
[2025-02-13 04:30:52,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:52,975][root][INFO] - Training Epoch: 2/2, step 6911/7134 completed (loss: 0.03207152336835861, acc: 0.9941860437393188)
[2025-02-13 04:30:53,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:53,416][root][INFO] - Training Epoch: 2/2, step 6912/7134 completed (loss: 0.031710486859083176, acc: 0.995006263256073)
[2025-02-13 04:30:53,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:53,901][root][INFO] - Training Epoch: 2/2, step 6913/7134 completed (loss: 0.029537945985794067, acc: 0.9914737939834595)
[2025-02-13 04:30:54,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:54,304][root][INFO] - Training Epoch: 2/2, step 6914/7134 completed (loss: 0.07501055300235748, acc: 0.9865319728851318)
[2025-02-13 04:30:54,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:54,753][root][INFO] - Training Epoch: 2/2, step 6915/7134 completed (loss: 0.021582430228590965, acc: 0.9948186278343201)
[2025-02-13 04:30:54,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:55,210][root][INFO] - Training Epoch: 2/2, step 6916/7134 completed (loss: 0.020815616473555565, acc: 0.9975460171699524)
[2025-02-13 04:30:55,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:55,676][root][INFO] - Training Epoch: 2/2, step 6917/7134 completed (loss: 0.04629906639456749, acc: 0.9911616444587708)
[2025-02-13 04:30:55,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:56,094][root][INFO] - Training Epoch: 2/2, step 6918/7134 completed (loss: 0.01129157841205597, acc: 0.9964871406555176)
[2025-02-13 04:30:56,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:56,532][root][INFO] - Training Epoch: 2/2, step 6919/7134 completed (loss: 0.027648381888866425, acc: 0.9953863620758057)
[2025-02-13 04:30:56,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:56,937][root][INFO] - Training Epoch: 2/2, step 6920/7134 completed (loss: 0.009252526797354221, acc: 0.995726466178894)
[2025-02-13 04:30:57,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:57,393][root][INFO] - Training Epoch: 2/2, step 6921/7134 completed (loss: 0.04637579619884491, acc: 0.991916835308075)
[2025-02-13 04:30:57,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:57,775][root][INFO] - Training Epoch: 2/2, step 6922/7134 completed (loss: 0.035339031368494034, acc: 0.9906396269798279)
[2025-02-13 04:30:57,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:58,218][root][INFO] - Training Epoch: 2/2, step 6923/7134 completed (loss: 0.045790135860443115, acc: 0.9922178983688354)
[2025-02-13 04:30:58,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:58,661][root][INFO] - Training Epoch: 2/2, step 6924/7134 completed (loss: 0.016363773494958878, acc: 0.9945504069328308)
[2025-02-13 04:30:58,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:59,088][root][INFO] - Training Epoch: 2/2, step 6925/7134 completed (loss: 0.012585685588419437, acc: 0.9973958134651184)
[2025-02-13 04:30:59,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:59,506][root][INFO] - Training Epoch: 2/2, step 6926/7134 completed (loss: 0.018251238390803337, acc: 0.9927667379379272)
[2025-02-13 04:30:59,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:59,958][root][INFO] - Training Epoch: 2/2, step 6927/7134 completed (loss: 0.049947842955589294, acc: 0.9856114983558655)
[2025-02-13 04:31:00,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:00,348][root][INFO] - Training Epoch: 2/2, step 6928/7134 completed (loss: 0.07124822586774826, acc: 0.9823874831199646)
[2025-02-13 04:31:00,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:00,839][root][INFO] - Training Epoch: 2/2, step 6929/7134 completed (loss: 0.02546634152531624, acc: 0.994350254535675)
[2025-02-13 04:31:00,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:01,301][root][INFO] - Training Epoch: 2/2, step 6930/7134 completed (loss: 0.027645761147141457, acc: 0.9875930547714233)
[2025-02-13 04:31:01,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:01,763][root][INFO] - Training Epoch: 2/2, step 6931/7134 completed (loss: 0.025101598352193832, acc: 0.9939024448394775)
[2025-02-13 04:31:01,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:02,137][root][INFO] - Training Epoch: 2/2, step 6932/7134 completed (loss: 0.031794916838407516, acc: 0.9897058606147766)
[2025-02-13 04:31:02,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:02,564][root][INFO] - Training Epoch: 2/2, step 6933/7134 completed (loss: 0.07697410136461258, acc: 0.9805951118469238)
[2025-02-13 04:31:02,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:02,944][root][INFO] - Training Epoch: 2/2, step 6934/7134 completed (loss: 0.03438327834010124, acc: 0.9903692007064819)
[2025-02-13 04:31:03,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:03,409][root][INFO] - Training Epoch: 2/2, step 6935/7134 completed (loss: 0.028680969029664993, acc: 0.9921466112136841)
[2025-02-13 04:31:03,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:03,865][root][INFO] - Training Epoch: 2/2, step 6936/7134 completed (loss: 0.012922371737658978, acc: 0.9959999918937683)
[2025-02-13 04:31:04,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:04,277][root][INFO] - Training Epoch: 2/2, step 6937/7134 completed (loss: 0.08094082772731781, acc: 0.9789842367172241)
[2025-02-13 04:31:04,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:04,724][root][INFO] - Training Epoch: 2/2, step 6938/7134 completed (loss: 0.08703505992889404, acc: 0.981249988079071)
[2025-02-13 04:31:04,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:05,189][root][INFO] - Training Epoch: 2/2, step 6939/7134 completed (loss: 0.07346814125776291, acc: 0.9888734221458435)
[2025-02-13 04:31:05,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:05,592][root][INFO] - Training Epoch: 2/2, step 6940/7134 completed (loss: 0.05374033376574516, acc: 0.9863013625144958)
[2025-02-13 04:31:05,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:05,948][root][INFO] - Training Epoch: 2/2, step 6941/7134 completed (loss: 0.024939898401498795, acc: 0.9947916865348816)
[2025-02-13 04:31:06,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:06,356][root][INFO] - Training Epoch: 2/2, step 6942/7134 completed (loss: 0.03192996606230736, acc: 0.9917808175086975)
[2025-02-13 04:31:06,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:06,703][root][INFO] - Training Epoch: 2/2, step 6943/7134 completed (loss: 0.03699205815792084, acc: 0.9937238693237305)
[2025-02-13 04:31:06,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:07,095][root][INFO] - Training Epoch: 2/2, step 6944/7134 completed (loss: 0.023708350956439972, acc: 0.9952229261398315)
[2025-02-13 04:31:07,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:07,453][root][INFO] - Training Epoch: 2/2, step 6945/7134 completed (loss: 0.051854562014341354, acc: 0.9877836108207703)
[2025-02-13 04:31:07,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:07,787][root][INFO] - Training Epoch: 2/2, step 6946/7134 completed (loss: 0.022176416590809822, acc: 0.9914039969444275)
[2025-02-13 04:31:07,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:08,188][root][INFO] - Training Epoch: 2/2, step 6947/7134 completed (loss: 0.03616456314921379, acc: 0.991304337978363)
[2025-02-13 04:31:08,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:08,611][root][INFO] - Training Epoch: 2/2, step 6948/7134 completed (loss: 0.028582101687788963, acc: 0.9899497628211975)
[2025-02-13 04:31:08,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:09,005][root][INFO] - Training Epoch: 2/2, step 6949/7134 completed (loss: 0.0455571785569191, acc: 0.9893162250518799)
[2025-02-13 04:31:09,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:09,336][root][INFO] - Training Epoch: 2/2, step 6950/7134 completed (loss: 0.02091403864324093, acc: 0.9974489808082581)
[2025-02-13 04:31:09,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:09,754][root][INFO] - Training Epoch: 2/2, step 6951/7134 completed (loss: 0.019659897312521935, acc: 0.993220329284668)
[2025-02-13 04:31:09,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:10,097][root][INFO] - Training Epoch: 2/2, step 6952/7134 completed (loss: 0.013947155326604843, acc: 0.9953271150588989)
[2025-02-13 04:31:10,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:10,495][root][INFO] - Training Epoch: 2/2, step 6953/7134 completed (loss: 0.015253104269504547, acc: 0.9934425950050354)
[2025-02-13 04:31:10,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:10,897][root][INFO] - Training Epoch: 2/2, step 6954/7134 completed (loss: 0.050637099891901016, acc: 0.9872068166732788)
[2025-02-13 04:31:11,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:11,265][root][INFO] - Training Epoch: 2/2, step 6955/7134 completed (loss: 0.02786257490515709, acc: 0.9939024448394775)
[2025-02-13 04:31:11,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:11,673][root][INFO] - Training Epoch: 2/2, step 6956/7134 completed (loss: 0.027598606422543526, acc: 0.9928951859474182)
[2025-02-13 04:31:11,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:12,072][root][INFO] - Training Epoch: 2/2, step 6957/7134 completed (loss: 0.013586917892098427, acc: 0.9960861206054688)
[2025-02-13 04:31:12,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:12,458][root][INFO] - Training Epoch: 2/2, step 6958/7134 completed (loss: 0.040899671614170074, acc: 0.9824561476707458)
[2025-02-13 04:31:12,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:12,851][root][INFO] - Training Epoch: 2/2, step 6959/7134 completed (loss: 0.03080632910132408, acc: 0.9866962432861328)
[2025-02-13 04:31:12,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:13,256][root][INFO] - Training Epoch: 2/2, step 6960/7134 completed (loss: 0.021742437034845352, acc: 0.9934959411621094)
[2025-02-13 04:31:13,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:13,649][root][INFO] - Training Epoch: 2/2, step 6961/7134 completed (loss: 0.03125346824526787, acc: 0.9939393997192383)
[2025-02-13 04:31:13,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:14,050][root][INFO] - Training Epoch: 2/2, step 6962/7134 completed (loss: 0.03146779537200928, acc: 0.9930434823036194)
[2025-02-13 04:31:14,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:14,459][root][INFO] - Training Epoch: 2/2, step 6963/7134 completed (loss: 0.012551340274512768, acc: 0.9983165264129639)
[2025-02-13 04:31:14,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:14,870][root][INFO] - Training Epoch: 2/2, step 6964/7134 completed (loss: 0.016382817178964615, acc: 0.994584858417511)
[2025-02-13 04:31:15,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:15,279][root][INFO] - Training Epoch: 2/2, step 6965/7134 completed (loss: 0.013516401872038841, acc: 0.9945054650306702)
[2025-02-13 04:31:15,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:15,641][root][INFO] - Training Epoch: 2/2, step 6966/7134 completed (loss: 0.027070391923189163, acc: 0.9850373864173889)
[2025-02-13 04:31:15,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:16,044][root][INFO] - Training Epoch: 2/2, step 6967/7134 completed (loss: 0.017926165834069252, acc: 0.9924585223197937)
[2025-02-13 04:31:16,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:16,439][root][INFO] - Training Epoch: 2/2, step 6968/7134 completed (loss: 0.026300830766558647, acc: 0.9913793206214905)
[2025-02-13 04:31:16,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:16,835][root][INFO] - Training Epoch: 2/2, step 6969/7134 completed (loss: 0.06090313568711281, acc: 0.9833679795265198)
[2025-02-13 04:31:16,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:17,236][root][INFO] - Training Epoch: 2/2, step 6970/7134 completed (loss: 0.06570357084274292, acc: 0.9789473414421082)
[2025-02-13 04:31:17,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:17,647][root][INFO] - Training Epoch: 2/2, step 6971/7134 completed (loss: 0.04279417172074318, acc: 0.984455943107605)
[2025-02-13 04:31:17,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:18,055][root][INFO] - Training Epoch: 2/2, step 6972/7134 completed (loss: 0.018837928771972656, acc: 0.9928698539733887)
[2025-02-13 04:31:18,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:18,446][root][INFO] - Training Epoch: 2/2, step 6973/7134 completed (loss: 0.0331634059548378, acc: 0.9933920502662659)
[2025-02-13 04:31:18,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:18,843][root][INFO] - Training Epoch: 2/2, step 6974/7134 completed (loss: 0.038542285561561584, acc: 0.9896551966667175)
[2025-02-13 04:31:18,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:19,234][root][INFO] - Training Epoch: 2/2, step 6975/7134 completed (loss: 0.0306264478713274, acc: 0.9934924244880676)
[2025-02-13 04:31:19,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:19,632][root][INFO] - Training Epoch: 2/2, step 6976/7134 completed (loss: 0.04408976435661316, acc: 0.9861351847648621)
[2025-02-13 04:31:19,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:20,029][root][INFO] - Training Epoch: 2/2, step 6977/7134 completed (loss: 0.03563627600669861, acc: 0.9887217879295349)
[2025-02-13 04:31:20,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:20,441][root][INFO] - Training Epoch: 2/2, step 6978/7134 completed (loss: 0.01466476172208786, acc: 0.9983739852905273)
[2025-02-13 04:31:20,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:20,877][root][INFO] - Training Epoch: 2/2, step 6979/7134 completed (loss: 0.030689815059304237, acc: 0.9939024448394775)
[2025-02-13 04:31:21,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:21,326][root][INFO] - Training Epoch: 2/2, step 6980/7134 completed (loss: 0.09190475940704346, acc: 0.9723270535469055)
[2025-02-13 04:31:21,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:21,787][root][INFO] - Training Epoch: 2/2, step 6981/7134 completed (loss: 0.09803705662488937, acc: 0.9782886505126953)
[2025-02-13 04:31:21,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:22,251][root][INFO] - Training Epoch: 2/2, step 6982/7134 completed (loss: 0.06239326670765877, acc: 0.9853836894035339)
[2025-02-13 04:31:22,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:22,699][root][INFO] - Training Epoch: 2/2, step 6983/7134 completed (loss: 0.09663911908864975, acc: 0.9777448177337646)
[2025-02-13 04:31:22,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:23,150][root][INFO] - Training Epoch: 2/2, step 6984/7134 completed (loss: 0.03082474134862423, acc: 0.9935691356658936)
[2025-02-13 04:31:23,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:23,627][root][INFO] - Training Epoch: 2/2, step 6985/7134 completed (loss: 0.09247253090143204, acc: 0.9794871807098389)
[2025-02-13 04:31:23,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:24,057][root][INFO] - Training Epoch: 2/2, step 6986/7134 completed (loss: 0.02800581604242325, acc: 0.9881129264831543)
[2025-02-13 04:31:24,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:24,533][root][INFO] - Training Epoch: 2/2, step 6987/7134 completed (loss: 0.03975118324160576, acc: 0.986868679523468)
[2025-02-13 04:31:24,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:24,804][root][INFO] - Training Epoch: 2/2, step 6988/7134 completed (loss: 0.012105311267077923, acc: 1.0)
[2025-02-13 04:31:24,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:25,152][root][INFO] - Training Epoch: 2/2, step 6989/7134 completed (loss: 0.033412303775548935, acc: 0.9927272796630859)
[2025-02-13 04:31:25,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:25,536][root][INFO] - Training Epoch: 2/2, step 6990/7134 completed (loss: 0.007680060807615519, acc: 1.0)
[2025-02-13 04:31:25,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:25,850][root][INFO] - Training Epoch: 2/2, step 6991/7134 completed (loss: 0.045670028775930405, acc: 0.9899749159812927)
[2025-02-13 04:31:25,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:26,205][root][INFO] - Training Epoch: 2/2, step 6992/7134 completed (loss: 0.03547422215342522, acc: 0.9894737005233765)
[2025-02-13 04:31:26,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:26,629][root][INFO] - Training Epoch: 2/2, step 6993/7134 completed (loss: 0.07902088761329651, acc: 0.9788838624954224)
[2025-02-13 04:31:26,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:27,100][root][INFO] - Training Epoch: 2/2, step 6994/7134 completed (loss: 0.08669275045394897, acc: 0.9791332483291626)
[2025-02-13 04:31:27,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:27,533][root][INFO] - Training Epoch: 2/2, step 6995/7134 completed (loss: 0.036414481699466705, acc: 0.9882199168205261)
[2025-02-13 04:31:27,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:27,942][root][INFO] - Training Epoch: 2/2, step 6996/7134 completed (loss: 0.024567613378167152, acc: 0.9918830990791321)
[2025-02-13 04:31:28,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:28,347][root][INFO] - Training Epoch: 2/2, step 6997/7134 completed (loss: 0.03488703444600105, acc: 0.9906542301177979)
[2025-02-13 04:31:28,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:28,666][root][INFO] - Training Epoch: 2/2, step 6998/7134 completed (loss: 0.04288024827837944, acc: 0.9901960492134094)
[2025-02-13 04:31:28,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:29,127][root][INFO] - Training Epoch: 2/2, step 6999/7134 completed (loss: 0.02017253078520298, acc: 0.9934640526771545)
[2025-02-13 04:31:29,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:29,447][root][INFO] - Training Epoch: 2/2, step 7000/7134 completed (loss: 0.012170590460300446, acc: 0.9957627058029175)
[2025-02-13 04:31:29,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:29,887][root][INFO] - Training Epoch: 2/2, step 7001/7134 completed (loss: 0.06664912402629852, acc: 0.9864457845687866)
[2025-02-13 04:31:30,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:30,331][root][INFO] - Training Epoch: 2/2, step 7002/7134 completed (loss: 0.016451191157102585, acc: 0.9918864369392395)
[2025-02-13 04:31:30,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:30,774][root][INFO] - Training Epoch: 2/2, step 7003/7134 completed (loss: 0.012300556525588036, acc: 0.9956188201904297)
[2025-02-13 04:31:30,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:31,234][root][INFO] - Training Epoch: 2/2, step 7004/7134 completed (loss: 0.05930647253990173, acc: 0.9842519760131836)
[2025-02-13 04:31:31,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:31,696][root][INFO] - Training Epoch: 2/2, step 7005/7134 completed (loss: 0.04655047133564949, acc: 0.9811066389083862)
[2025-02-13 04:31:31,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:32,150][root][INFO] - Training Epoch: 2/2, step 7006/7134 completed (loss: 0.02804119512438774, acc: 0.99452805519104)
[2025-02-13 04:31:32,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:32,593][root][INFO] - Training Epoch: 2/2, step 7007/7134 completed (loss: 0.02658879943192005, acc: 0.9940119981765747)
[2025-02-13 04:31:32,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:33,018][root][INFO] - Training Epoch: 2/2, step 7008/7134 completed (loss: 0.02326422743499279, acc: 0.9901356101036072)
[2025-02-13 04:31:33,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:33,466][root][INFO] - Training Epoch: 2/2, step 7009/7134 completed (loss: 0.021107852458953857, acc: 0.9942129850387573)
[2025-02-13 04:31:33,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:33,873][root][INFO] - Training Epoch: 2/2, step 7010/7134 completed (loss: 0.008081174455583096, acc: 0.9984591603279114)
[2025-02-13 04:31:34,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:34,323][root][INFO] - Training Epoch: 2/2, step 7011/7134 completed (loss: 0.027928190305829048, acc: 0.9879807829856873)
[2025-02-13 04:31:34,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:34,716][root][INFO] - Training Epoch: 2/2, step 7012/7134 completed (loss: 0.032012902200222015, acc: 0.9883381724357605)
[2025-02-13 04:31:34,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:35,160][root][INFO] - Training Epoch: 2/2, step 7013/7134 completed (loss: 0.0067898486740887165, acc: 0.9985590577125549)
[2025-02-13 04:31:35,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:35,578][root][INFO] - Training Epoch: 2/2, step 7014/7134 completed (loss: 0.020702961832284927, acc: 0.9919484853744507)
[2025-02-13 04:31:35,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:35,959][root][INFO] - Training Epoch: 2/2, step 7015/7134 completed (loss: 0.023378798738121986, acc: 0.989266574382782)
[2025-02-13 04:31:36,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:36,411][root][INFO] - Training Epoch: 2/2, step 7016/7134 completed (loss: 0.035556305199861526, acc: 0.9913294911384583)
[2025-02-13 04:31:36,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:36,763][root][INFO] - Training Epoch: 2/2, step 7017/7134 completed (loss: 0.0429370179772377, acc: 0.9875518679618835)
[2025-02-13 04:31:36,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:37,181][root][INFO] - Training Epoch: 2/2, step 7018/7134 completed (loss: 0.024918079376220703, acc: 0.9937984347343445)
[2025-02-13 04:31:37,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:37,570][root][INFO] - Training Epoch: 2/2, step 7019/7134 completed (loss: 0.09972458332777023, acc: 0.9780701994895935)
[2025-02-13 04:31:37,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:37,974][root][INFO] - Training Epoch: 2/2, step 7020/7134 completed (loss: 0.016563689336180687, acc: 0.9985895752906799)
[2025-02-13 04:31:38,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:38,438][root][INFO] - Training Epoch: 2/2, step 7021/7134 completed (loss: 0.020139046013355255, acc: 0.9931856989860535)
[2025-02-13 04:31:38,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:38,785][root][INFO] - Training Epoch: 2/2, step 7022/7134 completed (loss: 0.08908021450042725, acc: 0.9722222089767456)
[2025-02-13 04:31:38,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:39,161][root][INFO] - Training Epoch: 2/2, step 7023/7134 completed (loss: 0.07087386399507523, acc: 0.9761336445808411)
[2025-02-13 04:31:39,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:39,492][root][INFO] - Training Epoch: 2/2, step 7024/7134 completed (loss: 0.0316428504884243, acc: 0.9904458522796631)
[2025-02-13 04:31:39,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:39,866][root][INFO] - Training Epoch: 2/2, step 7025/7134 completed (loss: 0.0847000852227211, acc: 0.9655172228813171)
[2025-02-13 04:31:39,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:40,204][root][INFO] - Training Epoch: 2/2, step 7026/7134 completed (loss: 0.06251875311136246, acc: 0.9768339991569519)
[2025-02-13 04:31:40,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:40,527][root][INFO] - Training Epoch: 2/2, step 7027/7134 completed (loss: 0.05507718771696091, acc: 0.9786585569381714)
[2025-02-13 04:31:40,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:40,890][root][INFO] - Training Epoch: 2/2, step 7028/7134 completed (loss: 0.02587864361703396, acc: 0.990314781665802)
[2025-02-13 04:31:41,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:41,239][root][INFO] - Training Epoch: 2/2, step 7029/7134 completed (loss: 0.024312006309628487, acc: 0.9907192587852478)
[2025-02-13 04:31:41,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:41,583][root][INFO] - Training Epoch: 2/2, step 7030/7134 completed (loss: 0.05248238891363144, acc: 0.9857819676399231)
[2025-02-13 04:31:41,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:41,958][root][INFO] - Training Epoch: 2/2, step 7031/7134 completed (loss: 0.055361758917570114, acc: 0.9796954393386841)
[2025-02-13 04:31:42,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:42,291][root][INFO] - Training Epoch: 2/2, step 7032/7134 completed (loss: 0.037607237696647644, acc: 0.9918699264526367)
[2025-02-13 04:31:42,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:42,610][root][INFO] - Training Epoch: 2/2, step 7033/7134 completed (loss: 0.048012617975473404, acc: 0.9794721603393555)
[2025-02-13 04:31:42,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:42,956][root][INFO] - Training Epoch: 2/2, step 7034/7134 completed (loss: 0.058939605951309204, acc: 0.9787985682487488)
[2025-02-13 04:31:43,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:43,291][root][INFO] - Training Epoch: 2/2, step 7035/7134 completed (loss: 0.0718325525522232, acc: 0.976190447807312)
[2025-02-13 04:31:43,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:43,580][root][INFO] - Training Epoch: 2/2, step 7036/7134 completed (loss: 0.015779269859194756, acc: 1.0)
[2025-02-13 04:31:43,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:43,857][root][INFO] - Training Epoch: 2/2, step 7037/7134 completed (loss: 0.022176744416356087, acc: 0.99609375)
[2025-02-13 04:31:43,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:44,205][root][INFO] - Training Epoch: 2/2, step 7038/7134 completed (loss: 0.09379623085260391, acc: 0.9736147522926331)
[2025-02-13 04:31:44,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:44,559][root][INFO] - Training Epoch: 2/2, step 7039/7134 completed (loss: 0.016428008675575256, acc: 0.990338146686554)
[2025-02-13 04:31:44,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:44,896][root][INFO] - Training Epoch: 2/2, step 7040/7134 completed (loss: 0.089031882584095, acc: 0.9712643623352051)
[2025-02-13 04:31:45,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:45,229][root][INFO] - Training Epoch: 2/2, step 7041/7134 completed (loss: 0.08133412897586823, acc: 0.9779874086380005)
[2025-02-13 04:31:45,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:45,573][root][INFO] - Training Epoch: 2/2, step 7042/7134 completed (loss: 0.03838048502802849, acc: 0.9876922965049744)
[2025-02-13 04:31:45,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:45,896][root][INFO] - Training Epoch: 2/2, step 7043/7134 completed (loss: 0.06291873008012772, acc: 0.9733333587646484)
[2025-02-13 04:31:46,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:46,227][root][INFO] - Training Epoch: 2/2, step 7044/7134 completed (loss: 0.06234842538833618, acc: 0.9816513657569885)
[2025-02-13 04:31:46,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:46,625][root][INFO] - Training Epoch: 2/2, step 7045/7134 completed (loss: 0.03467869013547897, acc: 0.9914966225624084)
[2025-02-13 04:31:46,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:47,069][root][INFO] - Training Epoch: 2/2, step 7046/7134 completed (loss: 0.059213705360889435, acc: 0.9836065769195557)
[2025-02-13 04:31:47,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:47,502][root][INFO] - Training Epoch: 2/2, step 7047/7134 completed (loss: 0.04886086285114288, acc: 0.9833333492279053)
[2025-02-13 04:31:47,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:47,942][root][INFO] - Training Epoch: 2/2, step 7048/7134 completed (loss: 0.00418060040101409, acc: 0.9988248944282532)
[2025-02-13 04:31:48,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:48,342][root][INFO] - Training Epoch: 2/2, step 7049/7134 completed (loss: 0.02986198104918003, acc: 0.9919785857200623)
[2025-02-13 04:31:48,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:48,757][root][INFO] - Training Epoch: 2/2, step 7050/7134 completed (loss: 0.025448059663176537, acc: 0.9948186278343201)
[2025-02-13 04:31:48,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:49,199][root][INFO] - Training Epoch: 2/2, step 7051/7134 completed (loss: 0.028158217668533325, acc: 0.993678867816925)
[2025-02-13 04:31:49,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:49,644][root][INFO] - Training Epoch: 2/2, step 7052/7134 completed (loss: 0.028646083548665047, acc: 0.9906790852546692)
[2025-02-13 04:31:49,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:50,095][root][INFO] - Training Epoch: 2/2, step 7053/7134 completed (loss: 0.018995363265275955, acc: 0.9951279163360596)
[2025-02-13 04:31:50,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:50,534][root][INFO] - Training Epoch: 2/2, step 7054/7134 completed (loss: 0.06397070735692978, acc: 0.982594907283783)
[2025-02-13 04:31:50,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:50,954][root][INFO] - Training Epoch: 2/2, step 7055/7134 completed (loss: 0.03621706739068031, acc: 0.9897959232330322)
[2025-02-13 04:31:51,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:51,424][root][INFO] - Training Epoch: 2/2, step 7056/7134 completed (loss: 0.04158565029501915, acc: 0.9867947101593018)
[2025-02-13 04:31:51,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:51,885][root][INFO] - Training Epoch: 2/2, step 7057/7134 completed (loss: 0.0861203521490097, acc: 0.9808219075202942)
[2025-02-13 04:31:52,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:52,310][root][INFO] - Training Epoch: 2/2, step 7058/7134 completed (loss: 0.05580872669816017, acc: 0.9862204790115356)
[2025-02-13 04:31:52,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:52,735][root][INFO] - Training Epoch: 2/2, step 7059/7134 completed (loss: 0.008362253196537495, acc: 0.998603343963623)
[2025-02-13 04:31:52,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:53,136][root][INFO] - Training Epoch: 2/2, step 7060/7134 completed (loss: 0.0618506595492363, acc: 0.9822580814361572)
[2025-02-13 04:31:53,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:53,564][root][INFO] - Training Epoch: 2/2, step 7061/7134 completed (loss: 0.0310154240578413, acc: 0.9921875)
[2025-02-13 04:31:53,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:53,969][root][INFO] - Training Epoch: 2/2, step 7062/7134 completed (loss: 0.06959126144647598, acc: 0.9822221994400024)
[2025-02-13 04:31:54,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:54,399][root][INFO] - Training Epoch: 2/2, step 7063/7134 completed (loss: 0.05743153393268585, acc: 0.9895424842834473)
[2025-02-13 04:31:54,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:54,860][root][INFO] - Training Epoch: 2/2, step 7064/7134 completed (loss: 0.0207442045211792, acc: 0.9955489635467529)
[2025-02-13 04:31:54,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:55,309][root][INFO] - Training Epoch: 2/2, step 7065/7134 completed (loss: 0.013288429006934166, acc: 0.9942062497138977)
[2025-02-13 04:31:55,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:55,762][root][INFO] - Training Epoch: 2/2, step 7066/7134 completed (loss: 0.01402309350669384, acc: 0.9952437281608582)
[2025-02-13 04:31:55,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:56,192][root][INFO] - Training Epoch: 2/2, step 7067/7134 completed (loss: 0.019750753417611122, acc: 0.9910827875137329)
[2025-02-13 04:31:56,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:56,648][root][INFO] - Training Epoch: 2/2, step 7068/7134 completed (loss: 0.021700019016861916, acc: 0.9915561079978943)
[2025-02-13 04:31:56,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:57,124][root][INFO] - Training Epoch: 2/2, step 7069/7134 completed (loss: 0.02617393247783184, acc: 0.9940828680992126)
[2025-02-13 04:31:57,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:57,554][root][INFO] - Training Epoch: 2/2, step 7070/7134 completed (loss: 0.022303273901343346, acc: 0.9945130348205566)
[2025-02-13 04:31:57,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:57,975][root][INFO] - Training Epoch: 2/2, step 7071/7134 completed (loss: 0.041188158094882965, acc: 0.9924812316894531)
[2025-02-13 04:31:58,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:58,411][root][INFO] - Training Epoch: 2/2, step 7072/7134 completed (loss: 0.016628218814730644, acc: 0.9939393997192383)
[2025-02-13 04:31:58,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:58,883][root][INFO] - Training Epoch: 2/2, step 7073/7134 completed (loss: 0.026554688811302185, acc: 0.9897611141204834)
[2025-02-13 04:31:59,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:59,270][root][INFO] - Training Epoch: 2/2, step 7074/7134 completed (loss: 0.026335999369621277, acc: 0.9942638874053955)
[2025-02-13 04:31:59,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:59,696][root][INFO] - Training Epoch: 2/2, step 7075/7134 completed (loss: 0.023662937805056572, acc: 0.9910314083099365)
[2025-02-13 04:31:59,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:00,094][root][INFO] - Training Epoch: 2/2, step 7076/7134 completed (loss: 0.030411696061491966, acc: 0.9928774833679199)
[2025-02-13 04:32:00,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:00,481][root][INFO] - Training Epoch: 2/2, step 7077/7134 completed (loss: 0.05654273182153702, acc: 0.9836065769195557)
[2025-02-13 04:32:00,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:00,884][root][INFO] - Training Epoch: 2/2, step 7078/7134 completed (loss: 0.020349111407995224, acc: 0.995192289352417)
[2025-02-13 04:32:01,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:01,284][root][INFO] - Training Epoch: 2/2, step 7079/7134 completed (loss: 0.023591559380292892, acc: 0.9917898178100586)
[2025-02-13 04:32:01,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:01,678][root][INFO] - Training Epoch: 2/2, step 7080/7134 completed (loss: 0.02166913077235222, acc: 0.9933554530143738)
[2025-02-13 04:32:01,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:02,104][root][INFO] - Training Epoch: 2/2, step 7081/7134 completed (loss: 0.026645002886652946, acc: 0.9929873943328857)
[2025-02-13 04:32:02,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:02,516][root][INFO] - Training Epoch: 2/2, step 7082/7134 completed (loss: 0.03326490521430969, acc: 0.9951534867286682)
[2025-02-13 04:32:02,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:02,924][root][INFO] - Training Epoch: 2/2, step 7083/7134 completed (loss: 0.008594637736678123, acc: 0.9949832558631897)
[2025-02-13 04:32:03,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:03,344][root][INFO] - Training Epoch: 2/2, step 7084/7134 completed (loss: 0.03388388082385063, acc: 0.9898403286933899)
[2025-02-13 04:32:03,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:03,743][root][INFO] - Training Epoch: 2/2, step 7085/7134 completed (loss: 0.03870071843266487, acc: 0.9907407164573669)
[2025-02-13 04:32:03,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:04,173][root][INFO] - Training Epoch: 2/2, step 7086/7134 completed (loss: 0.014227267354726791, acc: 0.99589604139328)
[2025-02-13 04:32:04,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:04,571][root][INFO] - Training Epoch: 2/2, step 7087/7134 completed (loss: 0.023198306560516357, acc: 0.996303141117096)
[2025-02-13 04:32:04,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:04,984][root][INFO] - Training Epoch: 2/2, step 7088/7134 completed (loss: 0.042712677270174026, acc: 0.9933775067329407)
[2025-02-13 04:32:05,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:05,386][root][INFO] - Training Epoch: 2/2, step 7089/7134 completed (loss: 0.08358421176671982, acc: 0.9840764403343201)
[2025-02-13 04:32:05,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:05,812][root][INFO] - Training Epoch: 2/2, step 7090/7134 completed (loss: 0.02638820745050907, acc: 0.9932249188423157)
[2025-02-13 04:32:05,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:06,218][root][INFO] - Training Epoch: 2/2, step 7091/7134 completed (loss: 0.009016769006848335, acc: 0.9958217144012451)
[2025-02-13 04:32:06,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:06,640][root][INFO] - Training Epoch: 2/2, step 7092/7134 completed (loss: 0.017704837024211884, acc: 0.9914236664772034)
[2025-02-13 04:32:06,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:07,001][root][INFO] - Training Epoch: 2/2, step 7093/7134 completed (loss: 0.029099950566887856, acc: 0.9920634627342224)
[2025-02-13 04:32:07,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:07,393][root][INFO] - Training Epoch: 2/2, step 7094/7134 completed (loss: 0.014123964123427868, acc: 0.9953917264938354)
[2025-02-13 04:32:07,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:07,796][root][INFO] - Training Epoch: 2/2, step 7095/7134 completed (loss: 0.02912411279976368, acc: 0.9958677887916565)
[2025-02-13 04:32:07,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:08,185][root][INFO] - Training Epoch: 2/2, step 7096/7134 completed (loss: 0.04411541670560837, acc: 0.9882352948188782)
[2025-02-13 04:32:08,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:08,601][root][INFO] - Training Epoch: 2/2, step 7097/7134 completed (loss: 0.029401402920484543, acc: 0.9899135231971741)
[2025-02-13 04:32:08,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:09,005][root][INFO] - Training Epoch: 2/2, step 7098/7134 completed (loss: 0.013958914205431938, acc: 0.995184600353241)
[2025-02-13 04:32:09,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:09,364][root][INFO] - Training Epoch: 2/2, step 7099/7134 completed (loss: 0.024833641946315765, acc: 0.9914529919624329)
[2025-02-13 04:32:09,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:09,774][root][INFO] - Training Epoch: 2/2, step 7100/7134 completed (loss: 0.04341134801506996, acc: 0.9844961166381836)
[2025-02-13 04:32:09,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:10,125][root][INFO] - Training Epoch: 2/2, step 7101/7134 completed (loss: 0.04624184966087341, acc: 0.9874776601791382)
[2025-02-13 04:32:10,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:10,551][root][INFO] - Training Epoch: 2/2, step 7102/7134 completed (loss: 0.00844124797731638, acc: 0.998062014579773)
[2025-02-13 04:32:10,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:10,845][root][INFO] - Training Epoch: 2/2, step 7103/7134 completed (loss: 0.024291440844535828, acc: 0.9959677457809448)
[2025-02-13 04:32:10,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:11,244][root][INFO] - Training Epoch: 2/2, step 7104/7134 completed (loss: 0.014202666468918324, acc: 0.9958847761154175)
[2025-02-13 04:32:11,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:11,647][root][INFO] - Training Epoch: 2/2, step 7105/7134 completed (loss: 0.03886174038052559, acc: 0.9900000095367432)
[2025-02-13 04:32:11,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:12,026][root][INFO] - Training Epoch: 2/2, step 7106/7134 completed (loss: 0.017260249704122543, acc: 0.9966611266136169)
[2025-02-13 04:32:12,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:12,432][root][INFO] - Training Epoch: 2/2, step 7107/7134 completed (loss: 0.014502436853945255, acc: 0.9953415989875793)
[2025-02-13 04:32:12,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:12,834][root][INFO] - Training Epoch: 2/2, step 7108/7134 completed (loss: 0.028406841680407524, acc: 0.9933333396911621)
[2025-02-13 04:32:12,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:13,204][root][INFO] - Training Epoch: 2/2, step 7109/7134 completed (loss: 0.03991864621639252, acc: 0.9887429475784302)
[2025-02-13 04:32:13,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:13,609][root][INFO] - Training Epoch: 2/2, step 7110/7134 completed (loss: 0.01918541081249714, acc: 0.996610164642334)
[2025-02-13 04:32:13,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:13,949][root][INFO] - Training Epoch: 2/2, step 7111/7134 completed (loss: 0.03570990264415741, acc: 0.9911308288574219)
[2025-02-13 04:32:14,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:14,354][root][INFO] - Training Epoch: 2/2, step 7112/7134 completed (loss: 0.01750257797539234, acc: 0.9958847761154175)
[2025-02-13 04:32:14,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:14,787][root][INFO] - Training Epoch: 2/2, step 7113/7134 completed (loss: 0.05846207216382027, acc: 0.9779179692268372)
[2025-02-13 04:32:14,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:15,181][root][INFO] - Training Epoch: 2/2, step 7114/7134 completed (loss: 0.02418322116136551, acc: 0.990439772605896)
[2025-02-13 04:32:15,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:15,573][root][INFO] - Training Epoch: 2/2, step 7115/7134 completed (loss: 0.03365461155772209, acc: 0.9876543283462524)
[2025-02-13 04:32:15,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:15,972][root][INFO] - Training Epoch: 2/2, step 7116/7134 completed (loss: 0.0212787427008152, acc: 0.994727611541748)
[2025-02-13 04:32:16,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:16,359][root][INFO] - Training Epoch: 2/2, step 7117/7134 completed (loss: 0.01564628817141056, acc: 0.996666669845581)
[2025-02-13 04:32:16,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:16,776][root][INFO] - Training Epoch: 2/2, step 7118/7134 completed (loss: 0.04245788976550102, acc: 0.9912790656089783)
[2025-02-13 04:32:16,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:17,168][root][INFO] - Training Epoch: 2/2, step 7119/7134 completed (loss: 0.013405511155724525, acc: 0.9934533834457397)
[2025-02-13 04:32:17,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:17,526][root][INFO] - Training Epoch: 2/2, step 7120/7134 completed (loss: 0.03376620635390282, acc: 0.9883177280426025)
[2025-02-13 04:32:17,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:17,912][root][INFO] - Training Epoch: 2/2, step 7121/7134 completed (loss: 0.011282175779342651, acc: 0.9982394576072693)
[2025-02-13 04:32:18,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:18,259][root][INFO] - Training Epoch: 2/2, step 7122/7134 completed (loss: 0.018254784867167473, acc: 0.9919785857200623)
[2025-02-13 04:32:18,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:18,577][root][INFO] - Training Epoch: 2/2, step 7123/7134 completed (loss: 0.01717439480125904, acc: 0.9902439117431641)
[2025-02-13 04:32:18,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:18,976][root][INFO] - Training Epoch: 2/2, step 7124/7134 completed (loss: 0.025301484391093254, acc: 0.9906191229820251)
[2025-02-13 04:32:19,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:19,329][root][INFO] - Training Epoch: 2/2, step 7125/7134 completed (loss: 0.015705175697803497, acc: 0.9933664798736572)
[2025-02-13 04:32:19,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:19,730][root][INFO] - Training Epoch: 2/2, step 7126/7134 completed (loss: 0.05613615736365318, acc: 0.9831365942955017)
[2025-02-13 04:32:19,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:20,110][root][INFO] - Training Epoch: 2/2, step 7127/7134 completed (loss: 0.018290730193257332, acc: 0.993630588054657)
[2025-02-13 04:32:20,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:20,503][root][INFO] - Training Epoch: 2/2, step 7128/7134 completed (loss: 0.023612171411514282, acc: 0.9918699264526367)
[2025-02-13 04:32:20,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:20,903][root][INFO] - Training Epoch: 2/2, step 7129/7134 completed (loss: 0.046767957508563995, acc: 0.9798657894134521)
[2025-02-13 04:32:21,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:21,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:22,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:22,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:23,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:23,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:23,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:24,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:24,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:24,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:25,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:25,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:25,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:26,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:26,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:26,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:27,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:27,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:28,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:28,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:28,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:29,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:29,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:29,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:30,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:30,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:30,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:31,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:31,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:32,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:32,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:33,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:33,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:33,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:34,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:34,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:34,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:35,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:35,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:35,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:36,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:36,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:36,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:37,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:37,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:38,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:38,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:38,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:39,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:39,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:40,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:40,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:40,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:41,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:41,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:41,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:42,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:42,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:43,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:43,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:43,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:44,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:44,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:44,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:45,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:45,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:45,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:46,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:46,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:47,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:47,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:47,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:48,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:48,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:48,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:49,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:49,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:49,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:50,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:50,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:51,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:51,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:51,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:52,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:52,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:53,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:53,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:53,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:54,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:54,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:54,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:55,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:55,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:55,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:56,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:56,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:57,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:57,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:57,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:58,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:58,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:58,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:59,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:59,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:59,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:00,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:00,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:01,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:01,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:01,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:02,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:02,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:02,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:03,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:03,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:03,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:04,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:04,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:05,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:05,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:05,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:06,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:06,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:06,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:07,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:07,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:08,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:08,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:08,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:09,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:09,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:09,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:10,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:10,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:11,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:11,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:11,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:12,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:12,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:12,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:13,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:13,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:13,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:14,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:14,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:15,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:15,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:15,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:16,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:16,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:16,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:17,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:17,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:18,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:18,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:18,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:19,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:19,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:19,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:20,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:20,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:20,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:21,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:21,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:21,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:22,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:22,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:23,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:23,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:23,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:24,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:24,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:25,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:25,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:26,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:26,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:26,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:27,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:27,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:27,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:28,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:28,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:28,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:29,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:29,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:29,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:30,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:30,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:30,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:31,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:31,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:32,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:32,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:32,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:33,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:33,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:33,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:34,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:34,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:34,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:35,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:35,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:35,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:36,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:36,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:36,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:37,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:37,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:37,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:38,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:38,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:39,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:39,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:39,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:40,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:40,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:40,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:41,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:41,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:42,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:42,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:42,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:43,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:43,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:43,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:44,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:44,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:45,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:45,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:45,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:46,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:46,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:47,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:47,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:47,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:48,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:48,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:48,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:49,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:49,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:50,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:50,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:50,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:51,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:51,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:51,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:52,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:52,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:53,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:53,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:54,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:54,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:54,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:55,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:55,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:55,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:56,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:56,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:57,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:57,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:57,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:58,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:58,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:58,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:59,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:59,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:00,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:00,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:00,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:01,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:01,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:02,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:02,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:03,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:03,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:03,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:04,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:04,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:05,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:05,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:05,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:06,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:06,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:07,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:07,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:08,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:08,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:09,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:09,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:09,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:10,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:10,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:11,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:11,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:12,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:12,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:12,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:13,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:13,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:14,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:14,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:14,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:15,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:15,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:15,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:16,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:16,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:17,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:17,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:17,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:18,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:18,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:19,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:19,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:19,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:20,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:20,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:21,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:21,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:21,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:22,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:22,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:22,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:23,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:23,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:23,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:24,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:24,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:25,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:25,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:25,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:26,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:26,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:27,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:27,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:27,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:28,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:28,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:29,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:29,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:29,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:30,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:30,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:31,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:31,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:32,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:32,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:32,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:33,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:33,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:33,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:34,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:34,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:35,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:35,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:35,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:36,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:36,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:36,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:37,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:37,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:38,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:39,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:39,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:40,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:40,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:41,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:41,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:41,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:42,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:42,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:42,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:43,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:43,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:43,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:44,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:44,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:44,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:45,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:45,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:45,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:46,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:46,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:47,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:47,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:47,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:48,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:48,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:48,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:49,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:49,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:49,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:50,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:50,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:51,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:51,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:51,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:52,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:52,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:52,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:53,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:53,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:53,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:54,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:54,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:55,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:55,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:55,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:56,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:56,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:56,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:57,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:57,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:57,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:58,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:58,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:58,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:59,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:59,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:59,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:00,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:00,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:01,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:01,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:01,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:02,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:02,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:02,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:03,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:03,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:04,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:04,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:04,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:05,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:05,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:05,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:06,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:06,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:06,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:07,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:07,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:08,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:08,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:08,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:09,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:09,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:10,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:10,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:10,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:10,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:11,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:11,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:11,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:12,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:12,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:13,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:13,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:13,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:14,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:14,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:14,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:15,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:15,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:15,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:16,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:16,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:16,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:17,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:17,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:17,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:18,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:18,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:18,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:19,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:19,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:20,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:20,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:20,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:21,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:21,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:22,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:22,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:22,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:23,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:23,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:23,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:24,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:24,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:25,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:25,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:26,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:26,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:26,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:27,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:27,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:28,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:28,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:28,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:29,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:29,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:29,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:30,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:30,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:31,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:31,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:32,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:32,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:32,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:33,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:33,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:33,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:34,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:34,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:35,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:35,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:35,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:36,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:36,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:36,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:37,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:37,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:38,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:38,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:38,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:39,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:39,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:39,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:40,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:40,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:41,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:41,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:41,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:42,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:42,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:42,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:43,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:43,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:44,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:44,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:44,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:45,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:45,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:45,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:46,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:46,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:47,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:47,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:47,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:48,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:48,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:49,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:49,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:49,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:50,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:50,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:51,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:51,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:51,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:52,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:52,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:52,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:53,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:53,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:54,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:54,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:54,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:55,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:55,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:55,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:56,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:56,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:56,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:57,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:57,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:58,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:58,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:59,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:59,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:59,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:00,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:00,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:01,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:01,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:01,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:02,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:02,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:03,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:03,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:03,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:04,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:04,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:04,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:05,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:05,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:06,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:06,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:06,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:07,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:07,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:07,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:08,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:08,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:08,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:09,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:09,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:09,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:10,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:10,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:11,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:11,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:11,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:12,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:12,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:12,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:13,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:13,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:13,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:14,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:14,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:14,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:15,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:15,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:15,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:16,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:16,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:17,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:17,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:18,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:19,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:19,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:19,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:20,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:20,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:21,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:21,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:22,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:22,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:22,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:23,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:23,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:23,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:24,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:24,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:25,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:25,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:25,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:26,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:26,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:27,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:27,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:27,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:28,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:28,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:28,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:29,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:29,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:30,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:30,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:30,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:31,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:31,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:31,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:32,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:32,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:33,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:33,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:33,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:34,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:34,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:34,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:35,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:35,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:36,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:36,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:37,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:37,658][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0509, device='cuda:0') eval_epoch_loss=tensor(0.0497, device='cuda:0') eval_epoch_acc=tensor(0.9870, device='cuda:0')
[2025-02-13 04:36:37,660][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 04:36:37,661][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 04:36:37,945][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_7130_loss_0.04968715086579323/model.pt
[2025-02-13 04:36:37,949][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 04:36:37,949][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9870140552520752
[2025-02-13 04:36:38,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:38,385][root][INFO] - Training Epoch: 2/2, step 7130/7134 completed (loss: 0.080985426902771, acc: 0.9750000238418579)
[2025-02-13 04:36:38,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:38,776][root][INFO] - Training Epoch: 2/2, step 7131/7134 completed (loss: 0.011292980052530766, acc: 0.9930394291877747)
[2025-02-13 04:36:38,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:39,090][root][INFO] - Training Epoch: 2/2, step 7132/7134 completed (loss: 0.02481071464717388, acc: 0.9932659864425659)
[2025-02-13 04:36:39,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:39,454][root][INFO] - Training Epoch: 2/2, step 7133/7134 completed (loss: 0.010654238052666187, acc: 0.9944547414779663)
[2025-02-13 04:36:39,890][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.0385, train_epoch_loss=0.0377, epoch time 3981.0773158855736s
[2025-02-13 04:36:39,890][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2025-02-13 04:36:39,890][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 30 GB
[2025-02-13 04:36:39,890][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2025-02-13 04:36:39,891][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 12
[2025-02-13 04:36:39,891][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 8 GB
[2025-02-13 04:36:39,894][root][INFO] - Key: avg_train_prep, Value: 1.0796785354614258
[2025-02-13 04:36:39,896][root][INFO] - Key: avg_train_loss, Value: 0.07593381404876709
[2025-02-13 04:36:39,896][root][INFO] - Key: avg_train_acc, Value: 0.9795873761177063
[2025-02-13 04:36:39,896][root][INFO] - Key: avg_eval_prep, Value: 1.0628242492675781
[2025-02-13 04:36:39,896][root][INFO] - Key: avg_eval_loss, Value: 0.0607844777405262
[2025-02-13 04:36:39,896][root][INFO] - Key: avg_eval_acc, Value: 0.9836357831954956
[2025-02-13 04:36:39,897][root][INFO] - Key: avg_epoch_time, Value: 3975.2100779376924
[2025-02-13 04:36:39,897][root][INFO] - Key: avg_checkpoint_time, Value: 0.3045533783733845
