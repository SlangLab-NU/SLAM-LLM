[2025-01-03 01:37:15,378][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_q-former_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-01-03 01:37:15,378][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-03 01:37:15,379][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'q-former', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'psst_phoneme_wavlm_llama32_1b_q-former_peft'}
[2025-01-03 01:37:59,894][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_q-former_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-01-03 01:37:59,895][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-03 01:37:59,896][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'q-former', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'psst_phoneme_wavlm_llama32_1b_q-former_peft'}
[2025-01-03 01:43:57,581][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_q-former_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-01-03 01:43:57,582][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-03 01:43:57,583][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'q-former', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'psst_phoneme_wavlm_llama32_1b_q-former_peft'}
[2025-01-03 01:44:00,121][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-01-03 01:44:07,196][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-03 01:44:07,199][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-01-03 01:44:07,202][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-03 01:44:07,204][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-01-03 01:44:13,511][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-03 01:44:13,513][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-01-03 01:44:13,514][slam_llm.models.slam_model][INFO] - setup peft...
[2025-01-03 01:44:13,680][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-03 01:44:13,683][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-01-03 01:44:14,872][slam_llm.utils.train_utils][INFO] - --> Module q-former
[2025-01-03 01:44:14,874][slam_llm.utils.train_utils][INFO] - --> q-former has 69.361152 Million params

[2025-01-03 01:44:14,876][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_q-former_peft/asr_epoch_4_step_137_loss_1.845609426498413/model.pt
[2025-01-03 01:44:15,520][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-01-03 01:44:15,528][slam_llm.utils.train_utils][INFO] - --> asr has 74.997248 Million params

[2025-01-03 01:44:15,579][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-01-03 01:44:16,706][root][INFO] - --> Training Set Length = 652
[2025-01-03 01:44:16,707][root][INFO] - =====================================
[2025-01-03 01:44:22,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:45:15,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:47:52,224][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_q-former_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-01-03 01:47:52,226][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-03 01:47:52,226][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'q-former', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'psst_phoneme_wavlm_llama32_1b_q-former_peft'}
[2025-01-03 01:47:54,782][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-01-03 01:48:00,962][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-03 01:48:00,965][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-01-03 01:48:00,967][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-03 01:48:00,968][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-01-03 01:48:09,455][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-03 01:48:09,457][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-01-03 01:48:09,458][slam_llm.models.slam_model][INFO] - setup peft...
[2025-01-03 01:48:09,594][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-03 01:48:09,596][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-01-03 01:48:10,566][slam_llm.utils.train_utils][INFO] - --> Module q-former
[2025-01-03 01:48:10,567][slam_llm.utils.train_utils][INFO] - --> q-former has 69.361152 Million params

[2025-01-03 01:48:10,567][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_q-former_peft/asr_epoch_4_step_137_loss_1.845609426498413/model.pt
[2025-01-03 01:48:11,207][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-01-03 01:48:11,212][slam_llm.utils.train_utils][INFO] - --> asr has 74.997248 Million params

[2025-01-03 01:48:13,597][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-01-03 01:48:15,499][root][INFO] - --> Training Set Length = 652
[2025-01-03 01:48:15,499][root][INFO] - =====================================
[2025-01-03 01:48:17,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:48:18,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:48:24,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:48:25,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:48:31,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:48:38,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:48:45,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:48:52,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:48:58,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:49:04,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:49:11,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:49:17,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:49:24,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:49:31,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:49:37,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:49:38,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:49:44,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:49:50,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:49:56,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:50:02,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:50:08,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:50:09,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:50:09,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:50:15,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:50:21,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:50:28,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:50:34,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:50:40,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:50:46,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:50:47,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:50:48,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:50:54,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:00,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:06,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:12,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:18,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:24,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:25,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:31,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:37,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:43,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:44,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:50,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:51,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:57,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:58,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:51:58,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:52:05,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:52:11,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:52:18,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:52:25,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:52:30,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:52:31,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:52:37,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:52:38,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:52:44,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:52:50,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:52:57,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:53:03,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:53:10,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:53:10,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:53:17,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:53:23,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:53:29,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:53:36,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:53:43,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:53:49,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:53:49,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:53:50,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:53:56,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:54:02,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:54:08,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:54:15,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:54:22,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:54:22,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:54:28,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:54:34,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:54:41,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:54:48,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:54:55,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:55:01,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:55:08,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:55:14,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:55:15,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:55:15,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:55:22,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:55:28,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:55:34,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:55:34,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:55:41,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:55:48,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:55:55,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:02,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:10,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:16,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:22,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:28,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:35,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:41,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:47,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:54,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:55,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:55,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:56,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:56,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:56:57,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:57:03,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:57:04,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:57:04,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:57:05,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:57:05,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:57:11,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:57:18,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:57:24,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:57:30,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:57:36,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:57:43,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:57:49,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:57:55,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:01,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:07,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:07,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:13,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:20,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:26,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:32,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:32,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:33,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:39,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:45,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:51,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:52,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:58:58,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:59:05,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:59:05,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:59:11,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:59:18,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:59:24,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:59:30,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:59:36,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:59:36,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:59:43,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:59:43,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:59:49,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 01:59:55,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:00:01,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:00:07,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:00:13,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:00:14,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:00:20,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:00:26,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:00:32,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:00:39,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:00:47,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:00:53,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:01:00,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:01:06,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:01:06,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:01:07,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:01:13,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:01:20,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:01:26,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-03 02:01:34,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:35:34,754][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_q-former_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-02-07 07:35:34,754][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-07 07:35:34,754][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'q-former', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'psst_phoneme_wavlm_llama32_1b_q-former_peft'}
[2025-02-07 07:35:35,986][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-07 07:35:41,150][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-07 07:35:41,152][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-07 07:35:41,154][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-07 07:35:41,155][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-07 07:35:45,766][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-07 07:35:45,767][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-07 07:35:45,768][slam_llm.models.slam_model][INFO] - setup peft...
[2025-02-07 07:35:45,882][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-07 07:35:45,883][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-07 07:35:46,821][slam_llm.utils.train_utils][INFO] - --> Module q-former
[2025-02-07 07:35:46,822][slam_llm.utils.train_utils][INFO] - --> q-former has 69.361152 Million params

[2025-02-07 07:35:46,822][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_q-former_peft/asr_epoch_10_step_554_loss_2.2861762046813965/model.pt
[2025-02-07 07:35:47,485][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-07 07:35:47,490][slam_llm.utils.train_utils][INFO] - --> asr has 74.997248 Million params

[2025-02-07 07:35:49,058][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-07 07:35:49,880][root][INFO] - --> Training Set Length = 652
[2025-02-07 07:35:49,881][root][INFO] - =====================================
[2025-02-07 07:35:51,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:35:51,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:35:52,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:35:52,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:35:53,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:35:55,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:01,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:03,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:04,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:05,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:07,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:08,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:14,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:20,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:21,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:21,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:23,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:24,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:25,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:25,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:26,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:27,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:27,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:28,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:29,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:30,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:31,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:32,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:33,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:33,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:34,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:35,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:35,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:36,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:37,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:38,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:39,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:39,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:40,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:41,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:42,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:43,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:44,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:44,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:45,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:46,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:46,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:53,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:36:54,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:00,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:06,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:07,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:08,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:08,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:09,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:10,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:11,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:12,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:18,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:20,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:20,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:26,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:27,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:29,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:30,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:36,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:37,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:38,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:38,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:39,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:40,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:41,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:47,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:53,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:54,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:54,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:37:56,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:02,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:08,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:14,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:20,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:26,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:32,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:33,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:33,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:34,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:35,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:35,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:36,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:37,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:45,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:51,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:38:57,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:04,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:06,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:07,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:08,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:09,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:10,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:16,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:22,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:23,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:23,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:24,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:24,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:25,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:26,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:26,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:27,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:28,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:28,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:29,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:30,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:31,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:32,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:33,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:35,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:35,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:36,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:37,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:37,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:38,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:39,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:39,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:41,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:42,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:42,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:43,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:43,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:44,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:45,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:46,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:47,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:48,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:48,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:54,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:55,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:56,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:56,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:57,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:58,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:59,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:39:59,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:00,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:01,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:02,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:03,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:03,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:04,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:05,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:05,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:07,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:08,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:16,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:17,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:19,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:19,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:20,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:20,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:22,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:25,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:26,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:33,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-07 07:40:34,756][root][INFO] - Predictions written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_q-former_peft/decode_test_beam4_pred_20250207_073549
[2025-02-07 07:40:34,756][root][INFO] - Ground truth written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_q-former_peft/decode_test_beam4_gt_20250207_073549
