[2025-01-02 00:35:23,114][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-01-02 00:35:23,115][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-02 00:35:23,115][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-01-02 00:35:23,115][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-01-02_00-35-22.txt', 'log_interval': 5}
[2025-01-02 00:36:12,357][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-01-02 00:36:12,357][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-02 00:36:12,357][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-01-02 00:36:12,357][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-01-02_00-36-11.txt', 'log_interval': 5}
[2025-01-02 00:36:34,837][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-01-02 00:36:41,044][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-02 00:36:41,047][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-01-02 00:36:41,050][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-02 00:36:41,051][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-01-02 00:36:49,522][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-02 00:36:49,524][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-01-02 00:36:49,525][slam_llm.models.slam_model][INFO] - setup peft...
[2025-01-02 00:36:49,666][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-02 00:36:49,668][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-01-02 00:36:49,797][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-01-02 00:36:49,797][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-01-02 00:36:49,798][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.050975121557712555/model.pt
[2025-01-02 00:36:50,002][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-01-02 00:36:50,008][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-01-02 00:36:50,021][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-01-02 00:36:52,776][root][INFO] - --> Training Set Length = 2298
[2025-01-02 00:36:52,789][root][INFO] - --> Validation Set Length = 341
[2025-01-02 00:36:52,789][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-02 00:36:52,790][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-02 00:38:11,888][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-01-02 00:38:11,889][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-02 00:38:11,889][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-01-02 00:38:11,889][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-01-02_00-38-11.txt', 'log_interval': 5}
[2025-01-02 00:38:31,918][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-01-02 00:38:37,339][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-02 00:38:37,341][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-01-02 00:38:37,343][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-02 00:38:37,344][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-01-02 00:38:43,096][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-02 00:38:43,098][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-01-02 00:38:43,098][slam_llm.models.slam_model][INFO] - setup peft...
[2025-01-02 00:38:43,224][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-02 00:38:43,226][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-01-02 00:38:43,334][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-01-02 00:38:43,334][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-01-02 00:38:43,334][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.050975121557712555/model.pt
[2025-01-02 00:38:43,505][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-01-02 00:38:43,508][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-01-02 00:38:45,240][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-01-02 00:38:46,005][root][INFO] - --> Training Set Length = 2298
[2025-01-02 00:38:46,013][root][INFO] - --> Validation Set Length = 341
[2025-01-02 00:38:46,013][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-02 00:38:46,014][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-02 00:38:47,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:48,483][root][INFO] - Training Epoch: 1/2, step 0/574 completed (loss: 4.565918922424316, acc: 0.2222222238779068)
[2025-01-02 00:38:48,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:48,886][root][INFO] - Training Epoch: 1/2, step 1/574 completed (loss: 3.6701340675354004, acc: 0.2800000011920929)
[2025-01-02 00:38:49,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:49,410][root][INFO] - Training Epoch: 1/2, step 2/574 completed (loss: 3.0726566314697266, acc: 0.4054054021835327)
[2025-01-02 00:38:49,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:49,789][root][INFO] - Training Epoch: 1/2, step 3/574 completed (loss: 4.179104328155518, acc: 0.2368421107530594)
[2025-01-02 00:38:49,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:50,229][root][INFO] - Training Epoch: 1/2, step 4/574 completed (loss: 4.1991047859191895, acc: 0.21621622145175934)
[2025-01-02 00:38:50,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:50,637][root][INFO] - Training Epoch: 1/2, step 5/574 completed (loss: 3.3390657901763916, acc: 0.3928571343421936)
[2025-01-02 00:38:50,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:51,035][root][INFO] - Training Epoch: 1/2, step 6/574 completed (loss: 4.611793518066406, acc: 0.30612245202064514)
[2025-01-02 00:38:51,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:51,419][root][INFO] - Training Epoch: 1/2, step 7/574 completed (loss: 3.025235414505005, acc: 0.46666666865348816)
[2025-01-02 00:38:51,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:51,871][root][INFO] - Training Epoch: 1/2, step 8/574 completed (loss: 3.5062613487243652, acc: 0.5)
[2025-01-02 00:38:52,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:52,253][root][INFO] - Training Epoch: 1/2, step 9/574 completed (loss: 1.9377830028533936, acc: 0.692307710647583)
[2025-01-02 00:38:52,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:52,646][root][INFO] - Training Epoch: 1/2, step 10/574 completed (loss: 1.4856634140014648, acc: 0.6666666865348816)
[2025-01-02 00:38:52,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:53,053][root][INFO] - Training Epoch: 1/2, step 11/574 completed (loss: 3.778326988220215, acc: 0.3076923191547394)
[2025-01-02 00:38:53,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:53,490][root][INFO] - Training Epoch: 1/2, step 12/574 completed (loss: 3.122279405593872, acc: 0.4545454680919647)
[2025-01-02 00:38:53,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:53,864][root][INFO] - Training Epoch: 1/2, step 13/574 completed (loss: 3.3837459087371826, acc: 0.3695652186870575)
[2025-01-02 00:38:54,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:54,272][root][INFO] - Training Epoch: 1/2, step 14/574 completed (loss: 3.736240863800049, acc: 0.47058823704719543)
[2025-01-02 00:38:54,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:54,676][root][INFO] - Training Epoch: 1/2, step 15/574 completed (loss: 2.7348246574401855, acc: 0.5102040767669678)
[2025-01-02 00:38:54,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:55,087][root][INFO] - Training Epoch: 1/2, step 16/574 completed (loss: 3.640578031539917, acc: 0.42105263471603394)
[2025-01-02 00:38:55,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:55,480][root][INFO] - Training Epoch: 1/2, step 17/574 completed (loss: 2.8711702823638916, acc: 0.4583333432674408)
[2025-01-02 00:38:55,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:55,897][root][INFO] - Training Epoch: 1/2, step 18/574 completed (loss: 4.022885322570801, acc: 0.3611111044883728)
[2025-01-02 00:38:56,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:56,296][root][INFO] - Training Epoch: 1/2, step 19/574 completed (loss: 3.6987786293029785, acc: 0.5263158082962036)
[2025-01-02 00:38:56,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:56,691][root][INFO] - Training Epoch: 1/2, step 20/574 completed (loss: 2.517129421234131, acc: 0.5769230723381042)
[2025-01-02 00:38:56,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:57,045][root][INFO] - Training Epoch: 1/2, step 21/574 completed (loss: 2.9080355167388916, acc: 0.48275861144065857)
[2025-01-02 00:38:57,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:57,449][root][INFO] - Training Epoch: 1/2, step 22/574 completed (loss: 4.26425313949585, acc: 0.3199999928474426)
[2025-01-02 00:38:57,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:57,835][root][INFO] - Training Epoch: 1/2, step 23/574 completed (loss: 2.4392616748809814, acc: 0.6666666865348816)
[2025-01-02 00:38:57,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:58,189][root][INFO] - Training Epoch: 1/2, step 24/574 completed (loss: 2.8660073280334473, acc: 0.5625)
[2025-01-02 00:38:58,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:58,627][root][INFO] - Training Epoch: 1/2, step 25/574 completed (loss: 3.410203456878662, acc: 0.4150943458080292)
[2025-01-02 00:38:58,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:59,019][root][INFO] - Training Epoch: 1/2, step 26/574 completed (loss: 3.3793797492980957, acc: 0.27397260069847107)
[2025-01-02 00:38:59,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:00,421][root][INFO] - Training Epoch: 1/2, step 27/574 completed (loss: 3.235335111618042, acc: 0.3596837818622589)
[2025-01-02 00:39:00,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:00,883][root][INFO] - Training Epoch: 1/2, step 28/574 completed (loss: 3.8038437366485596, acc: 0.3488371968269348)
[2025-01-02 00:39:01,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:01,313][root][INFO] - Training Epoch: 1/2, step 29/574 completed (loss: 3.4300684928894043, acc: 0.40963855385780334)
[2025-01-02 00:39:01,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:01,757][root][INFO] - Training Epoch: 1/2, step 30/574 completed (loss: 3.1179370880126953, acc: 0.43209877610206604)
[2025-01-02 00:39:01,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:02,172][root][INFO] - Training Epoch: 1/2, step 31/574 completed (loss: 3.7029573917388916, acc: 0.3928571343421936)
[2025-01-02 00:39:02,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:02,538][root][INFO] - Training Epoch: 1/2, step 32/574 completed (loss: 2.630068778991699, acc: 0.5185185074806213)
[2025-01-02 00:39:02,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:02,923][root][INFO] - Training Epoch: 1/2, step 33/574 completed (loss: 2.9637179374694824, acc: 0.6086956262588501)
[2025-01-02 00:39:03,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:03,358][root][INFO] - Training Epoch: 1/2, step 34/574 completed (loss: 2.693861722946167, acc: 0.45378151535987854)
[2025-01-02 00:39:03,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:03,790][root][INFO] - Training Epoch: 1/2, step 35/574 completed (loss: 2.7415852546691895, acc: 0.5409836173057556)
[2025-01-02 00:39:03,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:04,226][root][INFO] - Training Epoch: 1/2, step 36/574 completed (loss: 2.9526925086975098, acc: 0.4285714328289032)
[2025-01-02 00:39:04,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:04,619][root][INFO] - Training Epoch: 1/2, step 37/574 completed (loss: 2.871122121810913, acc: 0.5423728823661804)
[2025-01-02 00:39:04,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:05,003][root][INFO] - Training Epoch: 1/2, step 38/574 completed (loss: 2.828233003616333, acc: 0.5632184147834778)
[2025-01-02 00:39:05,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:05,364][root][INFO] - Training Epoch: 1/2, step 39/574 completed (loss: 4.379754066467285, acc: 0.2380952388048172)
[2025-01-02 00:39:05,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:05,679][root][INFO] - Training Epoch: 1/2, step 40/574 completed (loss: 2.794499635696411, acc: 0.6538461446762085)
[2025-01-02 00:39:05,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:06,169][root][INFO] - Training Epoch: 1/2, step 41/574 completed (loss: 2.3172364234924316, acc: 0.5945945978164673)
[2025-01-02 00:39:06,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:06,561][root][INFO] - Training Epoch: 1/2, step 42/574 completed (loss: 3.5864038467407227, acc: 0.3692307770252228)
[2025-01-02 00:39:06,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:07,012][root][INFO] - Training Epoch: 1/2, step 43/574 completed (loss: 3.4926998615264893, acc: 0.3232323229312897)
[2025-01-02 00:39:07,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:07,460][root][INFO] - Training Epoch: 1/2, step 44/574 completed (loss: 2.6371116638183594, acc: 0.5670102834701538)
[2025-01-02 00:39:07,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:07,933][root][INFO] - Training Epoch: 1/2, step 45/574 completed (loss: 3.284898519515991, acc: 0.40441176295280457)
[2025-01-02 00:39:08,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:08,314][root][INFO] - Training Epoch: 1/2, step 46/574 completed (loss: 3.328519582748413, acc: 0.38461539149284363)
[2025-01-02 00:39:08,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:08,666][root][INFO] - Training Epoch: 1/2, step 47/574 completed (loss: 1.991088628768921, acc: 0.5925925970077515)
[2025-01-02 00:39:08,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:08,995][root][INFO] - Training Epoch: 1/2, step 48/574 completed (loss: 2.2159512042999268, acc: 0.5357142686843872)
[2025-01-02 00:39:09,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:09,388][root][INFO] - Training Epoch: 1/2, step 49/574 completed (loss: 2.1327288150787354, acc: 0.5833333134651184)
[2025-01-02 00:39:09,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:09,821][root][INFO] - Training Epoch: 1/2, step 50/574 completed (loss: 3.500363349914551, acc: 0.4736842215061188)
[2025-01-02 00:39:09,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:10,264][root][INFO] - Training Epoch: 1/2, step 51/574 completed (loss: 3.3304097652435303, acc: 0.460317462682724)
[2025-01-02 00:39:10,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:10,694][root][INFO] - Training Epoch: 1/2, step 52/574 completed (loss: 4.112359523773193, acc: 0.3661971688270569)
[2025-01-02 00:39:10,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:11,182][root][INFO] - Training Epoch: 1/2, step 53/574 completed (loss: 3.9678523540496826, acc: 0.2866666615009308)
[2025-01-02 00:39:11,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:11,630][root][INFO] - Training Epoch: 1/2, step 54/574 completed (loss: 4.3024091720581055, acc: 0.21621622145175934)
[2025-01-02 00:39:11,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:12,023][root][INFO] - Training Epoch: 1/2, step 55/574 completed (loss: 1.8599413633346558, acc: 0.5384615659713745)
[2025-01-02 00:39:13,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:15,402][root][INFO] - Training Epoch: 1/2, step 56/574 completed (loss: 3.06178617477417, acc: 0.33788394927978516)
[2025-01-02 00:39:15,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:16,648][root][INFO] - Training Epoch: 1/2, step 57/574 completed (loss: 3.0808749198913574, acc: 0.35729846358299255)
[2025-01-02 00:39:16,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:17,355][root][INFO] - Training Epoch: 1/2, step 58/574 completed (loss: 3.3392577171325684, acc: 0.3693181872367859)
[2025-01-02 00:39:17,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:17,942][root][INFO] - Training Epoch: 1/2, step 59/574 completed (loss: 2.519524574279785, acc: 0.4485294222831726)
[2025-01-02 00:39:18,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:18,565][root][INFO] - Training Epoch: 1/2, step 60/574 completed (loss: 2.8573834896087646, acc: 0.4057970941066742)
[2025-01-02 00:39:18,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:19,015][root][INFO] - Training Epoch: 1/2, step 61/574 completed (loss: 2.907433271408081, acc: 0.38749998807907104)
[2025-01-02 00:39:19,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:19,359][root][INFO] - Training Epoch: 1/2, step 62/574 completed (loss: 1.6543489694595337, acc: 0.529411792755127)
[2025-01-02 00:39:19,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:19,765][root][INFO] - Training Epoch: 1/2, step 63/574 completed (loss: 2.6071159839630127, acc: 0.5277777910232544)
[2025-01-02 00:39:19,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:20,154][root][INFO] - Training Epoch: 1/2, step 64/574 completed (loss: 1.9234144687652588, acc: 0.640625)
[2025-01-02 00:39:20,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:20,548][root][INFO] - Training Epoch: 1/2, step 65/574 completed (loss: 1.4698727130889893, acc: 0.6206896305084229)
[2025-01-02 00:39:20,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:20,927][root][INFO] - Training Epoch: 1/2, step 66/574 completed (loss: 3.459401845932007, acc: 0.3392857015132904)
[2025-01-02 00:39:21,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:21,308][root][INFO] - Training Epoch: 1/2, step 67/574 completed (loss: 2.7412240505218506, acc: 0.4000000059604645)
[2025-01-02 00:39:21,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:21,737][root][INFO] - Training Epoch: 1/2, step 68/574 completed (loss: 1.187901258468628, acc: 0.7200000286102295)
[2025-01-02 00:39:21,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:22,158][root][INFO] - Training Epoch: 1/2, step 69/574 completed (loss: 2.24293851852417, acc: 0.4166666567325592)
[2025-01-02 00:39:22,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:22,532][root][INFO] - Training Epoch: 1/2, step 70/574 completed (loss: 3.6772358417510986, acc: 0.3636363744735718)
[2025-01-02 00:39:22,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:22,924][root][INFO] - Training Epoch: 1/2, step 71/574 completed (loss: 2.6245367527008057, acc: 0.40441176295280457)
[2025-01-02 00:39:23,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:23,339][root][INFO] - Training Epoch: 1/2, step 72/574 completed (loss: 1.8136476278305054, acc: 0.5555555820465088)
[2025-01-02 00:39:23,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:23,777][root][INFO] - Training Epoch: 1/2, step 73/574 completed (loss: 2.6639866828918457, acc: 0.3692307770252228)
[2025-01-02 00:39:23,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:24,202][root][INFO] - Training Epoch: 1/2, step 74/574 completed (loss: 3.5617454051971436, acc: 0.30612245202064514)
[2025-01-02 00:39:24,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:24,571][root][INFO] - Training Epoch: 1/2, step 75/574 completed (loss: 2.5667800903320312, acc: 0.3805970251560211)
[2025-01-02 00:39:24,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:24,996][root][INFO] - Training Epoch: 1/2, step 76/574 completed (loss: 3.0736048221588135, acc: 0.3540146052837372)
[2025-01-02 00:39:25,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:25,349][root][INFO] - Training Epoch: 1/2, step 77/574 completed (loss: 1.0581291913986206, acc: 0.7142857313156128)
[2025-01-02 00:39:25,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:25,742][root][INFO] - Training Epoch: 1/2, step 78/574 completed (loss: 1.4690192937850952, acc: 0.625)
[2025-01-02 00:39:25,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:26,147][root][INFO] - Training Epoch: 1/2, step 79/574 completed (loss: 1.2073217630386353, acc: 0.6666666865348816)
[2025-01-02 00:39:26,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:26,524][root][INFO] - Training Epoch: 1/2, step 80/574 completed (loss: 1.9726632833480835, acc: 0.692307710647583)
[2025-01-02 00:39:26,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:26,878][root][INFO] - Training Epoch: 1/2, step 81/574 completed (loss: 2.6414265632629395, acc: 0.557692289352417)
[2025-01-02 00:39:27,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:27,240][root][INFO] - Training Epoch: 1/2, step 82/574 completed (loss: 2.7956719398498535, acc: 0.4423076808452606)
[2025-01-02 00:39:27,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:27,630][root][INFO] - Training Epoch: 1/2, step 83/574 completed (loss: 0.8084146976470947, acc: 0.84375)
[2025-01-02 00:39:27,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:27,974][root][INFO] - Training Epoch: 1/2, step 84/574 completed (loss: 1.9850724935531616, acc: 0.5942028760910034)
[2025-01-02 00:39:28,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:28,347][root][INFO] - Training Epoch: 1/2, step 85/574 completed (loss: 2.5030436515808105, acc: 0.5600000023841858)
[2025-01-02 00:39:28,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:28,714][root][INFO] - Training Epoch: 1/2, step 86/574 completed (loss: 0.9830278158187866, acc: 0.695652186870575)
[2025-01-02 00:39:28,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:29,200][root][INFO] - Training Epoch: 1/2, step 87/574 completed (loss: 2.8882803916931152, acc: 0.4000000059604645)
[2025-01-02 00:39:29,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:29,559][root][INFO] - Training Epoch: 1/2, step 88/574 completed (loss: 3.065053939819336, acc: 0.3883495032787323)
[2025-01-02 00:39:29,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:30,694][root][INFO] - Training Epoch: 1/2, step 89/574 completed (loss: 2.4016566276550293, acc: 0.49514561891555786)
[2025-01-02 00:39:30,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:31,577][root][INFO] - Training Epoch: 1/2, step 90/574 completed (loss: 2.8966856002807617, acc: 0.42473119497299194)
[2025-01-02 00:39:31,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:32,384][root][INFO] - Training Epoch: 1/2, step 91/574 completed (loss: 2.4038703441619873, acc: 0.5)
[2025-01-02 00:39:32,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:33,201][root][INFO] - Training Epoch: 1/2, step 92/574 completed (loss: 2.4089112281799316, acc: 0.5263158082962036)
[2025-01-02 00:39:33,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:34,194][root][INFO] - Training Epoch: 1/2, step 93/574 completed (loss: 3.3138771057128906, acc: 0.2673267424106598)
[2025-01-02 00:39:34,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:34,530][root][INFO] - Training Epoch: 1/2, step 94/574 completed (loss: 2.3489127159118652, acc: 0.4838709533214569)
[2025-01-02 00:39:34,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:34,919][root][INFO] - Training Epoch: 1/2, step 95/574 completed (loss: 2.417255401611328, acc: 0.3478260934352875)
[2025-01-02 00:39:35,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:35,384][root][INFO] - Training Epoch: 1/2, step 96/574 completed (loss: 3.222446918487549, acc: 0.32773110270500183)
[2025-01-02 00:39:35,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:35,759][root][INFO] - Training Epoch: 1/2, step 97/574 completed (loss: 3.275271415710449, acc: 0.32692307233810425)
[2025-01-02 00:39:35,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:36,167][root][INFO] - Training Epoch: 1/2, step 98/574 completed (loss: 3.3138062953948975, acc: 0.30656933784484863)
[2025-01-02 00:39:36,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:36,510][root][INFO] - Training Epoch: 1/2, step 99/574 completed (loss: 3.423379898071289, acc: 0.31343284249305725)
[2025-01-02 00:39:36,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:36,891][root][INFO] - Training Epoch: 1/2, step 100/574 completed (loss: 2.4203052520751953, acc: 0.550000011920929)
[2025-01-02 00:39:37,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:37,195][root][INFO] - Training Epoch: 1/2, step 101/574 completed (loss: 1.3619434833526611, acc: 0.7272727489471436)
[2025-01-02 00:39:37,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:37,544][root][INFO] - Training Epoch: 1/2, step 102/574 completed (loss: 0.7013345956802368, acc: 0.739130437374115)
[2025-01-02 00:39:37,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:37,969][root][INFO] - Training Epoch: 1/2, step 103/574 completed (loss: 0.9561676383018494, acc: 0.7727272510528564)
[2025-01-02 00:39:38,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:38,392][root][INFO] - Training Epoch: 1/2, step 104/574 completed (loss: 1.580527901649475, acc: 0.6206896305084229)
[2025-01-02 00:39:38,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:38,774][root][INFO] - Training Epoch: 1/2, step 105/574 completed (loss: 1.3592196702957153, acc: 0.6976743936538696)
[2025-01-02 00:39:38,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:39,137][root][INFO] - Training Epoch: 1/2, step 106/574 completed (loss: 0.9877244830131531, acc: 0.7599999904632568)
[2025-01-02 00:39:39,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:39,475][root][INFO] - Training Epoch: 1/2, step 107/574 completed (loss: 0.9230515956878662, acc: 0.8235294222831726)
[2025-01-02 00:39:39,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:39,876][root][INFO] - Training Epoch: 1/2, step 108/574 completed (loss: 0.8038516640663147, acc: 0.8461538553237915)
[2025-01-02 00:39:40,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:40,259][root][INFO] - Training Epoch: 1/2, step 109/574 completed (loss: 0.6703882217407227, acc: 0.8571428656578064)
[2025-01-02 00:39:40,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:40,643][root][INFO] - Training Epoch: 1/2, step 110/574 completed (loss: 1.8341482877731323, acc: 0.6615384817123413)
[2025-01-02 00:39:40,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:41,113][root][INFO] - Training Epoch: 1/2, step 111/574 completed (loss: 1.5186504125595093, acc: 0.6315789222717285)
[2025-01-02 00:39:41,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:41,509][root][INFO] - Training Epoch: 1/2, step 112/574 completed (loss: 2.771782875061035, acc: 0.4736842215061188)
[2025-01-02 00:39:41,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:41,872][root][INFO] - Training Epoch: 1/2, step 113/574 completed (loss: 1.5293385982513428, acc: 0.5897436141967773)
[2025-01-02 00:39:42,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:42,300][root][INFO] - Training Epoch: 1/2, step 114/574 completed (loss: 1.5130350589752197, acc: 0.6530612111091614)
[2025-01-02 00:39:42,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:42,665][root][INFO] - Training Epoch: 1/2, step 115/574 completed (loss: 1.3971327543258667, acc: 0.7272727489471436)
[2025-01-02 00:39:42,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:43,052][root][INFO] - Training Epoch: 1/2, step 116/574 completed (loss: 1.3162338733673096, acc: 0.6190476417541504)
[2025-01-02 00:39:43,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:43,445][root][INFO] - Training Epoch: 1/2, step 117/574 completed (loss: 1.486054539680481, acc: 0.6341463327407837)
[2025-01-02 00:39:43,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:43,898][root][INFO] - Training Epoch: 1/2, step 118/574 completed (loss: 1.597548484802246, acc: 0.7096773982048035)
[2025-01-02 00:39:44,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:44,881][root][INFO] - Training Epoch: 1/2, step 119/574 completed (loss: 1.993386149406433, acc: 0.5399239659309387)
[2025-01-02 00:39:45,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:45,299][root][INFO] - Training Epoch: 1/2, step 120/574 completed (loss: 1.396101713180542, acc: 0.6933333277702332)
[2025-01-02 00:39:45,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:45,749][root][INFO] - Training Epoch: 1/2, step 121/574 completed (loss: 2.0678627490997314, acc: 0.5961538553237915)
[2025-01-02 00:39:45,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:46,116][root][INFO] - Training Epoch: 1/2, step 122/574 completed (loss: 1.1890355348587036, acc: 0.6666666865348816)
[2025-01-02 00:39:46,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:46,491][root][INFO] - Training Epoch: 1/2, step 123/574 completed (loss: 1.0188530683517456, acc: 0.6842105388641357)
[2025-01-02 00:39:46,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:46,875][root][INFO] - Training Epoch: 1/2, step 124/574 completed (loss: 2.0181427001953125, acc: 0.5521472096443176)
[2025-01-02 00:39:47,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:47,256][root][INFO] - Training Epoch: 1/2, step 125/574 completed (loss: 2.0524840354919434, acc: 0.5)
[2025-01-02 00:39:47,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:47,592][root][INFO] - Training Epoch: 1/2, step 126/574 completed (loss: 1.9213632345199585, acc: 0.5)
[2025-01-02 00:39:47,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:48,014][root][INFO] - Training Epoch: 1/2, step 127/574 completed (loss: 1.6306703090667725, acc: 0.5833333134651184)
[2025-01-02 00:39:48,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:48,396][root][INFO] - Training Epoch: 1/2, step 128/574 completed (loss: 1.7955976724624634, acc: 0.5897436141967773)
[2025-01-02 00:39:48,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:48,822][root][INFO] - Training Epoch: 1/2, step 129/574 completed (loss: 1.710350751876831, acc: 0.5441176295280457)
[2025-01-02 00:39:48,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:49,162][root][INFO] - Training Epoch: 1/2, step 130/574 completed (loss: 2.664008140563965, acc: 0.3461538553237915)
[2025-01-02 00:39:49,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:49,533][root][INFO] - Training Epoch: 1/2, step 131/574 completed (loss: 2.4736194610595703, acc: 0.43478259444236755)
[2025-01-02 00:39:49,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:49,951][root][INFO] - Training Epoch: 1/2, step 132/574 completed (loss: 2.0687029361724854, acc: 0.5)
[2025-01-02 00:39:50,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:50,356][root][INFO] - Training Epoch: 1/2, step 133/574 completed (loss: 2.3381268978118896, acc: 0.3913043439388275)
[2025-01-02 00:39:50,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:50,732][root][INFO] - Training Epoch: 1/2, step 134/574 completed (loss: 1.697987675666809, acc: 0.5714285969734192)
[2025-01-02 00:39:50,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:51,106][root][INFO] - Training Epoch: 1/2, step 135/574 completed (loss: 1.8390274047851562, acc: 0.5)
[2025-01-02 00:39:51,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:51,439][root][INFO] - Training Epoch: 1/2, step 136/574 completed (loss: 1.8226277828216553, acc: 0.5952380895614624)
[2025-01-02 00:39:51,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:51,760][root][INFO] - Training Epoch: 1/2, step 137/574 completed (loss: 2.2117486000061035, acc: 0.36666667461395264)
[2025-01-02 00:39:51,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:52,111][root][INFO] - Training Epoch: 1/2, step 138/574 completed (loss: 1.7167556285858154, acc: 0.6521739363670349)
[2025-01-02 00:39:52,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:52,458][root][INFO] - Training Epoch: 1/2, step 139/574 completed (loss: 0.6512330770492554, acc: 0.8571428656578064)
[2025-01-02 00:39:52,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:52,863][root][INFO] - Training Epoch: 1/2, step 140/574 completed (loss: 0.7566673755645752, acc: 0.7692307829856873)
[2025-01-02 00:39:52,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:53,266][root][INFO] - Training Epoch: 1/2, step 141/574 completed (loss: 1.3652455806732178, acc: 0.6451612710952759)
[2025-01-02 00:39:53,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:53,683][root][INFO] - Training Epoch: 1/2, step 142/574 completed (loss: 1.5792851448059082, acc: 0.5945945978164673)
[2025-01-02 00:39:54,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:54,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:55,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:55,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:56,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:56,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:56,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:57,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:57,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:57,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:58,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:58,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:59,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:59,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:59,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:00,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:00,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:01,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:01,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:01,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:02,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:02,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:02,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:03,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:03,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:03,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:04,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:04,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:05,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:05,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:05,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:06,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:06,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:06,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:07,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:07,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:08,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:08,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:08,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:09,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:09,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:09,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:10,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:10,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:11,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:11,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:11,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:12,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:12,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:12,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:13,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:13,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:13,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:14,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:14,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:14,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:15,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:15,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:15,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:16,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:16,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:16,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:17,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:17,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:18,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:18,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:18,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:19,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:19,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:19,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:20,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:20,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:20,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:21,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:21,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:22,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:22,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:22,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:23,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:23,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:24,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:24,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:24,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:25,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:25,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:26,035][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.4350, device='cuda:0') eval_epoch_loss=tensor(1.2340, device='cuda:0') eval_epoch_acc=tensor(0.7119, device='cuda:0')
[2025-01-02 00:40:26,037][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:40:26,037][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:40:26,291][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_143_loss_1.2340302467346191/model.pt
[2025-01-02 00:40:26,300][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:40:26,301][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 1.2340302467346191
[2025-01-02 00:40:26,302][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7118833065032959
[2025-01-02 00:40:26,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:26,963][root][INFO] - Training Epoch: 1/2, step 143/574 completed (loss: 1.9555134773254395, acc: 0.5526315569877625)
[2025-01-02 00:40:27,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:27,372][root][INFO] - Training Epoch: 1/2, step 144/574 completed (loss: 1.4470372200012207, acc: 0.6865671873092651)
[2025-01-02 00:40:27,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:27,801][root][INFO] - Training Epoch: 1/2, step 145/574 completed (loss: 1.6237736940383911, acc: 0.5510203838348389)
[2025-01-02 00:40:27,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:28,308][root][INFO] - Training Epoch: 1/2, step 146/574 completed (loss: 1.9318411350250244, acc: 0.478723406791687)
[2025-01-02 00:40:28,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:28,664][root][INFO] - Training Epoch: 1/2, step 147/574 completed (loss: 2.225682497024536, acc: 0.4714285731315613)
[2025-01-02 00:40:28,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:29,085][root][INFO] - Training Epoch: 1/2, step 148/574 completed (loss: 1.9076851606369019, acc: 0.4642857015132904)
[2025-01-02 00:40:29,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:29,441][root][INFO] - Training Epoch: 1/2, step 149/574 completed (loss: 1.9024313688278198, acc: 0.52173912525177)
[2025-01-02 00:40:29,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:29,790][root][INFO] - Training Epoch: 1/2, step 150/574 completed (loss: 1.4507282972335815, acc: 0.5862069129943848)
[2025-01-02 00:40:29,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:30,119][root][INFO] - Training Epoch: 1/2, step 151/574 completed (loss: 2.0053417682647705, acc: 0.5652173757553101)
[2025-01-02 00:40:30,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:30,454][root][INFO] - Training Epoch: 1/2, step 152/574 completed (loss: 1.2923908233642578, acc: 0.6779661178588867)
[2025-01-02 00:40:30,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:30,838][root][INFO] - Training Epoch: 1/2, step 153/574 completed (loss: 1.47339928150177, acc: 0.6140350699424744)
[2025-01-02 00:40:31,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:31,266][root][INFO] - Training Epoch: 1/2, step 154/574 completed (loss: 1.68242609500885, acc: 0.6486486196517944)
[2025-01-02 00:40:31,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:31,658][root][INFO] - Training Epoch: 1/2, step 155/574 completed (loss: 1.2317341566085815, acc: 0.7857142686843872)
[2025-01-02 00:40:31,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:31,988][root][INFO] - Training Epoch: 1/2, step 156/574 completed (loss: 1.2030788660049438, acc: 0.6521739363670349)
[2025-01-02 00:40:32,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:32,289][root][INFO] - Training Epoch: 1/2, step 157/574 completed (loss: 2.9596621990203857, acc: 0.3684210479259491)
[2025-01-02 00:40:33,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:34,009][root][INFO] - Training Epoch: 1/2, step 158/574 completed (loss: 3.4603662490844727, acc: 0.3918918967247009)
[2025-01-02 00:40:34,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:34,327][root][INFO] - Training Epoch: 1/2, step 159/574 completed (loss: 2.5724847316741943, acc: 0.40740740299224854)
[2025-01-02 00:40:34,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:34,824][root][INFO] - Training Epoch: 1/2, step 160/574 completed (loss: 2.9554572105407715, acc: 0.3720930218696594)
[2025-01-02 00:40:35,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:35,470][root][INFO] - Training Epoch: 1/2, step 161/574 completed (loss: 3.0566723346710205, acc: 0.3529411852359772)
[2025-01-02 00:40:35,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:36,062][root][INFO] - Training Epoch: 1/2, step 162/574 completed (loss: 3.194429636001587, acc: 0.33707866072654724)
[2025-01-02 00:40:36,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:36,418][root][INFO] - Training Epoch: 1/2, step 163/574 completed (loss: 1.60767662525177, acc: 0.7045454382896423)
[2025-01-02 00:40:36,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:36,765][root][INFO] - Training Epoch: 1/2, step 164/574 completed (loss: 0.9492246508598328, acc: 0.7142857313156128)
[2025-01-02 00:40:36,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:37,101][root][INFO] - Training Epoch: 1/2, step 165/574 completed (loss: 1.6065897941589355, acc: 0.5517241358757019)
[2025-01-02 00:40:37,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:37,444][root][INFO] - Training Epoch: 1/2, step 166/574 completed (loss: 0.8231494426727295, acc: 0.8367347121238708)
[2025-01-02 00:40:37,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:37,807][root][INFO] - Training Epoch: 1/2, step 167/574 completed (loss: 0.7496836185455322, acc: 0.8199999928474426)
[2025-01-02 00:40:37,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:38,237][root][INFO] - Training Epoch: 1/2, step 168/574 completed (loss: 1.4847321510314941, acc: 0.7222222089767456)
[2025-01-02 00:40:38,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:38,660][root][INFO] - Training Epoch: 1/2, step 169/574 completed (loss: 1.3755122423171997, acc: 0.6470588445663452)
[2025-01-02 00:40:39,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:39,797][root][INFO] - Training Epoch: 1/2, step 170/574 completed (loss: 2.2570252418518066, acc: 0.5068492889404297)
[2025-01-02 00:40:39,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:40,108][root][INFO] - Training Epoch: 1/2, step 171/574 completed (loss: 1.0501433610916138, acc: 0.6666666865348816)
[2025-01-02 00:40:40,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:40,383][root][INFO] - Training Epoch: 1/2, step 172/574 completed (loss: 1.6915427446365356, acc: 0.5925925970077515)
[2025-01-02 00:40:40,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:40,740][root][INFO] - Training Epoch: 1/2, step 173/574 completed (loss: 2.112375497817993, acc: 0.4642857015132904)
[2025-01-02 00:40:40,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:41,323][root][INFO] - Training Epoch: 1/2, step 174/574 completed (loss: 1.4983453750610352, acc: 0.6371681690216064)
[2025-01-02 00:40:41,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:41,647][root][INFO] - Training Epoch: 1/2, step 175/574 completed (loss: 1.578614354133606, acc: 0.6376811861991882)
[2025-01-02 00:40:41,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:41,988][root][INFO] - Training Epoch: 1/2, step 176/574 completed (loss: 1.2939586639404297, acc: 0.6477272510528564)
[2025-01-02 00:40:42,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:42,905][root][INFO] - Training Epoch: 1/2, step 177/574 completed (loss: 2.226735830307007, acc: 0.49618321657180786)
[2025-01-02 00:40:43,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:43,573][root][INFO] - Training Epoch: 1/2, step 178/574 completed (loss: 2.3262643814086914, acc: 0.4888888895511627)
[2025-01-02 00:40:43,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:43,891][root][INFO] - Training Epoch: 1/2, step 179/574 completed (loss: 1.2655489444732666, acc: 0.6721311211585999)
[2025-01-02 00:40:43,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:44,202][root][INFO] - Training Epoch: 1/2, step 180/574 completed (loss: 0.6177307963371277, acc: 0.8333333134651184)
[2025-01-02 00:40:44,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:44,506][root][INFO] - Training Epoch: 1/2, step 181/574 completed (loss: 0.10565561056137085, acc: 1.0)
[2025-01-02 00:40:44,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:44,838][root][INFO] - Training Epoch: 1/2, step 182/574 completed (loss: 0.7209062576293945, acc: 0.7857142686843872)
[2025-01-02 00:40:44,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:45,210][root][INFO] - Training Epoch: 1/2, step 183/574 completed (loss: 0.8879206776618958, acc: 0.792682945728302)
[2025-01-02 00:40:45,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:45,564][root][INFO] - Training Epoch: 1/2, step 184/574 completed (loss: 1.1788359880447388, acc: 0.7673715949058533)
[2025-01-02 00:40:45,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:45,920][root][INFO] - Training Epoch: 1/2, step 185/574 completed (loss: 0.9879315495491028, acc: 0.7694524526596069)
[2025-01-02 00:40:46,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:46,448][root][INFO] - Training Epoch: 1/2, step 186/574 completed (loss: 1.0306320190429688, acc: 0.7437499761581421)
[2025-01-02 00:40:46,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:46,984][root][INFO] - Training Epoch: 1/2, step 187/574 completed (loss: 0.9307247400283813, acc: 0.7786116600036621)
[2025-01-02 00:40:47,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:47,393][root][INFO] - Training Epoch: 1/2, step 188/574 completed (loss: 1.147809624671936, acc: 0.6975088715553284)
[2025-01-02 00:40:47,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:47,710][root][INFO] - Training Epoch: 1/2, step 189/574 completed (loss: 1.1194149255752563, acc: 0.6800000071525574)
[2025-01-02 00:40:47,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:48,302][root][INFO] - Training Epoch: 1/2, step 190/574 completed (loss: 1.9097312688827515, acc: 0.5348837375640869)
[2025-01-02 00:40:48,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:49,147][root][INFO] - Training Epoch: 1/2, step 191/574 completed (loss: 2.7567241191864014, acc: 0.4126984179019928)
[2025-01-02 00:40:49,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:50,070][root][INFO] - Training Epoch: 1/2, step 192/574 completed (loss: 2.402129888534546, acc: 0.4545454680919647)
[2025-01-02 00:40:50,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:50,816][root][INFO] - Training Epoch: 1/2, step 193/574 completed (loss: 2.0187549591064453, acc: 0.5176470875740051)
[2025-01-02 00:40:51,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:51,948][root][INFO] - Training Epoch: 1/2, step 194/574 completed (loss: 2.287086248397827, acc: 0.42592594027519226)
[2025-01-02 00:40:52,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:52,905][root][INFO] - Training Epoch: 1/2, step 195/574 completed (loss: 2.0336925983428955, acc: 0.5161290168762207)
[2025-01-02 00:40:53,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:53,267][root][INFO] - Training Epoch: 1/2, step 196/574 completed (loss: 0.8821660280227661, acc: 0.6785714030265808)
[2025-01-02 00:40:53,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:53,629][root][INFO] - Training Epoch: 1/2, step 197/574 completed (loss: 2.126171588897705, acc: 0.550000011920929)
[2025-01-02 00:40:53,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:53,973][root][INFO] - Training Epoch: 1/2, step 198/574 completed (loss: 1.7755298614501953, acc: 0.6470588445663452)
[2025-01-02 00:40:54,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:54,304][root][INFO] - Training Epoch: 1/2, step 199/574 completed (loss: 1.7423195838928223, acc: 0.654411792755127)
[2025-01-02 00:40:54,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:54,643][root][INFO] - Training Epoch: 1/2, step 200/574 completed (loss: 1.3644949197769165, acc: 0.6440678238868713)
[2025-01-02 00:40:54,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:55,061][root][INFO] - Training Epoch: 1/2, step 201/574 completed (loss: 1.791461706161499, acc: 0.5820895433425903)
[2025-01-02 00:40:55,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:55,444][root][INFO] - Training Epoch: 1/2, step 202/574 completed (loss: 2.061650037765503, acc: 0.5436893105506897)
[2025-01-02 00:40:55,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:55,761][root][INFO] - Training Epoch: 1/2, step 203/574 completed (loss: 1.5478763580322266, acc: 0.60317462682724)
[2025-01-02 00:40:55,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:56,073][root][INFO] - Training Epoch: 1/2, step 204/574 completed (loss: 0.4092247784137726, acc: 0.901098906993866)
[2025-01-02 00:40:56,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:56,407][root][INFO] - Training Epoch: 1/2, step 205/574 completed (loss: 0.8969773650169373, acc: 0.7982062697410583)
[2025-01-02 00:40:56,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:56,814][root][INFO] - Training Epoch: 1/2, step 206/574 completed (loss: 0.8912990093231201, acc: 0.7795275449752808)
[2025-01-02 00:40:56,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:57,140][root][INFO] - Training Epoch: 1/2, step 207/574 completed (loss: 1.0292813777923584, acc: 0.7887930870056152)
[2025-01-02 00:40:57,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:57,505][root][INFO] - Training Epoch: 1/2, step 208/574 completed (loss: 0.8289788961410522, acc: 0.804347813129425)
[2025-01-02 00:40:57,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:57,910][root][INFO] - Training Epoch: 1/2, step 209/574 completed (loss: 1.0197430849075317, acc: 0.774319052696228)
[2025-01-02 00:40:58,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:58,258][root][INFO] - Training Epoch: 1/2, step 210/574 completed (loss: 0.783724308013916, acc: 0.804347813129425)
[2025-01-02 00:40:58,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:58,572][root][INFO] - Training Epoch: 1/2, step 211/574 completed (loss: 0.912301778793335, acc: 0.782608687877655)
[2025-01-02 00:40:58,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:58,871][root][INFO] - Training Epoch: 1/2, step 212/574 completed (loss: 0.45795607566833496, acc: 0.8571428656578064)
[2025-01-02 00:40:58,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:59,199][root][INFO] - Training Epoch: 1/2, step 213/574 completed (loss: 0.9606814384460449, acc: 0.8510638475418091)
[2025-01-02 00:40:59,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:59,927][root][INFO] - Training Epoch: 1/2, step 214/574 completed (loss: 1.0464251041412354, acc: 0.807692289352417)
[2025-01-02 00:41:00,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:00,249][root][INFO] - Training Epoch: 1/2, step 215/574 completed (loss: 0.722058892250061, acc: 0.8108108043670654)
[2025-01-02 00:41:00,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:00,583][root][INFO] - Training Epoch: 1/2, step 216/574 completed (loss: 0.8762537240982056, acc: 0.8372092843055725)
[2025-01-02 00:41:00,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:01,166][root][INFO] - Training Epoch: 1/2, step 217/574 completed (loss: 0.7745137810707092, acc: 0.837837815284729)
[2025-01-02 00:41:01,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:01,605][root][INFO] - Training Epoch: 1/2, step 218/574 completed (loss: 0.6508837938308716, acc: 0.8666666746139526)
[2025-01-02 00:41:01,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:01,960][root][INFO] - Training Epoch: 1/2, step 219/574 completed (loss: 0.6241779923439026, acc: 0.8484848737716675)
[2025-01-02 00:41:02,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:02,257][root][INFO] - Training Epoch: 1/2, step 220/574 completed (loss: 0.8419529795646667, acc: 0.7407407164573669)
[2025-01-02 00:41:02,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:02,585][root][INFO] - Training Epoch: 1/2, step 221/574 completed (loss: 0.4149685204029083, acc: 0.8799999952316284)
[2025-01-02 00:41:02,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:02,904][root][INFO] - Training Epoch: 1/2, step 222/574 completed (loss: 1.4953892230987549, acc: 0.6538461446762085)
[2025-01-02 00:41:03,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:03,689][root][INFO] - Training Epoch: 1/2, step 223/574 completed (loss: 1.245303750038147, acc: 0.7336956262588501)
[2025-01-02 00:41:03,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:04,247][root][INFO] - Training Epoch: 1/2, step 224/574 completed (loss: 1.2625855207443237, acc: 0.7102272510528564)
[2025-01-02 00:41:04,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:04,703][root][INFO] - Training Epoch: 1/2, step 225/574 completed (loss: 1.3112919330596924, acc: 0.6489361524581909)
[2025-01-02 00:41:04,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:05,075][root][INFO] - Training Epoch: 1/2, step 226/574 completed (loss: 1.9369306564331055, acc: 0.6037735939025879)
[2025-01-02 00:41:05,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:05,508][root][INFO] - Training Epoch: 1/2, step 227/574 completed (loss: 1.1363794803619385, acc: 0.6000000238418579)
[2025-01-02 00:41:05,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:05,913][root][INFO] - Training Epoch: 1/2, step 228/574 completed (loss: 1.4001473188400269, acc: 0.6279069781303406)
[2025-01-02 00:41:06,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:06,302][root][INFO] - Training Epoch: 1/2, step 229/574 completed (loss: 2.627236843109131, acc: 0.4333333373069763)
[2025-01-02 00:41:06,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:06,704][root][INFO] - Training Epoch: 1/2, step 230/574 completed (loss: 2.9859490394592285, acc: 0.3052631616592407)
[2025-01-02 00:41:06,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:07,054][root][INFO] - Training Epoch: 1/2, step 231/574 completed (loss: 2.3620665073394775, acc: 0.41111111640930176)
[2025-01-02 00:41:07,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:07,505][root][INFO] - Training Epoch: 1/2, step 232/574 completed (loss: 2.2843105792999268, acc: 0.4555555582046509)
[2025-01-02 00:41:07,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:08,036][root][INFO] - Training Epoch: 1/2, step 233/574 completed (loss: 2.4693572521209717, acc: 0.4082568883895874)
[2025-01-02 00:41:08,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:08,510][root][INFO] - Training Epoch: 1/2, step 234/574 completed (loss: 2.4532127380371094, acc: 0.4384615421295166)
[2025-01-02 00:41:08,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:08,845][root][INFO] - Training Epoch: 1/2, step 235/574 completed (loss: 1.0207843780517578, acc: 0.7368420958518982)
[2025-01-02 00:41:08,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:09,162][root][INFO] - Training Epoch: 1/2, step 236/574 completed (loss: 1.0650547742843628, acc: 0.7083333134651184)
[2025-01-02 00:41:09,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:09,535][root][INFO] - Training Epoch: 1/2, step 237/574 completed (loss: 1.4459179639816284, acc: 0.5909090638160706)
[2025-01-02 00:41:09,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:09,935][root][INFO] - Training Epoch: 1/2, step 238/574 completed (loss: 1.5181602239608765, acc: 0.5925925970077515)
[2025-01-02 00:41:10,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:10,304][root][INFO] - Training Epoch: 1/2, step 239/574 completed (loss: 1.3962138891220093, acc: 0.6285714507102966)
[2025-01-02 00:41:10,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:10,696][root][INFO] - Training Epoch: 1/2, step 240/574 completed (loss: 1.6877745389938354, acc: 0.5909090638160706)
[2025-01-02 00:41:10,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:11,040][root][INFO] - Training Epoch: 1/2, step 241/574 completed (loss: 1.236732006072998, acc: 0.7045454382896423)
[2025-01-02 00:41:11,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:11,626][root][INFO] - Training Epoch: 1/2, step 242/574 completed (loss: 2.297238349914551, acc: 0.4354838728904724)
[2025-01-02 00:41:11,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:12,167][root][INFO] - Training Epoch: 1/2, step 243/574 completed (loss: 1.9893345832824707, acc: 0.47727271914482117)
[2025-01-02 00:41:12,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:12,468][root][INFO] - Training Epoch: 1/2, step 244/574 completed (loss: 0.6570329666137695, acc: 0.8571428656578064)
[2025-01-02 00:41:12,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:12,840][root][INFO] - Training Epoch: 1/2, step 245/574 completed (loss: 0.9229353666305542, acc: 0.7307692170143127)
[2025-01-02 00:41:12,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:13,205][root][INFO] - Training Epoch: 1/2, step 246/574 completed (loss: 0.6210612654685974, acc: 0.8064516186714172)
[2025-01-02 00:41:13,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:13,536][root][INFO] - Training Epoch: 1/2, step 247/574 completed (loss: 0.9196876287460327, acc: 0.6000000238418579)
[2025-01-02 00:41:13,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:13,927][root][INFO] - Training Epoch: 1/2, step 248/574 completed (loss: 1.427586317062378, acc: 0.7027027010917664)
[2025-01-02 00:41:14,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:14,279][root][INFO] - Training Epoch: 1/2, step 249/574 completed (loss: 0.8502328395843506, acc: 0.7837837934494019)
[2025-01-02 00:41:14,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:14,637][root][INFO] - Training Epoch: 1/2, step 250/574 completed (loss: 0.9125193953514099, acc: 0.837837815284729)
[2025-01-02 00:41:14,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:15,026][root][INFO] - Training Epoch: 1/2, step 251/574 completed (loss: 0.7753826379776001, acc: 0.8088235259056091)
[2025-01-02 00:41:15,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:15,392][root][INFO] - Training Epoch: 1/2, step 252/574 completed (loss: 0.7566378116607666, acc: 0.8048780560493469)
[2025-01-02 00:41:15,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:15,768][root][INFO] - Training Epoch: 1/2, step 253/574 completed (loss: 0.726594090461731, acc: 0.7599999904632568)
[2025-01-02 00:41:15,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:16,143][root][INFO] - Training Epoch: 1/2, step 254/574 completed (loss: 0.24227012693881989, acc: 0.9599999785423279)
[2025-01-02 00:41:16,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:16,517][root][INFO] - Training Epoch: 1/2, step 255/574 completed (loss: 0.7476482391357422, acc: 0.774193525314331)
[2025-01-02 00:41:16,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:16,888][root][INFO] - Training Epoch: 1/2, step 256/574 completed (loss: 0.8070595264434814, acc: 0.859649121761322)
[2025-01-02 00:41:17,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:17,254][root][INFO] - Training Epoch: 1/2, step 257/574 completed (loss: 0.5401820540428162, acc: 0.8571428656578064)
[2025-01-02 00:41:17,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:17,603][root][INFO] - Training Epoch: 1/2, step 258/574 completed (loss: 0.45181363821029663, acc: 0.8947368264198303)
[2025-01-02 00:41:17,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:18,172][root][INFO] - Training Epoch: 1/2, step 259/574 completed (loss: 0.8321058750152588, acc: 0.7641509175300598)
[2025-01-02 00:41:18,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:18,770][root][INFO] - Training Epoch: 1/2, step 260/574 completed (loss: 0.6876882314682007, acc: 0.7916666865348816)
[2025-01-02 00:41:18,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:19,169][root][INFO] - Training Epoch: 1/2, step 261/574 completed (loss: 0.823515772819519, acc: 0.7777777910232544)
[2025-01-02 00:41:19,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:19,556][root][INFO] - Training Epoch: 1/2, step 262/574 completed (loss: 1.3888685703277588, acc: 0.6451612710952759)
[2025-01-02 00:41:19,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:19,982][root][INFO] - Training Epoch: 1/2, step 263/574 completed (loss: 1.763527274131775, acc: 0.6133333444595337)
[2025-01-02 00:41:20,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:20,369][root][INFO] - Training Epoch: 1/2, step 264/574 completed (loss: 1.2386854887008667, acc: 0.625)
[2025-01-02 00:41:20,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:21,244][root][INFO] - Training Epoch: 1/2, step 265/574 completed (loss: 2.057518720626831, acc: 0.4399999976158142)
[2025-01-02 00:41:21,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:21,652][root][INFO] - Training Epoch: 1/2, step 266/574 completed (loss: 1.9850060939788818, acc: 0.550561785697937)
[2025-01-02 00:41:21,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:22,066][root][INFO] - Training Epoch: 1/2, step 267/574 completed (loss: 1.7892911434173584, acc: 0.4864864945411682)
[2025-01-02 00:41:22,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:22,552][root][INFO] - Training Epoch: 1/2, step 268/574 completed (loss: 1.3424876928329468, acc: 0.568965494632721)
[2025-01-02 00:41:22,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:22,877][root][INFO] - Training Epoch: 1/2, step 269/574 completed (loss: 0.46009567379951477, acc: 0.8636363744735718)
[2025-01-02 00:41:22,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:23,243][root][INFO] - Training Epoch: 1/2, step 270/574 completed (loss: 0.3801385760307312, acc: 0.8636363744735718)
[2025-01-02 00:41:23,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:23,541][root][INFO] - Training Epoch: 1/2, step 271/574 completed (loss: 0.539517343044281, acc: 0.875)
[2025-01-02 00:41:23,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:23,865][root][INFO] - Training Epoch: 1/2, step 272/574 completed (loss: 0.2677713632583618, acc: 0.9333333373069763)
[2025-01-02 00:41:23,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:24,264][root][INFO] - Training Epoch: 1/2, step 273/574 completed (loss: 0.6041055917739868, acc: 0.9166666865348816)
[2025-01-02 00:41:24,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:24,650][root][INFO] - Training Epoch: 1/2, step 274/574 completed (loss: 0.7101876735687256, acc: 0.78125)
[2025-01-02 00:41:24,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:25,030][root][INFO] - Training Epoch: 1/2, step 275/574 completed (loss: 0.8153854012489319, acc: 0.8666666746139526)
[2025-01-02 00:41:25,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:25,441][root][INFO] - Training Epoch: 1/2, step 276/574 completed (loss: 0.8267799615859985, acc: 0.7931034564971924)
[2025-01-02 00:41:25,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:25,807][root][INFO] - Training Epoch: 1/2, step 277/574 completed (loss: 0.5826603770256042, acc: 0.8399999737739563)
[2025-01-02 00:41:25,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:26,163][root][INFO] - Training Epoch: 1/2, step 278/574 completed (loss: 1.188112735748291, acc: 0.7446808218955994)
[2025-01-02 00:41:26,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:26,565][root][INFO] - Training Epoch: 1/2, step 279/574 completed (loss: 0.8831205368041992, acc: 0.7708333134651184)
[2025-01-02 00:41:26,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:26,934][root][INFO] - Training Epoch: 1/2, step 280/574 completed (loss: 0.5034599900245667, acc: 0.8636363744735718)
[2025-01-02 00:41:27,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:27,363][root][INFO] - Training Epoch: 1/2, step 281/574 completed (loss: 1.427101731300354, acc: 0.6265060305595398)
[2025-01-02 00:41:27,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:27,731][root][INFO] - Training Epoch: 1/2, step 282/574 completed (loss: 1.5384513139724731, acc: 0.6018518805503845)
[2025-01-02 00:41:27,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:28,117][root][INFO] - Training Epoch: 1/2, step 283/574 completed (loss: 0.47995051741600037, acc: 0.8684210777282715)
[2025-01-02 00:41:28,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:28,487][root][INFO] - Training Epoch: 1/2, step 284/574 completed (loss: 0.8291048407554626, acc: 0.6764705777168274)
[2025-01-02 00:41:28,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:28,871][root][INFO] - Training Epoch: 1/2, step 285/574 completed (loss: 0.5241010785102844, acc: 0.8999999761581421)
[2025-01-02 00:41:29,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:29,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:30,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:30,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:30,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:31,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:31,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:31,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:32,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:32,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:33,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:33,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:33,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:34,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:34,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:34,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:35,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:35,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:35,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:36,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:36,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:36,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:37,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:37,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:37,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:38,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:38,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:39,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:39,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:39,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:39,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:40,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:40,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:41,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:41,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:41,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:42,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:42,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:42,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:42,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:43,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:43,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:43,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:44,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:44,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:44,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:45,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:45,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:45,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:46,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:46,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:46,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:46,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:47,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:47,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:47,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:48,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:48,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:48,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:49,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:49,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:49,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:50,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:50,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:50,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:51,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:51,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:51,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:52,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:52,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:53,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:53,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:53,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:54,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:54,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:54,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:55,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:55,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:55,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:55,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:56,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:56,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:56,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:57,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:57,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:58,212][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4781, device='cuda:0') eval_epoch_loss=tensor(0.9075, device='cuda:0') eval_epoch_acc=tensor(0.7641, device='cuda:0')
[2025-01-02 00:41:58,213][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:41:58,213][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:41:58,456][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_286_loss_0.9074847102165222/model.pt
[2025-01-02 00:41:58,460][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:41:58,461][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.9074847102165222
[2025-01-02 00:41:58,461][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7641414403915405
[2025-01-02 00:41:58,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:58,809][root][INFO] - Training Epoch: 1/2, step 286/574 completed (loss: 0.8005144596099854, acc: 0.8046875)
[2025-01-02 00:41:58,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:59,128][root][INFO] - Training Epoch: 1/2, step 287/574 completed (loss: 1.2074525356292725, acc: 0.6880000233650208)
[2025-01-02 00:41:59,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:59,456][root][INFO] - Training Epoch: 1/2, step 288/574 completed (loss: 1.182656168937683, acc: 0.7472527623176575)
[2025-01-02 00:41:59,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:59,816][root][INFO] - Training Epoch: 1/2, step 289/574 completed (loss: 1.168505311012268, acc: 0.7453415989875793)
[2025-01-02 00:41:59,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:00,221][root][INFO] - Training Epoch: 1/2, step 290/574 completed (loss: 1.132481336593628, acc: 0.7474226951599121)
[2025-01-02 00:42:00,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:00,564][root][INFO] - Training Epoch: 1/2, step 291/574 completed (loss: 0.6021957993507385, acc: 0.7727272510528564)
[2025-01-02 00:42:00,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:00,863][root][INFO] - Training Epoch: 1/2, step 292/574 completed (loss: 1.284606695175171, acc: 0.6428571343421936)
[2025-01-02 00:42:01,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:01,232][root][INFO] - Training Epoch: 1/2, step 293/574 completed (loss: 0.9799959659576416, acc: 0.7758620977401733)
[2025-01-02 00:42:01,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:01,737][root][INFO] - Training Epoch: 1/2, step 294/574 completed (loss: 0.900489866733551, acc: 0.6909090876579285)
[2025-01-02 00:42:01,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:02,320][root][INFO] - Training Epoch: 1/2, step 295/574 completed (loss: 0.9168203473091125, acc: 0.7525773048400879)
[2025-01-02 00:42:02,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:02,695][root][INFO] - Training Epoch: 1/2, step 296/574 completed (loss: 1.0030148029327393, acc: 0.7413793206214905)
[2025-01-02 00:42:02,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:03,072][root][INFO] - Training Epoch: 1/2, step 297/574 completed (loss: 0.6173954606056213, acc: 0.8148148059844971)
[2025-01-02 00:42:03,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:03,475][root][INFO] - Training Epoch: 1/2, step 298/574 completed (loss: 1.1562235355377197, acc: 0.6842105388641357)
[2025-01-02 00:42:03,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:03,831][root][INFO] - Training Epoch: 1/2, step 299/574 completed (loss: 0.45966798067092896, acc: 0.875)
[2025-01-02 00:42:03,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:04,226][root][INFO] - Training Epoch: 1/2, step 300/574 completed (loss: 0.3240152895450592, acc: 0.875)
[2025-01-02 00:42:04,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:04,577][root][INFO] - Training Epoch: 1/2, step 301/574 completed (loss: 0.8151790499687195, acc: 0.7735849022865295)
[2025-01-02 00:42:04,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:04,898][root][INFO] - Training Epoch: 1/2, step 302/574 completed (loss: 0.44566676020622253, acc: 0.9056603908538818)
[2025-01-02 00:42:05,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:05,245][root][INFO] - Training Epoch: 1/2, step 303/574 completed (loss: 0.2540920376777649, acc: 0.9117646813392639)
[2025-01-02 00:42:05,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:05,570][root][INFO] - Training Epoch: 1/2, step 304/574 completed (loss: 0.6150187253952026, acc: 0.75)
[2025-01-02 00:42:05,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:05,902][root][INFO] - Training Epoch: 1/2, step 305/574 completed (loss: 0.9109933972358704, acc: 0.7540983557701111)
[2025-01-02 00:42:06,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:06,225][root][INFO] - Training Epoch: 1/2, step 306/574 completed (loss: 0.9612130522727966, acc: 0.800000011920929)
[2025-01-02 00:42:06,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:06,570][root][INFO] - Training Epoch: 1/2, step 307/574 completed (loss: 0.4966517984867096, acc: 0.9473684430122375)
[2025-01-02 00:42:06,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:06,892][root][INFO] - Training Epoch: 1/2, step 308/574 completed (loss: 0.8949815630912781, acc: 0.7246376872062683)
[2025-01-02 00:42:07,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:07,349][root][INFO] - Training Epoch: 1/2, step 309/574 completed (loss: 0.955723762512207, acc: 0.75)
[2025-01-02 00:42:07,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:07,685][root][INFO] - Training Epoch: 1/2, step 310/574 completed (loss: 0.9806077480316162, acc: 0.7228915691375732)
[2025-01-02 00:42:07,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:07,992][root][INFO] - Training Epoch: 1/2, step 311/574 completed (loss: 0.8520811796188354, acc: 0.7564102411270142)
[2025-01-02 00:42:08,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:08,354][root][INFO] - Training Epoch: 1/2, step 312/574 completed (loss: 0.4738319218158722, acc: 0.8877550959587097)
[2025-01-02 00:42:08,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:08,659][root][INFO] - Training Epoch: 1/2, step 313/574 completed (loss: 0.27475616335868835, acc: 0.8333333134651184)
[2025-01-02 00:42:08,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:08,961][root][INFO] - Training Epoch: 1/2, step 314/574 completed (loss: 0.29481959342956543, acc: 0.8333333134651184)
[2025-01-02 00:42:09,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:09,288][root][INFO] - Training Epoch: 1/2, step 315/574 completed (loss: 0.41185203194618225, acc: 0.9032257795333862)
[2025-01-02 00:42:09,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:09,650][root][INFO] - Training Epoch: 1/2, step 316/574 completed (loss: 1.513351321220398, acc: 0.7096773982048035)
[2025-01-02 00:42:09,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:09,975][root][INFO] - Training Epoch: 1/2, step 317/574 completed (loss: 0.7793877124786377, acc: 0.8358209133148193)
[2025-01-02 00:42:10,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:10,303][root][INFO] - Training Epoch: 1/2, step 318/574 completed (loss: 0.2858263850212097, acc: 0.932692289352417)
[2025-01-02 00:42:10,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:10,635][root][INFO] - Training Epoch: 1/2, step 319/574 completed (loss: 0.4514736533164978, acc: 0.8444444537162781)
[2025-01-02 00:42:10,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:10,995][root][INFO] - Training Epoch: 1/2, step 320/574 completed (loss: 0.41369956731796265, acc: 0.9032257795333862)
[2025-01-02 00:42:11,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:11,316][root][INFO] - Training Epoch: 1/2, step 321/574 completed (loss: 0.35343509912490845, acc: 0.9399999976158142)
[2025-01-02 00:42:11,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:11,640][root][INFO] - Training Epoch: 1/2, step 322/574 completed (loss: 1.908878207206726, acc: 0.48148149251937866)
[2025-01-02 00:42:11,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:11,985][root][INFO] - Training Epoch: 1/2, step 323/574 completed (loss: 2.4788553714752197, acc: 0.37142857909202576)
[2025-01-02 00:42:12,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:12,320][root][INFO] - Training Epoch: 1/2, step 324/574 completed (loss: 2.5572173595428467, acc: 0.4615384638309479)
[2025-01-02 00:42:12,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:12,645][root][INFO] - Training Epoch: 1/2, step 325/574 completed (loss: 2.0950887203216553, acc: 0.46341463923454285)
[2025-01-02 00:42:12,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:12,946][root][INFO] - Training Epoch: 1/2, step 326/574 completed (loss: 2.217607021331787, acc: 0.3947368562221527)
[2025-01-02 00:42:13,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:13,249][root][INFO] - Training Epoch: 1/2, step 327/574 completed (loss: 0.9925418496131897, acc: 0.7368420958518982)
[2025-01-02 00:42:13,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:13,575][root][INFO] - Training Epoch: 1/2, step 328/574 completed (loss: 0.4225345551967621, acc: 0.8928571343421936)
[2025-01-02 00:42:13,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:13,907][root][INFO] - Training Epoch: 1/2, step 329/574 completed (loss: 0.494242399930954, acc: 0.8888888955116272)
[2025-01-02 00:42:14,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:14,241][root][INFO] - Training Epoch: 1/2, step 330/574 completed (loss: 0.2772162854671478, acc: 0.90625)
[2025-01-02 00:42:14,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:14,604][root][INFO] - Training Epoch: 1/2, step 331/574 completed (loss: 0.5302110314369202, acc: 0.8709677457809448)
[2025-01-02 00:42:14,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:14,998][root][INFO] - Training Epoch: 1/2, step 332/574 completed (loss: 0.5551416277885437, acc: 0.8771929740905762)
[2025-01-02 00:42:15,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:15,331][root][INFO] - Training Epoch: 1/2, step 333/574 completed (loss: 0.725619912147522, acc: 0.78125)
[2025-01-02 00:42:15,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:15,662][root][INFO] - Training Epoch: 1/2, step 334/574 completed (loss: 0.4008267819881439, acc: 0.8999999761581421)
[2025-01-02 00:42:15,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:15,985][root][INFO] - Training Epoch: 1/2, step 335/574 completed (loss: 0.9926006197929382, acc: 0.7368420958518982)
[2025-01-02 00:42:16,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:16,360][root][INFO] - Training Epoch: 1/2, step 336/574 completed (loss: 1.3239264488220215, acc: 0.6399999856948853)
[2025-01-02 00:42:16,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:16,689][root][INFO] - Training Epoch: 1/2, step 337/574 completed (loss: 1.7606555223464966, acc: 0.5632184147834778)
[2025-01-02 00:42:16,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:17,019][root][INFO] - Training Epoch: 1/2, step 338/574 completed (loss: 1.781680941581726, acc: 0.5106382966041565)
[2025-01-02 00:42:17,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:17,348][root][INFO] - Training Epoch: 1/2, step 339/574 completed (loss: 1.7319209575653076, acc: 0.5542168617248535)
[2025-01-02 00:42:17,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:17,672][root][INFO] - Training Epoch: 1/2, step 340/574 completed (loss: 0.4994173049926758, acc: 0.8260869383811951)
[2025-01-02 00:42:17,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:17,982][root][INFO] - Training Epoch: 1/2, step 341/574 completed (loss: 1.0782265663146973, acc: 0.7948718070983887)
[2025-01-02 00:42:18,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:18,319][root][INFO] - Training Epoch: 1/2, step 342/574 completed (loss: 0.9577940702438354, acc: 0.7228915691375732)
[2025-01-02 00:42:18,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:18,644][root][INFO] - Training Epoch: 1/2, step 343/574 completed (loss: 1.107060194015503, acc: 0.698113203048706)
[2025-01-02 00:42:18,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:19,015][root][INFO] - Training Epoch: 1/2, step 344/574 completed (loss: 0.40772533416748047, acc: 0.8860759735107422)
[2025-01-02 00:42:19,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:19,338][root][INFO] - Training Epoch: 1/2, step 345/574 completed (loss: 0.2990310788154602, acc: 0.9215686321258545)
[2025-01-02 00:42:19,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:19,682][root][INFO] - Training Epoch: 1/2, step 346/574 completed (loss: 1.2377433776855469, acc: 0.6865671873092651)
[2025-01-02 00:42:19,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:20,011][root][INFO] - Training Epoch: 1/2, step 347/574 completed (loss: 0.4397578835487366, acc: 0.8999999761581421)
[2025-01-02 00:42:20,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:20,337][root][INFO] - Training Epoch: 1/2, step 348/574 completed (loss: 1.0745346546173096, acc: 0.7200000286102295)
[2025-01-02 00:42:20,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:20,779][root][INFO] - Training Epoch: 1/2, step 349/574 completed (loss: 1.332379698753357, acc: 0.7222222089767456)
[2025-01-02 00:42:20,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:21,102][root][INFO] - Training Epoch: 1/2, step 350/574 completed (loss: 1.5012632608413696, acc: 0.5116279125213623)
[2025-01-02 00:42:21,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:21,451][root][INFO] - Training Epoch: 1/2, step 351/574 completed (loss: 0.9802061319351196, acc: 0.7179487347602844)
[2025-01-02 00:42:21,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:21,837][root][INFO] - Training Epoch: 1/2, step 352/574 completed (loss: 2.3192999362945557, acc: 0.3777777850627899)
[2025-01-02 00:42:21,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:22,152][root][INFO] - Training Epoch: 1/2, step 353/574 completed (loss: 0.3310868740081787, acc: 0.95652174949646)
[2025-01-02 00:42:22,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:22,466][root][INFO] - Training Epoch: 1/2, step 354/574 completed (loss: 1.0462113618850708, acc: 0.7307692170143127)
[2025-01-02 00:42:22,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:22,823][root][INFO] - Training Epoch: 1/2, step 355/574 completed (loss: 1.4438124895095825, acc: 0.5824176073074341)
[2025-01-02 00:42:22,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:23,345][root][INFO] - Training Epoch: 1/2, step 356/574 completed (loss: 1.3042254447937012, acc: 0.626086950302124)
[2025-01-02 00:42:23,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:23,684][root][INFO] - Training Epoch: 1/2, step 357/574 completed (loss: 1.1986453533172607, acc: 0.6521739363670349)
[2025-01-02 00:42:23,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:24,014][root][INFO] - Training Epoch: 1/2, step 358/574 completed (loss: 1.0386818647384644, acc: 0.6938775777816772)
[2025-01-02 00:42:24,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:24,345][root][INFO] - Training Epoch: 1/2, step 359/574 completed (loss: 0.1598534733057022, acc: 0.9583333134651184)
[2025-01-02 00:42:24,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:24,658][root][INFO] - Training Epoch: 1/2, step 360/574 completed (loss: 0.6345915198326111, acc: 0.7692307829856873)
[2025-01-02 00:42:24,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:24,988][root][INFO] - Training Epoch: 1/2, step 361/574 completed (loss: 1.1907578706741333, acc: 0.6585366129875183)
[2025-01-02 00:42:25,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:25,323][root][INFO] - Training Epoch: 1/2, step 362/574 completed (loss: 0.8904244303703308, acc: 0.8444444537162781)
[2025-01-02 00:42:25,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:25,677][root][INFO] - Training Epoch: 1/2, step 363/574 completed (loss: 0.6551095843315125, acc: 0.8421052694320679)
[2025-01-02 00:42:25,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:26,024][root][INFO] - Training Epoch: 1/2, step 364/574 completed (loss: 0.45005717873573303, acc: 0.9024389982223511)
[2025-01-02 00:42:26,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:26,362][root][INFO] - Training Epoch: 1/2, step 365/574 completed (loss: 0.6369144320487976, acc: 0.7575757503509521)
[2025-01-02 00:42:26,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:26,711][root][INFO] - Training Epoch: 1/2, step 366/574 completed (loss: 0.1336660236120224, acc: 0.9583333134651184)
[2025-01-02 00:42:26,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:27,068][root][INFO] - Training Epoch: 1/2, step 367/574 completed (loss: 0.7411879897117615, acc: 0.739130437374115)
[2025-01-02 00:42:27,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:27,390][root][INFO] - Training Epoch: 1/2, step 368/574 completed (loss: 0.42222243547439575, acc: 0.9285714030265808)
[2025-01-02 00:42:27,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:27,717][root][INFO] - Training Epoch: 1/2, step 369/574 completed (loss: 1.1520321369171143, acc: 0.78125)
[2025-01-02 00:42:27,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:28,354][root][INFO] - Training Epoch: 1/2, step 370/574 completed (loss: 1.2339346408843994, acc: 0.6424242258071899)
[2025-01-02 00:42:28,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:29,283][root][INFO] - Training Epoch: 1/2, step 371/574 completed (loss: 0.9755131006240845, acc: 0.7547169923782349)
[2025-01-02 00:42:29,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:29,647][root][INFO] - Training Epoch: 1/2, step 372/574 completed (loss: 0.4555671811103821, acc: 0.8999999761581421)
[2025-01-02 00:42:29,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:29,966][root][INFO] - Training Epoch: 1/2, step 373/574 completed (loss: 0.674355685710907, acc: 0.8928571343421936)
[2025-01-02 00:42:30,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:30,309][root][INFO] - Training Epoch: 1/2, step 374/574 completed (loss: 0.7297677993774414, acc: 0.8571428656578064)
[2025-01-02 00:42:30,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:30,624][root][INFO] - Training Epoch: 1/2, step 375/574 completed (loss: 0.06856150925159454, acc: 1.0)
[2025-01-02 00:42:30,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:30,972][root][INFO] - Training Epoch: 1/2, step 376/574 completed (loss: 0.3051396608352661, acc: 0.8695651888847351)
[2025-01-02 00:42:31,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:31,261][root][INFO] - Training Epoch: 1/2, step 377/574 completed (loss: 0.5195842385292053, acc: 0.8958333134651184)
[2025-01-02 00:42:31,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:31,630][root][INFO] - Training Epoch: 1/2, step 378/574 completed (loss: 0.35453152656555176, acc: 0.9052631855010986)
[2025-01-02 00:42:31,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:32,229][root][INFO] - Training Epoch: 1/2, step 379/574 completed (loss: 0.6176060438156128, acc: 0.8502994179725647)
[2025-01-02 00:42:32,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:32,651][root][INFO] - Training Epoch: 1/2, step 380/574 completed (loss: 0.7490099668502808, acc: 0.8045112490653992)
[2025-01-02 00:42:33,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:33,922][root][INFO] - Training Epoch: 1/2, step 381/574 completed (loss: 1.1628879308700562, acc: 0.6951871514320374)
[2025-01-02 00:42:34,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:34,481][root][INFO] - Training Epoch: 1/2, step 382/574 completed (loss: 0.5626320838928223, acc: 0.8738738894462585)
[2025-01-02 00:42:34,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:34,897][root][INFO] - Training Epoch: 1/2, step 383/574 completed (loss: 0.9197145104408264, acc: 0.7857142686843872)
[2025-01-02 00:42:35,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:35,204][root][INFO] - Training Epoch: 1/2, step 384/574 completed (loss: 0.13058039546012878, acc: 1.0)
[2025-01-02 00:42:35,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:35,533][root][INFO] - Training Epoch: 1/2, step 385/574 completed (loss: 0.39230021834373474, acc: 0.90625)
[2025-01-02 00:42:35,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:35,885][root][INFO] - Training Epoch: 1/2, step 386/574 completed (loss: 0.34171852469444275, acc: 0.9444444179534912)
[2025-01-02 00:42:35,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:36,201][root][INFO] - Training Epoch: 1/2, step 387/574 completed (loss: 0.1385039985179901, acc: 0.9736841917037964)
[2025-01-02 00:42:36,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:36,544][root][INFO] - Training Epoch: 1/2, step 388/574 completed (loss: 0.22423739731311798, acc: 0.9090909361839294)
[2025-01-02 00:42:36,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:36,875][root][INFO] - Training Epoch: 1/2, step 389/574 completed (loss: 0.16927513480186462, acc: 0.949999988079071)
[2025-01-02 00:42:36,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:37,196][root][INFO] - Training Epoch: 1/2, step 390/574 completed (loss: 0.9747900366783142, acc: 0.8095238208770752)
[2025-01-02 00:42:37,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:37,540][root][INFO] - Training Epoch: 1/2, step 391/574 completed (loss: 1.4042572975158691, acc: 0.5555555820465088)
[2025-01-02 00:42:37,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:38,042][root][INFO] - Training Epoch: 1/2, step 392/574 completed (loss: 1.3455485105514526, acc: 0.6796116232872009)
[2025-01-02 00:42:38,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:38,632][root][INFO] - Training Epoch: 1/2, step 393/574 completed (loss: 1.3183287382125854, acc: 0.7132353186607361)
[2025-01-02 00:42:38,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:39,052][root][INFO] - Training Epoch: 1/2, step 394/574 completed (loss: 1.4589347839355469, acc: 0.6200000047683716)
[2025-01-02 00:42:39,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:39,503][root][INFO] - Training Epoch: 1/2, step 395/574 completed (loss: 1.242250680923462, acc: 0.6666666865348816)
[2025-01-02 00:42:39,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:39,932][root][INFO] - Training Epoch: 1/2, step 396/574 completed (loss: 0.8797982931137085, acc: 0.7674418687820435)
[2025-01-02 00:42:40,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:40,320][root][INFO] - Training Epoch: 1/2, step 397/574 completed (loss: 0.5163269639015198, acc: 0.875)
[2025-01-02 00:42:40,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:40,711][root][INFO] - Training Epoch: 1/2, step 398/574 completed (loss: 0.6819582581520081, acc: 0.7674418687820435)
[2025-01-02 00:42:40,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:41,039][root][INFO] - Training Epoch: 1/2, step 399/574 completed (loss: 0.47976425290107727, acc: 0.8799999952316284)
[2025-01-02 00:42:41,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:41,659][root][INFO] - Training Epoch: 1/2, step 400/574 completed (loss: 0.6823372840881348, acc: 0.7941176295280457)
[2025-01-02 00:42:41,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:42,066][root][INFO] - Training Epoch: 1/2, step 401/574 completed (loss: 0.6368151903152466, acc: 0.800000011920929)
[2025-01-02 00:42:42,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:42,424][root][INFO] - Training Epoch: 1/2, step 402/574 completed (loss: 1.2284868955612183, acc: 0.6969696879386902)
[2025-01-02 00:42:42,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:42,712][root][INFO] - Training Epoch: 1/2, step 403/574 completed (loss: 0.5938553214073181, acc: 0.7575757503509521)
[2025-01-02 00:42:42,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:43,044][root][INFO] - Training Epoch: 1/2, step 404/574 completed (loss: 1.0687816143035889, acc: 0.7419354915618896)
[2025-01-02 00:42:43,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:43,346][root][INFO] - Training Epoch: 1/2, step 405/574 completed (loss: 0.2773459851741791, acc: 0.8518518805503845)
[2025-01-02 00:42:43,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:43,612][root][INFO] - Training Epoch: 1/2, step 406/574 completed (loss: 0.5483890771865845, acc: 0.8399999737739563)
[2025-01-02 00:42:43,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:43,954][root][INFO] - Training Epoch: 1/2, step 407/574 completed (loss: 0.32297712564468384, acc: 0.8888888955116272)
[2025-01-02 00:42:44,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:44,324][root][INFO] - Training Epoch: 1/2, step 408/574 completed (loss: 0.4743301272392273, acc: 0.8888888955116272)
[2025-01-02 00:42:44,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:44,688][root][INFO] - Training Epoch: 1/2, step 409/574 completed (loss: 0.4568483531475067, acc: 0.8846153616905212)
[2025-01-02 00:42:44,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:45,080][root][INFO] - Training Epoch: 1/2, step 410/574 completed (loss: 0.6482757329940796, acc: 0.8620689511299133)
[2025-01-02 00:42:45,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:45,440][root][INFO] - Training Epoch: 1/2, step 411/574 completed (loss: 0.15669049322605133, acc: 1.0)
[2025-01-02 00:42:45,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:45,778][root][INFO] - Training Epoch: 1/2, step 412/574 completed (loss: 0.42868563532829285, acc: 0.8666666746139526)
[2025-01-02 00:42:45,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:46,183][root][INFO] - Training Epoch: 1/2, step 413/574 completed (loss: 0.6229144334793091, acc: 0.8484848737716675)
[2025-01-02 00:42:46,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:46,556][root][INFO] - Training Epoch: 1/2, step 414/574 completed (loss: 0.6508092284202576, acc: 0.8636363744735718)
[2025-01-02 00:42:46,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:46,982][root][INFO] - Training Epoch: 1/2, step 415/574 completed (loss: 0.7787695527076721, acc: 0.7647058963775635)
[2025-01-02 00:42:47,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:47,398][root][INFO] - Training Epoch: 1/2, step 416/574 completed (loss: 0.6841326355934143, acc: 0.807692289352417)
[2025-01-02 00:42:47,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:47,745][root][INFO] - Training Epoch: 1/2, step 417/574 completed (loss: 0.781464695930481, acc: 0.7777777910232544)
[2025-01-02 00:42:47,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:48,128][root][INFO] - Training Epoch: 1/2, step 418/574 completed (loss: 0.7042449712753296, acc: 0.800000011920929)
[2025-01-02 00:42:48,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:48,524][root][INFO] - Training Epoch: 1/2, step 419/574 completed (loss: 0.9401046633720398, acc: 0.8500000238418579)
[2025-01-02 00:42:48,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:48,906][root][INFO] - Training Epoch: 1/2, step 420/574 completed (loss: 0.4138031005859375, acc: 0.8571428656578064)
[2025-01-02 00:42:49,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:49,297][root][INFO] - Training Epoch: 1/2, step 421/574 completed (loss: 0.636038064956665, acc: 0.8333333134651184)
[2025-01-02 00:42:49,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:49,673][root][INFO] - Training Epoch: 1/2, step 422/574 completed (loss: 0.7556023597717285, acc: 0.75)
[2025-01-02 00:42:49,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:50,017][root][INFO] - Training Epoch: 1/2, step 423/574 completed (loss: 1.1674339771270752, acc: 0.7222222089767456)
[2025-01-02 00:42:50,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:50,319][root][INFO] - Training Epoch: 1/2, step 424/574 completed (loss: 0.9309764504432678, acc: 0.8888888955116272)
[2025-01-02 00:42:50,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:50,700][root][INFO] - Training Epoch: 1/2, step 425/574 completed (loss: 0.5981173515319824, acc: 0.9090909361839294)
[2025-01-02 00:42:50,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:51,064][root][INFO] - Training Epoch: 1/2, step 426/574 completed (loss: 0.30733224749565125, acc: 0.8695651888847351)
[2025-01-02 00:42:51,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:51,421][root][INFO] - Training Epoch: 1/2, step 427/574 completed (loss: 0.45895782113075256, acc: 0.8918918967247009)
[2025-01-02 00:42:51,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:51,770][root][INFO] - Training Epoch: 1/2, step 428/574 completed (loss: 0.5391839146614075, acc: 0.8888888955116272)
[2025-01-02 00:42:52,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:52,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:53,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:53,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:53,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:53,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:54,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:54,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:54,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:55,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:55,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:55,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:56,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:56,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:56,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:57,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:57,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:57,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:58,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:58,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:58,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:59,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:59,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:59,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:59,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:00,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:00,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:00,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:01,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:01,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:02,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:02,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:02,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:03,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:03,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:03,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:04,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:04,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:04,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:05,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:05,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:05,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:05,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:06,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:06,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:06,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:07,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:07,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:07,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:08,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:08,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:08,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:09,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:09,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:09,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:10,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:10,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:10,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:10,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:11,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:11,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:12,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:12,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:12,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:13,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:13,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:13,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:14,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:14,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:15,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:15,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:15,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:16,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:16,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:16,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:17,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:17,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:17,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:18,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:18,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:18,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:18,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:19,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:19,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:19,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:20,383][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2726, device='cuda:0') eval_epoch_loss=tensor(0.8209, device='cuda:0') eval_epoch_acc=tensor(0.7820, device='cuda:0')
[2025-01-02 00:43:20,384][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:43:20,385][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:43:20,684][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_429_loss_0.8209146857261658/model.pt
[2025-01-02 00:43:20,692][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:43:20,693][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.8209146857261658
[2025-01-02 00:43:20,693][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7819832563400269
[2025-01-02 00:43:20,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:21,058][root][INFO] - Training Epoch: 1/2, step 429/574 completed (loss: 0.6097924113273621, acc: 0.8695651888847351)
[2025-01-02 00:43:21,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:21,373][root][INFO] - Training Epoch: 1/2, step 430/574 completed (loss: 0.05837055668234825, acc: 1.0)
[2025-01-02 00:43:21,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:21,689][root][INFO] - Training Epoch: 1/2, step 431/574 completed (loss: 0.30894240736961365, acc: 0.9259259104728699)
[2025-01-02 00:43:21,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:21,982][root][INFO] - Training Epoch: 1/2, step 432/574 completed (loss: 0.9460218548774719, acc: 0.782608687877655)
[2025-01-02 00:43:22,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:22,361][root][INFO] - Training Epoch: 1/2, step 433/574 completed (loss: 0.49343428015708923, acc: 0.8888888955116272)
[2025-01-02 00:43:22,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:22,725][root][INFO] - Training Epoch: 1/2, step 434/574 completed (loss: 0.013566466979682446, acc: 1.0)
[2025-01-02 00:43:22,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:23,084][root][INFO] - Training Epoch: 1/2, step 435/574 completed (loss: 0.19751609861850739, acc: 0.939393937587738)
[2025-01-02 00:43:23,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:23,426][root][INFO] - Training Epoch: 1/2, step 436/574 completed (loss: 0.5692617297172546, acc: 0.8611111044883728)
[2025-01-02 00:43:23,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:23,768][root][INFO] - Training Epoch: 1/2, step 437/574 completed (loss: 0.4383975863456726, acc: 0.8863636255264282)
[2025-01-02 00:43:23,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:24,067][root][INFO] - Training Epoch: 1/2, step 438/574 completed (loss: 0.20260637998580933, acc: 0.9523809552192688)
[2025-01-02 00:43:24,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:24,394][root][INFO] - Training Epoch: 1/2, step 439/574 completed (loss: 0.8319169878959656, acc: 0.8205128312110901)
[2025-01-02 00:43:24,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:24,938][root][INFO] - Training Epoch: 1/2, step 440/574 completed (loss: 0.9786441922187805, acc: 0.6969696879386902)
[2025-01-02 00:43:25,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:25,711][root][INFO] - Training Epoch: 1/2, step 441/574 completed (loss: 1.3853442668914795, acc: 0.6159999966621399)
[2025-01-02 00:43:25,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:26,147][root][INFO] - Training Epoch: 1/2, step 442/574 completed (loss: 1.2926418781280518, acc: 0.6451612710952759)
[2025-01-02 00:43:26,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:26,811][root][INFO] - Training Epoch: 1/2, step 443/574 completed (loss: 0.9762853980064392, acc: 0.7910447716712952)
[2025-01-02 00:43:26,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:27,142][root][INFO] - Training Epoch: 1/2, step 444/574 completed (loss: 0.6217185258865356, acc: 0.7924528121948242)
[2025-01-02 00:43:27,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:27,595][root][INFO] - Training Epoch: 1/2, step 445/574 completed (loss: 0.5872686505317688, acc: 0.8636363744735718)
[2025-01-02 00:43:27,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:27,931][root][INFO] - Training Epoch: 1/2, step 446/574 completed (loss: 0.7223207354545593, acc: 0.8260869383811951)
[2025-01-02 00:43:28,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:28,239][root][INFO] - Training Epoch: 1/2, step 447/574 completed (loss: 1.161546230316162, acc: 0.7692307829856873)
[2025-01-02 00:43:28,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:28,549][root][INFO] - Training Epoch: 1/2, step 448/574 completed (loss: 0.38916832208633423, acc: 0.9285714030265808)
[2025-01-02 00:43:28,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:28,874][root][INFO] - Training Epoch: 1/2, step 449/574 completed (loss: 0.5996367931365967, acc: 0.8656716346740723)
[2025-01-02 00:43:28,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:29,203][root][INFO] - Training Epoch: 1/2, step 450/574 completed (loss: 0.28945234417915344, acc: 0.9305555820465088)
[2025-01-02 00:43:29,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:29,539][root][INFO] - Training Epoch: 1/2, step 451/574 completed (loss: 0.2510492205619812, acc: 0.9347826242446899)
[2025-01-02 00:43:29,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:29,876][root][INFO] - Training Epoch: 1/2, step 452/574 completed (loss: 0.5720747113227844, acc: 0.8333333134651184)
[2025-01-02 00:43:29,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:30,212][root][INFO] - Training Epoch: 1/2, step 453/574 completed (loss: 0.9047011137008667, acc: 0.8421052694320679)
[2025-01-02 00:43:30,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:30,547][root][INFO] - Training Epoch: 1/2, step 454/574 completed (loss: 0.5115208029747009, acc: 0.8571428656578064)
[2025-01-02 00:43:30,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:30,870][root][INFO] - Training Epoch: 1/2, step 455/574 completed (loss: 0.5470913052558899, acc: 0.8787878751754761)
[2025-01-02 00:43:30,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:31,211][root][INFO] - Training Epoch: 1/2, step 456/574 completed (loss: 0.7634338736534119, acc: 0.8247422575950623)
[2025-01-02 00:43:31,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:31,551][root][INFO] - Training Epoch: 1/2, step 457/574 completed (loss: 0.2791040539741516, acc: 0.9428571462631226)
[2025-01-02 00:43:31,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:31,959][root][INFO] - Training Epoch: 1/2, step 458/574 completed (loss: 0.9676839113235474, acc: 0.7558139562606812)
[2025-01-02 00:43:32,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:32,276][root][INFO] - Training Epoch: 1/2, step 459/574 completed (loss: 0.21790345013141632, acc: 0.9464285969734192)
[2025-01-02 00:43:32,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:32,604][root][INFO] - Training Epoch: 1/2, step 460/574 completed (loss: 0.5495805144309998, acc: 0.8395061492919922)
[2025-01-02 00:43:32,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:32,935][root][INFO] - Training Epoch: 1/2, step 461/574 completed (loss: 0.9010792970657349, acc: 0.7222222089767456)
[2025-01-02 00:43:33,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:33,240][root][INFO] - Training Epoch: 1/2, step 462/574 completed (loss: 0.4345313608646393, acc: 0.875)
[2025-01-02 00:43:33,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:33,615][root][INFO] - Training Epoch: 1/2, step 463/574 completed (loss: 0.5494575500488281, acc: 0.8846153616905212)
[2025-01-02 00:43:33,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:33,960][root][INFO] - Training Epoch: 1/2, step 464/574 completed (loss: 0.5392590165138245, acc: 0.782608687877655)
[2025-01-02 00:43:34,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:34,300][root][INFO] - Training Epoch: 1/2, step 465/574 completed (loss: 0.7298398613929749, acc: 0.773809552192688)
[2025-01-02 00:43:34,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:34,640][root][INFO] - Training Epoch: 1/2, step 466/574 completed (loss: 0.8615332245826721, acc: 0.7831325531005859)
[2025-01-02 00:43:34,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:34,997][root][INFO] - Training Epoch: 1/2, step 467/574 completed (loss: 0.5326200723648071, acc: 0.8648648858070374)
[2025-01-02 00:43:35,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:35,348][root][INFO] - Training Epoch: 1/2, step 468/574 completed (loss: 1.2524218559265137, acc: 0.708737850189209)
[2025-01-02 00:43:35,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:35,692][root][INFO] - Training Epoch: 1/2, step 469/574 completed (loss: 0.8584423661231995, acc: 0.7642276287078857)
[2025-01-02 00:43:35,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:36,040][root][INFO] - Training Epoch: 1/2, step 470/574 completed (loss: 0.5410823225975037, acc: 0.8333333134651184)
[2025-01-02 00:43:36,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:36,373][root][INFO] - Training Epoch: 1/2, step 471/574 completed (loss: 0.8305784463882446, acc: 0.75)
[2025-01-02 00:43:36,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:36,810][root][INFO] - Training Epoch: 1/2, step 472/574 completed (loss: 1.2987197637557983, acc: 0.6078431606292725)
[2025-01-02 00:43:36,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:37,169][root][INFO] - Training Epoch: 1/2, step 473/574 completed (loss: 1.0241113901138306, acc: 0.7379912734031677)
[2025-01-02 00:43:37,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:37,504][root][INFO] - Training Epoch: 1/2, step 474/574 completed (loss: 0.9436690211296082, acc: 0.7395833134651184)
[2025-01-02 00:43:37,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:37,877][root][INFO] - Training Epoch: 1/2, step 475/574 completed (loss: 0.6169830560684204, acc: 0.8098159432411194)
[2025-01-02 00:43:38,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:38,235][root][INFO] - Training Epoch: 1/2, step 476/574 completed (loss: 0.6622976064682007, acc: 0.8201438784599304)
[2025-01-02 00:43:38,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:38,599][root][INFO] - Training Epoch: 1/2, step 477/574 completed (loss: 1.1834863424301147, acc: 0.6733668446540833)
[2025-01-02 00:43:38,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:38,904][root][INFO] - Training Epoch: 1/2, step 478/574 completed (loss: 0.9866683483123779, acc: 0.75)
[2025-01-02 00:43:38,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:39,206][root][INFO] - Training Epoch: 1/2, step 479/574 completed (loss: 1.2987689971923828, acc: 0.6363636255264282)
[2025-01-02 00:43:39,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:39,532][root][INFO] - Training Epoch: 1/2, step 480/574 completed (loss: 1.076332688331604, acc: 0.7037037014961243)
[2025-01-02 00:43:39,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:39,849][root][INFO] - Training Epoch: 1/2, step 481/574 completed (loss: 1.0995655059814453, acc: 0.699999988079071)
[2025-01-02 00:43:39,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:40,203][root][INFO] - Training Epoch: 1/2, step 482/574 completed (loss: 1.38528311252594, acc: 0.6499999761581421)
[2025-01-02 00:43:40,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:40,570][root][INFO] - Training Epoch: 1/2, step 483/574 completed (loss: 1.3727370500564575, acc: 0.517241358757019)
[2025-01-02 00:43:40,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:40,883][root][INFO] - Training Epoch: 1/2, step 484/574 completed (loss: 0.3402647376060486, acc: 0.9032257795333862)
[2025-01-02 00:43:40,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:41,205][root][INFO] - Training Epoch: 1/2, step 485/574 completed (loss: 1.0696340799331665, acc: 0.7894737124443054)
[2025-01-02 00:43:41,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:41,528][root][INFO] - Training Epoch: 1/2, step 486/574 completed (loss: 1.9880784749984741, acc: 0.37037035822868347)
[2025-01-02 00:43:41,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:41,847][root][INFO] - Training Epoch: 1/2, step 487/574 completed (loss: 0.9465441703796387, acc: 0.5714285969734192)
[2025-01-02 00:43:41,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:42,175][root][INFO] - Training Epoch: 1/2, step 488/574 completed (loss: 1.3652230501174927, acc: 0.7272727489471436)
[2025-01-02 00:43:42,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:42,575][root][INFO] - Training Epoch: 1/2, step 489/574 completed (loss: 1.3812777996063232, acc: 0.6307692527770996)
[2025-01-02 00:43:42,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:42,924][root][INFO] - Training Epoch: 1/2, step 490/574 completed (loss: 0.7470589280128479, acc: 0.8666666746139526)
[2025-01-02 00:43:43,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:43,256][root][INFO] - Training Epoch: 1/2, step 491/574 completed (loss: 0.845294177532196, acc: 0.8275862336158752)
[2025-01-02 00:43:43,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:43,610][root][INFO] - Training Epoch: 1/2, step 492/574 completed (loss: 0.8346754312515259, acc: 0.7058823704719543)
[2025-01-02 00:43:43,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:43,956][root][INFO] - Training Epoch: 1/2, step 493/574 completed (loss: 0.7304230332374573, acc: 0.7931034564971924)
[2025-01-02 00:43:44,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:44,282][root][INFO] - Training Epoch: 1/2, step 494/574 completed (loss: 0.8748641610145569, acc: 0.7894737124443054)
[2025-01-02 00:43:44,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:44,651][root][INFO] - Training Epoch: 1/2, step 495/574 completed (loss: 1.0833407640457153, acc: 0.7368420958518982)
[2025-01-02 00:43:44,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:45,019][root][INFO] - Training Epoch: 1/2, step 496/574 completed (loss: 1.0350700616836548, acc: 0.7232142686843872)
[2025-01-02 00:43:45,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:45,420][root][INFO] - Training Epoch: 1/2, step 497/574 completed (loss: 0.7698575258255005, acc: 0.7977527976036072)
[2025-01-02 00:43:45,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:45,767][root][INFO] - Training Epoch: 1/2, step 498/574 completed (loss: 1.0847077369689941, acc: 0.6853932738304138)
[2025-01-02 00:43:45,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:46,138][root][INFO] - Training Epoch: 1/2, step 499/574 completed (loss: 1.7180838584899902, acc: 0.5390070676803589)
[2025-01-02 00:43:46,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:46,477][root][INFO] - Training Epoch: 1/2, step 500/574 completed (loss: 1.256935477256775, acc: 0.739130437374115)
[2025-01-02 00:43:46,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:46,799][root][INFO] - Training Epoch: 1/2, step 501/574 completed (loss: 0.14654380083084106, acc: 1.0)
[2025-01-02 00:43:46,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:47,136][root][INFO] - Training Epoch: 1/2, step 502/574 completed (loss: 0.3405143916606903, acc: 0.8846153616905212)
[2025-01-02 00:43:47,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:47,497][root][INFO] - Training Epoch: 1/2, step 503/574 completed (loss: 0.6445003747940063, acc: 0.7777777910232544)
[2025-01-02 00:43:47,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:47,830][root][INFO] - Training Epoch: 1/2, step 504/574 completed (loss: 0.4984731376171112, acc: 0.8518518805503845)
[2025-01-02 00:43:47,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:48,202][root][INFO] - Training Epoch: 1/2, step 505/574 completed (loss: 0.8644101619720459, acc: 0.8301886916160583)
[2025-01-02 00:43:48,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:48,548][root][INFO] - Training Epoch: 1/2, step 506/574 completed (loss: 0.6563904881477356, acc: 0.8620689511299133)
[2025-01-02 00:43:48,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:49,199][root][INFO] - Training Epoch: 1/2, step 507/574 completed (loss: 1.6459354162216187, acc: 0.5495495200157166)
[2025-01-02 00:43:49,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:49,686][root][INFO] - Training Epoch: 1/2, step 508/574 completed (loss: 1.0780097246170044, acc: 0.7605633735656738)
[2025-01-02 00:43:49,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:49,995][root][INFO] - Training Epoch: 1/2, step 509/574 completed (loss: 0.4208000600337982, acc: 0.8500000238418579)
[2025-01-02 00:43:50,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:50,289][root][INFO] - Training Epoch: 1/2, step 510/574 completed (loss: 0.7068901062011719, acc: 0.7333333492279053)
[2025-01-02 00:43:50,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:50,609][root][INFO] - Training Epoch: 1/2, step 511/574 completed (loss: 0.9096481800079346, acc: 0.7307692170143127)
[2025-01-02 00:43:51,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:53,288][root][INFO] - Training Epoch: 1/2, step 512/574 completed (loss: 1.7806445360183716, acc: 0.5714285969734192)
[2025-01-02 00:43:53,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:54,061][root][INFO] - Training Epoch: 1/2, step 513/574 completed (loss: 0.7447519302368164, acc: 0.8333333134651184)
[2025-01-02 00:43:54,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:54,379][root][INFO] - Training Epoch: 1/2, step 514/574 completed (loss: 0.9827247858047485, acc: 0.7857142686843872)
[2025-01-02 00:43:54,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:54,752][root][INFO] - Training Epoch: 1/2, step 515/574 completed (loss: 0.32734590768814087, acc: 0.8999999761581421)
[2025-01-02 00:43:54,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:55,489][root][INFO] - Training Epoch: 1/2, step 516/574 completed (loss: 0.8480120897293091, acc: 0.7916666865348816)
[2025-01-02 00:43:55,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:55,812][root][INFO] - Training Epoch: 1/2, step 517/574 completed (loss: 0.08848810940980911, acc: 0.9615384340286255)
[2025-01-02 00:43:55,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:56,174][root][INFO] - Training Epoch: 1/2, step 518/574 completed (loss: 0.4141835570335388, acc: 0.8387096524238586)
[2025-01-02 00:43:56,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:56,478][root][INFO] - Training Epoch: 1/2, step 519/574 completed (loss: 0.5748397707939148, acc: 0.800000011920929)
[2025-01-02 00:43:56,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:56,792][root][INFO] - Training Epoch: 1/2, step 520/574 completed (loss: 0.6282604336738586, acc: 0.8148148059844971)
[2025-01-02 00:43:57,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:57,870][root][INFO] - Training Epoch: 1/2, step 521/574 completed (loss: 0.9966042637825012, acc: 0.7372881174087524)
[2025-01-02 00:43:57,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:58,239][root][INFO] - Training Epoch: 1/2, step 522/574 completed (loss: 0.5037257075309753, acc: 0.8507462739944458)
[2025-01-02 00:43:58,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:58,641][root][INFO] - Training Epoch: 1/2, step 523/574 completed (loss: 0.5584475994110107, acc: 0.8029196858406067)
[2025-01-02 00:43:58,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:59,242][root][INFO] - Training Epoch: 1/2, step 524/574 completed (loss: 1.0392217636108398, acc: 0.6899999976158142)
[2025-01-02 00:43:59,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:59,603][root][INFO] - Training Epoch: 1/2, step 525/574 completed (loss: 0.08316779136657715, acc: 1.0)
[2025-01-02 00:43:59,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:00,078][root][INFO] - Training Epoch: 1/2, step 526/574 completed (loss: 0.34885284304618835, acc: 0.8846153616905212)
[2025-01-02 00:44:00,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:00,457][root][INFO] - Training Epoch: 1/2, step 527/574 completed (loss: 0.648557186126709, acc: 0.8571428656578064)
[2025-01-02 00:44:00,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:00,863][root][INFO] - Training Epoch: 1/2, step 528/574 completed (loss: 2.3381805419921875, acc: 0.4262295067310333)
[2025-01-02 00:44:01,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:01,261][root][INFO] - Training Epoch: 1/2, step 529/574 completed (loss: 0.5051990151405334, acc: 0.7966101765632629)
[2025-01-02 00:44:01,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:01,589][root][INFO] - Training Epoch: 1/2, step 530/574 completed (loss: 1.8218848705291748, acc: 0.5581395626068115)
[2025-01-02 00:44:01,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:01,978][root][INFO] - Training Epoch: 1/2, step 531/574 completed (loss: 1.4684650897979736, acc: 0.6590909361839294)
[2025-01-02 00:44:02,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:02,348][root][INFO] - Training Epoch: 1/2, step 532/574 completed (loss: 1.5438965559005737, acc: 0.6037735939025879)
[2025-01-02 00:44:02,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:02,727][root][INFO] - Training Epoch: 1/2, step 533/574 completed (loss: 1.3665688037872314, acc: 0.6818181872367859)
[2025-01-02 00:44:02,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:03,059][root][INFO] - Training Epoch: 1/2, step 534/574 completed (loss: 1.002338171005249, acc: 0.7599999904632568)
[2025-01-02 00:44:03,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:03,406][root][INFO] - Training Epoch: 1/2, step 535/574 completed (loss: 0.8902385830879211, acc: 0.800000011920929)
[2025-01-02 00:44:03,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:03,804][root][INFO] - Training Epoch: 1/2, step 536/574 completed (loss: 0.546348512172699, acc: 0.8181818127632141)
[2025-01-02 00:44:03,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:04,242][root][INFO] - Training Epoch: 1/2, step 537/574 completed (loss: 0.9673441648483276, acc: 0.7692307829856873)
[2025-01-02 00:44:04,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:04,631][root][INFO] - Training Epoch: 1/2, step 538/574 completed (loss: 0.9858657121658325, acc: 0.75)
[2025-01-02 00:44:04,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:05,084][root][INFO] - Training Epoch: 1/2, step 539/574 completed (loss: 0.8467003107070923, acc: 0.75)
[2025-01-02 00:44:05,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:05,474][root][INFO] - Training Epoch: 1/2, step 540/574 completed (loss: 1.208911657333374, acc: 0.6666666865348816)
[2025-01-02 00:44:05,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:05,838][root][INFO] - Training Epoch: 1/2, step 541/574 completed (loss: 0.7867726683616638, acc: 0.6875)
[2025-01-02 00:44:05,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:06,190][root][INFO] - Training Epoch: 1/2, step 542/574 completed (loss: 0.1864745169878006, acc: 0.9354838728904724)
[2025-01-02 00:44:06,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:06,557][root][INFO] - Training Epoch: 1/2, step 543/574 completed (loss: 0.32991379499435425, acc: 0.95652174949646)
[2025-01-02 00:44:06,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:06,940][root][INFO] - Training Epoch: 1/2, step 544/574 completed (loss: 0.38899409770965576, acc: 0.8999999761581421)
[2025-01-02 00:44:07,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:07,285][root][INFO] - Training Epoch: 1/2, step 545/574 completed (loss: 0.30949610471725464, acc: 0.8780487775802612)
[2025-01-02 00:44:07,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:07,642][root][INFO] - Training Epoch: 1/2, step 546/574 completed (loss: 0.047699496150016785, acc: 0.9714285731315613)
[2025-01-02 00:44:07,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:08,018][root][INFO] - Training Epoch: 1/2, step 547/574 completed (loss: 0.10420898348093033, acc: 0.9736841917037964)
[2025-01-02 00:44:08,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:08,386][root][INFO] - Training Epoch: 1/2, step 548/574 completed (loss: 0.7057033777236938, acc: 0.774193525314331)
[2025-01-02 00:44:08,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:08,708][root][INFO] - Training Epoch: 1/2, step 549/574 completed (loss: 0.0734517052769661, acc: 1.0)
[2025-01-02 00:44:08,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:09,053][root][INFO] - Training Epoch: 1/2, step 550/574 completed (loss: 0.5114112496376038, acc: 0.9090909361839294)
[2025-01-02 00:44:09,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:09,455][root][INFO] - Training Epoch: 1/2, step 551/574 completed (loss: 0.34905555844306946, acc: 0.8999999761581421)
[2025-01-02 00:44:09,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:09,821][root][INFO] - Training Epoch: 1/2, step 552/574 completed (loss: 0.4086940288543701, acc: 0.8714285492897034)
[2025-01-02 00:44:09,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:10,242][root][INFO] - Training Epoch: 1/2, step 553/574 completed (loss: 0.6664745211601257, acc: 0.8321167826652527)
[2025-01-02 00:44:10,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:10,635][root][INFO] - Training Epoch: 1/2, step 554/574 completed (loss: 0.5600289106369019, acc: 0.8413792848587036)
[2025-01-02 00:44:10,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:11,060][root][INFO] - Training Epoch: 1/2, step 555/574 completed (loss: 0.6275069713592529, acc: 0.8428571224212646)
[2025-01-02 00:44:11,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:11,439][root][INFO] - Training Epoch: 1/2, step 556/574 completed (loss: 0.6385287046432495, acc: 0.8344370722770691)
[2025-01-02 00:44:11,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:11,812][root][INFO] - Training Epoch: 1/2, step 557/574 completed (loss: 0.5510947108268738, acc: 0.8803418874740601)
[2025-01-02 00:44:11,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:12,177][root][INFO] - Training Epoch: 1/2, step 558/574 completed (loss: 0.3278893530368805, acc: 0.8799999952316284)
[2025-01-02 00:44:12,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:12,531][root][INFO] - Training Epoch: 1/2, step 559/574 completed (loss: 0.6940935850143433, acc: 0.8461538553237915)
[2025-01-02 00:44:12,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:12,849][root][INFO] - Training Epoch: 1/2, step 560/574 completed (loss: 0.2221429944038391, acc: 0.9230769276618958)
[2025-01-02 00:44:13,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:13,201][root][INFO] - Training Epoch: 1/2, step 561/574 completed (loss: 0.28364527225494385, acc: 0.9487179517745972)
[2025-01-02 00:44:13,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:13,575][root][INFO] - Training Epoch: 1/2, step 562/574 completed (loss: 0.7907505631446838, acc: 0.8333333134651184)
[2025-01-02 00:44:13,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:13,890][root][INFO] - Training Epoch: 1/2, step 563/574 completed (loss: 0.4979042410850525, acc: 0.8831169009208679)
[2025-01-02 00:44:14,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:14,228][root][INFO] - Training Epoch: 1/2, step 564/574 completed (loss: 0.68310546875, acc: 0.8125)
[2025-01-02 00:44:14,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:14,579][root][INFO] - Training Epoch: 1/2, step 565/574 completed (loss: 0.4215903878211975, acc: 0.8448275923728943)
[2025-01-02 00:44:14,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:14,933][root][INFO] - Training Epoch: 1/2, step 566/574 completed (loss: 0.6061614155769348, acc: 0.8809523582458496)
[2025-01-02 00:44:15,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:15,303][root][INFO] - Training Epoch: 1/2, step 567/574 completed (loss: 0.08418630808591843, acc: 0.9736841917037964)
[2025-01-02 00:44:15,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:15,639][root][INFO] - Training Epoch: 1/2, step 568/574 completed (loss: 0.23149630427360535, acc: 0.8888888955116272)
[2025-01-02 00:44:15,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:16,069][root][INFO] - Training Epoch: 1/2, step 569/574 completed (loss: 0.4037863314151764, acc: 0.8823529481887817)
[2025-01-02 00:44:16,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:16,421][root][INFO] - Training Epoch: 1/2, step 570/574 completed (loss: 0.07807048410177231, acc: 0.9677419066429138)
[2025-01-02 00:44:16,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:16,817][root][INFO] - Training Epoch: 1/2, step 571/574 completed (loss: 0.6014483571052551, acc: 0.8547008633613586)
[2025-01-02 00:44:17,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:17,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:18,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:18,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:18,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:19,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:19,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:20,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:20,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:20,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:21,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:21,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:22,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:22,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:22,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:23,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:23,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:23,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:24,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:24,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:24,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:25,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:25,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:25,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:26,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:26,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:26,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:27,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:27,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:27,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:27,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:28,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:28,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:28,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:29,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:29,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:29,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:30,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:30,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:30,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:31,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:31,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:31,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:32,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:32,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:32,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:33,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:33,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:33,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:34,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:34,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:34,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:35,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:35,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:35,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:36,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:36,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:36,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:36,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:37,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:37,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:38,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:38,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:38,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:38,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:39,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:39,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:39,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:40,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:40,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:41,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:41,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:41,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:41,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:42,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:42,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:42,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:43,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:43,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:43,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:44,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:44,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:44,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:44,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:45,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:45,953][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9955, device='cuda:0') eval_epoch_loss=tensor(0.6909, device='cuda:0') eval_epoch_acc=tensor(0.8103, device='cuda:0')
[2025-01-02 00:44:45,955][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:44:45,955][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:44:46,178][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_572_loss_0.6909047961235046/model.pt
[2025-01-02 00:44:46,188][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:44:46,189][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.6909047961235046
[2025-01-02 00:44:46,190][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8103269934654236
[2025-01-02 00:44:46,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:46,574][root][INFO] - Training Epoch: 1/2, step 572/574 completed (loss: 0.5608293414115906, acc: 0.8367347121238708)
[2025-01-02 00:44:46,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:46,935][root][INFO] - Training Epoch: 1/2, step 573/574 completed (loss: 0.5298458337783813, acc: 0.849056601524353)
[2025-01-02 00:44:47,487][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=3.8628, train_epoch_loss=1.3514, epoch time 361.4659474529326s
[2025-01-02 00:44:47,487][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-02 00:44:47,487][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-02 00:44:47,487][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-02 00:44:47,487][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 3
[2025-01-02 00:44:47,487][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-02 00:44:48,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:48,307][root][INFO] - Training Epoch: 2/2, step 0/574 completed (loss: 0.9124844670295715, acc: 0.7037037014961243)
[2025-01-02 00:44:48,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:48,637][root][INFO] - Training Epoch: 2/2, step 1/574 completed (loss: 0.6774526834487915, acc: 0.800000011920929)
[2025-01-02 00:44:48,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:48,993][root][INFO] - Training Epoch: 2/2, step 2/574 completed (loss: 0.9458051323890686, acc: 0.7297297120094299)
[2025-01-02 00:44:49,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:49,352][root][INFO] - Training Epoch: 2/2, step 3/574 completed (loss: 0.8646947145462036, acc: 0.7894737124443054)
[2025-01-02 00:44:49,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:49,744][root][INFO] - Training Epoch: 2/2, step 4/574 completed (loss: 1.0276401042938232, acc: 0.7837837934494019)
[2025-01-02 00:44:49,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:50,159][root][INFO] - Training Epoch: 2/2, step 5/574 completed (loss: 0.9392654299736023, acc: 0.75)
[2025-01-02 00:44:50,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:50,547][root][INFO] - Training Epoch: 2/2, step 6/574 completed (loss: 1.4259871244430542, acc: 0.5714285969734192)
[2025-01-02 00:44:50,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:50,910][root][INFO] - Training Epoch: 2/2, step 7/574 completed (loss: 0.9141570329666138, acc: 0.800000011920929)
[2025-01-02 00:44:51,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:51,287][root][INFO] - Training Epoch: 2/2, step 8/574 completed (loss: 0.3185177445411682, acc: 0.8636363744735718)
[2025-01-02 00:44:51,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:51,662][root][INFO] - Training Epoch: 2/2, step 9/574 completed (loss: 0.15636955201625824, acc: 0.9615384340286255)
[2025-01-02 00:44:51,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:52,025][root][INFO] - Training Epoch: 2/2, step 10/574 completed (loss: 0.28775763511657715, acc: 0.9259259104728699)
[2025-01-02 00:44:52,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:52,363][root][INFO] - Training Epoch: 2/2, step 11/574 completed (loss: 0.6571784615516663, acc: 0.8461538553237915)
[2025-01-02 00:44:52,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:52,729][root][INFO] - Training Epoch: 2/2, step 12/574 completed (loss: 0.20439140498638153, acc: 0.939393937587738)
[2025-01-02 00:44:52,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:53,068][root][INFO] - Training Epoch: 2/2, step 13/574 completed (loss: 0.44879627227783203, acc: 0.804347813129425)
[2025-01-02 00:44:53,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:53,382][root][INFO] - Training Epoch: 2/2, step 14/574 completed (loss: 0.4021383225917816, acc: 0.8823529481887817)
[2025-01-02 00:44:53,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:53,761][root][INFO] - Training Epoch: 2/2, step 15/574 completed (loss: 0.6364148855209351, acc: 0.8571428656578064)
[2025-01-02 00:44:53,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:54,119][root][INFO] - Training Epoch: 2/2, step 16/574 completed (loss: 0.3361192047595978, acc: 0.8947368264198303)
[2025-01-02 00:44:54,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:54,501][root][INFO] - Training Epoch: 2/2, step 17/574 completed (loss: 0.883822500705719, acc: 0.75)
[2025-01-02 00:44:54,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:54,850][root][INFO] - Training Epoch: 2/2, step 18/574 completed (loss: 1.226986289024353, acc: 0.6944444179534912)
[2025-01-02 00:44:54,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:55,223][root][INFO] - Training Epoch: 2/2, step 19/574 completed (loss: 0.7229117155075073, acc: 0.8421052694320679)
[2025-01-02 00:44:55,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:55,589][root][INFO] - Training Epoch: 2/2, step 20/574 completed (loss: 0.5479753017425537, acc: 0.8461538553237915)
[2025-01-02 00:44:55,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:55,980][root][INFO] - Training Epoch: 2/2, step 21/574 completed (loss: 0.8600219488143921, acc: 0.7931034564971924)
[2025-01-02 00:44:56,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:56,306][root][INFO] - Training Epoch: 2/2, step 22/574 completed (loss: 1.3219943046569824, acc: 0.5199999809265137)
[2025-01-02 00:44:56,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:56,634][root][INFO] - Training Epoch: 2/2, step 23/574 completed (loss: 1.049065113067627, acc: 0.761904776096344)
[2025-01-02 00:44:56,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:57,031][root][INFO] - Training Epoch: 2/2, step 24/574 completed (loss: 0.24452653527259827, acc: 0.9375)
[2025-01-02 00:44:57,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:57,380][root][INFO] - Training Epoch: 2/2, step 25/574 completed (loss: 1.0662915706634521, acc: 0.7735849022865295)
[2025-01-02 00:44:57,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:57,784][root][INFO] - Training Epoch: 2/2, step 26/574 completed (loss: 1.1471112966537476, acc: 0.6712328791618347)
[2025-01-02 00:44:58,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:59,032][root][INFO] - Training Epoch: 2/2, step 27/574 completed (loss: 1.2401103973388672, acc: 0.6679841876029968)
[2025-01-02 00:44:59,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:59,388][root][INFO] - Training Epoch: 2/2, step 28/574 completed (loss: 0.7237554788589478, acc: 0.7209302186965942)
[2025-01-02 00:44:59,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:59,744][root][INFO] - Training Epoch: 2/2, step 29/574 completed (loss: 0.8927239775657654, acc: 0.6867470145225525)
[2025-01-02 00:44:59,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:00,111][root][INFO] - Training Epoch: 2/2, step 30/574 completed (loss: 1.1135969161987305, acc: 0.7037037014961243)
[2025-01-02 00:45:00,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:00,439][root][INFO] - Training Epoch: 2/2, step 31/574 completed (loss: 1.1999682188034058, acc: 0.6785714030265808)
[2025-01-02 00:45:00,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:00,789][root][INFO] - Training Epoch: 2/2, step 32/574 completed (loss: 0.7350943088531494, acc: 0.7777777910232544)
[2025-01-02 00:45:00,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:01,115][root][INFO] - Training Epoch: 2/2, step 33/574 completed (loss: 0.22624905407428741, acc: 0.95652174949646)
[2025-01-02 00:45:01,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:01,497][root][INFO] - Training Epoch: 2/2, step 34/574 completed (loss: 0.6019973158836365, acc: 0.7983193397521973)
[2025-01-02 00:45:01,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:01,878][root][INFO] - Training Epoch: 2/2, step 35/574 completed (loss: 0.5825110673904419, acc: 0.8524590134620667)
[2025-01-02 00:45:01,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:02,225][root][INFO] - Training Epoch: 2/2, step 36/574 completed (loss: 0.8271243572235107, acc: 0.8253968358039856)
[2025-01-02 00:45:02,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:02,542][root][INFO] - Training Epoch: 2/2, step 37/574 completed (loss: 0.7776505351066589, acc: 0.8474576473236084)
[2025-01-02 00:45:02,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:02,888][root][INFO] - Training Epoch: 2/2, step 38/574 completed (loss: 0.5135668516159058, acc: 0.8850574493408203)
[2025-01-02 00:45:03,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:03,280][root][INFO] - Training Epoch: 2/2, step 39/574 completed (loss: 0.6415225267410278, acc: 0.8095238208770752)
[2025-01-02 00:45:03,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:03,674][root][INFO] - Training Epoch: 2/2, step 40/574 completed (loss: 0.8192837834358215, acc: 0.7692307829856873)
[2025-01-02 00:45:03,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:04,107][root][INFO] - Training Epoch: 2/2, step 41/574 completed (loss: 0.547429621219635, acc: 0.837837815284729)
[2025-01-02 00:45:04,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:04,469][root][INFO] - Training Epoch: 2/2, step 42/574 completed (loss: 1.0855330228805542, acc: 0.692307710647583)
[2025-01-02 00:45:04,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:04,882][root][INFO] - Training Epoch: 2/2, step 43/574 completed (loss: 0.9188240766525269, acc: 0.7777777910232544)
[2025-01-02 00:45:05,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:05,295][root][INFO] - Training Epoch: 2/2, step 44/574 completed (loss: 0.6045804023742676, acc: 0.8350515365600586)
[2025-01-02 00:45:05,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:05,708][root][INFO] - Training Epoch: 2/2, step 45/574 completed (loss: 0.6241723299026489, acc: 0.8308823704719543)
[2025-01-02 00:45:05,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:06,025][root][INFO] - Training Epoch: 2/2, step 46/574 completed (loss: 0.7339200377464294, acc: 0.7692307829856873)
[2025-01-02 00:45:06,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:06,373][root][INFO] - Training Epoch: 2/2, step 47/574 completed (loss: 0.35399889945983887, acc: 0.9629629850387573)
[2025-01-02 00:45:06,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:06,765][root][INFO] - Training Epoch: 2/2, step 48/574 completed (loss: 0.8259038925170898, acc: 0.7857142686843872)
[2025-01-02 00:45:06,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:07,104][root][INFO] - Training Epoch: 2/2, step 49/574 completed (loss: 0.4473608434200287, acc: 0.8333333134651184)
[2025-01-02 00:45:07,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:07,450][root][INFO] - Training Epoch: 2/2, step 50/574 completed (loss: 1.021304726600647, acc: 0.7543859481811523)
[2025-01-02 00:45:07,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:07,808][root][INFO] - Training Epoch: 2/2, step 51/574 completed (loss: 1.0099763870239258, acc: 0.6984127163887024)
[2025-01-02 00:45:07,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:08,184][root][INFO] - Training Epoch: 2/2, step 52/574 completed (loss: 1.2451261281967163, acc: 0.6901408433914185)
[2025-01-02 00:45:08,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:08,641][root][INFO] - Training Epoch: 2/2, step 53/574 completed (loss: 1.7022897005081177, acc: 0.5266666412353516)
[2025-01-02 00:45:08,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:09,007][root][INFO] - Training Epoch: 2/2, step 54/574 completed (loss: 1.6704847812652588, acc: 0.6216216087341309)
[2025-01-02 00:45:09,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:09,398][root][INFO] - Training Epoch: 2/2, step 55/574 completed (loss: 0.27382469177246094, acc: 0.8846153616905212)
[2025-01-02 00:45:10,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:12,488][root][INFO] - Training Epoch: 2/2, step 56/574 completed (loss: 1.4419469833374023, acc: 0.6109215021133423)
[2025-01-02 00:45:12,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:13,657][root][INFO] - Training Epoch: 2/2, step 57/574 completed (loss: 1.4146056175231934, acc: 0.6165577173233032)
[2025-01-02 00:45:13,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:14,281][root][INFO] - Training Epoch: 2/2, step 58/574 completed (loss: 1.1291139125823975, acc: 0.6761363744735718)
[2025-01-02 00:45:14,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:14,848][root][INFO] - Training Epoch: 2/2, step 59/574 completed (loss: 0.5901892781257629, acc: 0.8308823704719543)
[2025-01-02 00:45:14,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:15,408][root][INFO] - Training Epoch: 2/2, step 60/574 completed (loss: 1.1555426120758057, acc: 0.695652186870575)
[2025-01-02 00:45:15,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:15,826][root][INFO] - Training Epoch: 2/2, step 61/574 completed (loss: 1.2733081579208374, acc: 0.625)
[2025-01-02 00:45:15,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:16,207][root][INFO] - Training Epoch: 2/2, step 62/574 completed (loss: 0.5915985703468323, acc: 0.8529411554336548)
[2025-01-02 00:45:16,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:16,599][root][INFO] - Training Epoch: 2/2, step 63/574 completed (loss: 0.5845121145248413, acc: 0.8055555820465088)
[2025-01-02 00:45:16,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:16,996][root][INFO] - Training Epoch: 2/2, step 64/574 completed (loss: 0.376249223947525, acc: 0.890625)
[2025-01-02 00:45:17,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:17,391][root][INFO] - Training Epoch: 2/2, step 65/574 completed (loss: 0.3331270217895508, acc: 0.8965517282485962)
[2025-01-02 00:45:17,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:17,766][root][INFO] - Training Epoch: 2/2, step 66/574 completed (loss: 1.2523125410079956, acc: 0.6785714030265808)
[2025-01-02 00:45:17,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:18,125][root][INFO] - Training Epoch: 2/2, step 67/574 completed (loss: 0.5362033247947693, acc: 0.8500000238418579)
[2025-01-02 00:45:18,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:18,511][root][INFO] - Training Epoch: 2/2, step 68/574 completed (loss: 0.1256266087293625, acc: 0.9599999785423279)
[2025-01-02 00:45:18,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:18,920][root][INFO] - Training Epoch: 2/2, step 69/574 completed (loss: 1.196810245513916, acc: 0.6388888955116272)
[2025-01-02 00:45:19,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:19,284][root][INFO] - Training Epoch: 2/2, step 70/574 completed (loss: 1.3453692197799683, acc: 0.5757575631141663)
[2025-01-02 00:45:19,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:19,656][root][INFO] - Training Epoch: 2/2, step 71/574 completed (loss: 1.2151399850845337, acc: 0.6397058963775635)
[2025-01-02 00:45:19,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:19,994][root][INFO] - Training Epoch: 2/2, step 72/574 completed (loss: 0.9216346144676208, acc: 0.7539682388305664)
[2025-01-02 00:45:20,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:20,304][root][INFO] - Training Epoch: 2/2, step 73/574 completed (loss: 1.6395318508148193, acc: 0.5692307949066162)
[2025-01-02 00:45:20,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:20,623][root][INFO] - Training Epoch: 2/2, step 74/574 completed (loss: 1.415593147277832, acc: 0.6428571343421936)
[2025-01-02 00:45:20,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:20,968][root][INFO] - Training Epoch: 2/2, step 75/574 completed (loss: 1.4690258502960205, acc: 0.5970149040222168)
[2025-01-02 00:45:21,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:21,367][root][INFO] - Training Epoch: 2/2, step 76/574 completed (loss: 1.6021902561187744, acc: 0.5729926824569702)
[2025-01-02 00:45:21,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:21,712][root][INFO] - Training Epoch: 2/2, step 77/574 completed (loss: 0.08229812234640121, acc: 1.0)
[2025-01-02 00:45:21,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:22,050][root][INFO] - Training Epoch: 2/2, step 78/574 completed (loss: 0.3998325765132904, acc: 0.9166666865348816)
[2025-01-02 00:45:22,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:22,412][root][INFO] - Training Epoch: 2/2, step 79/574 completed (loss: 0.2946641743183136, acc: 0.8787878751754761)
[2025-01-02 00:45:22,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:22,801][root][INFO] - Training Epoch: 2/2, step 80/574 completed (loss: 0.572117269039154, acc: 0.8461538553237915)
[2025-01-02 00:45:22,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:23,180][root][INFO] - Training Epoch: 2/2, step 81/574 completed (loss: 0.906161367893219, acc: 0.7692307829856873)
[2025-01-02 00:45:23,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:23,533][root][INFO] - Training Epoch: 2/2, step 82/574 completed (loss: 0.9909586310386658, acc: 0.7692307829856873)
[2025-01-02 00:45:23,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:23,884][root][INFO] - Training Epoch: 2/2, step 83/574 completed (loss: 0.46997761726379395, acc: 0.875)
[2025-01-02 00:45:23,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:24,250][root][INFO] - Training Epoch: 2/2, step 84/574 completed (loss: 0.6538348197937012, acc: 0.8260869383811951)
[2025-01-02 00:45:24,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:24,599][root][INFO] - Training Epoch: 2/2, step 85/574 completed (loss: 0.961769163608551, acc: 0.7200000286102295)
[2025-01-02 00:45:24,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:24,908][root][INFO] - Training Epoch: 2/2, step 86/574 completed (loss: 0.5245355367660522, acc: 0.8695651888847351)
[2025-01-02 00:45:25,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:25,404][root][INFO] - Training Epoch: 2/2, step 87/574 completed (loss: 1.259476661682129, acc: 0.6000000238418579)
[2025-01-02 00:45:25,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:25,742][root][INFO] - Training Epoch: 2/2, step 88/574 completed (loss: 1.2209806442260742, acc: 0.6796116232872009)
[2025-01-02 00:45:26,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:26,833][root][INFO] - Training Epoch: 2/2, step 89/574 completed (loss: 1.1009974479675293, acc: 0.708737850189209)
[2025-01-02 00:45:27,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:27,653][root][INFO] - Training Epoch: 2/2, step 90/574 completed (loss: 1.4801056385040283, acc: 0.6129032373428345)
[2025-01-02 00:45:27,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:28,454][root][INFO] - Training Epoch: 2/2, step 91/574 completed (loss: 1.221411943435669, acc: 0.6637930870056152)
[2025-01-02 00:45:28,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:29,196][root][INFO] - Training Epoch: 2/2, step 92/574 completed (loss: 0.9035508036613464, acc: 0.7263157963752747)
[2025-01-02 00:45:29,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:30,185][root][INFO] - Training Epoch: 2/2, step 93/574 completed (loss: 1.809672236442566, acc: 0.48514851927757263)
[2025-01-02 00:45:30,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:30,471][root][INFO] - Training Epoch: 2/2, step 94/574 completed (loss: 1.4718034267425537, acc: 0.5483871102333069)
[2025-01-02 00:45:30,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:30,782][root][INFO] - Training Epoch: 2/2, step 95/574 completed (loss: 1.2238073348999023, acc: 0.6376811861991882)
[2025-01-02 00:45:30,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:31,104][root][INFO] - Training Epoch: 2/2, step 96/574 completed (loss: 1.4892754554748535, acc: 0.5546218752861023)
[2025-01-02 00:45:31,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:31,461][root][INFO] - Training Epoch: 2/2, step 97/574 completed (loss: 1.7006657123565674, acc: 0.5384615659713745)
[2025-01-02 00:45:31,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:31,847][root][INFO] - Training Epoch: 2/2, step 98/574 completed (loss: 1.6561508178710938, acc: 0.525547444820404)
[2025-01-02 00:45:31,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:32,183][root][INFO] - Training Epoch: 2/2, step 99/574 completed (loss: 1.7892506122589111, acc: 0.5223880410194397)
[2025-01-02 00:45:32,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:32,524][root][INFO] - Training Epoch: 2/2, step 100/574 completed (loss: 0.6476337313652039, acc: 0.800000011920929)
[2025-01-02 00:45:32,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:32,867][root][INFO] - Training Epoch: 2/2, step 101/574 completed (loss: 0.11194231361150742, acc: 1.0)
[2025-01-02 00:45:32,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:33,188][root][INFO] - Training Epoch: 2/2, step 102/574 completed (loss: 0.1558501273393631, acc: 1.0)
[2025-01-02 00:45:33,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:33,501][root][INFO] - Training Epoch: 2/2, step 103/574 completed (loss: 0.276003360748291, acc: 0.8863636255264282)
[2025-01-02 00:45:33,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:33,822][root][INFO] - Training Epoch: 2/2, step 104/574 completed (loss: 0.7620695233345032, acc: 0.7931034564971924)
[2025-01-02 00:45:33,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:34,162][root][INFO] - Training Epoch: 2/2, step 105/574 completed (loss: 0.46034660935401917, acc: 0.8604651093482971)
[2025-01-02 00:45:34,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:34,464][root][INFO] - Training Epoch: 2/2, step 106/574 completed (loss: 0.4543876647949219, acc: 0.8799999952316284)
[2025-01-02 00:45:34,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:34,764][root][INFO] - Training Epoch: 2/2, step 107/574 completed (loss: 0.07582376152276993, acc: 1.0)
[2025-01-02 00:45:34,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:35,107][root][INFO] - Training Epoch: 2/2, step 108/574 completed (loss: 0.14180204272270203, acc: 0.9615384340286255)
[2025-01-02 00:45:35,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:35,418][root][INFO] - Training Epoch: 2/2, step 109/574 completed (loss: 0.187638059258461, acc: 0.9047619104385376)
[2025-01-02 00:45:35,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:35,802][root][INFO] - Training Epoch: 2/2, step 110/574 completed (loss: 0.20384575426578522, acc: 0.9538461565971375)
[2025-01-02 00:45:35,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:36,202][root][INFO] - Training Epoch: 2/2, step 111/574 completed (loss: 0.7443204522132874, acc: 0.7894737124443054)
[2025-01-02 00:45:36,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:36,546][root][INFO] - Training Epoch: 2/2, step 112/574 completed (loss: 1.3024191856384277, acc: 0.6491228342056274)
[2025-01-02 00:45:36,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:36,862][root][INFO] - Training Epoch: 2/2, step 113/574 completed (loss: 0.8238357901573181, acc: 0.7948718070983887)
[2025-01-02 00:45:36,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:37,271][root][INFO] - Training Epoch: 2/2, step 114/574 completed (loss: 0.5462307333946228, acc: 0.8367347121238708)
[2025-01-02 00:45:37,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:37,607][root][INFO] - Training Epoch: 2/2, step 115/574 completed (loss: 0.23140357434749603, acc: 0.9545454382896423)
[2025-01-02 00:45:37,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:38,001][root][INFO] - Training Epoch: 2/2, step 116/574 completed (loss: 0.6766969561576843, acc: 0.8095238208770752)
[2025-01-02 00:45:38,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:38,315][root][INFO] - Training Epoch: 2/2, step 117/574 completed (loss: 0.6669535636901855, acc: 0.8292682766914368)
[2025-01-02 00:45:38,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:38,633][root][INFO] - Training Epoch: 2/2, step 118/574 completed (loss: 0.40581393241882324, acc: 0.9032257795333862)
[2025-01-02 00:45:38,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:39,486][root][INFO] - Training Epoch: 2/2, step 119/574 completed (loss: 0.9073807001113892, acc: 0.7756654024124146)
[2025-01-02 00:45:39,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:39,801][root][INFO] - Training Epoch: 2/2, step 120/574 completed (loss: 0.5184493660926819, acc: 0.8399999737739563)
[2025-01-02 00:45:39,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:40,213][root][INFO] - Training Epoch: 2/2, step 121/574 completed (loss: 0.6274763345718384, acc: 0.8653846383094788)
[2025-01-02 00:45:40,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:40,516][root][INFO] - Training Epoch: 2/2, step 122/574 completed (loss: 0.4319418668746948, acc: 0.7916666865348816)
[2025-01-02 00:45:40,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:40,831][root][INFO] - Training Epoch: 2/2, step 123/574 completed (loss: 0.4832645356655121, acc: 0.8421052694320679)
[2025-01-02 00:45:40,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:41,161][root][INFO] - Training Epoch: 2/2, step 124/574 completed (loss: 1.3103513717651367, acc: 0.6441717743873596)
[2025-01-02 00:45:41,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:41,509][root][INFO] - Training Epoch: 2/2, step 125/574 completed (loss: 1.3808741569519043, acc: 0.6180555820465088)
[2025-01-02 00:45:41,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:41,825][root][INFO] - Training Epoch: 2/2, step 126/574 completed (loss: 1.4481858015060425, acc: 0.6000000238418579)
[2025-01-02 00:45:41,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:42,156][root][INFO] - Training Epoch: 2/2, step 127/574 completed (loss: 1.0194635391235352, acc: 0.6964285969734192)
[2025-01-02 00:45:42,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:42,484][root][INFO] - Training Epoch: 2/2, step 128/574 completed (loss: 1.0719903707504272, acc: 0.7076923251152039)
[2025-01-02 00:45:42,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:42,888][root][INFO] - Training Epoch: 2/2, step 129/574 completed (loss: 1.1706552505493164, acc: 0.6691176295280457)
[2025-01-02 00:45:42,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:43,197][root][INFO] - Training Epoch: 2/2, step 130/574 completed (loss: 1.1137207746505737, acc: 0.692307710647583)
[2025-01-02 00:45:43,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:43,493][root][INFO] - Training Epoch: 2/2, step 131/574 completed (loss: 0.8669174313545227, acc: 0.6086956262588501)
[2025-01-02 00:45:43,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:43,795][root][INFO] - Training Epoch: 2/2, step 132/574 completed (loss: 1.395291805267334, acc: 0.59375)
[2025-01-02 00:45:43,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:44,195][root][INFO] - Training Epoch: 2/2, step 133/574 completed (loss: 1.6353415250778198, acc: 0.5652173757553101)
[2025-01-02 00:45:44,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:44,534][root][INFO] - Training Epoch: 2/2, step 134/574 completed (loss: 1.0082969665527344, acc: 0.6285714507102966)
[2025-01-02 00:45:44,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:44,861][root][INFO] - Training Epoch: 2/2, step 135/574 completed (loss: 1.1564621925354004, acc: 0.7307692170143127)
[2025-01-02 00:45:44,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:45,197][root][INFO] - Training Epoch: 2/2, step 136/574 completed (loss: 0.9973049163818359, acc: 0.7142857313156128)
[2025-01-02 00:45:45,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:45,516][root][INFO] - Training Epoch: 2/2, step 137/574 completed (loss: 1.4318007230758667, acc: 0.5)
[2025-01-02 00:45:45,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:45,842][root][INFO] - Training Epoch: 2/2, step 138/574 completed (loss: 1.1971009969711304, acc: 0.695652186870575)
[2025-01-02 00:45:45,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:46,180][root][INFO] - Training Epoch: 2/2, step 139/574 completed (loss: 0.4443656802177429, acc: 0.8571428656578064)
[2025-01-02 00:45:46,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:46,499][root][INFO] - Training Epoch: 2/2, step 140/574 completed (loss: 0.5591672658920288, acc: 0.7692307829856873)
[2025-01-02 00:45:47,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:47,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:47,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:48,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:48,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:48,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:49,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:49,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:49,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:50,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:50,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:50,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:51,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:51,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:51,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:52,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:52,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:52,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:53,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:53,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:53,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:54,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:54,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:54,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:55,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:55,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:55,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:56,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:56,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:56,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:57,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:57,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:57,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:57,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:58,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:58,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:58,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:59,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:59,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:59,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:00,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:00,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:00,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:01,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:01,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:01,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:02,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:02,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:02,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:03,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:03,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:03,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:04,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:04,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:04,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:05,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:05,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:05,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:06,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:06,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:06,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:07,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:07,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:08,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:08,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:08,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:09,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:09,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:09,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:10,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:10,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:10,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:11,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:11,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:11,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:12,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:12,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:13,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:13,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:13,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:14,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:14,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:14,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:14,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:15,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:16,009][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0195, device='cuda:0') eval_epoch_loss=tensor(0.7028, device='cuda:0') eval_epoch_acc=tensor(0.8059, device='cuda:0')
[2025-01-02 00:46:16,012][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:46:16,013][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:46:16,291][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_141_loss_0.7028403282165527/model.pt
[2025-01-02 00:46:16,298][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:46:16,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:16,760][root][INFO] - Training Epoch: 2/2, step 141/574 completed (loss: 1.111975908279419, acc: 0.7096773982048035)
[2025-01-02 00:46:16,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:17,141][root][INFO] - Training Epoch: 2/2, step 142/574 completed (loss: 1.3321881294250488, acc: 0.6486486196517944)
[2025-01-02 00:46:17,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:17,671][root][INFO] - Training Epoch: 2/2, step 143/574 completed (loss: 1.1621216535568237, acc: 0.6666666865348816)
[2025-01-02 00:46:17,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:18,014][root][INFO] - Training Epoch: 2/2, step 144/574 completed (loss: 0.9174838662147522, acc: 0.7835820913314819)
[2025-01-02 00:46:18,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:18,393][root][INFO] - Training Epoch: 2/2, step 145/574 completed (loss: 0.9219920039176941, acc: 0.6938775777816772)
[2025-01-02 00:46:18,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:18,832][root][INFO] - Training Epoch: 2/2, step 146/574 completed (loss: 1.3442537784576416, acc: 0.5744680762290955)
[2025-01-02 00:46:18,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:19,181][root][INFO] - Training Epoch: 2/2, step 147/574 completed (loss: 1.151896357536316, acc: 0.6714285612106323)
[2025-01-02 00:46:19,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:19,551][root][INFO] - Training Epoch: 2/2, step 148/574 completed (loss: 1.2521506547927856, acc: 0.6428571343421936)
[2025-01-02 00:46:19,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:19,911][root][INFO] - Training Epoch: 2/2, step 149/574 completed (loss: 1.0773855447769165, acc: 0.6521739363670349)
[2025-01-02 00:46:20,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:20,259][root][INFO] - Training Epoch: 2/2, step 150/574 completed (loss: 0.9225040078163147, acc: 0.7241379022598267)
[2025-01-02 00:46:20,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:20,634][root][INFO] - Training Epoch: 2/2, step 151/574 completed (loss: 1.4070367813110352, acc: 0.6521739363670349)
[2025-01-02 00:46:20,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:21,034][root][INFO] - Training Epoch: 2/2, step 152/574 completed (loss: 0.8809707760810852, acc: 0.7796609997749329)
[2025-01-02 00:46:21,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:21,410][root][INFO] - Training Epoch: 2/2, step 153/574 completed (loss: 1.1554696559906006, acc: 0.6666666865348816)
[2025-01-02 00:46:21,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:21,777][root][INFO] - Training Epoch: 2/2, step 154/574 completed (loss: 0.9410272836685181, acc: 0.7567567825317383)
[2025-01-02 00:46:21,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:22,138][root][INFO] - Training Epoch: 2/2, step 155/574 completed (loss: 0.4600163400173187, acc: 0.8214285969734192)
[2025-01-02 00:46:22,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:22,479][root][INFO] - Training Epoch: 2/2, step 156/574 completed (loss: 0.6059880256652832, acc: 0.8260869383811951)
[2025-01-02 00:46:22,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:22,828][root][INFO] - Training Epoch: 2/2, step 157/574 completed (loss: 2.2107927799224854, acc: 0.3684210479259491)
[2025-01-02 00:46:23,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:24,504][root][INFO] - Training Epoch: 2/2, step 158/574 completed (loss: 1.3966407775878906, acc: 0.5675675868988037)
[2025-01-02 00:46:24,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:24,859][root][INFO] - Training Epoch: 2/2, step 159/574 completed (loss: 1.5614553689956665, acc: 0.5)
[2025-01-02 00:46:24,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:25,262][root][INFO] - Training Epoch: 2/2, step 160/574 completed (loss: 1.5636985301971436, acc: 0.569767415523529)
[2025-01-02 00:46:25,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:25,849][root][INFO] - Training Epoch: 2/2, step 161/574 completed (loss: 1.8473049402236938, acc: 0.4588235318660736)
[2025-01-02 00:46:25,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:26,411][root][INFO] - Training Epoch: 2/2, step 162/574 completed (loss: 1.8602302074432373, acc: 0.550561785697937)
[2025-01-02 00:46:26,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:26,722][root][INFO] - Training Epoch: 2/2, step 163/574 completed (loss: 0.6748567819595337, acc: 0.8863636255264282)
[2025-01-02 00:46:26,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:27,033][root][INFO] - Training Epoch: 2/2, step 164/574 completed (loss: 0.634529173374176, acc: 0.9047619104385376)
[2025-01-02 00:46:27,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:27,388][root][INFO] - Training Epoch: 2/2, step 165/574 completed (loss: 1.2255070209503174, acc: 0.6551724076271057)
[2025-01-02 00:46:27,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:27,749][root][INFO] - Training Epoch: 2/2, step 166/574 completed (loss: 0.2856191396713257, acc: 0.8775510191917419)
[2025-01-02 00:46:27,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:28,069][root][INFO] - Training Epoch: 2/2, step 167/574 completed (loss: 0.38573840260505676, acc: 0.8399999737739563)
[2025-01-02 00:46:28,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:28,480][root][INFO] - Training Epoch: 2/2, step 168/574 completed (loss: 0.7384030818939209, acc: 0.7916666865348816)
[2025-01-02 00:46:28,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:28,862][root][INFO] - Training Epoch: 2/2, step 169/574 completed (loss: 1.1500128507614136, acc: 0.7450980544090271)
[2025-01-02 00:46:29,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:29,889][root][INFO] - Training Epoch: 2/2, step 170/574 completed (loss: 1.1921526193618774, acc: 0.6780821681022644)
[2025-01-02 00:46:29,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:30,237][root][INFO] - Training Epoch: 2/2, step 171/574 completed (loss: 0.19833029806613922, acc: 0.9583333134651184)
[2025-01-02 00:46:30,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:30,616][root][INFO] - Training Epoch: 2/2, step 172/574 completed (loss: 0.6238297820091248, acc: 0.7777777910232544)
[2025-01-02 00:46:30,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:30,972][root][INFO] - Training Epoch: 2/2, step 173/574 completed (loss: 1.1669389009475708, acc: 0.6785714030265808)
[2025-01-02 00:46:31,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:31,509][root][INFO] - Training Epoch: 2/2, step 174/574 completed (loss: 1.1923125982284546, acc: 0.7168141603469849)
[2025-01-02 00:46:31,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:31,830][root][INFO] - Training Epoch: 2/2, step 175/574 completed (loss: 0.9634955525398254, acc: 0.7681159377098083)
[2025-01-02 00:46:31,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:32,235][root][INFO] - Training Epoch: 2/2, step 176/574 completed (loss: 0.746195375919342, acc: 0.7840909361839294)
[2025-01-02 00:46:32,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:33,200][root][INFO] - Training Epoch: 2/2, step 177/574 completed (loss: 1.4408495426177979, acc: 0.572519063949585)
[2025-01-02 00:46:33,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:33,868][root][INFO] - Training Epoch: 2/2, step 178/574 completed (loss: 1.3520551919937134, acc: 0.6222222447395325)
[2025-01-02 00:46:33,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:34,181][root][INFO] - Training Epoch: 2/2, step 179/574 completed (loss: 0.7153638005256653, acc: 0.8032786846160889)
[2025-01-02 00:46:34,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:34,508][root][INFO] - Training Epoch: 2/2, step 180/574 completed (loss: 0.09580855816602707, acc: 0.9583333134651184)
[2025-01-02 00:46:34,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:34,837][root][INFO] - Training Epoch: 2/2, step 181/574 completed (loss: 0.14568859338760376, acc: 0.9200000166893005)
[2025-01-02 00:46:34,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:35,201][root][INFO] - Training Epoch: 2/2, step 182/574 completed (loss: 0.3413195312023163, acc: 0.8928571343421936)
[2025-01-02 00:46:35,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:35,546][root][INFO] - Training Epoch: 2/2, step 183/574 completed (loss: 0.4495880901813507, acc: 0.8780487775802612)
[2025-01-02 00:46:35,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:35,947][root][INFO] - Training Epoch: 2/2, step 184/574 completed (loss: 0.553597092628479, acc: 0.8580060601234436)
[2025-01-02 00:46:36,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:36,342][root][INFO] - Training Epoch: 2/2, step 185/574 completed (loss: 0.6147966384887695, acc: 0.8443803787231445)
[2025-01-02 00:46:36,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:36,846][root][INFO] - Training Epoch: 2/2, step 186/574 completed (loss: 0.5313649773597717, acc: 0.8031250238418579)
[2025-01-02 00:46:37,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:37,374][root][INFO] - Training Epoch: 2/2, step 187/574 completed (loss: 0.5826783180236816, acc: 0.8386491537094116)
[2025-01-02 00:46:37,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:37,785][root][INFO] - Training Epoch: 2/2, step 188/574 completed (loss: 0.6797831058502197, acc: 0.7971529960632324)
[2025-01-02 00:46:37,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:38,174][root][INFO] - Training Epoch: 2/2, step 189/574 completed (loss: 0.6838026642799377, acc: 0.7599999904632568)
[2025-01-02 00:46:38,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:38,723][root][INFO] - Training Epoch: 2/2, step 190/574 completed (loss: 1.2994730472564697, acc: 0.604651153087616)
[2025-01-02 00:46:38,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:39,522][root][INFO] - Training Epoch: 2/2, step 191/574 completed (loss: 1.8537015914916992, acc: 0.5079365372657776)
[2025-01-02 00:46:39,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:40,476][root][INFO] - Training Epoch: 2/2, step 192/574 completed (loss: 1.4746090173721313, acc: 0.5757575631141663)
[2025-01-02 00:46:40,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:41,217][root][INFO] - Training Epoch: 2/2, step 193/574 completed (loss: 1.168418288230896, acc: 0.6352941393852234)
[2025-01-02 00:46:41,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:42,293][root][INFO] - Training Epoch: 2/2, step 194/574 completed (loss: 1.310115098953247, acc: 0.604938268661499)
[2025-01-02 00:46:42,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:43,245][root][INFO] - Training Epoch: 2/2, step 195/574 completed (loss: 0.82551109790802, acc: 0.725806474685669)
[2025-01-02 00:46:43,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:43,599][root][INFO] - Training Epoch: 2/2, step 196/574 completed (loss: 0.39548900723457336, acc: 0.8928571343421936)
[2025-01-02 00:46:43,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:43,965][root][INFO] - Training Epoch: 2/2, step 197/574 completed (loss: 1.2718628644943237, acc: 0.625)
[2025-01-02 00:46:44,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:44,373][root][INFO] - Training Epoch: 2/2, step 198/574 completed (loss: 1.120703101158142, acc: 0.6911764740943909)
[2025-01-02 00:46:44,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:44,747][root][INFO] - Training Epoch: 2/2, step 199/574 completed (loss: 1.2111268043518066, acc: 0.6985294222831726)
[2025-01-02 00:46:44,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:45,077][root][INFO] - Training Epoch: 2/2, step 200/574 completed (loss: 1.0008800029754639, acc: 0.6864407062530518)
[2025-01-02 00:46:45,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:45,457][root][INFO] - Training Epoch: 2/2, step 201/574 completed (loss: 1.1266902685165405, acc: 0.7089552283287048)
[2025-01-02 00:46:45,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:45,836][root][INFO] - Training Epoch: 2/2, step 202/574 completed (loss: 1.1111516952514648, acc: 0.7281553149223328)
[2025-01-02 00:46:45,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:46,149][root][INFO] - Training Epoch: 2/2, step 203/574 completed (loss: 0.9769495129585266, acc: 0.7142857313156128)
[2025-01-02 00:46:46,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:46,494][root][INFO] - Training Epoch: 2/2, step 204/574 completed (loss: 0.2022113800048828, acc: 0.9560439586639404)
[2025-01-02 00:46:46,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:46,852][root][INFO] - Training Epoch: 2/2, step 205/574 completed (loss: 0.44021832942962646, acc: 0.8834080696105957)
[2025-01-02 00:46:46,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:47,260][root][INFO] - Training Epoch: 2/2, step 206/574 completed (loss: 0.5160427093505859, acc: 0.8543307185173035)
[2025-01-02 00:46:47,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:47,605][root][INFO] - Training Epoch: 2/2, step 207/574 completed (loss: 0.5217982530593872, acc: 0.8663793206214905)
[2025-01-02 00:46:47,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:47,973][root][INFO] - Training Epoch: 2/2, step 208/574 completed (loss: 0.5748584866523743, acc: 0.8695651888847351)
[2025-01-02 00:46:48,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:48,305][root][INFO] - Training Epoch: 2/2, step 209/574 completed (loss: 0.5150178074836731, acc: 0.8638132214546204)
[2025-01-02 00:46:48,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:48,595][root][INFO] - Training Epoch: 2/2, step 210/574 completed (loss: 0.45107510685920715, acc: 0.9130434989929199)
[2025-01-02 00:46:48,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:48,935][root][INFO] - Training Epoch: 2/2, step 211/574 completed (loss: 0.5547838807106018, acc: 0.8260869383811951)
[2025-01-02 00:46:49,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:49,330][root][INFO] - Training Epoch: 2/2, step 212/574 completed (loss: 0.24880042672157288, acc: 0.9285714030265808)
[2025-01-02 00:46:49,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:49,728][root][INFO] - Training Epoch: 2/2, step 213/574 completed (loss: 0.3117772936820984, acc: 0.914893627166748)
[2025-01-02 00:46:49,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:50,467][root][INFO] - Training Epoch: 2/2, step 214/574 completed (loss: 0.33827418088912964, acc: 0.9153845906257629)
[2025-01-02 00:46:50,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:50,850][root][INFO] - Training Epoch: 2/2, step 215/574 completed (loss: 0.37115585803985596, acc: 0.8918918967247009)
[2025-01-02 00:46:50,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:51,253][root][INFO] - Training Epoch: 2/2, step 216/574 completed (loss: 0.34776297211647034, acc: 0.895348846912384)
[2025-01-02 00:46:51,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:51,787][root][INFO] - Training Epoch: 2/2, step 217/574 completed (loss: 0.4420880973339081, acc: 0.9009009003639221)
[2025-01-02 00:46:51,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:52,173][root][INFO] - Training Epoch: 2/2, step 218/574 completed (loss: 0.24332386255264282, acc: 0.9333333373069763)
[2025-01-02 00:46:52,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:52,507][root][INFO] - Training Epoch: 2/2, step 219/574 completed (loss: 0.2996911406517029, acc: 0.9090909361839294)
[2025-01-02 00:46:52,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:52,863][root][INFO] - Training Epoch: 2/2, step 220/574 completed (loss: 0.2599581182003021, acc: 0.8518518805503845)
[2025-01-02 00:46:52,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:53,267][root][INFO] - Training Epoch: 2/2, step 221/574 completed (loss: 0.3187073767185211, acc: 0.8399999737739563)
[2025-01-02 00:46:53,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:53,617][root][INFO] - Training Epoch: 2/2, step 222/574 completed (loss: 1.0362471342086792, acc: 0.692307710647583)
[2025-01-02 00:46:53,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:54,371][root][INFO] - Training Epoch: 2/2, step 223/574 completed (loss: 0.5044193863868713, acc: 0.8804348111152649)
[2025-01-02 00:46:54,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:54,924][root][INFO] - Training Epoch: 2/2, step 224/574 completed (loss: 0.7234319448471069, acc: 0.7613636255264282)
[2025-01-02 00:46:55,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:55,362][root][INFO] - Training Epoch: 2/2, step 225/574 completed (loss: 1.0311483144760132, acc: 0.7659574747085571)
[2025-01-02 00:46:55,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:55,777][root][INFO] - Training Epoch: 2/2, step 226/574 completed (loss: 0.8231806755065918, acc: 0.7735849022865295)
[2025-01-02 00:46:55,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:56,135][root][INFO] - Training Epoch: 2/2, step 227/574 completed (loss: 0.516089677810669, acc: 0.800000011920929)
[2025-01-02 00:46:56,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:56,456][root][INFO] - Training Epoch: 2/2, step 228/574 completed (loss: 0.4088277518749237, acc: 0.8604651093482971)
[2025-01-02 00:46:56,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:56,809][root][INFO] - Training Epoch: 2/2, step 229/574 completed (loss: 1.7844713926315308, acc: 0.46666666865348816)
[2025-01-02 00:46:56,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:57,180][root][INFO] - Training Epoch: 2/2, step 230/574 completed (loss: 2.1617705821990967, acc: 0.46315789222717285)
[2025-01-02 00:46:57,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:57,544][root][INFO] - Training Epoch: 2/2, step 231/574 completed (loss: 1.6447198390960693, acc: 0.5777778029441833)
[2025-01-02 00:46:57,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:57,962][root][INFO] - Training Epoch: 2/2, step 232/574 completed (loss: 1.6778175830841064, acc: 0.5222222208976746)
[2025-01-02 00:46:58,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:58,449][root][INFO] - Training Epoch: 2/2, step 233/574 completed (loss: 1.9350014925003052, acc: 0.47706422209739685)
[2025-01-02 00:46:58,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:58,920][root][INFO] - Training Epoch: 2/2, step 234/574 completed (loss: 1.6489176750183105, acc: 0.5230769515037537)
[2025-01-02 00:46:59,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:59,270][root][INFO] - Training Epoch: 2/2, step 235/574 completed (loss: 0.6037936806678772, acc: 0.8421052694320679)
[2025-01-02 00:46:59,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:59,624][root][INFO] - Training Epoch: 2/2, step 236/574 completed (loss: 0.7434808611869812, acc: 0.75)
[2025-01-02 00:46:59,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:59,973][root][INFO] - Training Epoch: 2/2, step 237/574 completed (loss: 1.4272652864456177, acc: 0.6363636255264282)
[2025-01-02 00:47:00,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:00,341][root][INFO] - Training Epoch: 2/2, step 238/574 completed (loss: 0.83221435546875, acc: 0.7777777910232544)
[2025-01-02 00:47:00,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:00,705][root][INFO] - Training Epoch: 2/2, step 239/574 completed (loss: 1.033820390701294, acc: 0.7142857313156128)
[2025-01-02 00:47:00,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:01,074][root][INFO] - Training Epoch: 2/2, step 240/574 completed (loss: 1.2613255977630615, acc: 0.6590909361839294)
[2025-01-02 00:47:01,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:01,435][root][INFO] - Training Epoch: 2/2, step 241/574 completed (loss: 0.8955293297767639, acc: 0.7272727489471436)
[2025-01-02 00:47:01,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:02,034][root][INFO] - Training Epoch: 2/2, step 242/574 completed (loss: 1.4194068908691406, acc: 0.5645161271095276)
[2025-01-02 00:47:02,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:02,569][root][INFO] - Training Epoch: 2/2, step 243/574 completed (loss: 1.4863208532333374, acc: 0.5909090638160706)
[2025-01-02 00:47:02,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:02,926][root][INFO] - Training Epoch: 2/2, step 244/574 completed (loss: 0.16377563774585724, acc: 0.9523809552192688)
[2025-01-02 00:47:03,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:03,318][root][INFO] - Training Epoch: 2/2, step 245/574 completed (loss: 0.674846887588501, acc: 0.7692307829856873)
[2025-01-02 00:47:03,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:03,675][root][INFO] - Training Epoch: 2/2, step 246/574 completed (loss: 0.484453409910202, acc: 0.8709677457809448)
[2025-01-02 00:47:03,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:03,971][root][INFO] - Training Epoch: 2/2, step 247/574 completed (loss: 0.47655004262924194, acc: 0.800000011920929)
[2025-01-02 00:47:04,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:04,370][root][INFO] - Training Epoch: 2/2, step 248/574 completed (loss: 0.4059503674507141, acc: 0.9189189076423645)
[2025-01-02 00:47:04,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:04,750][root][INFO] - Training Epoch: 2/2, step 249/574 completed (loss: 0.5715776085853577, acc: 0.8648648858070374)
[2025-01-02 00:47:04,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:05,136][root][INFO] - Training Epoch: 2/2, step 250/574 completed (loss: 0.15919199585914612, acc: 1.0)
[2025-01-02 00:47:05,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:05,503][root][INFO] - Training Epoch: 2/2, step 251/574 completed (loss: 0.470597505569458, acc: 0.8382353186607361)
[2025-01-02 00:47:05,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:05,851][root][INFO] - Training Epoch: 2/2, step 252/574 completed (loss: 0.2181006371974945, acc: 0.9268292784690857)
[2025-01-02 00:47:05,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:06,157][root][INFO] - Training Epoch: 2/2, step 253/574 completed (loss: 0.15158426761627197, acc: 0.9200000166893005)
[2025-01-02 00:47:06,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:06,495][root][INFO] - Training Epoch: 2/2, step 254/574 completed (loss: 0.06938344985246658, acc: 0.9599999785423279)
[2025-01-02 00:47:06,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:06,879][root][INFO] - Training Epoch: 2/2, step 255/574 completed (loss: 0.30988022685050964, acc: 0.8709677457809448)
[2025-01-02 00:47:06,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:07,234][root][INFO] - Training Epoch: 2/2, step 256/574 completed (loss: 0.35594311356544495, acc: 0.9122806787490845)
[2025-01-02 00:47:07,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:07,547][root][INFO] - Training Epoch: 2/2, step 257/574 completed (loss: 0.2923228144645691, acc: 0.8999999761581421)
[2025-01-02 00:47:07,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:07,891][root][INFO] - Training Epoch: 2/2, step 258/574 completed (loss: 0.23984374105930328, acc: 0.9210526347160339)
[2025-01-02 00:47:08,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:08,461][root][INFO] - Training Epoch: 2/2, step 259/574 completed (loss: 0.5169633030891418, acc: 0.8867924809455872)
[2025-01-02 00:47:08,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:09,040][root][INFO] - Training Epoch: 2/2, step 260/574 completed (loss: 0.48412826657295227, acc: 0.8833333253860474)
[2025-01-02 00:47:09,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:09,378][root][INFO] - Training Epoch: 2/2, step 261/574 completed (loss: 0.2616880536079407, acc: 0.9166666865348816)
[2025-01-02 00:47:09,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:09,685][root][INFO] - Training Epoch: 2/2, step 262/574 completed (loss: 0.898044228553772, acc: 0.7419354915618896)
[2025-01-02 00:47:09,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:10,043][root][INFO] - Training Epoch: 2/2, step 263/574 completed (loss: 1.6264326572418213, acc: 0.653333306312561)
[2025-01-02 00:47:10,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:10,454][root][INFO] - Training Epoch: 2/2, step 264/574 completed (loss: 0.9267535209655762, acc: 0.6458333134651184)
[2025-01-02 00:47:10,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:11,334][root][INFO] - Training Epoch: 2/2, step 265/574 completed (loss: 1.5329816341400146, acc: 0.5759999752044678)
[2025-01-02 00:47:11,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:11,690][root][INFO] - Training Epoch: 2/2, step 266/574 completed (loss: 1.7422881126403809, acc: 0.584269642829895)
[2025-01-02 00:47:11,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:12,062][root][INFO] - Training Epoch: 2/2, step 267/574 completed (loss: 1.332161545753479, acc: 0.5810810923576355)
[2025-01-02 00:47:12,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:12,516][root][INFO] - Training Epoch: 2/2, step 268/574 completed (loss: 0.9834010601043701, acc: 0.7068965435028076)
[2025-01-02 00:47:12,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:12,882][root][INFO] - Training Epoch: 2/2, step 269/574 completed (loss: 0.27745354175567627, acc: 0.9090909361839294)
[2025-01-02 00:47:12,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:13,263][root][INFO] - Training Epoch: 2/2, step 270/574 completed (loss: 0.2696075141429901, acc: 0.9090909361839294)
[2025-01-02 00:47:13,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:13,655][root][INFO] - Training Epoch: 2/2, step 271/574 completed (loss: 0.32025620341300964, acc: 0.90625)
[2025-01-02 00:47:13,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:14,035][root][INFO] - Training Epoch: 2/2, step 272/574 completed (loss: 0.2315036505460739, acc: 0.9666666388511658)
[2025-01-02 00:47:14,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:14,426][root][INFO] - Training Epoch: 2/2, step 273/574 completed (loss: 0.44091513752937317, acc: 0.9166666865348816)
[2025-01-02 00:47:14,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:14,772][root][INFO] - Training Epoch: 2/2, step 274/574 completed (loss: 0.47320517897605896, acc: 0.84375)
[2025-01-02 00:47:14,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:15,123][root][INFO] - Training Epoch: 2/2, step 275/574 completed (loss: 0.3550277650356293, acc: 0.9333333373069763)
[2025-01-02 00:47:15,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:15,470][root][INFO] - Training Epoch: 2/2, step 276/574 completed (loss: 0.5499384999275208, acc: 0.8965517282485962)
[2025-01-02 00:47:15,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:15,825][root][INFO] - Training Epoch: 2/2, step 277/574 completed (loss: 0.2802356779575348, acc: 0.9599999785423279)
[2025-01-02 00:47:15,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:16,181][root][INFO] - Training Epoch: 2/2, step 278/574 completed (loss: 0.7449029088020325, acc: 0.8085106611251831)
[2025-01-02 00:47:16,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:16,560][root][INFO] - Training Epoch: 2/2, step 279/574 completed (loss: 0.6255424618721008, acc: 0.8541666865348816)
[2025-01-02 00:47:16,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:16,908][root][INFO] - Training Epoch: 2/2, step 280/574 completed (loss: 0.2743220925331116, acc: 0.9318181872367859)
[2025-01-02 00:47:17,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:17,322][root][INFO] - Training Epoch: 2/2, step 281/574 completed (loss: 0.994485080242157, acc: 0.7228915691375732)
[2025-01-02 00:47:17,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:17,685][root][INFO] - Training Epoch: 2/2, step 282/574 completed (loss: 1.1246556043624878, acc: 0.7129629850387573)
[2025-01-02 00:47:17,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:18,028][root][INFO] - Training Epoch: 2/2, step 283/574 completed (loss: 0.27558279037475586, acc: 0.9736841917037964)
[2025-01-02 00:47:18,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:19,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:19,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:19,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:19,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:20,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:20,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:21,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:21,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:21,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:21,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:22,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:22,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:23,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:23,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:23,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:23,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:24,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:24,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:24,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:25,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:25,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:25,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:26,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:26,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:26,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:27,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:27,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:27,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:28,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:28,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:28,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:29,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:29,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:29,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:29,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:30,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:30,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:31,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:31,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:31,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:32,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:32,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:32,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:33,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:33,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:33,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:34,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:34,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:34,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:35,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:35,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:35,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:36,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:36,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:36,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:37,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:37,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:37,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:38,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:38,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:38,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:39,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:39,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:40,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:40,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:40,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:41,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:41,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:41,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:42,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:42,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:42,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:43,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:43,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:43,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:44,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:44,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:44,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:45,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:45,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:45,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:45,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:46,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:46,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:47,559][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9307, device='cuda:0') eval_epoch_loss=tensor(0.6579, device='cuda:0') eval_epoch_acc=tensor(0.8161, device='cuda:0')
[2025-01-02 00:47:47,560][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:47:47,561][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:47:47,786][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.6578754186630249/model.pt
[2025-01-02 00:47:47,792][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:47:47,792][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.6578754186630249
[2025-01-02 00:47:47,793][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8160508275032043
[2025-01-02 00:47:47,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:48,159][root][INFO] - Training Epoch: 2/2, step 284/574 completed (loss: 0.5990118980407715, acc: 0.8235294222831726)
[2025-01-02 00:47:48,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:48,525][root][INFO] - Training Epoch: 2/2, step 285/574 completed (loss: 0.5418704748153687, acc: 0.8999999761581421)
[2025-01-02 00:47:48,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:48,944][root][INFO] - Training Epoch: 2/2, step 286/574 completed (loss: 0.6615339517593384, acc: 0.8203125)
[2025-01-02 00:47:49,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:49,341][root][INFO] - Training Epoch: 2/2, step 287/574 completed (loss: 0.6842729449272156, acc: 0.8159999847412109)
[2025-01-02 00:47:49,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:49,695][root][INFO] - Training Epoch: 2/2, step 288/574 completed (loss: 0.6449192762374878, acc: 0.8131868243217468)
[2025-01-02 00:47:49,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:49,971][root][INFO] - Training Epoch: 2/2, step 289/574 completed (loss: 0.6195071935653687, acc: 0.8260869383811951)
[2025-01-02 00:47:50,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:50,340][root][INFO] - Training Epoch: 2/2, step 290/574 completed (loss: 0.6437070965766907, acc: 0.8195876479148865)
[2025-01-02 00:47:50,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:50,676][root][INFO] - Training Epoch: 2/2, step 291/574 completed (loss: 0.1234099343419075, acc: 0.9545454382896423)
[2025-01-02 00:47:50,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:50,990][root][INFO] - Training Epoch: 2/2, step 292/574 completed (loss: 0.7898426055908203, acc: 0.8095238208770752)
[2025-01-02 00:47:51,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:51,334][root][INFO] - Training Epoch: 2/2, step 293/574 completed (loss: 0.294440895318985, acc: 0.931034505367279)
[2025-01-02 00:47:51,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:51,809][root][INFO] - Training Epoch: 2/2, step 294/574 completed (loss: 0.6962355375289917, acc: 0.800000011920929)
[2025-01-02 00:47:51,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:52,366][root][INFO] - Training Epoch: 2/2, step 295/574 completed (loss: 0.7055964469909668, acc: 0.8144329786300659)
[2025-01-02 00:47:52,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:52,688][root][INFO] - Training Epoch: 2/2, step 296/574 completed (loss: 0.6044155359268188, acc: 0.8103448152542114)
[2025-01-02 00:47:52,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:53,082][root][INFO] - Training Epoch: 2/2, step 297/574 completed (loss: 0.19098952412605286, acc: 0.9629629850387573)
[2025-01-02 00:47:53,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:53,459][root][INFO] - Training Epoch: 2/2, step 298/574 completed (loss: 0.6317788362503052, acc: 0.7368420958518982)
[2025-01-02 00:47:53,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:53,812][root][INFO] - Training Epoch: 2/2, step 299/574 completed (loss: 0.2689908742904663, acc: 0.9285714030265808)
[2025-01-02 00:47:53,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:54,167][root][INFO] - Training Epoch: 2/2, step 300/574 completed (loss: 0.22139519453048706, acc: 0.875)
[2025-01-02 00:47:54,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:54,515][root][INFO] - Training Epoch: 2/2, step 301/574 completed (loss: 0.5770386457443237, acc: 0.849056601524353)
[2025-01-02 00:47:54,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:54,856][root][INFO] - Training Epoch: 2/2, step 302/574 completed (loss: 0.07491473108530045, acc: 0.9811320900917053)
[2025-01-02 00:47:54,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:55,238][root][INFO] - Training Epoch: 2/2, step 303/574 completed (loss: 0.13929474353790283, acc: 0.9411764740943909)
[2025-01-02 00:47:55,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:55,589][root][INFO] - Training Epoch: 2/2, step 304/574 completed (loss: 0.30400726199150085, acc: 0.90625)
[2025-01-02 00:47:55,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:55,937][root][INFO] - Training Epoch: 2/2, step 305/574 completed (loss: 0.5692434310913086, acc: 0.8360655903816223)
[2025-01-02 00:47:56,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:56,284][root][INFO] - Training Epoch: 2/2, step 306/574 completed (loss: 0.1702529639005661, acc: 0.9333333373069763)
[2025-01-02 00:47:56,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:56,636][root][INFO] - Training Epoch: 2/2, step 307/574 completed (loss: 0.03772460296750069, acc: 1.0)
[2025-01-02 00:47:56,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:56,972][root][INFO] - Training Epoch: 2/2, step 308/574 completed (loss: 0.468142569065094, acc: 0.8695651888847351)
[2025-01-02 00:47:57,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:57,380][root][INFO] - Training Epoch: 2/2, step 309/574 completed (loss: 0.5173702836036682, acc: 0.8888888955116272)
[2025-01-02 00:47:57,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:57,775][root][INFO] - Training Epoch: 2/2, step 310/574 completed (loss: 0.38378578424453735, acc: 0.8674699068069458)
[2025-01-02 00:47:57,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:58,117][root][INFO] - Training Epoch: 2/2, step 311/574 completed (loss: 0.5052756071090698, acc: 0.807692289352417)
[2025-01-02 00:47:58,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:58,492][root][INFO] - Training Epoch: 2/2, step 312/574 completed (loss: 0.16356372833251953, acc: 0.9591836929321289)
[2025-01-02 00:47:58,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:58,849][root][INFO] - Training Epoch: 2/2, step 313/574 completed (loss: 0.030792566016316414, acc: 1.0)
[2025-01-02 00:47:58,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:59,180][root][INFO] - Training Epoch: 2/2, step 314/574 completed (loss: 0.21877919137477875, acc: 0.9166666865348816)
[2025-01-02 00:47:59,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:59,524][root][INFO] - Training Epoch: 2/2, step 315/574 completed (loss: 0.301364928483963, acc: 0.9677419066429138)
[2025-01-02 00:47:59,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:59,860][root][INFO] - Training Epoch: 2/2, step 316/574 completed (loss: 0.734933614730835, acc: 0.8064516186714172)
[2025-01-02 00:47:59,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:00,243][root][INFO] - Training Epoch: 2/2, step 317/574 completed (loss: 0.35470154881477356, acc: 0.8805969953536987)
[2025-01-02 00:48:00,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:00,601][root][INFO] - Training Epoch: 2/2, step 318/574 completed (loss: 0.12660542130470276, acc: 0.9615384340286255)
[2025-01-02 00:48:00,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:00,906][root][INFO] - Training Epoch: 2/2, step 319/574 completed (loss: 0.3009259104728699, acc: 0.9333333373069763)
[2025-01-02 00:48:01,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:01,261][root][INFO] - Training Epoch: 2/2, step 320/574 completed (loss: 0.29672273993492126, acc: 0.9193548560142517)
[2025-01-02 00:48:01,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:01,634][root][INFO] - Training Epoch: 2/2, step 321/574 completed (loss: 0.02773737907409668, acc: 1.0)
[2025-01-02 00:48:01,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:01,957][root][INFO] - Training Epoch: 2/2, step 322/574 completed (loss: 1.5351834297180176, acc: 0.5925925970077515)
[2025-01-02 00:48:02,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:02,275][root][INFO] - Training Epoch: 2/2, step 323/574 completed (loss: 2.12319016456604, acc: 0.4285714328289032)
[2025-01-02 00:48:02,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:02,653][root][INFO] - Training Epoch: 2/2, step 324/574 completed (loss: 1.637305736541748, acc: 0.6153846383094788)
[2025-01-02 00:48:02,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:02,952][root][INFO] - Training Epoch: 2/2, step 325/574 completed (loss: 1.6535040140151978, acc: 0.5609756112098694)
[2025-01-02 00:48:03,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:03,270][root][INFO] - Training Epoch: 2/2, step 326/574 completed (loss: 1.499983787536621, acc: 0.6052631735801697)
[2025-01-02 00:48:03,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:03,605][root][INFO] - Training Epoch: 2/2, step 327/574 completed (loss: 0.5348008275032043, acc: 0.8947368264198303)
[2025-01-02 00:48:03,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:03,940][root][INFO] - Training Epoch: 2/2, step 328/574 completed (loss: 0.12355675548315048, acc: 0.9642857313156128)
[2025-01-02 00:48:04,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:04,258][root][INFO] - Training Epoch: 2/2, step 329/574 completed (loss: 0.2643274664878845, acc: 0.8518518805503845)
[2025-01-02 00:48:04,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:04,629][root][INFO] - Training Epoch: 2/2, step 330/574 completed (loss: 0.0752735286951065, acc: 0.96875)
[2025-01-02 00:48:04,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:05,011][root][INFO] - Training Epoch: 2/2, step 331/574 completed (loss: 0.3570118844509125, acc: 0.8870967626571655)
[2025-01-02 00:48:05,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:05,405][root][INFO] - Training Epoch: 2/2, step 332/574 completed (loss: 0.2022656351327896, acc: 0.9473684430122375)
[2025-01-02 00:48:05,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:05,763][root][INFO] - Training Epoch: 2/2, step 333/574 completed (loss: 0.5564901232719421, acc: 0.875)
[2025-01-02 00:48:05,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:06,117][root][INFO] - Training Epoch: 2/2, step 334/574 completed (loss: 0.2682091295719147, acc: 0.9333333373069763)
[2025-01-02 00:48:06,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:06,492][root][INFO] - Training Epoch: 2/2, step 335/574 completed (loss: 0.5474388599395752, acc: 0.8947368264198303)
[2025-01-02 00:48:06,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:06,884][root][INFO] - Training Epoch: 2/2, step 336/574 completed (loss: 1.0090289115905762, acc: 0.6800000071525574)
[2025-01-02 00:48:06,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:07,235][root][INFO] - Training Epoch: 2/2, step 337/574 completed (loss: 1.4837937355041504, acc: 0.5977011322975159)
[2025-01-02 00:48:07,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:07,592][root][INFO] - Training Epoch: 2/2, step 338/574 completed (loss: 1.3380897045135498, acc: 0.6063829660415649)
[2025-01-02 00:48:07,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:07,972][root][INFO] - Training Epoch: 2/2, step 339/574 completed (loss: 1.5413577556610107, acc: 0.6144578456878662)
[2025-01-02 00:48:08,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:08,372][root][INFO] - Training Epoch: 2/2, step 340/574 completed (loss: 0.08624375611543655, acc: 1.0)
[2025-01-02 00:48:08,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:08,729][root][INFO] - Training Epoch: 2/2, step 341/574 completed (loss: 0.7511066794395447, acc: 0.7948718070983887)
[2025-01-02 00:48:08,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:09,106][root][INFO] - Training Epoch: 2/2, step 342/574 completed (loss: 0.5061196684837341, acc: 0.8554216623306274)
[2025-01-02 00:48:09,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:09,446][root][INFO] - Training Epoch: 2/2, step 343/574 completed (loss: 0.8414302468299866, acc: 0.7358490824699402)
[2025-01-02 00:48:09,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:09,764][root][INFO] - Training Epoch: 2/2, step 344/574 completed (loss: 0.17948202788829803, acc: 0.949367105960846)
[2025-01-02 00:48:09,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:10,111][root][INFO] - Training Epoch: 2/2, step 345/574 completed (loss: 0.1154036745429039, acc: 0.9803921580314636)
[2025-01-02 00:48:10,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:10,485][root][INFO] - Training Epoch: 2/2, step 346/574 completed (loss: 0.5462613105773926, acc: 0.8656716346740723)
[2025-01-02 00:48:10,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:10,861][root][INFO] - Training Epoch: 2/2, step 347/574 completed (loss: 0.14793702960014343, acc: 0.949999988079071)
[2025-01-02 00:48:10,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:11,266][root][INFO] - Training Epoch: 2/2, step 348/574 completed (loss: 0.2330361008644104, acc: 0.9599999785423279)
[2025-01-02 00:48:11,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:11,659][root][INFO] - Training Epoch: 2/2, step 349/574 completed (loss: 0.9722051620483398, acc: 0.75)
[2025-01-02 00:48:11,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:12,045][root][INFO] - Training Epoch: 2/2, step 350/574 completed (loss: 0.9408183693885803, acc: 0.7441860437393188)
[2025-01-02 00:48:12,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:12,419][root][INFO] - Training Epoch: 2/2, step 351/574 completed (loss: 0.41816744208335876, acc: 0.8717948794364929)
[2025-01-02 00:48:12,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:12,807][root][INFO] - Training Epoch: 2/2, step 352/574 completed (loss: 1.469020128250122, acc: 0.5555555820465088)
[2025-01-02 00:48:12,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:13,183][root][INFO] - Training Epoch: 2/2, step 353/574 completed (loss: 0.23143640160560608, acc: 0.95652174949646)
[2025-01-02 00:48:13,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:13,530][root][INFO] - Training Epoch: 2/2, step 354/574 completed (loss: 0.5376570224761963, acc: 0.7692307829856873)
[2025-01-02 00:48:13,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:13,880][root][INFO] - Training Epoch: 2/2, step 355/574 completed (loss: 1.009989857673645, acc: 0.692307710647583)
[2025-01-02 00:48:14,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:14,380][root][INFO] - Training Epoch: 2/2, step 356/574 completed (loss: 0.7981839179992676, acc: 0.747826099395752)
[2025-01-02 00:48:14,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:14,757][root][INFO] - Training Epoch: 2/2, step 357/574 completed (loss: 0.615811288356781, acc: 0.8260869383811951)
[2025-01-02 00:48:14,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:15,153][root][INFO] - Training Epoch: 2/2, step 358/574 completed (loss: 0.7782459855079651, acc: 0.7551020383834839)
[2025-01-02 00:48:15,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:15,492][root][INFO] - Training Epoch: 2/2, step 359/574 completed (loss: 0.010184361599385738, acc: 1.0)
[2025-01-02 00:48:15,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:15,838][root][INFO] - Training Epoch: 2/2, step 360/574 completed (loss: 0.3157027065753937, acc: 0.9230769276618958)
[2025-01-02 00:48:15,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:16,180][root][INFO] - Training Epoch: 2/2, step 361/574 completed (loss: 0.6263071298599243, acc: 0.8048780560493469)
[2025-01-02 00:48:16,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:16,547][root][INFO] - Training Epoch: 2/2, step 362/574 completed (loss: 0.5536567568778992, acc: 0.8666666746139526)
[2025-01-02 00:48:16,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:16,886][root][INFO] - Training Epoch: 2/2, step 363/574 completed (loss: 0.27019283175468445, acc: 0.9078947305679321)
[2025-01-02 00:48:16,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:17,246][root][INFO] - Training Epoch: 2/2, step 364/574 completed (loss: 0.1735139638185501, acc: 0.9268292784690857)
[2025-01-02 00:48:17,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:17,623][root][INFO] - Training Epoch: 2/2, step 365/574 completed (loss: 0.19815589487552643, acc: 0.9696969985961914)
[2025-01-02 00:48:17,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:17,984][root][INFO] - Training Epoch: 2/2, step 366/574 completed (loss: 0.010017733089625835, acc: 1.0)
[2025-01-02 00:48:18,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:18,320][root][INFO] - Training Epoch: 2/2, step 367/574 completed (loss: 0.08799762278795242, acc: 0.95652174949646)
[2025-01-02 00:48:18,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:18,713][root][INFO] - Training Epoch: 2/2, step 368/574 completed (loss: 0.13878293335437775, acc: 0.9642857313156128)
[2025-01-02 00:48:18,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:19,089][root][INFO] - Training Epoch: 2/2, step 369/574 completed (loss: 0.4520649313926697, acc: 0.875)
[2025-01-02 00:48:19,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:19,699][root][INFO] - Training Epoch: 2/2, step 370/574 completed (loss: 0.6813608407974243, acc: 0.8121212124824524)
[2025-01-02 00:48:19,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:20,569][root][INFO] - Training Epoch: 2/2, step 371/574 completed (loss: 0.4775567650794983, acc: 0.8773584961891174)
[2025-01-02 00:48:20,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:20,931][root][INFO] - Training Epoch: 2/2, step 372/574 completed (loss: 0.26357516646385193, acc: 0.9222221970558167)
[2025-01-02 00:48:21,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:21,274][root][INFO] - Training Epoch: 2/2, step 373/574 completed (loss: 0.3044445514678955, acc: 0.9642857313156128)
[2025-01-02 00:48:21,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:21,616][root][INFO] - Training Epoch: 2/2, step 374/574 completed (loss: 0.2739754021167755, acc: 0.9142857193946838)
[2025-01-02 00:48:21,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:21,928][root][INFO] - Training Epoch: 2/2, step 375/574 completed (loss: 0.008376657031476498, acc: 1.0)
[2025-01-02 00:48:22,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:22,233][root][INFO] - Training Epoch: 2/2, step 376/574 completed (loss: 0.11987566947937012, acc: 0.9130434989929199)
[2025-01-02 00:48:22,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:22,537][root][INFO] - Training Epoch: 2/2, step 377/574 completed (loss: 0.18246476352214813, acc: 0.8958333134651184)
[2025-01-02 00:48:22,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:22,850][root][INFO] - Training Epoch: 2/2, step 378/574 completed (loss: 0.0995660275220871, acc: 0.9684210419654846)
[2025-01-02 00:48:22,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:23,418][root][INFO] - Training Epoch: 2/2, step 379/574 completed (loss: 0.35844436287879944, acc: 0.8982036113739014)
[2025-01-02 00:48:23,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:23,818][root][INFO] - Training Epoch: 2/2, step 380/574 completed (loss: 0.44310882687568665, acc: 0.9097744226455688)
[2025-01-02 00:48:24,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:25,044][root][INFO] - Training Epoch: 2/2, step 381/574 completed (loss: 0.7215206623077393, acc: 0.8235294222831726)
[2025-01-02 00:48:25,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:25,606][root][INFO] - Training Epoch: 2/2, step 382/574 completed (loss: 0.16626258194446564, acc: 0.9459459185600281)
[2025-01-02 00:48:25,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:25,928][root][INFO] - Training Epoch: 2/2, step 383/574 completed (loss: 0.5390798449516296, acc: 0.8571428656578064)
[2025-01-02 00:48:26,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:26,224][root][INFO] - Training Epoch: 2/2, step 384/574 completed (loss: 0.05125012993812561, acc: 1.0)
[2025-01-02 00:48:26,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:26,521][root][INFO] - Training Epoch: 2/2, step 385/574 completed (loss: 0.08312999457120895, acc: 0.96875)
[2025-01-02 00:48:26,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:26,818][root][INFO] - Training Epoch: 2/2, step 386/574 completed (loss: 0.023185541853308678, acc: 1.0)
[2025-01-02 00:48:26,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:27,117][root][INFO] - Training Epoch: 2/2, step 387/574 completed (loss: 0.04627525433897972, acc: 0.9736841917037964)
[2025-01-02 00:48:27,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:27,432][root][INFO] - Training Epoch: 2/2, step 388/574 completed (loss: 0.08834069222211838, acc: 0.9545454382896423)
[2025-01-02 00:48:27,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:27,784][root][INFO] - Training Epoch: 2/2, step 389/574 completed (loss: 0.014071579091250896, acc: 1.0)
[2025-01-02 00:48:27,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:28,133][root][INFO] - Training Epoch: 2/2, step 390/574 completed (loss: 0.30430588126182556, acc: 0.8571428656578064)
[2025-01-02 00:48:28,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:28,470][root][INFO] - Training Epoch: 2/2, step 391/574 completed (loss: 1.3259576559066772, acc: 0.6666666865348816)
[2025-01-02 00:48:28,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:28,860][root][INFO] - Training Epoch: 2/2, step 392/574 completed (loss: 1.198205590248108, acc: 0.6893203854560852)
[2025-01-02 00:48:28,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:29,383][root][INFO] - Training Epoch: 2/2, step 393/574 completed (loss: 1.1195297241210938, acc: 0.720588207244873)
[2025-01-02 00:48:29,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:29,748][root][INFO] - Training Epoch: 2/2, step 394/574 completed (loss: 1.058861255645752, acc: 0.6666666865348816)
[2025-01-02 00:48:29,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:30,131][root][INFO] - Training Epoch: 2/2, step 395/574 completed (loss: 1.0146396160125732, acc: 0.7569444179534912)
[2025-01-02 00:48:30,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:30,473][root][INFO] - Training Epoch: 2/2, step 396/574 completed (loss: 0.6774406433105469, acc: 0.7906976938247681)
[2025-01-02 00:48:30,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:30,786][root][INFO] - Training Epoch: 2/2, step 397/574 completed (loss: 0.2759086787700653, acc: 0.9166666865348816)
[2025-01-02 00:48:30,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:31,112][root][INFO] - Training Epoch: 2/2, step 398/574 completed (loss: 0.33219078183174133, acc: 0.8837209343910217)
[2025-01-02 00:48:31,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:31,450][root][INFO] - Training Epoch: 2/2, step 399/574 completed (loss: 0.2790674567222595, acc: 0.9200000166893005)
[2025-01-02 00:48:31,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:31,979][root][INFO] - Training Epoch: 2/2, step 400/574 completed (loss: 0.36517009139060974, acc: 0.8970588445663452)
[2025-01-02 00:48:32,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:32,302][root][INFO] - Training Epoch: 2/2, step 401/574 completed (loss: 0.5438112616539001, acc: 0.8266666531562805)
[2025-01-02 00:48:32,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:32,605][root][INFO] - Training Epoch: 2/2, step 402/574 completed (loss: 0.735771119594574, acc: 0.8787878751754761)
[2025-01-02 00:48:32,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:32,905][root][INFO] - Training Epoch: 2/2, step 403/574 completed (loss: 0.37137579917907715, acc: 0.9090909361839294)
[2025-01-02 00:48:32,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:33,212][root][INFO] - Training Epoch: 2/2, step 404/574 completed (loss: 0.40346068143844604, acc: 0.8709677457809448)
[2025-01-02 00:48:33,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:33,528][root][INFO] - Training Epoch: 2/2, step 405/574 completed (loss: 0.15819033980369568, acc: 0.9259259104728699)
[2025-01-02 00:48:33,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:33,826][root][INFO] - Training Epoch: 2/2, step 406/574 completed (loss: 0.21722137928009033, acc: 0.9599999785423279)
[2025-01-02 00:48:33,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:34,174][root][INFO] - Training Epoch: 2/2, step 407/574 completed (loss: 0.08935532718896866, acc: 0.9722222089767456)
[2025-01-02 00:48:34,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:34,486][root][INFO] - Training Epoch: 2/2, step 408/574 completed (loss: 0.25564122200012207, acc: 0.9259259104728699)
[2025-01-02 00:48:34,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:34,792][root][INFO] - Training Epoch: 2/2, step 409/574 completed (loss: 0.19020342826843262, acc: 0.9615384340286255)
[2025-01-02 00:48:34,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:35,095][root][INFO] - Training Epoch: 2/2, step 410/574 completed (loss: 0.21168409287929535, acc: 0.9655172228813171)
[2025-01-02 00:48:35,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:35,428][root][INFO] - Training Epoch: 2/2, step 411/574 completed (loss: 0.05376775190234184, acc: 1.0)
[2025-01-02 00:48:35,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:35,741][root][INFO] - Training Epoch: 2/2, step 412/574 completed (loss: 0.18381769955158234, acc: 0.9666666388511658)
[2025-01-02 00:48:35,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:36,051][root][INFO] - Training Epoch: 2/2, step 413/574 completed (loss: 0.2924279272556305, acc: 0.939393937587738)
[2025-01-02 00:48:36,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:36,353][root][INFO] - Training Epoch: 2/2, step 414/574 completed (loss: 0.12082414329051971, acc: 0.9545454382896423)
[2025-01-02 00:48:36,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:36,700][root][INFO] - Training Epoch: 2/2, step 415/574 completed (loss: 0.604336678981781, acc: 0.8235294222831726)
[2025-01-02 00:48:36,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:37,005][root][INFO] - Training Epoch: 2/2, step 416/574 completed (loss: 0.3830519914627075, acc: 0.8846153616905212)
[2025-01-02 00:48:37,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:37,326][root][INFO] - Training Epoch: 2/2, step 417/574 completed (loss: 0.34063395857810974, acc: 0.9444444179534912)
[2025-01-02 00:48:37,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:37,678][root][INFO] - Training Epoch: 2/2, step 418/574 completed (loss: 0.46317917108535767, acc: 0.8999999761581421)
[2025-01-02 00:48:37,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:37,985][root][INFO] - Training Epoch: 2/2, step 419/574 completed (loss: 0.4908730387687683, acc: 0.8999999761581421)
[2025-01-02 00:48:38,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:38,288][root][INFO] - Training Epoch: 2/2, step 420/574 completed (loss: 0.04957888275384903, acc: 1.0)
[2025-01-02 00:48:38,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:38,584][root][INFO] - Training Epoch: 2/2, step 421/574 completed (loss: 0.3043033480644226, acc: 0.9333333373069763)
[2025-01-02 00:48:38,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:38,874][root][INFO] - Training Epoch: 2/2, step 422/574 completed (loss: 0.3333136737346649, acc: 0.90625)
[2025-01-02 00:48:38,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:39,188][root][INFO] - Training Epoch: 2/2, step 423/574 completed (loss: 0.566709578037262, acc: 0.8611111044883728)
[2025-01-02 00:48:39,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:39,486][root][INFO] - Training Epoch: 2/2, step 424/574 completed (loss: 0.4973582923412323, acc: 0.9259259104728699)
[2025-01-02 00:48:39,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:39,796][root][INFO] - Training Epoch: 2/2, step 425/574 completed (loss: 0.17641279101371765, acc: 0.939393937587738)
[2025-01-02 00:48:39,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:40,104][root][INFO] - Training Epoch: 2/2, step 426/574 completed (loss: 0.022014213725924492, acc: 1.0)
[2025-01-02 00:48:40,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:41,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:41,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:41,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:42,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:42,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:42,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:43,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:43,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:43,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:44,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:44,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:44,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:45,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:45,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:45,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:46,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:46,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:46,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:47,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:47,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:47,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:48,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:48,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:48,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:49,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:49,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:49,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:50,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:50,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:50,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:51,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:51,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:51,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:52,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:52,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:53,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:53,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:53,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:54,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:54,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:54,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:55,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:55,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:55,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:56,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:56,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:56,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:57,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:57,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:57,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:57,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:58,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:58,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:58,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:59,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:59,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:59,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:00,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:00,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:01,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:01,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:01,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:02,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:02,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:02,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:03,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:03,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:03,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:04,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:04,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:05,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:05,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:05,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:05,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:06,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:06,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:06,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:07,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:07,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:07,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:08,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:08,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:08,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:09,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:09,718][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8662, device='cuda:0') eval_epoch_loss=tensor(0.6239, device='cuda:0') eval_epoch_acc=tensor(0.8262, device='cuda:0')
[2025-01-02 00:49:09,719][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:49:09,720][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:49:09,951][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_427_loss_0.6238870024681091/model.pt
[2025-01-02 00:49:09,960][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:49:09,961][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.6238870024681091
[2025-01-02 00:49:09,962][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.826225757598877
[2025-01-02 00:49:10,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:10,376][root][INFO] - Training Epoch: 2/2, step 427/574 completed (loss: 0.24313436448574066, acc: 0.9189189076423645)
[2025-01-02 00:49:10,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:10,706][root][INFO] - Training Epoch: 2/2, step 428/574 completed (loss: 0.0341838076710701, acc: 1.0)
[2025-01-02 00:49:10,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:11,058][root][INFO] - Training Epoch: 2/2, step 429/574 completed (loss: 0.3209170401096344, acc: 0.9130434989929199)
[2025-01-02 00:49:11,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:11,402][root][INFO] - Training Epoch: 2/2, step 430/574 completed (loss: 0.005056640598922968, acc: 1.0)
[2025-01-02 00:49:11,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:11,699][root][INFO] - Training Epoch: 2/2, step 431/574 completed (loss: 0.01781943254172802, acc: 1.0)
[2025-01-02 00:49:11,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:11,999][root][INFO] - Training Epoch: 2/2, step 432/574 completed (loss: 0.31653374433517456, acc: 0.9130434989929199)
[2025-01-02 00:49:12,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:12,368][root][INFO] - Training Epoch: 2/2, step 433/574 completed (loss: 0.26259666681289673, acc: 0.9166666865348816)
[2025-01-02 00:49:12,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:12,668][root][INFO] - Training Epoch: 2/2, step 434/574 completed (loss: 0.002150279004126787, acc: 1.0)
[2025-01-02 00:49:12,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:12,998][root][INFO] - Training Epoch: 2/2, step 435/574 completed (loss: 0.04477909579873085, acc: 0.9696969985961914)
[2025-01-02 00:49:13,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:13,314][root][INFO] - Training Epoch: 2/2, step 436/574 completed (loss: 0.5629217624664307, acc: 0.8333333134651184)
[2025-01-02 00:49:13,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:13,618][root][INFO] - Training Epoch: 2/2, step 437/574 completed (loss: 0.07971729338169098, acc: 0.9545454382896423)
[2025-01-02 00:49:13,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:13,920][root][INFO] - Training Epoch: 2/2, step 438/574 completed (loss: 0.020442456007003784, acc: 1.0)
[2025-01-02 00:49:14,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:14,255][root][INFO] - Training Epoch: 2/2, step 439/574 completed (loss: 0.47963690757751465, acc: 0.8717948794364929)
[2025-01-02 00:49:14,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:14,711][root][INFO] - Training Epoch: 2/2, step 440/574 completed (loss: 0.524680495262146, acc: 0.8484848737716675)
[2025-01-02 00:49:14,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:15,404][root][INFO] - Training Epoch: 2/2, step 441/574 completed (loss: 1.026268482208252, acc: 0.6959999799728394)
[2025-01-02 00:49:15,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:15,824][root][INFO] - Training Epoch: 2/2, step 442/574 completed (loss: 1.030521035194397, acc: 0.75)
[2025-01-02 00:49:15,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:16,471][root][INFO] - Training Epoch: 2/2, step 443/574 completed (loss: 0.6317675113677979, acc: 0.8557214140892029)
[2025-01-02 00:49:16,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:16,794][root][INFO] - Training Epoch: 2/2, step 444/574 completed (loss: 0.35535332560539246, acc: 0.8679245114326477)
[2025-01-02 00:49:16,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:17,211][root][INFO] - Training Epoch: 2/2, step 445/574 completed (loss: 0.237130269408226, acc: 0.9090909361839294)
[2025-01-02 00:49:17,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:17,519][root][INFO] - Training Epoch: 2/2, step 446/574 completed (loss: 0.39897337555885315, acc: 0.8260869383811951)
[2025-01-02 00:49:17,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:17,822][root][INFO] - Training Epoch: 2/2, step 447/574 completed (loss: 0.7137216329574585, acc: 0.807692289352417)
[2025-01-02 00:49:17,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:18,143][root][INFO] - Training Epoch: 2/2, step 448/574 completed (loss: 0.2620560824871063, acc: 0.9642857313156128)
[2025-01-02 00:49:18,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:18,476][root][INFO] - Training Epoch: 2/2, step 449/574 completed (loss: 0.2192775160074234, acc: 0.9253731369972229)
[2025-01-02 00:49:18,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:18,796][root][INFO] - Training Epoch: 2/2, step 450/574 completed (loss: 0.07067989557981491, acc: 1.0)
[2025-01-02 00:49:18,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:19,091][root][INFO] - Training Epoch: 2/2, step 451/574 completed (loss: 0.11219379305839539, acc: 0.95652174949646)
[2025-01-02 00:49:19,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:19,399][root][INFO] - Training Epoch: 2/2, step 452/574 completed (loss: 0.3601292669773102, acc: 0.8974359035491943)
[2025-01-02 00:49:19,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:19,699][root][INFO] - Training Epoch: 2/2, step 453/574 completed (loss: 0.43413102626800537, acc: 0.8552631735801697)
[2025-01-02 00:49:19,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:20,002][root][INFO] - Training Epoch: 2/2, step 454/574 completed (loss: 0.24114200472831726, acc: 0.9591836929321289)
[2025-01-02 00:49:20,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:20,305][root][INFO] - Training Epoch: 2/2, step 455/574 completed (loss: 0.34109461307525635, acc: 0.9090909361839294)
[2025-01-02 00:49:20,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:20,620][root][INFO] - Training Epoch: 2/2, step 456/574 completed (loss: 0.6768763661384583, acc: 0.8350515365600586)
[2025-01-02 00:49:20,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:20,936][root][INFO] - Training Epoch: 2/2, step 457/574 completed (loss: 0.07024888694286346, acc: 0.9714285731315613)
[2025-01-02 00:49:21,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:21,309][root][INFO] - Training Epoch: 2/2, step 458/574 completed (loss: 0.5429477691650391, acc: 0.8720930218696594)
[2025-01-02 00:49:21,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:21,631][root][INFO] - Training Epoch: 2/2, step 459/574 completed (loss: 0.12649479508399963, acc: 0.9285714030265808)
[2025-01-02 00:49:21,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:21,950][root][INFO] - Training Epoch: 2/2, step 460/574 completed (loss: 0.3227265179157257, acc: 0.9382715821266174)
[2025-01-02 00:49:22,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:22,255][root][INFO] - Training Epoch: 2/2, step 461/574 completed (loss: 0.3538317084312439, acc: 0.8888888955116272)
[2025-01-02 00:49:22,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:22,554][root][INFO] - Training Epoch: 2/2, step 462/574 completed (loss: 0.11893121898174286, acc: 0.96875)
[2025-01-02 00:49:22,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:22,853][root][INFO] - Training Epoch: 2/2, step 463/574 completed (loss: 0.5683345198631287, acc: 0.8846153616905212)
[2025-01-02 00:49:22,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:23,171][root][INFO] - Training Epoch: 2/2, step 464/574 completed (loss: 0.32817986607551575, acc: 0.9130434989929199)
[2025-01-02 00:49:23,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:23,539][root][INFO] - Training Epoch: 2/2, step 465/574 completed (loss: 0.4404368996620178, acc: 0.8809523582458496)
[2025-01-02 00:49:23,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:23,789][root][INFO] - Training Epoch: 2/2, step 466/574 completed (loss: 0.6607702374458313, acc: 0.8554216623306274)
[2025-01-02 00:49:23,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:24,122][root][INFO] - Training Epoch: 2/2, step 467/574 completed (loss: 0.31962403655052185, acc: 0.8828828930854797)
[2025-01-02 00:49:24,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:24,447][root][INFO] - Training Epoch: 2/2, step 468/574 completed (loss: 0.8685567378997803, acc: 0.7961165308952332)
[2025-01-02 00:49:24,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:24,741][root][INFO] - Training Epoch: 2/2, step 469/574 completed (loss: 0.6878997087478638, acc: 0.8211382031440735)
[2025-01-02 00:49:24,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:25,074][root][INFO] - Training Epoch: 2/2, step 470/574 completed (loss: 0.18888740241527557, acc: 0.9166666865348816)
[2025-01-02 00:49:25,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:25,384][root][INFO] - Training Epoch: 2/2, step 471/574 completed (loss: 0.4810551106929779, acc: 0.8571428656578064)
[2025-01-02 00:49:25,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:25,802][root][INFO] - Training Epoch: 2/2, step 472/574 completed (loss: 0.7223342657089233, acc: 0.7941176295280457)
[2025-01-02 00:49:25,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:26,163][root][INFO] - Training Epoch: 2/2, step 473/574 completed (loss: 0.9303083419799805, acc: 0.7685589790344238)
[2025-01-02 00:49:26,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:26,466][root][INFO] - Training Epoch: 2/2, step 474/574 completed (loss: 0.6669576168060303, acc: 0.7708333134651184)
[2025-01-02 00:49:26,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:26,779][root][INFO] - Training Epoch: 2/2, step 475/574 completed (loss: 0.44827303290367126, acc: 0.8588957190513611)
[2025-01-02 00:49:26,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:27,133][root][INFO] - Training Epoch: 2/2, step 476/574 completed (loss: 0.5434756875038147, acc: 0.8417266011238098)
[2025-01-02 00:49:27,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:27,485][root][INFO] - Training Epoch: 2/2, step 477/574 completed (loss: 0.9014036059379578, acc: 0.7487437129020691)
[2025-01-02 00:49:27,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:27,820][root][INFO] - Training Epoch: 2/2, step 478/574 completed (loss: 0.6013895273208618, acc: 0.8055555820465088)
[2025-01-02 00:49:27,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:28,166][root][INFO] - Training Epoch: 2/2, step 479/574 completed (loss: 0.656624972820282, acc: 0.7878788113594055)
[2025-01-02 00:49:28,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:28,491][root][INFO] - Training Epoch: 2/2, step 480/574 completed (loss: 0.33533188700675964, acc: 0.8888888955116272)
[2025-01-02 00:49:28,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:28,820][root][INFO] - Training Epoch: 2/2, step 481/574 completed (loss: 0.504776120185852, acc: 0.8500000238418579)
[2025-01-02 00:49:28,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:29,168][root][INFO] - Training Epoch: 2/2, step 482/574 completed (loss: 0.6639146208763123, acc: 0.75)
[2025-01-02 00:49:29,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:29,553][root][INFO] - Training Epoch: 2/2, step 483/574 completed (loss: 0.8638572096824646, acc: 0.6724137663841248)
[2025-01-02 00:49:29,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:29,876][root][INFO] - Training Epoch: 2/2, step 484/574 completed (loss: 0.15865513682365417, acc: 0.8709677457809448)
[2025-01-02 00:49:29,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:30,196][root][INFO] - Training Epoch: 2/2, step 485/574 completed (loss: 0.4599360525608063, acc: 0.8421052694320679)
[2025-01-02 00:49:30,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:30,487][root][INFO] - Training Epoch: 2/2, step 486/574 completed (loss: 0.9993571043014526, acc: 0.6296296119689941)
[2025-01-02 00:49:30,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:30,794][root][INFO] - Training Epoch: 2/2, step 487/574 completed (loss: 0.6365112066268921, acc: 0.8095238208770752)
[2025-01-02 00:49:30,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:31,111][root][INFO] - Training Epoch: 2/2, step 488/574 completed (loss: 0.6564621329307556, acc: 0.7272727489471436)
[2025-01-02 00:49:31,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:31,524][root][INFO] - Training Epoch: 2/2, step 489/574 completed (loss: 1.0504417419433594, acc: 0.7076923251152039)
[2025-01-02 00:49:31,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:31,850][root][INFO] - Training Epoch: 2/2, step 490/574 completed (loss: 0.28852999210357666, acc: 0.8999999761581421)
[2025-01-02 00:49:31,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:32,188][root][INFO] - Training Epoch: 2/2, step 491/574 completed (loss: 0.5997211337089539, acc: 0.7931034564971924)
[2025-01-02 00:49:32,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:32,545][root][INFO] - Training Epoch: 2/2, step 492/574 completed (loss: 0.5793434381484985, acc: 0.7647058963775635)
[2025-01-02 00:49:32,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:32,894][root][INFO] - Training Epoch: 2/2, step 493/574 completed (loss: 0.6038863658905029, acc: 0.7931034564971924)
[2025-01-02 00:49:33,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:33,252][root][INFO] - Training Epoch: 2/2, step 494/574 completed (loss: 0.6000206470489502, acc: 0.8947368264198303)
[2025-01-02 00:49:33,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:33,596][root][INFO] - Training Epoch: 2/2, step 495/574 completed (loss: 0.7998450994491577, acc: 0.7368420958518982)
[2025-01-02 00:49:33,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:33,987][root][INFO] - Training Epoch: 2/2, step 496/574 completed (loss: 0.8080825805664062, acc: 0.7946428656578064)
[2025-01-02 00:49:34,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:34,396][root][INFO] - Training Epoch: 2/2, step 497/574 completed (loss: 0.5385922193527222, acc: 0.8202247023582458)
[2025-01-02 00:49:34,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:34,740][root][INFO] - Training Epoch: 2/2, step 498/574 completed (loss: 0.8185279369354248, acc: 0.7528089880943298)
[2025-01-02 00:49:34,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:35,087][root][INFO] - Training Epoch: 2/2, step 499/574 completed (loss: 1.4562774896621704, acc: 0.6028369069099426)
[2025-01-02 00:49:35,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:35,449][root][INFO] - Training Epoch: 2/2, step 500/574 completed (loss: 0.9814066886901855, acc: 0.75)
[2025-01-02 00:49:35,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:35,765][root][INFO] - Training Epoch: 2/2, step 501/574 completed (loss: 0.05237557739019394, acc: 1.0)
[2025-01-02 00:49:35,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:36,071][root][INFO] - Training Epoch: 2/2, step 502/574 completed (loss: 0.08437420427799225, acc: 0.9615384340286255)
[2025-01-02 00:49:36,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:36,417][root][INFO] - Training Epoch: 2/2, step 503/574 completed (loss: 0.18780425190925598, acc: 0.9629629850387573)
[2025-01-02 00:49:36,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:36,804][root][INFO] - Training Epoch: 2/2, step 504/574 completed (loss: 0.1256549060344696, acc: 0.9629629850387573)
[2025-01-02 00:49:36,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:37,170][root][INFO] - Training Epoch: 2/2, step 505/574 completed (loss: 0.7737933397293091, acc: 0.849056601524353)
[2025-01-02 00:49:37,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:37,497][root][INFO] - Training Epoch: 2/2, step 506/574 completed (loss: 0.8231225609779358, acc: 0.7586206793785095)
[2025-01-02 00:49:37,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:38,095][root][INFO] - Training Epoch: 2/2, step 507/574 completed (loss: 1.299957036972046, acc: 0.6126126050949097)
[2025-01-02 00:49:38,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:38,532][root][INFO] - Training Epoch: 2/2, step 508/574 completed (loss: 1.0083539485931396, acc: 0.7323943376541138)
[2025-01-02 00:49:38,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:38,883][root][INFO] - Training Epoch: 2/2, step 509/574 completed (loss: 0.1796262562274933, acc: 0.949999988079071)
[2025-01-02 00:49:38,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:39,241][root][INFO] - Training Epoch: 2/2, step 510/574 completed (loss: 0.34507811069488525, acc: 0.8666666746139526)
[2025-01-02 00:49:39,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:39,614][root][INFO] - Training Epoch: 2/2, step 511/574 completed (loss: 0.4521266520023346, acc: 0.807692289352417)
[2025-01-02 00:49:40,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:42,089][root][INFO] - Training Epoch: 2/2, step 512/574 completed (loss: 1.2971861362457275, acc: 0.6428571343421936)
[2025-01-02 00:49:42,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:42,861][root][INFO] - Training Epoch: 2/2, step 513/574 completed (loss: 0.2639964520931244, acc: 0.9126983880996704)
[2025-01-02 00:49:42,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:43,202][root][INFO] - Training Epoch: 2/2, step 514/574 completed (loss: 0.7563806772232056, acc: 0.75)
[2025-01-02 00:49:43,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:43,551][root][INFO] - Training Epoch: 2/2, step 515/574 completed (loss: 0.16663450002670288, acc: 0.9666666388511658)
[2025-01-02 00:49:43,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:44,240][root][INFO] - Training Epoch: 2/2, step 516/574 completed (loss: 0.6835288405418396, acc: 0.8472222089767456)
[2025-01-02 00:49:44,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:44,546][root][INFO] - Training Epoch: 2/2, step 517/574 completed (loss: 0.01423941645771265, acc: 1.0)
[2025-01-02 00:49:44,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:44,852][root][INFO] - Training Epoch: 2/2, step 518/574 completed (loss: 0.1810530424118042, acc: 0.9354838728904724)
[2025-01-02 00:49:44,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:45,240][root][INFO] - Training Epoch: 2/2, step 519/574 completed (loss: 0.3367961049079895, acc: 0.949999988079071)
[2025-01-02 00:49:45,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:45,590][root][INFO] - Training Epoch: 2/2, step 520/574 completed (loss: 0.6514290571212769, acc: 0.8518518805503845)
[2025-01-02 00:49:45,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:46,592][root][INFO] - Training Epoch: 2/2, step 521/574 completed (loss: 0.7655441164970398, acc: 0.7796609997749329)
[2025-01-02 00:49:46,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:46,939][root][INFO] - Training Epoch: 2/2, step 522/574 completed (loss: 0.362483948469162, acc: 0.8656716346740723)
[2025-01-02 00:49:47,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:47,305][root][INFO] - Training Epoch: 2/2, step 523/574 completed (loss: 0.4630281925201416, acc: 0.8394160866737366)
[2025-01-02 00:49:47,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:47,868][root][INFO] - Training Epoch: 2/2, step 524/574 completed (loss: 0.7992565035820007, acc: 0.8050000071525574)
[2025-01-02 00:49:47,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:48,221][root][INFO] - Training Epoch: 2/2, step 525/574 completed (loss: 0.06882359087467194, acc: 0.9814814925193787)
[2025-01-02 00:49:48,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:48,519][root][INFO] - Training Epoch: 2/2, step 526/574 completed (loss: 0.2605605125427246, acc: 0.9230769276618958)
[2025-01-02 00:49:48,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:48,863][root][INFO] - Training Epoch: 2/2, step 527/574 completed (loss: 0.4544468820095062, acc: 0.8571428656578064)
[2025-01-02 00:49:48,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:49,217][root][INFO] - Training Epoch: 2/2, step 528/574 completed (loss: 1.9160934686660767, acc: 0.5409836173057556)
[2025-01-02 00:49:49,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:49,560][root][INFO] - Training Epoch: 2/2, step 529/574 completed (loss: 0.34217530488967896, acc: 0.9152542352676392)
[2025-01-02 00:49:49,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:49,909][root][INFO] - Training Epoch: 2/2, step 530/574 completed (loss: 1.5099186897277832, acc: 0.5116279125213623)
[2025-01-02 00:49:49,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:50,275][root][INFO] - Training Epoch: 2/2, step 531/574 completed (loss: 0.9787791967391968, acc: 0.7727272510528564)
[2025-01-02 00:49:50,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:50,661][root][INFO] - Training Epoch: 2/2, step 532/574 completed (loss: 1.1922868490219116, acc: 0.7358490824699402)
[2025-01-02 00:49:50,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:51,016][root][INFO] - Training Epoch: 2/2, step 533/574 completed (loss: 0.9506902098655701, acc: 0.75)
[2025-01-02 00:49:51,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:51,320][root][INFO] - Training Epoch: 2/2, step 534/574 completed (loss: 0.6536093950271606, acc: 0.800000011920929)
[2025-01-02 00:49:51,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:51,627][root][INFO] - Training Epoch: 2/2, step 535/574 completed (loss: 0.527251660823822, acc: 0.8999999761581421)
[2025-01-02 00:49:51,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:51,982][root][INFO] - Training Epoch: 2/2, step 536/574 completed (loss: 0.31275948882102966, acc: 0.9090909361839294)
[2025-01-02 00:49:52,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:52,419][root][INFO] - Training Epoch: 2/2, step 537/574 completed (loss: 0.7740687727928162, acc: 0.800000011920929)
[2025-01-02 00:49:52,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:52,789][root][INFO] - Training Epoch: 2/2, step 538/574 completed (loss: 0.5767817497253418, acc: 0.8125)
[2025-01-02 00:49:52,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:53,184][root][INFO] - Training Epoch: 2/2, step 539/574 completed (loss: 0.5255203247070312, acc: 0.90625)
[2025-01-02 00:49:53,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:53,537][root][INFO] - Training Epoch: 2/2, step 540/574 completed (loss: 0.7679576873779297, acc: 0.7575757503509521)
[2025-01-02 00:49:53,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:53,874][root][INFO] - Training Epoch: 2/2, step 541/574 completed (loss: 0.22283513844013214, acc: 0.9375)
[2025-01-02 00:49:53,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:54,226][root][INFO] - Training Epoch: 2/2, step 542/574 completed (loss: 0.06865264475345612, acc: 1.0)
[2025-01-02 00:49:54,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:54,572][root][INFO] - Training Epoch: 2/2, step 543/574 completed (loss: 0.0910535603761673, acc: 0.95652174949646)
[2025-01-02 00:49:54,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:54,881][root][INFO] - Training Epoch: 2/2, step 544/574 completed (loss: 0.1324910968542099, acc: 0.9666666388511658)
[2025-01-02 00:49:54,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:55,228][root][INFO] - Training Epoch: 2/2, step 545/574 completed (loss: 0.13618206977844238, acc: 0.9512194991111755)
[2025-01-02 00:49:55,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:55,621][root][INFO] - Training Epoch: 2/2, step 546/574 completed (loss: 0.030324792489409447, acc: 1.0)
[2025-01-02 00:49:55,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:55,985][root][INFO] - Training Epoch: 2/2, step 547/574 completed (loss: 0.0279703289270401, acc: 1.0)
[2025-01-02 00:49:56,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:56,330][root][INFO] - Training Epoch: 2/2, step 548/574 completed (loss: 0.30661678314208984, acc: 0.9032257795333862)
[2025-01-02 00:49:56,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:56,687][root][INFO] - Training Epoch: 2/2, step 549/574 completed (loss: 0.0166938453912735, acc: 1.0)
[2025-01-02 00:49:56,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:57,057][root][INFO] - Training Epoch: 2/2, step 550/574 completed (loss: 0.3882424831390381, acc: 0.8787878751754761)
[2025-01-02 00:49:57,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:57,389][root][INFO] - Training Epoch: 2/2, step 551/574 completed (loss: 0.16426554322242737, acc: 0.949999988079071)
[2025-01-02 00:49:57,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:57,735][root][INFO] - Training Epoch: 2/2, step 552/574 completed (loss: 0.18707819283008575, acc: 0.9285714030265808)
[2025-01-02 00:49:57,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:58,124][root][INFO] - Training Epoch: 2/2, step 553/574 completed (loss: 0.5609815120697021, acc: 0.8467153310775757)
[2025-01-02 00:49:58,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:58,512][root][INFO] - Training Epoch: 2/2, step 554/574 completed (loss: 0.23998534679412842, acc: 0.9172413945198059)
[2025-01-02 00:49:58,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:58,862][root][INFO] - Training Epoch: 2/2, step 555/574 completed (loss: 0.4237699508666992, acc: 0.8642857074737549)
[2025-01-02 00:49:58,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:59,220][root][INFO] - Training Epoch: 2/2, step 556/574 completed (loss: 0.43038409948349, acc: 0.8741722106933594)
[2025-01-02 00:49:59,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:59,565][root][INFO] - Training Epoch: 2/2, step 557/574 completed (loss: 0.266554594039917, acc: 0.9145299196243286)
[2025-01-02 00:49:59,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:59,923][root][INFO] - Training Epoch: 2/2, step 558/574 completed (loss: 0.10524069517850876, acc: 0.9599999785423279)
[2025-01-02 00:50:00,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:00,276][root][INFO] - Training Epoch: 2/2, step 559/574 completed (loss: 0.37969568371772766, acc: 0.9230769276618958)
[2025-01-02 00:50:00,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:00,655][root][INFO] - Training Epoch: 2/2, step 560/574 completed (loss: 0.02612951397895813, acc: 1.0)
[2025-01-02 00:50:00,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:00,995][root][INFO] - Training Epoch: 2/2, step 561/574 completed (loss: 0.0711427554488182, acc: 0.9743589758872986)
[2025-01-02 00:50:01,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:01,403][root][INFO] - Training Epoch: 2/2, step 562/574 completed (loss: 0.6171661615371704, acc: 0.8666666746139526)
[2025-01-02 00:50:01,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:01,740][root][INFO] - Training Epoch: 2/2, step 563/574 completed (loss: 0.5039026737213135, acc: 0.8831169009208679)
[2025-01-02 00:50:01,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:02,040][root][INFO] - Training Epoch: 2/2, step 564/574 completed (loss: 0.2661310136318207, acc: 0.875)
[2025-01-02 00:50:02,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:02,383][root][INFO] - Training Epoch: 2/2, step 565/574 completed (loss: 0.22732657194137573, acc: 0.9137930870056152)
[2025-01-02 00:50:02,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:02,735][root][INFO] - Training Epoch: 2/2, step 566/574 completed (loss: 0.37475913763046265, acc: 0.9285714030265808)
[2025-01-02 00:50:02,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:03,121][root][INFO] - Training Epoch: 2/2, step 567/574 completed (loss: 0.0371011421084404, acc: 1.0)
[2025-01-02 00:50:03,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:03,466][root][INFO] - Training Epoch: 2/2, step 568/574 completed (loss: 0.07090317457914352, acc: 0.9629629850387573)
[2025-01-02 00:50:03,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:03,874][root][INFO] - Training Epoch: 2/2, step 569/574 completed (loss: 0.22911430895328522, acc: 0.9358288645744324)
[2025-01-02 00:50:04,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:04,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:05,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:05,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:05,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:06,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:06,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:06,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:07,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:07,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:07,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:08,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:08,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:09,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:09,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:09,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:10,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:10,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:10,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:11,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:11,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:11,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:12,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:12,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:12,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:13,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:13,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:13,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:14,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:14,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:14,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:15,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:15,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:15,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:16,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:16,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:17,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:17,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:17,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:18,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:18,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:18,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:19,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:19,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:19,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:20,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:20,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:20,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:21,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:21,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:21,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:22,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:22,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:22,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:23,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:23,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:23,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:23,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:24,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:24,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:24,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:25,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:25,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:26,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:26,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:26,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:26,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:27,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:27,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:28,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:28,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:28,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:28,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:29,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:29,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:29,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:30,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:30,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:30,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:31,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:31,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:31,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:32,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:32,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:32,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:33,494][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9333, device='cuda:0') eval_epoch_loss=tensor(0.6592, device='cuda:0') eval_epoch_acc=tensor(0.8267, device='cuda:0')
[2025-01-02 00:50:33,495][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:50:33,496][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:50:33,707][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_570_loss_0.6592080593109131/model.pt
[2025-01-02 00:50:33,714][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:50:33,715][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8267472386360168
[2025-01-02 00:50:33,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:34,116][root][INFO] - Training Epoch: 2/2, step 570/574 completed (loss: 0.055733900517225266, acc: 0.9838709831237793)
[2025-01-02 00:50:34,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:34,430][root][INFO] - Training Epoch: 2/2, step 571/574 completed (loss: 0.3753429353237152, acc: 0.9145299196243286)
[2025-01-02 00:50:34,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:34,728][root][INFO] - Training Epoch: 2/2, step 572/574 completed (loss: 0.43216946721076965, acc: 0.8520408272743225)
[2025-01-02 00:50:34,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:35,063][root][INFO] - Training Epoch: 2/2, step 573/574 completed (loss: 0.3863102197647095, acc: 0.893081784248352)
[2025-01-02 00:50:35,433][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.9167, train_epoch_loss=0.6506, epoch time 347.9451165497303s
[2025-01-02 00:50:35,434][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-02 00:50:35,434][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-02 00:50:35,434][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-02 00:50:35,434][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2025-01-02 00:50:35,434][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-02 00:50:35,437][root][INFO] - Key: avg_train_prep, Value: 2.88970947265625
[2025-01-02 00:50:35,439][root][INFO] - Key: avg_train_loss, Value: 1.0009819269180298
[2025-01-02 00:50:35,439][root][INFO] - Key: avg_train_acc, Value: 0.7553256750106812
[2025-01-02 00:50:35,439][root][INFO] - Key: avg_eval_prep, Value: 2.2413525581359863
[2025-01-02 00:50:35,439][root][INFO] - Key: avg_eval_loss, Value: 0.7871432304382324
[2025-01-02 00:50:35,439][root][INFO] - Key: avg_eval_acc, Value: 0.7929084897041321
[2025-01-02 00:50:35,439][root][INFO] - Key: avg_epoch_time, Value: 354.70553200133145
[2025-01-02 00:50:35,439][root][INFO] - Key: avg_checkpoint_time, Value: 0.2541820714250207
[2025-01-06 00:59:04,493][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-01-06 00:59:04,494][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-06 00:59:04,494][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-01-06 00:59:04,494][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-01-06_00-59-03.txt', 'log_interval': 5}
[2025-01-06 00:59:32,312][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-01-06 00:59:37,514][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-06 00:59:37,516][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-01-06 00:59:37,518][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-06 00:59:37,520][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-01-06 00:59:47,544][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-06 00:59:47,546][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-01-06 00:59:47,546][slam_llm.models.slam_model][INFO] - setup peft...
[2025-01-06 00:59:47,684][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-06 00:59:47,686][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-01-06 00:59:47,788][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-01-06 00:59:47,789][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-01-06 00:59:47,789][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.050975121557712555/model.pt
[2025-01-06 00:59:48,011][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-01-06 00:59:48,015][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-01-06 00:59:50,016][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-01-06 00:59:53,253][root][INFO] - --> Training Set Length = 2298
[2025-01-06 00:59:53,264][root][INFO] - --> Validation Set Length = 341
[2025-01-06 00:59:53,265][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-06 00:59:53,266][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-06 00:59:55,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:56,915][root][INFO] - Training Epoch: 1/10, step 0/574 completed (loss: 4.565918922424316, acc: 0.2222222238779068)
[2025-01-06 00:59:57,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:57,230][root][INFO] - Training Epoch: 1/10, step 1/574 completed (loss: 3.6701340675354004, acc: 0.2800000011920929)
[2025-01-06 00:59:57,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:57,735][root][INFO] - Training Epoch: 1/10, step 2/574 completed (loss: 3.0726566314697266, acc: 0.4054054021835327)
[2025-01-06 00:59:57,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:58,152][root][INFO] - Training Epoch: 1/10, step 3/574 completed (loss: 4.179104328155518, acc: 0.2368421107530594)
[2025-01-06 00:59:58,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:58,587][root][INFO] - Training Epoch: 1/10, step 4/574 completed (loss: 4.1991047859191895, acc: 0.21621622145175934)
[2025-01-06 00:59:58,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:58,961][root][INFO] - Training Epoch: 1/10, step 5/574 completed (loss: 3.3390657901763916, acc: 0.3928571343421936)
[2025-01-06 00:59:59,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:59,343][root][INFO] - Training Epoch: 1/10, step 6/574 completed (loss: 4.611793518066406, acc: 0.30612245202064514)
[2025-01-06 00:59:59,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:59,759][root][INFO] - Training Epoch: 1/10, step 7/574 completed (loss: 3.025235414505005, acc: 0.46666666865348816)
[2025-01-06 00:59:59,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:00,164][root][INFO] - Training Epoch: 1/10, step 8/574 completed (loss: 3.5062613487243652, acc: 0.5)
[2025-01-06 01:00:00,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:00,519][root][INFO] - Training Epoch: 1/10, step 9/574 completed (loss: 1.9377830028533936, acc: 0.692307710647583)
[2025-01-06 01:00:00,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:00,947][root][INFO] - Training Epoch: 1/10, step 10/574 completed (loss: 1.4856634140014648, acc: 0.6666666865348816)
[2025-01-06 01:00:01,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:01,362][root][INFO] - Training Epoch: 1/10, step 11/574 completed (loss: 3.778326988220215, acc: 0.3076923191547394)
[2025-01-06 01:00:01,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:01,814][root][INFO] - Training Epoch: 1/10, step 12/574 completed (loss: 3.122279405593872, acc: 0.4545454680919647)
[2025-01-06 01:00:01,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:02,220][root][INFO] - Training Epoch: 1/10, step 13/574 completed (loss: 3.3837459087371826, acc: 0.3695652186870575)
[2025-01-06 01:00:02,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:02,587][root][INFO] - Training Epoch: 1/10, step 14/574 completed (loss: 3.736240863800049, acc: 0.47058823704719543)
[2025-01-06 01:00:02,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:02,925][root][INFO] - Training Epoch: 1/10, step 15/574 completed (loss: 2.7348246574401855, acc: 0.5102040767669678)
[2025-01-06 01:00:03,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:03,249][root][INFO] - Training Epoch: 1/10, step 16/574 completed (loss: 3.640578031539917, acc: 0.42105263471603394)
[2025-01-06 01:00:03,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:03,602][root][INFO] - Training Epoch: 1/10, step 17/574 completed (loss: 2.8711702823638916, acc: 0.4583333432674408)
[2025-01-06 01:00:03,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:03,934][root][INFO] - Training Epoch: 1/10, step 18/574 completed (loss: 4.022885322570801, acc: 0.3611111044883728)
[2025-01-06 01:00:04,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:04,323][root][INFO] - Training Epoch: 1/10, step 19/574 completed (loss: 3.6987786293029785, acc: 0.5263158082962036)
[2025-01-06 01:00:04,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:04,696][root][INFO] - Training Epoch: 1/10, step 20/574 completed (loss: 2.517129421234131, acc: 0.5769230723381042)
[2025-01-06 01:00:04,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:04,991][root][INFO] - Training Epoch: 1/10, step 21/574 completed (loss: 2.9080355167388916, acc: 0.48275861144065857)
[2025-01-06 01:00:05,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:05,376][root][INFO] - Training Epoch: 1/10, step 22/574 completed (loss: 4.26425313949585, acc: 0.3199999928474426)
[2025-01-06 01:00:05,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:05,717][root][INFO] - Training Epoch: 1/10, step 23/574 completed (loss: 2.4392616748809814, acc: 0.6666666865348816)
[2025-01-06 01:00:05,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:06,098][root][INFO] - Training Epoch: 1/10, step 24/574 completed (loss: 2.8660073280334473, acc: 0.5625)
[2025-01-06 01:00:06,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:06,498][root][INFO] - Training Epoch: 1/10, step 25/574 completed (loss: 3.410203456878662, acc: 0.4150943458080292)
[2025-01-06 01:00:06,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:06,850][root][INFO] - Training Epoch: 1/10, step 26/574 completed (loss: 3.3793797492980957, acc: 0.27397260069847107)
[2025-01-06 01:00:07,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:08,212][root][INFO] - Training Epoch: 1/10, step 27/574 completed (loss: 3.235335111618042, acc: 0.3596837818622589)
[2025-01-06 01:00:08,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:08,576][root][INFO] - Training Epoch: 1/10, step 28/574 completed (loss: 3.8047492504119873, acc: 0.3488371968269348)
[2025-01-06 01:00:08,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:08,998][root][INFO] - Training Epoch: 1/10, step 29/574 completed (loss: 3.430837631225586, acc: 0.40963855385780334)
[2025-01-06 01:00:09,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:09,387][root][INFO] - Training Epoch: 1/10, step 30/574 completed (loss: 3.1183393001556396, acc: 0.43209877610206604)
[2025-01-06 01:00:09,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:09,711][root][INFO] - Training Epoch: 1/10, step 31/574 completed (loss: 3.7057902812957764, acc: 0.3928571343421936)
[2025-01-06 01:00:09,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:10,128][root][INFO] - Training Epoch: 1/10, step 32/574 completed (loss: 2.630953073501587, acc: 0.5185185074806213)
[2025-01-06 01:00:10,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:10,621][root][INFO] - Training Epoch: 1/10, step 33/574 completed (loss: 2.9639978408813477, acc: 0.6086956262588501)
[2025-01-06 01:00:10,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:11,093][root][INFO] - Training Epoch: 1/10, step 34/574 completed (loss: 2.693814754486084, acc: 0.45378151535987854)
[2025-01-06 01:00:11,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:11,535][root][INFO] - Training Epoch: 1/10, step 35/574 completed (loss: 2.742159366607666, acc: 0.5409836173057556)
[2025-01-06 01:00:11,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:11,920][root][INFO] - Training Epoch: 1/10, step 36/574 completed (loss: 2.9530844688415527, acc: 0.4285714328289032)
[2025-01-06 01:00:12,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:12,290][root][INFO] - Training Epoch: 1/10, step 37/574 completed (loss: 2.8723032474517822, acc: 0.5423728823661804)
[2025-01-06 01:00:12,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:12,686][root][INFO] - Training Epoch: 1/10, step 38/574 completed (loss: 2.8279476165771484, acc: 0.5632184147834778)
[2025-01-06 01:00:12,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:13,030][root][INFO] - Training Epoch: 1/10, step 39/574 completed (loss: 4.378084659576416, acc: 0.2380952388048172)
[2025-01-06 01:00:13,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:13,402][root][INFO] - Training Epoch: 1/10, step 40/574 completed (loss: 2.7955381870269775, acc: 0.6538461446762085)
[2025-01-06 01:00:13,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:13,837][root][INFO] - Training Epoch: 1/10, step 41/574 completed (loss: 2.3172032833099365, acc: 0.5945945978164673)
[2025-01-06 01:00:13,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:14,268][root][INFO] - Training Epoch: 1/10, step 42/574 completed (loss: 3.5862834453582764, acc: 0.3692307770252228)
[2025-01-06 01:00:14,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:14,770][root][INFO] - Training Epoch: 1/10, step 43/574 completed (loss: 3.4920356273651123, acc: 0.3232323229312897)
[2025-01-06 01:00:14,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:15,215][root][INFO] - Training Epoch: 1/10, step 44/574 completed (loss: 2.637434244155884, acc: 0.5670102834701538)
[2025-01-06 01:00:15,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:15,651][root][INFO] - Training Epoch: 1/10, step 45/574 completed (loss: 3.285001039505005, acc: 0.40441176295280457)
[2025-01-06 01:00:15,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:16,030][root][INFO] - Training Epoch: 1/10, step 46/574 completed (loss: 3.3272156715393066, acc: 0.38461539149284363)
[2025-01-06 01:00:16,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:16,386][root][INFO] - Training Epoch: 1/10, step 47/574 completed (loss: 1.9891725778579712, acc: 0.5925925970077515)
[2025-01-06 01:00:16,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:16,788][root][INFO] - Training Epoch: 1/10, step 48/574 completed (loss: 2.2156660556793213, acc: 0.5357142686843872)
[2025-01-06 01:00:16,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:17,133][root][INFO] - Training Epoch: 1/10, step 49/574 completed (loss: 2.1350274085998535, acc: 0.5833333134651184)
[2025-01-06 01:00:17,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:17,467][root][INFO] - Training Epoch: 1/10, step 50/574 completed (loss: 3.502253293991089, acc: 0.4736842215061188)
[2025-01-06 01:00:17,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:17,798][root][INFO] - Training Epoch: 1/10, step 51/574 completed (loss: 3.330608606338501, acc: 0.460317462682724)
[2025-01-06 01:00:17,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:18,129][root][INFO] - Training Epoch: 1/10, step 52/574 completed (loss: 4.11309814453125, acc: 0.35211268067359924)
[2025-01-06 01:00:18,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:18,637][root][INFO] - Training Epoch: 1/10, step 53/574 completed (loss: 3.9685912132263184, acc: 0.2866666615009308)
[2025-01-06 01:00:18,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:19,001][root][INFO] - Training Epoch: 1/10, step 54/574 completed (loss: 4.304090976715088, acc: 0.21621622145175934)
[2025-01-06 01:00:19,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:19,466][root][INFO] - Training Epoch: 1/10, step 55/574 completed (loss: 1.8609943389892578, acc: 0.5384615659713745)
[2025-01-06 01:00:21,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:22,667][root][INFO] - Training Epoch: 1/10, step 56/574 completed (loss: 3.0619962215423584, acc: 0.3344709873199463)
[2025-01-06 01:00:23,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:23,901][root][INFO] - Training Epoch: 1/10, step 57/574 completed (loss: 3.080946207046509, acc: 0.35729846358299255)
[2025-01-06 01:00:24,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:24,581][root][INFO] - Training Epoch: 1/10, step 58/574 completed (loss: 3.339125156402588, acc: 0.3693181872367859)
[2025-01-06 01:00:24,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:25,155][root][INFO] - Training Epoch: 1/10, step 59/574 completed (loss: 2.5193119049072266, acc: 0.4485294222831726)
[2025-01-06 01:00:25,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:25,753][root][INFO] - Training Epoch: 1/10, step 60/574 completed (loss: 2.8575000762939453, acc: 0.4057970941066742)
[2025-01-06 01:00:25,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:26,199][root][INFO] - Training Epoch: 1/10, step 61/574 completed (loss: 2.9075052738189697, acc: 0.38749998807907104)
[2025-01-06 01:00:26,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:26,568][root][INFO] - Training Epoch: 1/10, step 62/574 completed (loss: 1.6552412509918213, acc: 0.529411792755127)
[2025-01-06 01:00:26,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:26,983][root][INFO] - Training Epoch: 1/10, step 63/574 completed (loss: 2.6073663234710693, acc: 0.5277777910232544)
[2025-01-06 01:00:27,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:27,352][root][INFO] - Training Epoch: 1/10, step 64/574 completed (loss: 1.9236969947814941, acc: 0.640625)
[2025-01-06 01:00:27,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:27,719][root][INFO] - Training Epoch: 1/10, step 65/574 completed (loss: 1.4708648920059204, acc: 0.6206896305084229)
[2025-01-06 01:00:27,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:28,075][root][INFO] - Training Epoch: 1/10, step 66/574 completed (loss: 3.460084915161133, acc: 0.3392857015132904)
[2025-01-06 01:00:28,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:28,509][root][INFO] - Training Epoch: 1/10, step 67/574 completed (loss: 2.7413833141326904, acc: 0.4000000059604645)
[2025-01-06 01:00:28,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:28,949][root][INFO] - Training Epoch: 1/10, step 68/574 completed (loss: 1.189255714416504, acc: 0.7200000286102295)
[2025-01-06 01:00:29,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:29,403][root][INFO] - Training Epoch: 1/10, step 69/574 completed (loss: 2.2416422367095947, acc: 0.4166666567325592)
[2025-01-06 01:00:29,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:29,775][root][INFO] - Training Epoch: 1/10, step 70/574 completed (loss: 3.6767418384552, acc: 0.3636363744735718)
[2025-01-06 01:00:29,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:30,162][root][INFO] - Training Epoch: 1/10, step 71/574 completed (loss: 2.6239001750946045, acc: 0.40441176295280457)
[2025-01-06 01:00:30,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:30,563][root][INFO] - Training Epoch: 1/10, step 72/574 completed (loss: 1.813328742980957, acc: 0.5555555820465088)
[2025-01-06 01:00:30,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:31,039][root][INFO] - Training Epoch: 1/10, step 73/574 completed (loss: 2.6633832454681396, acc: 0.3692307770252228)
[2025-01-06 01:00:31,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:31,388][root][INFO] - Training Epoch: 1/10, step 74/574 completed (loss: 3.560786008834839, acc: 0.30612245202064514)
[2025-01-06 01:00:31,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:31,718][root][INFO] - Training Epoch: 1/10, step 75/574 completed (loss: 2.566763162612915, acc: 0.3805970251560211)
[2025-01-06 01:00:31,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:32,123][root][INFO] - Training Epoch: 1/10, step 76/574 completed (loss: 3.073969602584839, acc: 0.3540146052837372)
[2025-01-06 01:00:32,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:32,447][root][INFO] - Training Epoch: 1/10, step 77/574 completed (loss: 1.0587918758392334, acc: 0.7142857313156128)
[2025-01-06 01:00:32,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:32,820][root][INFO] - Training Epoch: 1/10, step 78/574 completed (loss: 1.4693115949630737, acc: 0.625)
[2025-01-06 01:00:32,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:33,189][root][INFO] - Training Epoch: 1/10, step 79/574 completed (loss: 1.207265019416809, acc: 0.6666666865348816)
[2025-01-06 01:00:33,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:33,559][root][INFO] - Training Epoch: 1/10, step 80/574 completed (loss: 1.974440097808838, acc: 0.692307710647583)
[2025-01-06 01:00:33,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:33,982][root][INFO] - Training Epoch: 1/10, step 81/574 completed (loss: 2.6414620876312256, acc: 0.557692289352417)
[2025-01-06 01:00:34,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:34,354][root][INFO] - Training Epoch: 1/10, step 82/574 completed (loss: 2.7963244915008545, acc: 0.4423076808452606)
[2025-01-06 01:00:34,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:34,710][root][INFO] - Training Epoch: 1/10, step 83/574 completed (loss: 0.808995246887207, acc: 0.84375)
[2025-01-06 01:00:34,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:35,051][root][INFO] - Training Epoch: 1/10, step 84/574 completed (loss: 1.9839420318603516, acc: 0.5942028760910034)
[2025-01-06 01:00:35,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:35,405][root][INFO] - Training Epoch: 1/10, step 85/574 completed (loss: 2.5024800300598145, acc: 0.5600000023841858)
[2025-01-06 01:00:35,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:35,709][root][INFO] - Training Epoch: 1/10, step 86/574 completed (loss: 0.9810752272605896, acc: 0.695652186870575)
[2025-01-06 01:00:35,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:36,215][root][INFO] - Training Epoch: 1/10, step 87/574 completed (loss: 2.8885109424591064, acc: 0.4000000059604645)
[2025-01-06 01:00:36,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:36,573][root][INFO] - Training Epoch: 1/10, step 88/574 completed (loss: 3.065351963043213, acc: 0.3883495032787323)
[2025-01-06 01:00:36,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:37,724][root][INFO] - Training Epoch: 1/10, step 89/574 completed (loss: 2.4020309448242188, acc: 0.49514561891555786)
[2025-01-06 01:00:37,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:38,596][root][INFO] - Training Epoch: 1/10, step 90/574 completed (loss: 2.8965706825256348, acc: 0.42473119497299194)
[2025-01-06 01:00:38,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:39,395][root][INFO] - Training Epoch: 1/10, step 91/574 completed (loss: 2.404137372970581, acc: 0.5)
[2025-01-06 01:00:39,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:40,135][root][INFO] - Training Epoch: 1/10, step 92/574 completed (loss: 2.4086596965789795, acc: 0.5263158082962036)
[2025-01-06 01:00:40,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:41,122][root][INFO] - Training Epoch: 1/10, step 93/574 completed (loss: 3.314141035079956, acc: 0.2673267424106598)
[2025-01-06 01:00:41,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:41,528][root][INFO] - Training Epoch: 1/10, step 94/574 completed (loss: 2.3490023612976074, acc: 0.4838709533214569)
[2025-01-06 01:00:41,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:41,911][root][INFO] - Training Epoch: 1/10, step 95/574 completed (loss: 2.4161081314086914, acc: 0.3478260934352875)
[2025-01-06 01:00:42,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:42,275][root][INFO] - Training Epoch: 1/10, step 96/574 completed (loss: 3.2233736515045166, acc: 0.32773110270500183)
[2025-01-06 01:00:42,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:42,671][root][INFO] - Training Epoch: 1/10, step 97/574 completed (loss: 3.274331569671631, acc: 0.32692307233810425)
[2025-01-06 01:00:42,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:43,096][root][INFO] - Training Epoch: 1/10, step 98/574 completed (loss: 3.3142571449279785, acc: 0.30656933784484863)
[2025-01-06 01:00:43,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:43,428][root][INFO] - Training Epoch: 1/10, step 99/574 completed (loss: 3.4230663776397705, acc: 0.31343284249305725)
[2025-01-06 01:00:43,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:43,761][root][INFO] - Training Epoch: 1/10, step 100/574 completed (loss: 2.4190423488616943, acc: 0.550000011920929)
[2025-01-06 01:00:43,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:44,143][root][INFO] - Training Epoch: 1/10, step 101/574 completed (loss: 1.362923502922058, acc: 0.7272727489471436)
[2025-01-06 01:00:44,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:44,565][root][INFO] - Training Epoch: 1/10, step 102/574 completed (loss: 0.7006093859672546, acc: 0.739130437374115)
[2025-01-06 01:00:44,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:44,923][root][INFO] - Training Epoch: 1/10, step 103/574 completed (loss: 0.9565266966819763, acc: 0.7727272510528564)
[2025-01-06 01:00:45,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:45,298][root][INFO] - Training Epoch: 1/10, step 104/574 completed (loss: 1.5804924964904785, acc: 0.6206896305084229)
[2025-01-06 01:00:45,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:45,730][root][INFO] - Training Epoch: 1/10, step 105/574 completed (loss: 1.3597323894500732, acc: 0.6976743936538696)
[2025-01-06 01:00:45,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:46,104][root][INFO] - Training Epoch: 1/10, step 106/574 completed (loss: 0.986910343170166, acc: 0.7599999904632568)
[2025-01-06 01:00:46,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:46,432][root][INFO] - Training Epoch: 1/10, step 107/574 completed (loss: 0.9224901795387268, acc: 0.8235294222831726)
[2025-01-06 01:00:46,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:46,791][root][INFO] - Training Epoch: 1/10, step 108/574 completed (loss: 0.8043091297149658, acc: 0.8461538553237915)
[2025-01-06 01:00:46,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:47,154][root][INFO] - Training Epoch: 1/10, step 109/574 completed (loss: 0.6700358390808105, acc: 0.8809523582458496)
[2025-01-06 01:00:47,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:47,606][root][INFO] - Training Epoch: 1/10, step 110/574 completed (loss: 1.8345379829406738, acc: 0.6615384817123413)
[2025-01-06 01:00:47,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:48,071][root][INFO] - Training Epoch: 1/10, step 111/574 completed (loss: 1.5172370672225952, acc: 0.6315789222717285)
[2025-01-06 01:00:48,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:48,534][root][INFO] - Training Epoch: 1/10, step 112/574 completed (loss: 2.771487236022949, acc: 0.4736842215061188)
[2025-01-06 01:00:48,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:48,945][root][INFO] - Training Epoch: 1/10, step 113/574 completed (loss: 1.5295499563217163, acc: 0.5897436141967773)
[2025-01-06 01:00:49,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:49,352][root][INFO] - Training Epoch: 1/10, step 114/574 completed (loss: 1.512942910194397, acc: 0.6530612111091614)
[2025-01-06 01:00:49,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:49,714][root][INFO] - Training Epoch: 1/10, step 115/574 completed (loss: 1.3965150117874146, acc: 0.7272727489471436)
[2025-01-06 01:00:49,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:50,147][root][INFO] - Training Epoch: 1/10, step 116/574 completed (loss: 1.3159472942352295, acc: 0.6190476417541504)
[2025-01-06 01:00:50,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:50,537][root][INFO] - Training Epoch: 1/10, step 117/574 completed (loss: 1.4864555597305298, acc: 0.6341463327407837)
[2025-01-06 01:00:50,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:50,955][root][INFO] - Training Epoch: 1/10, step 118/574 completed (loss: 1.5972137451171875, acc: 0.7096773982048035)
[2025-01-06 01:00:51,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:51,863][root][INFO] - Training Epoch: 1/10, step 119/574 completed (loss: 1.9935545921325684, acc: 0.5399239659309387)
[2025-01-06 01:00:51,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:52,267][root][INFO] - Training Epoch: 1/10, step 120/574 completed (loss: 1.3953064680099487, acc: 0.6933333277702332)
[2025-01-06 01:00:52,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:52,758][root][INFO] - Training Epoch: 1/10, step 121/574 completed (loss: 2.0686235427856445, acc: 0.5961538553237915)
[2025-01-06 01:00:52,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:53,196][root][INFO] - Training Epoch: 1/10, step 122/574 completed (loss: 1.1900187730789185, acc: 0.6666666865348816)
[2025-01-06 01:00:53,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:53,624][root][INFO] - Training Epoch: 1/10, step 123/574 completed (loss: 1.0193647146224976, acc: 0.6842105388641357)
[2025-01-06 01:00:53,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:54,079][root][INFO] - Training Epoch: 1/10, step 124/574 completed (loss: 2.0188281536102295, acc: 0.5521472096443176)
[2025-01-06 01:00:54,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:54,515][root][INFO] - Training Epoch: 1/10, step 125/574 completed (loss: 2.0531625747680664, acc: 0.5)
[2025-01-06 01:00:54,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:54,860][root][INFO] - Training Epoch: 1/10, step 126/574 completed (loss: 1.9220811128616333, acc: 0.49166667461395264)
[2025-01-06 01:00:54,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:55,219][root][INFO] - Training Epoch: 1/10, step 127/574 completed (loss: 1.6308506727218628, acc: 0.5773809552192688)
[2025-01-06 01:00:55,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:55,617][root][INFO] - Training Epoch: 1/10, step 128/574 completed (loss: 1.7957671880722046, acc: 0.5897436141967773)
[2025-01-06 01:00:55,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:56,056][root][INFO] - Training Epoch: 1/10, step 129/574 completed (loss: 1.7102088928222656, acc: 0.5441176295280457)
[2025-01-06 01:00:56,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:56,441][root][INFO] - Training Epoch: 1/10, step 130/574 completed (loss: 2.6644608974456787, acc: 0.3461538553237915)
[2025-01-06 01:00:56,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:56,810][root][INFO] - Training Epoch: 1/10, step 131/574 completed (loss: 2.473231554031372, acc: 0.43478259444236755)
[2025-01-06 01:00:56,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:57,184][root][INFO] - Training Epoch: 1/10, step 132/574 completed (loss: 2.0688424110412598, acc: 0.5)
[2025-01-06 01:00:57,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:57,591][root][INFO] - Training Epoch: 1/10, step 133/574 completed (loss: 2.3398048877716064, acc: 0.3913043439388275)
[2025-01-06 01:00:57,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:57,951][root][INFO] - Training Epoch: 1/10, step 134/574 completed (loss: 1.6979541778564453, acc: 0.5714285969734192)
[2025-01-06 01:00:58,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:58,284][root][INFO] - Training Epoch: 1/10, step 135/574 completed (loss: 1.8395874500274658, acc: 0.5)
[2025-01-06 01:00:58,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:58,681][root][INFO] - Training Epoch: 1/10, step 136/574 completed (loss: 1.822038173675537, acc: 0.5952380895614624)
[2025-01-06 01:00:58,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:59,072][root][INFO] - Training Epoch: 1/10, step 137/574 completed (loss: 2.21230149269104, acc: 0.36666667461395264)
[2025-01-06 01:00:59,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:59,421][root][INFO] - Training Epoch: 1/10, step 138/574 completed (loss: 1.7173728942871094, acc: 0.6521739363670349)
[2025-01-06 01:00:59,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:59,745][root][INFO] - Training Epoch: 1/10, step 139/574 completed (loss: 0.6513708233833313, acc: 0.8571428656578064)
[2025-01-06 01:00:59,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:00,104][root][INFO] - Training Epoch: 1/10, step 140/574 completed (loss: 0.7563849687576294, acc: 0.7692307829856873)
[2025-01-06 01:01:00,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:00,486][root][INFO] - Training Epoch: 1/10, step 141/574 completed (loss: 1.36531662940979, acc: 0.6451612710952759)
[2025-01-06 01:01:00,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:00,887][root][INFO] - Training Epoch: 1/10, step 142/574 completed (loss: 1.5777860879898071, acc: 0.5945945978164673)
[2025-01-06 01:01:01,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:02,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:02,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:02,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:03,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:03,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:03,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:04,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:04,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:05,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:05,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:05,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:06,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:06,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:07,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:07,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:07,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:08,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:08,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:08,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:09,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:09,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:09,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:10,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:10,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:11,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:11,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:11,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:12,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:12,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:12,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:13,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:13,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:13,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:14,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:14,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:15,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:15,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:15,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:16,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:16,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:16,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:17,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:17,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:18,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:18,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:18,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:19,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:19,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:19,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:20,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:20,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:20,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:21,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:21,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:22,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:22,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:22,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:23,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:23,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:23,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:24,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:24,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:25,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:25,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:26,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:26,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:26,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:27,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:27,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:28,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:28,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:28,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:29,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:29,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:30,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:30,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:30,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:31,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:31,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:31,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:32,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:32,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:33,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:33,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:33,971][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.4351, device='cuda:0') eval_epoch_loss=tensor(1.2341, device='cuda:0') eval_epoch_acc=tensor(0.7115, device='cuda:0')
[2025-01-06 01:01:33,972][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:01:33,973][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:01:34,350][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_143_loss_1.2340573072433472/model.pt
[2025-01-06 01:01:34,360][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:01:34,361][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 1.2340573072433472
[2025-01-06 01:01:34,362][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.711460292339325
[2025-01-06 01:01:34,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:34,949][root][INFO] - Training Epoch: 1/10, step 143/574 completed (loss: 1.9549590349197388, acc: 0.5526315569877625)
[2025-01-06 01:01:35,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:35,289][root][INFO] - Training Epoch: 1/10, step 144/574 completed (loss: 1.4470947980880737, acc: 0.6865671873092651)
[2025-01-06 01:01:35,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:35,702][root][INFO] - Training Epoch: 1/10, step 145/574 completed (loss: 1.6241623163223267, acc: 0.5510203838348389)
[2025-01-06 01:01:35,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:36,203][root][INFO] - Training Epoch: 1/10, step 146/574 completed (loss: 1.932931900024414, acc: 0.478723406791687)
[2025-01-06 01:01:36,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:36,556][root][INFO] - Training Epoch: 1/10, step 147/574 completed (loss: 2.225083827972412, acc: 0.4714285731315613)
[2025-01-06 01:01:36,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:36,907][root][INFO] - Training Epoch: 1/10, step 148/574 completed (loss: 1.9084231853485107, acc: 0.4642857015132904)
[2025-01-06 01:01:36,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:37,236][root][INFO] - Training Epoch: 1/10, step 149/574 completed (loss: 1.9028998613357544, acc: 0.52173912525177)
[2025-01-06 01:01:37,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:37,616][root][INFO] - Training Epoch: 1/10, step 150/574 completed (loss: 1.4511011838912964, acc: 0.5862069129943848)
[2025-01-06 01:01:37,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:38,026][root][INFO] - Training Epoch: 1/10, step 151/574 completed (loss: 2.0051543712615967, acc: 0.5652173757553101)
[2025-01-06 01:01:38,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:38,394][root][INFO] - Training Epoch: 1/10, step 152/574 completed (loss: 1.2925492525100708, acc: 0.6610169410705566)
[2025-01-06 01:01:38,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:38,778][root][INFO] - Training Epoch: 1/10, step 153/574 completed (loss: 1.4737142324447632, acc: 0.6140350699424744)
[2025-01-06 01:01:38,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:39,145][root][INFO] - Training Epoch: 1/10, step 154/574 completed (loss: 1.682312250137329, acc: 0.6486486196517944)
[2025-01-06 01:01:39,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:39,535][root][INFO] - Training Epoch: 1/10, step 155/574 completed (loss: 1.2318115234375, acc: 0.7857142686843872)
[2025-01-06 01:01:39,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:39,922][root][INFO] - Training Epoch: 1/10, step 156/574 completed (loss: 1.203286051750183, acc: 0.6521739363670349)
[2025-01-06 01:01:40,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:40,243][root][INFO] - Training Epoch: 1/10, step 157/574 completed (loss: 2.9605486392974854, acc: 0.3684210479259491)
[2025-01-06 01:01:40,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:41,968][root][INFO] - Training Epoch: 1/10, step 158/574 completed (loss: 3.461270809173584, acc: 0.3918918967247009)
[2025-01-06 01:01:42,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:42,300][root][INFO] - Training Epoch: 1/10, step 159/574 completed (loss: 2.571671724319458, acc: 0.40740740299224854)
[2025-01-06 01:01:42,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:42,727][root][INFO] - Training Epoch: 1/10, step 160/574 completed (loss: 2.955364227294922, acc: 0.3720930218696594)
[2025-01-06 01:01:42,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:43,324][root][INFO] - Training Epoch: 1/10, step 161/574 completed (loss: 3.0573654174804688, acc: 0.3529411852359772)
[2025-01-06 01:01:43,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:43,889][root][INFO] - Training Epoch: 1/10, step 162/574 completed (loss: 3.1949915885925293, acc: 0.33707866072654724)
[2025-01-06 01:01:44,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:44,282][root][INFO] - Training Epoch: 1/10, step 163/574 completed (loss: 1.6060246229171753, acc: 0.7045454382896423)
[2025-01-06 01:01:44,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:44,696][root][INFO] - Training Epoch: 1/10, step 164/574 completed (loss: 0.9491782188415527, acc: 0.7142857313156128)
[2025-01-06 01:01:44,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:45,107][root][INFO] - Training Epoch: 1/10, step 165/574 completed (loss: 1.607667326927185, acc: 0.5517241358757019)
[2025-01-06 01:01:45,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:45,514][root][INFO] - Training Epoch: 1/10, step 166/574 completed (loss: 0.8239145278930664, acc: 0.8367347121238708)
[2025-01-06 01:01:45,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:45,944][root][INFO] - Training Epoch: 1/10, step 167/574 completed (loss: 0.7499496340751648, acc: 0.8199999928474426)
[2025-01-06 01:01:46,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:46,400][root][INFO] - Training Epoch: 1/10, step 168/574 completed (loss: 1.4847655296325684, acc: 0.7222222089767456)
[2025-01-06 01:01:46,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:46,763][root][INFO] - Training Epoch: 1/10, step 169/574 completed (loss: 1.375377893447876, acc: 0.6470588445663452)
[2025-01-06 01:01:47,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:47,792][root][INFO] - Training Epoch: 1/10, step 170/574 completed (loss: 2.257110595703125, acc: 0.5068492889404297)
[2025-01-06 01:01:47,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:48,092][root][INFO] - Training Epoch: 1/10, step 171/574 completed (loss: 1.0497509241104126, acc: 0.6666666865348816)
[2025-01-06 01:01:48,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:48,384][root][INFO] - Training Epoch: 1/10, step 172/574 completed (loss: 1.6921805143356323, acc: 0.5925925970077515)
[2025-01-06 01:01:48,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:48,704][root][INFO] - Training Epoch: 1/10, step 173/574 completed (loss: 2.112257719039917, acc: 0.4642857015132904)
[2025-01-06 01:01:48,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:49,276][root][INFO] - Training Epoch: 1/10, step 174/574 completed (loss: 1.498665452003479, acc: 0.6371681690216064)
[2025-01-06 01:01:49,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:49,653][root][INFO] - Training Epoch: 1/10, step 175/574 completed (loss: 1.578950047492981, acc: 0.6376811861991882)
[2025-01-06 01:01:49,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:50,057][root][INFO] - Training Epoch: 1/10, step 176/574 completed (loss: 1.294198989868164, acc: 0.6477272510528564)
[2025-01-06 01:01:50,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:50,987][root][INFO] - Training Epoch: 1/10, step 177/574 completed (loss: 2.227262496948242, acc: 0.49618321657180786)
[2025-01-06 01:01:51,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:51,664][root][INFO] - Training Epoch: 1/10, step 178/574 completed (loss: 2.326470136642456, acc: 0.4888888895511627)
[2025-01-06 01:01:51,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:52,073][root][INFO] - Training Epoch: 1/10, step 179/574 completed (loss: 1.264778971672058, acc: 0.6721311211585999)
[2025-01-06 01:01:52,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:52,485][root][INFO] - Training Epoch: 1/10, step 180/574 completed (loss: 0.6173291802406311, acc: 0.8333333134651184)
[2025-01-06 01:01:52,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:52,840][root][INFO] - Training Epoch: 1/10, step 181/574 completed (loss: 0.1056605651974678, acc: 1.0)
[2025-01-06 01:01:52,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:53,245][root][INFO] - Training Epoch: 1/10, step 182/574 completed (loss: 0.72105872631073, acc: 0.7857142686843872)
[2025-01-06 01:01:53,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:53,656][root][INFO] - Training Epoch: 1/10, step 183/574 completed (loss: 0.8877244591712952, acc: 0.792682945728302)
[2025-01-06 01:01:53,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:54,077][root][INFO] - Training Epoch: 1/10, step 184/574 completed (loss: 1.178899884223938, acc: 0.7643504738807678)
[2025-01-06 01:01:54,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:54,383][root][INFO] - Training Epoch: 1/10, step 185/574 completed (loss: 0.9880927205085754, acc: 0.7694524526596069)
[2025-01-06 01:01:54,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:54,943][root][INFO] - Training Epoch: 1/10, step 186/574 completed (loss: 1.0301824808120728, acc: 0.746874988079071)
[2025-01-06 01:01:55,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:55,496][root][INFO] - Training Epoch: 1/10, step 187/574 completed (loss: 0.9308052062988281, acc: 0.7786116600036621)
[2025-01-06 01:01:55,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:55,925][root][INFO] - Training Epoch: 1/10, step 188/574 completed (loss: 1.1477410793304443, acc: 0.6975088715553284)
[2025-01-06 01:01:56,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:56,216][root][INFO] - Training Epoch: 1/10, step 189/574 completed (loss: 1.1203824281692505, acc: 0.6800000071525574)
[2025-01-06 01:01:56,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:56,764][root][INFO] - Training Epoch: 1/10, step 190/574 completed (loss: 1.9090445041656494, acc: 0.5348837375640869)
[2025-01-06 01:01:56,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:57,558][root][INFO] - Training Epoch: 1/10, step 191/574 completed (loss: 2.757408618927002, acc: 0.4126984179019928)
[2025-01-06 01:01:57,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:58,481][root][INFO] - Training Epoch: 1/10, step 192/574 completed (loss: 2.4030003547668457, acc: 0.4545454680919647)
[2025-01-06 01:01:58,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:59,220][root][INFO] - Training Epoch: 1/10, step 193/574 completed (loss: 2.01914644241333, acc: 0.5176470875740051)
[2025-01-06 01:01:59,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:00,288][root][INFO] - Training Epoch: 1/10, step 194/574 completed (loss: 2.286694049835205, acc: 0.42592594027519226)
[2025-01-06 01:02:00,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:01,237][root][INFO] - Training Epoch: 1/10, step 195/574 completed (loss: 2.034234046936035, acc: 0.5161290168762207)
[2025-01-06 01:02:01,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:01,541][root][INFO] - Training Epoch: 1/10, step 196/574 completed (loss: 0.8813538551330566, acc: 0.6785714030265808)
[2025-01-06 01:02:01,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:01,891][root][INFO] - Training Epoch: 1/10, step 197/574 completed (loss: 2.1258342266082764, acc: 0.550000011920929)
[2025-01-06 01:02:02,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:02,325][root][INFO] - Training Epoch: 1/10, step 198/574 completed (loss: 1.7744529247283936, acc: 0.6470588445663452)
[2025-01-06 01:02:02,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:02,723][root][INFO] - Training Epoch: 1/10, step 199/574 completed (loss: 1.7422068119049072, acc: 0.654411792755127)
[2025-01-06 01:02:02,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:03,138][root][INFO] - Training Epoch: 1/10, step 200/574 completed (loss: 1.3642441034317017, acc: 0.6440678238868713)
[2025-01-06 01:02:03,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:03,504][root][INFO] - Training Epoch: 1/10, step 201/574 completed (loss: 1.7914937734603882, acc: 0.5820895433425903)
[2025-01-06 01:02:03,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:03,845][root][INFO] - Training Epoch: 1/10, step 202/574 completed (loss: 2.061892509460449, acc: 0.5436893105506897)
[2025-01-06 01:02:04,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:04,278][root][INFO] - Training Epoch: 1/10, step 203/574 completed (loss: 1.547580599784851, acc: 0.60317462682724)
[2025-01-06 01:02:04,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:04,617][root][INFO] - Training Epoch: 1/10, step 204/574 completed (loss: 0.40879595279693604, acc: 0.901098906993866)
[2025-01-06 01:02:04,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:04,945][root][INFO] - Training Epoch: 1/10, step 205/574 completed (loss: 0.896608829498291, acc: 0.7982062697410583)
[2025-01-06 01:02:05,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:05,364][root][INFO] - Training Epoch: 1/10, step 206/574 completed (loss: 0.8917878270149231, acc: 0.7795275449752808)
[2025-01-06 01:02:05,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:05,760][root][INFO] - Training Epoch: 1/10, step 207/574 completed (loss: 1.0290192365646362, acc: 0.7887930870056152)
[2025-01-06 01:02:05,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:06,160][root][INFO] - Training Epoch: 1/10, step 208/574 completed (loss: 0.829142689704895, acc: 0.8007246255874634)
[2025-01-06 01:02:06,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:06,562][root][INFO] - Training Epoch: 1/10, step 209/574 completed (loss: 1.019532322883606, acc: 0.774319052696228)
[2025-01-06 01:02:06,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:06,946][root][INFO] - Training Epoch: 1/10, step 210/574 completed (loss: 0.783573567867279, acc: 0.804347813129425)
[2025-01-06 01:02:07,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:07,270][root][INFO] - Training Epoch: 1/10, step 211/574 completed (loss: 0.9122943878173828, acc: 0.782608687877655)
[2025-01-06 01:02:07,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:07,588][root][INFO] - Training Epoch: 1/10, step 212/574 completed (loss: 0.45959654450416565, acc: 0.8571428656578064)
[2025-01-06 01:02:07,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:07,932][root][INFO] - Training Epoch: 1/10, step 213/574 completed (loss: 0.9614325761795044, acc: 0.8510638475418091)
[2025-01-06 01:02:08,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:08,627][root][INFO] - Training Epoch: 1/10, step 214/574 completed (loss: 1.046085000038147, acc: 0.807692289352417)
[2025-01-06 01:02:08,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:09,054][root][INFO] - Training Epoch: 1/10, step 215/574 completed (loss: 0.721667468547821, acc: 0.8108108043670654)
[2025-01-06 01:02:09,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:09,427][root][INFO] - Training Epoch: 1/10, step 216/574 completed (loss: 0.8763503432273865, acc: 0.8372092843055725)
[2025-01-06 01:02:09,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:09,991][root][INFO] - Training Epoch: 1/10, step 217/574 completed (loss: 0.774082362651825, acc: 0.837837815284729)
[2025-01-06 01:02:10,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:10,410][root][INFO] - Training Epoch: 1/10, step 218/574 completed (loss: 0.6503811478614807, acc: 0.8666666746139526)
[2025-01-06 01:02:10,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:10,751][root][INFO] - Training Epoch: 1/10, step 219/574 completed (loss: 0.6247416138648987, acc: 0.8484848737716675)
[2025-01-06 01:02:10,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:11,129][root][INFO] - Training Epoch: 1/10, step 220/574 completed (loss: 0.841682493686676, acc: 0.7407407164573669)
[2025-01-06 01:02:11,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:11,474][root][INFO] - Training Epoch: 1/10, step 221/574 completed (loss: 0.41468486189842224, acc: 0.8799999952316284)
[2025-01-06 01:02:11,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:11,869][root][INFO] - Training Epoch: 1/10, step 222/574 completed (loss: 1.4962539672851562, acc: 0.6538461446762085)
[2025-01-06 01:02:12,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:12,673][root][INFO] - Training Epoch: 1/10, step 223/574 completed (loss: 1.2454911470413208, acc: 0.7336956262588501)
[2025-01-06 01:02:12,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:13,226][root][INFO] - Training Epoch: 1/10, step 224/574 completed (loss: 1.2627167701721191, acc: 0.7102272510528564)
[2025-01-06 01:02:13,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:13,675][root][INFO] - Training Epoch: 1/10, step 225/574 completed (loss: 1.3118818998336792, acc: 0.6489361524581909)
[2025-01-06 01:02:13,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:14,069][root][INFO] - Training Epoch: 1/10, step 226/574 completed (loss: 1.9364824295043945, acc: 0.6037735939025879)
[2025-01-06 01:02:14,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:14,421][root][INFO] - Training Epoch: 1/10, step 227/574 completed (loss: 1.1364891529083252, acc: 0.6000000238418579)
[2025-01-06 01:02:14,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:14,749][root][INFO] - Training Epoch: 1/10, step 228/574 completed (loss: 1.4006057977676392, acc: 0.6279069781303406)
[2025-01-06 01:02:14,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:15,073][root][INFO] - Training Epoch: 1/10, step 229/574 completed (loss: 2.627882957458496, acc: 0.4333333373069763)
[2025-01-06 01:02:15,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:15,499][root][INFO] - Training Epoch: 1/10, step 230/574 completed (loss: 2.986215591430664, acc: 0.3052631616592407)
[2025-01-06 01:02:15,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:15,878][root][INFO] - Training Epoch: 1/10, step 231/574 completed (loss: 2.362199544906616, acc: 0.41111111640930176)
[2025-01-06 01:02:16,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:16,313][root][INFO] - Training Epoch: 1/10, step 232/574 completed (loss: 2.284888982772827, acc: 0.44999998807907104)
[2025-01-06 01:02:16,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:16,809][root][INFO] - Training Epoch: 1/10, step 233/574 completed (loss: 2.469341278076172, acc: 0.4082568883895874)
[2025-01-06 01:02:16,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:17,282][root][INFO] - Training Epoch: 1/10, step 234/574 completed (loss: 2.4537644386291504, acc: 0.4384615421295166)
[2025-01-06 01:02:17,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:17,566][root][INFO] - Training Epoch: 1/10, step 235/574 completed (loss: 1.0212740898132324, acc: 0.7368420958518982)
[2025-01-06 01:02:17,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:17,931][root][INFO] - Training Epoch: 1/10, step 236/574 completed (loss: 1.0644100904464722, acc: 0.7083333134651184)
[2025-01-06 01:02:18,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:18,291][root][INFO] - Training Epoch: 1/10, step 237/574 completed (loss: 1.4477680921554565, acc: 0.5909090638160706)
[2025-01-06 01:02:18,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:18,620][root][INFO] - Training Epoch: 1/10, step 238/574 completed (loss: 1.5191031694412231, acc: 0.5925925970077515)
[2025-01-06 01:02:18,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:19,027][root][INFO] - Training Epoch: 1/10, step 239/574 completed (loss: 1.396470546722412, acc: 0.6285714507102966)
[2025-01-06 01:02:19,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:19,509][root][INFO] - Training Epoch: 1/10, step 240/574 completed (loss: 1.687599539756775, acc: 0.5909090638160706)
[2025-01-06 01:02:19,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:19,926][root][INFO] - Training Epoch: 1/10, step 241/574 completed (loss: 1.2372450828552246, acc: 0.7045454382896423)
[2025-01-06 01:02:20,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:20,549][root][INFO] - Training Epoch: 1/10, step 242/574 completed (loss: 2.298060655593872, acc: 0.4354838728904724)
[2025-01-06 01:02:20,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:21,086][root][INFO] - Training Epoch: 1/10, step 243/574 completed (loss: 1.98967707157135, acc: 0.47727271914482117)
[2025-01-06 01:02:21,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:21,468][root][INFO] - Training Epoch: 1/10, step 244/574 completed (loss: 0.6574301719665527, acc: 0.8571428656578064)
[2025-01-06 01:02:21,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:21,824][root][INFO] - Training Epoch: 1/10, step 245/574 completed (loss: 0.9231048822402954, acc: 0.7307692170143127)
[2025-01-06 01:02:21,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:22,200][root][INFO] - Training Epoch: 1/10, step 246/574 completed (loss: 0.6213657259941101, acc: 0.8064516186714172)
[2025-01-06 01:02:22,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:22,607][root][INFO] - Training Epoch: 1/10, step 247/574 completed (loss: 0.9209882616996765, acc: 0.6000000238418579)
[2025-01-06 01:02:22,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:23,045][root][INFO] - Training Epoch: 1/10, step 248/574 completed (loss: 1.426820993423462, acc: 0.7027027010917664)
[2025-01-06 01:02:23,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:23,461][root][INFO] - Training Epoch: 1/10, step 249/574 completed (loss: 0.8498873114585876, acc: 0.7837837934494019)
[2025-01-06 01:02:23,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:23,868][root][INFO] - Training Epoch: 1/10, step 250/574 completed (loss: 0.9119333028793335, acc: 0.837837815284729)
[2025-01-06 01:02:24,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:24,269][root][INFO] - Training Epoch: 1/10, step 251/574 completed (loss: 0.7752217054367065, acc: 0.8088235259056091)
[2025-01-06 01:02:24,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:24,641][root][INFO] - Training Epoch: 1/10, step 252/574 completed (loss: 0.756057858467102, acc: 0.8048780560493469)
[2025-01-06 01:02:24,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:25,029][root][INFO] - Training Epoch: 1/10, step 253/574 completed (loss: 0.7279499769210815, acc: 0.7599999904632568)
[2025-01-06 01:02:25,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:25,433][root][INFO] - Training Epoch: 1/10, step 254/574 completed (loss: 0.24241739511489868, acc: 0.9599999785423279)
[2025-01-06 01:02:25,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:25,847][root][INFO] - Training Epoch: 1/10, step 255/574 completed (loss: 0.746372640132904, acc: 0.774193525314331)
[2025-01-06 01:02:25,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:26,212][root][INFO] - Training Epoch: 1/10, step 256/574 completed (loss: 0.8064952492713928, acc: 0.859649121761322)
[2025-01-06 01:02:26,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:26,582][root][INFO] - Training Epoch: 1/10, step 257/574 completed (loss: 0.5405676960945129, acc: 0.8571428656578064)
[2025-01-06 01:02:26,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:26,926][root][INFO] - Training Epoch: 1/10, step 258/574 completed (loss: 0.45191457867622375, acc: 0.8947368264198303)
[2025-01-06 01:02:27,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:27,521][root][INFO] - Training Epoch: 1/10, step 259/574 completed (loss: 0.8321369290351868, acc: 0.7735849022865295)
[2025-01-06 01:02:27,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:28,111][root][INFO] - Training Epoch: 1/10, step 260/574 completed (loss: 0.6877263784408569, acc: 0.7916666865348816)
[2025-01-06 01:02:28,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:28,429][root][INFO] - Training Epoch: 1/10, step 261/574 completed (loss: 0.8236425518989563, acc: 0.7777777910232544)
[2025-01-06 01:02:28,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:28,810][root][INFO] - Training Epoch: 1/10, step 262/574 completed (loss: 1.3900659084320068, acc: 0.6451612710952759)
[2025-01-06 01:02:28,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:29,235][root][INFO] - Training Epoch: 1/10, step 263/574 completed (loss: 1.7632949352264404, acc: 0.6133333444595337)
[2025-01-06 01:02:29,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:29,595][root][INFO] - Training Epoch: 1/10, step 264/574 completed (loss: 1.2382087707519531, acc: 0.625)
[2025-01-06 01:02:29,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:30,474][root][INFO] - Training Epoch: 1/10, step 265/574 completed (loss: 2.0578370094299316, acc: 0.4399999976158142)
[2025-01-06 01:02:30,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:30,876][root][INFO] - Training Epoch: 1/10, step 266/574 completed (loss: 1.9848097562789917, acc: 0.550561785697937)
[2025-01-06 01:02:31,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:31,266][root][INFO] - Training Epoch: 1/10, step 267/574 completed (loss: 1.7886708974838257, acc: 0.4864864945411682)
[2025-01-06 01:02:31,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:31,744][root][INFO] - Training Epoch: 1/10, step 268/574 completed (loss: 1.342010259628296, acc: 0.568965494632721)
[2025-01-06 01:02:31,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:32,070][root][INFO] - Training Epoch: 1/10, step 269/574 completed (loss: 0.4594060182571411, acc: 0.8636363744735718)
[2025-01-06 01:02:32,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:32,491][root][INFO] - Training Epoch: 1/10, step 270/574 completed (loss: 0.38095492124557495, acc: 0.8636363744735718)
[2025-01-06 01:02:32,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:32,842][root][INFO] - Training Epoch: 1/10, step 271/574 completed (loss: 0.5395060181617737, acc: 0.875)
[2025-01-06 01:02:32,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:33,237][root][INFO] - Training Epoch: 1/10, step 272/574 completed (loss: 0.2678928077220917, acc: 0.9333333373069763)
[2025-01-06 01:02:33,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:33,703][root][INFO] - Training Epoch: 1/10, step 273/574 completed (loss: 0.6038561463356018, acc: 0.9166666865348816)
[2025-01-06 01:02:33,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:34,074][root][INFO] - Training Epoch: 1/10, step 274/574 completed (loss: 0.7103723287582397, acc: 0.78125)
[2025-01-06 01:02:34,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:34,486][root][INFO] - Training Epoch: 1/10, step 275/574 completed (loss: 0.8151403665542603, acc: 0.8666666746139526)
[2025-01-06 01:02:34,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:34,844][root][INFO] - Training Epoch: 1/10, step 276/574 completed (loss: 0.8275508880615234, acc: 0.7931034564971924)
[2025-01-06 01:02:34,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:35,180][root][INFO] - Training Epoch: 1/10, step 277/574 completed (loss: 0.5819392204284668, acc: 0.8399999737739563)
[2025-01-06 01:02:35,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:35,475][root][INFO] - Training Epoch: 1/10, step 278/574 completed (loss: 1.18849778175354, acc: 0.7446808218955994)
[2025-01-06 01:02:35,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:35,811][root][INFO] - Training Epoch: 1/10, step 279/574 completed (loss: 0.8837224841117859, acc: 0.7708333134651184)
[2025-01-06 01:02:35,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:36,164][root][INFO] - Training Epoch: 1/10, step 280/574 completed (loss: 0.5034644603729248, acc: 0.8636363744735718)
[2025-01-06 01:02:36,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:36,600][root][INFO] - Training Epoch: 1/10, step 281/574 completed (loss: 1.4269325733184814, acc: 0.6265060305595398)
[2025-01-06 01:02:36,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:36,981][root][INFO] - Training Epoch: 1/10, step 282/574 completed (loss: 1.5381371974945068, acc: 0.6018518805503845)
[2025-01-06 01:02:37,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:37,271][root][INFO] - Training Epoch: 1/10, step 283/574 completed (loss: 0.4800514280796051, acc: 0.8684210777282715)
[2025-01-06 01:02:37,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:37,590][root][INFO] - Training Epoch: 1/10, step 284/574 completed (loss: 0.829131543636322, acc: 0.6764705777168274)
[2025-01-06 01:02:37,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:38,008][root][INFO] - Training Epoch: 1/10, step 285/574 completed (loss: 0.524396538734436, acc: 0.8999999761581421)
[2025-01-06 01:02:38,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:39,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:39,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:39,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:40,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:40,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:41,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:41,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:41,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:42,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:42,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:42,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:43,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:43,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:44,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:44,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:44,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:45,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:45,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:45,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:46,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:46,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:46,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:47,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:47,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:47,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:48,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:48,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:48,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:49,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:49,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:49,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:50,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:50,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:50,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:51,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:51,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:51,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:52,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:52,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:52,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:53,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:53,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:53,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:54,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:54,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:54,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:56,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:56,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:56,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:57,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:57,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:57,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:58,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:58,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:58,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:59,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:59,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:00,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:00,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:00,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:01,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:01,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:01,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:02,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:02,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:03,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:03,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:03,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:04,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:04,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:04,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:05,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:05,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:05,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:06,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:06,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:06,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:08,305][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4780, device='cuda:0') eval_epoch_loss=tensor(0.9075, device='cuda:0') eval_epoch_acc=tensor(0.7637, device='cuda:0')
[2025-01-06 01:03:08,306][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:03:08,306][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:03:08,592][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_286_loss_0.9074683785438538/model.pt
[2025-01-06 01:03:08,600][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:03:08,603][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.9074683785438538
[2025-01-06 01:03:08,603][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7636511325836182
[2025-01-06 01:03:08,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:09,035][root][INFO] - Training Epoch: 1/10, step 286/574 completed (loss: 0.800575852394104, acc: 0.8046875)
[2025-01-06 01:03:09,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:09,378][root][INFO] - Training Epoch: 1/10, step 287/574 completed (loss: 1.2071130275726318, acc: 0.6880000233650208)
[2025-01-06 01:03:09,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:09,735][root][INFO] - Training Epoch: 1/10, step 288/574 completed (loss: 1.182611346244812, acc: 0.7472527623176575)
[2025-01-06 01:03:09,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:10,065][root][INFO] - Training Epoch: 1/10, step 289/574 completed (loss: 1.1689751148223877, acc: 0.7453415989875793)
[2025-01-06 01:03:10,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:10,465][root][INFO] - Training Epoch: 1/10, step 290/574 completed (loss: 1.1326242685317993, acc: 0.7474226951599121)
[2025-01-06 01:03:10,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:10,826][root][INFO] - Training Epoch: 1/10, step 291/574 completed (loss: 0.6020389199256897, acc: 0.7727272510528564)
[2025-01-06 01:03:10,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:11,216][root][INFO] - Training Epoch: 1/10, step 292/574 completed (loss: 1.2851964235305786, acc: 0.6190476417541504)
[2025-01-06 01:03:11,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:11,702][root][INFO] - Training Epoch: 1/10, step 293/574 completed (loss: 0.980122983455658, acc: 0.7758620977401733)
[2025-01-06 01:03:11,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:12,216][root][INFO] - Training Epoch: 1/10, step 294/574 completed (loss: 0.9006593823432922, acc: 0.6909090876579285)
[2025-01-06 01:03:12,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:12,796][root][INFO] - Training Epoch: 1/10, step 295/574 completed (loss: 0.9171807169914246, acc: 0.7525773048400879)
[2025-01-06 01:03:12,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:13,169][root][INFO] - Training Epoch: 1/10, step 296/574 completed (loss: 1.0033504962921143, acc: 0.7413793206214905)
[2025-01-06 01:03:13,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:13,572][root][INFO] - Training Epoch: 1/10, step 297/574 completed (loss: 0.617792010307312, acc: 0.8148148059844971)
[2025-01-06 01:03:13,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:13,937][root][INFO] - Training Epoch: 1/10, step 298/574 completed (loss: 1.1562755107879639, acc: 0.6842105388641357)
[2025-01-06 01:03:14,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:14,368][root][INFO] - Training Epoch: 1/10, step 299/574 completed (loss: 0.4605979025363922, acc: 0.875)
[2025-01-06 01:03:14,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:14,788][root][INFO] - Training Epoch: 1/10, step 300/574 completed (loss: 0.32380232214927673, acc: 0.875)
[2025-01-06 01:03:14,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:15,132][root][INFO] - Training Epoch: 1/10, step 301/574 completed (loss: 0.8154354691505432, acc: 0.7735849022865295)
[2025-01-06 01:03:15,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:15,465][root][INFO] - Training Epoch: 1/10, step 302/574 completed (loss: 0.4464772641658783, acc: 0.9056603908538818)
[2025-01-06 01:03:15,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:15,787][root][INFO] - Training Epoch: 1/10, step 303/574 completed (loss: 0.25509384274482727, acc: 0.9117646813392639)
[2025-01-06 01:03:15,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:16,183][root][INFO] - Training Epoch: 1/10, step 304/574 completed (loss: 0.6133726835250854, acc: 0.75)
[2025-01-06 01:03:16,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:16,592][root][INFO] - Training Epoch: 1/10, step 305/574 completed (loss: 0.9109714031219482, acc: 0.7540983557701111)
[2025-01-06 01:03:16,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:16,964][root][INFO] - Training Epoch: 1/10, step 306/574 completed (loss: 0.9649522304534912, acc: 0.800000011920929)
[2025-01-06 01:03:17,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:17,306][root][INFO] - Training Epoch: 1/10, step 307/574 completed (loss: 0.49676308035850525, acc: 0.9473684430122375)
[2025-01-06 01:03:17,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:17,658][root][INFO] - Training Epoch: 1/10, step 308/574 completed (loss: 0.8945132493972778, acc: 0.7246376872062683)
[2025-01-06 01:03:17,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:18,120][root][INFO] - Training Epoch: 1/10, step 309/574 completed (loss: 0.9551562070846558, acc: 0.7638888955116272)
[2025-01-06 01:03:18,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:18,436][root][INFO] - Training Epoch: 1/10, step 310/574 completed (loss: 0.9806303381919861, acc: 0.7228915691375732)
[2025-01-06 01:03:18,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:18,838][root][INFO] - Training Epoch: 1/10, step 311/574 completed (loss: 0.851824939250946, acc: 0.7564102411270142)
[2025-01-06 01:03:19,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:19,270][root][INFO] - Training Epoch: 1/10, step 312/574 completed (loss: 0.4736308157444, acc: 0.8877550959587097)
[2025-01-06 01:03:19,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:19,649][root][INFO] - Training Epoch: 1/10, step 313/574 completed (loss: 0.2748422920703888, acc: 0.8333333134651184)
[2025-01-06 01:03:19,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:20,030][root][INFO] - Training Epoch: 1/10, step 314/574 completed (loss: 0.2949470579624176, acc: 0.8333333134651184)
[2025-01-06 01:03:20,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:20,430][root][INFO] - Training Epoch: 1/10, step 315/574 completed (loss: 0.41140976548194885, acc: 0.9032257795333862)
[2025-01-06 01:03:20,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:20,792][root][INFO] - Training Epoch: 1/10, step 316/574 completed (loss: 1.5117298364639282, acc: 0.7096773982048035)
[2025-01-06 01:03:20,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:21,255][root][INFO] - Training Epoch: 1/10, step 317/574 completed (loss: 0.7789700627326965, acc: 0.8358209133148193)
[2025-01-06 01:03:21,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:21,640][root][INFO] - Training Epoch: 1/10, step 318/574 completed (loss: 0.2856273353099823, acc: 0.932692289352417)
[2025-01-06 01:03:21,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:21,966][root][INFO] - Training Epoch: 1/10, step 319/574 completed (loss: 0.4508999288082123, acc: 0.8666666746139526)
[2025-01-06 01:03:22,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:22,363][root][INFO] - Training Epoch: 1/10, step 320/574 completed (loss: 0.4137743413448334, acc: 0.9032257795333862)
[2025-01-06 01:03:22,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:22,717][root][INFO] - Training Epoch: 1/10, step 321/574 completed (loss: 0.3533754348754883, acc: 0.9399999976158142)
[2025-01-06 01:03:22,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:23,093][root][INFO] - Training Epoch: 1/10, step 322/574 completed (loss: 1.9104934930801392, acc: 0.48148149251937866)
[2025-01-06 01:03:23,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:23,421][root][INFO] - Training Epoch: 1/10, step 323/574 completed (loss: 2.4777448177337646, acc: 0.37142857909202576)
[2025-01-06 01:03:23,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:23,778][root][INFO] - Training Epoch: 1/10, step 324/574 completed (loss: 2.5568830966949463, acc: 0.4615384638309479)
[2025-01-06 01:03:23,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:24,176][root][INFO] - Training Epoch: 1/10, step 325/574 completed (loss: 2.0951898097991943, acc: 0.46341463923454285)
[2025-01-06 01:03:24,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:24,536][root][INFO] - Training Epoch: 1/10, step 326/574 completed (loss: 2.2164864540100098, acc: 0.3947368562221527)
[2025-01-06 01:03:24,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:24,897][root][INFO] - Training Epoch: 1/10, step 327/574 completed (loss: 0.992021381855011, acc: 0.7368420958518982)
[2025-01-06 01:03:25,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:25,237][root][INFO] - Training Epoch: 1/10, step 328/574 completed (loss: 0.4220721423625946, acc: 0.8928571343421936)
[2025-01-06 01:03:25,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:25,561][root][INFO] - Training Epoch: 1/10, step 329/574 completed (loss: 0.4945085644721985, acc: 0.8888888955116272)
[2025-01-06 01:03:25,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:25,914][root][INFO] - Training Epoch: 1/10, step 330/574 completed (loss: 0.27728354930877686, acc: 0.90625)
[2025-01-06 01:03:26,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:26,196][root][INFO] - Training Epoch: 1/10, step 331/574 completed (loss: 0.5294846296310425, acc: 0.8709677457809448)
[2025-01-06 01:03:26,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:26,618][root][INFO] - Training Epoch: 1/10, step 332/574 completed (loss: 0.5554423928260803, acc: 0.8771929740905762)
[2025-01-06 01:03:26,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:26,988][root][INFO] - Training Epoch: 1/10, step 333/574 completed (loss: 0.7267000079154968, acc: 0.78125)
[2025-01-06 01:03:27,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:27,401][root][INFO] - Training Epoch: 1/10, step 334/574 completed (loss: 0.4007596969604492, acc: 0.8999999761581421)
[2025-01-06 01:03:27,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:27,794][root][INFO] - Training Epoch: 1/10, step 335/574 completed (loss: 0.9927200675010681, acc: 0.7368420958518982)
[2025-01-06 01:03:27,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:28,188][root][INFO] - Training Epoch: 1/10, step 336/574 completed (loss: 1.3236831426620483, acc: 0.6399999856948853)
[2025-01-06 01:03:28,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:28,622][root][INFO] - Training Epoch: 1/10, step 337/574 completed (loss: 1.7609409093856812, acc: 0.5747126340866089)
[2025-01-06 01:03:28,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29,028][root][INFO] - Training Epoch: 1/10, step 338/574 completed (loss: 1.782442569732666, acc: 0.521276593208313)
[2025-01-06 01:03:29,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29,457][root][INFO] - Training Epoch: 1/10, step 339/574 completed (loss: 1.7319250106811523, acc: 0.5542168617248535)
[2025-01-06 01:03:29,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29,826][root][INFO] - Training Epoch: 1/10, step 340/574 completed (loss: 0.4994446635246277, acc: 0.8260869383811951)
[2025-01-06 01:03:29,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:30,226][root][INFO] - Training Epoch: 1/10, step 341/574 completed (loss: 1.0789324045181274, acc: 0.7948718070983887)
[2025-01-06 01:03:30,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:30,641][root][INFO] - Training Epoch: 1/10, step 342/574 completed (loss: 0.9572851061820984, acc: 0.7228915691375732)
[2025-01-06 01:03:30,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:31,034][root][INFO] - Training Epoch: 1/10, step 343/574 completed (loss: 1.1073215007781982, acc: 0.698113203048706)
[2025-01-06 01:03:31,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:31,379][root][INFO] - Training Epoch: 1/10, step 344/574 completed (loss: 0.40788283944129944, acc: 0.8860759735107422)
[2025-01-06 01:03:31,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:31,774][root][INFO] - Training Epoch: 1/10, step 345/574 completed (loss: 0.2984617352485657, acc: 0.9215686321258545)
[2025-01-06 01:03:31,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:32,141][root][INFO] - Training Epoch: 1/10, step 346/574 completed (loss: 1.2375860214233398, acc: 0.6865671873092651)
[2025-01-06 01:03:32,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:32,530][root][INFO] - Training Epoch: 1/10, step 347/574 completed (loss: 0.43967288732528687, acc: 0.8999999761581421)
[2025-01-06 01:03:32,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:32,884][root][INFO] - Training Epoch: 1/10, step 348/574 completed (loss: 1.075378656387329, acc: 0.7200000286102295)
[2025-01-06 01:03:33,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:33,303][root][INFO] - Training Epoch: 1/10, step 349/574 completed (loss: 1.3324178457260132, acc: 0.7222222089767456)
[2025-01-06 01:03:33,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:33,624][root][INFO] - Training Epoch: 1/10, step 350/574 completed (loss: 1.5017699003219604, acc: 0.5116279125213623)
[2025-01-06 01:03:33,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:33,998][root][INFO] - Training Epoch: 1/10, step 351/574 completed (loss: 0.9801742434501648, acc: 0.7179487347602844)
[2025-01-06 01:03:34,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:34,410][root][INFO] - Training Epoch: 1/10, step 352/574 completed (loss: 2.320504665374756, acc: 0.3777777850627899)
[2025-01-06 01:03:34,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:34,766][root][INFO] - Training Epoch: 1/10, step 353/574 completed (loss: 0.33119118213653564, acc: 0.95652174949646)
[2025-01-06 01:03:34,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:35,139][root][INFO] - Training Epoch: 1/10, step 354/574 completed (loss: 1.048629641532898, acc: 0.7307692170143127)
[2025-01-06 01:03:35,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:35,534][root][INFO] - Training Epoch: 1/10, step 355/574 completed (loss: 1.4434646368026733, acc: 0.5824176073074341)
[2025-01-06 01:03:35,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:36,068][root][INFO] - Training Epoch: 1/10, step 356/574 completed (loss: 1.304717779159546, acc: 0.6173912882804871)
[2025-01-06 01:03:36,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:36,458][root][INFO] - Training Epoch: 1/10, step 357/574 completed (loss: 1.1995216608047485, acc: 0.6521739363670349)
[2025-01-06 01:03:36,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:36,833][root][INFO] - Training Epoch: 1/10, step 358/574 completed (loss: 1.0373660326004028, acc: 0.6938775777816772)
[2025-01-06 01:03:36,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:37,206][root][INFO] - Training Epoch: 1/10, step 359/574 completed (loss: 0.1597065031528473, acc: 0.9583333134651184)
[2025-01-06 01:03:37,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:37,612][root][INFO] - Training Epoch: 1/10, step 360/574 completed (loss: 0.6347118616104126, acc: 0.7692307829856873)
[2025-01-06 01:03:37,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:38,042][root][INFO] - Training Epoch: 1/10, step 361/574 completed (loss: 1.1913514137268066, acc: 0.6585366129875183)
[2025-01-06 01:03:38,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:38,465][root][INFO] - Training Epoch: 1/10, step 362/574 completed (loss: 0.8908540606498718, acc: 0.8444444537162781)
[2025-01-06 01:03:38,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:38,821][root][INFO] - Training Epoch: 1/10, step 363/574 completed (loss: 0.6556486487388611, acc: 0.8421052694320679)
[2025-01-06 01:03:38,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:39,145][root][INFO] - Training Epoch: 1/10, step 364/574 completed (loss: 0.4499427080154419, acc: 0.9024389982223511)
[2025-01-06 01:03:39,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:39,533][root][INFO] - Training Epoch: 1/10, step 365/574 completed (loss: 0.6365296244621277, acc: 0.7575757503509521)
[2025-01-06 01:03:39,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:39,966][root][INFO] - Training Epoch: 1/10, step 366/574 completed (loss: 0.1341606229543686, acc: 0.9583333134651184)
[2025-01-06 01:03:40,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:40,354][root][INFO] - Training Epoch: 1/10, step 367/574 completed (loss: 0.7416619062423706, acc: 0.739130437374115)
[2025-01-06 01:03:40,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:40,782][root][INFO] - Training Epoch: 1/10, step 368/574 completed (loss: 0.42271074652671814, acc: 0.9285714030265808)
[2025-01-06 01:03:40,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:41,211][root][INFO] - Training Epoch: 1/10, step 369/574 completed (loss: 1.1527148485183716, acc: 0.78125)
[2025-01-06 01:03:41,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:41,850][root][INFO] - Training Epoch: 1/10, step 370/574 completed (loss: 1.2338299751281738, acc: 0.6424242258071899)
[2025-01-06 01:03:42,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:42,780][root][INFO] - Training Epoch: 1/10, step 371/574 completed (loss: 0.9760611057281494, acc: 0.7547169923782349)
[2025-01-06 01:03:42,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:43,171][root][INFO] - Training Epoch: 1/10, step 372/574 completed (loss: 0.4556219279766083, acc: 0.8999999761581421)
[2025-01-06 01:03:43,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:43,542][root][INFO] - Training Epoch: 1/10, step 373/574 completed (loss: 0.6744551658630371, acc: 0.875)
[2025-01-06 01:03:43,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:43,915][root][INFO] - Training Epoch: 1/10, step 374/574 completed (loss: 0.7303482890129089, acc: 0.8571428656578064)
[2025-01-06 01:03:44,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:44,289][root][INFO] - Training Epoch: 1/10, step 375/574 completed (loss: 0.06871733069419861, acc: 1.0)
[2025-01-06 01:03:44,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:44,624][root][INFO] - Training Epoch: 1/10, step 376/574 completed (loss: 0.30556172132492065, acc: 0.8695651888847351)
[2025-01-06 01:03:44,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:45,023][root][INFO] - Training Epoch: 1/10, step 377/574 completed (loss: 0.520036518573761, acc: 0.8958333134651184)
[2025-01-06 01:03:45,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:45,458][root][INFO] - Training Epoch: 1/10, step 378/574 completed (loss: 0.3549460172653198, acc: 0.9052631855010986)
[2025-01-06 01:03:45,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:46,081][root][INFO] - Training Epoch: 1/10, step 379/574 completed (loss: 0.617901623249054, acc: 0.8502994179725647)
[2025-01-06 01:03:46,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:46,515][root][INFO] - Training Epoch: 1/10, step 380/574 completed (loss: 0.7491542100906372, acc: 0.8045112490653992)
[2025-01-06 01:03:47,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:47,847][root][INFO] - Training Epoch: 1/10, step 381/574 completed (loss: 1.1627987623214722, acc: 0.6951871514320374)
[2025-01-06 01:03:48,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:48,419][root][INFO] - Training Epoch: 1/10, step 382/574 completed (loss: 0.5621185898780823, acc: 0.8738738894462585)
[2025-01-06 01:03:48,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:48,725][root][INFO] - Training Epoch: 1/10, step 383/574 completed (loss: 0.9198907017707825, acc: 0.7857142686843872)
[2025-01-06 01:03:48,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:49,059][root][INFO] - Training Epoch: 1/10, step 384/574 completed (loss: 0.130144864320755, acc: 1.0)
[2025-01-06 01:03:49,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:49,392][root][INFO] - Training Epoch: 1/10, step 385/574 completed (loss: 0.392193078994751, acc: 0.90625)
[2025-01-06 01:03:49,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:49,736][root][INFO] - Training Epoch: 1/10, step 386/574 completed (loss: 0.3417164087295532, acc: 0.9444444179534912)
[2025-01-06 01:03:49,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:50,100][root][INFO] - Training Epoch: 1/10, step 387/574 completed (loss: 0.13900156319141388, acc: 0.9736841917037964)
[2025-01-06 01:03:50,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:50,479][root][INFO] - Training Epoch: 1/10, step 388/574 completed (loss: 0.22418004274368286, acc: 0.9090909361839294)
[2025-01-06 01:03:50,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:50,862][root][INFO] - Training Epoch: 1/10, step 389/574 completed (loss: 0.16893219947814941, acc: 0.949999988079071)
[2025-01-06 01:03:50,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:51,187][root][INFO] - Training Epoch: 1/10, step 390/574 completed (loss: 0.9741240739822388, acc: 0.8095238208770752)
[2025-01-06 01:03:51,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:51,615][root][INFO] - Training Epoch: 1/10, step 391/574 completed (loss: 1.4040613174438477, acc: 0.5555555820465088)
[2025-01-06 01:03:51,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:51,989][root][INFO] - Training Epoch: 1/10, step 392/574 completed (loss: 1.3457921743392944, acc: 0.6796116232872009)
[2025-01-06 01:03:52,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:52,522][root][INFO] - Training Epoch: 1/10, step 393/574 completed (loss: 1.3180418014526367, acc: 0.7132353186607361)
[2025-01-06 01:03:52,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:52,900][root][INFO] - Training Epoch: 1/10, step 394/574 completed (loss: 1.458412766456604, acc: 0.6200000047683716)
[2025-01-06 01:03:53,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:53,297][root][INFO] - Training Epoch: 1/10, step 395/574 completed (loss: 1.2426193952560425, acc: 0.6666666865348816)
[2025-01-06 01:03:53,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:53,630][root][INFO] - Training Epoch: 1/10, step 396/574 completed (loss: 0.8792814016342163, acc: 0.7674418687820435)
[2025-01-06 01:03:53,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:53,948][root][INFO] - Training Epoch: 1/10, step 397/574 completed (loss: 0.5168969035148621, acc: 0.875)
[2025-01-06 01:03:54,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:54,311][root][INFO] - Training Epoch: 1/10, step 398/574 completed (loss: 0.6819217801094055, acc: 0.7674418687820435)
[2025-01-06 01:03:54,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:54,715][root][INFO] - Training Epoch: 1/10, step 399/574 completed (loss: 0.47838741540908813, acc: 0.8799999952316284)
[2025-01-06 01:03:54,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:55,333][root][INFO] - Training Epoch: 1/10, step 400/574 completed (loss: 0.6825177669525146, acc: 0.7941176295280457)
[2025-01-06 01:03:55,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:55,708][root][INFO] - Training Epoch: 1/10, step 401/574 completed (loss: 0.6369131207466125, acc: 0.800000011920929)
[2025-01-06 01:03:55,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:56,108][root][INFO] - Training Epoch: 1/10, step 402/574 completed (loss: 1.2276859283447266, acc: 0.6969696879386902)
[2025-01-06 01:03:56,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:56,449][root][INFO] - Training Epoch: 1/10, step 403/574 completed (loss: 0.593711793422699, acc: 0.7575757503509521)
[2025-01-06 01:03:56,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:56,830][root][INFO] - Training Epoch: 1/10, step 404/574 completed (loss: 1.0676870346069336, acc: 0.7419354915618896)
[2025-01-06 01:03:56,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:57,221][root][INFO] - Training Epoch: 1/10, step 405/574 completed (loss: 0.27686911821365356, acc: 0.8518518805503845)
[2025-01-06 01:03:57,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:57,651][root][INFO] - Training Epoch: 1/10, step 406/574 completed (loss: 0.5479732751846313, acc: 0.8399999737739563)
[2025-01-06 01:03:57,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:58,036][root][INFO] - Training Epoch: 1/10, step 407/574 completed (loss: 0.32269275188446045, acc: 0.8888888955116272)
[2025-01-06 01:03:58,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:58,410][root][INFO] - Training Epoch: 1/10, step 408/574 completed (loss: 0.4737546741962433, acc: 0.8888888955116272)
[2025-01-06 01:03:58,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:58,743][root][INFO] - Training Epoch: 1/10, step 409/574 completed (loss: 0.4564135670661926, acc: 0.8846153616905212)
[2025-01-06 01:03:58,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:59,090][root][INFO] - Training Epoch: 1/10, step 410/574 completed (loss: 0.648516058921814, acc: 0.8620689511299133)
[2025-01-06 01:03:59,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:59,475][root][INFO] - Training Epoch: 1/10, step 411/574 completed (loss: 0.15739594399929047, acc: 1.0)
[2025-01-06 01:03:59,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:59,868][root][INFO] - Training Epoch: 1/10, step 412/574 completed (loss: 0.4289228320121765, acc: 0.8666666746139526)
[2025-01-06 01:04:00,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:00,272][root][INFO] - Training Epoch: 1/10, step 413/574 completed (loss: 0.6232861280441284, acc: 0.8484848737716675)
[2025-01-06 01:04:00,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:00,643][root][INFO] - Training Epoch: 1/10, step 414/574 completed (loss: 0.6507843136787415, acc: 0.8636363744735718)
[2025-01-06 01:04:00,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01,003][root][INFO] - Training Epoch: 1/10, step 415/574 completed (loss: 0.777901291847229, acc: 0.7647058963775635)
[2025-01-06 01:04:01,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01,329][root][INFO] - Training Epoch: 1/10, step 416/574 completed (loss: 0.6846603155136108, acc: 0.807692289352417)
[2025-01-06 01:04:01,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01,643][root][INFO] - Training Epoch: 1/10, step 417/574 completed (loss: 0.7815579771995544, acc: 0.7777777910232544)
[2025-01-06 01:04:01,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01,995][root][INFO] - Training Epoch: 1/10, step 418/574 completed (loss: 0.7046415209770203, acc: 0.800000011920929)
[2025-01-06 01:04:02,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:02,318][root][INFO] - Training Epoch: 1/10, step 419/574 completed (loss: 0.9392013549804688, acc: 0.8500000238418579)
[2025-01-06 01:04:02,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:02,702][root][INFO] - Training Epoch: 1/10, step 420/574 completed (loss: 0.4147797226905823, acc: 0.8571428656578064)
[2025-01-06 01:04:02,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:03,104][root][INFO] - Training Epoch: 1/10, step 421/574 completed (loss: 0.6348479986190796, acc: 0.8333333134651184)
[2025-01-06 01:04:03,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:03,454][root][INFO] - Training Epoch: 1/10, step 422/574 completed (loss: 0.7560218572616577, acc: 0.75)
[2025-01-06 01:04:03,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:03,842][root][INFO] - Training Epoch: 1/10, step 423/574 completed (loss: 1.1663399934768677, acc: 0.7222222089767456)
[2025-01-06 01:04:03,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:04,238][root][INFO] - Training Epoch: 1/10, step 424/574 completed (loss: 0.9293185472488403, acc: 0.8888888955116272)
[2025-01-06 01:04:04,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:04,635][root][INFO] - Training Epoch: 1/10, step 425/574 completed (loss: 0.5985555052757263, acc: 0.9090909361839294)
[2025-01-06 01:04:04,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:04,975][root][INFO] - Training Epoch: 1/10, step 426/574 completed (loss: 0.3075922429561615, acc: 0.9130434989929199)
[2025-01-06 01:04:05,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:05,364][root][INFO] - Training Epoch: 1/10, step 427/574 completed (loss: 0.45873352885246277, acc: 0.8918918967247009)
[2025-01-06 01:04:05,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:05,763][root][INFO] - Training Epoch: 1/10, step 428/574 completed (loss: 0.5386975407600403, acc: 0.8888888955116272)
[2025-01-06 01:04:06,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:06,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:07,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:07,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:07,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:08,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:08,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:08,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:09,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:09,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:10,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:10,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:10,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:11,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:11,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:12,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:12,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:12,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:13,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:13,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:14,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:14,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:14,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:15,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:15,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:16,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:16,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:16,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:17,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:17,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:18,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:18,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:18,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:19,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:19,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:19,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:20,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:20,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:20,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:22,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:22,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:23,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:23,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:23,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:24,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:24,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:24,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:25,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:25,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:25,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:26,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:26,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:26,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:27,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:27,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:27,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:28,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:28,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:28,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:29,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:29,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:30,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:30,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:30,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:31,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:31,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:31,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:32,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:32,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:32,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:33,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:33,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:33,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:34,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:34,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:34,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:35,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:35,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:35,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:36,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:36,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:37,261][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2722, device='cuda:0') eval_epoch_loss=tensor(0.8207, device='cuda:0') eval_epoch_acc=tensor(0.7822, device='cuda:0')
[2025-01-06 01:04:37,262][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:04:37,263][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:04:37,524][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_429_loss_0.8207386136054993/model.pt
[2025-01-06 01:04:37,536][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:04:37,537][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.8207386136054993
[2025-01-06 01:04:37,538][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7821862697601318
[2025-01-06 01:04:37,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:37,936][root][INFO] - Training Epoch: 1/10, step 429/574 completed (loss: 0.6107479333877563, acc: 0.8695651888847351)
[2025-01-06 01:04:38,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:38,307][root][INFO] - Training Epoch: 1/10, step 430/574 completed (loss: 0.0583699494600296, acc: 1.0)
[2025-01-06 01:04:38,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:38,683][root][INFO] - Training Epoch: 1/10, step 431/574 completed (loss: 0.3080192506313324, acc: 0.9259259104728699)
[2025-01-06 01:04:38,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:39,058][root][INFO] - Training Epoch: 1/10, step 432/574 completed (loss: 0.9462891221046448, acc: 0.782608687877655)
[2025-01-06 01:04:39,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:39,520][root][INFO] - Training Epoch: 1/10, step 433/574 completed (loss: 0.4937931299209595, acc: 0.8888888955116272)
[2025-01-06 01:04:39,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:39,928][root][INFO] - Training Epoch: 1/10, step 434/574 completed (loss: 0.0135536203160882, acc: 1.0)
[2025-01-06 01:04:40,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:40,342][root][INFO] - Training Epoch: 1/10, step 435/574 completed (loss: 0.19751256704330444, acc: 0.939393937587738)
[2025-01-06 01:04:40,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:40,769][root][INFO] - Training Epoch: 1/10, step 436/574 completed (loss: 0.5690749287605286, acc: 0.8611111044883728)
[2025-01-06 01:04:40,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:41,186][root][INFO] - Training Epoch: 1/10, step 437/574 completed (loss: 0.43743622303009033, acc: 0.8863636255264282)
[2025-01-06 01:04:41,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:41,537][root][INFO] - Training Epoch: 1/10, step 438/574 completed (loss: 0.20373959839344025, acc: 0.9523809552192688)
[2025-01-06 01:04:41,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:41,918][root][INFO] - Training Epoch: 1/10, step 439/574 completed (loss: 0.8317444920539856, acc: 0.8205128312110901)
[2025-01-06 01:04:42,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:42,425][root][INFO] - Training Epoch: 1/10, step 440/574 completed (loss: 0.9778807759284973, acc: 0.6969696879386902)
[2025-01-06 01:04:42,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:43,180][root][INFO] - Training Epoch: 1/10, step 441/574 completed (loss: 1.385555624961853, acc: 0.6159999966621399)
[2025-01-06 01:04:43,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:43,612][root][INFO] - Training Epoch: 1/10, step 442/574 completed (loss: 1.2924500703811646, acc: 0.6451612710952759)
[2025-01-06 01:04:43,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:44,323][root][INFO] - Training Epoch: 1/10, step 443/574 completed (loss: 0.9760524034500122, acc: 0.7910447716712952)
[2025-01-06 01:04:44,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:44,728][root][INFO] - Training Epoch: 1/10, step 444/574 completed (loss: 0.6218789219856262, acc: 0.7924528121948242)
[2025-01-06 01:04:44,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:45,193][root][INFO] - Training Epoch: 1/10, step 445/574 completed (loss: 0.5878549814224243, acc: 0.8636363744735718)
[2025-01-06 01:04:45,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:45,575][root][INFO] - Training Epoch: 1/10, step 446/574 completed (loss: 0.7213225364685059, acc: 0.8260869383811951)
[2025-01-06 01:04:45,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:45,951][root][INFO] - Training Epoch: 1/10, step 447/574 completed (loss: 1.1609572172164917, acc: 0.7692307829856873)
[2025-01-06 01:04:46,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:46,312][root][INFO] - Training Epoch: 1/10, step 448/574 completed (loss: 0.3881829082965851, acc: 0.9285714030265808)
[2025-01-06 01:04:46,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:46,661][root][INFO] - Training Epoch: 1/10, step 449/574 completed (loss: 0.5998897552490234, acc: 0.8656716346740723)
[2025-01-06 01:04:46,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:46,977][root][INFO] - Training Epoch: 1/10, step 450/574 completed (loss: 0.2890923023223877, acc: 0.9305555820465088)
[2025-01-06 01:04:47,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:47,348][root][INFO] - Training Epoch: 1/10, step 451/574 completed (loss: 0.2511082589626312, acc: 0.9347826242446899)
[2025-01-06 01:04:47,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:47,701][root][INFO] - Training Epoch: 1/10, step 452/574 completed (loss: 0.5718399882316589, acc: 0.8333333134651184)
[2025-01-06 01:04:47,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:48,031][root][INFO] - Training Epoch: 1/10, step 453/574 completed (loss: 0.9038090705871582, acc: 0.8421052694320679)
[2025-01-06 01:04:48,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:48,459][root][INFO] - Training Epoch: 1/10, step 454/574 completed (loss: 0.5114114284515381, acc: 0.8571428656578064)
[2025-01-06 01:04:48,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:48,859][root][INFO] - Training Epoch: 1/10, step 455/574 completed (loss: 0.5477638840675354, acc: 0.8787878751754761)
[2025-01-06 01:04:48,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:49,215][root][INFO] - Training Epoch: 1/10, step 456/574 completed (loss: 0.7635353207588196, acc: 0.8247422575950623)
[2025-01-06 01:04:49,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:49,594][root][INFO] - Training Epoch: 1/10, step 457/574 completed (loss: 0.278919517993927, acc: 0.9428571462631226)
[2025-01-06 01:04:49,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:49,988][root][INFO] - Training Epoch: 1/10, step 458/574 completed (loss: 0.9675696492195129, acc: 0.7558139562606812)
[2025-01-06 01:04:50,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:50,299][root][INFO] - Training Epoch: 1/10, step 459/574 completed (loss: 0.2173621654510498, acc: 0.9464285969734192)
[2025-01-06 01:04:50,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:50,666][root][INFO] - Training Epoch: 1/10, step 460/574 completed (loss: 0.5489699244499207, acc: 0.8395061492919922)
[2025-01-06 01:04:50,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:51,009][root][INFO] - Training Epoch: 1/10, step 461/574 completed (loss: 0.9013494849205017, acc: 0.7222222089767456)
[2025-01-06 01:04:51,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:51,347][root][INFO] - Training Epoch: 1/10, step 462/574 completed (loss: 0.4354873597621918, acc: 0.875)
[2025-01-06 01:04:51,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:51,726][root][INFO] - Training Epoch: 1/10, step 463/574 completed (loss: 0.5493838787078857, acc: 0.8846153616905212)
[2025-01-06 01:04:51,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:52,073][root][INFO] - Training Epoch: 1/10, step 464/574 completed (loss: 0.5402861833572388, acc: 0.782608687877655)
[2025-01-06 01:04:52,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:52,458][root][INFO] - Training Epoch: 1/10, step 465/574 completed (loss: 0.7293288111686707, acc: 0.773809552192688)
[2025-01-06 01:04:52,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:52,796][root][INFO] - Training Epoch: 1/10, step 466/574 completed (loss: 0.8611147999763489, acc: 0.7831325531005859)
[2025-01-06 01:04:52,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:53,205][root][INFO] - Training Epoch: 1/10, step 467/574 completed (loss: 0.5321391820907593, acc: 0.8648648858070374)
[2025-01-06 01:04:53,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:53,572][root][INFO] - Training Epoch: 1/10, step 468/574 completed (loss: 1.2532474994659424, acc: 0.708737850189209)
[2025-01-06 01:04:53,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:53,900][root][INFO] - Training Epoch: 1/10, step 469/574 completed (loss: 0.8582654595375061, acc: 0.772357702255249)
[2025-01-06 01:04:53,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:54,230][root][INFO] - Training Epoch: 1/10, step 470/574 completed (loss: 0.5404624938964844, acc: 0.8333333134651184)
[2025-01-06 01:04:54,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:54,627][root][INFO] - Training Epoch: 1/10, step 471/574 completed (loss: 0.8309752345085144, acc: 0.75)
[2025-01-06 01:04:54,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:55,086][root][INFO] - Training Epoch: 1/10, step 472/574 completed (loss: 1.300202488899231, acc: 0.6078431606292725)
[2025-01-06 01:04:55,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:55,507][root][INFO] - Training Epoch: 1/10, step 473/574 completed (loss: 1.024533748626709, acc: 0.7379912734031677)
[2025-01-06 01:04:55,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:55,925][root][INFO] - Training Epoch: 1/10, step 474/574 completed (loss: 0.9437771439552307, acc: 0.7395833134651184)
[2025-01-06 01:04:56,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:56,310][root][INFO] - Training Epoch: 1/10, step 475/574 completed (loss: 0.6173370480537415, acc: 0.8098159432411194)
[2025-01-06 01:04:56,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:56,716][root][INFO] - Training Epoch: 1/10, step 476/574 completed (loss: 0.6626073718070984, acc: 0.8201438784599304)
[2025-01-06 01:04:56,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:57,107][root][INFO] - Training Epoch: 1/10, step 477/574 completed (loss: 1.1828337907791138, acc: 0.6733668446540833)
[2025-01-06 01:04:57,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:57,504][root][INFO] - Training Epoch: 1/10, step 478/574 completed (loss: 0.9856246113777161, acc: 0.75)
[2025-01-06 01:04:57,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:57,873][root][INFO] - Training Epoch: 1/10, step 479/574 completed (loss: 1.298697590827942, acc: 0.6363636255264282)
[2025-01-06 01:04:58,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:58,254][root][INFO] - Training Epoch: 1/10, step 480/574 completed (loss: 1.0767120122909546, acc: 0.7037037014961243)
[2025-01-06 01:04:58,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:58,652][root][INFO] - Training Epoch: 1/10, step 481/574 completed (loss: 1.096494436264038, acc: 0.699999988079071)
[2025-01-06 01:04:58,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:59,065][root][INFO] - Training Epoch: 1/10, step 482/574 completed (loss: 1.3855215311050415, acc: 0.6499999761581421)
[2025-01-06 01:04:59,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:59,489][root][INFO] - Training Epoch: 1/10, step 483/574 completed (loss: 1.3720108270645142, acc: 0.517241358757019)
[2025-01-06 01:04:59,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:59,875][root][INFO] - Training Epoch: 1/10, step 484/574 completed (loss: 0.34149572253227234, acc: 0.9032257795333862)
[2025-01-06 01:05:00,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:00,250][root][INFO] - Training Epoch: 1/10, step 485/574 completed (loss: 1.070867657661438, acc: 0.7894737124443054)
[2025-01-06 01:05:00,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:00,572][root][INFO] - Training Epoch: 1/10, step 486/574 completed (loss: 1.9896806478500366, acc: 0.37037035822868347)
[2025-01-06 01:05:00,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:00,884][root][INFO] - Training Epoch: 1/10, step 487/574 completed (loss: 0.9430075287818909, acc: 0.5714285969734192)
[2025-01-06 01:05:00,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:01,234][root][INFO] - Training Epoch: 1/10, step 488/574 completed (loss: 1.364682912826538, acc: 0.7272727489471436)
[2025-01-06 01:05:01,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:01,650][root][INFO] - Training Epoch: 1/10, step 489/574 completed (loss: 1.380945086479187, acc: 0.6307692527770996)
[2025-01-06 01:05:01,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:02,001][root][INFO] - Training Epoch: 1/10, step 490/574 completed (loss: 0.748026430606842, acc: 0.8333333134651184)
[2025-01-06 01:05:02,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:02,337][root][INFO] - Training Epoch: 1/10, step 491/574 completed (loss: 0.8464487195014954, acc: 0.8275862336158752)
[2025-01-06 01:05:02,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:02,691][root][INFO] - Training Epoch: 1/10, step 492/574 completed (loss: 0.8348668217658997, acc: 0.7058823704719543)
[2025-01-06 01:05:02,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:03,013][root][INFO] - Training Epoch: 1/10, step 493/574 completed (loss: 0.7297345399856567, acc: 0.7931034564971924)
[2025-01-06 01:05:03,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:03,397][root][INFO] - Training Epoch: 1/10, step 494/574 completed (loss: 0.8753478527069092, acc: 0.7894737124443054)
[2025-01-06 01:05:03,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:03,807][root][INFO] - Training Epoch: 1/10, step 495/574 completed (loss: 1.0834988355636597, acc: 0.7368420958518982)
[2025-01-06 01:05:03,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:04,217][root][INFO] - Training Epoch: 1/10, step 496/574 completed (loss: 1.034338355064392, acc: 0.7232142686843872)
[2025-01-06 01:05:04,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:04,613][root][INFO] - Training Epoch: 1/10, step 497/574 completed (loss: 0.7710062265396118, acc: 0.7977527976036072)
[2025-01-06 01:05:04,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:04,964][root][INFO] - Training Epoch: 1/10, step 498/574 completed (loss: 1.0848405361175537, acc: 0.6853932738304138)
[2025-01-06 01:05:05,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:05,318][root][INFO] - Training Epoch: 1/10, step 499/574 completed (loss: 1.717499017715454, acc: 0.5390070676803589)
[2025-01-06 01:05:05,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:05,719][root][INFO] - Training Epoch: 1/10, step 500/574 completed (loss: 1.2562012672424316, acc: 0.739130437374115)
[2025-01-06 01:05:05,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:06,057][root][INFO] - Training Epoch: 1/10, step 501/574 completed (loss: 0.14643943309783936, acc: 1.0)
[2025-01-06 01:05:06,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:06,426][root][INFO] - Training Epoch: 1/10, step 502/574 completed (loss: 0.34026798605918884, acc: 0.8846153616905212)
[2025-01-06 01:05:06,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:06,812][root][INFO] - Training Epoch: 1/10, step 503/574 completed (loss: 0.6445216536521912, acc: 0.7777777910232544)
[2025-01-06 01:05:06,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:07,127][root][INFO] - Training Epoch: 1/10, step 504/574 completed (loss: 0.49842390418052673, acc: 0.8518518805503845)
[2025-01-06 01:05:07,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:07,422][root][INFO] - Training Epoch: 1/10, step 505/574 completed (loss: 0.8640716671943665, acc: 0.8301886916160583)
[2025-01-06 01:05:07,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:07,805][root][INFO] - Training Epoch: 1/10, step 506/574 completed (loss: 0.6573166251182556, acc: 0.8620689511299133)
[2025-01-06 01:05:08,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:08,456][root][INFO] - Training Epoch: 1/10, step 507/574 completed (loss: 1.6458792686462402, acc: 0.5495495200157166)
[2025-01-06 01:05:08,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:08,915][root][INFO] - Training Epoch: 1/10, step 508/574 completed (loss: 1.0776070356369019, acc: 0.7605633735656738)
[2025-01-06 01:05:08,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:09,256][root][INFO] - Training Epoch: 1/10, step 509/574 completed (loss: 0.42100539803504944, acc: 0.8500000238418579)
[2025-01-06 01:05:09,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:09,614][root][INFO] - Training Epoch: 1/10, step 510/574 completed (loss: 0.7084566950798035, acc: 0.7333333492279053)
[2025-01-06 01:05:09,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:09,998][root][INFO] - Training Epoch: 1/10, step 511/574 completed (loss: 0.909852147102356, acc: 0.7307692170143127)
[2025-01-06 01:05:11,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:12,635][root][INFO] - Training Epoch: 1/10, step 512/574 completed (loss: 1.7807687520980835, acc: 0.5714285969734192)
[2025-01-06 01:05:12,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:13,396][root][INFO] - Training Epoch: 1/10, step 513/574 completed (loss: 0.7448337078094482, acc: 0.8333333134651184)
[2025-01-06 01:05:13,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:13,773][root][INFO] - Training Epoch: 1/10, step 514/574 completed (loss: 0.982801616191864, acc: 0.7857142686843872)
[2025-01-06 01:05:13,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:14,177][root][INFO] - Training Epoch: 1/10, step 515/574 completed (loss: 0.32773035764694214, acc: 0.8999999761581421)
[2025-01-06 01:05:14,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:14,893][root][INFO] - Training Epoch: 1/10, step 516/574 completed (loss: 0.8477798700332642, acc: 0.7916666865348816)
[2025-01-06 01:05:15,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:15,269][root][INFO] - Training Epoch: 1/10, step 517/574 completed (loss: 0.08881428837776184, acc: 0.9615384340286255)
[2025-01-06 01:05:15,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:15,606][root][INFO] - Training Epoch: 1/10, step 518/574 completed (loss: 0.4121459424495697, acc: 0.8387096524238586)
[2025-01-06 01:05:15,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:15,918][root][INFO] - Training Epoch: 1/10, step 519/574 completed (loss: 0.5745356678962708, acc: 0.800000011920929)
[2025-01-06 01:05:16,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:16,347][root][INFO] - Training Epoch: 1/10, step 520/574 completed (loss: 0.6283704042434692, acc: 0.8148148059844971)
[2025-01-06 01:05:16,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:17,372][root][INFO] - Training Epoch: 1/10, step 521/574 completed (loss: 0.9966349005699158, acc: 0.7372881174087524)
[2025-01-06 01:05:17,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:17,739][root][INFO] - Training Epoch: 1/10, step 522/574 completed (loss: 0.5043137669563293, acc: 0.8507462739944458)
[2025-01-06 01:05:17,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:18,164][root][INFO] - Training Epoch: 1/10, step 523/574 completed (loss: 0.5579720735549927, acc: 0.8029196858406067)
[2025-01-06 01:05:18,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:18,743][root][INFO] - Training Epoch: 1/10, step 524/574 completed (loss: 1.038009524345398, acc: 0.6899999976158142)
[2025-01-06 01:05:18,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:19,142][root][INFO] - Training Epoch: 1/10, step 525/574 completed (loss: 0.08338426798582077, acc: 1.0)
[2025-01-06 01:05:19,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:19,512][root][INFO] - Training Epoch: 1/10, step 526/574 completed (loss: 0.3480377793312073, acc: 0.8846153616905212)
[2025-01-06 01:05:19,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:19,880][root][INFO] - Training Epoch: 1/10, step 527/574 completed (loss: 0.6482868194580078, acc: 0.8571428656578064)
[2025-01-06 01:05:20,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:20,266][root][INFO] - Training Epoch: 1/10, step 528/574 completed (loss: 2.339184284210205, acc: 0.4262295067310333)
[2025-01-06 01:05:20,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:20,638][root][INFO] - Training Epoch: 1/10, step 529/574 completed (loss: 0.5050150752067566, acc: 0.7966101765632629)
[2025-01-06 01:05:20,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:20,999][root][INFO] - Training Epoch: 1/10, step 530/574 completed (loss: 1.8235059976577759, acc: 0.5581395626068115)
[2025-01-06 01:05:21,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:21,390][root][INFO] - Training Epoch: 1/10, step 531/574 completed (loss: 1.4658379554748535, acc: 0.6590909361839294)
[2025-01-06 01:05:21,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:21,780][root][INFO] - Training Epoch: 1/10, step 532/574 completed (loss: 1.544353723526001, acc: 0.6037735939025879)
[2025-01-06 01:05:21,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:22,128][root][INFO] - Training Epoch: 1/10, step 533/574 completed (loss: 1.3673487901687622, acc: 0.6818181872367859)
[2025-01-06 01:05:22,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:22,471][root][INFO] - Training Epoch: 1/10, step 534/574 completed (loss: 1.0020240545272827, acc: 0.7599999904632568)
[2025-01-06 01:05:22,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:22,837][root][INFO] - Training Epoch: 1/10, step 535/574 completed (loss: 0.8914302587509155, acc: 0.800000011920929)
[2025-01-06 01:05:22,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:23,162][root][INFO] - Training Epoch: 1/10, step 536/574 completed (loss: 0.5468420386314392, acc: 0.8181818127632141)
[2025-01-06 01:05:23,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:23,595][root][INFO] - Training Epoch: 1/10, step 537/574 completed (loss: 0.9660962820053101, acc: 0.7692307829856873)
[2025-01-06 01:05:23,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:24,007][root][INFO] - Training Epoch: 1/10, step 538/574 completed (loss: 0.9848487377166748, acc: 0.75)
[2025-01-06 01:05:24,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:24,440][root][INFO] - Training Epoch: 1/10, step 539/574 completed (loss: 0.8472231030464172, acc: 0.75)
[2025-01-06 01:05:24,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:24,803][root][INFO] - Training Epoch: 1/10, step 540/574 completed (loss: 1.2104729413986206, acc: 0.6666666865348816)
[2025-01-06 01:05:24,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25,118][root][INFO] - Training Epoch: 1/10, step 541/574 completed (loss: 0.7870060801506042, acc: 0.6875)
[2025-01-06 01:05:25,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25,466][root][INFO] - Training Epoch: 1/10, step 542/574 completed (loss: 0.18602575361728668, acc: 0.9354838728904724)
[2025-01-06 01:05:25,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25,837][root][INFO] - Training Epoch: 1/10, step 543/574 completed (loss: 0.32974886894226074, acc: 0.95652174949646)
[2025-01-06 01:05:25,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:26,184][root][INFO] - Training Epoch: 1/10, step 544/574 completed (loss: 0.38916221261024475, acc: 0.8999999761581421)
[2025-01-06 01:05:26,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:26,565][root][INFO] - Training Epoch: 1/10, step 545/574 completed (loss: 0.30903303623199463, acc: 0.8780487775802612)
[2025-01-06 01:05:26,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:26,928][root][INFO] - Training Epoch: 1/10, step 546/574 completed (loss: 0.04746584966778755, acc: 0.9714285731315613)
[2025-01-06 01:05:27,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:27,291][root][INFO] - Training Epoch: 1/10, step 547/574 completed (loss: 0.10361748933792114, acc: 0.9736841917037964)
[2025-01-06 01:05:27,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:27,652][root][INFO] - Training Epoch: 1/10, step 548/574 completed (loss: 0.7043358683586121, acc: 0.8064516186714172)
[2025-01-06 01:05:27,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:28,015][root][INFO] - Training Epoch: 1/10, step 549/574 completed (loss: 0.07201486825942993, acc: 1.0)
[2025-01-06 01:05:28,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:28,381][root][INFO] - Training Epoch: 1/10, step 550/574 completed (loss: 0.5123323798179626, acc: 0.9090909361839294)
[2025-01-06 01:05:28,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:28,730][root][INFO] - Training Epoch: 1/10, step 551/574 completed (loss: 0.3498573899269104, acc: 0.8999999761581421)
[2025-01-06 01:05:28,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:29,090][root][INFO] - Training Epoch: 1/10, step 552/574 completed (loss: 0.40772366523742676, acc: 0.8714285492897034)
[2025-01-06 01:05:29,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:29,456][root][INFO] - Training Epoch: 1/10, step 553/574 completed (loss: 0.6664871573448181, acc: 0.8321167826652527)
[2025-01-06 01:05:29,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:29,863][root][INFO] - Training Epoch: 1/10, step 554/574 completed (loss: 0.55913907289505, acc: 0.8413792848587036)
[2025-01-06 01:05:29,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:30,222][root][INFO] - Training Epoch: 1/10, step 555/574 completed (loss: 0.6280606985092163, acc: 0.8428571224212646)
[2025-01-06 01:05:30,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:30,623][root][INFO] - Training Epoch: 1/10, step 556/574 completed (loss: 0.6382216215133667, acc: 0.8344370722770691)
[2025-01-06 01:05:30,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:31,052][root][INFO] - Training Epoch: 1/10, step 557/574 completed (loss: 0.5511949062347412, acc: 0.8803418874740601)
[2025-01-06 01:05:31,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:31,441][root][INFO] - Training Epoch: 1/10, step 558/574 completed (loss: 0.32688361406326294, acc: 0.8799999952316284)
[2025-01-06 01:05:31,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:31,825][root][INFO] - Training Epoch: 1/10, step 559/574 completed (loss: 0.6946844458580017, acc: 0.8461538553237915)
[2025-01-06 01:05:31,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:32,183][root][INFO] - Training Epoch: 1/10, step 560/574 completed (loss: 0.22296521067619324, acc: 0.9230769276618958)
[2025-01-06 01:05:32,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:32,534][root][INFO] - Training Epoch: 1/10, step 561/574 completed (loss: 0.2832656800746918, acc: 0.9487179517745972)
[2025-01-06 01:05:32,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:32,886][root][INFO] - Training Epoch: 1/10, step 562/574 completed (loss: 0.791174054145813, acc: 0.8333333134651184)
[2025-01-06 01:05:33,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:33,270][root][INFO] - Training Epoch: 1/10, step 563/574 completed (loss: 0.497627854347229, acc: 0.8831169009208679)
[2025-01-06 01:05:33,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:33,586][root][INFO] - Training Epoch: 1/10, step 564/574 completed (loss: 0.6823926568031311, acc: 0.8125)
[2025-01-06 01:05:33,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:33,955][root][INFO] - Training Epoch: 1/10, step 565/574 completed (loss: 0.42319729924201965, acc: 0.8448275923728943)
[2025-01-06 01:05:34,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:34,329][root][INFO] - Training Epoch: 1/10, step 566/574 completed (loss: 0.6059696078300476, acc: 0.8809523582458496)
[2025-01-06 01:05:34,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:34,636][root][INFO] - Training Epoch: 1/10, step 567/574 completed (loss: 0.08468832075595856, acc: 0.9736841917037964)
[2025-01-06 01:05:34,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:35,035][root][INFO] - Training Epoch: 1/10, step 568/574 completed (loss: 0.23136001825332642, acc: 0.8888888955116272)
[2025-01-06 01:05:35,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:35,443][root][INFO] - Training Epoch: 1/10, step 569/574 completed (loss: 0.4034912884235382, acc: 0.8823529481887817)
[2025-01-06 01:05:35,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:35,867][root][INFO] - Training Epoch: 1/10, step 570/574 completed (loss: 0.07803592085838318, acc: 0.9677419066429138)
[2025-01-06 01:05:36,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:36,289][root][INFO] - Training Epoch: 1/10, step 571/574 completed (loss: 0.6019605994224548, acc: 0.8547008633613586)
[2025-01-06 01:05:36,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:37,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:37,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:37,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:38,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:38,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:38,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:39,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:39,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:40,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:40,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:40,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:41,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:41,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:41,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:42,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:42,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:42,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:43,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:43,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:43,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:45,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:45,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:45,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:46,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:46,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:46,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:46,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:47,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:47,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:48,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:48,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:48,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:49,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:49,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:49,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:50,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:50,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:50,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:51,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:51,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:51,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:52,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:52,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:52,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:53,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:53,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:54,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:54,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:54,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:55,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:55,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:55,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:56,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:56,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:56,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:57,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:57,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:57,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:58,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:58,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:59,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:59,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:59,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:00,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:00,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:00,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:01,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:01,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:01,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:02,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:02,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:02,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:03,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:03,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:04,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:04,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:04,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:05,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:05,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:05,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:06,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:06,769][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9956, device='cuda:0') eval_epoch_loss=tensor(0.6909, device='cuda:0') eval_epoch_acc=tensor(0.8103, device='cuda:0')
[2025-01-06 01:06:06,770][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:06:06,771][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:06:07,029][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_572_loss_0.6909285187721252/model.pt
[2025-01-06 01:06:07,032][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:06:07,032][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.6909285187721252
[2025-01-06 01:06:07,033][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8103269934654236
[2025-01-06 01:06:07,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:07,364][root][INFO] - Training Epoch: 1/10, step 572/574 completed (loss: 0.5614253282546997, acc: 0.8367347121238708)
[2025-01-06 01:06:07,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:07,723][root][INFO] - Training Epoch: 1/10, step 573/574 completed (loss: 0.5300466418266296, acc: 0.8427672982215881)
[2025-01-06 01:06:08,228][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=3.8630, train_epoch_loss=1.3514, epoch time 374.95060996711254s
[2025-01-06 01:06:08,228][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-06 01:06:08,228][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:06:08,229][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-06 01:06:08,229][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 3
[2025-01-06 01:06:08,229][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:06:08,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:09,040][root][INFO] - Training Epoch: 2/10, step 0/574 completed (loss: 0.9133339524269104, acc: 0.7037037014961243)
[2025-01-06 01:06:09,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:09,418][root][INFO] - Training Epoch: 2/10, step 1/574 completed (loss: 0.6777719855308533, acc: 0.800000011920929)
[2025-01-06 01:06:09,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:09,797][root][INFO] - Training Epoch: 2/10, step 2/574 completed (loss: 0.9455122947692871, acc: 0.7297297120094299)
[2025-01-06 01:06:09,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:10,197][root][INFO] - Training Epoch: 2/10, step 3/574 completed (loss: 0.8657743334770203, acc: 0.7894737124443054)
[2025-01-06 01:06:10,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:10,580][root][INFO] - Training Epoch: 2/10, step 4/574 completed (loss: 1.0268062353134155, acc: 0.7837837934494019)
[2025-01-06 01:06:10,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:10,923][root][INFO] - Training Epoch: 2/10, step 5/574 completed (loss: 0.9393935799598694, acc: 0.75)
[2025-01-06 01:06:11,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:11,239][root][INFO] - Training Epoch: 2/10, step 6/574 completed (loss: 1.426051139831543, acc: 0.5714285969734192)
[2025-01-06 01:06:11,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:11,574][root][INFO] - Training Epoch: 2/10, step 7/574 completed (loss: 0.9144074320793152, acc: 0.800000011920929)
[2025-01-06 01:06:11,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:11,987][root][INFO] - Training Epoch: 2/10, step 8/574 completed (loss: 0.32294389605522156, acc: 0.8636363744735718)
[2025-01-06 01:06:12,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:12,342][root][INFO] - Training Epoch: 2/10, step 9/574 completed (loss: 0.1564411073923111, acc: 0.9615384340286255)
[2025-01-06 01:06:12,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:12,736][root][INFO] - Training Epoch: 2/10, step 10/574 completed (loss: 0.28796252608299255, acc: 0.9259259104728699)
[2025-01-06 01:06:12,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:13,082][root][INFO] - Training Epoch: 2/10, step 11/574 completed (loss: 0.6557880640029907, acc: 0.8461538553237915)
[2025-01-06 01:06:13,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:13,452][root][INFO] - Training Epoch: 2/10, step 12/574 completed (loss: 0.20382533967494965, acc: 0.939393937587738)
[2025-01-06 01:06:13,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:13,862][root][INFO] - Training Epoch: 2/10, step 13/574 completed (loss: 0.4493919909000397, acc: 0.804347813129425)
[2025-01-06 01:06:13,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:14,208][root][INFO] - Training Epoch: 2/10, step 14/574 completed (loss: 0.4013718366622925, acc: 0.8823529481887817)
[2025-01-06 01:06:14,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:14,603][root][INFO] - Training Epoch: 2/10, step 15/574 completed (loss: 0.6352143883705139, acc: 0.8571428656578064)
[2025-01-06 01:06:14,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:14,965][root][INFO] - Training Epoch: 2/10, step 16/574 completed (loss: 0.3341027498245239, acc: 0.8947368264198303)
[2025-01-06 01:06:15,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:15,314][root][INFO] - Training Epoch: 2/10, step 17/574 completed (loss: 0.884286642074585, acc: 0.75)
[2025-01-06 01:06:15,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:15,641][root][INFO] - Training Epoch: 2/10, step 18/574 completed (loss: 1.2267910242080688, acc: 0.6944444179534912)
[2025-01-06 01:06:15,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:16,010][root][INFO] - Training Epoch: 2/10, step 19/574 completed (loss: 0.7219353318214417, acc: 0.8421052694320679)
[2025-01-06 01:06:16,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:16,354][root][INFO] - Training Epoch: 2/10, step 20/574 completed (loss: 0.5476475358009338, acc: 0.8461538553237915)
[2025-01-06 01:06:16,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:16,740][root][INFO] - Training Epoch: 2/10, step 21/574 completed (loss: 0.8605398535728455, acc: 0.7931034564971924)
[2025-01-06 01:06:16,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:17,090][root][INFO] - Training Epoch: 2/10, step 22/574 completed (loss: 1.3240591287612915, acc: 0.5199999809265137)
[2025-01-06 01:06:17,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:17,430][root][INFO] - Training Epoch: 2/10, step 23/574 completed (loss: 1.0482147932052612, acc: 0.761904776096344)
[2025-01-06 01:06:17,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:17,832][root][INFO] - Training Epoch: 2/10, step 24/574 completed (loss: 0.243309885263443, acc: 0.9375)
[2025-01-06 01:06:17,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:18,188][root][INFO] - Training Epoch: 2/10, step 25/574 completed (loss: 1.0673270225524902, acc: 0.7547169923782349)
[2025-01-06 01:06:18,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:18,514][root][INFO] - Training Epoch: 2/10, step 26/574 completed (loss: 1.1473195552825928, acc: 0.6712328791618347)
[2025-01-06 01:06:18,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:19,685][root][INFO] - Training Epoch: 2/10, step 27/574 completed (loss: 1.2405128479003906, acc: 0.6640316247940063)
[2025-01-06 01:06:19,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20,012][root][INFO] - Training Epoch: 2/10, step 28/574 completed (loss: 0.7247819900512695, acc: 0.7209302186965942)
[2025-01-06 01:06:20,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20,361][root][INFO] - Training Epoch: 2/10, step 29/574 completed (loss: 0.8935021758079529, acc: 0.6867470145225525)
[2025-01-06 01:06:20,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20,681][root][INFO] - Training Epoch: 2/10, step 30/574 completed (loss: 1.1130157709121704, acc: 0.7037037014961243)
[2025-01-06 01:06:20,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20,970][root][INFO] - Training Epoch: 2/10, step 31/574 completed (loss: 1.200917363166809, acc: 0.6785714030265808)
[2025-01-06 01:06:21,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:21,237][root][INFO] - Training Epoch: 2/10, step 32/574 completed (loss: 0.7356235384941101, acc: 0.7777777910232544)
[2025-01-06 01:06:21,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:21,624][root][INFO] - Training Epoch: 2/10, step 33/574 completed (loss: 0.22570620477199554, acc: 0.95652174949646)
[2025-01-06 01:06:21,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:22,004][root][INFO] - Training Epoch: 2/10, step 34/574 completed (loss: 0.6020208597183228, acc: 0.7983193397521973)
[2025-01-06 01:06:22,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:22,355][root][INFO] - Training Epoch: 2/10, step 35/574 completed (loss: 0.5814775824546814, acc: 0.8524590134620667)
[2025-01-06 01:06:22,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:22,764][root][INFO] - Training Epoch: 2/10, step 36/574 completed (loss: 0.8273266553878784, acc: 0.8253968358039856)
[2025-01-06 01:06:22,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:23,099][root][INFO] - Training Epoch: 2/10, step 37/574 completed (loss: 0.7772249579429626, acc: 0.8474576473236084)
[2025-01-06 01:06:23,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:23,474][root][INFO] - Training Epoch: 2/10, step 38/574 completed (loss: 0.5141057968139648, acc: 0.8850574493408203)
[2025-01-06 01:06:23,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:23,757][root][INFO] - Training Epoch: 2/10, step 39/574 completed (loss: 0.6401789784431458, acc: 0.8095238208770752)
[2025-01-06 01:06:23,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:24,102][root][INFO] - Training Epoch: 2/10, step 40/574 completed (loss: 0.8183995485305786, acc: 0.7692307829856873)
[2025-01-06 01:06:24,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:24,465][root][INFO] - Training Epoch: 2/10, step 41/574 completed (loss: 0.5470304489135742, acc: 0.837837815284729)
[2025-01-06 01:06:24,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:24,852][root][INFO] - Training Epoch: 2/10, step 42/574 completed (loss: 1.0828540325164795, acc: 0.692307710647583)
[2025-01-06 01:06:24,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:25,271][root][INFO] - Training Epoch: 2/10, step 43/574 completed (loss: 0.9191879034042358, acc: 0.7777777910232544)
[2025-01-06 01:06:25,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:25,692][root][INFO] - Training Epoch: 2/10, step 44/574 completed (loss: 0.6063882112503052, acc: 0.8350515365600586)
[2025-01-06 01:06:25,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:26,108][root][INFO] - Training Epoch: 2/10, step 45/574 completed (loss: 0.6248728036880493, acc: 0.8308823704719543)
[2025-01-06 01:06:26,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:26,449][root][INFO] - Training Epoch: 2/10, step 46/574 completed (loss: 0.7312148809432983, acc: 0.7692307829856873)
[2025-01-06 01:06:26,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:26,806][root][INFO] - Training Epoch: 2/10, step 47/574 completed (loss: 0.35456177592277527, acc: 0.9629629850387573)
[2025-01-06 01:06:26,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:27,153][root][INFO] - Training Epoch: 2/10, step 48/574 completed (loss: 0.8277263641357422, acc: 0.7857142686843872)
[2025-01-06 01:06:27,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:27,476][root][INFO] - Training Epoch: 2/10, step 49/574 completed (loss: 0.44759538769721985, acc: 0.8333333134651184)
[2025-01-06 01:06:27,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:27,812][root][INFO] - Training Epoch: 2/10, step 50/574 completed (loss: 1.0201468467712402, acc: 0.7543859481811523)
[2025-01-06 01:06:27,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:28,174][root][INFO] - Training Epoch: 2/10, step 51/574 completed (loss: 1.0099365711212158, acc: 0.6984127163887024)
[2025-01-06 01:06:28,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:28,542][root][INFO] - Training Epoch: 2/10, step 52/574 completed (loss: 1.2446990013122559, acc: 0.6901408433914185)
[2025-01-06 01:06:28,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:28,987][root][INFO] - Training Epoch: 2/10, step 53/574 completed (loss: 1.7022802829742432, acc: 0.5266666412353516)
[2025-01-06 01:06:29,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:29,349][root][INFO] - Training Epoch: 2/10, step 54/574 completed (loss: 1.6674350500106812, acc: 0.6216216087341309)
[2025-01-06 01:06:29,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:29,689][root][INFO] - Training Epoch: 2/10, step 55/574 completed (loss: 0.2721173167228699, acc: 0.8846153616905212)
[2025-01-06 01:06:31,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:32,736][root][INFO] - Training Epoch: 2/10, step 56/574 completed (loss: 1.4432233572006226, acc: 0.6109215021133423)
[2025-01-06 01:06:33,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:33,901][root][INFO] - Training Epoch: 2/10, step 57/574 completed (loss: 1.4148821830749512, acc: 0.6143791079521179)
[2025-01-06 01:06:34,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:34,518][root][INFO] - Training Epoch: 2/10, step 58/574 completed (loss: 1.1297752857208252, acc: 0.6761363744735718)
[2025-01-06 01:06:34,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:35,083][root][INFO] - Training Epoch: 2/10, step 59/574 completed (loss: 0.589104413986206, acc: 0.8308823704719543)
[2025-01-06 01:06:35,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:35,641][root][INFO] - Training Epoch: 2/10, step 60/574 completed (loss: 1.1552704572677612, acc: 0.695652186870575)
[2025-01-06 01:06:35,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:36,033][root][INFO] - Training Epoch: 2/10, step 61/574 completed (loss: 1.2747642993927002, acc: 0.625)
[2025-01-06 01:06:36,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:36,316][root][INFO] - Training Epoch: 2/10, step 62/574 completed (loss: 0.5923870205879211, acc: 0.8529411554336548)
[2025-01-06 01:06:36,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:36,673][root][INFO] - Training Epoch: 2/10, step 63/574 completed (loss: 0.586694598197937, acc: 0.8055555820465088)
[2025-01-06 01:06:36,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:37,022][root][INFO] - Training Epoch: 2/10, step 64/574 completed (loss: 0.37577974796295166, acc: 0.890625)
[2025-01-06 01:06:37,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:37,398][root][INFO] - Training Epoch: 2/10, step 65/574 completed (loss: 0.3316398859024048, acc: 0.8965517282485962)
[2025-01-06 01:06:37,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:37,746][root][INFO] - Training Epoch: 2/10, step 66/574 completed (loss: 1.2506530284881592, acc: 0.6785714030265808)
[2025-01-06 01:06:37,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38,085][root][INFO] - Training Epoch: 2/10, step 67/574 completed (loss: 0.5357372164726257, acc: 0.8500000238418579)
[2025-01-06 01:06:38,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38,403][root][INFO] - Training Epoch: 2/10, step 68/574 completed (loss: 0.12450381368398666, acc: 0.9599999785423279)
[2025-01-06 01:06:38,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38,668][root][INFO] - Training Epoch: 2/10, step 69/574 completed (loss: 1.1978963613510132, acc: 0.6388888955116272)
[2025-01-06 01:06:38,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38,926][root][INFO] - Training Epoch: 2/10, step 70/574 completed (loss: 1.3448141813278198, acc: 0.5757575631141663)
[2025-01-06 01:06:39,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:39,296][root][INFO] - Training Epoch: 2/10, step 71/574 completed (loss: 1.2154016494750977, acc: 0.6397058963775635)
[2025-01-06 01:06:39,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:39,681][root][INFO] - Training Epoch: 2/10, step 72/574 completed (loss: 0.9209513068199158, acc: 0.7539682388305664)
[2025-01-06 01:06:39,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:40,044][root][INFO] - Training Epoch: 2/10, step 73/574 completed (loss: 1.6394895315170288, acc: 0.5692307949066162)
[2025-01-06 01:06:40,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:40,413][root][INFO] - Training Epoch: 2/10, step 74/574 completed (loss: 1.41606867313385, acc: 0.6428571343421936)
[2025-01-06 01:06:40,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:40,797][root][INFO] - Training Epoch: 2/10, step 75/574 completed (loss: 1.4697654247283936, acc: 0.5970149040222168)
[2025-01-06 01:06:40,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:41,199][root][INFO] - Training Epoch: 2/10, step 76/574 completed (loss: 1.60781729221344, acc: 0.5729926824569702)
[2025-01-06 01:06:41,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:41,568][root][INFO] - Training Epoch: 2/10, step 77/574 completed (loss: 0.08179499208927155, acc: 1.0)
[2025-01-06 01:06:41,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:41,912][root][INFO] - Training Epoch: 2/10, step 78/574 completed (loss: 0.39916980266571045, acc: 0.9166666865348816)
[2025-01-06 01:06:42,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:42,311][root][INFO] - Training Epoch: 2/10, step 79/574 completed (loss: 0.2954017221927643, acc: 0.8787878751754761)
[2025-01-06 01:06:42,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:42,678][root][INFO] - Training Epoch: 2/10, step 80/574 completed (loss: 0.5731922388076782, acc: 0.8461538553237915)
[2025-01-06 01:06:42,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:43,051][root][INFO] - Training Epoch: 2/10, step 81/574 completed (loss: 0.9046562910079956, acc: 0.7692307829856873)
[2025-01-06 01:06:43,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:43,417][root][INFO] - Training Epoch: 2/10, step 82/574 completed (loss: 0.991618812084198, acc: 0.7692307829856873)
[2025-01-06 01:06:43,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:43,808][root][INFO] - Training Epoch: 2/10, step 83/574 completed (loss: 0.46963316202163696, acc: 0.875)
[2025-01-06 01:06:43,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:44,149][root][INFO] - Training Epoch: 2/10, step 84/574 completed (loss: 0.651268720626831, acc: 0.8260869383811951)
[2025-01-06 01:06:44,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:44,521][root][INFO] - Training Epoch: 2/10, step 85/574 completed (loss: 0.9607124328613281, acc: 0.7200000286102295)
[2025-01-06 01:06:44,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:44,846][root][INFO] - Training Epoch: 2/10, step 86/574 completed (loss: 0.5214285850524902, acc: 0.8695651888847351)
[2025-01-06 01:06:45,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:45,326][root][INFO] - Training Epoch: 2/10, step 87/574 completed (loss: 1.2623591423034668, acc: 0.6000000238418579)
[2025-01-06 01:06:45,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:45,689][root][INFO] - Training Epoch: 2/10, step 88/574 completed (loss: 1.2196738719940186, acc: 0.6796116232872009)
[2025-01-06 01:06:45,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:46,733][root][INFO] - Training Epoch: 2/10, step 89/574 completed (loss: 1.0991523265838623, acc: 0.708737850189209)
[2025-01-06 01:06:46,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:47,549][root][INFO] - Training Epoch: 2/10, step 90/574 completed (loss: 1.4791160821914673, acc: 0.6129032373428345)
[2025-01-06 01:06:47,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:48,347][root][INFO] - Training Epoch: 2/10, step 91/574 completed (loss: 1.2229055166244507, acc: 0.6594827771186829)
[2025-01-06 01:06:48,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:49,084][root][INFO] - Training Epoch: 2/10, step 92/574 completed (loss: 0.9012254476547241, acc: 0.7263157963752747)
[2025-01-06 01:06:49,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:50,065][root][INFO] - Training Epoch: 2/10, step 93/574 completed (loss: 1.8095333576202393, acc: 0.4752475321292877)
[2025-01-06 01:06:50,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:50,367][root][INFO] - Training Epoch: 2/10, step 94/574 completed (loss: 1.4738682508468628, acc: 0.5483871102333069)
[2025-01-06 01:06:50,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:50,705][root][INFO] - Training Epoch: 2/10, step 95/574 completed (loss: 1.225109338760376, acc: 0.6376811861991882)
[2025-01-06 01:06:50,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:51,097][root][INFO] - Training Epoch: 2/10, step 96/574 completed (loss: 1.4899866580963135, acc: 0.5546218752861023)
[2025-01-06 01:06:51,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:51,449][root][INFO] - Training Epoch: 2/10, step 97/574 completed (loss: 1.703063726425171, acc: 0.5384615659713745)
[2025-01-06 01:06:51,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:51,838][root][INFO] - Training Epoch: 2/10, step 98/574 completed (loss: 1.6566827297210693, acc: 0.525547444820404)
[2025-01-06 01:06:51,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:52,200][root][INFO] - Training Epoch: 2/10, step 99/574 completed (loss: 1.789255976676941, acc: 0.5223880410194397)
[2025-01-06 01:06:52,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:52,593][root][INFO] - Training Epoch: 2/10, step 100/574 completed (loss: 0.616720974445343, acc: 0.8500000238418579)
[2025-01-06 01:06:52,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:52,978][root][INFO] - Training Epoch: 2/10, step 101/574 completed (loss: 0.11157049983739853, acc: 1.0)
[2025-01-06 01:06:53,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:53,397][root][INFO] - Training Epoch: 2/10, step 102/574 completed (loss: 0.15495948493480682, acc: 1.0)
[2025-01-06 01:06:53,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:53,764][root][INFO] - Training Epoch: 2/10, step 103/574 completed (loss: 0.2740823030471802, acc: 0.8863636255264282)
[2025-01-06 01:06:53,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:54,142][root][INFO] - Training Epoch: 2/10, step 104/574 completed (loss: 0.7611956000328064, acc: 0.7931034564971924)
[2025-01-06 01:06:54,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:54,493][root][INFO] - Training Epoch: 2/10, step 105/574 completed (loss: 0.4637688100337982, acc: 0.8604651093482971)
[2025-01-06 01:06:54,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:54,858][root][INFO] - Training Epoch: 2/10, step 106/574 completed (loss: 0.4685255289077759, acc: 0.8799999952316284)
[2025-01-06 01:06:54,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:55,243][root][INFO] - Training Epoch: 2/10, step 107/574 completed (loss: 0.07409940659999847, acc: 1.0)
[2025-01-06 01:06:55,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:55,662][root][INFO] - Training Epoch: 2/10, step 108/574 completed (loss: 0.14384879171848297, acc: 0.9615384340286255)
[2025-01-06 01:06:55,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:56,049][root][INFO] - Training Epoch: 2/10, step 109/574 completed (loss: 0.18903996050357819, acc: 0.9047619104385376)
[2025-01-06 01:06:56,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:56,446][root][INFO] - Training Epoch: 2/10, step 110/574 completed (loss: 0.20350012183189392, acc: 0.9538461565971375)
[2025-01-06 01:06:56,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:56,872][root][INFO] - Training Epoch: 2/10, step 111/574 completed (loss: 0.7454786896705627, acc: 0.7894737124443054)
[2025-01-06 01:06:57,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:57,267][root][INFO] - Training Epoch: 2/10, step 112/574 completed (loss: 1.2991013526916504, acc: 0.6491228342056274)
[2025-01-06 01:06:57,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:57,653][root][INFO] - Training Epoch: 2/10, step 113/574 completed (loss: 0.8160881996154785, acc: 0.7948718070983887)
[2025-01-06 01:06:57,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:58,025][root][INFO] - Training Epoch: 2/10, step 114/574 completed (loss: 0.5404468774795532, acc: 0.8367347121238708)
[2025-01-06 01:06:58,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:58,407][root][INFO] - Training Epoch: 2/10, step 115/574 completed (loss: 0.22033271193504333, acc: 0.9545454382896423)
[2025-01-06 01:06:58,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:58,821][root][INFO] - Training Epoch: 2/10, step 116/574 completed (loss: 0.6746638417243958, acc: 0.8095238208770752)
[2025-01-06 01:06:58,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:59,189][root][INFO] - Training Epoch: 2/10, step 117/574 completed (loss: 0.6661014556884766, acc: 0.8292682766914368)
[2025-01-06 01:06:59,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:59,515][root][INFO] - Training Epoch: 2/10, step 118/574 completed (loss: 0.4066845178604126, acc: 0.9032257795333862)
[2025-01-06 01:06:59,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:00,362][root][INFO] - Training Epoch: 2/10, step 119/574 completed (loss: 0.9176133275032043, acc: 0.7642585635185242)
[2025-01-06 01:07:00,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:00,746][root][INFO] - Training Epoch: 2/10, step 120/574 completed (loss: 0.5087900161743164, acc: 0.8399999737739563)
[2025-01-06 01:07:00,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:01,170][root][INFO] - Training Epoch: 2/10, step 121/574 completed (loss: 0.6349326372146606, acc: 0.8653846383094788)
[2025-01-06 01:07:01,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:01,543][root][INFO] - Training Epoch: 2/10, step 122/574 completed (loss: 0.43168124556541443, acc: 0.7916666865348816)
[2025-01-06 01:07:01,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:01,892][root][INFO] - Training Epoch: 2/10, step 123/574 completed (loss: 0.43594273924827576, acc: 0.8947368264198303)
[2025-01-06 01:07:01,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:02,287][root][INFO] - Training Epoch: 2/10, step 124/574 completed (loss: 1.314558506011963, acc: 0.6380367875099182)
[2025-01-06 01:07:02,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:02,666][root][INFO] - Training Epoch: 2/10, step 125/574 completed (loss: 1.3862396478652954, acc: 0.6319444179534912)
[2025-01-06 01:07:02,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:02,978][root][INFO] - Training Epoch: 2/10, step 126/574 completed (loss: 1.4471691846847534, acc: 0.6083333492279053)
[2025-01-06 01:07:03,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:03,348][root][INFO] - Training Epoch: 2/10, step 127/574 completed (loss: 1.0160081386566162, acc: 0.6964285969734192)
[2025-01-06 01:07:03,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:03,711][root][INFO] - Training Epoch: 2/10, step 128/574 completed (loss: 1.0685341358184814, acc: 0.7128205299377441)
[2025-01-06 01:07:03,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:04,110][root][INFO] - Training Epoch: 2/10, step 129/574 completed (loss: 1.1767491102218628, acc: 0.6617646813392639)
[2025-01-06 01:07:04,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:04,413][root][INFO] - Training Epoch: 2/10, step 130/574 completed (loss: 1.1313488483428955, acc: 0.692307710647583)
[2025-01-06 01:07:04,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:04,748][root][INFO] - Training Epoch: 2/10, step 131/574 completed (loss: 0.8780121803283691, acc: 0.6086956262588501)
[2025-01-06 01:07:04,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:05,079][root][INFO] - Training Epoch: 2/10, step 132/574 completed (loss: 1.3835855722427368, acc: 0.59375)
[2025-01-06 01:07:05,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:05,408][root][INFO] - Training Epoch: 2/10, step 133/574 completed (loss: 1.6373695135116577, acc: 0.5652173757553101)
[2025-01-06 01:07:05,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:05,750][root][INFO] - Training Epoch: 2/10, step 134/574 completed (loss: 1.027084469795227, acc: 0.6571428775787354)
[2025-01-06 01:07:05,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:06,064][root][INFO] - Training Epoch: 2/10, step 135/574 completed (loss: 1.1612765789031982, acc: 0.7307692170143127)
[2025-01-06 01:07:06,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:06,373][root][INFO] - Training Epoch: 2/10, step 136/574 completed (loss: 0.9953247308731079, acc: 0.7142857313156128)
[2025-01-06 01:07:06,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:06,716][root][INFO] - Training Epoch: 2/10, step 137/574 completed (loss: 1.426124930381775, acc: 0.5)
[2025-01-06 01:07:06,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:07,030][root][INFO] - Training Epoch: 2/10, step 138/574 completed (loss: 1.1886519193649292, acc: 0.695652186870575)
[2025-01-06 01:07:07,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:07,332][root][INFO] - Training Epoch: 2/10, step 139/574 completed (loss: 0.4343222975730896, acc: 0.8571428656578064)
[2025-01-06 01:07:07,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:07,707][root][INFO] - Training Epoch: 2/10, step 140/574 completed (loss: 0.5515655279159546, acc: 0.7692307829856873)
[2025-01-06 01:07:08,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:08,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:09,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:09,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:09,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:10,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:10,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:10,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:11,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:11,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:12,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:12,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:12,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:13,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:13,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:14,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:14,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:14,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:15,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:15,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:15,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:17,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:17,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:17,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:18,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:18,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:18,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:19,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:19,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:21,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:21,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:21,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:23,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:23,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:24,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:24,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:24,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:26,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:26,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:26,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:27,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:27,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:27,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:28,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:28,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:28,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:29,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:29,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:29,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:30,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:30,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:30,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:31,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:31,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:31,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:32,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:32,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:33,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:33,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:33,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:34,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:34,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:35,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:35,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:35,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:35,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:36,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:36,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:38,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:38,780][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0212, device='cuda:0') eval_epoch_loss=tensor(0.7037, device='cuda:0') eval_epoch_acc=tensor(0.8032, device='cuda:0')
[2025-01-06 01:07:38,781][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:07:38,781][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:07:39,045][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_141_loss_0.7036666870117188/model.pt
[2025-01-06 01:07:39,050][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:07:39,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:39,430][root][INFO] - Training Epoch: 2/10, step 141/574 completed (loss: 1.1107159852981567, acc: 0.7096773982048035)
[2025-01-06 01:07:39,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:39,767][root][INFO] - Training Epoch: 2/10, step 142/574 completed (loss: 1.338393211364746, acc: 0.6486486196517944)
[2025-01-06 01:07:39,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:40,295][root][INFO] - Training Epoch: 2/10, step 143/574 completed (loss: 1.1484960317611694, acc: 0.6666666865348816)
[2025-01-06 01:07:40,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:40,694][root][INFO] - Training Epoch: 2/10, step 144/574 completed (loss: 0.9128457903862, acc: 0.7835820913314819)
[2025-01-06 01:07:40,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:41,100][root][INFO] - Training Epoch: 2/10, step 145/574 completed (loss: 0.9253627061843872, acc: 0.6938775777816772)
[2025-01-06 01:07:41,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:41,563][root][INFO] - Training Epoch: 2/10, step 146/574 completed (loss: 1.3438760042190552, acc: 0.585106372833252)
[2025-01-06 01:07:41,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:41,875][root][INFO] - Training Epoch: 2/10, step 147/574 completed (loss: 1.138297200202942, acc: 0.6714285612106323)
[2025-01-06 01:07:41,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:42,237][root][INFO] - Training Epoch: 2/10, step 148/574 completed (loss: 1.2592055797576904, acc: 0.6428571343421936)
[2025-01-06 01:07:42,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:42,563][root][INFO] - Training Epoch: 2/10, step 149/574 completed (loss: 1.0741157531738281, acc: 0.695652186870575)
[2025-01-06 01:07:42,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:42,933][root][INFO] - Training Epoch: 2/10, step 150/574 completed (loss: 0.9285979270935059, acc: 0.7241379022598267)
[2025-01-06 01:07:43,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:43,298][root][INFO] - Training Epoch: 2/10, step 151/574 completed (loss: 1.4082645177841187, acc: 0.6521739363670349)
[2025-01-06 01:07:43,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:43,657][root][INFO] - Training Epoch: 2/10, step 152/574 completed (loss: 0.8926615715026855, acc: 0.7627118825912476)
[2025-01-06 01:07:43,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:43,973][root][INFO] - Training Epoch: 2/10, step 153/574 completed (loss: 1.1508110761642456, acc: 0.6491228342056274)
[2025-01-06 01:07:44,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:44,352][root][INFO] - Training Epoch: 2/10, step 154/574 completed (loss: 0.9452139139175415, acc: 0.7702702879905701)
[2025-01-06 01:07:44,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:44,724][root][INFO] - Training Epoch: 2/10, step 155/574 completed (loss: 0.4679718017578125, acc: 0.8214285969734192)
[2025-01-06 01:07:44,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:45,093][root][INFO] - Training Epoch: 2/10, step 156/574 completed (loss: 0.6239530444145203, acc: 0.8260869383811951)
[2025-01-06 01:07:45,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:45,436][root][INFO] - Training Epoch: 2/10, step 157/574 completed (loss: 2.2824904918670654, acc: 0.3684210479259491)
[2025-01-06 01:07:46,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:47,077][root][INFO] - Training Epoch: 2/10, step 158/574 completed (loss: 1.391862154006958, acc: 0.5675675868988037)
[2025-01-06 01:07:47,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:47,390][root][INFO] - Training Epoch: 2/10, step 159/574 completed (loss: 1.5636720657348633, acc: 0.5)
[2025-01-06 01:07:47,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:47,785][root][INFO] - Training Epoch: 2/10, step 160/574 completed (loss: 1.559796690940857, acc: 0.569767415523529)
[2025-01-06 01:07:47,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:48,374][root][INFO] - Training Epoch: 2/10, step 161/574 completed (loss: 1.8415956497192383, acc: 0.4588235318660736)
[2025-01-06 01:07:48,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:48,926][root][INFO] - Training Epoch: 2/10, step 162/574 completed (loss: 1.8640868663787842, acc: 0.550561785697937)
[2025-01-06 01:07:49,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:49,287][root][INFO] - Training Epoch: 2/10, step 163/574 completed (loss: 0.6927237510681152, acc: 0.8863636255264282)
[2025-01-06 01:07:49,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:49,646][root][INFO] - Training Epoch: 2/10, step 164/574 completed (loss: 0.6448192000389099, acc: 0.9047619104385376)
[2025-01-06 01:07:49,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:50,056][root][INFO] - Training Epoch: 2/10, step 165/574 completed (loss: 1.2257397174835205, acc: 0.6551724076271057)
[2025-01-06 01:07:50,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:50,432][root][INFO] - Training Epoch: 2/10, step 166/574 completed (loss: 0.2816638648509979, acc: 0.8775510191917419)
[2025-01-06 01:07:50,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:50,760][root][INFO] - Training Epoch: 2/10, step 167/574 completed (loss: 0.3582890033721924, acc: 0.8799999952316284)
[2025-01-06 01:07:50,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:51,176][root][INFO] - Training Epoch: 2/10, step 168/574 completed (loss: 0.7416444420814514, acc: 0.8055555820465088)
[2025-01-06 01:07:51,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:51,526][root][INFO] - Training Epoch: 2/10, step 169/574 completed (loss: 1.150766134262085, acc: 0.7352941036224365)
[2025-01-06 01:07:51,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:52,546][root][INFO] - Training Epoch: 2/10, step 170/574 completed (loss: 1.1843867301940918, acc: 0.6780821681022644)
[2025-01-06 01:07:52,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:52,897][root][INFO] - Training Epoch: 2/10, step 171/574 completed (loss: 0.20448057353496552, acc: 0.9583333134651184)
[2025-01-06 01:07:52,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:53,278][root][INFO] - Training Epoch: 2/10, step 172/574 completed (loss: 0.6186341643333435, acc: 0.7777777910232544)
[2025-01-06 01:07:53,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:53,645][root][INFO] - Training Epoch: 2/10, step 173/574 completed (loss: 1.1657450199127197, acc: 0.6785714030265808)
[2025-01-06 01:07:53,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:54,189][root][INFO] - Training Epoch: 2/10, step 174/574 completed (loss: 1.19468355178833, acc: 0.6991150379180908)
[2025-01-06 01:07:54,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:54,501][root][INFO] - Training Epoch: 2/10, step 175/574 completed (loss: 0.9587777256965637, acc: 0.7681159377098083)
[2025-01-06 01:07:54,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:54,902][root][INFO] - Training Epoch: 2/10, step 176/574 completed (loss: 0.7448518872261047, acc: 0.7840909361839294)
[2025-01-06 01:07:55,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:55,807][root][INFO] - Training Epoch: 2/10, step 177/574 completed (loss: 1.4390206336975098, acc: 0.580152690410614)
[2025-01-06 01:07:55,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:56,473][root][INFO] - Training Epoch: 2/10, step 178/574 completed (loss: 1.351707100868225, acc: 0.614814817905426)
[2025-01-06 01:07:56,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:56,792][root][INFO] - Training Epoch: 2/10, step 179/574 completed (loss: 0.7173662185668945, acc: 0.8196721076965332)
[2025-01-06 01:07:56,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:57,161][root][INFO] - Training Epoch: 2/10, step 180/574 completed (loss: 0.09252341836690903, acc: 0.9583333134651184)
[2025-01-06 01:07:57,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:57,509][root][INFO] - Training Epoch: 2/10, step 181/574 completed (loss: 0.15208859741687775, acc: 0.8799999952316284)
[2025-01-06 01:07:57,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:57,821][root][INFO] - Training Epoch: 2/10, step 182/574 completed (loss: 0.3389569818973541, acc: 0.8928571343421936)
[2025-01-06 01:07:57,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:58,220][root][INFO] - Training Epoch: 2/10, step 183/574 completed (loss: 0.44498756527900696, acc: 0.8780487775802612)
[2025-01-06 01:07:58,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:58,586][root][INFO] - Training Epoch: 2/10, step 184/574 completed (loss: 0.5518134832382202, acc: 0.861027181148529)
[2025-01-06 01:07:58,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:58,965][root][INFO] - Training Epoch: 2/10, step 185/574 completed (loss: 0.6129282116889954, acc: 0.8472622632980347)
[2025-01-06 01:07:59,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:59,461][root][INFO] - Training Epoch: 2/10, step 186/574 completed (loss: 0.5316880345344543, acc: 0.8031250238418579)
[2025-01-06 01:07:59,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:59,988][root][INFO] - Training Epoch: 2/10, step 187/574 completed (loss: 0.5804099440574646, acc: 0.8405253291130066)
[2025-01-06 01:08:00,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:00,419][root][INFO] - Training Epoch: 2/10, step 188/574 completed (loss: 0.6789749264717102, acc: 0.7971529960632324)
[2025-01-06 01:08:00,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:00,731][root][INFO] - Training Epoch: 2/10, step 189/574 completed (loss: 0.6927076578140259, acc: 0.7599999904632568)
[2025-01-06 01:08:00,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:01,277][root][INFO] - Training Epoch: 2/10, step 190/574 completed (loss: 1.290390968322754, acc: 0.6162790656089783)
[2025-01-06 01:08:01,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:02,070][root][INFO] - Training Epoch: 2/10, step 191/574 completed (loss: 1.8432713747024536, acc: 0.523809552192688)
[2025-01-06 01:08:02,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:02,980][root][INFO] - Training Epoch: 2/10, step 192/574 completed (loss: 1.471564531326294, acc: 0.5757575631141663)
[2025-01-06 01:08:03,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:03,724][root][INFO] - Training Epoch: 2/10, step 193/574 completed (loss: 1.1743431091308594, acc: 0.6470588445663452)
[2025-01-06 01:08:04,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:04,804][root][INFO] - Training Epoch: 2/10, step 194/574 completed (loss: 1.2994639873504639, acc: 0.6111111044883728)
[2025-01-06 01:08:05,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:05,757][root][INFO] - Training Epoch: 2/10, step 195/574 completed (loss: 0.8253999948501587, acc: 0.725806474685669)
[2025-01-06 01:08:05,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:06,106][root][INFO] - Training Epoch: 2/10, step 196/574 completed (loss: 0.38803812861442566, acc: 0.8928571343421936)
[2025-01-06 01:08:06,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:06,427][root][INFO] - Training Epoch: 2/10, step 197/574 completed (loss: 1.2606747150421143, acc: 0.625)
[2025-01-06 01:08:06,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:06,765][root][INFO] - Training Epoch: 2/10, step 198/574 completed (loss: 1.1138734817504883, acc: 0.6911764740943909)
[2025-01-06 01:08:06,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:07,163][root][INFO] - Training Epoch: 2/10, step 199/574 completed (loss: 1.2111351490020752, acc: 0.6985294222831726)
[2025-01-06 01:08:07,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:07,502][root][INFO] - Training Epoch: 2/10, step 200/574 completed (loss: 1.0008152723312378, acc: 0.6864407062530518)
[2025-01-06 01:08:07,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:07,830][root][INFO] - Training Epoch: 2/10, step 201/574 completed (loss: 1.1293528079986572, acc: 0.7164179086685181)
[2025-01-06 01:08:07,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:08,240][root][INFO] - Training Epoch: 2/10, step 202/574 completed (loss: 1.116487741470337, acc: 0.7281553149223328)
[2025-01-06 01:08:08,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:08,590][root][INFO] - Training Epoch: 2/10, step 203/574 completed (loss: 0.9825674295425415, acc: 0.6984127163887024)
[2025-01-06 01:08:08,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:08,936][root][INFO] - Training Epoch: 2/10, step 204/574 completed (loss: 0.20517607033252716, acc: 0.9560439586639404)
[2025-01-06 01:08:09,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:09,322][root][INFO] - Training Epoch: 2/10, step 205/574 completed (loss: 0.44004425406455994, acc: 0.878923773765564)
[2025-01-06 01:08:09,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:09,752][root][INFO] - Training Epoch: 2/10, step 206/574 completed (loss: 0.5178534388542175, acc: 0.8503937125205994)
[2025-01-06 01:08:09,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:10,172][root][INFO] - Training Epoch: 2/10, step 207/574 completed (loss: 0.5225468873977661, acc: 0.8663793206214905)
[2025-01-06 01:08:10,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:10,572][root][INFO] - Training Epoch: 2/10, step 208/574 completed (loss: 0.5759710073471069, acc: 0.8731883764266968)
[2025-01-06 01:08:10,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:10,954][root][INFO] - Training Epoch: 2/10, step 209/574 completed (loss: 0.5134397149085999, acc: 0.8677042722702026)
[2025-01-06 01:08:11,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:11,288][root][INFO] - Training Epoch: 2/10, step 210/574 completed (loss: 0.44975611567497253, acc: 0.9130434989929199)
[2025-01-06 01:08:11,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:11,626][root][INFO] - Training Epoch: 2/10, step 211/574 completed (loss: 0.5391284227371216, acc: 0.8260869383811951)
[2025-01-06 01:08:11,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:11,946][root][INFO] - Training Epoch: 2/10, step 212/574 completed (loss: 0.24943843483924866, acc: 0.9285714030265808)
[2025-01-06 01:08:12,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:12,269][root][INFO] - Training Epoch: 2/10, step 213/574 completed (loss: 0.3177429437637329, acc: 0.914893627166748)
[2025-01-06 01:08:12,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:12,944][root][INFO] - Training Epoch: 2/10, step 214/574 completed (loss: 0.3348345160484314, acc: 0.9153845906257629)
[2025-01-06 01:08:13,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:13,314][root][INFO] - Training Epoch: 2/10, step 215/574 completed (loss: 0.36478880047798157, acc: 0.8918918967247009)
[2025-01-06 01:08:13,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:13,669][root][INFO] - Training Epoch: 2/10, step 216/574 completed (loss: 0.3427331745624542, acc: 0.895348846912384)
[2025-01-06 01:08:13,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:14,197][root][INFO] - Training Epoch: 2/10, step 217/574 completed (loss: 0.4472092390060425, acc: 0.9009009003639221)
[2025-01-06 01:08:14,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:14,593][root][INFO] - Training Epoch: 2/10, step 218/574 completed (loss: 0.247029110789299, acc: 0.9333333373069763)
[2025-01-06 01:08:14,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:14,940][root][INFO] - Training Epoch: 2/10, step 219/574 completed (loss: 0.30232441425323486, acc: 0.9090909361839294)
[2025-01-06 01:08:15,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:15,299][root][INFO] - Training Epoch: 2/10, step 220/574 completed (loss: 0.2709232568740845, acc: 0.8518518805503845)
[2025-01-06 01:08:15,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:15,643][root][INFO] - Training Epoch: 2/10, step 221/574 completed (loss: 0.3248930275440216, acc: 0.8399999737739563)
[2025-01-06 01:08:15,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:16,028][root][INFO] - Training Epoch: 2/10, step 222/574 completed (loss: 1.024468183517456, acc: 0.7115384340286255)
[2025-01-06 01:08:16,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:16,792][root][INFO] - Training Epoch: 2/10, step 223/574 completed (loss: 0.5085110068321228, acc: 0.8804348111152649)
[2025-01-06 01:08:16,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:17,331][root][INFO] - Training Epoch: 2/10, step 224/574 completed (loss: 0.7215670943260193, acc: 0.7613636255264282)
[2025-01-06 01:08:17,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:17,763][root][INFO] - Training Epoch: 2/10, step 225/574 completed (loss: 1.0328419208526611, acc: 0.7765957713127136)
[2025-01-06 01:08:17,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:18,125][root][INFO] - Training Epoch: 2/10, step 226/574 completed (loss: 0.8248197436332703, acc: 0.7547169923782349)
[2025-01-06 01:08:18,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:18,492][root][INFO] - Training Epoch: 2/10, step 227/574 completed (loss: 0.5166445374488831, acc: 0.800000011920929)
[2025-01-06 01:08:18,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:18,883][root][INFO] - Training Epoch: 2/10, step 228/574 completed (loss: 0.41050755977630615, acc: 0.8604651093482971)
[2025-01-06 01:08:18,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:19,252][root][INFO] - Training Epoch: 2/10, step 229/574 completed (loss: 1.7885030508041382, acc: 0.5)
[2025-01-06 01:08:19,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:19,615][root][INFO] - Training Epoch: 2/10, step 230/574 completed (loss: 2.156675100326538, acc: 0.4842105209827423)
[2025-01-06 01:08:19,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:19,978][root][INFO] - Training Epoch: 2/10, step 231/574 completed (loss: 1.648942232131958, acc: 0.5777778029441833)
[2025-01-06 01:08:20,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:20,409][root][INFO] - Training Epoch: 2/10, step 232/574 completed (loss: 1.679942011833191, acc: 0.5166666507720947)
[2025-01-06 01:08:20,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:20,894][root][INFO] - Training Epoch: 2/10, step 233/574 completed (loss: 1.9365042448043823, acc: 0.47706422209739685)
[2025-01-06 01:08:21,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:21,364][root][INFO] - Training Epoch: 2/10, step 234/574 completed (loss: 1.6456435918807983, acc: 0.5384615659713745)
[2025-01-06 01:08:21,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:21,646][root][INFO] - Training Epoch: 2/10, step 235/574 completed (loss: 0.5908181071281433, acc: 0.8421052694320679)
[2025-01-06 01:08:21,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:21,980][root][INFO] - Training Epoch: 2/10, step 236/574 completed (loss: 0.7237928509712219, acc: 0.75)
[2025-01-06 01:08:22,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:22,330][root][INFO] - Training Epoch: 2/10, step 237/574 completed (loss: 1.3903725147247314, acc: 0.6363636255264282)
[2025-01-06 01:08:22,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:22,720][root][INFO] - Training Epoch: 2/10, step 238/574 completed (loss: 0.8137289881706238, acc: 0.7407407164573669)
[2025-01-06 01:08:22,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:23,112][root][INFO] - Training Epoch: 2/10, step 239/574 completed (loss: 1.0234472751617432, acc: 0.7142857313156128)
[2025-01-06 01:08:23,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:23,526][root][INFO] - Training Epoch: 2/10, step 240/574 completed (loss: 1.2512983083724976, acc: 0.6818181872367859)
[2025-01-06 01:08:23,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:23,907][root][INFO] - Training Epoch: 2/10, step 241/574 completed (loss: 0.8742824196815491, acc: 0.7045454382896423)
[2025-01-06 01:08:24,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:24,481][root][INFO] - Training Epoch: 2/10, step 242/574 completed (loss: 1.4291614294052124, acc: 0.5645161271095276)
[2025-01-06 01:08:24,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:25,001][root][INFO] - Training Epoch: 2/10, step 243/574 completed (loss: 1.4617270231246948, acc: 0.5909090638160706)
[2025-01-06 01:08:25,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:25,346][root][INFO] - Training Epoch: 2/10, step 244/574 completed (loss: 0.1645388901233673, acc: 0.9523809552192688)
[2025-01-06 01:08:25,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:25,754][root][INFO] - Training Epoch: 2/10, step 245/574 completed (loss: 0.6848965883255005, acc: 0.7692307829856873)
[2025-01-06 01:08:25,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:26,106][root][INFO] - Training Epoch: 2/10, step 246/574 completed (loss: 0.47292816638946533, acc: 0.8709677457809448)
[2025-01-06 01:08:26,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:26,456][root][INFO] - Training Epoch: 2/10, step 247/574 completed (loss: 0.4774598479270935, acc: 0.800000011920929)
[2025-01-06 01:08:26,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:26,829][root][INFO] - Training Epoch: 2/10, step 248/574 completed (loss: 0.39929139614105225, acc: 0.8918918967247009)
[2025-01-06 01:08:26,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:27,198][root][INFO] - Training Epoch: 2/10, step 249/574 completed (loss: 0.5828083157539368, acc: 0.8648648858070374)
[2025-01-06 01:08:27,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:27,576][root][INFO] - Training Epoch: 2/10, step 250/574 completed (loss: 0.15384477376937866, acc: 1.0)
[2025-01-06 01:08:27,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:27,933][root][INFO] - Training Epoch: 2/10, step 251/574 completed (loss: 0.4606676995754242, acc: 0.8382353186607361)
[2025-01-06 01:08:28,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:28,298][root][INFO] - Training Epoch: 2/10, step 252/574 completed (loss: 0.22685541212558746, acc: 0.9268292784690857)
[2025-01-06 01:08:28,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:28,676][root][INFO] - Training Epoch: 2/10, step 253/574 completed (loss: 0.16112935543060303, acc: 0.9200000166893005)
[2025-01-06 01:08:28,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:28,991][root][INFO] - Training Epoch: 2/10, step 254/574 completed (loss: 0.07000767439603806, acc: 0.9599999785423279)
[2025-01-06 01:08:29,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:29,358][root][INFO] - Training Epoch: 2/10, step 255/574 completed (loss: 0.30965256690979004, acc: 0.9032257795333862)
[2025-01-06 01:08:29,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:29,732][root][INFO] - Training Epoch: 2/10, step 256/574 completed (loss: 0.3608449399471283, acc: 0.9122806787490845)
[2025-01-06 01:08:29,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:30,068][root][INFO] - Training Epoch: 2/10, step 257/574 completed (loss: 0.2807997167110443, acc: 0.8999999761581421)
[2025-01-06 01:08:30,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:30,366][root][INFO] - Training Epoch: 2/10, step 258/574 completed (loss: 0.23863835632801056, acc: 0.9210526347160339)
[2025-01-06 01:08:30,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:30,924][root][INFO] - Training Epoch: 2/10, step 259/574 completed (loss: 0.5140376091003418, acc: 0.8867924809455872)
[2025-01-06 01:08:31,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:31,500][root][INFO] - Training Epoch: 2/10, step 260/574 completed (loss: 0.4816912114620209, acc: 0.875)
[2025-01-06 01:08:31,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:31,793][root][INFO] - Training Epoch: 2/10, step 261/574 completed (loss: 0.25921133160591125, acc: 0.9166666865348816)
[2025-01-06 01:08:31,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:32,153][root][INFO] - Training Epoch: 2/10, step 262/574 completed (loss: 0.8781804442405701, acc: 0.7419354915618896)
[2025-01-06 01:08:32,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:32,488][root][INFO] - Training Epoch: 2/10, step 263/574 completed (loss: 1.6142441034317017, acc: 0.653333306312561)
[2025-01-06 01:08:32,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:32,789][root][INFO] - Training Epoch: 2/10, step 264/574 completed (loss: 0.9257746338844299, acc: 0.6458333134651184)
[2025-01-06 01:08:33,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:33,648][root][INFO] - Training Epoch: 2/10, step 265/574 completed (loss: 1.5328377485275269, acc: 0.5920000076293945)
[2025-01-06 01:08:33,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:33,955][root][INFO] - Training Epoch: 2/10, step 266/574 completed (loss: 1.7311326265335083, acc: 0.584269642829895)
[2025-01-06 01:08:34,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:34,293][root][INFO] - Training Epoch: 2/10, step 267/574 completed (loss: 1.3272011280059814, acc: 0.5810810923576355)
[2025-01-06 01:08:34,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:34,741][root][INFO] - Training Epoch: 2/10, step 268/574 completed (loss: 0.9866839647293091, acc: 0.6724137663841248)
[2025-01-06 01:08:34,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:35,102][root][INFO] - Training Epoch: 2/10, step 269/574 completed (loss: 0.27042487263679504, acc: 0.9090909361839294)
[2025-01-06 01:08:35,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:35,443][root][INFO] - Training Epoch: 2/10, step 270/574 completed (loss: 0.27540504932403564, acc: 0.8636363744735718)
[2025-01-06 01:08:35,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:35,756][root][INFO] - Training Epoch: 2/10, step 271/574 completed (loss: 0.32662853598594666, acc: 0.90625)
[2025-01-06 01:08:35,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:36,075][root][INFO] - Training Epoch: 2/10, step 272/574 completed (loss: 0.23037365078926086, acc: 0.9666666388511658)
[2025-01-06 01:08:36,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:36,486][root][INFO] - Training Epoch: 2/10, step 273/574 completed (loss: 0.4436975419521332, acc: 0.9166666865348816)
[2025-01-06 01:08:36,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:36,874][root][INFO] - Training Epoch: 2/10, step 274/574 completed (loss: 0.4884307384490967, acc: 0.8125)
[2025-01-06 01:08:36,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:37,264][root][INFO] - Training Epoch: 2/10, step 275/574 completed (loss: 0.3587588667869568, acc: 0.9333333373069763)
[2025-01-06 01:08:37,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:37,697][root][INFO] - Training Epoch: 2/10, step 276/574 completed (loss: 0.548528254032135, acc: 0.8965517282485962)
[2025-01-06 01:08:37,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38,073][root][INFO] - Training Epoch: 2/10, step 277/574 completed (loss: 0.2840701937675476, acc: 0.9599999785423279)
[2025-01-06 01:08:38,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38,450][root][INFO] - Training Epoch: 2/10, step 278/574 completed (loss: 0.7451688647270203, acc: 0.8085106611251831)
[2025-01-06 01:08:38,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38,824][root][INFO] - Training Epoch: 2/10, step 279/574 completed (loss: 0.6262233853340149, acc: 0.8541666865348816)
[2025-01-06 01:08:38,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:39,193][root][INFO] - Training Epoch: 2/10, step 280/574 completed (loss: 0.2707430422306061, acc: 0.9318181872367859)
[2025-01-06 01:08:39,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:39,603][root][INFO] - Training Epoch: 2/10, step 281/574 completed (loss: 0.9986971020698547, acc: 0.7228915691375732)
[2025-01-06 01:08:39,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:39,948][root][INFO] - Training Epoch: 2/10, step 282/574 completed (loss: 1.1496199369430542, acc: 0.6944444179534912)
[2025-01-06 01:08:40,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:40,283][root][INFO] - Training Epoch: 2/10, step 283/574 completed (loss: 0.27838239073753357, acc: 0.9736841917037964)
[2025-01-06 01:08:41,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:41,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:41,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:42,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:42,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:42,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:43,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:43,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:43,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:44,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:44,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:44,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:45,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:45,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:45,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:46,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:46,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:47,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:47,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:47,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:48,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:48,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:49,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:49,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:49,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:50,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:50,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:50,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:51,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:51,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:51,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:52,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:52,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:53,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:53,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:53,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:54,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:54,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:56,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:56,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:56,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:57,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:57,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:57,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:58,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:58,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:58,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:59,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:59,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:59,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:00,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:00,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:00,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:01,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:01,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:02,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:02,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:02,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:03,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:03,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:03,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:04,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:04,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:04,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:05,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:05,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:06,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:06,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:06,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:07,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:07,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:07,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:07,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:08,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:08,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:09,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:09,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:09,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:10,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:10,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:10,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:11,177][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9448, device='cuda:0') eval_epoch_loss=tensor(0.6652, device='cuda:0') eval_epoch_acc=tensor(0.8124, device='cuda:0')
[2025-01-06 01:09:11,178][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:09:11,178][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:09:11,393][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.6651830673217773/model.pt
[2025-01-06 01:09:11,397][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:09:11,397][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.6651830673217773
[2025-01-06 01:09:11,397][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.812403678894043
[2025-01-06 01:09:11,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:11,724][root][INFO] - Training Epoch: 2/10, step 284/574 completed (loss: 0.6168701648712158, acc: 0.7941176295280457)
[2025-01-06 01:09:11,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:12,133][root][INFO] - Training Epoch: 2/10, step 285/574 completed (loss: 0.5492855310440063, acc: 0.8999999761581421)
[2025-01-06 01:09:12,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:12,469][root][INFO] - Training Epoch: 2/10, step 286/574 completed (loss: 0.6606484651565552, acc: 0.828125)
[2025-01-06 01:09:12,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:12,862][root][INFO] - Training Epoch: 2/10, step 287/574 completed (loss: 0.6651232242584229, acc: 0.8159999847412109)
[2025-01-06 01:09:12,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:13,210][root][INFO] - Training Epoch: 2/10, step 288/574 completed (loss: 0.6465622782707214, acc: 0.8131868243217468)
[2025-01-06 01:09:13,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:13,601][root][INFO] - Training Epoch: 2/10, step 289/574 completed (loss: 0.6241762042045593, acc: 0.8260869383811951)
[2025-01-06 01:09:13,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:13,974][root][INFO] - Training Epoch: 2/10, step 290/574 completed (loss: 0.6455180048942566, acc: 0.8247422575950623)
[2025-01-06 01:09:14,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:14,340][root][INFO] - Training Epoch: 2/10, step 291/574 completed (loss: 0.12009942531585693, acc: 0.9545454382896423)
[2025-01-06 01:09:14,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:14,694][root][INFO] - Training Epoch: 2/10, step 292/574 completed (loss: 0.8011101484298706, acc: 0.8095238208770752)
[2025-01-06 01:09:14,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:15,054][root][INFO] - Training Epoch: 2/10, step 293/574 completed (loss: 0.2818209230899811, acc: 0.931034505367279)
[2025-01-06 01:09:15,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:15,528][root][INFO] - Training Epoch: 2/10, step 294/574 completed (loss: 0.7031814455986023, acc: 0.800000011920929)
[2025-01-06 01:09:15,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:16,074][root][INFO] - Training Epoch: 2/10, step 295/574 completed (loss: 0.7168219685554504, acc: 0.8144329786300659)
[2025-01-06 01:09:16,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:16,424][root][INFO] - Training Epoch: 2/10, step 296/574 completed (loss: 0.5965160727500916, acc: 0.8103448152542114)
[2025-01-06 01:09:16,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:16,806][root][INFO] - Training Epoch: 2/10, step 297/574 completed (loss: 0.19028055667877197, acc: 0.9629629850387573)
[2025-01-06 01:09:16,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:17,178][root][INFO] - Training Epoch: 2/10, step 298/574 completed (loss: 0.6389436721801758, acc: 0.7894737124443054)
[2025-01-06 01:09:17,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:17,560][root][INFO] - Training Epoch: 2/10, step 299/574 completed (loss: 0.2737196087837219, acc: 0.9285714030265808)
[2025-01-06 01:09:17,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:17,931][root][INFO] - Training Epoch: 2/10, step 300/574 completed (loss: 0.21276552975177765, acc: 0.875)
[2025-01-06 01:09:18,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:18,298][root][INFO] - Training Epoch: 2/10, step 301/574 completed (loss: 0.5832223892211914, acc: 0.849056601524353)
[2025-01-06 01:09:18,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:18,689][root][INFO] - Training Epoch: 2/10, step 302/574 completed (loss: 0.07429399341344833, acc: 0.9811320900917053)
[2025-01-06 01:09:18,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:19,106][root][INFO] - Training Epoch: 2/10, step 303/574 completed (loss: 0.14134302735328674, acc: 0.9411764740943909)
[2025-01-06 01:09:19,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:19,466][root][INFO] - Training Epoch: 2/10, step 304/574 completed (loss: 0.31531980633735657, acc: 0.875)
[2025-01-06 01:09:19,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:19,824][root][INFO] - Training Epoch: 2/10, step 305/574 completed (loss: 0.5841683149337769, acc: 0.8196721076965332)
[2025-01-06 01:09:19,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:20,207][root][INFO] - Training Epoch: 2/10, step 306/574 completed (loss: 0.15854591131210327, acc: 0.9666666388511658)
[2025-01-06 01:09:20,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:20,544][root][INFO] - Training Epoch: 2/10, step 307/574 completed (loss: 0.03738139942288399, acc: 1.0)
[2025-01-06 01:09:20,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:20,928][root][INFO] - Training Epoch: 2/10, step 308/574 completed (loss: 0.47030794620513916, acc: 0.8695651888847351)
[2025-01-06 01:09:21,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:21,337][root][INFO] - Training Epoch: 2/10, step 309/574 completed (loss: 0.5245707631111145, acc: 0.8888888955116272)
[2025-01-06 01:09:21,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:21,696][root][INFO] - Training Epoch: 2/10, step 310/574 completed (loss: 0.39003437757492065, acc: 0.8674699068069458)
[2025-01-06 01:09:21,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:22,066][root][INFO] - Training Epoch: 2/10, step 311/574 completed (loss: 0.5097469687461853, acc: 0.807692289352417)
[2025-01-06 01:09:22,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:22,427][root][INFO] - Training Epoch: 2/10, step 312/574 completed (loss: 0.16562628746032715, acc: 0.9591836929321289)
[2025-01-06 01:09:22,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:22,738][root][INFO] - Training Epoch: 2/10, step 313/574 completed (loss: 0.0333632193505764, acc: 1.0)
[2025-01-06 01:09:22,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:23,078][root][INFO] - Training Epoch: 2/10, step 314/574 completed (loss: 0.224751815199852, acc: 0.9166666865348816)
[2025-01-06 01:09:23,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:23,447][root][INFO] - Training Epoch: 2/10, step 315/574 completed (loss: 0.3069223463535309, acc: 0.9677419066429138)
[2025-01-06 01:09:23,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:23,817][root][INFO] - Training Epoch: 2/10, step 316/574 completed (loss: 0.7794902920722961, acc: 0.774193525314331)
[2025-01-06 01:09:23,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:24,200][root][INFO] - Training Epoch: 2/10, step 317/574 completed (loss: 0.3509006202220917, acc: 0.8805969953536987)
[2025-01-06 01:09:24,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:24,510][root][INFO] - Training Epoch: 2/10, step 318/574 completed (loss: 0.1283833235502243, acc: 0.9615384340286255)
[2025-01-06 01:09:24,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:24,807][root][INFO] - Training Epoch: 2/10, step 319/574 completed (loss: 0.3025926351547241, acc: 0.9333333373069763)
[2025-01-06 01:09:24,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:25,100][root][INFO] - Training Epoch: 2/10, step 320/574 completed (loss: 0.29721564054489136, acc: 0.9193548560142517)
[2025-01-06 01:09:25,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:25,451][root][INFO] - Training Epoch: 2/10, step 321/574 completed (loss: 0.02706492692232132, acc: 1.0)
[2025-01-06 01:09:25,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:25,858][root][INFO] - Training Epoch: 2/10, step 322/574 completed (loss: 1.6276440620422363, acc: 0.5925925970077515)
[2025-01-06 01:09:25,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:26,204][root][INFO] - Training Epoch: 2/10, step 323/574 completed (loss: 2.1261789798736572, acc: 0.4285714328289032)
[2025-01-06 01:09:26,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:26,557][root][INFO] - Training Epoch: 2/10, step 324/574 completed (loss: 1.5780341625213623, acc: 0.6410256624221802)
[2025-01-06 01:09:26,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:26,863][root][INFO] - Training Epoch: 2/10, step 325/574 completed (loss: 1.6940350532531738, acc: 0.6341463327407837)
[2025-01-06 01:09:26,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:27,175][root][INFO] - Training Epoch: 2/10, step 326/574 completed (loss: 1.481992483139038, acc: 0.6052631735801697)
[2025-01-06 01:09:27,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:27,518][root][INFO] - Training Epoch: 2/10, step 327/574 completed (loss: 0.5474200248718262, acc: 0.8421052694320679)
[2025-01-06 01:09:27,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:27,904][root][INFO] - Training Epoch: 2/10, step 328/574 completed (loss: 0.12916651368141174, acc: 0.9642857313156128)
[2025-01-06 01:09:28,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:28,308][root][INFO] - Training Epoch: 2/10, step 329/574 completed (loss: 0.2567994296550751, acc: 0.8888888955116272)
[2025-01-06 01:09:28,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:28,669][root][INFO] - Training Epoch: 2/10, step 330/574 completed (loss: 0.09706132858991623, acc: 0.96875)
[2025-01-06 01:09:28,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:29,072][root][INFO] - Training Epoch: 2/10, step 331/574 completed (loss: 0.36778780817985535, acc: 0.8870967626571655)
[2025-01-06 01:09:29,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:29,481][root][INFO] - Training Epoch: 2/10, step 332/574 completed (loss: 0.2047010362148285, acc: 0.9473684430122375)
[2025-01-06 01:09:29,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:29,825][root][INFO] - Training Epoch: 2/10, step 333/574 completed (loss: 0.6009753942489624, acc: 0.84375)
[2025-01-06 01:09:29,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:30,204][root][INFO] - Training Epoch: 2/10, step 334/574 completed (loss: 0.2603321373462677, acc: 0.9333333373069763)
[2025-01-06 01:09:30,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:30,569][root][INFO] - Training Epoch: 2/10, step 335/574 completed (loss: 0.5891343951225281, acc: 0.8947368264198303)
[2025-01-06 01:09:30,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:30,937][root][INFO] - Training Epoch: 2/10, step 336/574 completed (loss: 1.0494027137756348, acc: 0.6800000071525574)
[2025-01-06 01:09:31,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:31,286][root][INFO] - Training Epoch: 2/10, step 337/574 completed (loss: 1.4862301349639893, acc: 0.5862069129943848)
[2025-01-06 01:09:31,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:31,695][root][INFO] - Training Epoch: 2/10, step 338/574 completed (loss: 1.3762805461883545, acc: 0.5957446694374084)
[2025-01-06 01:09:31,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:32,085][root][INFO] - Training Epoch: 2/10, step 339/574 completed (loss: 1.5745128393173218, acc: 0.6144578456878662)
[2025-01-06 01:09:32,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:32,439][root][INFO] - Training Epoch: 2/10, step 340/574 completed (loss: 0.08595943450927734, acc: 1.0)
[2025-01-06 01:09:32,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:32,797][root][INFO] - Training Epoch: 2/10, step 341/574 completed (loss: 0.7647905349731445, acc: 0.8205128312110901)
[2025-01-06 01:09:32,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:33,173][root][INFO] - Training Epoch: 2/10, step 342/574 completed (loss: 0.4887314736843109, acc: 0.8554216623306274)
[2025-01-06 01:09:33,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:33,510][root][INFO] - Training Epoch: 2/10, step 343/574 completed (loss: 0.8174465894699097, acc: 0.7735849022865295)
[2025-01-06 01:09:33,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:33,818][root][INFO] - Training Epoch: 2/10, step 344/574 completed (loss: 0.19233040511608124, acc: 0.949367105960846)
[2025-01-06 01:09:33,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:34,173][root][INFO] - Training Epoch: 2/10, step 345/574 completed (loss: 0.11620313674211502, acc: 0.9607843160629272)
[2025-01-06 01:09:34,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:34,527][root][INFO] - Training Epoch: 2/10, step 346/574 completed (loss: 0.527898907661438, acc: 0.8805969953536987)
[2025-01-06 01:09:34,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:34,911][root][INFO] - Training Epoch: 2/10, step 347/574 completed (loss: 0.15993864834308624, acc: 0.949999988079071)
[2025-01-06 01:09:35,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:35,281][root][INFO] - Training Epoch: 2/10, step 348/574 completed (loss: 0.22145487368106842, acc: 0.9599999785423279)
[2025-01-06 01:09:35,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:35,659][root][INFO] - Training Epoch: 2/10, step 349/574 completed (loss: 1.0039154291152954, acc: 0.7222222089767456)
[2025-01-06 01:09:35,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:36,036][root][INFO] - Training Epoch: 2/10, step 350/574 completed (loss: 0.9951119422912598, acc: 0.7209302186965942)
[2025-01-06 01:09:36,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:36,418][root][INFO] - Training Epoch: 2/10, step 351/574 completed (loss: 0.46675628423690796, acc: 0.8205128312110901)
[2025-01-06 01:09:36,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:36,799][root][INFO] - Training Epoch: 2/10, step 352/574 completed (loss: 1.5320285558700562, acc: 0.5555555820465088)
[2025-01-06 01:09:36,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:37,174][root][INFO] - Training Epoch: 2/10, step 353/574 completed (loss: 0.21157999336719513, acc: 0.95652174949646)
[2025-01-06 01:09:37,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:37,529][root][INFO] - Training Epoch: 2/10, step 354/574 completed (loss: 0.47858163714408875, acc: 0.7692307829856873)
[2025-01-06 01:09:37,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:37,912][root][INFO] - Training Epoch: 2/10, step 355/574 completed (loss: 1.0630406141281128, acc: 0.6813187003135681)
[2025-01-06 01:09:38,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:38,417][root][INFO] - Training Epoch: 2/10, step 356/574 completed (loss: 0.8129240870475769, acc: 0.739130437374115)
[2025-01-06 01:09:38,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:38,766][root][INFO] - Training Epoch: 2/10, step 357/574 completed (loss: 0.61314457654953, acc: 0.8260869383811951)
[2025-01-06 01:09:38,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:39,160][root][INFO] - Training Epoch: 2/10, step 358/574 completed (loss: 0.8070095777511597, acc: 0.7551020383834839)
[2025-01-06 01:09:39,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:39,532][root][INFO] - Training Epoch: 2/10, step 359/574 completed (loss: 0.011247295886278152, acc: 1.0)
[2025-01-06 01:09:39,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:39,915][root][INFO] - Training Epoch: 2/10, step 360/574 completed (loss: 0.31717145442962646, acc: 0.9230769276618958)
[2025-01-06 01:09:40,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:40,259][root][INFO] - Training Epoch: 2/10, step 361/574 completed (loss: 0.6410989761352539, acc: 0.7560975551605225)
[2025-01-06 01:09:40,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:40,596][root][INFO] - Training Epoch: 2/10, step 362/574 completed (loss: 0.5756107568740845, acc: 0.8666666746139526)
[2025-01-06 01:09:40,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:40,928][root][INFO] - Training Epoch: 2/10, step 363/574 completed (loss: 0.2690730690956116, acc: 0.9078947305679321)
[2025-01-06 01:09:41,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:41,310][root][INFO] - Training Epoch: 2/10, step 364/574 completed (loss: 0.16566899418830872, acc: 0.9268292784690857)
[2025-01-06 01:09:41,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:41,670][root][INFO] - Training Epoch: 2/10, step 365/574 completed (loss: 0.26001209020614624, acc: 0.9090909361839294)
[2025-01-06 01:09:41,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:42,034][root][INFO] - Training Epoch: 2/10, step 366/574 completed (loss: 0.010090782307088375, acc: 1.0)
[2025-01-06 01:09:42,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:42,366][root][INFO] - Training Epoch: 2/10, step 367/574 completed (loss: 0.09916984289884567, acc: 0.95652174949646)
[2025-01-06 01:09:42,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:42,694][root][INFO] - Training Epoch: 2/10, step 368/574 completed (loss: 0.15661272406578064, acc: 0.9642857313156128)
[2025-01-06 01:09:42,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:42,957][root][INFO] - Training Epoch: 2/10, step 369/574 completed (loss: 0.4494965076446533, acc: 0.84375)
[2025-01-06 01:09:43,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:43,560][root][INFO] - Training Epoch: 2/10, step 370/574 completed (loss: 0.6670113801956177, acc: 0.800000011920929)
[2025-01-06 01:09:43,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:44,415][root][INFO] - Training Epoch: 2/10, step 371/574 completed (loss: 0.49591344594955444, acc: 0.8679245114326477)
[2025-01-06 01:09:44,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:44,752][root][INFO] - Training Epoch: 2/10, step 372/574 completed (loss: 0.2848043739795685, acc: 0.9444444179534912)
[2025-01-06 01:09:44,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:45,099][root][INFO] - Training Epoch: 2/10, step 373/574 completed (loss: 0.3176209032535553, acc: 0.9642857313156128)
[2025-01-06 01:09:45,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:45,423][root][INFO] - Training Epoch: 2/10, step 374/574 completed (loss: 0.2478903979063034, acc: 0.9142857193946838)
[2025-01-06 01:09:45,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:45,717][root][INFO] - Training Epoch: 2/10, step 375/574 completed (loss: 0.010029543191194534, acc: 1.0)
[2025-01-06 01:09:45,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:46,052][root][INFO] - Training Epoch: 2/10, step 376/574 completed (loss: 0.09454314410686493, acc: 0.9130434989929199)
[2025-01-06 01:09:46,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:46,398][root][INFO] - Training Epoch: 2/10, step 377/574 completed (loss: 0.17936229705810547, acc: 0.875)
[2025-01-06 01:09:46,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:46,807][root][INFO] - Training Epoch: 2/10, step 378/574 completed (loss: 0.10418328642845154, acc: 0.9578947424888611)
[2025-01-06 01:09:47,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:47,398][root][INFO] - Training Epoch: 2/10, step 379/574 completed (loss: 0.36025166511535645, acc: 0.910179615020752)
[2025-01-06 01:09:47,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:47,814][root][INFO] - Training Epoch: 2/10, step 380/574 completed (loss: 0.4501027762889862, acc: 0.9097744226455688)
[2025-01-06 01:09:48,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:49,054][root][INFO] - Training Epoch: 2/10, step 381/574 completed (loss: 0.6939969658851624, acc: 0.8235294222831726)
[2025-01-06 01:09:49,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:49,617][root][INFO] - Training Epoch: 2/10, step 382/574 completed (loss: 0.2274595946073532, acc: 0.9459459185600281)
[2025-01-06 01:09:49,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:49,947][root][INFO] - Training Epoch: 2/10, step 383/574 completed (loss: 0.528541624546051, acc: 0.8928571343421936)
[2025-01-06 01:09:50,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:50,276][root][INFO] - Training Epoch: 2/10, step 384/574 completed (loss: 0.06688641756772995, acc: 1.0)
[2025-01-06 01:09:50,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:50,646][root][INFO] - Training Epoch: 2/10, step 385/574 completed (loss: 0.0771106705069542, acc: 0.96875)
[2025-01-06 01:09:50,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:51,002][root][INFO] - Training Epoch: 2/10, step 386/574 completed (loss: 0.024916164577007294, acc: 1.0)
[2025-01-06 01:09:51,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:51,367][root][INFO] - Training Epoch: 2/10, step 387/574 completed (loss: 0.04450995475053787, acc: 0.9736841917037964)
[2025-01-06 01:09:51,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:51,721][root][INFO] - Training Epoch: 2/10, step 388/574 completed (loss: 0.10319656878709793, acc: 0.9545454382896423)
[2025-01-06 01:09:51,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:52,100][root][INFO] - Training Epoch: 2/10, step 389/574 completed (loss: 0.019262904301285744, acc: 1.0)
[2025-01-06 01:09:52,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:52,408][root][INFO] - Training Epoch: 2/10, step 390/574 completed (loss: 0.31717580556869507, acc: 0.8571428656578064)
[2025-01-06 01:09:52,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:52,783][root][INFO] - Training Epoch: 2/10, step 391/574 completed (loss: 1.328967571258545, acc: 0.6296296119689941)
[2025-01-06 01:09:52,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:53,137][root][INFO] - Training Epoch: 2/10, step 392/574 completed (loss: 1.2751805782318115, acc: 0.6893203854560852)
[2025-01-06 01:09:53,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:53,660][root][INFO] - Training Epoch: 2/10, step 393/574 completed (loss: 1.1472257375717163, acc: 0.7352941036224365)
[2025-01-06 01:09:53,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:54,032][root][INFO] - Training Epoch: 2/10, step 394/574 completed (loss: 1.107508897781372, acc: 0.6399999856948853)
[2025-01-06 01:09:54,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:54,405][root][INFO] - Training Epoch: 2/10, step 395/574 completed (loss: 1.0389156341552734, acc: 0.75)
[2025-01-06 01:09:54,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:54,787][root][INFO] - Training Epoch: 2/10, step 396/574 completed (loss: 0.6651244759559631, acc: 0.8139534592628479)
[2025-01-06 01:09:54,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:55,147][root][INFO] - Training Epoch: 2/10, step 397/574 completed (loss: 0.31915155053138733, acc: 0.875)
[2025-01-06 01:09:55,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:55,564][root][INFO] - Training Epoch: 2/10, step 398/574 completed (loss: 0.31795263290405273, acc: 0.930232584476471)
[2025-01-06 01:09:55,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:55,913][root][INFO] - Training Epoch: 2/10, step 399/574 completed (loss: 0.2779463529586792, acc: 0.9599999785423279)
[2025-01-06 01:09:56,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:56,448][root][INFO] - Training Epoch: 2/10, step 400/574 completed (loss: 0.363051176071167, acc: 0.9117646813392639)
[2025-01-06 01:09:56,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:56,810][root][INFO] - Training Epoch: 2/10, step 401/574 completed (loss: 0.6040917038917542, acc: 0.800000011920929)
[2025-01-06 01:09:56,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:57,181][root][INFO] - Training Epoch: 2/10, step 402/574 completed (loss: 0.8297485113143921, acc: 0.8181818127632141)
[2025-01-06 01:09:57,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:57,555][root][INFO] - Training Epoch: 2/10, step 403/574 completed (loss: 0.3363214433193207, acc: 0.9090909361839294)
[2025-01-06 01:09:57,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:57,914][root][INFO] - Training Epoch: 2/10, step 404/574 completed (loss: 0.4623267948627472, acc: 0.8709677457809448)
[2025-01-06 01:09:58,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:58,283][root][INFO] - Training Epoch: 2/10, step 405/574 completed (loss: 0.12635573744773865, acc: 0.9629629850387573)
[2025-01-06 01:09:58,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:58,607][root][INFO] - Training Epoch: 2/10, step 406/574 completed (loss: 0.2146584689617157, acc: 0.9599999785423279)
[2025-01-06 01:09:58,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:58,938][root][INFO] - Training Epoch: 2/10, step 407/574 completed (loss: 0.10952962934970856, acc: 0.9444444179534912)
[2025-01-06 01:09:59,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:59,294][root][INFO] - Training Epoch: 2/10, step 408/574 completed (loss: 0.23821204900741577, acc: 0.9259259104728699)
[2025-01-06 01:09:59,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:59,606][root][INFO] - Training Epoch: 2/10, step 409/574 completed (loss: 0.18722344934940338, acc: 1.0)
[2025-01-06 01:09:59,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:59,978][root][INFO] - Training Epoch: 2/10, step 410/574 completed (loss: 0.21929322183132172, acc: 0.9655172228813171)
[2025-01-06 01:10:00,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:00,375][root][INFO] - Training Epoch: 2/10, step 411/574 completed (loss: 0.052036020904779434, acc: 1.0)
[2025-01-06 01:10:00,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:00,747][root][INFO] - Training Epoch: 2/10, step 412/574 completed (loss: 0.19314555823802948, acc: 0.9666666388511658)
[2025-01-06 01:10:00,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01,037][root][INFO] - Training Epoch: 2/10, step 413/574 completed (loss: 0.2812613844871521, acc: 0.9696969985961914)
[2025-01-06 01:10:01,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01,338][root][INFO] - Training Epoch: 2/10, step 414/574 completed (loss: 0.10105549544095993, acc: 1.0)
[2025-01-06 01:10:01,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01,633][root][INFO] - Training Epoch: 2/10, step 415/574 completed (loss: 0.6512507200241089, acc: 0.8039215803146362)
[2025-01-06 01:10:01,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01,930][root][INFO] - Training Epoch: 2/10, step 416/574 completed (loss: 0.4154156446456909, acc: 0.8846153616905212)
[2025-01-06 01:10:02,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:02,258][root][INFO] - Training Epoch: 2/10, step 417/574 completed (loss: 0.32837915420532227, acc: 0.9444444179534912)
[2025-01-06 01:10:02,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:02,654][root][INFO] - Training Epoch: 2/10, step 418/574 completed (loss: 0.4806744158267975, acc: 0.875)
[2025-01-06 01:10:02,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:03,052][root][INFO] - Training Epoch: 2/10, step 419/574 completed (loss: 0.4938703179359436, acc: 0.8999999761581421)
[2025-01-06 01:10:03,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:03,456][root][INFO] - Training Epoch: 2/10, step 420/574 completed (loss: 0.0491812564432621, acc: 1.0)
[2025-01-06 01:10:03,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:03,834][root][INFO] - Training Epoch: 2/10, step 421/574 completed (loss: 0.3204670250415802, acc: 0.9333333373069763)
[2025-01-06 01:10:03,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:04,219][root][INFO] - Training Epoch: 2/10, step 422/574 completed (loss: 0.32321760058403015, acc: 0.90625)
[2025-01-06 01:10:04,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:04,622][root][INFO] - Training Epoch: 2/10, step 423/574 completed (loss: 0.531399667263031, acc: 0.8888888955116272)
[2025-01-06 01:10:04,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:04,987][root][INFO] - Training Epoch: 2/10, step 424/574 completed (loss: 0.5174068808555603, acc: 0.9259259104728699)
[2025-01-06 01:10:05,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:05,384][root][INFO] - Training Epoch: 2/10, step 425/574 completed (loss: 0.15421438217163086, acc: 0.9696969985961914)
[2025-01-06 01:10:05,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:05,773][root][INFO] - Training Epoch: 2/10, step 426/574 completed (loss: 0.0247255377471447, acc: 1.0)
[2025-01-06 01:10:06,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:06,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:07,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:07,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:07,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:08,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:08,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:08,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:09,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:09,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:09,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:10,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:10,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:10,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:11,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:11,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:11,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:12,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:12,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:13,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:13,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:13,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:14,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:14,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:14,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:15,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:15,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:15,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:16,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:16,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:17,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:17,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:17,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:19,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:19,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:20,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:20,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:20,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:21,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:21,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:21,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:22,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:22,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:22,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:23,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:23,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:23,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:24,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:24,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:24,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:25,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:25,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:25,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:26,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:26,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:27,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:27,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:27,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:28,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:28,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:28,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:29,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:29,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:30,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:30,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:30,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:31,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:31,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:31,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:32,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:32,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:32,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:33,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:33,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:33,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:35,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:35,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:36,255][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8975, device='cuda:0') eval_epoch_loss=tensor(0.6405, device='cuda:0') eval_epoch_acc=tensor(0.8231, device='cuda:0')
[2025-01-06 01:10:36,256][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:10:36,256][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:10:36,483][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_427_loss_0.6405490040779114/model.pt
[2025-01-06 01:10:36,486][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:10:36,487][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.6405490040779114
[2025-01-06 01:10:36,487][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8230716586112976
[2025-01-06 01:10:36,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:36,849][root][INFO] - Training Epoch: 2/10, step 427/574 completed (loss: 0.2849588990211487, acc: 0.9189189076423645)
[2025-01-06 01:10:36,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:37,160][root][INFO] - Training Epoch: 2/10, step 428/574 completed (loss: 0.04519073665142059, acc: 1.0)
[2025-01-06 01:10:37,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:37,527][root][INFO] - Training Epoch: 2/10, step 429/574 completed (loss: 0.3486056625843048, acc: 0.9130434989929199)
[2025-01-06 01:10:37,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:37,890][root][INFO] - Training Epoch: 2/10, step 430/574 completed (loss: 0.006068844348192215, acc: 1.0)
[2025-01-06 01:10:38,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:38,279][root][INFO] - Training Epoch: 2/10, step 431/574 completed (loss: 0.015352661721408367, acc: 1.0)
[2025-01-06 01:10:38,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:38,662][root][INFO] - Training Epoch: 2/10, step 432/574 completed (loss: 0.29622504115104675, acc: 0.95652174949646)
[2025-01-06 01:10:38,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:39,063][root][INFO] - Training Epoch: 2/10, step 433/574 completed (loss: 0.28522443771362305, acc: 0.9166666865348816)
[2025-01-06 01:10:39,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:39,454][root][INFO] - Training Epoch: 2/10, step 434/574 completed (loss: 0.0019678231328725815, acc: 1.0)
[2025-01-06 01:10:39,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:39,791][root][INFO] - Training Epoch: 2/10, step 435/574 completed (loss: 0.04568886384367943, acc: 1.0)
[2025-01-06 01:10:39,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:40,147][root][INFO] - Training Epoch: 2/10, step 436/574 completed (loss: 0.5636825561523438, acc: 0.8333333134651184)
[2025-01-06 01:10:40,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:40,538][root][INFO] - Training Epoch: 2/10, step 437/574 completed (loss: 0.09642784297466278, acc: 0.9772727489471436)
[2025-01-06 01:10:40,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:40,921][root][INFO] - Training Epoch: 2/10, step 438/574 completed (loss: 0.016112664714455605, acc: 1.0)
[2025-01-06 01:10:41,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:41,237][root][INFO] - Training Epoch: 2/10, step 439/574 completed (loss: 0.4888823926448822, acc: 0.8717948794364929)
[2025-01-06 01:10:41,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:41,709][root][INFO] - Training Epoch: 2/10, step 440/574 completed (loss: 0.5602733492851257, acc: 0.8484848737716675)
[2025-01-06 01:10:41,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:42,393][root][INFO] - Training Epoch: 2/10, step 441/574 completed (loss: 1.036765694618225, acc: 0.7039999961853027)
[2025-01-06 01:10:42,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:42,827][root][INFO] - Training Epoch: 2/10, step 442/574 completed (loss: 1.0389773845672607, acc: 0.7338709831237793)
[2025-01-06 01:10:42,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:43,476][root][INFO] - Training Epoch: 2/10, step 443/574 completed (loss: 0.681837260723114, acc: 0.8407959938049316)
[2025-01-06 01:10:43,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:43,850][root][INFO] - Training Epoch: 2/10, step 444/574 completed (loss: 0.40219399333000183, acc: 0.849056601524353)
[2025-01-06 01:10:44,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:44,271][root][INFO] - Training Epoch: 2/10, step 445/574 completed (loss: 0.2313428521156311, acc: 0.9090909361839294)
[2025-01-06 01:10:44,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:44,583][root][INFO] - Training Epoch: 2/10, step 446/574 completed (loss: 0.38690119981765747, acc: 0.8695651888847351)
[2025-01-06 01:10:44,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:44,885][root][INFO] - Training Epoch: 2/10, step 447/574 completed (loss: 0.6503881216049194, acc: 0.8461538553237915)
[2025-01-06 01:10:44,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:45,244][root][INFO] - Training Epoch: 2/10, step 448/574 completed (loss: 0.24027416110038757, acc: 0.9642857313156128)
[2025-01-06 01:10:45,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:45,597][root][INFO] - Training Epoch: 2/10, step 449/574 completed (loss: 0.2125880867242813, acc: 0.9253731369972229)
[2025-01-06 01:10:45,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:45,943][root][INFO] - Training Epoch: 2/10, step 450/574 completed (loss: 0.07370167970657349, acc: 1.0)
[2025-01-06 01:10:46,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:46,304][root][INFO] - Training Epoch: 2/10, step 451/574 completed (loss: 0.12399082630872726, acc: 0.945652186870575)
[2025-01-06 01:10:46,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:46,695][root][INFO] - Training Epoch: 2/10, step 452/574 completed (loss: 0.3591907322406769, acc: 0.8974359035491943)
[2025-01-06 01:10:46,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:47,027][root][INFO] - Training Epoch: 2/10, step 453/574 completed (loss: 0.40343764424324036, acc: 0.8815789222717285)
[2025-01-06 01:10:47,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:47,397][root][INFO] - Training Epoch: 2/10, step 454/574 completed (loss: 0.22122415900230408, acc: 0.9387755393981934)
[2025-01-06 01:10:47,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:47,804][root][INFO] - Training Epoch: 2/10, step 455/574 completed (loss: 0.34365034103393555, acc: 0.939393937587738)
[2025-01-06 01:10:47,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:48,165][root][INFO] - Training Epoch: 2/10, step 456/574 completed (loss: 0.6678654551506042, acc: 0.8350515365600586)
[2025-01-06 01:10:48,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:48,562][root][INFO] - Training Epoch: 2/10, step 457/574 completed (loss: 0.07520558685064316, acc: 0.9714285731315613)
[2025-01-06 01:10:48,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:48,949][root][INFO] - Training Epoch: 2/10, step 458/574 completed (loss: 0.5238818526268005, acc: 0.854651153087616)
[2025-01-06 01:10:49,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:49,262][root][INFO] - Training Epoch: 2/10, step 459/574 completed (loss: 0.11464543640613556, acc: 0.9464285969734192)
[2025-01-06 01:10:49,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:49,642][root][INFO] - Training Epoch: 2/10, step 460/574 completed (loss: 0.3232545554637909, acc: 0.9382715821266174)
[2025-01-06 01:10:49,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:49,974][root][INFO] - Training Epoch: 2/10, step 461/574 completed (loss: 0.35763970017433167, acc: 0.8888888955116272)
[2025-01-06 01:10:50,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:50,311][root][INFO] - Training Epoch: 2/10, step 462/574 completed (loss: 0.12366125732660294, acc: 0.96875)
[2025-01-06 01:10:50,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:50,694][root][INFO] - Training Epoch: 2/10, step 463/574 completed (loss: 0.5497692823410034, acc: 0.8461538553237915)
[2025-01-06 01:10:50,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:51,029][root][INFO] - Training Epoch: 2/10, step 464/574 completed (loss: 0.3248831629753113, acc: 0.9130434989929199)
[2025-01-06 01:10:51,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:51,394][root][INFO] - Training Epoch: 2/10, step 465/574 completed (loss: 0.438041627407074, acc: 0.8690476417541504)
[2025-01-06 01:10:51,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:51,747][root][INFO] - Training Epoch: 2/10, step 466/574 completed (loss: 0.6879991888999939, acc: 0.8554216623306274)
[2025-01-06 01:10:51,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:52,167][root][INFO] - Training Epoch: 2/10, step 467/574 completed (loss: 0.2989368736743927, acc: 0.8738738894462585)
[2025-01-06 01:10:52,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:52,534][root][INFO] - Training Epoch: 2/10, step 468/574 completed (loss: 0.8723134994506836, acc: 0.7961165308952332)
[2025-01-06 01:10:52,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:52,856][root][INFO] - Training Epoch: 2/10, step 469/574 completed (loss: 0.6739899516105652, acc: 0.8211382031440735)
[2025-01-06 01:10:52,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:53,155][root][INFO] - Training Epoch: 2/10, step 470/574 completed (loss: 0.194166898727417, acc: 0.9166666865348816)
[2025-01-06 01:10:53,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:53,481][root][INFO] - Training Epoch: 2/10, step 471/574 completed (loss: 0.49355369806289673, acc: 0.8571428656578064)
[2025-01-06 01:10:53,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:53,895][root][INFO] - Training Epoch: 2/10, step 472/574 completed (loss: 0.7360285520553589, acc: 0.7941176295280457)
[2025-01-06 01:10:54,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:54,308][root][INFO] - Training Epoch: 2/10, step 473/574 completed (loss: 0.9375327229499817, acc: 0.7423580884933472)
[2025-01-06 01:10:54,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:54,703][root][INFO] - Training Epoch: 2/10, step 474/574 completed (loss: 0.6587620377540588, acc: 0.7708333134651184)
[2025-01-06 01:10:54,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:55,031][root][INFO] - Training Epoch: 2/10, step 475/574 completed (loss: 0.4416588544845581, acc: 0.8527607321739197)
[2025-01-06 01:10:55,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:55,358][root][INFO] - Training Epoch: 2/10, step 476/574 completed (loss: 0.5558565855026245, acc: 0.8345323801040649)
[2025-01-06 01:10:55,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:55,716][root][INFO] - Training Epoch: 2/10, step 477/574 completed (loss: 0.9334056377410889, acc: 0.7537688612937927)
[2025-01-06 01:10:55,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:56,077][root][INFO] - Training Epoch: 2/10, step 478/574 completed (loss: 0.5687565803527832, acc: 0.8055555820465088)
[2025-01-06 01:10:56,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:56,477][root][INFO] - Training Epoch: 2/10, step 479/574 completed (loss: 0.7035488486289978, acc: 0.7878788113594055)
[2025-01-06 01:10:56,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:56,844][root][INFO] - Training Epoch: 2/10, step 480/574 completed (loss: 0.379312664270401, acc: 0.8518518805503845)
[2025-01-06 01:10:56,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:57,202][root][INFO] - Training Epoch: 2/10, step 481/574 completed (loss: 0.5532132387161255, acc: 0.800000011920929)
[2025-01-06 01:10:57,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:57,536][root][INFO] - Training Epoch: 2/10, step 482/574 completed (loss: 0.6576956510543823, acc: 0.75)
[2025-01-06 01:10:57,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:57,896][root][INFO] - Training Epoch: 2/10, step 483/574 completed (loss: 0.874003529548645, acc: 0.6551724076271057)
[2025-01-06 01:10:58,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:58,284][root][INFO] - Training Epoch: 2/10, step 484/574 completed (loss: 0.14652971923351288, acc: 0.9032257795333862)
[2025-01-06 01:10:58,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:58,627][root][INFO] - Training Epoch: 2/10, step 485/574 completed (loss: 0.4842515289783478, acc: 0.8421052694320679)
[2025-01-06 01:10:58,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:58,982][root][INFO] - Training Epoch: 2/10, step 486/574 completed (loss: 1.03984534740448, acc: 0.6666666865348816)
[2025-01-06 01:10:59,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:59,304][root][INFO] - Training Epoch: 2/10, step 487/574 completed (loss: 0.6370152831077576, acc: 0.8571428656578064)
[2025-01-06 01:10:59,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:59,654][root][INFO] - Training Epoch: 2/10, step 488/574 completed (loss: 0.6591618061065674, acc: 0.7272727489471436)
[2025-01-06 01:10:59,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:00,075][root][INFO] - Training Epoch: 2/10, step 489/574 completed (loss: 1.1156779527664185, acc: 0.7076923251152039)
[2025-01-06 01:11:00,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:00,421][root][INFO] - Training Epoch: 2/10, step 490/574 completed (loss: 0.29634249210357666, acc: 0.8999999761581421)
[2025-01-06 01:11:00,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:00,751][root][INFO] - Training Epoch: 2/10, step 491/574 completed (loss: 0.6492756009101868, acc: 0.8275862336158752)
[2025-01-06 01:11:00,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:01,095][root][INFO] - Training Epoch: 2/10, step 492/574 completed (loss: 0.5760522484779358, acc: 0.7647058963775635)
[2025-01-06 01:11:01,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:01,374][root][INFO] - Training Epoch: 2/10, step 493/574 completed (loss: 0.6068443655967712, acc: 0.8275862336158752)
[2025-01-06 01:11:01,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:01,729][root][INFO] - Training Epoch: 2/10, step 494/574 completed (loss: 0.6140942573547363, acc: 0.8947368264198303)
[2025-01-06 01:11:01,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:01,976][root][INFO] - Training Epoch: 2/10, step 495/574 completed (loss: 0.8346094489097595, acc: 0.7368420958518982)
[2025-01-06 01:11:02,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:02,327][root][INFO] - Training Epoch: 2/10, step 496/574 completed (loss: 0.8245750069618225, acc: 0.7946428656578064)
[2025-01-06 01:11:02,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:02,710][root][INFO] - Training Epoch: 2/10, step 497/574 completed (loss: 0.5816889405250549, acc: 0.8089887499809265)
[2025-01-06 01:11:02,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:03,031][root][INFO] - Training Epoch: 2/10, step 498/574 completed (loss: 0.8450591564178467, acc: 0.7415730357170105)
[2025-01-06 01:11:03,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:03,376][root][INFO] - Training Epoch: 2/10, step 499/574 completed (loss: 1.4758129119873047, acc: 0.6028369069099426)
[2025-01-06 01:11:03,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:03,800][root][INFO] - Training Epoch: 2/10, step 500/574 completed (loss: 0.9911988973617554, acc: 0.75)
[2025-01-06 01:11:03,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:04,165][root][INFO] - Training Epoch: 2/10, step 501/574 completed (loss: 0.14787045121192932, acc: 0.9599999785423279)
[2025-01-06 01:11:04,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:04,541][root][INFO] - Training Epoch: 2/10, step 502/574 completed (loss: 0.07813665270805359, acc: 0.9615384340286255)
[2025-01-06 01:11:04,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:04,885][root][INFO] - Training Epoch: 2/10, step 503/574 completed (loss: 0.21526287496089935, acc: 0.9629629850387573)
[2025-01-06 01:11:05,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:05,249][root][INFO] - Training Epoch: 2/10, step 504/574 completed (loss: 0.14470598101615906, acc: 0.9259259104728699)
[2025-01-06 01:11:05,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:05,623][root][INFO] - Training Epoch: 2/10, step 505/574 completed (loss: 0.7612183690071106, acc: 0.849056601524353)
[2025-01-06 01:11:05,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:05,962][root][INFO] - Training Epoch: 2/10, step 506/574 completed (loss: 0.75859534740448, acc: 0.7931034564971924)
[2025-01-06 01:11:06,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:06,541][root][INFO] - Training Epoch: 2/10, step 507/574 completed (loss: 1.278651475906372, acc: 0.6486486196517944)
[2025-01-06 01:11:06,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:06,977][root][INFO] - Training Epoch: 2/10, step 508/574 completed (loss: 0.9559063911437988, acc: 0.7183098793029785)
[2025-01-06 01:11:07,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:07,284][root][INFO] - Training Epoch: 2/10, step 509/574 completed (loss: 0.17709192633628845, acc: 0.949999988079071)
[2025-01-06 01:11:07,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:07,535][root][INFO] - Training Epoch: 2/10, step 510/574 completed (loss: 0.3577384352684021, acc: 0.8333333134651184)
[2025-01-06 01:11:07,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:07,835][root][INFO] - Training Epoch: 2/10, step 511/574 completed (loss: 0.47905606031417847, acc: 0.807692289352417)
[2025-01-06 01:11:09,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:10,310][root][INFO] - Training Epoch: 2/10, step 512/574 completed (loss: 1.3694146871566772, acc: 0.6071428656578064)
[2025-01-06 01:11:10,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:11,068][root][INFO] - Training Epoch: 2/10, step 513/574 completed (loss: 0.2563062608242035, acc: 0.9126983880996704)
[2025-01-06 01:11:11,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:11,352][root][INFO] - Training Epoch: 2/10, step 514/574 completed (loss: 0.7277806997299194, acc: 0.75)
[2025-01-06 01:11:11,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:11,706][root][INFO] - Training Epoch: 2/10, step 515/574 completed (loss: 0.16635937988758087, acc: 0.9666666388511658)
[2025-01-06 01:11:11,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:12,391][root][INFO] - Training Epoch: 2/10, step 516/574 completed (loss: 0.6812318563461304, acc: 0.8333333134651184)
[2025-01-06 01:11:12,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:12,707][root][INFO] - Training Epoch: 2/10, step 517/574 completed (loss: 0.008811053819954395, acc: 1.0)
[2025-01-06 01:11:12,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:13,081][root][INFO] - Training Epoch: 2/10, step 518/574 completed (loss: 0.18448109924793243, acc: 0.9032257795333862)
[2025-01-06 01:11:13,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:13,470][root][INFO] - Training Epoch: 2/10, step 519/574 completed (loss: 0.3471771478652954, acc: 0.949999988079071)
[2025-01-06 01:11:13,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:13,853][root][INFO] - Training Epoch: 2/10, step 520/574 completed (loss: 0.5741863250732422, acc: 0.8148148059844971)
[2025-01-06 01:11:14,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:14,842][root][INFO] - Training Epoch: 2/10, step 521/574 completed (loss: 0.8659738302230835, acc: 0.7542372941970825)
[2025-01-06 01:11:14,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:15,188][root][INFO] - Training Epoch: 2/10, step 522/574 completed (loss: 0.355327844619751, acc: 0.888059675693512)
[2025-01-06 01:11:15,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:15,571][root][INFO] - Training Epoch: 2/10, step 523/574 completed (loss: 0.4796633720397949, acc: 0.8394160866737366)
[2025-01-06 01:11:15,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:16,123][root][INFO] - Training Epoch: 2/10, step 524/574 completed (loss: 0.8562946319580078, acc: 0.800000011920929)
[2025-01-06 01:11:16,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:16,435][root][INFO] - Training Epoch: 2/10, step 525/574 completed (loss: 0.08796323090791702, acc: 0.9814814925193787)
[2025-01-06 01:11:16,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:16,823][root][INFO] - Training Epoch: 2/10, step 526/574 completed (loss: 0.25693896412849426, acc: 0.9230769276618958)
[2025-01-06 01:11:16,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:17,173][root][INFO] - Training Epoch: 2/10, step 527/574 completed (loss: 0.4258284866809845, acc: 0.8571428656578064)
[2025-01-06 01:11:17,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:17,523][root][INFO] - Training Epoch: 2/10, step 528/574 completed (loss: 1.880699634552002, acc: 0.5737704634666443)
[2025-01-06 01:11:17,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:17,845][root][INFO] - Training Epoch: 2/10, step 529/574 completed (loss: 0.34461790323257446, acc: 0.9322034120559692)
[2025-01-06 01:11:17,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:18,184][root][INFO] - Training Epoch: 2/10, step 530/574 completed (loss: 1.5330902338027954, acc: 0.5581395626068115)
[2025-01-06 01:11:18,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:18,564][root][INFO] - Training Epoch: 2/10, step 531/574 completed (loss: 0.9447256326675415, acc: 0.7727272510528564)
[2025-01-06 01:11:18,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:18,947][root][INFO] - Training Epoch: 2/10, step 532/574 completed (loss: 1.2534576654434204, acc: 0.698113203048706)
[2025-01-06 01:11:19,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:19,326][root][INFO] - Training Epoch: 2/10, step 533/574 completed (loss: 0.9677390456199646, acc: 0.7727272510528564)
[2025-01-06 01:11:19,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:19,663][root][INFO] - Training Epoch: 2/10, step 534/574 completed (loss: 0.7308734059333801, acc: 0.800000011920929)
[2025-01-06 01:11:19,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:19,988][root][INFO] - Training Epoch: 2/10, step 535/574 completed (loss: 0.5254901647567749, acc: 0.8999999761581421)
[2025-01-06 01:11:20,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:20,278][root][INFO] - Training Epoch: 2/10, step 536/574 completed (loss: 0.2941884696483612, acc: 0.9090909361839294)
[2025-01-06 01:11:20,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:20,703][root][INFO] - Training Epoch: 2/10, step 537/574 completed (loss: 0.7827628254890442, acc: 0.7692307829856873)
[2025-01-06 01:11:20,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:21,066][root][INFO] - Training Epoch: 2/10, step 538/574 completed (loss: 0.5839024782180786, acc: 0.84375)
[2025-01-06 01:11:21,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:21,475][root][INFO] - Training Epoch: 2/10, step 539/574 completed (loss: 0.5395469069480896, acc: 0.875)
[2025-01-06 01:11:21,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:21,804][root][INFO] - Training Epoch: 2/10, step 540/574 completed (loss: 0.7892234921455383, acc: 0.7878788113594055)
[2025-01-06 01:11:21,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:22,157][root][INFO] - Training Epoch: 2/10, step 541/574 completed (loss: 0.2698283791542053, acc: 0.875)
[2025-01-06 01:11:22,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:22,519][root][INFO] - Training Epoch: 2/10, step 542/574 completed (loss: 0.07680760324001312, acc: 1.0)
[2025-01-06 01:11:22,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:22,897][root][INFO] - Training Epoch: 2/10, step 543/574 completed (loss: 0.09416507929563522, acc: 0.95652174949646)
[2025-01-06 01:11:23,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:23,288][root][INFO] - Training Epoch: 2/10, step 544/574 completed (loss: 0.15187953412532806, acc: 0.9666666388511658)
[2025-01-06 01:11:23,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:23,635][root][INFO] - Training Epoch: 2/10, step 545/574 completed (loss: 0.145022451877594, acc: 0.9512194991111755)
[2025-01-06 01:11:23,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:23,965][root][INFO] - Training Epoch: 2/10, step 546/574 completed (loss: 0.031694330275058746, acc: 1.0)
[2025-01-06 01:11:24,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:24,286][root][INFO] - Training Epoch: 2/10, step 547/574 completed (loss: 0.026211680844426155, acc: 1.0)
[2025-01-06 01:11:24,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:24,663][root][INFO] - Training Epoch: 2/10, step 548/574 completed (loss: 0.3324419856071472, acc: 0.9032257795333862)
[2025-01-06 01:11:24,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:25,012][root][INFO] - Training Epoch: 2/10, step 549/574 completed (loss: 0.01596648432314396, acc: 1.0)
[2025-01-06 01:11:25,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:25,398][root][INFO] - Training Epoch: 2/10, step 550/574 completed (loss: 0.34736567735671997, acc: 0.9090909361839294)
[2025-01-06 01:11:25,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:25,774][root][INFO] - Training Epoch: 2/10, step 551/574 completed (loss: 0.14163705706596375, acc: 0.9750000238418579)
[2025-01-06 01:11:25,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:26,138][root][INFO] - Training Epoch: 2/10, step 552/574 completed (loss: 0.19319598376750946, acc: 0.8999999761581421)
[2025-01-06 01:11:26,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:26,490][root][INFO] - Training Epoch: 2/10, step 553/574 completed (loss: 0.546653687953949, acc: 0.8540145754814148)
[2025-01-06 01:11:26,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:26,824][root][INFO] - Training Epoch: 2/10, step 554/574 completed (loss: 0.24139487743377686, acc: 0.9241379499435425)
[2025-01-06 01:11:26,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:27,214][root][INFO] - Training Epoch: 2/10, step 555/574 completed (loss: 0.4271969199180603, acc: 0.8714285492897034)
[2025-01-06 01:11:27,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:27,551][root][INFO] - Training Epoch: 2/10, step 556/574 completed (loss: 0.4633369743824005, acc: 0.887417197227478)
[2025-01-06 01:11:27,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:27,866][root][INFO] - Training Epoch: 2/10, step 557/574 completed (loss: 0.24788255989551544, acc: 0.9059829115867615)
[2025-01-06 01:11:27,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:28,231][root][INFO] - Training Epoch: 2/10, step 558/574 completed (loss: 0.142043337225914, acc: 0.9200000166893005)
[2025-01-06 01:11:28,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:28,572][root][INFO] - Training Epoch: 2/10, step 559/574 completed (loss: 0.44133901596069336, acc: 0.9230769276618958)
[2025-01-06 01:11:28,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:28,924][root][INFO] - Training Epoch: 2/10, step 560/574 completed (loss: 0.019989589229226112, acc: 1.0)
[2025-01-06 01:11:29,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:29,269][root][INFO] - Training Epoch: 2/10, step 561/574 completed (loss: 0.05176506191492081, acc: 1.0)
[2025-01-06 01:11:29,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:29,598][root][INFO] - Training Epoch: 2/10, step 562/574 completed (loss: 0.5967127680778503, acc: 0.855555534362793)
[2025-01-06 01:11:29,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:29,920][root][INFO] - Training Epoch: 2/10, step 563/574 completed (loss: 0.47364896535873413, acc: 0.8701298832893372)
[2025-01-06 01:11:30,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:30,259][root][INFO] - Training Epoch: 2/10, step 564/574 completed (loss: 0.2598082423210144, acc: 0.8958333134651184)
[2025-01-06 01:11:30,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:30,589][root][INFO] - Training Epoch: 2/10, step 565/574 completed (loss: 0.2278451770544052, acc: 0.8965517282485962)
[2025-01-06 01:11:30,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:30,926][root][INFO] - Training Epoch: 2/10, step 566/574 completed (loss: 0.3654184937477112, acc: 0.9166666865348816)
[2025-01-06 01:11:31,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:31,274][root][INFO] - Training Epoch: 2/10, step 567/574 completed (loss: 0.05148665979504585, acc: 1.0)
[2025-01-06 01:11:31,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:31,639][root][INFO] - Training Epoch: 2/10, step 568/574 completed (loss: 0.07089527696371078, acc: 0.9629629850387573)
[2025-01-06 01:11:31,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:32,035][root][INFO] - Training Epoch: 2/10, step 569/574 completed (loss: 0.21853910386562347, acc: 0.9304812550544739)
[2025-01-06 01:11:32,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:33,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:33,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:33,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:34,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:34,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:34,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:35,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:35,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:35,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:36,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:36,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:36,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:37,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:37,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:39,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:39,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:39,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:40,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:40,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:41,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:41,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:41,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:42,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:42,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:42,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:44,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:44,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:45,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:45,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:45,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:46,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:46,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:46,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:47,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:47,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:47,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:48,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:48,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:49,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:49,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:49,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:49,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:50,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:50,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:50,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:51,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:51,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:51,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:52,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:52,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:52,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:53,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:53,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:54,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:54,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:54,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:55,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:55,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:55,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:56,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:56,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:57,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:57,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:57,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:58,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:58,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:58,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:59,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:59,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:59,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:00,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:00,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:00,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:01,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:01,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:01,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:02,532][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9307, device='cuda:0') eval_epoch_loss=tensor(0.6579, device='cuda:0') eval_epoch_acc=tensor(0.8247, device='cuda:0')
[2025-01-06 01:12:02,534][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:12:02,534][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:12:02,753][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_570_loss_0.657892107963562/model.pt
[2025-01-06 01:12:02,757][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:12:02,757][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8246538043022156
[2025-01-06 01:12:02,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:03,135][root][INFO] - Training Epoch: 2/10, step 570/574 completed (loss: 0.058918263763189316, acc: 0.9677419066429138)
[2025-01-06 01:12:03,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:03,539][root][INFO] - Training Epoch: 2/10, step 571/574 completed (loss: 0.35964691638946533, acc: 0.9230769276618958)
[2025-01-06 01:12:03,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:03,868][root][INFO] - Training Epoch: 2/10, step 572/574 completed (loss: 0.42932558059692383, acc: 0.8673469424247742)
[2025-01-06 01:12:03,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:04,212][root][INFO] - Training Epoch: 2/10, step 573/574 completed (loss: 0.36012065410614014, acc: 0.8805031180381775)
[2025-01-06 01:12:04,623][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.9230, train_epoch_loss=0.6539, epoch time 356.3926611430943s
[2025-01-06 01:12:04,623][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-06 01:12:04,623][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:12:04,623][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-06 01:12:04,623][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2025-01-06 01:12:04,623][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:12:05,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:05,500][root][INFO] - Training Epoch: 3/10, step 0/574 completed (loss: 0.2969715893268585, acc: 0.8888888955116272)
[2025-01-06 01:12:05,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:05,861][root][INFO] - Training Epoch: 3/10, step 1/574 completed (loss: 0.46744704246520996, acc: 0.8799999952316284)
[2025-01-06 01:12:05,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:06,229][root][INFO] - Training Epoch: 3/10, step 2/574 completed (loss: 0.8835070729255676, acc: 0.7567567825317383)
[2025-01-06 01:12:06,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:06,615][root][INFO] - Training Epoch: 3/10, step 3/574 completed (loss: 0.48732316493988037, acc: 0.8947368264198303)
[2025-01-06 01:12:06,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:06,945][root][INFO] - Training Epoch: 3/10, step 4/574 completed (loss: 0.6756794452667236, acc: 0.7837837934494019)
[2025-01-06 01:12:07,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:07,290][root][INFO] - Training Epoch: 3/10, step 5/574 completed (loss: 0.27157095074653625, acc: 0.8928571343421936)
[2025-01-06 01:12:07,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:07,674][root][INFO] - Training Epoch: 3/10, step 6/574 completed (loss: 1.055375099182129, acc: 0.6938775777816772)
[2025-01-06 01:12:07,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:08,077][root][INFO] - Training Epoch: 3/10, step 7/574 completed (loss: 0.7153661847114563, acc: 0.8666666746139526)
[2025-01-06 01:12:08,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:08,487][root][INFO] - Training Epoch: 3/10, step 8/574 completed (loss: 0.15766330063343048, acc: 0.9545454382896423)
[2025-01-06 01:12:08,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:08,885][root][INFO] - Training Epoch: 3/10, step 9/574 completed (loss: 0.06890681385993958, acc: 0.9615384340286255)
[2025-01-06 01:12:09,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:09,245][root][INFO] - Training Epoch: 3/10, step 10/574 completed (loss: 0.3150053024291992, acc: 0.9259259104728699)
[2025-01-06 01:12:09,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:09,558][root][INFO] - Training Epoch: 3/10, step 11/574 completed (loss: 0.33812853693962097, acc: 0.8974359035491943)
[2025-01-06 01:12:09,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:09,872][root][INFO] - Training Epoch: 3/10, step 12/574 completed (loss: 0.05262838676571846, acc: 1.0)
[2025-01-06 01:12:09,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:10,204][root][INFO] - Training Epoch: 3/10, step 13/574 completed (loss: 0.18848365545272827, acc: 0.95652174949646)
[2025-01-06 01:12:10,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:10,590][root][INFO] - Training Epoch: 3/10, step 14/574 completed (loss: 0.20208357274532318, acc: 0.9803921580314636)
[2025-01-06 01:12:10,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:10,982][root][INFO] - Training Epoch: 3/10, step 15/574 completed (loss: 0.5153516530990601, acc: 0.8979591727256775)
[2025-01-06 01:12:11,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:11,358][root][INFO] - Training Epoch: 3/10, step 16/574 completed (loss: 0.16581639647483826, acc: 0.9473684430122375)
[2025-01-06 01:12:11,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:11,695][root][INFO] - Training Epoch: 3/10, step 17/574 completed (loss: 0.4369361102581024, acc: 0.875)
[2025-01-06 01:12:11,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:12,129][root][INFO] - Training Epoch: 3/10, step 18/574 completed (loss: 0.725810170173645, acc: 0.7777777910232544)
[2025-01-06 01:12:12,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:12,513][root][INFO] - Training Epoch: 3/10, step 19/574 completed (loss: 0.30993136763572693, acc: 0.8421052694320679)
[2025-01-06 01:12:12,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:12,834][root][INFO] - Training Epoch: 3/10, step 20/574 completed (loss: 0.3086751699447632, acc: 0.9615384340286255)
[2025-01-06 01:12:12,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:13,169][root][INFO] - Training Epoch: 3/10, step 21/574 completed (loss: 0.7116150856018066, acc: 0.8620689511299133)
[2025-01-06 01:12:13,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:13,515][root][INFO] - Training Epoch: 3/10, step 22/574 completed (loss: 0.7018419504165649, acc: 0.8399999737739563)
[2025-01-06 01:12:13,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:13,851][root][INFO] - Training Epoch: 3/10, step 23/574 completed (loss: 0.7727564573287964, acc: 0.8571428656578064)
[2025-01-06 01:12:13,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:14,242][root][INFO] - Training Epoch: 3/10, step 24/574 completed (loss: 0.5306544899940491, acc: 0.875)
[2025-01-06 01:12:14,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:14,590][root][INFO] - Training Epoch: 3/10, step 25/574 completed (loss: 0.7066053748130798, acc: 0.8301886916160583)
[2025-01-06 01:12:14,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:14,927][root][INFO] - Training Epoch: 3/10, step 26/574 completed (loss: 0.8184294700622559, acc: 0.767123281955719)
[2025-01-06 01:12:15,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:16,145][root][INFO] - Training Epoch: 3/10, step 27/574 completed (loss: 0.9461022615432739, acc: 0.747035562992096)
[2025-01-06 01:12:16,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:16,471][root][INFO] - Training Epoch: 3/10, step 28/574 completed (loss: 0.4574422538280487, acc: 0.8139534592628479)
[2025-01-06 01:12:16,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:16,865][root][INFO] - Training Epoch: 3/10, step 29/574 completed (loss: 0.6188654899597168, acc: 0.759036123752594)
[2025-01-06 01:12:16,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:17,251][root][INFO] - Training Epoch: 3/10, step 30/574 completed (loss: 0.648086667060852, acc: 0.8148148059844971)
[2025-01-06 01:12:17,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:17,587][root][INFO] - Training Epoch: 3/10, step 31/574 completed (loss: 0.6971855759620667, acc: 0.7857142686843872)
[2025-01-06 01:12:17,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:17,957][root][INFO] - Training Epoch: 3/10, step 32/574 completed (loss: 0.3343809247016907, acc: 0.8888888955116272)
[2025-01-06 01:12:18,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:18,345][root][INFO] - Training Epoch: 3/10, step 33/574 completed (loss: 0.07848891615867615, acc: 1.0)
[2025-01-06 01:12:18,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:18,746][root][INFO] - Training Epoch: 3/10, step 34/574 completed (loss: 0.4710620641708374, acc: 0.8571428656578064)
[2025-01-06 01:12:18,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:19,108][root][INFO] - Training Epoch: 3/10, step 35/574 completed (loss: 0.3867657482624054, acc: 0.9016393423080444)
[2025-01-06 01:12:19,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:19,524][root][INFO] - Training Epoch: 3/10, step 36/574 completed (loss: 0.5857899188995361, acc: 0.8571428656578064)
[2025-01-06 01:12:19,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:19,904][root][INFO] - Training Epoch: 3/10, step 37/574 completed (loss: 0.659912109375, acc: 0.8644067645072937)
[2025-01-06 01:12:20,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:20,261][root][INFO] - Training Epoch: 3/10, step 38/574 completed (loss: 0.3222159147262573, acc: 0.8965517282485962)
[2025-01-06 01:12:20,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:20,645][root][INFO] - Training Epoch: 3/10, step 39/574 completed (loss: 0.23712237179279327, acc: 0.9523809552192688)
[2025-01-06 01:12:20,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:21,021][root][INFO] - Training Epoch: 3/10, step 40/574 completed (loss: 0.44764238595962524, acc: 0.8846153616905212)
[2025-01-06 01:12:21,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:21,439][root][INFO] - Training Epoch: 3/10, step 41/574 completed (loss: 0.2723695635795593, acc: 0.9189189076423645)
[2025-01-06 01:12:21,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:21,842][root][INFO] - Training Epoch: 3/10, step 42/574 completed (loss: 0.5640169978141785, acc: 0.8153846263885498)
[2025-01-06 01:12:21,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:22,272][root][INFO] - Training Epoch: 3/10, step 43/574 completed (loss: 0.5983984470367432, acc: 0.808080792427063)
[2025-01-06 01:12:22,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:22,703][root][INFO] - Training Epoch: 3/10, step 44/574 completed (loss: 0.3955545723438263, acc: 0.8865979313850403)
[2025-01-06 01:12:22,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:23,129][root][INFO] - Training Epoch: 3/10, step 45/574 completed (loss: 0.4528030455112457, acc: 0.875)
[2025-01-06 01:12:23,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:23,506][root][INFO] - Training Epoch: 3/10, step 46/574 completed (loss: 0.677653431892395, acc: 0.807692289352417)
[2025-01-06 01:12:23,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:23,846][root][INFO] - Training Epoch: 3/10, step 47/574 completed (loss: 0.25315889716148376, acc: 0.9629629850387573)
[2025-01-06 01:12:23,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:24,183][root][INFO] - Training Epoch: 3/10, step 48/574 completed (loss: 0.2792011797428131, acc: 0.9642857313156128)
[2025-01-06 01:12:24,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:24,541][root][INFO] - Training Epoch: 3/10, step 49/574 completed (loss: 0.07347944378852844, acc: 1.0)
[2025-01-06 01:12:24,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:24,886][root][INFO] - Training Epoch: 3/10, step 50/574 completed (loss: 0.7061207890510559, acc: 0.7894737124443054)
[2025-01-06 01:12:24,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:25,253][root][INFO] - Training Epoch: 3/10, step 51/574 completed (loss: 0.5107907652854919, acc: 0.8571428656578064)
[2025-01-06 01:12:25,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:25,595][root][INFO] - Training Epoch: 3/10, step 52/574 completed (loss: 0.912458062171936, acc: 0.7746478915214539)
[2025-01-06 01:12:25,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:26,048][root][INFO] - Training Epoch: 3/10, step 53/574 completed (loss: 1.4076746702194214, acc: 0.5933333039283752)
[2025-01-06 01:12:26,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:26,355][root][INFO] - Training Epoch: 3/10, step 54/574 completed (loss: 0.81767737865448, acc: 0.7837837934494019)
[2025-01-06 01:12:26,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:26,741][root][INFO] - Training Epoch: 3/10, step 55/574 completed (loss: 0.10880298167467117, acc: 0.9615384340286255)
[2025-01-06 01:12:28,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:29,715][root][INFO] - Training Epoch: 3/10, step 56/574 completed (loss: 1.021549105644226, acc: 0.7098976373672485)
[2025-01-06 01:12:30,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:30,879][root][INFO] - Training Epoch: 3/10, step 57/574 completed (loss: 1.1707760095596313, acc: 0.673202633857727)
[2025-01-06 01:12:31,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:31,498][root][INFO] - Training Epoch: 3/10, step 58/574 completed (loss: 0.8751172423362732, acc: 0.7386363744735718)
[2025-01-06 01:12:31,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:32,063][root][INFO] - Training Epoch: 3/10, step 59/574 completed (loss: 0.40065714716911316, acc: 0.8897058963775635)
[2025-01-06 01:12:32,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:32,624][root][INFO] - Training Epoch: 3/10, step 60/574 completed (loss: 0.9432113170623779, acc: 0.7028985619544983)
[2025-01-06 01:12:32,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:33,035][root][INFO] - Training Epoch: 3/10, step 61/574 completed (loss: 0.786537230014801, acc: 0.75)
[2025-01-06 01:12:33,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:33,392][root][INFO] - Training Epoch: 3/10, step 62/574 completed (loss: 0.4225085973739624, acc: 0.9117646813392639)
[2025-01-06 01:12:33,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:33,808][root][INFO] - Training Epoch: 3/10, step 63/574 completed (loss: 0.4055541455745697, acc: 0.8055555820465088)
[2025-01-06 01:12:33,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:34,227][root][INFO] - Training Epoch: 3/10, step 64/574 completed (loss: 0.2126307338476181, acc: 0.9375)
[2025-01-06 01:12:34,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:34,582][root][INFO] - Training Epoch: 3/10, step 65/574 completed (loss: 0.10122071951627731, acc: 1.0)
[2025-01-06 01:12:34,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:34,913][root][INFO] - Training Epoch: 3/10, step 66/574 completed (loss: 0.7945946455001831, acc: 0.7678571343421936)
[2025-01-06 01:12:35,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:35,287][root][INFO] - Training Epoch: 3/10, step 67/574 completed (loss: 0.4720187187194824, acc: 0.8333333134651184)
[2025-01-06 01:12:35,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:35,648][root][INFO] - Training Epoch: 3/10, step 68/574 completed (loss: 0.022817080840468407, acc: 1.0)
[2025-01-06 01:12:35,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:35,959][root][INFO] - Training Epoch: 3/10, step 69/574 completed (loss: 0.6695833206176758, acc: 0.7777777910232544)
[2025-01-06 01:12:36,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:36,372][root][INFO] - Training Epoch: 3/10, step 70/574 completed (loss: 0.7335181832313538, acc: 0.7878788113594055)
[2025-01-06 01:12:36,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:36,762][root][INFO] - Training Epoch: 3/10, step 71/574 completed (loss: 1.051248550415039, acc: 0.6985294222831726)
[2025-01-06 01:12:36,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:37,168][root][INFO] - Training Epoch: 3/10, step 72/574 completed (loss: 0.7595371603965759, acc: 0.7857142686843872)
[2025-01-06 01:12:37,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:37,524][root][INFO] - Training Epoch: 3/10, step 73/574 completed (loss: 1.3735201358795166, acc: 0.6307692527770996)
[2025-01-06 01:12:37,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:37,859][root][INFO] - Training Epoch: 3/10, step 74/574 completed (loss: 1.1263517141342163, acc: 0.7142857313156128)
[2025-01-06 01:12:37,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:38,245][root][INFO] - Training Epoch: 3/10, step 75/574 completed (loss: 1.1627302169799805, acc: 0.7164179086685181)
[2025-01-06 01:12:38,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:38,645][root][INFO] - Training Epoch: 3/10, step 76/574 completed (loss: 1.440047264099121, acc: 0.6277372241020203)
[2025-01-06 01:12:38,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:39,018][root][INFO] - Training Epoch: 3/10, step 77/574 completed (loss: 0.03694751858711243, acc: 1.0)
[2025-01-06 01:12:39,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:39,362][root][INFO] - Training Epoch: 3/10, step 78/574 completed (loss: 0.17323656380176544, acc: 0.9583333134651184)
[2025-01-06 01:12:39,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:39,735][root][INFO] - Training Epoch: 3/10, step 79/574 completed (loss: 0.13632000982761383, acc: 0.9696969985961914)
[2025-01-06 01:12:39,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:40,060][root][INFO] - Training Epoch: 3/10, step 80/574 completed (loss: 0.3069800138473511, acc: 0.9230769276618958)
[2025-01-06 01:12:40,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:40,419][root][INFO] - Training Epoch: 3/10, step 81/574 completed (loss: 0.5213814377784729, acc: 0.807692289352417)
[2025-01-06 01:12:40,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:40,846][root][INFO] - Training Epoch: 3/10, step 82/574 completed (loss: 0.6798635721206665, acc: 0.807692289352417)
[2025-01-06 01:12:40,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:41,260][root][INFO] - Training Epoch: 3/10, step 83/574 completed (loss: 0.35583215951919556, acc: 0.90625)
[2025-01-06 01:12:41,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:41,647][root][INFO] - Training Epoch: 3/10, step 84/574 completed (loss: 0.37025511264801025, acc: 0.8985507488250732)
[2025-01-06 01:12:41,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:41,967][root][INFO] - Training Epoch: 3/10, step 85/574 completed (loss: 0.4180171489715576, acc: 0.8600000143051147)
[2025-01-06 01:12:42,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:42,348][root][INFO] - Training Epoch: 3/10, step 86/574 completed (loss: 0.3969917297363281, acc: 0.8695651888847351)
[2025-01-06 01:12:42,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:42,834][root][INFO] - Training Epoch: 3/10, step 87/574 completed (loss: 0.8406510353088379, acc: 0.7599999904632568)
[2025-01-06 01:12:42,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:43,205][root][INFO] - Training Epoch: 3/10, step 88/574 completed (loss: 0.818023681640625, acc: 0.8155339956283569)
[2025-01-06 01:12:43,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:44,286][root][INFO] - Training Epoch: 3/10, step 89/574 completed (loss: 0.9349761605262756, acc: 0.7669903039932251)
[2025-01-06 01:12:44,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:45,108][root][INFO] - Training Epoch: 3/10, step 90/574 completed (loss: 1.0387440919876099, acc: 0.7311828136444092)
[2025-01-06 01:12:45,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:45,916][root][INFO] - Training Epoch: 3/10, step 91/574 completed (loss: 0.9956043362617493, acc: 0.7284482717514038)
[2025-01-06 01:12:46,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:46,656][root][INFO] - Training Epoch: 3/10, step 92/574 completed (loss: 0.6736940145492554, acc: 0.821052610874176)
[2025-01-06 01:12:46,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:47,639][root][INFO] - Training Epoch: 3/10, step 93/574 completed (loss: 1.2249929904937744, acc: 0.603960394859314)
[2025-01-06 01:12:47,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:47,933][root][INFO] - Training Epoch: 3/10, step 94/574 completed (loss: 1.118378758430481, acc: 0.6129032373428345)
[2025-01-06 01:12:48,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:48,276][root][INFO] - Training Epoch: 3/10, step 95/574 completed (loss: 0.8677001595497131, acc: 0.7101449370384216)
[2025-01-06 01:12:48,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:48,690][root][INFO] - Training Epoch: 3/10, step 96/574 completed (loss: 1.019187569618225, acc: 0.6386554837226868)
[2025-01-06 01:12:48,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:49,100][root][INFO] - Training Epoch: 3/10, step 97/574 completed (loss: 1.1715366840362549, acc: 0.7019230723381042)
[2025-01-06 01:12:49,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:49,482][root][INFO] - Training Epoch: 3/10, step 98/574 completed (loss: 1.2410436868667603, acc: 0.6350364685058594)
[2025-01-06 01:12:49,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:49,806][root][INFO] - Training Epoch: 3/10, step 99/574 completed (loss: 1.31521737575531, acc: 0.5820895433425903)
[2025-01-06 01:12:49,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:50,107][root][INFO] - Training Epoch: 3/10, step 100/574 completed (loss: 0.41302689909935, acc: 0.8999999761581421)
[2025-01-06 01:12:50,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:50,402][root][INFO] - Training Epoch: 3/10, step 101/574 completed (loss: 0.014900913462042809, acc: 1.0)
[2025-01-06 01:12:50,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:50,701][root][INFO] - Training Epoch: 3/10, step 102/574 completed (loss: 0.048842333257198334, acc: 1.0)
[2025-01-06 01:12:50,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:51,040][root][INFO] - Training Epoch: 3/10, step 103/574 completed (loss: 0.03846347704529762, acc: 1.0)
[2025-01-06 01:12:51,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:51,390][root][INFO] - Training Epoch: 3/10, step 104/574 completed (loss: 0.4750395119190216, acc: 0.8793103694915771)
[2025-01-06 01:12:51,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:51,762][root][INFO] - Training Epoch: 3/10, step 105/574 completed (loss: 0.20411905646324158, acc: 0.9534883499145508)
[2025-01-06 01:12:51,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:52,128][root][INFO] - Training Epoch: 3/10, step 106/574 completed (loss: 0.3474701941013336, acc: 0.8799999952316284)
[2025-01-06 01:12:52,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:52,478][root][INFO] - Training Epoch: 3/10, step 107/574 completed (loss: 0.015886373817920685, acc: 1.0)
[2025-01-06 01:12:52,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:52,878][root][INFO] - Training Epoch: 3/10, step 108/574 completed (loss: 0.027669651433825493, acc: 1.0)
[2025-01-06 01:12:52,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:53,225][root][INFO] - Training Epoch: 3/10, step 109/574 completed (loss: 0.04575137421488762, acc: 0.976190447807312)
[2025-01-06 01:12:53,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:53,617][root][INFO] - Training Epoch: 3/10, step 110/574 completed (loss: 0.149763822555542, acc: 0.9384615421295166)
[2025-01-06 01:12:53,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:54,054][root][INFO] - Training Epoch: 3/10, step 111/574 completed (loss: 0.4639698565006256, acc: 0.8771929740905762)
[2025-01-06 01:12:54,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:54,447][root][INFO] - Training Epoch: 3/10, step 112/574 completed (loss: 0.7883179783821106, acc: 0.7543859481811523)
[2025-01-06 01:12:54,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:54,843][root][INFO] - Training Epoch: 3/10, step 113/574 completed (loss: 0.5692649483680725, acc: 0.9230769276618958)
[2025-01-06 01:12:54,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:55,206][root][INFO] - Training Epoch: 3/10, step 114/574 completed (loss: 0.3139324486255646, acc: 0.8979591727256775)
[2025-01-06 01:12:55,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:55,513][root][INFO] - Training Epoch: 3/10, step 115/574 completed (loss: 0.03258367255330086, acc: 1.0)
[2025-01-06 01:12:55,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:55,840][root][INFO] - Training Epoch: 3/10, step 116/574 completed (loss: 0.4621047377586365, acc: 0.8730158805847168)
[2025-01-06 01:12:55,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:56,218][root][INFO] - Training Epoch: 3/10, step 117/574 completed (loss: 0.5142388343811035, acc: 0.8617886304855347)
[2025-01-06 01:12:56,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:56,575][root][INFO] - Training Epoch: 3/10, step 118/574 completed (loss: 0.2767086625099182, acc: 0.9354838728904724)
[2025-01-06 01:12:56,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:57,439][root][INFO] - Training Epoch: 3/10, step 119/574 completed (loss: 0.6306795477867126, acc: 0.8212927579879761)
[2025-01-06 01:12:57,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:57,806][root][INFO] - Training Epoch: 3/10, step 120/574 completed (loss: 0.2916421890258789, acc: 0.9200000166893005)
[2025-01-06 01:12:57,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:58,208][root][INFO] - Training Epoch: 3/10, step 121/574 completed (loss: 0.41610047221183777, acc: 0.942307710647583)
[2025-01-06 01:12:58,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:58,523][root][INFO] - Training Epoch: 3/10, step 122/574 completed (loss: 0.21084065735340118, acc: 0.9166666865348816)
[2025-01-06 01:12:58,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:58,892][root][INFO] - Training Epoch: 3/10, step 123/574 completed (loss: 0.19386200606822968, acc: 0.9473684430122375)
[2025-01-06 01:12:59,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:59,303][root][INFO] - Training Epoch: 3/10, step 124/574 completed (loss: 1.0558736324310303, acc: 0.7055214643478394)
[2025-01-06 01:12:59,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:59,755][root][INFO] - Training Epoch: 3/10, step 125/574 completed (loss: 1.0804702043533325, acc: 0.7152777910232544)
[2025-01-06 01:12:59,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:00,187][root][INFO] - Training Epoch: 3/10, step 126/574 completed (loss: 1.1419291496276855, acc: 0.699999988079071)
[2025-01-06 01:13:00,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:00,551][root][INFO] - Training Epoch: 3/10, step 127/574 completed (loss: 0.6826201677322388, acc: 0.7976190447807312)
[2025-01-06 01:13:00,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:00,952][root][INFO] - Training Epoch: 3/10, step 128/574 completed (loss: 0.7889321446418762, acc: 0.7794871926307678)
[2025-01-06 01:13:01,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:01,405][root][INFO] - Training Epoch: 3/10, step 129/574 completed (loss: 0.9723509550094604, acc: 0.7426470518112183)
[2025-01-06 01:13:01,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:01,775][root][INFO] - Training Epoch: 3/10, step 130/574 completed (loss: 0.5785620212554932, acc: 0.807692289352417)
[2025-01-06 01:13:01,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:02,161][root][INFO] - Training Epoch: 3/10, step 131/574 completed (loss: 0.30657002329826355, acc: 0.8695651888847351)
[2025-01-06 01:13:02,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:02,556][root][INFO] - Training Epoch: 3/10, step 132/574 completed (loss: 0.404623806476593, acc: 0.875)
[2025-01-06 01:13:02,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:02,942][root][INFO] - Training Epoch: 3/10, step 133/574 completed (loss: 0.6955602765083313, acc: 0.8260869383811951)
[2025-01-06 01:13:03,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:03,297][root][INFO] - Training Epoch: 3/10, step 134/574 completed (loss: 0.3498804271221161, acc: 0.8857142925262451)
[2025-01-06 01:13:03,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:03,666][root][INFO] - Training Epoch: 3/10, step 135/574 completed (loss: 0.40919816493988037, acc: 0.8846153616905212)
[2025-01-06 01:13:03,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:04,040][root][INFO] - Training Epoch: 3/10, step 136/574 completed (loss: 0.6360428929328918, acc: 0.8571428656578064)
[2025-01-06 01:13:04,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:04,421][root][INFO] - Training Epoch: 3/10, step 137/574 completed (loss: 0.7994871139526367, acc: 0.7666666507720947)
[2025-01-06 01:13:04,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:04,744][root][INFO] - Training Epoch: 3/10, step 138/574 completed (loss: 0.5126736164093018, acc: 0.8260869383811951)
[2025-01-06 01:13:05,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:05,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:07,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:07,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:07,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:08,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:08,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:08,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:09,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:09,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:10,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:10,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:10,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:11,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:11,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:11,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:12,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:12,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:12,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:13,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:13,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:13,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:14,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:14,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:15,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:15,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:15,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:17,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:17,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:18,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:18,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:18,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:19,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:19,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:19,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:20,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:20,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:20,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:21,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:21,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:21,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:23,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:23,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:24,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:24,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:24,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:25,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:25,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:25,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:26,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:26,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:28,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:28,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:28,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:29,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:29,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:30,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:30,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:30,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:30,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:31,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:31,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:32,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:32,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:32,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:33,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:33,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:33,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:34,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:34,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:34,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:35,267][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8659, device='cuda:0') eval_epoch_loss=tensor(0.6238, device='cuda:0') eval_epoch_acc=tensor(0.8318, device='cuda:0')
[2025-01-06 01:13:35,269][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:13:35,269][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:13:35,492][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_139_loss_0.6237507462501526/model.pt
[2025-01-06 01:13:35,496][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:13:35,496][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 0.6237507462501526
[2025-01-06 01:13:35,497][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.8318161964416504
[2025-01-06 01:13:35,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:35,876][root][INFO] - Training Epoch: 3/10, step 139/574 completed (loss: 0.06416343152523041, acc: 1.0)
[2025-01-06 01:13:35,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:36,229][root][INFO] - Training Epoch: 3/10, step 140/574 completed (loss: 0.16638115048408508, acc: 0.9615384340286255)
[2025-01-06 01:13:36,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:36,622][root][INFO] - Training Epoch: 3/10, step 141/574 completed (loss: 0.5335115194320679, acc: 0.8387096524238586)
[2025-01-06 01:13:36,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:37,004][root][INFO] - Training Epoch: 3/10, step 142/574 completed (loss: 0.8274957537651062, acc: 0.7837837934494019)
[2025-01-06 01:13:37,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:37,536][root][INFO] - Training Epoch: 3/10, step 143/574 completed (loss: 0.8206576108932495, acc: 0.7280701994895935)
[2025-01-06 01:13:37,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:37,963][root][INFO] - Training Epoch: 3/10, step 144/574 completed (loss: 0.8491632342338562, acc: 0.7313432693481445)
[2025-01-06 01:13:38,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:38,346][root][INFO] - Training Epoch: 3/10, step 145/574 completed (loss: 0.6551363468170166, acc: 0.7653061151504517)
[2025-01-06 01:13:38,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:38,790][root][INFO] - Training Epoch: 3/10, step 146/574 completed (loss: 1.0641086101531982, acc: 0.6170212626457214)
[2025-01-06 01:13:38,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:39,177][root][INFO] - Training Epoch: 3/10, step 147/574 completed (loss: 0.7201356887817383, acc: 0.7571428418159485)
[2025-01-06 01:13:39,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:39,553][root][INFO] - Training Epoch: 3/10, step 148/574 completed (loss: 0.6557187438011169, acc: 0.6785714030265808)
[2025-01-06 01:13:39,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:39,888][root][INFO] - Training Epoch: 3/10, step 149/574 completed (loss: 1.0667182207107544, acc: 0.739130437374115)
[2025-01-06 01:13:39,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:40,198][root][INFO] - Training Epoch: 3/10, step 150/574 completed (loss: 0.43496695160865784, acc: 0.8620689511299133)
[2025-01-06 01:13:40,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:40,488][root][INFO] - Training Epoch: 3/10, step 151/574 completed (loss: 0.8687807321548462, acc: 0.782608687877655)
[2025-01-06 01:13:40,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:40,860][root][INFO] - Training Epoch: 3/10, step 152/574 completed (loss: 0.6702168583869934, acc: 0.8305084705352783)
[2025-01-06 01:13:40,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:41,251][root][INFO] - Training Epoch: 3/10, step 153/574 completed (loss: 0.6868039965629578, acc: 0.8070175647735596)
[2025-01-06 01:13:41,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:41,616][root][INFO] - Training Epoch: 3/10, step 154/574 completed (loss: 0.8243822455406189, acc: 0.7702702879905701)
[2025-01-06 01:13:41,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:41,969][root][INFO] - Training Epoch: 3/10, step 155/574 completed (loss: 0.17373108863830566, acc: 0.8928571343421936)
[2025-01-06 01:13:42,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:42,280][root][INFO] - Training Epoch: 3/10, step 156/574 completed (loss: 0.24244916439056396, acc: 0.9130434989929199)
[2025-01-06 01:13:42,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:42,661][root][INFO] - Training Epoch: 3/10, step 157/574 completed (loss: 1.5861811637878418, acc: 0.5263158082962036)
[2025-01-06 01:13:43,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:44,291][root][INFO] - Training Epoch: 3/10, step 158/574 completed (loss: 0.8428838849067688, acc: 0.7297297120094299)
[2025-01-06 01:13:44,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:44,638][root][INFO] - Training Epoch: 3/10, step 159/574 completed (loss: 1.4368919134140015, acc: 0.5555555820465088)
[2025-01-06 01:13:44,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:45,050][root][INFO] - Training Epoch: 3/10, step 160/574 completed (loss: 1.1934137344360352, acc: 0.6976743936538696)
[2025-01-06 01:13:45,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:45,633][root][INFO] - Training Epoch: 3/10, step 161/574 completed (loss: 1.3912482261657715, acc: 0.6000000238418579)
[2025-01-06 01:13:45,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:46,186][root][INFO] - Training Epoch: 3/10, step 162/574 completed (loss: 1.4005590677261353, acc: 0.6404494643211365)
[2025-01-06 01:13:46,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:46,581][root][INFO] - Training Epoch: 3/10, step 163/574 completed (loss: 0.3488502502441406, acc: 0.9318181872367859)
[2025-01-06 01:13:46,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:46,957][root][INFO] - Training Epoch: 3/10, step 164/574 completed (loss: 0.48737049102783203, acc: 0.9047619104385376)
[2025-01-06 01:13:47,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:47,360][root][INFO] - Training Epoch: 3/10, step 165/574 completed (loss: 0.730234682559967, acc: 0.7241379022598267)
[2025-01-06 01:13:47,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:47,748][root][INFO] - Training Epoch: 3/10, step 166/574 completed (loss: 0.20801587402820587, acc: 0.918367326259613)
[2025-01-06 01:13:47,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:48,130][root][INFO] - Training Epoch: 3/10, step 167/574 completed (loss: 0.26969507336616516, acc: 0.9200000166893005)
[2025-01-06 01:13:48,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:48,520][root][INFO] - Training Epoch: 3/10, step 168/574 completed (loss: 0.4657936096191406, acc: 0.875)
[2025-01-06 01:13:48,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:48,857][root][INFO] - Training Epoch: 3/10, step 169/574 completed (loss: 0.9687838554382324, acc: 0.7156862616539001)
[2025-01-06 01:13:49,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:49,880][root][INFO] - Training Epoch: 3/10, step 170/574 completed (loss: 1.019410490989685, acc: 0.6849315166473389)
[2025-01-06 01:13:49,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:50,220][root][INFO] - Training Epoch: 3/10, step 171/574 completed (loss: 0.07042814046144485, acc: 1.0)
[2025-01-06 01:13:50,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:50,592][root][INFO] - Training Epoch: 3/10, step 172/574 completed (loss: 0.6944243907928467, acc: 0.7407407164573669)
[2025-01-06 01:13:50,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:50,966][root][INFO] - Training Epoch: 3/10, step 173/574 completed (loss: 0.325202614068985, acc: 0.9285714030265808)
[2025-01-06 01:13:51,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:51,503][root][INFO] - Training Epoch: 3/10, step 174/574 completed (loss: 1.0099748373031616, acc: 0.7433628439903259)
[2025-01-06 01:13:51,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:51,829][root][INFO] - Training Epoch: 3/10, step 175/574 completed (loss: 0.6870454549789429, acc: 0.8115941882133484)
[2025-01-06 01:13:51,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:52,256][root][INFO] - Training Epoch: 3/10, step 176/574 completed (loss: 0.5225124955177307, acc: 0.8295454382896423)
[2025-01-06 01:13:52,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:53,163][root][INFO] - Training Epoch: 3/10, step 177/574 completed (loss: 1.1238247156143188, acc: 0.6793892979621887)
[2025-01-06 01:13:53,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:53,829][root][INFO] - Training Epoch: 3/10, step 178/574 completed (loss: 0.9420706033706665, acc: 0.7259259223937988)
[2025-01-06 01:13:53,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:54,187][root][INFO] - Training Epoch: 3/10, step 179/574 completed (loss: 0.5099349021911621, acc: 0.8360655903816223)
[2025-01-06 01:13:54,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:54,547][root][INFO] - Training Epoch: 3/10, step 180/574 completed (loss: 0.06360957771539688, acc: 0.9583333134651184)
[2025-01-06 01:13:54,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:54,919][root][INFO] - Training Epoch: 3/10, step 181/574 completed (loss: 0.05271025002002716, acc: 1.0)
[2025-01-06 01:13:55,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:55,257][root][INFO] - Training Epoch: 3/10, step 182/574 completed (loss: 0.23901258409023285, acc: 0.9285714030265808)
[2025-01-06 01:13:55,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:55,598][root][INFO] - Training Epoch: 3/10, step 183/574 completed (loss: 0.25371047854423523, acc: 0.9268292784690857)
[2025-01-06 01:13:55,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:55,956][root][INFO] - Training Epoch: 3/10, step 184/574 completed (loss: 0.4548562169075012, acc: 0.8942598104476929)
[2025-01-06 01:13:56,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:56,332][root][INFO] - Training Epoch: 3/10, step 185/574 completed (loss: 0.4758659899234772, acc: 0.8760806918144226)
[2025-01-06 01:13:56,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:56,813][root][INFO] - Training Epoch: 3/10, step 186/574 completed (loss: 0.43346577882766724, acc: 0.8687499761581421)
[2025-01-06 01:13:56,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:57,337][root][INFO] - Training Epoch: 3/10, step 187/574 completed (loss: 0.47528621554374695, acc: 0.8742964267730713)
[2025-01-06 01:13:57,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:57,726][root][INFO] - Training Epoch: 3/10, step 188/574 completed (loss: 0.5461955666542053, acc: 0.8612099885940552)
[2025-01-06 01:13:57,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:58,105][root][INFO] - Training Epoch: 3/10, step 189/574 completed (loss: 0.18250232934951782, acc: 0.9200000166893005)
[2025-01-06 01:13:58,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:58,655][root][INFO] - Training Epoch: 3/10, step 190/574 completed (loss: 0.9652294516563416, acc: 0.7325581312179565)
[2025-01-06 01:13:58,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:59,447][root][INFO] - Training Epoch: 3/10, step 191/574 completed (loss: 1.3182814121246338, acc: 0.6507936716079712)
[2025-01-06 01:13:59,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:00,355][root][INFO] - Training Epoch: 3/10, step 192/574 completed (loss: 1.0311870574951172, acc: 0.6439393758773804)
[2025-01-06 01:14:00,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:01,090][root][INFO] - Training Epoch: 3/10, step 193/574 completed (loss: 0.7787827253341675, acc: 0.7529411911964417)
[2025-01-06 01:14:01,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:02,159][root][INFO] - Training Epoch: 3/10, step 194/574 completed (loss: 1.0607200860977173, acc: 0.7222222089767456)
[2025-01-06 01:14:02,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:03,106][root][INFO] - Training Epoch: 3/10, step 195/574 completed (loss: 0.44860169291496277, acc: 0.8225806355476379)
[2025-01-06 01:14:03,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:03,464][root][INFO] - Training Epoch: 3/10, step 196/574 completed (loss: 0.14288724958896637, acc: 0.9642857313156128)
[2025-01-06 01:14:03,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:03,801][root][INFO] - Training Epoch: 3/10, step 197/574 completed (loss: 0.8216242790222168, acc: 0.800000011920929)
[2025-01-06 01:14:03,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:04,194][root][INFO] - Training Epoch: 3/10, step 198/574 completed (loss: 0.945226788520813, acc: 0.720588207244873)
[2025-01-06 01:14:04,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:04,543][root][INFO] - Training Epoch: 3/10, step 199/574 completed (loss: 1.0020174980163574, acc: 0.7647058963775635)
[2025-01-06 01:14:04,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:04,944][root][INFO] - Training Epoch: 3/10, step 200/574 completed (loss: 0.81917804479599, acc: 0.7372881174087524)
[2025-01-06 01:14:05,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:05,287][root][INFO] - Training Epoch: 3/10, step 201/574 completed (loss: 0.876707911491394, acc: 0.7910447716712952)
[2025-01-06 01:14:05,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:05,672][root][INFO] - Training Epoch: 3/10, step 202/574 completed (loss: 0.909838855266571, acc: 0.737864077091217)
[2025-01-06 01:14:05,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:06,037][root][INFO] - Training Epoch: 3/10, step 203/574 completed (loss: 0.6117732524871826, acc: 0.8253968358039856)
[2025-01-06 01:14:06,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:06,415][root][INFO] - Training Epoch: 3/10, step 204/574 completed (loss: 0.15180537104606628, acc: 0.9450549483299255)
[2025-01-06 01:14:06,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:06,830][root][INFO] - Training Epoch: 3/10, step 205/574 completed (loss: 0.367413192987442, acc: 0.9058296084403992)
[2025-01-06 01:14:06,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:07,245][root][INFO] - Training Epoch: 3/10, step 206/574 completed (loss: 0.451217919588089, acc: 0.8661417365074158)
[2025-01-06 01:14:07,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:07,602][root][INFO] - Training Epoch: 3/10, step 207/574 completed (loss: 0.3234395980834961, acc: 0.9137930870056152)
[2025-01-06 01:14:07,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:07,953][root][INFO] - Training Epoch: 3/10, step 208/574 completed (loss: 0.43919065594673157, acc: 0.8768116235733032)
[2025-01-06 01:14:08,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:08,322][root][INFO] - Training Epoch: 3/10, step 209/574 completed (loss: 0.3763013184070587, acc: 0.8949416279792786)
[2025-01-06 01:14:08,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:08,692][root][INFO] - Training Epoch: 3/10, step 210/574 completed (loss: 0.2100914716720581, acc: 0.945652186870575)
[2025-01-06 01:14:08,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:09,012][root][INFO] - Training Epoch: 3/10, step 211/574 completed (loss: 0.1728481650352478, acc: 0.95652174949646)
[2025-01-06 01:14:09,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:09,301][root][INFO] - Training Epoch: 3/10, step 212/574 completed (loss: 0.10350947827100754, acc: 0.9642857313156128)
[2025-01-06 01:14:09,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:09,578][root][INFO] - Training Epoch: 3/10, step 213/574 completed (loss: 0.17529359459877014, acc: 0.957446813583374)
[2025-01-06 01:14:09,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:10,273][root][INFO] - Training Epoch: 3/10, step 214/574 completed (loss: 0.18679893016815186, acc: 0.9384615421295166)
[2025-01-06 01:14:10,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:10,658][root][INFO] - Training Epoch: 3/10, step 215/574 completed (loss: 0.21481485664844513, acc: 0.9459459185600281)
[2025-01-06 01:14:10,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:11,046][root][INFO] - Training Epoch: 3/10, step 216/574 completed (loss: 0.19511905312538147, acc: 0.930232584476471)
[2025-01-06 01:14:11,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:11,580][root][INFO] - Training Epoch: 3/10, step 217/574 completed (loss: 0.39241722226142883, acc: 0.9099099040031433)
[2025-01-06 01:14:11,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:11,989][root][INFO] - Training Epoch: 3/10, step 218/574 completed (loss: 0.1735450029373169, acc: 0.9333333373069763)
[2025-01-06 01:14:12,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:12,367][root][INFO] - Training Epoch: 3/10, step 219/574 completed (loss: 0.16147339344024658, acc: 0.939393937587738)
[2025-01-06 01:14:12,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:12,713][root][INFO] - Training Epoch: 3/10, step 220/574 completed (loss: 0.045502670109272, acc: 1.0)
[2025-01-06 01:14:12,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:13,109][root][INFO] - Training Epoch: 3/10, step 221/574 completed (loss: 0.09455527365207672, acc: 0.9599999785423279)
[2025-01-06 01:14:13,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:13,456][root][INFO] - Training Epoch: 3/10, step 222/574 completed (loss: 0.6807976961135864, acc: 0.8269230723381042)
[2025-01-06 01:14:13,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:14,218][root][INFO] - Training Epoch: 3/10, step 223/574 completed (loss: 0.3999723494052887, acc: 0.89673912525177)
[2025-01-06 01:14:14,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:14,756][root][INFO] - Training Epoch: 3/10, step 224/574 completed (loss: 0.6766343712806702, acc: 0.8068181872367859)
[2025-01-06 01:14:14,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:15,187][root][INFO] - Training Epoch: 3/10, step 225/574 completed (loss: 0.9125653505325317, acc: 0.7553191781044006)
[2025-01-06 01:14:15,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:15,525][root][INFO] - Training Epoch: 3/10, step 226/574 completed (loss: 0.5562268495559692, acc: 0.849056601524353)
[2025-01-06 01:14:15,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:15,909][root][INFO] - Training Epoch: 3/10, step 227/574 completed (loss: 0.3264996409416199, acc: 0.8833333253860474)
[2025-01-06 01:14:15,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:16,229][root][INFO] - Training Epoch: 3/10, step 228/574 completed (loss: 0.26599445939064026, acc: 0.930232584476471)
[2025-01-06 01:14:16,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:16,594][root][INFO] - Training Epoch: 3/10, step 229/574 completed (loss: 0.8729885220527649, acc: 0.7666666507720947)
[2025-01-06 01:14:16,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:16,998][root][INFO] - Training Epoch: 3/10, step 230/574 completed (loss: 1.8695404529571533, acc: 0.5473684072494507)
[2025-01-06 01:14:17,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:17,256][root][INFO] - Training Epoch: 3/10, step 231/574 completed (loss: 1.4446157217025757, acc: 0.644444465637207)
[2025-01-06 01:14:17,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:17,666][root][INFO] - Training Epoch: 3/10, step 232/574 completed (loss: 1.384101390838623, acc: 0.605555534362793)
[2025-01-06 01:14:17,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:18,150][root][INFO] - Training Epoch: 3/10, step 233/574 completed (loss: 1.8265550136566162, acc: 0.5)
[2025-01-06 01:14:18,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:18,622][root][INFO] - Training Epoch: 3/10, step 234/574 completed (loss: 1.348418951034546, acc: 0.6230769157409668)
[2025-01-06 01:14:18,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:19,007][root][INFO] - Training Epoch: 3/10, step 235/574 completed (loss: 0.21431797742843628, acc: 0.8947368264198303)
[2025-01-06 01:14:19,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:19,365][root][INFO] - Training Epoch: 3/10, step 236/574 completed (loss: 0.17529423534870148, acc: 0.9583333134651184)
[2025-01-06 01:14:19,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:19,737][root][INFO] - Training Epoch: 3/10, step 237/574 completed (loss: 0.5626278519630432, acc: 0.8636363744735718)
[2025-01-06 01:14:19,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:20,062][root][INFO] - Training Epoch: 3/10, step 238/574 completed (loss: 0.5852564573287964, acc: 0.8518518805503845)
[2025-01-06 01:14:20,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:20,388][root][INFO] - Training Epoch: 3/10, step 239/574 completed (loss: 0.6190545558929443, acc: 0.800000011920929)
[2025-01-06 01:14:20,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:20,785][root][INFO] - Training Epoch: 3/10, step 240/574 completed (loss: 0.9346797466278076, acc: 0.7954545617103577)
[2025-01-06 01:14:20,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:21,129][root][INFO] - Training Epoch: 3/10, step 241/574 completed (loss: 0.45315447449684143, acc: 0.8409090638160706)
[2025-01-06 01:14:21,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:21,704][root][INFO] - Training Epoch: 3/10, step 242/574 completed (loss: 1.0750685930252075, acc: 0.6774193644523621)
[2025-01-06 01:14:21,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:22,226][root][INFO] - Training Epoch: 3/10, step 243/574 completed (loss: 0.8437646627426147, acc: 0.75)
[2025-01-06 01:14:22,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:22,518][root][INFO] - Training Epoch: 3/10, step 244/574 completed (loss: 0.0362459160387516, acc: 1.0)
[2025-01-06 01:14:22,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:22,863][root][INFO] - Training Epoch: 3/10, step 245/574 completed (loss: 0.21349546313285828, acc: 0.8846153616905212)
[2025-01-06 01:14:22,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:23,206][root][INFO] - Training Epoch: 3/10, step 246/574 completed (loss: 0.36595457792282104, acc: 0.9032257795333862)
[2025-01-06 01:14:23,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:23,570][root][INFO] - Training Epoch: 3/10, step 247/574 completed (loss: 0.11488036066293716, acc: 0.949999988079071)
[2025-01-06 01:14:23,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:23,933][root][INFO] - Training Epoch: 3/10, step 248/574 completed (loss: 0.15973901748657227, acc: 0.9729729890823364)
[2025-01-06 01:14:24,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:24,289][root][INFO] - Training Epoch: 3/10, step 249/574 completed (loss: 0.3648070991039276, acc: 0.9189189076423645)
[2025-01-06 01:14:24,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:24,630][root][INFO] - Training Epoch: 3/10, step 250/574 completed (loss: 0.017631281167268753, acc: 1.0)
[2025-01-06 01:14:24,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:24,973][root][INFO] - Training Epoch: 3/10, step 251/574 completed (loss: 0.18662510812282562, acc: 0.9264705777168274)
[2025-01-06 01:14:25,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:25,294][root][INFO] - Training Epoch: 3/10, step 252/574 completed (loss: 0.0930001512169838, acc: 0.9268292784690857)
[2025-01-06 01:14:25,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:25,622][root][INFO] - Training Epoch: 3/10, step 253/574 completed (loss: 0.024219265207648277, acc: 1.0)
[2025-01-06 01:14:25,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:25,981][root][INFO] - Training Epoch: 3/10, step 254/574 completed (loss: 0.04437189921736717, acc: 0.9599999785423279)
[2025-01-06 01:14:26,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:26,330][root][INFO] - Training Epoch: 3/10, step 255/574 completed (loss: 0.08513307571411133, acc: 1.0)
[2025-01-06 01:14:26,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:26,720][root][INFO] - Training Epoch: 3/10, step 256/574 completed (loss: 0.2846541404724121, acc: 0.9122806787490845)
[2025-01-06 01:14:26,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:27,082][root][INFO] - Training Epoch: 3/10, step 257/574 completed (loss: 0.11639082431793213, acc: 0.9571428298950195)
[2025-01-06 01:14:27,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:27,422][root][INFO] - Training Epoch: 3/10, step 258/574 completed (loss: 0.15864861011505127, acc: 0.9473684430122375)
[2025-01-06 01:14:27,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:27,990][root][INFO] - Training Epoch: 3/10, step 259/574 completed (loss: 0.33952006697654724, acc: 0.8867924809455872)
[2025-01-06 01:14:28,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:28,569][root][INFO] - Training Epoch: 3/10, step 260/574 completed (loss: 0.42473042011260986, acc: 0.8999999761581421)
[2025-01-06 01:14:28,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:28,928][root][INFO] - Training Epoch: 3/10, step 261/574 completed (loss: 0.14450155198574066, acc: 0.9444444179534912)
[2025-01-06 01:14:29,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:29,299][root][INFO] - Training Epoch: 3/10, step 262/574 completed (loss: 0.3882101774215698, acc: 0.9032257795333862)
[2025-01-06 01:14:29,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:29,682][root][INFO] - Training Epoch: 3/10, step 263/574 completed (loss: 1.0891592502593994, acc: 0.746666669845581)
[2025-01-06 01:14:29,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:30,070][root][INFO] - Training Epoch: 3/10, step 264/574 completed (loss: 0.6773910522460938, acc: 0.7916666865348816)
[2025-01-06 01:14:30,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:30,902][root][INFO] - Training Epoch: 3/10, step 265/574 completed (loss: 1.2856931686401367, acc: 0.6800000071525574)
[2025-01-06 01:14:30,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:31,211][root][INFO] - Training Epoch: 3/10, step 266/574 completed (loss: 1.3556040525436401, acc: 0.6853932738304138)
[2025-01-06 01:14:31,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:31,558][root][INFO] - Training Epoch: 3/10, step 267/574 completed (loss: 0.7885806560516357, acc: 0.7567567825317383)
[2025-01-06 01:14:31,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32,008][root][INFO] - Training Epoch: 3/10, step 268/574 completed (loss: 0.6928821206092834, acc: 0.7931034564971924)
[2025-01-06 01:14:32,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32,303][root][INFO] - Training Epoch: 3/10, step 269/574 completed (loss: 0.039059121161699295, acc: 1.0)
[2025-01-06 01:14:32,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32,642][root][INFO] - Training Epoch: 3/10, step 270/574 completed (loss: 0.10388296097517014, acc: 1.0)
[2025-01-06 01:14:32,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32,961][root][INFO] - Training Epoch: 3/10, step 271/574 completed (loss: 0.13035467267036438, acc: 0.96875)
[2025-01-06 01:14:33,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:33,343][root][INFO] - Training Epoch: 3/10, step 272/574 completed (loss: 0.18902209401130676, acc: 0.9666666388511658)
[2025-01-06 01:14:33,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:33,734][root][INFO] - Training Epoch: 3/10, step 273/574 completed (loss: 0.21485260128974915, acc: 0.9666666388511658)
[2025-01-06 01:14:33,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:34,083][root][INFO] - Training Epoch: 3/10, step 274/574 completed (loss: 0.22690050303936005, acc: 0.96875)
[2025-01-06 01:14:34,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:34,424][root][INFO] - Training Epoch: 3/10, step 275/574 completed (loss: 0.14762796461582184, acc: 0.9333333373069763)
[2025-01-06 01:14:34,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:34,759][root][INFO] - Training Epoch: 3/10, step 276/574 completed (loss: 0.43226808309555054, acc: 0.8965517282485962)
[2025-01-06 01:14:34,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:35,053][root][INFO] - Training Epoch: 3/10, step 277/574 completed (loss: 0.18403583765029907, acc: 0.9200000166893005)
[2025-01-06 01:14:35,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:35,430][root][INFO] - Training Epoch: 3/10, step 278/574 completed (loss: 0.35200682282447815, acc: 0.8723404407501221)
[2025-01-06 01:14:35,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:35,817][root][INFO] - Training Epoch: 3/10, step 279/574 completed (loss: 0.37936079502105713, acc: 0.8958333134651184)
[2025-01-06 01:14:35,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:36,172][root][INFO] - Training Epoch: 3/10, step 280/574 completed (loss: 0.10405722260475159, acc: 0.9545454382896423)
[2025-01-06 01:14:36,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:36,592][root][INFO] - Training Epoch: 3/10, step 281/574 completed (loss: 0.6354795098304749, acc: 0.8072289228439331)
[2025-01-06 01:14:37,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:37,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:39,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:39,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:39,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:40,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:40,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:42,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:42,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:42,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:43,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:43,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:43,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:44,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:44,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:44,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:45,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:45,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:45,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:46,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:46,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:46,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:47,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:47,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:48,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:48,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:48,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:49,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:49,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:49,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:50,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:50,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:51,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:51,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:51,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:52,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:52,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:52,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:53,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:53,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:53,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:54,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:54,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:54,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:55,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:55,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:56,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:56,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:56,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:57,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:57,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:57,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:58,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:58,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:58,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:59,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:59,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:00,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:00,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:01,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:01,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:01,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:02,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:02,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:02,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:03,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:03,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:03,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:04,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:04,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:04,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:05,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:05,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:05,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:06,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:06,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:06,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:07,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:07,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:08,055][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9138, device='cuda:0') eval_epoch_loss=tensor(0.6491, device='cuda:0') eval_epoch_acc=tensor(0.8248, device='cuda:0')
[2025-01-06 01:15:08,056][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:15:08,056][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:15:08,302][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_282_loss_0.6490678787231445/model.pt
[2025-01-06 01:15:08,309][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:15:08,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:08,715][root][INFO] - Training Epoch: 3/10, step 282/574 completed (loss: 0.7938093543052673, acc: 0.7685185074806213)
[2025-01-06 01:15:08,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:09,073][root][INFO] - Training Epoch: 3/10, step 283/574 completed (loss: 0.05642525851726532, acc: 1.0)
[2025-01-06 01:15:09,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:09,404][root][INFO] - Training Epoch: 3/10, step 284/574 completed (loss: 0.20272275805473328, acc: 0.970588207244873)
[2025-01-06 01:15:09,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:09,736][root][INFO] - Training Epoch: 3/10, step 285/574 completed (loss: 0.3243986666202545, acc: 0.949999988079071)
[2025-01-06 01:15:09,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:10,033][root][INFO] - Training Epoch: 3/10, step 286/574 completed (loss: 0.5116139650344849, acc: 0.8828125)
[2025-01-06 01:15:10,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:10,456][root][INFO] - Training Epoch: 3/10, step 287/574 completed (loss: 0.4497109353542328, acc: 0.8880000114440918)
[2025-01-06 01:15:10,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:10,807][root][INFO] - Training Epoch: 3/10, step 288/574 completed (loss: 0.4709470868110657, acc: 0.8901098966598511)
[2025-01-06 01:15:10,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:11,186][root][INFO] - Training Epoch: 3/10, step 289/574 completed (loss: 0.46812698245048523, acc: 0.8447204828262329)
[2025-01-06 01:15:11,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:11,572][root][INFO] - Training Epoch: 3/10, step 290/574 completed (loss: 0.5253749489784241, acc: 0.8505154848098755)
[2025-01-06 01:15:11,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:11,936][root][INFO] - Training Epoch: 3/10, step 291/574 completed (loss: 0.030947471037507057, acc: 1.0)
[2025-01-06 01:15:12,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:12,296][root][INFO] - Training Epoch: 3/10, step 292/574 completed (loss: 0.2414332777261734, acc: 0.9523809552192688)
[2025-01-06 01:15:12,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:12,651][root][INFO] - Training Epoch: 3/10, step 293/574 completed (loss: 0.1089262142777443, acc: 0.982758641242981)
[2025-01-06 01:15:12,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:13,124][root][INFO] - Training Epoch: 3/10, step 294/574 completed (loss: 0.33350706100463867, acc: 0.9090909361839294)
[2025-01-06 01:15:13,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:13,669][root][INFO] - Training Epoch: 3/10, step 295/574 completed (loss: 0.6376342177391052, acc: 0.8247422575950623)
[2025-01-06 01:15:13,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:13,937][root][INFO] - Training Epoch: 3/10, step 296/574 completed (loss: 0.25727730989456177, acc: 0.931034505367279)
[2025-01-06 01:15:14,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:14,313][root][INFO] - Training Epoch: 3/10, step 297/574 completed (loss: 0.12968558073043823, acc: 0.9629629850387573)
[2025-01-06 01:15:14,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:14,690][root][INFO] - Training Epoch: 3/10, step 298/574 completed (loss: 0.31571024656295776, acc: 0.8684210777282715)
[2025-01-06 01:15:14,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15,037][root][INFO] - Training Epoch: 3/10, step 299/574 completed (loss: 0.09524931013584137, acc: 0.9821428656578064)
[2025-01-06 01:15:15,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15,397][root][INFO] - Training Epoch: 3/10, step 300/574 completed (loss: 0.07614228874444962, acc: 0.96875)
[2025-01-06 01:15:15,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15,789][root][INFO] - Training Epoch: 3/10, step 301/574 completed (loss: 0.2492920160293579, acc: 0.9622641801834106)
[2025-01-06 01:15:15,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:16,186][root][INFO] - Training Epoch: 3/10, step 302/574 completed (loss: 0.011992232874035835, acc: 1.0)
[2025-01-06 01:15:16,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:16,532][root][INFO] - Training Epoch: 3/10, step 303/574 completed (loss: 0.03665999695658684, acc: 1.0)
[2025-01-06 01:15:16,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:16,841][root][INFO] - Training Epoch: 3/10, step 304/574 completed (loss: 0.07891428470611572, acc: 0.96875)
[2025-01-06 01:15:16,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:17,176][root][INFO] - Training Epoch: 3/10, step 305/574 completed (loss: 0.2924841046333313, acc: 0.9344262480735779)
[2025-01-06 01:15:17,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:17,531][root][INFO] - Training Epoch: 3/10, step 306/574 completed (loss: 0.044469866901636124, acc: 1.0)
[2025-01-06 01:15:17,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:17,861][root][INFO] - Training Epoch: 3/10, step 307/574 completed (loss: 0.007169403601437807, acc: 1.0)
[2025-01-06 01:15:17,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:18,235][root][INFO] - Training Epoch: 3/10, step 308/574 completed (loss: 0.2517654001712799, acc: 0.9275362491607666)
[2025-01-06 01:15:18,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:18,649][root][INFO] - Training Epoch: 3/10, step 309/574 completed (loss: 0.2484862506389618, acc: 0.9305555820465088)
[2025-01-06 01:15:18,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:19,003][root][INFO] - Training Epoch: 3/10, step 310/574 completed (loss: 0.1948668509721756, acc: 0.9397590160369873)
[2025-01-06 01:15:19,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:19,370][root][INFO] - Training Epoch: 3/10, step 311/574 completed (loss: 0.289458692073822, acc: 0.8974359035491943)
[2025-01-06 01:15:19,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:19,722][root][INFO] - Training Epoch: 3/10, step 312/574 completed (loss: 0.09563061594963074, acc: 0.9693877696990967)
[2025-01-06 01:15:19,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:20,097][root][INFO] - Training Epoch: 3/10, step 313/574 completed (loss: 0.014484360814094543, acc: 1.0)
[2025-01-06 01:15:20,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:20,454][root][INFO] - Training Epoch: 3/10, step 314/574 completed (loss: 0.03843170776963234, acc: 1.0)
[2025-01-06 01:15:20,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:20,815][root][INFO] - Training Epoch: 3/10, step 315/574 completed (loss: 0.1864684671163559, acc: 0.9354838728904724)
[2025-01-06 01:15:20,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:21,178][root][INFO] - Training Epoch: 3/10, step 316/574 completed (loss: 0.48675310611724854, acc: 0.8709677457809448)
[2025-01-06 01:15:21,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:21,542][root][INFO] - Training Epoch: 3/10, step 317/574 completed (loss: 0.19571155309677124, acc: 0.9402984976768494)
[2025-01-06 01:15:21,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:21,909][root][INFO] - Training Epoch: 3/10, step 318/574 completed (loss: 0.10043726861476898, acc: 0.9711538553237915)
[2025-01-06 01:15:21,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:22,214][root][INFO] - Training Epoch: 3/10, step 319/574 completed (loss: 0.2206631302833557, acc: 0.9555555582046509)
[2025-01-06 01:15:22,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:22,586][root][INFO] - Training Epoch: 3/10, step 320/574 completed (loss: 0.07792617380619049, acc: 0.9838709831237793)
[2025-01-06 01:15:22,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:22,936][root][INFO] - Training Epoch: 3/10, step 321/574 completed (loss: 0.016495557501912117, acc: 1.0)
[2025-01-06 01:15:23,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:23,315][root][INFO] - Training Epoch: 3/10, step 322/574 completed (loss: 0.9523965120315552, acc: 0.7407407164573669)
[2025-01-06 01:15:23,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:23,670][root][INFO] - Training Epoch: 3/10, step 323/574 completed (loss: 1.0232012271881104, acc: 0.6857143044471741)
[2025-01-06 01:15:23,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:24,082][root][INFO] - Training Epoch: 3/10, step 324/574 completed (loss: 1.1991486549377441, acc: 0.7179487347602844)
[2025-01-06 01:15:24,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:24,459][root][INFO] - Training Epoch: 3/10, step 325/574 completed (loss: 1.1301724910736084, acc: 0.6341463327407837)
[2025-01-06 01:15:24,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:24,822][root][INFO] - Training Epoch: 3/10, step 326/574 completed (loss: 0.7316701412200928, acc: 0.8421052694320679)
[2025-01-06 01:15:24,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:25,183][root][INFO] - Training Epoch: 3/10, step 327/574 completed (loss: 0.2481403648853302, acc: 0.8947368264198303)
[2025-01-06 01:15:25,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:25,552][root][INFO] - Training Epoch: 3/10, step 328/574 completed (loss: 0.058537568897008896, acc: 1.0)
[2025-01-06 01:15:25,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:25,875][root][INFO] - Training Epoch: 3/10, step 329/574 completed (loss: 0.04279946908354759, acc: 1.0)
[2025-01-06 01:15:25,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:26,234][root][INFO] - Training Epoch: 3/10, step 330/574 completed (loss: 0.02718840353190899, acc: 1.0)
[2025-01-06 01:15:26,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:26,626][root][INFO] - Training Epoch: 3/10, step 331/574 completed (loss: 0.2427111566066742, acc: 0.9354838728904724)
[2025-01-06 01:15:26,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:27,008][root][INFO] - Training Epoch: 3/10, step 332/574 completed (loss: 0.14751775562763214, acc: 0.9473684430122375)
[2025-01-06 01:15:27,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:27,336][root][INFO] - Training Epoch: 3/10, step 333/574 completed (loss: 0.35041335225105286, acc: 0.9375)
[2025-01-06 01:15:27,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:27,689][root][INFO] - Training Epoch: 3/10, step 334/574 completed (loss: 0.029620399698615074, acc: 1.0)
[2025-01-06 01:15:27,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:28,035][root][INFO] - Training Epoch: 3/10, step 335/574 completed (loss: 0.043285269290208817, acc: 1.0)
[2025-01-06 01:15:28,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:28,349][root][INFO] - Training Epoch: 3/10, step 336/574 completed (loss: 0.7131581902503967, acc: 0.7400000095367432)
[2025-01-06 01:15:28,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:28,750][root][INFO] - Training Epoch: 3/10, step 337/574 completed (loss: 1.1159641742706299, acc: 0.6666666865348816)
[2025-01-06 01:15:28,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:29,118][root][INFO] - Training Epoch: 3/10, step 338/574 completed (loss: 1.1144872903823853, acc: 0.6382978558540344)
[2025-01-06 01:15:29,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:29,476][root][INFO] - Training Epoch: 3/10, step 339/574 completed (loss: 1.2136934995651245, acc: 0.6746987700462341)
[2025-01-06 01:15:29,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:29,852][root][INFO] - Training Epoch: 3/10, step 340/574 completed (loss: 0.030933957546949387, acc: 1.0)
[2025-01-06 01:15:29,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:30,183][root][INFO] - Training Epoch: 3/10, step 341/574 completed (loss: 0.3550207018852234, acc: 0.9230769276618958)
[2025-01-06 01:15:30,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:30,567][root][INFO] - Training Epoch: 3/10, step 342/574 completed (loss: 0.3018559515476227, acc: 0.9156626462936401)
[2025-01-06 01:15:30,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:30,946][root][INFO] - Training Epoch: 3/10, step 343/574 completed (loss: 0.3971070647239685, acc: 0.8867924809455872)
[2025-01-06 01:15:31,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:31,286][root][INFO] - Training Epoch: 3/10, step 344/574 completed (loss: 0.13258399069309235, acc: 0.949367105960846)
[2025-01-06 01:15:31,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:31,623][root][INFO] - Training Epoch: 3/10, step 345/574 completed (loss: 0.08560283482074738, acc: 0.9607843160629272)
[2025-01-06 01:15:31,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:31,982][root][INFO] - Training Epoch: 3/10, step 346/574 completed (loss: 0.4651908576488495, acc: 0.8805969953536987)
[2025-01-06 01:15:32,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:32,341][root][INFO] - Training Epoch: 3/10, step 347/574 completed (loss: 0.09901846200227737, acc: 0.949999988079071)
[2025-01-06 01:15:32,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:32,698][root][INFO] - Training Epoch: 3/10, step 348/574 completed (loss: 0.1113874539732933, acc: 0.9599999785423279)
[2025-01-06 01:15:32,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:33,087][root][INFO] - Training Epoch: 3/10, step 349/574 completed (loss: 0.7341336607933044, acc: 0.8333333134651184)
[2025-01-06 01:15:33,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:33,403][root][INFO] - Training Epoch: 3/10, step 350/574 completed (loss: 0.5264135599136353, acc: 0.8372092843055725)
[2025-01-06 01:15:33,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:33,754][root][INFO] - Training Epoch: 3/10, step 351/574 completed (loss: 0.2478056699037552, acc: 0.9230769276618958)
[2025-01-06 01:15:33,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:34,119][root][INFO] - Training Epoch: 3/10, step 352/574 completed (loss: 0.8173856139183044, acc: 0.7111111283302307)
[2025-01-06 01:15:34,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:34,475][root][INFO] - Training Epoch: 3/10, step 353/574 completed (loss: 0.04147307202219963, acc: 1.0)
[2025-01-06 01:15:34,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:34,886][root][INFO] - Training Epoch: 3/10, step 354/574 completed (loss: 0.24682481586933136, acc: 0.9615384340286255)
[2025-01-06 01:15:35,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:35,276][root][INFO] - Training Epoch: 3/10, step 355/574 completed (loss: 0.5860764384269714, acc: 0.8241758346557617)
[2025-01-06 01:15:35,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:35,766][root][INFO] - Training Epoch: 3/10, step 356/574 completed (loss: 0.4918621778488159, acc: 0.8782608509063721)
[2025-01-06 01:15:35,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:36,120][root][INFO] - Training Epoch: 3/10, step 357/574 completed (loss: 0.4580104649066925, acc: 0.8695651888847351)
[2025-01-06 01:15:36,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:36,520][root][INFO] - Training Epoch: 3/10, step 358/574 completed (loss: 0.4927566945552826, acc: 0.8163265585899353)
[2025-01-06 01:15:36,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:36,881][root][INFO] - Training Epoch: 3/10, step 359/574 completed (loss: 0.0029643706511706114, acc: 1.0)
[2025-01-06 01:15:36,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:37,249][root][INFO] - Training Epoch: 3/10, step 360/574 completed (loss: 0.18181052803993225, acc: 0.9615384340286255)
[2025-01-06 01:15:37,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:37,582][root][INFO] - Training Epoch: 3/10, step 361/574 completed (loss: 0.2690717875957489, acc: 0.9268292784690857)
[2025-01-06 01:15:37,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:37,968][root][INFO] - Training Epoch: 3/10, step 362/574 completed (loss: 0.43291211128234863, acc: 0.9111111164093018)
[2025-01-06 01:15:38,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:38,375][root][INFO] - Training Epoch: 3/10, step 363/574 completed (loss: 0.12655746936798096, acc: 0.9473684430122375)
[2025-01-06 01:15:38,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:38,772][root][INFO] - Training Epoch: 3/10, step 364/574 completed (loss: 0.0991176962852478, acc: 0.9512194991111755)
[2025-01-06 01:15:38,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:39,144][root][INFO] - Training Epoch: 3/10, step 365/574 completed (loss: 0.0507643036544323, acc: 1.0)
[2025-01-06 01:15:39,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:39,480][root][INFO] - Training Epoch: 3/10, step 366/574 completed (loss: 0.01074989140033722, acc: 1.0)
[2025-01-06 01:15:39,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:39,830][root][INFO] - Training Epoch: 3/10, step 367/574 completed (loss: 0.10245286673307419, acc: 0.95652174949646)
[2025-01-06 01:15:39,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:40,190][root][INFO] - Training Epoch: 3/10, step 368/574 completed (loss: 0.026752609759569168, acc: 1.0)
[2025-01-06 01:15:40,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:40,569][root][INFO] - Training Epoch: 3/10, step 369/574 completed (loss: 0.17189617455005646, acc: 0.96875)
[2025-01-06 01:15:40,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:41,191][root][INFO] - Training Epoch: 3/10, step 370/574 completed (loss: 0.5395378470420837, acc: 0.842424213886261)
[2025-01-06 01:15:41,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:42,063][root][INFO] - Training Epoch: 3/10, step 371/574 completed (loss: 0.36140546202659607, acc: 0.8679245114326477)
[2025-01-06 01:15:42,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:42,495][root][INFO] - Training Epoch: 3/10, step 372/574 completed (loss: 0.20778796076774597, acc: 0.9222221970558167)
[2025-01-06 01:15:42,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:42,872][root][INFO] - Training Epoch: 3/10, step 373/574 completed (loss: 0.16468189656734467, acc: 0.9642857313156128)
[2025-01-06 01:15:43,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:43,283][root][INFO] - Training Epoch: 3/10, step 374/574 completed (loss: 0.08020570129156113, acc: 0.9428571462631226)
[2025-01-06 01:15:43,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:43,680][root][INFO] - Training Epoch: 3/10, step 375/574 completed (loss: 0.0014177280245348811, acc: 1.0)
[2025-01-06 01:15:43,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:44,094][root][INFO] - Training Epoch: 3/10, step 376/574 completed (loss: 0.017732633277773857, acc: 1.0)
[2025-01-06 01:15:44,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:44,522][root][INFO] - Training Epoch: 3/10, step 377/574 completed (loss: 0.03412945196032524, acc: 1.0)
[2025-01-06 01:15:44,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:44,905][root][INFO] - Training Epoch: 3/10, step 378/574 completed (loss: 0.027988217771053314, acc: 0.9894737005233765)
[2025-01-06 01:15:45,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:45,474][root][INFO] - Training Epoch: 3/10, step 379/574 completed (loss: 0.30499300360679626, acc: 0.9281437397003174)
[2025-01-06 01:15:45,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:45,874][root][INFO] - Training Epoch: 3/10, step 380/574 completed (loss: 0.3885491192340851, acc: 0.8947368264198303)
[2025-01-06 01:15:46,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:47,078][root][INFO] - Training Epoch: 3/10, step 381/574 completed (loss: 0.510583221912384, acc: 0.8449198007583618)
[2025-01-06 01:15:47,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:47,637][root][INFO] - Training Epoch: 3/10, step 382/574 completed (loss: 0.0879257544875145, acc: 0.9729729890823364)
[2025-01-06 01:15:47,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:47,985][root][INFO] - Training Epoch: 3/10, step 383/574 completed (loss: 0.14248155057430267, acc: 0.9642857313156128)
[2025-01-06 01:15:48,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:48,327][root][INFO] - Training Epoch: 3/10, step 384/574 completed (loss: 0.016508718952536583, acc: 1.0)
[2025-01-06 01:15:48,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:48,669][root][INFO] - Training Epoch: 3/10, step 385/574 completed (loss: 0.04986963048577309, acc: 1.0)
[2025-01-06 01:15:48,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49,039][root][INFO] - Training Epoch: 3/10, step 386/574 completed (loss: 0.003072814317420125, acc: 1.0)
[2025-01-06 01:15:49,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49,368][root][INFO] - Training Epoch: 3/10, step 387/574 completed (loss: 0.005350311286747456, acc: 1.0)
[2025-01-06 01:15:49,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49,690][root][INFO] - Training Epoch: 3/10, step 388/574 completed (loss: 0.0029854984022676945, acc: 1.0)
[2025-01-06 01:15:49,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49,914][root][INFO] - Training Epoch: 3/10, step 389/574 completed (loss: 0.007839367724955082, acc: 1.0)
[2025-01-06 01:15:50,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:50,286][root][INFO] - Training Epoch: 3/10, step 390/574 completed (loss: 0.293498158454895, acc: 0.9523809552192688)
[2025-01-06 01:15:50,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:50,623][root][INFO] - Training Epoch: 3/10, step 391/574 completed (loss: 0.7035606503486633, acc: 0.7222222089767456)
[2025-01-06 01:15:50,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:51,030][root][INFO] - Training Epoch: 3/10, step 392/574 completed (loss: 0.9253897666931152, acc: 0.7669903039932251)
[2025-01-06 01:15:51,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:51,551][root][INFO] - Training Epoch: 3/10, step 393/574 completed (loss: 0.7591496706008911, acc: 0.8161764740943909)
[2025-01-06 01:15:51,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:51,939][root][INFO] - Training Epoch: 3/10, step 394/574 completed (loss: 0.7577818036079407, acc: 0.7733333110809326)
[2025-01-06 01:15:52,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:52,370][root][INFO] - Training Epoch: 3/10, step 395/574 completed (loss: 0.6880870461463928, acc: 0.8194444179534912)
[2025-01-06 01:15:52,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:52,729][root][INFO] - Training Epoch: 3/10, step 396/574 completed (loss: 0.2494271695613861, acc: 0.930232584476471)
[2025-01-06 01:15:52,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:53,057][root][INFO] - Training Epoch: 3/10, step 397/574 completed (loss: 0.08450823277235031, acc: 0.9583333134651184)
[2025-01-06 01:15:53,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:53,444][root][INFO] - Training Epoch: 3/10, step 398/574 completed (loss: 0.2836136519908905, acc: 0.930232584476471)
[2025-01-06 01:15:53,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:53,830][root][INFO] - Training Epoch: 3/10, step 399/574 completed (loss: 0.04839092120528221, acc: 1.0)
[2025-01-06 01:15:54,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:54,365][root][INFO] - Training Epoch: 3/10, step 400/574 completed (loss: 0.22722794115543365, acc: 0.9264705777168274)
[2025-01-06 01:15:54,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:54,747][root][INFO] - Training Epoch: 3/10, step 401/574 completed (loss: 0.4111926555633545, acc: 0.9066666960716248)
[2025-01-06 01:15:54,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:55,136][root][INFO] - Training Epoch: 3/10, step 402/574 completed (loss: 0.2489480972290039, acc: 0.9090909361839294)
[2025-01-06 01:15:55,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:55,490][root][INFO] - Training Epoch: 3/10, step 403/574 completed (loss: 0.08816812187433243, acc: 1.0)
[2025-01-06 01:15:55,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:55,838][root][INFO] - Training Epoch: 3/10, step 404/574 completed (loss: 0.08068209886550903, acc: 0.9677419066429138)
[2025-01-06 01:15:55,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:56,164][root][INFO] - Training Epoch: 3/10, step 405/574 completed (loss: 0.007555874064564705, acc: 1.0)
[2025-01-06 01:15:56,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:56,465][root][INFO] - Training Epoch: 3/10, step 406/574 completed (loss: 0.01107053179293871, acc: 1.0)
[2025-01-06 01:15:56,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:56,812][root][INFO] - Training Epoch: 3/10, step 407/574 completed (loss: 0.0280768945813179, acc: 1.0)
[2025-01-06 01:15:56,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:57,148][root][INFO] - Training Epoch: 3/10, step 408/574 completed (loss: 0.07464251667261124, acc: 1.0)
[2025-01-06 01:15:57,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:57,480][root][INFO] - Training Epoch: 3/10, step 409/574 completed (loss: 0.05082641541957855, acc: 1.0)
[2025-01-06 01:15:57,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:57,816][root][INFO] - Training Epoch: 3/10, step 410/574 completed (loss: 0.08860082924365997, acc: 0.9655172228813171)
[2025-01-06 01:15:57,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:58,167][root][INFO] - Training Epoch: 3/10, step 411/574 completed (loss: 0.007608836982399225, acc: 1.0)
[2025-01-06 01:15:58,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:58,519][root][INFO] - Training Epoch: 3/10, step 412/574 completed (loss: 0.022236274555325508, acc: 1.0)
[2025-01-06 01:15:58,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:58,835][root][INFO] - Training Epoch: 3/10, step 413/574 completed (loss: 0.11889122426509857, acc: 0.939393937587738)
[2025-01-06 01:15:58,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:59,136][root][INFO] - Training Epoch: 3/10, step 414/574 completed (loss: 0.06965141743421555, acc: 0.9545454382896423)
[2025-01-06 01:15:59,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:59,441][root][INFO] - Training Epoch: 3/10, step 415/574 completed (loss: 0.19108349084854126, acc: 0.9607843160629272)
[2025-01-06 01:15:59,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:59,738][root][INFO] - Training Epoch: 3/10, step 416/574 completed (loss: 0.09822952002286911, acc: 1.0)
[2025-01-06 01:15:59,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:00,049][root][INFO] - Training Epoch: 3/10, step 417/574 completed (loss: 0.06902056932449341, acc: 1.0)
[2025-01-06 01:16:00,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:00,379][root][INFO] - Training Epoch: 3/10, step 418/574 completed (loss: 0.1699514389038086, acc: 0.925000011920929)
[2025-01-06 01:16:00,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:00,736][root][INFO] - Training Epoch: 3/10, step 419/574 completed (loss: 0.0772695317864418, acc: 0.949999988079071)
[2025-01-06 01:16:00,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:01,109][root][INFO] - Training Epoch: 3/10, step 420/574 completed (loss: 0.025835206732153893, acc: 1.0)
[2025-01-06 01:16:01,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:01,482][root][INFO] - Training Epoch: 3/10, step 421/574 completed (loss: 0.10125043243169785, acc: 0.9666666388511658)
[2025-01-06 01:16:01,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:01,872][root][INFO] - Training Epoch: 3/10, step 422/574 completed (loss: 0.054414913058280945, acc: 0.96875)
[2025-01-06 01:16:02,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:02,294][root][INFO] - Training Epoch: 3/10, step 423/574 completed (loss: 0.12420007586479187, acc: 0.9444444179534912)
[2025-01-06 01:16:02,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:02,627][root][INFO] - Training Epoch: 3/10, step 424/574 completed (loss: 0.06926039606332779, acc: 0.9629629850387573)
[2025-01-06 01:16:03,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:03,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:04,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:04,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:04,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:05,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:05,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:05,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:06,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:06,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:07,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:07,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:07,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:08,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:08,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:08,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:09,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:09,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:09,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:10,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:10,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:11,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:11,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:11,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:12,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:12,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:12,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:13,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:13,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:13,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:14,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:14,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:14,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:15,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:15,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:15,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:16,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:16,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:16,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:17,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:17,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:17,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:18,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:18,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:18,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:19,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:19,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:19,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:20,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:20,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:20,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:21,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:21,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:21,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:23,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:23,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:23,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:24,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:24,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:24,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:25,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:25,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:26,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:26,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:26,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:27,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:27,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:27,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:28,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:28,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:29,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:29,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:29,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:30,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:30,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:30,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:31,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:31,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:31,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:32,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:32,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:32,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:33,403][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0050, device='cuda:0') eval_epoch_loss=tensor(0.6957, device='cuda:0') eval_epoch_acc=tensor(0.8313, device='cuda:0')
[2025-01-06 01:16:33,405][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:16:33,405][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:16:33,617][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_425_loss_0.6956643462181091/model.pt
[2025-01-06 01:16:33,630][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:16:33,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:34,089][root][INFO] - Training Epoch: 3/10, step 425/574 completed (loss: 0.32046154141426086, acc: 0.9090909361839294)
[2025-01-06 01:16:34,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:34,449][root][INFO] - Training Epoch: 3/10, step 426/574 completed (loss: 0.006335684563964605, acc: 1.0)
[2025-01-06 01:16:34,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:34,803][root][INFO] - Training Epoch: 3/10, step 427/574 completed (loss: 0.08747099339962006, acc: 0.9729729890823364)
[2025-01-06 01:16:34,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:35,158][root][INFO] - Training Epoch: 3/10, step 428/574 completed (loss: 0.00467432476580143, acc: 1.0)
[2025-01-06 01:16:35,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:35,477][root][INFO] - Training Epoch: 3/10, step 429/574 completed (loss: 0.010933632031083107, acc: 1.0)
[2025-01-06 01:16:35,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:35,830][root][INFO] - Training Epoch: 3/10, step 430/574 completed (loss: 0.0025649338494986296, acc: 1.0)
[2025-01-06 01:16:35,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:36,139][root][INFO] - Training Epoch: 3/10, step 431/574 completed (loss: 0.00869173463433981, acc: 1.0)
[2025-01-06 01:16:36,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:36,488][root][INFO] - Training Epoch: 3/10, step 432/574 completed (loss: 0.011874904856085777, acc: 1.0)
[2025-01-06 01:16:36,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:36,897][root][INFO] - Training Epoch: 3/10, step 433/574 completed (loss: 0.18298856914043427, acc: 0.9444444179534912)
[2025-01-06 01:16:36,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:37,235][root][INFO] - Training Epoch: 3/10, step 434/574 completed (loss: 0.07338668406009674, acc: 0.9599999785423279)
[2025-01-06 01:16:37,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:37,584][root][INFO] - Training Epoch: 3/10, step 435/574 completed (loss: 0.0010105199180543423, acc: 1.0)
[2025-01-06 01:16:37,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:37,960][root][INFO] - Training Epoch: 3/10, step 436/574 completed (loss: 0.2600998282432556, acc: 0.8888888955116272)
[2025-01-06 01:16:38,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:38,313][root][INFO] - Training Epoch: 3/10, step 437/574 completed (loss: 0.01447305642068386, acc: 1.0)
[2025-01-06 01:16:38,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:38,562][root][INFO] - Training Epoch: 3/10, step 438/574 completed (loss: 0.05322973057627678, acc: 0.9523809552192688)
[2025-01-06 01:16:38,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:38,903][root][INFO] - Training Epoch: 3/10, step 439/574 completed (loss: 0.11676646769046783, acc: 0.9487179517745972)
[2025-01-06 01:16:39,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:39,359][root][INFO] - Training Epoch: 3/10, step 440/574 completed (loss: 0.33996570110321045, acc: 0.8787878751754761)
[2025-01-06 01:16:39,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:40,036][root][INFO] - Training Epoch: 3/10, step 441/574 completed (loss: 0.8082929253578186, acc: 0.7680000066757202)
[2025-01-06 01:16:40,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:40,476][root][INFO] - Training Epoch: 3/10, step 442/574 completed (loss: 0.8577013611793518, acc: 0.7903226017951965)
[2025-01-06 01:16:40,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:41,125][root][INFO] - Training Epoch: 3/10, step 443/574 completed (loss: 0.5054462552070618, acc: 0.9104477763175964)
[2025-01-06 01:16:41,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:41,536][root][INFO] - Training Epoch: 3/10, step 444/574 completed (loss: 0.14162255823612213, acc: 0.9622641801834106)
[2025-01-06 01:16:41,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:41,966][root][INFO] - Training Epoch: 3/10, step 445/574 completed (loss: 0.15050701797008514, acc: 0.9772727489471436)
[2025-01-06 01:16:42,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:42,343][root][INFO] - Training Epoch: 3/10, step 446/574 completed (loss: 0.2686474621295929, acc: 0.9130434989929199)
[2025-01-06 01:16:42,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:42,709][root][INFO] - Training Epoch: 3/10, step 447/574 completed (loss: 0.1453745812177658, acc: 0.9230769276618958)
[2025-01-06 01:16:42,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:43,099][root][INFO] - Training Epoch: 3/10, step 448/574 completed (loss: 0.14054502546787262, acc: 0.9285714030265808)
[2025-01-06 01:16:43,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:43,438][root][INFO] - Training Epoch: 3/10, step 449/574 completed (loss: 0.25170406699180603, acc: 0.9552238583564758)
[2025-01-06 01:16:43,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:43,865][root][INFO] - Training Epoch: 3/10, step 450/574 completed (loss: 0.036410775035619736, acc: 1.0)
[2025-01-06 01:16:43,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:44,218][root][INFO] - Training Epoch: 3/10, step 451/574 completed (loss: 0.055278170853853226, acc: 0.989130437374115)
[2025-01-06 01:16:44,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:44,600][root][INFO] - Training Epoch: 3/10, step 452/574 completed (loss: 0.19824615120887756, acc: 0.9358974099159241)
[2025-01-06 01:16:44,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:44,949][root][INFO] - Training Epoch: 3/10, step 453/574 completed (loss: 0.23202595114707947, acc: 0.9078947305679321)
[2025-01-06 01:16:45,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:45,284][root][INFO] - Training Epoch: 3/10, step 454/574 completed (loss: 0.07718682289123535, acc: 0.9795918464660645)
[2025-01-06 01:16:45,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:45,630][root][INFO] - Training Epoch: 3/10, step 455/574 completed (loss: 0.3387884497642517, acc: 0.8787878751754761)
[2025-01-06 01:16:45,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:46,041][root][INFO] - Training Epoch: 3/10, step 456/574 completed (loss: 0.5351601243019104, acc: 0.876288652420044)
[2025-01-06 01:16:46,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:46,424][root][INFO] - Training Epoch: 3/10, step 457/574 completed (loss: 0.04056369140744209, acc: 0.9857142567634583)
[2025-01-06 01:16:46,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:46,816][root][INFO] - Training Epoch: 3/10, step 458/574 completed (loss: 0.2746192216873169, acc: 0.930232584476471)
[2025-01-06 01:16:46,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:47,202][root][INFO] - Training Epoch: 3/10, step 459/574 completed (loss: 0.04570302367210388, acc: 0.9821428656578064)
[2025-01-06 01:16:47,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:47,546][root][INFO] - Training Epoch: 3/10, step 460/574 completed (loss: 0.19184643030166626, acc: 0.9506173133850098)
[2025-01-06 01:16:47,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:47,863][root][INFO] - Training Epoch: 3/10, step 461/574 completed (loss: 0.21794456243515015, acc: 0.9166666865348816)
[2025-01-06 01:16:47,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:48,225][root][INFO] - Training Epoch: 3/10, step 462/574 completed (loss: 0.07138444483280182, acc: 0.96875)
[2025-01-06 01:16:48,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:48,600][root][INFO] - Training Epoch: 3/10, step 463/574 completed (loss: 0.32570409774780273, acc: 0.9615384340286255)
[2025-01-06 01:16:48,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:49,006][root][INFO] - Training Epoch: 3/10, step 464/574 completed (loss: 0.16699087619781494, acc: 0.9347826242446899)
[2025-01-06 01:16:49,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:49,383][root][INFO] - Training Epoch: 3/10, step 465/574 completed (loss: 0.36776769161224365, acc: 0.8928571343421936)
[2025-01-06 01:16:49,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:49,723][root][INFO] - Training Epoch: 3/10, step 466/574 completed (loss: 0.47041982412338257, acc: 0.8674699068069458)
[2025-01-06 01:16:49,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:50,118][root][INFO] - Training Epoch: 3/10, step 467/574 completed (loss: 0.21125635504722595, acc: 0.9369369149208069)
[2025-01-06 01:16:50,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:50,475][root][INFO] - Training Epoch: 3/10, step 468/574 completed (loss: 0.5620225071907043, acc: 0.8543689250946045)
[2025-01-06 01:16:50,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:50,796][root][INFO] - Training Epoch: 3/10, step 469/574 completed (loss: 0.39464399218559265, acc: 0.8861788511276245)
[2025-01-06 01:16:50,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:51,101][root][INFO] - Training Epoch: 3/10, step 470/574 completed (loss: 0.16004343330860138, acc: 0.9166666865348816)
[2025-01-06 01:16:51,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:51,436][root][INFO] - Training Epoch: 3/10, step 471/574 completed (loss: 0.3914039134979248, acc: 0.8928571343421936)
[2025-01-06 01:16:51,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:51,834][root][INFO] - Training Epoch: 3/10, step 472/574 completed (loss: 0.5580617189407349, acc: 0.843137264251709)
[2025-01-06 01:16:51,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:52,196][root][INFO] - Training Epoch: 3/10, step 473/574 completed (loss: 0.7277350425720215, acc: 0.807860255241394)
[2025-01-06 01:16:52,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:52,532][root][INFO] - Training Epoch: 3/10, step 474/574 completed (loss: 0.44096890091896057, acc: 0.84375)
[2025-01-06 01:16:52,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:52,832][root][INFO] - Training Epoch: 3/10, step 475/574 completed (loss: 0.37670037150382996, acc: 0.9018405079841614)
[2025-01-06 01:16:52,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:53,185][root][INFO] - Training Epoch: 3/10, step 476/574 completed (loss: 0.3539148271083832, acc: 0.9136690497398376)
[2025-01-06 01:16:53,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:53,620][root][INFO] - Training Epoch: 3/10, step 477/574 completed (loss: 0.7339615821838379, acc: 0.7839195728302002)
[2025-01-06 01:16:53,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:53,977][root][INFO] - Training Epoch: 3/10, step 478/574 completed (loss: 0.23272565007209778, acc: 0.9444444179534912)
[2025-01-06 01:16:54,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:54,303][root][INFO] - Training Epoch: 3/10, step 479/574 completed (loss: 0.23271703720092773, acc: 0.939393937587738)
[2025-01-06 01:16:54,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:54,675][root][INFO] - Training Epoch: 3/10, step 480/574 completed (loss: 0.5013447403907776, acc: 0.8888888955116272)
[2025-01-06 01:16:54,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:55,059][root][INFO] - Training Epoch: 3/10, step 481/574 completed (loss: 0.6254019141197205, acc: 0.8500000238418579)
[2025-01-06 01:16:55,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:55,398][root][INFO] - Training Epoch: 3/10, step 482/574 completed (loss: 0.4783399701118469, acc: 0.949999988079071)
[2025-01-06 01:16:55,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:55,774][root][INFO] - Training Epoch: 3/10, step 483/574 completed (loss: 0.5032283663749695, acc: 0.7931034564971924)
[2025-01-06 01:16:55,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:56,176][root][INFO] - Training Epoch: 3/10, step 484/574 completed (loss: 0.09189873188734055, acc: 0.9677419066429138)
[2025-01-06 01:16:56,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:56,547][root][INFO] - Training Epoch: 3/10, step 485/574 completed (loss: 0.21131104230880737, acc: 0.8947368264198303)
[2025-01-06 01:16:56,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:56,889][root][INFO] - Training Epoch: 3/10, step 486/574 completed (loss: 0.4743732810020447, acc: 0.8518518805503845)
[2025-01-06 01:16:56,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:57,196][root][INFO] - Training Epoch: 3/10, step 487/574 completed (loss: 0.32513347268104553, acc: 0.9047619104385376)
[2025-01-06 01:16:57,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:57,553][root][INFO] - Training Epoch: 3/10, step 488/574 completed (loss: 0.1922198235988617, acc: 0.9545454382896423)
[2025-01-06 01:16:57,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:57,912][root][INFO] - Training Epoch: 3/10, step 489/574 completed (loss: 0.9553903341293335, acc: 0.7230769395828247)
[2025-01-06 01:16:57,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:58,210][root][INFO] - Training Epoch: 3/10, step 490/574 completed (loss: 0.18137803673744202, acc: 0.9666666388511658)
[2025-01-06 01:16:58,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:58,547][root][INFO] - Training Epoch: 3/10, step 491/574 completed (loss: 0.3227439820766449, acc: 0.8965517282485962)
[2025-01-06 01:16:58,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:58,904][root][INFO] - Training Epoch: 3/10, step 492/574 completed (loss: 0.4201149046421051, acc: 0.9019607901573181)
[2025-01-06 01:16:59,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:59,263][root][INFO] - Training Epoch: 3/10, step 493/574 completed (loss: 0.25507956743240356, acc: 0.8965517282485962)
[2025-01-06 01:16:59,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:59,645][root][INFO] - Training Epoch: 3/10, step 494/574 completed (loss: 0.2823775112628937, acc: 0.8947368264198303)
[2025-01-06 01:16:59,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:59,982][root][INFO] - Training Epoch: 3/10, step 495/574 completed (loss: 0.219626784324646, acc: 0.9473684430122375)
[2025-01-06 01:17:00,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:00,375][root][INFO] - Training Epoch: 3/10, step 496/574 completed (loss: 0.6188015937805176, acc: 0.8392857313156128)
[2025-01-06 01:17:00,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:00,798][root][INFO] - Training Epoch: 3/10, step 497/574 completed (loss: 0.3899924159049988, acc: 0.8539325594902039)
[2025-01-06 01:17:00,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:01,194][root][INFO] - Training Epoch: 3/10, step 498/574 completed (loss: 0.6506513953208923, acc: 0.8089887499809265)
[2025-01-06 01:17:01,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:01,576][root][INFO] - Training Epoch: 3/10, step 499/574 completed (loss: 1.1668671369552612, acc: 0.6241135001182556)
[2025-01-06 01:17:01,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:01,989][root][INFO] - Training Epoch: 3/10, step 500/574 completed (loss: 0.7028369307518005, acc: 0.8369565010070801)
[2025-01-06 01:17:02,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:02,366][root][INFO] - Training Epoch: 3/10, step 501/574 completed (loss: 0.010846695862710476, acc: 1.0)
[2025-01-06 01:17:02,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:02,709][root][INFO] - Training Epoch: 3/10, step 502/574 completed (loss: 0.005208149552345276, acc: 1.0)
[2025-01-06 01:17:02,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:03,011][root][INFO] - Training Epoch: 3/10, step 503/574 completed (loss: 0.20358887314796448, acc: 0.8888888955116272)
[2025-01-06 01:17:03,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:03,316][root][INFO] - Training Epoch: 3/10, step 504/574 completed (loss: 0.06371233612298965, acc: 1.0)
[2025-01-06 01:17:03,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:03,671][root][INFO] - Training Epoch: 3/10, step 505/574 completed (loss: 0.5511409044265747, acc: 0.8301886916160583)
[2025-01-06 01:17:03,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:04,059][root][INFO] - Training Epoch: 3/10, step 506/574 completed (loss: 0.5272637009620667, acc: 0.8620689511299133)
[2025-01-06 01:17:04,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:04,648][root][INFO] - Training Epoch: 3/10, step 507/574 completed (loss: 0.8694260716438293, acc: 0.7837837934494019)
[2025-01-06 01:17:04,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:05,081][root][INFO] - Training Epoch: 3/10, step 508/574 completed (loss: 0.624876081943512, acc: 0.8309859037399292)
[2025-01-06 01:17:05,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:05,387][root][INFO] - Training Epoch: 3/10, step 509/574 completed (loss: 0.5761305689811707, acc: 0.8999999761581421)
[2025-01-06 01:17:05,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:05,756][root][INFO] - Training Epoch: 3/10, step 510/574 completed (loss: 0.09969920665025711, acc: 0.9666666388511658)
[2025-01-06 01:17:05,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:06,115][root][INFO] - Training Epoch: 3/10, step 511/574 completed (loss: 0.14282503724098206, acc: 0.9230769276618958)
[2025-01-06 01:17:07,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:08,493][root][INFO] - Training Epoch: 3/10, step 512/574 completed (loss: 0.9666503071784973, acc: 0.7214285731315613)
[2025-01-06 01:17:08,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:09,247][root][INFO] - Training Epoch: 3/10, step 513/574 completed (loss: 0.2693909704685211, acc: 0.9126983880996704)
[2025-01-06 01:17:09,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:09,534][root][INFO] - Training Epoch: 3/10, step 514/574 completed (loss: 0.5381748676300049, acc: 0.8571428656578064)
[2025-01-06 01:17:09,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:09,868][root][INFO] - Training Epoch: 3/10, step 515/574 completed (loss: 0.07251861691474915, acc: 0.9666666388511658)
[2025-01-06 01:17:10,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:10,559][root][INFO] - Training Epoch: 3/10, step 516/574 completed (loss: 0.44115859270095825, acc: 0.875)
[2025-01-06 01:17:10,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:10,924][root][INFO] - Training Epoch: 3/10, step 517/574 completed (loss: 0.0022854176349937916, acc: 1.0)
[2025-01-06 01:17:11,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:11,249][root][INFO] - Training Epoch: 3/10, step 518/574 completed (loss: 0.048522572964429855, acc: 1.0)
[2025-01-06 01:17:11,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:11,607][root][INFO] - Training Epoch: 3/10, step 519/574 completed (loss: 0.15483129024505615, acc: 0.949999988079071)
[2025-01-06 01:17:11,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:11,991][root][INFO] - Training Epoch: 3/10, step 520/574 completed (loss: 0.2175174355506897, acc: 0.8888888955116272)
[2025-01-06 01:17:12,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:12,966][root][INFO] - Training Epoch: 3/10, step 521/574 completed (loss: 0.5801116824150085, acc: 0.8220338821411133)
[2025-01-06 01:17:13,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:13,352][root][INFO] - Training Epoch: 3/10, step 522/574 completed (loss: 0.30690231919288635, acc: 0.9029850959777832)
[2025-01-06 01:17:13,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:13,798][root][INFO] - Training Epoch: 3/10, step 523/574 completed (loss: 0.35000112652778625, acc: 0.8905109763145447)
[2025-01-06 01:17:13,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:14,379][root][INFO] - Training Epoch: 3/10, step 524/574 completed (loss: 0.6275310516357422, acc: 0.8399999737739563)
[2025-01-06 01:17:14,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:14,773][root][INFO] - Training Epoch: 3/10, step 525/574 completed (loss: 0.13394750654697418, acc: 0.9629629850387573)
[2025-01-06 01:17:14,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:15,093][root][INFO] - Training Epoch: 3/10, step 526/574 completed (loss: 0.17820808291435242, acc: 0.942307710647583)
[2025-01-06 01:17:15,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:15,441][root][INFO] - Training Epoch: 3/10, step 527/574 completed (loss: 0.10425693541765213, acc: 1.0)
[2025-01-06 01:17:15,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:15,812][root][INFO] - Training Epoch: 3/10, step 528/574 completed (loss: 1.2861210107803345, acc: 0.6557376980781555)
[2025-01-06 01:17:15,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:16,192][root][INFO] - Training Epoch: 3/10, step 529/574 completed (loss: 0.27841904759407043, acc: 0.9322034120559692)
[2025-01-06 01:17:16,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:16,577][root][INFO] - Training Epoch: 3/10, step 530/574 completed (loss: 0.921375036239624, acc: 0.7209302186965942)
[2025-01-06 01:17:16,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:16,920][root][INFO] - Training Epoch: 3/10, step 531/574 completed (loss: 0.5653144121170044, acc: 0.8181818127632141)
[2025-01-06 01:17:17,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:17,286][root][INFO] - Training Epoch: 3/10, step 532/574 completed (loss: 0.4901794195175171, acc: 0.849056601524353)
[2025-01-06 01:17:17,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:17,663][root][INFO] - Training Epoch: 3/10, step 533/574 completed (loss: 0.6266969442367554, acc: 0.8636363744735718)
[2025-01-06 01:17:17,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:17,990][root][INFO] - Training Epoch: 3/10, step 534/574 completed (loss: 0.2718747556209564, acc: 0.9200000166893005)
[2025-01-06 01:17:18,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:18,395][root][INFO] - Training Epoch: 3/10, step 535/574 completed (loss: 0.28422898054122925, acc: 0.8999999761581421)
[2025-01-06 01:17:18,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:18,753][root][INFO] - Training Epoch: 3/10, step 536/574 completed (loss: 0.0394948273897171, acc: 1.0)
[2025-01-06 01:17:18,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:19,142][root][INFO] - Training Epoch: 3/10, step 537/574 completed (loss: 0.5271860361099243, acc: 0.8307692408561707)
[2025-01-06 01:17:19,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:19,466][root][INFO] - Training Epoch: 3/10, step 538/574 completed (loss: 0.31669193506240845, acc: 0.921875)
[2025-01-06 01:17:19,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:19,857][root][INFO] - Training Epoch: 3/10, step 539/574 completed (loss: 0.5114613175392151, acc: 0.875)
[2025-01-06 01:17:19,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:20,149][root][INFO] - Training Epoch: 3/10, step 540/574 completed (loss: 0.4606063663959503, acc: 0.8787878751754761)
[2025-01-06 01:17:20,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:20,467][root][INFO] - Training Epoch: 3/10, step 541/574 completed (loss: 0.06641175597906113, acc: 0.9375)
[2025-01-06 01:17:20,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:20,801][root][INFO] - Training Epoch: 3/10, step 542/574 completed (loss: 0.01657508872449398, acc: 1.0)
[2025-01-06 01:17:20,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:21,103][root][INFO] - Training Epoch: 3/10, step 543/574 completed (loss: 0.07741523534059525, acc: 0.95652174949646)
[2025-01-06 01:17:21,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:21,413][root][INFO] - Training Epoch: 3/10, step 544/574 completed (loss: 0.008575190789997578, acc: 1.0)
[2025-01-06 01:17:21,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:21,760][root][INFO] - Training Epoch: 3/10, step 545/574 completed (loss: 0.0369839072227478, acc: 1.0)
[2025-01-06 01:17:21,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:22,076][root][INFO] - Training Epoch: 3/10, step 546/574 completed (loss: 0.027471642941236496, acc: 1.0)
[2025-01-06 01:17:22,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:22,401][root][INFO] - Training Epoch: 3/10, step 547/574 completed (loss: 0.03397027775645256, acc: 0.9736841917037964)
[2025-01-06 01:17:22,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:22,738][root][INFO] - Training Epoch: 3/10, step 548/574 completed (loss: 0.041830871254205704, acc: 0.9677419066429138)
[2025-01-06 01:17:22,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:23,092][root][INFO] - Training Epoch: 3/10, step 549/574 completed (loss: 0.0024004606530070305, acc: 1.0)
[2025-01-06 01:17:23,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:23,413][root][INFO] - Training Epoch: 3/10, step 550/574 completed (loss: 0.25217005610466003, acc: 0.939393937587738)
[2025-01-06 01:17:23,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:23,772][root][INFO] - Training Epoch: 3/10, step 551/574 completed (loss: 0.016424963250756264, acc: 1.0)
[2025-01-06 01:17:23,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:24,196][root][INFO] - Training Epoch: 3/10, step 552/574 completed (loss: 0.13590902090072632, acc: 0.9571428298950195)
[2025-01-06 01:17:24,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:24,601][root][INFO] - Training Epoch: 3/10, step 553/574 completed (loss: 0.531755805015564, acc: 0.8759124279022217)
[2025-01-06 01:17:24,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:24,983][root][INFO] - Training Epoch: 3/10, step 554/574 completed (loss: 0.2682551145553589, acc: 0.9034482836723328)
[2025-01-06 01:17:25,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:25,341][root][INFO] - Training Epoch: 3/10, step 555/574 completed (loss: 0.3215143084526062, acc: 0.9071428775787354)
[2025-01-06 01:17:25,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:25,689][root][INFO] - Training Epoch: 3/10, step 556/574 completed (loss: 0.34960368275642395, acc: 0.9337748289108276)
[2025-01-06 01:17:25,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:26,023][root][INFO] - Training Epoch: 3/10, step 557/574 completed (loss: 0.09240107238292694, acc: 0.9572649598121643)
[2025-01-06 01:17:26,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:26,397][root][INFO] - Training Epoch: 3/10, step 558/574 completed (loss: 0.028470315039157867, acc: 1.0)
[2025-01-06 01:17:26,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:26,735][root][INFO] - Training Epoch: 3/10, step 559/574 completed (loss: 0.08460167795419693, acc: 0.9615384340286255)
[2025-01-06 01:17:26,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:27,064][root][INFO] - Training Epoch: 3/10, step 560/574 completed (loss: 0.145677387714386, acc: 0.9230769276618958)
[2025-01-06 01:17:27,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:27,419][root][INFO] - Training Epoch: 3/10, step 561/574 completed (loss: 0.014552495442330837, acc: 1.0)
[2025-01-06 01:17:27,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:27,773][root][INFO] - Training Epoch: 3/10, step 562/574 completed (loss: 0.3486213982105255, acc: 0.8888888955116272)
[2025-01-06 01:17:27,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:28,094][root][INFO] - Training Epoch: 3/10, step 563/574 completed (loss: 0.33394235372543335, acc: 0.8961039185523987)
[2025-01-06 01:17:28,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:28,459][root][INFO] - Training Epoch: 3/10, step 564/574 completed (loss: 0.06830577552318573, acc: 0.9791666865348816)
[2025-01-06 01:17:28,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:28,785][root][INFO] - Training Epoch: 3/10, step 565/574 completed (loss: 0.12731888890266418, acc: 0.9482758641242981)
[2025-01-06 01:17:28,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:29,124][root][INFO] - Training Epoch: 3/10, step 566/574 completed (loss: 0.26082202792167664, acc: 0.9285714030265808)
[2025-01-06 01:17:29,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:29,467][root][INFO] - Training Epoch: 3/10, step 567/574 completed (loss: 0.005393632221966982, acc: 1.0)
[2025-01-06 01:17:30,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:30,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:30,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:31,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:31,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:31,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:32,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:32,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:32,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:33,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:33,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:33,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:34,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:34,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:35,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:35,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:36,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:36,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:36,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:38,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:38,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:40,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:40,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:40,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:41,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:41,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:41,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:42,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:42,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:43,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:43,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:43,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:44,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:44,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:44,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:45,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:45,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:46,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:46,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:46,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:47,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:47,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:47,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:48,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:48,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:48,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:49,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:49,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:49,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:50,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:50,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:52,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:52,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:52,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:53,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:53,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:54,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:54,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:54,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:55,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:55,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:56,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:56,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:56,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:57,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:57,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:57,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:59,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:59,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:59,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:00,518][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1218, device='cuda:0') eval_epoch_loss=tensor(0.7523, device='cuda:0') eval_epoch_acc=tensor(0.8247, device='cuda:0')
[2025-01-06 01:18:00,519][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:18:00,519][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:18:00,742][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_568_loss_0.7522604465484619/model.pt
[2025-01-06 01:18:00,754][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:18:00,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:01,155][root][INFO] - Training Epoch: 3/10, step 568/574 completed (loss: 0.017584344372153282, acc: 1.0)
[2025-01-06 01:18:01,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:01,551][root][INFO] - Training Epoch: 3/10, step 569/574 completed (loss: 0.13858453929424286, acc: 0.9572192430496216)
[2025-01-06 01:18:01,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:01,883][root][INFO] - Training Epoch: 3/10, step 570/574 completed (loss: 0.019572226330637932, acc: 0.9838709831237793)
[2025-01-06 01:18:02,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:02,263][root][INFO] - Training Epoch: 3/10, step 571/574 completed (loss: 0.22209486365318298, acc: 0.9572649598121643)
[2025-01-06 01:18:02,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:02,575][root][INFO] - Training Epoch: 3/10, step 572/574 completed (loss: 0.3008228540420532, acc: 0.9132652878761292)
[2025-01-06 01:18:02,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:02,937][root][INFO] - Training Epoch: 3/10, step 573/574 completed (loss: 0.2623974680900574, acc: 0.8993710875511169)
[2025-01-06 01:18:03,370][slam_llm.utils.train_utils][INFO] - Epoch 3: train_perplexity=1.5145, train_epoch_loss=0.4151, epoch time 358.7459137327969s
[2025-01-06 01:18:03,371][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:18:03,371][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 16 GB
[2025-01-06 01:18:03,371][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:18:03,371][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 9
[2025-01-06 01:18:03,371][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:18:03,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:04,188][root][INFO] - Training Epoch: 4/10, step 0/574 completed (loss: 0.1399715542793274, acc: 0.9629629850387573)
[2025-01-06 01:18:04,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:04,524][root][INFO] - Training Epoch: 4/10, step 1/574 completed (loss: 0.05825558304786682, acc: 0.9599999785423279)
[2025-01-06 01:18:04,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:04,892][root][INFO] - Training Epoch: 4/10, step 2/574 completed (loss: 0.5325559973716736, acc: 0.8648648858070374)
[2025-01-06 01:18:05,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:05,255][root][INFO] - Training Epoch: 4/10, step 3/574 completed (loss: 0.24084630608558655, acc: 0.9210526347160339)
[2025-01-06 01:18:05,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:05,600][root][INFO] - Training Epoch: 4/10, step 4/574 completed (loss: 0.46459662914276123, acc: 0.8918918967247009)
[2025-01-06 01:18:05,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:05,979][root][INFO] - Training Epoch: 4/10, step 5/574 completed (loss: 0.16418816149234772, acc: 0.9642857313156128)
[2025-01-06 01:18:06,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:06,277][root][INFO] - Training Epoch: 4/10, step 6/574 completed (loss: 0.7419252991676331, acc: 0.795918345451355)
[2025-01-06 01:18:06,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:06,630][root][INFO] - Training Epoch: 4/10, step 7/574 completed (loss: 0.16459152102470398, acc: 0.9333333373069763)
[2025-01-06 01:18:06,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:07,026][root][INFO] - Training Epoch: 4/10, step 8/574 completed (loss: 0.01635882817208767, acc: 1.0)
[2025-01-06 01:18:07,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:07,396][root][INFO] - Training Epoch: 4/10, step 9/574 completed (loss: 0.006790520623326302, acc: 1.0)
[2025-01-06 01:18:07,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:07,764][root][INFO] - Training Epoch: 4/10, step 10/574 completed (loss: 0.019588414579629898, acc: 1.0)
[2025-01-06 01:18:07,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:08,084][root][INFO] - Training Epoch: 4/10, step 11/574 completed (loss: 0.18087705969810486, acc: 0.9487179517745972)
[2025-01-06 01:18:08,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:08,449][root][INFO] - Training Epoch: 4/10, step 12/574 completed (loss: 0.04808439686894417, acc: 1.0)
[2025-01-06 01:18:08,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:08,860][root][INFO] - Training Epoch: 4/10, step 13/574 completed (loss: 0.1346047818660736, acc: 0.9130434989929199)
[2025-01-06 01:18:08,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:09,254][root][INFO] - Training Epoch: 4/10, step 14/574 completed (loss: 0.05733349174261093, acc: 1.0)
[2025-01-06 01:18:09,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:09,662][root][INFO] - Training Epoch: 4/10, step 15/574 completed (loss: 0.32167115807533264, acc: 0.8979591727256775)
[2025-01-06 01:18:09,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:10,053][root][INFO] - Training Epoch: 4/10, step 16/574 completed (loss: 0.028416050598025322, acc: 1.0)
[2025-01-06 01:18:10,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:10,383][root][INFO] - Training Epoch: 4/10, step 17/574 completed (loss: 0.041381511837244034, acc: 1.0)
[2025-01-06 01:18:10,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:10,752][root][INFO] - Training Epoch: 4/10, step 18/574 completed (loss: 0.303844153881073, acc: 0.9166666865348816)
[2025-01-06 01:18:10,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:11,092][root][INFO] - Training Epoch: 4/10, step 19/574 completed (loss: 0.06409524381160736, acc: 0.9473684430122375)
[2025-01-06 01:18:11,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:11,448][root][INFO] - Training Epoch: 4/10, step 20/574 completed (loss: 0.05396197363734245, acc: 1.0)
[2025-01-06 01:18:11,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:11,815][root][INFO] - Training Epoch: 4/10, step 21/574 completed (loss: 0.21182921528816223, acc: 0.8965517282485962)
[2025-01-06 01:18:11,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:12,199][root][INFO] - Training Epoch: 4/10, step 22/574 completed (loss: 0.27289631962776184, acc: 0.8799999952316284)
[2025-01-06 01:18:12,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:12,630][root][INFO] - Training Epoch: 4/10, step 23/574 completed (loss: 0.37770622968673706, acc: 0.9047619104385376)
[2025-01-06 01:18:12,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:12,961][root][INFO] - Training Epoch: 4/10, step 24/574 completed (loss: 0.013538476079702377, acc: 1.0)
[2025-01-06 01:18:13,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:13,309][root][INFO] - Training Epoch: 4/10, step 25/574 completed (loss: 0.6293315291404724, acc: 0.9056603908538818)
[2025-01-06 01:18:13,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:13,664][root][INFO] - Training Epoch: 4/10, step 26/574 completed (loss: 0.7996149063110352, acc: 0.7808219194412231)
[2025-01-06 01:18:14,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:14,845][root][INFO] - Training Epoch: 4/10, step 27/574 completed (loss: 0.8297079205513, acc: 0.7628458738327026)
[2025-01-06 01:18:14,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:15,199][root][INFO] - Training Epoch: 4/10, step 28/574 completed (loss: 0.3325270414352417, acc: 0.9069767594337463)
[2025-01-06 01:18:15,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:15,550][root][INFO] - Training Epoch: 4/10, step 29/574 completed (loss: 0.3887271285057068, acc: 0.8433734774589539)
[2025-01-06 01:18:15,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:15,912][root][INFO] - Training Epoch: 4/10, step 30/574 completed (loss: 0.37670671939849854, acc: 0.8765432238578796)
[2025-01-06 01:18:16,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:16,279][root][INFO] - Training Epoch: 4/10, step 31/574 completed (loss: 0.22929278016090393, acc: 0.8928571343421936)
[2025-01-06 01:18:16,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:16,610][root][INFO] - Training Epoch: 4/10, step 32/574 completed (loss: 0.07861226052045822, acc: 0.9629629850387573)
[2025-01-06 01:18:16,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:16,991][root][INFO] - Training Epoch: 4/10, step 33/574 completed (loss: 0.03207525610923767, acc: 1.0)
[2025-01-06 01:18:17,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:17,380][root][INFO] - Training Epoch: 4/10, step 34/574 completed (loss: 0.48182228207588196, acc: 0.8571428656578064)
[2025-01-06 01:18:17,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:17,730][root][INFO] - Training Epoch: 4/10, step 35/574 completed (loss: 0.22871357202529907, acc: 0.9344262480735779)
[2025-01-06 01:18:17,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:18,083][root][INFO] - Training Epoch: 4/10, step 36/574 completed (loss: 0.49908241629600525, acc: 0.8730158805847168)
[2025-01-06 01:18:18,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:18,445][root][INFO] - Training Epoch: 4/10, step 37/574 completed (loss: 0.47910037636756897, acc: 0.8813559412956238)
[2025-01-06 01:18:18,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:18,796][root][INFO] - Training Epoch: 4/10, step 38/574 completed (loss: 0.25196677446365356, acc: 0.9195402264595032)
[2025-01-06 01:18:18,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:19,154][root][INFO] - Training Epoch: 4/10, step 39/574 completed (loss: 0.15299689769744873, acc: 0.9523809552192688)
[2025-01-06 01:18:19,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:19,571][root][INFO] - Training Epoch: 4/10, step 40/574 completed (loss: 0.2041599005460739, acc: 0.9230769276618958)
[2025-01-06 01:18:19,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:19,989][root][INFO] - Training Epoch: 4/10, step 41/574 completed (loss: 0.21846918761730194, acc: 0.9324324131011963)
[2025-01-06 01:18:20,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:20,385][root][INFO] - Training Epoch: 4/10, step 42/574 completed (loss: 0.3926714062690735, acc: 0.892307698726654)
[2025-01-06 01:18:20,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:20,816][root][INFO] - Training Epoch: 4/10, step 43/574 completed (loss: 0.4204394817352295, acc: 0.8989899158477783)
[2025-01-06 01:18:20,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:21,246][root][INFO] - Training Epoch: 4/10, step 44/574 completed (loss: 0.2286277860403061, acc: 0.9587628841400146)
[2025-01-06 01:18:21,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:21,678][root][INFO] - Training Epoch: 4/10, step 45/574 completed (loss: 0.3164675533771515, acc: 0.875)
[2025-01-06 01:18:21,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:22,058][root][INFO] - Training Epoch: 4/10, step 46/574 completed (loss: 0.123244509100914, acc: 0.9615384340286255)
[2025-01-06 01:18:22,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:22,429][root][INFO] - Training Epoch: 4/10, step 47/574 completed (loss: 0.0705571323633194, acc: 0.9629629850387573)
[2025-01-06 01:18:22,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:22,817][root][INFO] - Training Epoch: 4/10, step 48/574 completed (loss: 0.13534867763519287, acc: 0.9642857313156128)
[2025-01-06 01:18:22,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:23,189][root][INFO] - Training Epoch: 4/10, step 49/574 completed (loss: 0.016676349565386772, acc: 1.0)
[2025-01-06 01:18:23,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:23,574][root][INFO] - Training Epoch: 4/10, step 50/574 completed (loss: 0.4462243318557739, acc: 0.8421052694320679)
[2025-01-06 01:18:23,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:23,980][root][INFO] - Training Epoch: 4/10, step 51/574 completed (loss: 0.32661640644073486, acc: 0.8888888955116272)
[2025-01-06 01:18:24,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:24,363][root][INFO] - Training Epoch: 4/10, step 52/574 completed (loss: 0.5445103049278259, acc: 0.8309859037399292)
[2025-01-06 01:18:24,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:24,817][root][INFO] - Training Epoch: 4/10, step 53/574 completed (loss: 1.2897841930389404, acc: 0.6466666460037231)
[2025-01-06 01:18:24,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:25,134][root][INFO] - Training Epoch: 4/10, step 54/574 completed (loss: 0.5317159295082092, acc: 0.8108108043670654)
[2025-01-06 01:18:25,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:25,517][root][INFO] - Training Epoch: 4/10, step 55/574 completed (loss: 0.15143798291683197, acc: 0.9615384340286255)
[2025-01-06 01:18:27,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:28,692][root][INFO] - Training Epoch: 4/10, step 56/574 completed (loss: 0.8373098373413086, acc: 0.7542662024497986)
[2025-01-06 01:18:29,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:30,035][root][INFO] - Training Epoch: 4/10, step 57/574 completed (loss: 1.103640079498291, acc: 0.6884531378746033)
[2025-01-06 01:18:30,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:30,656][root][INFO] - Training Epoch: 4/10, step 58/574 completed (loss: 0.6631272435188293, acc: 0.7897727489471436)
[2025-01-06 01:18:30,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:31,231][root][INFO] - Training Epoch: 4/10, step 59/574 completed (loss: 0.29491353034973145, acc: 0.9264705777168274)
[2025-01-06 01:18:31,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:31,799][root][INFO] - Training Epoch: 4/10, step 60/574 completed (loss: 0.7412809729576111, acc: 0.782608687877655)
[2025-01-06 01:18:31,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:32,204][root][INFO] - Training Epoch: 4/10, step 61/574 completed (loss: 0.537589967250824, acc: 0.8500000238418579)
[2025-01-06 01:18:32,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:32,514][root][INFO] - Training Epoch: 4/10, step 62/574 completed (loss: 0.24426676332950592, acc: 0.9117646813392639)
[2025-01-06 01:18:32,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:32,901][root][INFO] - Training Epoch: 4/10, step 63/574 completed (loss: 0.25607484579086304, acc: 0.9166666865348816)
[2025-01-06 01:18:33,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:33,312][root][INFO] - Training Epoch: 4/10, step 64/574 completed (loss: 0.08647508919239044, acc: 0.984375)
[2025-01-06 01:18:33,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:33,725][root][INFO] - Training Epoch: 4/10, step 65/574 completed (loss: 0.07557505369186401, acc: 1.0)
[2025-01-06 01:18:33,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:34,124][root][INFO] - Training Epoch: 4/10, step 66/574 completed (loss: 0.43134790658950806, acc: 0.8571428656578064)
[2025-01-06 01:18:34,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:34,531][root][INFO] - Training Epoch: 4/10, step 67/574 completed (loss: 0.16013269126415253, acc: 0.9666666388511658)
[2025-01-06 01:18:34,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:34,875][root][INFO] - Training Epoch: 4/10, step 68/574 completed (loss: 0.061402760446071625, acc: 0.9599999785423279)
[2025-01-06 01:18:34,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:35,237][root][INFO] - Training Epoch: 4/10, step 69/574 completed (loss: 0.3118288516998291, acc: 0.9166666865348816)
[2025-01-06 01:18:35,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:35,606][root][INFO] - Training Epoch: 4/10, step 70/574 completed (loss: 0.38438838720321655, acc: 0.8787878751754761)
[2025-01-06 01:18:35,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:35,997][root][INFO] - Training Epoch: 4/10, step 71/574 completed (loss: 0.7470759153366089, acc: 0.779411792755127)
[2025-01-06 01:18:36,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:36,356][root][INFO] - Training Epoch: 4/10, step 72/574 completed (loss: 0.6198471188545227, acc: 0.8333333134651184)
[2025-01-06 01:18:36,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:36,739][root][INFO] - Training Epoch: 4/10, step 73/574 completed (loss: 1.2564903497695923, acc: 0.6512820720672607)
[2025-01-06 01:18:36,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:37,092][root][INFO] - Training Epoch: 4/10, step 74/574 completed (loss: 0.8872793316841125, acc: 0.7551020383834839)
[2025-01-06 01:18:37,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:37,444][root][INFO] - Training Epoch: 4/10, step 75/574 completed (loss: 0.9910928606987, acc: 0.7238805890083313)
[2025-01-06 01:18:37,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:37,841][root][INFO] - Training Epoch: 4/10, step 76/574 completed (loss: 1.3383458852767944, acc: 0.6386861205101013)
[2025-01-06 01:18:37,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:38,215][root][INFO] - Training Epoch: 4/10, step 77/574 completed (loss: 0.023512806743383408, acc: 1.0)
[2025-01-06 01:18:38,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:38,562][root][INFO] - Training Epoch: 4/10, step 78/574 completed (loss: 0.1403171420097351, acc: 0.9583333134651184)
[2025-01-06 01:18:38,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:38,873][root][INFO] - Training Epoch: 4/10, step 79/574 completed (loss: 0.24905502796173096, acc: 0.9696969985961914)
[2025-01-06 01:18:38,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:39,212][root][INFO] - Training Epoch: 4/10, step 80/574 completed (loss: 0.1313323825597763, acc: 0.9615384340286255)
[2025-01-06 01:18:39,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:39,589][root][INFO] - Training Epoch: 4/10, step 81/574 completed (loss: 0.33864662051200867, acc: 0.9038461446762085)
[2025-01-06 01:18:39,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:40,013][root][INFO] - Training Epoch: 4/10, step 82/574 completed (loss: 0.41329261660575867, acc: 0.8846153616905212)
[2025-01-06 01:18:40,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:40,427][root][INFO] - Training Epoch: 4/10, step 83/574 completed (loss: 0.26981890201568604, acc: 0.9375)
[2025-01-06 01:18:40,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:40,842][root][INFO] - Training Epoch: 4/10, step 84/574 completed (loss: 0.28440284729003906, acc: 0.9130434989929199)
[2025-01-06 01:18:40,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:41,252][root][INFO] - Training Epoch: 4/10, step 85/574 completed (loss: 0.20283707976341248, acc: 0.9200000166893005)
[2025-01-06 01:18:41,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:41,626][root][INFO] - Training Epoch: 4/10, step 86/574 completed (loss: 0.11380242556333542, acc: 0.95652174949646)
[2025-01-06 01:18:41,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:42,090][root][INFO] - Training Epoch: 4/10, step 87/574 completed (loss: 0.5786399245262146, acc: 0.7799999713897705)
[2025-01-06 01:18:42,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:42,419][root][INFO] - Training Epoch: 4/10, step 88/574 completed (loss: 0.7061253786087036, acc: 0.7864077687263489)
[2025-01-06 01:18:42,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:43,492][root][INFO] - Training Epoch: 4/10, step 89/574 completed (loss: 0.8472293615341187, acc: 0.7961165308952332)
[2025-01-06 01:18:43,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:44,304][root][INFO] - Training Epoch: 4/10, step 90/574 completed (loss: 0.9007250070571899, acc: 0.7580645084381104)
[2025-01-06 01:18:44,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:45,099][root][INFO] - Training Epoch: 4/10, step 91/574 completed (loss: 0.85622638463974, acc: 0.7715517282485962)
[2025-01-06 01:18:45,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:45,837][root][INFO] - Training Epoch: 4/10, step 92/574 completed (loss: 0.6246361136436462, acc: 0.8105263113975525)
[2025-01-06 01:18:46,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:46,819][root][INFO] - Training Epoch: 4/10, step 93/574 completed (loss: 0.8289371132850647, acc: 0.7524752616882324)
[2025-01-06 01:18:46,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:47,174][root][INFO] - Training Epoch: 4/10, step 94/574 completed (loss: 0.6176038980484009, acc: 0.8548387289047241)
[2025-01-06 01:18:47,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:47,562][root][INFO] - Training Epoch: 4/10, step 95/574 completed (loss: 0.6225376129150391, acc: 0.8115941882133484)
[2025-01-06 01:18:47,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:47,894][root][INFO] - Training Epoch: 4/10, step 96/574 completed (loss: 0.8775286674499512, acc: 0.7142857313156128)
[2025-01-06 01:18:48,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:48,302][root][INFO] - Training Epoch: 4/10, step 97/574 completed (loss: 0.931708037853241, acc: 0.7403846383094788)
[2025-01-06 01:18:48,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:48,706][root][INFO] - Training Epoch: 4/10, step 98/574 completed (loss: 1.109279751777649, acc: 0.6642335653305054)
[2025-01-06 01:18:48,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:49,069][root][INFO] - Training Epoch: 4/10, step 99/574 completed (loss: 0.7883751392364502, acc: 0.746268630027771)
[2025-01-06 01:18:49,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:49,408][root][INFO] - Training Epoch: 4/10, step 100/574 completed (loss: 0.17495808005332947, acc: 0.949999988079071)
[2025-01-06 01:18:49,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:49,784][root][INFO] - Training Epoch: 4/10, step 101/574 completed (loss: 0.010944616049528122, acc: 1.0)
[2025-01-06 01:18:49,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:50,174][root][INFO] - Training Epoch: 4/10, step 102/574 completed (loss: 0.017396483570337296, acc: 1.0)
[2025-01-06 01:18:50,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:50,537][root][INFO] - Training Epoch: 4/10, step 103/574 completed (loss: 0.03828638792037964, acc: 0.9772727489471436)
[2025-01-06 01:18:50,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:50,948][root][INFO] - Training Epoch: 4/10, step 104/574 completed (loss: 0.32829633355140686, acc: 0.8965517282485962)
[2025-01-06 01:18:51,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:51,290][root][INFO] - Training Epoch: 4/10, step 105/574 completed (loss: 0.09164189547300339, acc: 0.9534883499145508)
[2025-01-06 01:18:51,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:51,656][root][INFO] - Training Epoch: 4/10, step 106/574 completed (loss: 0.2383081614971161, acc: 0.9200000166893005)
[2025-01-06 01:18:51,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:52,000][root][INFO] - Training Epoch: 4/10, step 107/574 completed (loss: 0.012627365998923779, acc: 1.0)
[2025-01-06 01:18:52,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:52,359][root][INFO] - Training Epoch: 4/10, step 108/574 completed (loss: 0.006031022407114506, acc: 1.0)
[2025-01-06 01:18:52,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:52,701][root][INFO] - Training Epoch: 4/10, step 109/574 completed (loss: 0.038848891854286194, acc: 0.976190447807312)
[2025-01-06 01:18:52,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:53,030][root][INFO] - Training Epoch: 4/10, step 110/574 completed (loss: 0.15161225199699402, acc: 0.9384615421295166)
[2025-01-06 01:18:53,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:53,423][root][INFO] - Training Epoch: 4/10, step 111/574 completed (loss: 0.3272704780101776, acc: 0.8947368264198303)
[2025-01-06 01:18:53,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:53,761][root][INFO] - Training Epoch: 4/10, step 112/574 completed (loss: 0.5505033135414124, acc: 0.8245614171028137)
[2025-01-06 01:18:53,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:54,113][root][INFO] - Training Epoch: 4/10, step 113/574 completed (loss: 0.2598865330219269, acc: 0.9487179517745972)
[2025-01-06 01:18:54,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:54,483][root][INFO] - Training Epoch: 4/10, step 114/574 completed (loss: 0.22864671051502228, acc: 0.918367326259613)
[2025-01-06 01:18:54,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:54,833][root][INFO] - Training Epoch: 4/10, step 115/574 completed (loss: 0.005392166785895824, acc: 1.0)
[2025-01-06 01:18:54,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:55,224][root][INFO] - Training Epoch: 4/10, step 116/574 completed (loss: 0.25657200813293457, acc: 0.9365079402923584)
[2025-01-06 01:18:55,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:55,578][root][INFO] - Training Epoch: 4/10, step 117/574 completed (loss: 0.3613496720790863, acc: 0.8943089246749878)
[2025-01-06 01:18:55,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:56,002][root][INFO] - Training Epoch: 4/10, step 118/574 completed (loss: 0.18282561004161835, acc: 0.9516128897666931)
[2025-01-06 01:18:56,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:56,870][root][INFO] - Training Epoch: 4/10, step 119/574 completed (loss: 0.6281323432922363, acc: 0.8365018963813782)
[2025-01-06 01:18:56,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:57,273][root][INFO] - Training Epoch: 4/10, step 120/574 completed (loss: 0.16933922469615936, acc: 0.9599999785423279)
[2025-01-06 01:18:57,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:57,678][root][INFO] - Training Epoch: 4/10, step 121/574 completed (loss: 0.34238573908805847, acc: 0.9230769276618958)
[2025-01-06 01:18:57,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:58,029][root][INFO] - Training Epoch: 4/10, step 122/574 completed (loss: 0.021643327549099922, acc: 1.0)
[2025-01-06 01:18:58,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:58,383][root][INFO] - Training Epoch: 4/10, step 123/574 completed (loss: 0.10135834664106369, acc: 0.9473684430122375)
[2025-01-06 01:18:58,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:58,790][root][INFO] - Training Epoch: 4/10, step 124/574 completed (loss: 0.7494078874588013, acc: 0.7484662532806396)
[2025-01-06 01:18:58,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:59,205][root][INFO] - Training Epoch: 4/10, step 125/574 completed (loss: 0.8461563587188721, acc: 0.7777777910232544)
[2025-01-06 01:18:59,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:59,597][root][INFO] - Training Epoch: 4/10, step 126/574 completed (loss: 0.8733332753181458, acc: 0.7333333492279053)
[2025-01-06 01:18:59,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:59,993][root][INFO] - Training Epoch: 4/10, step 127/574 completed (loss: 0.5047833323478699, acc: 0.8214285969734192)
[2025-01-06 01:19:00,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:00,349][root][INFO] - Training Epoch: 4/10, step 128/574 completed (loss: 0.6494486331939697, acc: 0.800000011920929)
[2025-01-06 01:19:00,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:00,780][root][INFO] - Training Epoch: 4/10, step 129/574 completed (loss: 0.63129723072052, acc: 0.8161764740943909)
[2025-01-06 01:19:00,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:01,164][root][INFO] - Training Epoch: 4/10, step 130/574 completed (loss: 0.13190732896327972, acc: 0.9615384340286255)
[2025-01-06 01:19:01,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:01,468][root][INFO] - Training Epoch: 4/10, step 131/574 completed (loss: 0.3995800018310547, acc: 0.9130434989929199)
[2025-01-06 01:19:01,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:01,808][root][INFO] - Training Epoch: 4/10, step 132/574 completed (loss: 0.12644295394420624, acc: 1.0)
[2025-01-06 01:19:01,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:02,142][root][INFO] - Training Epoch: 4/10, step 133/574 completed (loss: 0.42923256754875183, acc: 0.8260869383811951)
[2025-01-06 01:19:02,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:02,478][root][INFO] - Training Epoch: 4/10, step 134/574 completed (loss: 0.10291770845651627, acc: 0.9714285731315613)
[2025-01-06 01:19:02,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:02,844][root][INFO] - Training Epoch: 4/10, step 135/574 completed (loss: 0.20792429149150848, acc: 0.9615384340286255)
[2025-01-06 01:19:02,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:03,199][root][INFO] - Training Epoch: 4/10, step 136/574 completed (loss: 0.2692171633243561, acc: 0.9047619104385376)
[2025-01-06 01:19:03,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:04,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:04,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:06,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:06,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:06,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:07,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:07,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:07,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:08,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:08,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:09,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:09,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:09,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:10,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:10,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:10,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:11,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:11,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:11,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:12,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:12,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:12,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:13,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:13,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:13,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:15,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:15,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:16,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:16,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:16,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:17,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:17,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:17,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:18,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:18,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:18,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:19,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:19,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:20,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:20,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:20,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:21,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:21,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:21,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:22,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:22,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:23,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:23,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:23,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:24,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:24,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:24,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:25,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:25,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:25,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:26,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:26,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:26,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:27,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:27,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:27,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:28,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:28,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:28,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:29,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:29,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:29,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:30,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:30,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:30,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:31,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:31,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:31,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:32,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:32,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:32,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:33,488][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9606, device='cuda:0') eval_epoch_loss=tensor(0.6733, device='cuda:0') eval_epoch_acc=tensor(0.8349, device='cuda:0')
[2025-01-06 01:19:33,489][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:19:33,489][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:19:33,716][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_137_loss_0.6732606291770935/model.pt
[2025-01-06 01:19:33,725][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:19:33,726][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 4 is 0.8348817825317383
[2025-01-06 01:19:33,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:34,120][root][INFO] - Training Epoch: 4/10, step 137/574 completed (loss: 0.3507217764854431, acc: 0.8999999761581421)
[2025-01-06 01:19:34,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:34,480][root][INFO] - Training Epoch: 4/10, step 138/574 completed (loss: 0.03613696247339249, acc: 1.0)
[2025-01-06 01:19:34,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:34,844][root][INFO] - Training Epoch: 4/10, step 139/574 completed (loss: 0.024240439757704735, acc: 1.0)
[2025-01-06 01:19:34,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:35,212][root][INFO] - Training Epoch: 4/10, step 140/574 completed (loss: 0.12973825633525848, acc: 0.9615384340286255)
[2025-01-06 01:19:35,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:35,615][root][INFO] - Training Epoch: 4/10, step 141/574 completed (loss: 0.1588064283132553, acc: 0.9354838728904724)
[2025-01-06 01:19:35,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:36,010][root][INFO] - Training Epoch: 4/10, step 142/574 completed (loss: 0.24500060081481934, acc: 0.9459459185600281)
[2025-01-06 01:19:36,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:36,603][root][INFO] - Training Epoch: 4/10, step 143/574 completed (loss: 0.5391263365745544, acc: 0.7982456088066101)
[2025-01-06 01:19:36,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:37,009][root][INFO] - Training Epoch: 4/10, step 144/574 completed (loss: 0.6243345737457275, acc: 0.8059701323509216)
[2025-01-06 01:19:37,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:37,438][root][INFO] - Training Epoch: 4/10, step 145/574 completed (loss: 0.3923567831516266, acc: 0.8775510191917419)
[2025-01-06 01:19:37,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:37,904][root][INFO] - Training Epoch: 4/10, step 146/574 completed (loss: 0.8226107954978943, acc: 0.7340425252914429)
[2025-01-06 01:19:37,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:38,229][root][INFO] - Training Epoch: 4/10, step 147/574 completed (loss: 0.3526741564273834, acc: 0.8714285492897034)
[2025-01-06 01:19:38,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:38,557][root][INFO] - Training Epoch: 4/10, step 148/574 completed (loss: 0.21439680457115173, acc: 0.9285714030265808)
[2025-01-06 01:19:38,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:38,886][root][INFO] - Training Epoch: 4/10, step 149/574 completed (loss: 0.08658505976200104, acc: 0.95652174949646)
[2025-01-06 01:19:38,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:39,236][root][INFO] - Training Epoch: 4/10, step 150/574 completed (loss: 0.045463163405656815, acc: 1.0)
[2025-01-06 01:19:39,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:39,604][root][INFO] - Training Epoch: 4/10, step 151/574 completed (loss: 0.5652123093605042, acc: 0.804347813129425)
[2025-01-06 01:19:39,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:39,958][root][INFO] - Training Epoch: 4/10, step 152/574 completed (loss: 0.3312835991382599, acc: 0.8983050584793091)
[2025-01-06 01:19:40,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:40,328][root][INFO] - Training Epoch: 4/10, step 153/574 completed (loss: 0.38315775990486145, acc: 0.8947368264198303)
[2025-01-06 01:19:40,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:40,695][root][INFO] - Training Epoch: 4/10, step 154/574 completed (loss: 0.5013526678085327, acc: 0.837837815284729)
[2025-01-06 01:19:40,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:41,072][root][INFO] - Training Epoch: 4/10, step 155/574 completed (loss: 0.12984026968479156, acc: 0.8928571343421936)
[2025-01-06 01:19:41,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:41,424][root][INFO] - Training Epoch: 4/10, step 156/574 completed (loss: 0.028890011832118034, acc: 1.0)
[2025-01-06 01:19:41,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:41,793][root][INFO] - Training Epoch: 4/10, step 157/574 completed (loss: 0.6187095046043396, acc: 0.8421052694320679)
[2025-01-06 01:19:42,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:43,572][root][INFO] - Training Epoch: 4/10, step 158/574 completed (loss: 0.5810673832893372, acc: 0.7702702879905701)
[2025-01-06 01:19:43,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:43,866][root][INFO] - Training Epoch: 4/10, step 159/574 completed (loss: 0.7853304743766785, acc: 0.7222222089767456)
[2025-01-06 01:19:43,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:44,263][root][INFO] - Training Epoch: 4/10, step 160/574 completed (loss: 0.7728598117828369, acc: 0.7674418687820435)
[2025-01-06 01:19:44,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:44,843][root][INFO] - Training Epoch: 4/10, step 161/574 completed (loss: 1.0559359788894653, acc: 0.7411764860153198)
[2025-01-06 01:19:44,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:45,393][root][INFO] - Training Epoch: 4/10, step 162/574 completed (loss: 1.0237449407577515, acc: 0.7640449404716492)
[2025-01-06 01:19:45,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:45,754][root][INFO] - Training Epoch: 4/10, step 163/574 completed (loss: 0.3383086919784546, acc: 0.9090909361839294)
[2025-01-06 01:19:45,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:46,120][root][INFO] - Training Epoch: 4/10, step 164/574 completed (loss: 0.18647174537181854, acc: 0.9523809552192688)
[2025-01-06 01:19:46,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:46,449][root][INFO] - Training Epoch: 4/10, step 165/574 completed (loss: 0.569567859172821, acc: 0.8275862336158752)
[2025-01-06 01:19:46,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:46,831][root][INFO] - Training Epoch: 4/10, step 166/574 completed (loss: 0.2379361242055893, acc: 0.9387755393981934)
[2025-01-06 01:19:46,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:47,218][root][INFO] - Training Epoch: 4/10, step 167/574 completed (loss: 0.08440667390823364, acc: 0.9800000190734863)
[2025-01-06 01:19:47,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:47,631][root][INFO] - Training Epoch: 4/10, step 168/574 completed (loss: 0.4037245512008667, acc: 0.8888888955116272)
[2025-01-06 01:19:47,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:47,933][root][INFO] - Training Epoch: 4/10, step 169/574 completed (loss: 0.9330845475196838, acc: 0.7647058963775635)
[2025-01-06 01:19:48,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:48,952][root][INFO] - Training Epoch: 4/10, step 170/574 completed (loss: 0.673824667930603, acc: 0.8082191944122314)
[2025-01-06 01:19:49,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:49,230][root][INFO] - Training Epoch: 4/10, step 171/574 completed (loss: 0.17332255840301514, acc: 0.9583333134651184)
[2025-01-06 01:19:49,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:49,565][root][INFO] - Training Epoch: 4/10, step 172/574 completed (loss: 0.5129712820053101, acc: 0.8518518805503845)
[2025-01-06 01:19:49,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:49,954][root][INFO] - Training Epoch: 4/10, step 173/574 completed (loss: 0.2075737565755844, acc: 0.9642857313156128)
[2025-01-06 01:19:50,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:50,492][root][INFO] - Training Epoch: 4/10, step 174/574 completed (loss: 0.9648066759109497, acc: 0.7345132827758789)
[2025-01-06 01:19:50,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:50,795][root][INFO] - Training Epoch: 4/10, step 175/574 completed (loss: 0.6829808354377747, acc: 0.7971014380455017)
[2025-01-06 01:19:50,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:51,191][root][INFO] - Training Epoch: 4/10, step 176/574 completed (loss: 0.22799935936927795, acc: 0.9204545617103577)
[2025-01-06 01:19:51,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:52,094][root][INFO] - Training Epoch: 4/10, step 177/574 completed (loss: 1.1668695211410522, acc: 0.6717557311058044)
[2025-01-06 01:19:52,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:52,777][root][INFO] - Training Epoch: 4/10, step 178/574 completed (loss: 0.8339667916297913, acc: 0.7555555701255798)
[2025-01-06 01:19:52,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:53,084][root][INFO] - Training Epoch: 4/10, step 179/574 completed (loss: 0.36522310972213745, acc: 0.868852436542511)
[2025-01-06 01:19:53,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:53,459][root][INFO] - Training Epoch: 4/10, step 180/574 completed (loss: 0.0035420414060354233, acc: 1.0)
[2025-01-06 01:19:53,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:53,758][root][INFO] - Training Epoch: 4/10, step 181/574 completed (loss: 0.012440201826393604, acc: 1.0)
[2025-01-06 01:19:53,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:54,060][root][INFO] - Training Epoch: 4/10, step 182/574 completed (loss: 0.08723090589046478, acc: 0.9642857313156128)
[2025-01-06 01:19:54,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:54,372][root][INFO] - Training Epoch: 4/10, step 183/574 completed (loss: 0.2256375551223755, acc: 0.9268292784690857)
[2025-01-06 01:19:54,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:54,765][root][INFO] - Training Epoch: 4/10, step 184/574 completed (loss: 0.37289419770240784, acc: 0.9093655347824097)
[2025-01-06 01:19:54,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:55,175][root][INFO] - Training Epoch: 4/10, step 185/574 completed (loss: 0.43154898285865784, acc: 0.8962535858154297)
[2025-01-06 01:19:55,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:55,651][root][INFO] - Training Epoch: 4/10, step 186/574 completed (loss: 0.3742423951625824, acc: 0.8999999761581421)
[2025-01-06 01:19:55,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:56,179][root][INFO] - Training Epoch: 4/10, step 187/574 completed (loss: 0.5095587968826294, acc: 0.8611631989479065)
[2025-01-06 01:19:56,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:56,594][root][INFO] - Training Epoch: 4/10, step 188/574 completed (loss: 0.47390052676200867, acc: 0.854092538356781)
[2025-01-06 01:19:56,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:56,947][root][INFO] - Training Epoch: 4/10, step 189/574 completed (loss: 0.10047892481088638, acc: 1.0)
[2025-01-06 01:19:57,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:57,497][root][INFO] - Training Epoch: 4/10, step 190/574 completed (loss: 0.6478592753410339, acc: 0.7906976938247681)
[2025-01-06 01:19:57,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:58,288][root][INFO] - Training Epoch: 4/10, step 191/574 completed (loss: 1.0336121320724487, acc: 0.6984127163887024)
[2025-01-06 01:19:58,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:59,197][root][INFO] - Training Epoch: 4/10, step 192/574 completed (loss: 0.9025354385375977, acc: 0.7272727489471436)
[2025-01-06 01:19:59,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:59,933][root][INFO] - Training Epoch: 4/10, step 193/574 completed (loss: 0.6506028771400452, acc: 0.8117647171020508)
[2025-01-06 01:20:00,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:01,003][root][INFO] - Training Epoch: 4/10, step 194/574 completed (loss: 0.8836892247200012, acc: 0.7592592835426331)
[2025-01-06 01:20:01,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:01,951][root][INFO] - Training Epoch: 4/10, step 195/574 completed (loss: 0.30984076857566833, acc: 0.9193548560142517)
[2025-01-06 01:20:02,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:02,225][root][INFO] - Training Epoch: 4/10, step 196/574 completed (loss: 0.035883910953998566, acc: 1.0)
[2025-01-06 01:20:02,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:02,603][root][INFO] - Training Epoch: 4/10, step 197/574 completed (loss: 0.3199087083339691, acc: 0.8500000238418579)
[2025-01-06 01:20:02,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:02,962][root][INFO] - Training Epoch: 4/10, step 198/574 completed (loss: 0.6028430461883545, acc: 0.8529411554336548)
[2025-01-06 01:20:03,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:03,329][root][INFO] - Training Epoch: 4/10, step 199/574 completed (loss: 0.9757100343704224, acc: 0.7426470518112183)
[2025-01-06 01:20:03,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:03,685][root][INFO] - Training Epoch: 4/10, step 200/574 completed (loss: 0.6432700157165527, acc: 0.8220338821411133)
[2025-01-06 01:20:03,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:04,041][root][INFO] - Training Epoch: 4/10, step 201/574 completed (loss: 0.7736186385154724, acc: 0.7761194109916687)
[2025-01-06 01:20:04,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:04,404][root][INFO] - Training Epoch: 4/10, step 202/574 completed (loss: 0.7095327377319336, acc: 0.7961165308952332)
[2025-01-06 01:20:04,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:04,789][root][INFO] - Training Epoch: 4/10, step 203/574 completed (loss: 0.48252439498901367, acc: 0.8730158805847168)
[2025-01-06 01:20:04,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:05,162][root][INFO] - Training Epoch: 4/10, step 204/574 completed (loss: 0.07934793084859848, acc: 0.9560439586639404)
[2025-01-06 01:20:05,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:05,547][root][INFO] - Training Epoch: 4/10, step 205/574 completed (loss: 0.20436272025108337, acc: 0.9506726264953613)
[2025-01-06 01:20:05,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:05,958][root][INFO] - Training Epoch: 4/10, step 206/574 completed (loss: 0.3621352016925812, acc: 0.8858267664909363)
[2025-01-06 01:20:06,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:06,360][root][INFO] - Training Epoch: 4/10, step 207/574 completed (loss: 0.259611040353775, acc: 0.9353448152542114)
[2025-01-06 01:20:06,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:06,726][root][INFO] - Training Epoch: 4/10, step 208/574 completed (loss: 0.3682381808757782, acc: 0.9094203114509583)
[2025-01-06 01:20:06,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:07,129][root][INFO] - Training Epoch: 4/10, step 209/574 completed (loss: 0.2861364483833313, acc: 0.9027237296104431)
[2025-01-06 01:20:07,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:07,504][root][INFO] - Training Epoch: 4/10, step 210/574 completed (loss: 0.1417105346918106, acc: 0.95652174949646)
[2025-01-06 01:20:07,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:07,905][root][INFO] - Training Epoch: 4/10, step 211/574 completed (loss: 0.04632413759827614, acc: 1.0)
[2025-01-06 01:20:08,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:08,283][root][INFO] - Training Epoch: 4/10, step 212/574 completed (loss: 0.49452584981918335, acc: 0.8571428656578064)
[2025-01-06 01:20:08,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:08,637][root][INFO] - Training Epoch: 4/10, step 213/574 completed (loss: 0.10086455941200256, acc: 0.957446813583374)
[2025-01-06 01:20:08,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:09,341][root][INFO] - Training Epoch: 4/10, step 214/574 completed (loss: 0.15171784162521362, acc: 0.9692307710647583)
[2025-01-06 01:20:09,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:09,645][root][INFO] - Training Epoch: 4/10, step 215/574 completed (loss: 0.14613063633441925, acc: 0.9729729890823364)
[2025-01-06 01:20:09,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:09,972][root][INFO] - Training Epoch: 4/10, step 216/574 completed (loss: 0.09446313977241516, acc: 0.9767441749572754)
[2025-01-06 01:20:10,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:10,502][root][INFO] - Training Epoch: 4/10, step 217/574 completed (loss: 0.20821548998355865, acc: 0.9459459185600281)
[2025-01-06 01:20:10,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:10,912][root][INFO] - Training Epoch: 4/10, step 218/574 completed (loss: 0.13689938187599182, acc: 0.9444444179534912)
[2025-01-06 01:20:11,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:11,285][root][INFO] - Training Epoch: 4/10, step 219/574 completed (loss: 0.0359843447804451, acc: 1.0)
[2025-01-06 01:20:11,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:11,649][root][INFO] - Training Epoch: 4/10, step 220/574 completed (loss: 0.05595815181732178, acc: 0.9629629850387573)
[2025-01-06 01:20:11,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:12,014][root][INFO] - Training Epoch: 4/10, step 221/574 completed (loss: 0.18912586569786072, acc: 0.9599999785423279)
[2025-01-06 01:20:12,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:12,372][root][INFO] - Training Epoch: 4/10, step 222/574 completed (loss: 0.43739035725593567, acc: 0.9038461446762085)
[2025-01-06 01:20:12,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:13,129][root][INFO] - Training Epoch: 4/10, step 223/574 completed (loss: 0.3307238817214966, acc: 0.91847825050354)
[2025-01-06 01:20:13,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:13,668][root][INFO] - Training Epoch: 4/10, step 224/574 completed (loss: 0.519719660282135, acc: 0.8181818127632141)
[2025-01-06 01:20:13,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:14,112][root][INFO] - Training Epoch: 4/10, step 225/574 completed (loss: 0.5965326428413391, acc: 0.8085106611251831)
[2025-01-06 01:20:14,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:14,505][root][INFO] - Training Epoch: 4/10, step 226/574 completed (loss: 0.37341710925102234, acc: 0.9056603908538818)
[2025-01-06 01:20:14,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:14,856][root][INFO] - Training Epoch: 4/10, step 227/574 completed (loss: 0.3014659285545349, acc: 0.9166666865348816)
[2025-01-06 01:20:14,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:15,227][root][INFO] - Training Epoch: 4/10, step 228/574 completed (loss: 0.17822366952896118, acc: 0.9069767594337463)
[2025-01-06 01:20:15,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:15,560][root][INFO] - Training Epoch: 4/10, step 229/574 completed (loss: 0.6171354651451111, acc: 0.800000011920929)
[2025-01-06 01:20:15,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:15,972][root][INFO] - Training Epoch: 4/10, step 230/574 completed (loss: 1.6458402872085571, acc: 0.5684210658073425)
[2025-01-06 01:20:16,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:16,381][root][INFO] - Training Epoch: 4/10, step 231/574 completed (loss: 1.2535566091537476, acc: 0.644444465637207)
[2025-01-06 01:20:16,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:16,797][root][INFO] - Training Epoch: 4/10, step 232/574 completed (loss: 1.0760852098464966, acc: 0.699999988079071)
[2025-01-06 01:20:16,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:17,281][root][INFO] - Training Epoch: 4/10, step 233/574 completed (loss: 1.6066793203353882, acc: 0.536697268486023)
[2025-01-06 01:20:17,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:17,746][root][INFO] - Training Epoch: 4/10, step 234/574 completed (loss: 1.1541016101837158, acc: 0.6692307591438293)
[2025-01-06 01:20:17,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:18,100][root][INFO] - Training Epoch: 4/10, step 235/574 completed (loss: 0.2997646927833557, acc: 0.8947368264198303)
[2025-01-06 01:20:18,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:18,425][root][INFO] - Training Epoch: 4/10, step 236/574 completed (loss: 0.08273845165967941, acc: 1.0)
[2025-01-06 01:20:18,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:18,746][root][INFO] - Training Epoch: 4/10, step 237/574 completed (loss: 0.41701382398605347, acc: 0.8636363744735718)
[2025-01-06 01:20:18,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:19,123][root][INFO] - Training Epoch: 4/10, step 238/574 completed (loss: 0.4047975540161133, acc: 0.8888888955116272)
[2025-01-06 01:20:19,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:19,471][root][INFO] - Training Epoch: 4/10, step 239/574 completed (loss: 0.19042949378490448, acc: 0.9714285731315613)
[2025-01-06 01:20:19,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:19,862][root][INFO] - Training Epoch: 4/10, step 240/574 completed (loss: 0.6569918394088745, acc: 0.8863636255264282)
[2025-01-06 01:20:19,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:20,250][root][INFO] - Training Epoch: 4/10, step 241/574 completed (loss: 0.19773341715335846, acc: 0.9318181872367859)
[2025-01-06 01:20:20,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:20,824][root][INFO] - Training Epoch: 4/10, step 242/574 completed (loss: 0.6774840950965881, acc: 0.7903226017951965)
[2025-01-06 01:20:20,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:21,351][root][INFO] - Training Epoch: 4/10, step 243/574 completed (loss: 0.6391875743865967, acc: 0.8181818127632141)
[2025-01-06 01:20:21,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:21,650][root][INFO] - Training Epoch: 4/10, step 244/574 completed (loss: 0.028497235849499702, acc: 1.0)
[2025-01-06 01:20:21,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:21,939][root][INFO] - Training Epoch: 4/10, step 245/574 completed (loss: 0.1549498438835144, acc: 0.9615384340286255)
[2025-01-06 01:20:22,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:22,280][root][INFO] - Training Epoch: 4/10, step 246/574 completed (loss: 0.040901318192481995, acc: 0.9677419066429138)
[2025-01-06 01:20:22,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:22,639][root][INFO] - Training Epoch: 4/10, step 247/574 completed (loss: 0.12861087918281555, acc: 0.949999988079071)
[2025-01-06 01:20:22,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:23,043][root][INFO] - Training Epoch: 4/10, step 248/574 completed (loss: 0.12859965860843658, acc: 0.9459459185600281)
[2025-01-06 01:20:23,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:23,387][root][INFO] - Training Epoch: 4/10, step 249/574 completed (loss: 0.2745617628097534, acc: 0.9189189076423645)
[2025-01-06 01:20:23,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:23,770][root][INFO] - Training Epoch: 4/10, step 250/574 completed (loss: 0.029276402667164803, acc: 1.0)
[2025-01-06 01:20:23,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:24,115][root][INFO] - Training Epoch: 4/10, step 251/574 completed (loss: 0.13995306193828583, acc: 0.9411764740943909)
[2025-01-06 01:20:24,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:24,489][root][INFO] - Training Epoch: 4/10, step 252/574 completed (loss: 0.11648611724376678, acc: 0.9512194991111755)
[2025-01-06 01:20:24,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:24,852][root][INFO] - Training Epoch: 4/10, step 253/574 completed (loss: 0.003987269476056099, acc: 1.0)
[2025-01-06 01:20:24,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:25,217][root][INFO] - Training Epoch: 4/10, step 254/574 completed (loss: 0.01645890437066555, acc: 1.0)
[2025-01-06 01:20:25,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:25,593][root][INFO] - Training Epoch: 4/10, step 255/574 completed (loss: 0.010393165983259678, acc: 1.0)
[2025-01-06 01:20:25,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:25,968][root][INFO] - Training Epoch: 4/10, step 256/574 completed (loss: 0.20810075104236603, acc: 0.9473684430122375)
[2025-01-06 01:20:26,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:26,369][root][INFO] - Training Epoch: 4/10, step 257/574 completed (loss: 0.27890288829803467, acc: 0.9285714030265808)
[2025-01-06 01:20:26,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:26,777][root][INFO] - Training Epoch: 4/10, step 258/574 completed (loss: 0.18651674687862396, acc: 0.9473684430122375)
[2025-01-06 01:20:26,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:27,355][root][INFO] - Training Epoch: 4/10, step 259/574 completed (loss: 0.4420483112335205, acc: 0.8679245114326477)
[2025-01-06 01:20:27,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:27,931][root][INFO] - Training Epoch: 4/10, step 260/574 completed (loss: 0.24279151856899261, acc: 0.9166666865348816)
[2025-01-06 01:20:27,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:28,173][root][INFO] - Training Epoch: 4/10, step 261/574 completed (loss: 0.03913861885666847, acc: 1.0)
[2025-01-06 01:20:28,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:28,515][root][INFO] - Training Epoch: 4/10, step 262/574 completed (loss: 0.25006598234176636, acc: 0.9677419066429138)
[2025-01-06 01:20:28,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:28,911][root][INFO] - Training Epoch: 4/10, step 263/574 completed (loss: 0.8515480160713196, acc: 0.800000011920929)
[2025-01-06 01:20:29,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:29,267][root][INFO] - Training Epoch: 4/10, step 264/574 completed (loss: 0.6735844016075134, acc: 0.7291666865348816)
[2025-01-06 01:20:29,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:30,085][root][INFO] - Training Epoch: 4/10, step 265/574 completed (loss: 1.194926381111145, acc: 0.6959999799728394)
[2025-01-06 01:20:30,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:30,484][root][INFO] - Training Epoch: 4/10, step 266/574 completed (loss: 1.0903522968292236, acc: 0.7078651785850525)
[2025-01-06 01:20:30,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:30,848][root][INFO] - Training Epoch: 4/10, step 267/574 completed (loss: 0.5487020611763, acc: 0.8108108043670654)
[2025-01-06 01:20:30,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:31,290][root][INFO] - Training Epoch: 4/10, step 268/574 completed (loss: 0.5169104933738708, acc: 0.8103448152542114)
[2025-01-06 01:20:31,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:31,611][root][INFO] - Training Epoch: 4/10, step 269/574 completed (loss: 0.016308866441249847, acc: 1.0)
[2025-01-06 01:20:31,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:31,977][root][INFO] - Training Epoch: 4/10, step 270/574 completed (loss: 0.07240009307861328, acc: 1.0)
[2025-01-06 01:20:32,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:32,324][root][INFO] - Training Epoch: 4/10, step 271/574 completed (loss: 0.009807689115405083, acc: 1.0)
[2025-01-06 01:20:32,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:32,673][root][INFO] - Training Epoch: 4/10, step 272/574 completed (loss: 0.09545411914587021, acc: 0.9666666388511658)
[2025-01-06 01:20:32,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33,074][root][INFO] - Training Epoch: 4/10, step 273/574 completed (loss: 0.1424160599708557, acc: 0.9666666388511658)
[2025-01-06 01:20:33,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33,402][root][INFO] - Training Epoch: 4/10, step 274/574 completed (loss: 0.21571630239486694, acc: 0.9375)
[2025-01-06 01:20:33,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33,794][root][INFO] - Training Epoch: 4/10, step 275/574 completed (loss: 0.09034531563520432, acc: 1.0)
[2025-01-06 01:20:33,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:34,148][root][INFO] - Training Epoch: 4/10, step 276/574 completed (loss: 0.2652711272239685, acc: 0.931034505367279)
[2025-01-06 01:20:34,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:34,507][root][INFO] - Training Epoch: 4/10, step 277/574 completed (loss: 0.21901682019233704, acc: 0.9200000166893005)
[2025-01-06 01:20:34,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:34,872][root][INFO] - Training Epoch: 4/10, step 278/574 completed (loss: 0.5585406422615051, acc: 0.8510638475418091)
[2025-01-06 01:20:34,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:35,224][root][INFO] - Training Epoch: 4/10, step 279/574 completed (loss: 0.27524349093437195, acc: 0.8958333134651184)
[2025-01-06 01:20:35,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:36,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:36,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:36,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:37,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:37,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:38,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:38,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:38,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:39,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:39,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:39,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:40,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:40,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:40,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:41,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:41,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:41,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:42,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:42,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:42,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:43,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:43,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:43,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:44,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:44,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:44,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:45,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:45,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:46,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:46,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:46,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:47,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:47,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:47,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:48,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:48,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:50,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:50,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:51,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:51,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:51,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:52,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:52,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:52,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:53,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:53,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:53,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:54,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:54,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:54,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:55,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:55,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:55,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:55,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:56,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:56,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:57,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:57,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:57,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:58,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:58,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:58,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:59,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:59,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:00,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:00,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:00,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:01,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:01,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:01,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:03,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:03,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:04,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:04,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:04,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:05,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:05,842][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0713, device='cuda:0') eval_epoch_loss=tensor(0.7282, device='cuda:0') eval_epoch_acc=tensor(0.8149, device='cuda:0')
[2025-01-06 01:21:05,844][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:21:05,844][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:21:06,136][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_280_loss_0.7281871438026428/model.pt
[2025-01-06 01:21:06,145][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:21:06,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:06,541][root][INFO] - Training Epoch: 4/10, step 280/574 completed (loss: 0.15222173929214478, acc: 0.9318181872367859)
[2025-01-06 01:21:06,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:06,960][root][INFO] - Training Epoch: 4/10, step 281/574 completed (loss: 0.4450002908706665, acc: 0.8313252925872803)
[2025-01-06 01:21:07,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:07,322][root][INFO] - Training Epoch: 4/10, step 282/574 completed (loss: 0.7683567404747009, acc: 0.8055555820465088)
[2025-01-06 01:21:07,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:07,627][root][INFO] - Training Epoch: 4/10, step 283/574 completed (loss: 0.03555142506957054, acc: 1.0)
[2025-01-06 01:21:07,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:07,930][root][INFO] - Training Epoch: 4/10, step 284/574 completed (loss: 0.3964216411113739, acc: 0.9411764740943909)
[2025-01-06 01:21:08,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:08,264][root][INFO] - Training Epoch: 4/10, step 285/574 completed (loss: 0.07220914959907532, acc: 0.9750000238418579)
[2025-01-06 01:21:08,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:08,657][root][INFO] - Training Epoch: 4/10, step 286/574 completed (loss: 0.3607608675956726, acc: 0.90625)
[2025-01-06 01:21:08,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:09,019][root][INFO] - Training Epoch: 4/10, step 287/574 completed (loss: 0.4251275062561035, acc: 0.8799999952316284)
[2025-01-06 01:21:09,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:09,415][root][INFO] - Training Epoch: 4/10, step 288/574 completed (loss: 0.3022698760032654, acc: 0.8901098966598511)
[2025-01-06 01:21:09,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:09,785][root][INFO] - Training Epoch: 4/10, step 289/574 completed (loss: 0.35760948061943054, acc: 0.888198733329773)
[2025-01-06 01:21:09,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:10,172][root][INFO] - Training Epoch: 4/10, step 290/574 completed (loss: 0.4320169687271118, acc: 0.8917526006698608)
[2025-01-06 01:21:10,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:10,537][root][INFO] - Training Epoch: 4/10, step 291/574 completed (loss: 0.03621724620461464, acc: 1.0)
[2025-01-06 01:21:10,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:10,826][root][INFO] - Training Epoch: 4/10, step 292/574 completed (loss: 0.17612753808498383, acc: 0.9523809552192688)
[2025-01-06 01:21:10,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:11,228][root][INFO] - Training Epoch: 4/10, step 293/574 completed (loss: 0.12567827105522156, acc: 0.9655172228813171)
[2025-01-06 01:21:11,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:13,012][root][INFO] - Training Epoch: 4/10, step 294/574 completed (loss: 0.23976507782936096, acc: 0.9272727370262146)
[2025-01-06 01:21:13,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:14,058][root][INFO] - Training Epoch: 4/10, step 295/574 completed (loss: 0.6130373477935791, acc: 0.8453608155250549)
[2025-01-06 01:21:14,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:14,377][root][INFO] - Training Epoch: 4/10, step 296/574 completed (loss: 0.2190001755952835, acc: 0.8620689511299133)
[2025-01-06 01:21:14,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:14,737][root][INFO] - Training Epoch: 4/10, step 297/574 completed (loss: 0.016672981902956963, acc: 1.0)
[2025-01-06 01:21:14,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:15,043][root][INFO] - Training Epoch: 4/10, step 298/574 completed (loss: 0.16415934264659882, acc: 0.9473684430122375)
[2025-01-06 01:21:15,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:15,387][root][INFO] - Training Epoch: 4/10, step 299/574 completed (loss: 0.06936071068048477, acc: 0.9821428656578064)
[2025-01-06 01:21:15,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:15,753][root][INFO] - Training Epoch: 4/10, step 300/574 completed (loss: 0.2828173041343689, acc: 0.90625)
[2025-01-06 01:21:15,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:16,146][root][INFO] - Training Epoch: 4/10, step 301/574 completed (loss: 0.17218761146068573, acc: 0.9245283007621765)
[2025-01-06 01:21:16,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:16,514][root][INFO] - Training Epoch: 4/10, step 302/574 completed (loss: 0.015957552939653397, acc: 1.0)
[2025-01-06 01:21:16,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:16,870][root][INFO] - Training Epoch: 4/10, step 303/574 completed (loss: 0.05598403513431549, acc: 0.970588207244873)
[2025-01-06 01:21:16,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:17,201][root][INFO] - Training Epoch: 4/10, step 304/574 completed (loss: 0.0423211008310318, acc: 1.0)
[2025-01-06 01:21:17,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:17,565][root][INFO] - Training Epoch: 4/10, step 305/574 completed (loss: 0.2123013287782669, acc: 0.9180327653884888)
[2025-01-06 01:21:17,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:17,891][root][INFO] - Training Epoch: 4/10, step 306/574 completed (loss: 0.060674652457237244, acc: 1.0)
[2025-01-06 01:21:17,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:18,225][root][INFO] - Training Epoch: 4/10, step 307/574 completed (loss: 0.004761653486639261, acc: 1.0)
[2025-01-06 01:21:18,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:18,612][root][INFO] - Training Epoch: 4/10, step 308/574 completed (loss: 0.20445966720581055, acc: 0.9275362491607666)
[2025-01-06 01:21:18,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:19,009][root][INFO] - Training Epoch: 4/10, step 309/574 completed (loss: 0.09994117170572281, acc: 0.9722222089767456)
[2025-01-06 01:21:19,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:19,391][root][INFO] - Training Epoch: 4/10, step 310/574 completed (loss: 0.0937582328915596, acc: 0.9759036302566528)
[2025-01-06 01:21:19,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:19,730][root][INFO] - Training Epoch: 4/10, step 311/574 completed (loss: 0.15515699982643127, acc: 0.9615384340286255)
[2025-01-06 01:21:19,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:20,076][root][INFO] - Training Epoch: 4/10, step 312/574 completed (loss: 0.1024276614189148, acc: 0.9795918464660645)
[2025-01-06 01:21:20,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:20,430][root][INFO] - Training Epoch: 4/10, step 313/574 completed (loss: 0.004831863567233086, acc: 1.0)
[2025-01-06 01:21:20,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:20,775][root][INFO] - Training Epoch: 4/10, step 314/574 completed (loss: 0.005703693721443415, acc: 1.0)
[2025-01-06 01:21:20,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:21,144][root][INFO] - Training Epoch: 4/10, step 315/574 completed (loss: 0.16618698835372925, acc: 0.9677419066429138)
[2025-01-06 01:21:21,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:21,486][root][INFO] - Training Epoch: 4/10, step 316/574 completed (loss: 0.33269813656806946, acc: 0.8709677457809448)
[2025-01-06 01:21:21,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:21,836][root][INFO] - Training Epoch: 4/10, step 317/574 completed (loss: 0.180448979139328, acc: 0.9253731369972229)
[2025-01-06 01:21:21,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:22,158][root][INFO] - Training Epoch: 4/10, step 318/574 completed (loss: 0.10186808556318283, acc: 0.9615384340286255)
[2025-01-06 01:21:22,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:22,438][root][INFO] - Training Epoch: 4/10, step 319/574 completed (loss: 0.16408155858516693, acc: 0.9333333373069763)
[2025-01-06 01:21:22,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:22,798][root][INFO] - Training Epoch: 4/10, step 320/574 completed (loss: 0.05968225374817848, acc: 0.9677419066429138)
[2025-01-06 01:21:22,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:23,126][root][INFO] - Training Epoch: 4/10, step 321/574 completed (loss: 0.006041589193046093, acc: 1.0)
[2025-01-06 01:21:23,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:23,458][root][INFO] - Training Epoch: 4/10, step 322/574 completed (loss: 0.7607033252716064, acc: 0.7777777910232544)
[2025-01-06 01:21:23,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:23,773][root][INFO] - Training Epoch: 4/10, step 323/574 completed (loss: 0.641832709312439, acc: 0.8285714387893677)
[2025-01-06 01:21:23,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:24,068][root][INFO] - Training Epoch: 4/10, step 324/574 completed (loss: 0.5186849236488342, acc: 0.8717948794364929)
[2025-01-06 01:21:24,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:24,435][root][INFO] - Training Epoch: 4/10, step 325/574 completed (loss: 0.7182384133338928, acc: 0.7317073345184326)
[2025-01-06 01:21:24,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:24,835][root][INFO] - Training Epoch: 4/10, step 326/574 completed (loss: 0.3662148714065552, acc: 0.8157894611358643)
[2025-01-06 01:21:24,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:25,203][root][INFO] - Training Epoch: 4/10, step 327/574 completed (loss: 0.038249943405389786, acc: 1.0)
[2025-01-06 01:21:25,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:25,537][root][INFO] - Training Epoch: 4/10, step 328/574 completed (loss: 0.010504183359444141, acc: 1.0)
[2025-01-06 01:21:25,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:25,868][root][INFO] - Training Epoch: 4/10, step 329/574 completed (loss: 0.12264353781938553, acc: 0.9629629850387573)
[2025-01-06 01:21:25,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:26,164][root][INFO] - Training Epoch: 4/10, step 330/574 completed (loss: 0.003111542435362935, acc: 1.0)
[2025-01-06 01:21:26,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:26,466][root][INFO] - Training Epoch: 4/10, step 331/574 completed (loss: 0.28246772289276123, acc: 0.9516128897666931)
[2025-01-06 01:21:26,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:26,801][root][INFO] - Training Epoch: 4/10, step 332/574 completed (loss: 0.017503833398222923, acc: 1.0)
[2025-01-06 01:21:26,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27,195][root][INFO] - Training Epoch: 4/10, step 333/574 completed (loss: 0.14382131397724152, acc: 0.96875)
[2025-01-06 01:21:27,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27,549][root][INFO] - Training Epoch: 4/10, step 334/574 completed (loss: 0.014750657603144646, acc: 1.0)
[2025-01-06 01:21:27,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27,877][root][INFO] - Training Epoch: 4/10, step 335/574 completed (loss: 0.17064766585826874, acc: 0.8947368264198303)
[2025-01-06 01:21:27,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:28,219][root][INFO] - Training Epoch: 4/10, step 336/574 completed (loss: 0.30788227915763855, acc: 0.8999999761581421)
[2025-01-06 01:21:28,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:28,565][root][INFO] - Training Epoch: 4/10, step 337/574 completed (loss: 0.6801286339759827, acc: 0.7471264600753784)
[2025-01-06 01:21:28,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:28,926][root][INFO] - Training Epoch: 4/10, step 338/574 completed (loss: 1.007913589477539, acc: 0.6914893388748169)
[2025-01-06 01:21:29,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:29,294][root][INFO] - Training Epoch: 4/10, step 339/574 completed (loss: 0.8326373100280762, acc: 0.759036123752594)
[2025-01-06 01:21:29,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:29,614][root][INFO] - Training Epoch: 4/10, step 340/574 completed (loss: 0.00480172224342823, acc: 1.0)
[2025-01-06 01:21:29,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:29,955][root][INFO] - Training Epoch: 4/10, step 341/574 completed (loss: 0.1382802277803421, acc: 0.9230769276618958)
[2025-01-06 01:21:30,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:30,330][root][INFO] - Training Epoch: 4/10, step 342/574 completed (loss: 0.12498508393764496, acc: 0.9638554453849792)
[2025-01-06 01:21:30,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:30,695][root][INFO] - Training Epoch: 4/10, step 343/574 completed (loss: 0.3145267963409424, acc: 0.9056603908538818)
[2025-01-06 01:21:30,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:31,044][root][INFO] - Training Epoch: 4/10, step 344/574 completed (loss: 0.1282031089067459, acc: 0.9620253443717957)
[2025-01-06 01:21:31,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:31,411][root][INFO] - Training Epoch: 4/10, step 345/574 completed (loss: 0.05416213721036911, acc: 0.9803921580314636)
[2025-01-06 01:21:31,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:31,741][root][INFO] - Training Epoch: 4/10, step 346/574 completed (loss: 0.21543055772781372, acc: 0.9552238583564758)
[2025-01-06 01:21:31,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:32,060][root][INFO] - Training Epoch: 4/10, step 347/574 completed (loss: 0.0463738739490509, acc: 0.949999988079071)
[2025-01-06 01:21:32,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:32,389][root][INFO] - Training Epoch: 4/10, step 348/574 completed (loss: 0.0901418998837471, acc: 0.9599999785423279)
[2025-01-06 01:21:32,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:32,754][root][INFO] - Training Epoch: 4/10, step 349/574 completed (loss: 0.4509819746017456, acc: 0.8611111044883728)
[2025-01-06 01:21:32,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:33,125][root][INFO] - Training Epoch: 4/10, step 350/574 completed (loss: 0.6013359427452087, acc: 0.7906976938247681)
[2025-01-06 01:21:33,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:33,467][root][INFO] - Training Epoch: 4/10, step 351/574 completed (loss: 0.0982942208647728, acc: 0.9743589758872986)
[2025-01-06 01:21:33,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:33,831][root][INFO] - Training Epoch: 4/10, step 352/574 completed (loss: 0.6148399710655212, acc: 0.7555555701255798)
[2025-01-06 01:21:33,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:34,156][root][INFO] - Training Epoch: 4/10, step 353/574 completed (loss: 0.014246582984924316, acc: 1.0)
[2025-01-06 01:21:34,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:34,460][root][INFO] - Training Epoch: 4/10, step 354/574 completed (loss: 0.40411561727523804, acc: 0.8846153616905212)
[2025-01-06 01:21:34,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:34,797][root][INFO] - Training Epoch: 4/10, step 355/574 completed (loss: 0.7333722114562988, acc: 0.8021978139877319)
[2025-01-06 01:21:34,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:35,296][root][INFO] - Training Epoch: 4/10, step 356/574 completed (loss: 0.43997544050216675, acc: 0.843478262424469)
[2025-01-06 01:21:35,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:35,665][root][INFO] - Training Epoch: 4/10, step 357/574 completed (loss: 0.31581711769104004, acc: 0.9021739363670349)
[2025-01-06 01:21:35,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:36,016][root][INFO] - Training Epoch: 4/10, step 358/574 completed (loss: 0.26962482929229736, acc: 0.9591836929321289)
[2025-01-06 01:21:36,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:36,368][root][INFO] - Training Epoch: 4/10, step 359/574 completed (loss: 0.0011892591137439013, acc: 1.0)
[2025-01-06 01:21:36,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:36,680][root][INFO] - Training Epoch: 4/10, step 360/574 completed (loss: 0.11493802070617676, acc: 0.9230769276618958)
[2025-01-06 01:21:36,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37,000][root][INFO] - Training Epoch: 4/10, step 361/574 completed (loss: 0.27374041080474854, acc: 0.8780487775802612)
[2025-01-06 01:21:37,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37,315][root][INFO] - Training Epoch: 4/10, step 362/574 completed (loss: 0.5707768201828003, acc: 0.8666666746139526)
[2025-01-06 01:21:37,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37,615][root][INFO] - Training Epoch: 4/10, step 363/574 completed (loss: 0.07340347766876221, acc: 0.9868420958518982)
[2025-01-06 01:21:37,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37,913][root][INFO] - Training Epoch: 4/10, step 364/574 completed (loss: 0.04781655594706535, acc: 1.0)
[2025-01-06 01:21:37,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:38,251][root][INFO] - Training Epoch: 4/10, step 365/574 completed (loss: 0.041060108691453934, acc: 1.0)
[2025-01-06 01:21:38,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:38,612][root][INFO] - Training Epoch: 4/10, step 366/574 completed (loss: 0.185704305768013, acc: 0.9583333134651184)
[2025-01-06 01:21:38,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:38,951][root][INFO] - Training Epoch: 4/10, step 367/574 completed (loss: 0.00156989018432796, acc: 1.0)
[2025-01-06 01:21:39,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:39,300][root][INFO] - Training Epoch: 4/10, step 368/574 completed (loss: 0.03659499064087868, acc: 1.0)
[2025-01-06 01:21:39,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:39,638][root][INFO] - Training Epoch: 4/10, step 369/574 completed (loss: 0.27371665835380554, acc: 0.90625)
[2025-01-06 01:21:39,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:40,256][root][INFO] - Training Epoch: 4/10, step 370/574 completed (loss: 0.36944523453712463, acc: 0.903030276298523)
[2025-01-06 01:21:40,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:41,125][root][INFO] - Training Epoch: 4/10, step 371/574 completed (loss: 0.2494678497314453, acc: 0.9150943160057068)
[2025-01-06 01:21:41,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:41,487][root][INFO] - Training Epoch: 4/10, step 372/574 completed (loss: 0.14909440279006958, acc: 0.9555555582046509)
[2025-01-06 01:21:41,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:41,831][root][INFO] - Training Epoch: 4/10, step 373/574 completed (loss: 0.09719843417406082, acc: 0.9821428656578064)
[2025-01-06 01:21:41,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:42,153][root][INFO] - Training Epoch: 4/10, step 374/574 completed (loss: 0.048467688262462616, acc: 1.0)
[2025-01-06 01:21:42,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:42,490][root][INFO] - Training Epoch: 4/10, step 375/574 completed (loss: 0.0038903376553207636, acc: 1.0)
[2025-01-06 01:21:42,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:42,795][root][INFO] - Training Epoch: 4/10, step 376/574 completed (loss: 0.04414154216647148, acc: 0.95652174949646)
[2025-01-06 01:21:42,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:43,142][root][INFO] - Training Epoch: 4/10, step 377/574 completed (loss: 0.037184346467256546, acc: 1.0)
[2025-01-06 01:21:43,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:43,513][root][INFO] - Training Epoch: 4/10, step 378/574 completed (loss: 0.012583471834659576, acc: 1.0)
[2025-01-06 01:21:43,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:44,104][root][INFO] - Training Epoch: 4/10, step 379/574 completed (loss: 0.2492944449186325, acc: 0.916167676448822)
[2025-01-06 01:21:44,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:44,542][root][INFO] - Training Epoch: 4/10, step 380/574 completed (loss: 0.3528273105621338, acc: 0.9097744226455688)
[2025-01-06 01:21:44,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:45,764][root][INFO] - Training Epoch: 4/10, step 381/574 completed (loss: 0.4913289248943329, acc: 0.8449198007583618)
[2025-01-06 01:21:45,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:46,350][root][INFO] - Training Epoch: 4/10, step 382/574 completed (loss: 0.1046077162027359, acc: 0.954954981803894)
[2025-01-06 01:21:46,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:46,729][root][INFO] - Training Epoch: 4/10, step 383/574 completed (loss: 0.014304899610579014, acc: 1.0)
[2025-01-06 01:21:46,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:47,057][root][INFO] - Training Epoch: 4/10, step 384/574 completed (loss: 0.003953320439904928, acc: 1.0)
[2025-01-06 01:21:47,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:47,388][root][INFO] - Training Epoch: 4/10, step 385/574 completed (loss: 0.010528836399316788, acc: 1.0)
[2025-01-06 01:21:47,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:47,731][root][INFO] - Training Epoch: 4/10, step 386/574 completed (loss: 0.0016149583971127868, acc: 1.0)
[2025-01-06 01:21:47,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:48,054][root][INFO] - Training Epoch: 4/10, step 387/574 completed (loss: 0.04081099107861519, acc: 0.9736841917037964)
[2025-01-06 01:21:48,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:48,383][root][INFO] - Training Epoch: 4/10, step 388/574 completed (loss: 0.1268429458141327, acc: 0.9545454382896423)
[2025-01-06 01:21:48,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:48,715][root][INFO] - Training Epoch: 4/10, step 389/574 completed (loss: 0.002009653951972723, acc: 1.0)
[2025-01-06 01:21:48,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:49,036][root][INFO] - Training Epoch: 4/10, step 390/574 completed (loss: 0.3262891173362732, acc: 0.9047619104385376)
[2025-01-06 01:21:49,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:49,380][root][INFO] - Training Epoch: 4/10, step 391/574 completed (loss: 0.4982603192329407, acc: 0.8148148059844971)
[2025-01-06 01:21:49,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:49,740][root][INFO] - Training Epoch: 4/10, step 392/574 completed (loss: 0.6785101294517517, acc: 0.8155339956283569)
[2025-01-06 01:21:49,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:50,283][root][INFO] - Training Epoch: 4/10, step 393/574 completed (loss: 0.7651153802871704, acc: 0.8161764740943909)
[2025-01-06 01:21:50,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:50,704][root][INFO] - Training Epoch: 4/10, step 394/574 completed (loss: 0.5320213437080383, acc: 0.8199999928474426)
[2025-01-06 01:21:50,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:51,111][root][INFO] - Training Epoch: 4/10, step 395/574 completed (loss: 0.5320644378662109, acc: 0.8402777910232544)
[2025-01-06 01:21:51,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:51,526][root][INFO] - Training Epoch: 4/10, step 396/574 completed (loss: 0.08636265993118286, acc: 1.0)
[2025-01-06 01:21:51,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:51,899][root][INFO] - Training Epoch: 4/10, step 397/574 completed (loss: 0.049033407121896744, acc: 0.9583333134651184)
[2025-01-06 01:21:51,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:52,251][root][INFO] - Training Epoch: 4/10, step 398/574 completed (loss: 0.16126365959644318, acc: 0.9534883499145508)
[2025-01-06 01:21:52,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:52,551][root][INFO] - Training Epoch: 4/10, step 399/574 completed (loss: 0.032127827405929565, acc: 1.0)
[2025-01-06 01:21:52,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:53,081][root][INFO] - Training Epoch: 4/10, step 400/574 completed (loss: 0.22477920353412628, acc: 0.9264705777168274)
[2025-01-06 01:21:53,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:53,402][root][INFO] - Training Epoch: 4/10, step 401/574 completed (loss: 0.2285977452993393, acc: 0.9200000166893005)
[2025-01-06 01:21:53,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:53,708][root][INFO] - Training Epoch: 4/10, step 402/574 completed (loss: 0.2076028436422348, acc: 0.9090909361839294)
[2025-01-06 01:21:53,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:54,072][root][INFO] - Training Epoch: 4/10, step 403/574 completed (loss: 0.021198315545916557, acc: 1.0)
[2025-01-06 01:21:54,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:54,458][root][INFO] - Training Epoch: 4/10, step 404/574 completed (loss: 0.06027401238679886, acc: 1.0)
[2025-01-06 01:21:54,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:54,793][root][INFO] - Training Epoch: 4/10, step 405/574 completed (loss: 0.05890626087784767, acc: 0.9629629850387573)
[2025-01-06 01:21:54,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:55,127][root][INFO] - Training Epoch: 4/10, step 406/574 completed (loss: 0.09812594205141068, acc: 0.9599999785423279)
[2025-01-06 01:21:55,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:55,434][root][INFO] - Training Epoch: 4/10, step 407/574 completed (loss: 0.01433519646525383, acc: 1.0)
[2025-01-06 01:21:55,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:55,735][root][INFO] - Training Epoch: 4/10, step 408/574 completed (loss: 0.034168973565101624, acc: 1.0)
[2025-01-06 01:21:55,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56,046][root][INFO] - Training Epoch: 4/10, step 409/574 completed (loss: 0.054039184004068375, acc: 0.9615384340286255)
[2025-01-06 01:21:56,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56,360][root][INFO] - Training Epoch: 4/10, step 410/574 completed (loss: 0.058832354843616486, acc: 0.982758641242981)
[2025-01-06 01:21:56,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56,657][root][INFO] - Training Epoch: 4/10, step 411/574 completed (loss: 0.008215324021875858, acc: 1.0)
[2025-01-06 01:21:56,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56,957][root][INFO] - Training Epoch: 4/10, step 412/574 completed (loss: 0.06851939857006073, acc: 1.0)
[2025-01-06 01:21:57,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:57,248][root][INFO] - Training Epoch: 4/10, step 413/574 completed (loss: 0.024616921320557594, acc: 1.0)
[2025-01-06 01:21:57,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:57,573][root][INFO] - Training Epoch: 4/10, step 414/574 completed (loss: 0.22863611578941345, acc: 0.9545454382896423)
[2025-01-06 01:21:57,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:57,893][root][INFO] - Training Epoch: 4/10, step 415/574 completed (loss: 0.29507648944854736, acc: 0.8823529481887817)
[2025-01-06 01:21:57,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:58,198][root][INFO] - Training Epoch: 4/10, step 416/574 completed (loss: 0.03826937451958656, acc: 1.0)
[2025-01-06 01:21:58,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:58,505][root][INFO] - Training Epoch: 4/10, step 417/574 completed (loss: 0.0387914702296257, acc: 1.0)
[2025-01-06 01:21:58,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:58,828][root][INFO] - Training Epoch: 4/10, step 418/574 completed (loss: 0.06389330327510834, acc: 0.9750000238418579)
[2025-01-06 01:21:58,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:59,160][root][INFO] - Training Epoch: 4/10, step 419/574 completed (loss: 0.039621688425540924, acc: 1.0)
[2025-01-06 01:21:59,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:59,492][root][INFO] - Training Epoch: 4/10, step 420/574 completed (loss: 0.021754834800958633, acc: 1.0)
[2025-01-06 01:21:59,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:59,803][root][INFO] - Training Epoch: 4/10, step 421/574 completed (loss: 0.03892768546938896, acc: 1.0)
[2025-01-06 01:21:59,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:00,140][root][INFO] - Training Epoch: 4/10, step 422/574 completed (loss: 0.11125194281339645, acc: 0.96875)
[2025-01-06 01:22:00,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:01,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:01,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:01,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:03,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:03,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:03,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:04,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:04,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:04,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:05,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:05,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:05,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:06,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:06,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:06,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:08,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:08,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:08,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:10,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:10,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:10,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:11,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:11,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:11,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:12,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:12,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:13,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:13,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:13,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:14,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:14,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:15,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:15,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:15,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:18,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:18,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:18,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:19,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:19,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:19,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:20,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:20,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:20,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:21,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:21,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:21,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:22,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:22,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:22,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:23,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:23,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:23,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:23,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:24,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:24,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:24,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:25,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:25,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:25,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:26,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:26,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:26,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:27,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:27,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:28,157][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2197, device='cuda:0') eval_epoch_loss=tensor(0.7974, device='cuda:0') eval_epoch_acc=tensor(0.8197, device='cuda:0')
[2025-01-06 01:22:28,159][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:22:28,160][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:22:28,430][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_423_loss_0.7973592281341553/model.pt
[2025-01-06 01:22:28,437][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:22:28,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:28,889][root][INFO] - Training Epoch: 4/10, step 423/574 completed (loss: 0.06323947012424469, acc: 1.0)
[2025-01-06 01:22:28,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:29,231][root][INFO] - Training Epoch: 4/10, step 424/574 completed (loss: 0.12017863243818283, acc: 0.9629629850387573)
[2025-01-06 01:22:29,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:29,551][root][INFO] - Training Epoch: 4/10, step 425/574 completed (loss: 0.35735613107681274, acc: 0.9696969985961914)
[2025-01-06 01:22:29,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:29,877][root][INFO] - Training Epoch: 4/10, step 426/574 completed (loss: 0.0023238311987370253, acc: 1.0)
[2025-01-06 01:22:29,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:30,198][root][INFO] - Training Epoch: 4/10, step 427/574 completed (loss: 0.07662884891033173, acc: 0.9729729890823364)
[2025-01-06 01:22:30,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:30,512][root][INFO] - Training Epoch: 4/10, step 428/574 completed (loss: 0.024895118549466133, acc: 1.0)
[2025-01-06 01:22:30,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:30,837][root][INFO] - Training Epoch: 4/10, step 429/574 completed (loss: 0.012890007346868515, acc: 1.0)
[2025-01-06 01:22:30,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:31,146][root][INFO] - Training Epoch: 4/10, step 430/574 completed (loss: 0.0008905597496777773, acc: 1.0)
[2025-01-06 01:22:31,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:31,446][root][INFO] - Training Epoch: 4/10, step 431/574 completed (loss: 0.004207609221339226, acc: 1.0)
[2025-01-06 01:22:31,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:31,758][root][INFO] - Training Epoch: 4/10, step 432/574 completed (loss: 0.21018363535404205, acc: 0.9130434989929199)
[2025-01-06 01:22:31,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:32,107][root][INFO] - Training Epoch: 4/10, step 433/574 completed (loss: 0.06134774163365364, acc: 0.9722222089767456)
[2025-01-06 01:22:32,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:32,405][root][INFO] - Training Epoch: 4/10, step 434/574 completed (loss: 0.006480048876255751, acc: 1.0)
[2025-01-06 01:22:32,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:32,726][root][INFO] - Training Epoch: 4/10, step 435/574 completed (loss: 0.015198580920696259, acc: 1.0)
[2025-01-06 01:22:32,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33,046][root][INFO] - Training Epoch: 4/10, step 436/574 completed (loss: 0.04134620353579521, acc: 1.0)
[2025-01-06 01:22:33,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33,349][root][INFO] - Training Epoch: 4/10, step 437/574 completed (loss: 0.10889771580696106, acc: 0.9772727489471436)
[2025-01-06 01:22:33,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33,629][root][INFO] - Training Epoch: 4/10, step 438/574 completed (loss: 0.000863320310600102, acc: 1.0)
[2025-01-06 01:22:33,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33,962][root][INFO] - Training Epoch: 4/10, step 439/574 completed (loss: 0.03861968219280243, acc: 1.0)
[2025-01-06 01:22:34,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:34,427][root][INFO] - Training Epoch: 4/10, step 440/574 completed (loss: 0.2170870304107666, acc: 0.9242424368858337)
[2025-01-06 01:22:34,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:35,116][root][INFO] - Training Epoch: 4/10, step 441/574 completed (loss: 0.6236439347267151, acc: 0.8320000171661377)
[2025-01-06 01:22:35,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:35,530][root][INFO] - Training Epoch: 4/10, step 442/574 completed (loss: 0.6185793876647949, acc: 0.8145161271095276)
[2025-01-06 01:22:35,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:36,192][root][INFO] - Training Epoch: 4/10, step 443/574 completed (loss: 0.36474883556365967, acc: 0.9004974961280823)
[2025-01-06 01:22:36,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:36,503][root][INFO] - Training Epoch: 4/10, step 444/574 completed (loss: 0.11292330920696259, acc: 0.9433962106704712)
[2025-01-06 01:22:36,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:36,916][root][INFO] - Training Epoch: 4/10, step 445/574 completed (loss: 0.05550414323806763, acc: 1.0)
[2025-01-06 01:22:36,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:37,212][root][INFO] - Training Epoch: 4/10, step 446/574 completed (loss: 0.20715077221393585, acc: 0.8695651888847351)
[2025-01-06 01:22:37,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:37,555][root][INFO] - Training Epoch: 4/10, step 447/574 completed (loss: 0.017618926241993904, acc: 1.0)
[2025-01-06 01:22:37,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:37,875][root][INFO] - Training Epoch: 4/10, step 448/574 completed (loss: 0.024911612272262573, acc: 1.0)
[2025-01-06 01:22:37,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:38,186][root][INFO] - Training Epoch: 4/10, step 449/574 completed (loss: 0.07261493057012558, acc: 0.9850746393203735)
[2025-01-06 01:22:38,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:38,504][root][INFO] - Training Epoch: 4/10, step 450/574 completed (loss: 0.05327631160616875, acc: 0.9722222089767456)
[2025-01-06 01:22:38,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:38,812][root][INFO] - Training Epoch: 4/10, step 451/574 completed (loss: 0.018603742122650146, acc: 1.0)
[2025-01-06 01:22:38,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:39,150][root][INFO] - Training Epoch: 4/10, step 452/574 completed (loss: 0.11896369606256485, acc: 0.9743589758872986)
[2025-01-06 01:22:39,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:39,467][root][INFO] - Training Epoch: 4/10, step 453/574 completed (loss: 0.19282881915569305, acc: 0.9210526347160339)
[2025-01-06 01:22:39,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:39,781][root][INFO] - Training Epoch: 4/10, step 454/574 completed (loss: 0.03639229014515877, acc: 0.9795918464660645)
[2025-01-06 01:22:39,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:40,115][root][INFO] - Training Epoch: 4/10, step 455/574 completed (loss: 0.09566587209701538, acc: 0.939393937587738)
[2025-01-06 01:22:40,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:40,441][root][INFO] - Training Epoch: 4/10, step 456/574 completed (loss: 0.37826940417289734, acc: 0.907216489315033)
[2025-01-06 01:22:40,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:40,753][root][INFO] - Training Epoch: 4/10, step 457/574 completed (loss: 0.03848157823085785, acc: 0.9857142567634583)
[2025-01-06 01:22:40,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:41,119][root][INFO] - Training Epoch: 4/10, step 458/574 completed (loss: 0.32179611921310425, acc: 0.9069767594337463)
[2025-01-06 01:22:41,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:41,412][root][INFO] - Training Epoch: 4/10, step 459/574 completed (loss: 0.0842776969075203, acc: 0.9821428656578064)
[2025-01-06 01:22:41,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:41,722][root][INFO] - Training Epoch: 4/10, step 460/574 completed (loss: 0.0931936651468277, acc: 0.9629629850387573)
[2025-01-06 01:22:41,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:42,020][root][INFO] - Training Epoch: 4/10, step 461/574 completed (loss: 0.0910695344209671, acc: 0.9444444179534912)
[2025-01-06 01:22:42,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:42,313][root][INFO] - Training Epoch: 4/10, step 462/574 completed (loss: 0.20738749206066132, acc: 0.9375)
[2025-01-06 01:22:42,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:42,653][root][INFO] - Training Epoch: 4/10, step 463/574 completed (loss: 0.10574112832546234, acc: 0.9615384340286255)
[2025-01-06 01:22:42,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:43,014][root][INFO] - Training Epoch: 4/10, step 464/574 completed (loss: 0.2892673909664154, acc: 0.9347826242446899)
[2025-01-06 01:22:43,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:43,362][root][INFO] - Training Epoch: 4/10, step 465/574 completed (loss: 0.1950216293334961, acc: 0.9404761791229248)
[2025-01-06 01:22:43,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:43,706][root][INFO] - Training Epoch: 4/10, step 466/574 completed (loss: 0.4034315049648285, acc: 0.8795180916786194)
[2025-01-06 01:22:43,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:44,069][root][INFO] - Training Epoch: 4/10, step 467/574 completed (loss: 0.11726292967796326, acc: 0.9729729890823364)
[2025-01-06 01:22:44,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:44,411][root][INFO] - Training Epoch: 4/10, step 468/574 completed (loss: 0.39935368299484253, acc: 0.8737863898277283)
[2025-01-06 01:22:44,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:44,764][root][INFO] - Training Epoch: 4/10, step 469/574 completed (loss: 0.29284870624542236, acc: 0.9512194991111755)
[2025-01-06 01:22:44,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:45,094][root][INFO] - Training Epoch: 4/10, step 470/574 completed (loss: 0.03823625296354294, acc: 1.0)
[2025-01-06 01:22:45,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:45,460][root][INFO] - Training Epoch: 4/10, step 471/574 completed (loss: 0.1314634084701538, acc: 0.9642857313156128)
[2025-01-06 01:22:45,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:45,869][root][INFO] - Training Epoch: 4/10, step 472/574 completed (loss: 0.20980766415596008, acc: 0.9313725233078003)
[2025-01-06 01:22:46,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:46,270][root][INFO] - Training Epoch: 4/10, step 473/574 completed (loss: 0.5958543419837952, acc: 0.8253275156021118)
[2025-01-06 01:22:46,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:46,617][root][INFO] - Training Epoch: 4/10, step 474/574 completed (loss: 0.22833800315856934, acc: 0.8958333134651184)
[2025-01-06 01:22:46,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:46,949][root][INFO] - Training Epoch: 4/10, step 475/574 completed (loss: 0.36633461713790894, acc: 0.89570552110672)
[2025-01-06 01:22:47,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:47,271][root][INFO] - Training Epoch: 4/10, step 476/574 completed (loss: 0.40893521904945374, acc: 0.8776978254318237)
[2025-01-06 01:22:47,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:47,608][root][INFO] - Training Epoch: 4/10, step 477/574 completed (loss: 0.6551988124847412, acc: 0.7839195728302002)
[2025-01-06 01:22:47,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:47,912][root][INFO] - Training Epoch: 4/10, step 478/574 completed (loss: 0.2099432349205017, acc: 0.8888888955116272)
[2025-01-06 01:22:47,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:48,225][root][INFO] - Training Epoch: 4/10, step 479/574 completed (loss: 0.13191451132297516, acc: 0.939393937587738)
[2025-01-06 01:22:48,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:48,556][root][INFO] - Training Epoch: 4/10, step 480/574 completed (loss: 0.1959751695394516, acc: 0.9259259104728699)
[2025-01-06 01:22:48,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:48,870][root][INFO] - Training Epoch: 4/10, step 481/574 completed (loss: 0.032229986041784286, acc: 1.0)
[2025-01-06 01:22:48,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:49,185][root][INFO] - Training Epoch: 4/10, step 482/574 completed (loss: 0.38581138849258423, acc: 0.8500000238418579)
[2025-01-06 01:22:49,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:49,552][root][INFO] - Training Epoch: 4/10, step 483/574 completed (loss: 0.4260374903678894, acc: 0.8620689511299133)
[2025-01-06 01:22:49,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:49,853][root][INFO] - Training Epoch: 4/10, step 484/574 completed (loss: 0.07956818491220474, acc: 0.9677419066429138)
[2025-01-06 01:22:49,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:50,149][root][INFO] - Training Epoch: 4/10, step 485/574 completed (loss: 0.012035146355628967, acc: 1.0)
[2025-01-06 01:22:50,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:50,435][root][INFO] - Training Epoch: 4/10, step 486/574 completed (loss: 0.10299853980541229, acc: 1.0)
[2025-01-06 01:22:50,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:50,721][root][INFO] - Training Epoch: 4/10, step 487/574 completed (loss: 0.30103492736816406, acc: 0.8095238208770752)
[2025-01-06 01:22:50,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51,017][root][INFO] - Training Epoch: 4/10, step 488/574 completed (loss: 0.07112368941307068, acc: 0.9545454382896423)
[2025-01-06 01:22:51,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51,347][root][INFO] - Training Epoch: 4/10, step 489/574 completed (loss: 0.6194183230400085, acc: 0.800000011920929)
[2025-01-06 01:22:51,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51,644][root][INFO] - Training Epoch: 4/10, step 490/574 completed (loss: 0.05165845900774002, acc: 1.0)
[2025-01-06 01:22:51,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51,952][root][INFO] - Training Epoch: 4/10, step 491/574 completed (loss: 0.07308417558670044, acc: 0.9655172228813171)
[2025-01-06 01:22:52,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:52,245][root][INFO] - Training Epoch: 4/10, step 492/574 completed (loss: 0.15894654393196106, acc: 0.9019607901573181)
[2025-01-06 01:22:52,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:52,545][root][INFO] - Training Epoch: 4/10, step 493/574 completed (loss: 0.1777518391609192, acc: 0.931034505367279)
[2025-01-06 01:22:52,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:52,825][root][INFO] - Training Epoch: 4/10, step 494/574 completed (loss: 0.3206091523170471, acc: 0.9473684430122375)
[2025-01-06 01:22:52,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:53,119][root][INFO] - Training Epoch: 4/10, step 495/574 completed (loss: 0.24434785544872284, acc: 0.9473684430122375)
[2025-01-06 01:22:53,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:53,443][root][INFO] - Training Epoch: 4/10, step 496/574 completed (loss: 0.41561177372932434, acc: 0.9107142686843872)
[2025-01-06 01:22:53,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:53,828][root][INFO] - Training Epoch: 4/10, step 497/574 completed (loss: 0.2872997522354126, acc: 0.898876428604126)
[2025-01-06 01:22:53,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:54,172][root][INFO] - Training Epoch: 4/10, step 498/574 completed (loss: 0.4714609980583191, acc: 0.8764045238494873)
[2025-01-06 01:22:54,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:54,494][root][INFO] - Training Epoch: 4/10, step 499/574 completed (loss: 0.8859103918075562, acc: 0.7234042286872864)
[2025-01-06 01:22:54,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:54,805][root][INFO] - Training Epoch: 4/10, step 500/574 completed (loss: 0.46045029163360596, acc: 0.8804348111152649)
[2025-01-06 01:22:54,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:55,106][root][INFO] - Training Epoch: 4/10, step 501/574 completed (loss: 0.007947621867060661, acc: 1.0)
[2025-01-06 01:22:55,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:55,386][root][INFO] - Training Epoch: 4/10, step 502/574 completed (loss: 0.003387964330613613, acc: 1.0)
[2025-01-06 01:22:55,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:55,680][root][INFO] - Training Epoch: 4/10, step 503/574 completed (loss: 0.1803082823753357, acc: 0.8888888955116272)
[2025-01-06 01:22:55,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:55,976][root][INFO] - Training Epoch: 4/10, step 504/574 completed (loss: 0.20364944636821747, acc: 0.9629629850387573)
[2025-01-06 01:22:56,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:56,280][root][INFO] - Training Epoch: 4/10, step 505/574 completed (loss: 0.33179405331611633, acc: 0.9245283007621765)
[2025-01-06 01:22:56,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:56,564][root][INFO] - Training Epoch: 4/10, step 506/574 completed (loss: 0.6674069762229919, acc: 0.8620689511299133)
[2025-01-06 01:22:56,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:57,144][root][INFO] - Training Epoch: 4/10, step 507/574 completed (loss: 0.7982131242752075, acc: 0.7837837934494019)
[2025-01-06 01:22:57,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:57,574][root][INFO] - Training Epoch: 4/10, step 508/574 completed (loss: 0.5264246463775635, acc: 0.8450704216957092)
[2025-01-06 01:22:57,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:57,876][root][INFO] - Training Epoch: 4/10, step 509/574 completed (loss: 0.09685276448726654, acc: 0.949999988079071)
[2025-01-06 01:22:57,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:58,171][root][INFO] - Training Epoch: 4/10, step 510/574 completed (loss: 0.10434329509735107, acc: 0.9666666388511658)
[2025-01-06 01:22:58,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:58,508][root][INFO] - Training Epoch: 4/10, step 511/574 completed (loss: 0.23686066269874573, acc: 0.9615384340286255)
[2025-01-06 01:22:59,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:00,932][root][INFO] - Training Epoch: 4/10, step 512/574 completed (loss: 0.8645724654197693, acc: 0.7357142567634583)
[2025-01-06 01:23:01,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:01,697][root][INFO] - Training Epoch: 4/10, step 513/574 completed (loss: 0.19247761368751526, acc: 0.9285714030265808)
[2025-01-06 01:23:01,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:02,007][root][INFO] - Training Epoch: 4/10, step 514/574 completed (loss: 0.3862728178501129, acc: 0.8571428656578064)
[2025-01-06 01:23:02,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:02,310][root][INFO] - Training Epoch: 4/10, step 515/574 completed (loss: 0.04077287018299103, acc: 1.0)
[2025-01-06 01:23:02,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:02,997][root][INFO] - Training Epoch: 4/10, step 516/574 completed (loss: 0.3522711396217346, acc: 0.8888888955116272)
[2025-01-06 01:23:03,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:03,394][root][INFO] - Training Epoch: 4/10, step 517/574 completed (loss: 0.003098860615864396, acc: 1.0)
[2025-01-06 01:23:03,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:03,722][root][INFO] - Training Epoch: 4/10, step 518/574 completed (loss: 0.09185205399990082, acc: 0.9677419066429138)
[2025-01-06 01:23:03,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:04,039][root][INFO] - Training Epoch: 4/10, step 519/574 completed (loss: 0.0803263857960701, acc: 1.0)
[2025-01-06 01:23:04,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:04,325][root][INFO] - Training Epoch: 4/10, step 520/574 completed (loss: 0.24206963181495667, acc: 0.9259259104728699)
[2025-01-06 01:23:04,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:05,297][root][INFO] - Training Epoch: 4/10, step 521/574 completed (loss: 0.535933792591095, acc: 0.8644067645072937)
[2025-01-06 01:23:05,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:05,637][root][INFO] - Training Epoch: 4/10, step 522/574 completed (loss: 0.24363374710083008, acc: 0.9402984976768494)
[2025-01-06 01:23:05,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:05,992][root][INFO] - Training Epoch: 4/10, step 523/574 completed (loss: 0.2519153654575348, acc: 0.9124087691307068)
[2025-01-06 01:23:06,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:06,555][root][INFO] - Training Epoch: 4/10, step 524/574 completed (loss: 0.5593525171279907, acc: 0.8550000190734863)
[2025-01-06 01:23:06,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:06,890][root][INFO] - Training Epoch: 4/10, step 525/574 completed (loss: 0.05102510377764702, acc: 0.9814814925193787)
[2025-01-06 01:23:06,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:07,211][root][INFO] - Training Epoch: 4/10, step 526/574 completed (loss: 0.12516695261001587, acc: 0.9615384340286255)
[2025-01-06 01:23:07,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:07,509][root][INFO] - Training Epoch: 4/10, step 527/574 completed (loss: 0.17288754880428314, acc: 0.9047619104385376)
[2025-01-06 01:23:07,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:07,838][root][INFO] - Training Epoch: 4/10, step 528/574 completed (loss: 1.085253357887268, acc: 0.7213114500045776)
[2025-01-06 01:23:07,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:08,132][root][INFO] - Training Epoch: 4/10, step 529/574 completed (loss: 0.22630099952220917, acc: 0.9491525292396545)
[2025-01-06 01:23:08,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:08,447][root][INFO] - Training Epoch: 4/10, step 530/574 completed (loss: 0.6846886873245239, acc: 0.7441860437393188)
[2025-01-06 01:23:08,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:08,749][root][INFO] - Training Epoch: 4/10, step 531/574 completed (loss: 0.2614665925502777, acc: 0.9090909361839294)
[2025-01-06 01:23:08,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09,055][root][INFO] - Training Epoch: 4/10, step 532/574 completed (loss: 0.2438683956861496, acc: 0.9433962106704712)
[2025-01-06 01:23:09,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09,333][root][INFO] - Training Epoch: 4/10, step 533/574 completed (loss: 0.3821519613265991, acc: 0.9090909361839294)
[2025-01-06 01:23:09,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09,635][root][INFO] - Training Epoch: 4/10, step 534/574 completed (loss: 0.12868434190750122, acc: 0.9599999785423279)
[2025-01-06 01:23:09,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09,964][root][INFO] - Training Epoch: 4/10, step 535/574 completed (loss: 0.1761985719203949, acc: 0.949999988079071)
[2025-01-06 01:23:10,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:10,302][root][INFO] - Training Epoch: 4/10, step 536/574 completed (loss: 0.030517591163516045, acc: 1.0)
[2025-01-06 01:23:10,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:10,700][root][INFO] - Training Epoch: 4/10, step 537/574 completed (loss: 0.24300187826156616, acc: 0.9538461565971375)
[2025-01-06 01:23:10,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:11,054][root][INFO] - Training Epoch: 4/10, step 538/574 completed (loss: 0.20113636553287506, acc: 0.90625)
[2025-01-06 01:23:11,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:11,445][root][INFO] - Training Epoch: 4/10, step 539/574 completed (loss: 0.3999653160572052, acc: 0.875)
[2025-01-06 01:23:11,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:11,754][root][INFO] - Training Epoch: 4/10, step 540/574 completed (loss: 0.2411472350358963, acc: 0.939393937587738)
[2025-01-06 01:23:11,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:12,127][root][INFO] - Training Epoch: 4/10, step 541/574 completed (loss: 0.08707943558692932, acc: 1.0)
[2025-01-06 01:23:12,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:12,430][root][INFO] - Training Epoch: 4/10, step 542/574 completed (loss: 0.02462976612150669, acc: 1.0)
[2025-01-06 01:23:12,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:12,740][root][INFO] - Training Epoch: 4/10, step 543/574 completed (loss: 0.05360109731554985, acc: 0.95652174949646)
[2025-01-06 01:23:12,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:13,047][root][INFO] - Training Epoch: 4/10, step 544/574 completed (loss: 0.08394782990217209, acc: 0.9666666388511658)
[2025-01-06 01:23:13,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:13,372][root][INFO] - Training Epoch: 4/10, step 545/574 completed (loss: 0.023089570924639702, acc: 1.0)
[2025-01-06 01:23:13,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:13,706][root][INFO] - Training Epoch: 4/10, step 546/574 completed (loss: 0.0022501181811094284, acc: 1.0)
[2025-01-06 01:23:13,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:14,092][root][INFO] - Training Epoch: 4/10, step 547/574 completed (loss: 0.0033594374544918537, acc: 1.0)
[2025-01-06 01:23:14,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:14,397][root][INFO] - Training Epoch: 4/10, step 548/574 completed (loss: 0.06876944750547409, acc: 0.9677419066429138)
[2025-01-06 01:23:14,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:14,699][root][INFO] - Training Epoch: 4/10, step 549/574 completed (loss: 0.002886863425374031, acc: 1.0)
[2025-01-06 01:23:14,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:15,011][root][INFO] - Training Epoch: 4/10, step 550/574 completed (loss: 0.14141038060188293, acc: 0.9696969985961914)
[2025-01-06 01:23:15,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:15,298][root][INFO] - Training Epoch: 4/10, step 551/574 completed (loss: 0.10419692099094391, acc: 0.9750000238418579)
[2025-01-06 01:23:15,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:15,605][root][INFO] - Training Epoch: 4/10, step 552/574 completed (loss: 0.0927051231265068, acc: 0.9857142567634583)
[2025-01-06 01:23:15,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:15,920][root][INFO] - Training Epoch: 4/10, step 553/574 completed (loss: 0.3398391604423523, acc: 0.9051094651222229)
[2025-01-06 01:23:16,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:16,252][root][INFO] - Training Epoch: 4/10, step 554/574 completed (loss: 0.15180739760398865, acc: 0.9379310607910156)
[2025-01-06 01:23:16,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:16,592][root][INFO] - Training Epoch: 4/10, step 555/574 completed (loss: 0.16645757853984833, acc: 0.949999988079071)
[2025-01-06 01:23:16,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:16,928][root][INFO] - Training Epoch: 4/10, step 556/574 completed (loss: 0.28905603289604187, acc: 0.9205297827720642)
[2025-01-06 01:23:17,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:17,288][root][INFO] - Training Epoch: 4/10, step 557/574 completed (loss: 0.09116626530885696, acc: 0.9658119678497314)
[2025-01-06 01:23:17,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:17,604][root][INFO] - Training Epoch: 4/10, step 558/574 completed (loss: 0.01964888535439968, acc: 1.0)
[2025-01-06 01:23:17,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:17,920][root][INFO] - Training Epoch: 4/10, step 559/574 completed (loss: 0.4145185053348541, acc: 0.9230769276618958)
[2025-01-06 01:23:18,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:18,252][root][INFO] - Training Epoch: 4/10, step 560/574 completed (loss: 0.11728302389383316, acc: 0.9615384340286255)
[2025-01-06 01:23:18,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:18,584][root][INFO] - Training Epoch: 4/10, step 561/574 completed (loss: 0.03066842444241047, acc: 1.0)
[2025-01-06 01:23:18,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:18,920][root][INFO] - Training Epoch: 4/10, step 562/574 completed (loss: 0.28765514492988586, acc: 0.9444444179534912)
[2025-01-06 01:23:19,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:19,251][root][INFO] - Training Epoch: 4/10, step 563/574 completed (loss: 0.3114374577999115, acc: 0.9220778942108154)
[2025-01-06 01:23:19,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:19,559][root][INFO] - Training Epoch: 4/10, step 564/574 completed (loss: 0.12258636951446533, acc: 0.9583333134651184)
[2025-01-06 01:23:19,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:19,897][root][INFO] - Training Epoch: 4/10, step 565/574 completed (loss: 0.083772674202919, acc: 0.9482758641242981)
[2025-01-06 01:23:20,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:20,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:21,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:21,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:21,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:22,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:22,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:22,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:23,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:23,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:23,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:25,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:25,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:25,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:26,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:26,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:26,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:27,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:27,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:27,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:28,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:28,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:28,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:31,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:31,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:31,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:32,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:32,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:32,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:33,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:33,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:33,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:36,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:36,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:36,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:38,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:38,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:38,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:39,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:39,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:39,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:40,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:40,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:40,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:41,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:41,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:44,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:44,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:45,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:45,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:45,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:45,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:46,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:46,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:47,327][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2180, device='cuda:0') eval_epoch_loss=tensor(0.7966, device='cuda:0') eval_epoch_acc=tensor(0.8216, device='cuda:0')
[2025-01-06 01:23:47,328][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:23:47,328][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:23:47,563][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_566_loss_0.7965880632400513/model.pt
[2025-01-06 01:23:47,571][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:23:47,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:47,936][root][INFO] - Training Epoch: 4/10, step 566/574 completed (loss: 0.12054792046546936, acc: 0.988095223903656)
[2025-01-06 01:23:48,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:48,246][root][INFO] - Training Epoch: 4/10, step 567/574 completed (loss: 0.010292035527527332, acc: 1.0)
[2025-01-06 01:23:48,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:48,549][root][INFO] - Training Epoch: 4/10, step 568/574 completed (loss: 0.024124758318066597, acc: 1.0)
[2025-01-06 01:23:48,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:48,917][root][INFO] - Training Epoch: 4/10, step 569/574 completed (loss: 0.13754720985889435, acc: 0.9679144620895386)
[2025-01-06 01:23:48,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:49,212][root][INFO] - Training Epoch: 4/10, step 570/574 completed (loss: 0.009369137696921825, acc: 1.0)
[2025-01-06 01:23:49,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:49,515][root][INFO] - Training Epoch: 4/10, step 571/574 completed (loss: 0.13227543234825134, acc: 0.9743589758872986)
[2025-01-06 01:23:49,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:49,822][root][INFO] - Training Epoch: 4/10, step 572/574 completed (loss: 0.28141146898269653, acc: 0.918367326259613)
[2025-01-06 01:23:49,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:50,170][root][INFO] - Training Epoch: 4/10, step 573/574 completed (loss: 0.18590644001960754, acc: 0.9182389974594116)
[2025-01-06 01:23:50,642][slam_llm.utils.train_utils][INFO] - Epoch 4: train_perplexity=1.3426, train_epoch_loss=0.2946, epoch time 347.26975252851844s
[2025-01-06 01:23:50,642][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:23:50,643][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 14 GB
[2025-01-06 01:23:50,643][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:23:50,643][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 12
[2025-01-06 01:23:50,643][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:23:51,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:51,408][root][INFO] - Training Epoch: 5/10, step 0/574 completed (loss: 0.042511891573667526, acc: 0.9629629850387573)
[2025-01-06 01:23:51,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:51,704][root][INFO] - Training Epoch: 5/10, step 1/574 completed (loss: 0.14490270614624023, acc: 0.9599999785423279)
[2025-01-06 01:23:51,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52,004][root][INFO] - Training Epoch: 5/10, step 2/574 completed (loss: 0.5421382188796997, acc: 0.8918918967247009)
[2025-01-06 01:23:52,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52,315][root][INFO] - Training Epoch: 5/10, step 3/574 completed (loss: 0.21070344746112823, acc: 0.9210526347160339)
[2025-01-06 01:23:52,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52,622][root][INFO] - Training Epoch: 5/10, step 4/574 completed (loss: 0.29521986842155457, acc: 0.9459459185600281)
[2025-01-06 01:23:52,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52,970][root][INFO] - Training Epoch: 5/10, step 5/574 completed (loss: 0.03585276007652283, acc: 1.0)
[2025-01-06 01:23:53,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:53,349][root][INFO] - Training Epoch: 5/10, step 6/574 completed (loss: 0.3836023807525635, acc: 0.8571428656578064)
[2025-01-06 01:23:53,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:53,704][root][INFO] - Training Epoch: 5/10, step 7/574 completed (loss: 0.061826594173908234, acc: 1.0)
[2025-01-06 01:23:53,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:54,079][root][INFO] - Training Epoch: 5/10, step 8/574 completed (loss: 0.022074537351727486, acc: 1.0)
[2025-01-06 01:23:54,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:54,408][root][INFO] - Training Epoch: 5/10, step 9/574 completed (loss: 0.36869674921035767, acc: 0.9230769276618958)
[2025-01-06 01:23:54,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:54,758][root][INFO] - Training Epoch: 5/10, step 10/574 completed (loss: 0.004682532045990229, acc: 1.0)
[2025-01-06 01:23:54,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:55,136][root][INFO] - Training Epoch: 5/10, step 11/574 completed (loss: 0.12962153553962708, acc: 0.9487179517745972)
[2025-01-06 01:23:55,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:55,483][root][INFO] - Training Epoch: 5/10, step 12/574 completed (loss: 0.026287630200386047, acc: 1.0)
[2025-01-06 01:23:55,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:55,843][root][INFO] - Training Epoch: 5/10, step 13/574 completed (loss: 0.10970594733953476, acc: 0.95652174949646)
[2025-01-06 01:23:55,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:56,173][root][INFO] - Training Epoch: 5/10, step 14/574 completed (loss: 0.08736486732959747, acc: 0.9607843160629272)
[2025-01-06 01:23:56,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:56,508][root][INFO] - Training Epoch: 5/10, step 15/574 completed (loss: 0.11781901121139526, acc: 0.9591836929321289)
[2025-01-06 01:23:56,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:56,874][root][INFO] - Training Epoch: 5/10, step 16/574 completed (loss: 0.0051681362092494965, acc: 1.0)
[2025-01-06 01:23:56,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:57,245][root][INFO] - Training Epoch: 5/10, step 17/574 completed (loss: 0.07266836613416672, acc: 0.9583333134651184)
[2025-01-06 01:23:57,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:57,603][root][INFO] - Training Epoch: 5/10, step 18/574 completed (loss: 0.4189874231815338, acc: 0.9166666865348816)
[2025-01-06 01:23:57,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:57,908][root][INFO] - Training Epoch: 5/10, step 19/574 completed (loss: 0.2877032160758972, acc: 0.8947368264198303)
[2025-01-06 01:23:57,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:58,222][root][INFO] - Training Epoch: 5/10, step 20/574 completed (loss: 0.29439297318458557, acc: 0.9230769276618958)
[2025-01-06 01:23:58,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:58,518][root][INFO] - Training Epoch: 5/10, step 21/574 completed (loss: 0.05571673810482025, acc: 0.9655172228813171)
[2025-01-06 01:23:58,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:58,823][root][INFO] - Training Epoch: 5/10, step 22/574 completed (loss: 0.04743504151701927, acc: 1.0)
[2025-01-06 01:23:58,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:59,119][root][INFO] - Training Epoch: 5/10, step 23/574 completed (loss: 0.11622361838817596, acc: 0.9523809552192688)
[2025-01-06 01:23:59,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:59,418][root][INFO] - Training Epoch: 5/10, step 24/574 completed (loss: 0.012843431904911995, acc: 1.0)
[2025-01-06 01:23:59,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:59,728][root][INFO] - Training Epoch: 5/10, step 25/574 completed (loss: 0.543913722038269, acc: 0.849056601524353)
[2025-01-06 01:23:59,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:00,047][root][INFO] - Training Epoch: 5/10, step 26/574 completed (loss: 0.4928233027458191, acc: 0.8493150472640991)
[2025-01-06 01:24:00,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:01,257][root][INFO] - Training Epoch: 5/10, step 27/574 completed (loss: 0.6757522225379944, acc: 0.8063241243362427)
[2025-01-06 01:24:01,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:01,638][root][INFO] - Training Epoch: 5/10, step 28/574 completed (loss: 0.19546332955360413, acc: 0.8837209343910217)
[2025-01-06 01:24:01,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:01,986][root][INFO] - Training Epoch: 5/10, step 29/574 completed (loss: 0.23184983432292938, acc: 0.9277108311653137)
[2025-01-06 01:24:02,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:02,331][root][INFO] - Training Epoch: 5/10, step 30/574 completed (loss: 0.3591136932373047, acc: 0.8888888955116272)
[2025-01-06 01:24:02,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:02,671][root][INFO] - Training Epoch: 5/10, step 31/574 completed (loss: 0.2698059380054474, acc: 0.9642857313156128)
[2025-01-06 01:24:02,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:02,992][root][INFO] - Training Epoch: 5/10, step 32/574 completed (loss: 0.1534896194934845, acc: 0.9259259104728699)
[2025-01-06 01:24:03,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:03,312][root][INFO] - Training Epoch: 5/10, step 33/574 completed (loss: 0.0024783979170024395, acc: 1.0)
[2025-01-06 01:24:03,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:03,643][root][INFO] - Training Epoch: 5/10, step 34/574 completed (loss: 0.2858404815196991, acc: 0.8823529481887817)
[2025-01-06 01:24:03,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:04,012][root][INFO] - Training Epoch: 5/10, step 35/574 completed (loss: 0.24241939187049866, acc: 0.9344262480735779)
[2025-01-06 01:24:04,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:04,372][root][INFO] - Training Epoch: 5/10, step 36/574 completed (loss: 0.29493415355682373, acc: 0.920634925365448)
[2025-01-06 01:24:04,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:04,719][root][INFO] - Training Epoch: 5/10, step 37/574 completed (loss: 0.16715174913406372, acc: 0.9491525292396545)
[2025-01-06 01:24:04,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:05,068][root][INFO] - Training Epoch: 5/10, step 38/574 completed (loss: 0.19993124902248383, acc: 0.9655172228813171)
[2025-01-06 01:24:05,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:05,387][root][INFO] - Training Epoch: 5/10, step 39/574 completed (loss: 0.03342073783278465, acc: 1.0)
[2025-01-06 01:24:05,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:05,729][root][INFO] - Training Epoch: 5/10, step 40/574 completed (loss: 0.10674410313367844, acc: 0.9615384340286255)
[2025-01-06 01:24:05,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:06,107][root][INFO] - Training Epoch: 5/10, step 41/574 completed (loss: 0.1004740297794342, acc: 0.9594594836235046)
[2025-01-06 01:24:06,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:06,448][root][INFO] - Training Epoch: 5/10, step 42/574 completed (loss: 0.29416489601135254, acc: 0.9076923131942749)
[2025-01-06 01:24:06,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:06,863][root][INFO] - Training Epoch: 5/10, step 43/574 completed (loss: 0.4947601854801178, acc: 0.8585858345031738)
[2025-01-06 01:24:06,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:07,278][root][INFO] - Training Epoch: 5/10, step 44/574 completed (loss: 0.20577576756477356, acc: 0.938144326210022)
[2025-01-06 01:24:07,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:07,676][root][INFO] - Training Epoch: 5/10, step 45/574 completed (loss: 0.18365295231342316, acc: 0.9264705777168274)
[2025-01-06 01:24:07,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:07,970][root][INFO] - Training Epoch: 5/10, step 46/574 completed (loss: 0.1138027086853981, acc: 0.9230769276618958)
[2025-01-06 01:24:08,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:08,272][root][INFO] - Training Epoch: 5/10, step 47/574 completed (loss: 0.18041470646858215, acc: 0.9629629850387573)
[2025-01-06 01:24:08,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:08,576][root][INFO] - Training Epoch: 5/10, step 48/574 completed (loss: 0.08622419834136963, acc: 0.9642857313156128)
[2025-01-06 01:24:08,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:08,884][root][INFO] - Training Epoch: 5/10, step 49/574 completed (loss: 0.0744021087884903, acc: 0.9722222089767456)
[2025-01-06 01:24:08,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:09,202][root][INFO] - Training Epoch: 5/10, step 50/574 completed (loss: 0.459373414516449, acc: 0.8421052694320679)
[2025-01-06 01:24:09,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:09,544][root][INFO] - Training Epoch: 5/10, step 51/574 completed (loss: 0.11781123280525208, acc: 0.9841269850730896)
[2025-01-06 01:24:09,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:09,866][root][INFO] - Training Epoch: 5/10, step 52/574 completed (loss: 0.31634488701820374, acc: 0.8591549396514893)
[2025-01-06 01:24:09,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:10,324][root][INFO] - Training Epoch: 5/10, step 53/574 completed (loss: 1.0545570850372314, acc: 0.6666666865348816)
[2025-01-06 01:24:10,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:10,661][root][INFO] - Training Epoch: 5/10, step 54/574 completed (loss: 0.37017127871513367, acc: 0.8648648858070374)
[2025-01-06 01:24:10,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:10,983][root][INFO] - Training Epoch: 5/10, step 55/574 completed (loss: 0.1018521636724472, acc: 0.9230769276618958)
[2025-01-06 01:24:12,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:14,064][root][INFO] - Training Epoch: 5/10, step 56/574 completed (loss: 0.6769174933433533, acc: 0.80887371301651)
[2025-01-06 01:24:14,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:15,232][root][INFO] - Training Epoch: 5/10, step 57/574 completed (loss: 1.0139861106872559, acc: 0.7320261597633362)
[2025-01-06 01:24:15,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:15,856][root][INFO] - Training Epoch: 5/10, step 58/574 completed (loss: 0.5706998109817505, acc: 0.8011363744735718)
[2025-01-06 01:24:15,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:16,423][root][INFO] - Training Epoch: 5/10, step 59/574 completed (loss: 0.19377189874649048, acc: 0.9485294222831726)
[2025-01-06 01:24:16,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:16,983][root][INFO] - Training Epoch: 5/10, step 60/574 completed (loss: 0.6361778378486633, acc: 0.8188405632972717)
[2025-01-06 01:24:17,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:17,386][root][INFO] - Training Epoch: 5/10, step 61/574 completed (loss: 0.40464216470718384, acc: 0.875)
[2025-01-06 01:24:17,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:17,749][root][INFO] - Training Epoch: 5/10, step 62/574 completed (loss: 0.08862245082855225, acc: 0.970588207244873)
[2025-01-06 01:24:17,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:18,095][root][INFO] - Training Epoch: 5/10, step 63/574 completed (loss: 0.11122985929250717, acc: 0.9722222089767456)
[2025-01-06 01:24:18,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:18,442][root][INFO] - Training Epoch: 5/10, step 64/574 completed (loss: 0.08155645430088043, acc: 0.984375)
[2025-01-06 01:24:18,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:18,742][root][INFO] - Training Epoch: 5/10, step 65/574 completed (loss: 0.016835596412420273, acc: 1.0)
[2025-01-06 01:24:18,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19,059][root][INFO] - Training Epoch: 5/10, step 66/574 completed (loss: 0.36944350600242615, acc: 0.9285714030265808)
[2025-01-06 01:24:19,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19,377][root][INFO] - Training Epoch: 5/10, step 67/574 completed (loss: 0.1169133186340332, acc: 0.9666666388511658)
[2025-01-06 01:24:19,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19,692][root][INFO] - Training Epoch: 5/10, step 68/574 completed (loss: 0.006466264836490154, acc: 1.0)
[2025-01-06 01:24:19,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19,984][root][INFO] - Training Epoch: 5/10, step 69/574 completed (loss: 0.24582798779010773, acc: 0.9444444179534912)
[2025-01-06 01:24:20,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:20,291][root][INFO] - Training Epoch: 5/10, step 70/574 completed (loss: 0.37024009227752686, acc: 0.9090909361839294)
[2025-01-06 01:24:20,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:20,600][root][INFO] - Training Epoch: 5/10, step 71/574 completed (loss: 0.6553786993026733, acc: 0.8308823704719543)
[2025-01-06 01:24:20,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:20,908][root][INFO] - Training Epoch: 5/10, step 72/574 completed (loss: 0.4560847580432892, acc: 0.8095238208770752)
[2025-01-06 01:24:20,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:21,219][root][INFO] - Training Epoch: 5/10, step 73/574 completed (loss: 0.977643609046936, acc: 0.7230769395828247)
[2025-01-06 01:24:21,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:21,528][root][INFO] - Training Epoch: 5/10, step 74/574 completed (loss: 0.6762596964836121, acc: 0.8265306353569031)
[2025-01-06 01:24:21,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:21,910][root][INFO] - Training Epoch: 5/10, step 75/574 completed (loss: 0.7135188579559326, acc: 0.8059701323509216)
[2025-01-06 01:24:22,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:22,300][root][INFO] - Training Epoch: 5/10, step 76/574 completed (loss: 1.2188321352005005, acc: 0.6788321137428284)
[2025-01-06 01:24:22,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:22,601][root][INFO] - Training Epoch: 5/10, step 77/574 completed (loss: 0.003159929532557726, acc: 1.0)
[2025-01-06 01:24:22,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:22,913][root][INFO] - Training Epoch: 5/10, step 78/574 completed (loss: 0.03271247819066048, acc: 1.0)
[2025-01-06 01:24:22,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:23,197][root][INFO] - Training Epoch: 5/10, step 79/574 completed (loss: 0.027476664632558823, acc: 1.0)
[2025-01-06 01:24:23,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:23,501][root][INFO] - Training Epoch: 5/10, step 80/574 completed (loss: 0.012755136936903, acc: 1.0)
[2025-01-06 01:24:23,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:23,804][root][INFO] - Training Epoch: 5/10, step 81/574 completed (loss: 0.1662454605102539, acc: 0.9230769276618958)
[2025-01-06 01:24:23,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:24,105][root][INFO] - Training Epoch: 5/10, step 82/574 completed (loss: 0.20534609258174896, acc: 0.942307710647583)
[2025-01-06 01:24:24,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:24,404][root][INFO] - Training Epoch: 5/10, step 83/574 completed (loss: 0.25221604108810425, acc: 0.90625)
[2025-01-06 01:24:24,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:24,725][root][INFO] - Training Epoch: 5/10, step 84/574 completed (loss: 0.0980674996972084, acc: 0.9855072498321533)
[2025-01-06 01:24:24,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:25,051][root][INFO] - Training Epoch: 5/10, step 85/574 completed (loss: 0.13819622993469238, acc: 0.9599999785423279)
[2025-01-06 01:24:25,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:25,380][root][INFO] - Training Epoch: 5/10, step 86/574 completed (loss: 0.020317066460847855, acc: 1.0)
[2025-01-06 01:24:25,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:25,861][root][INFO] - Training Epoch: 5/10, step 87/574 completed (loss: 0.23646265268325806, acc: 0.9599999785423279)
[2025-01-06 01:24:25,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:26,223][root][INFO] - Training Epoch: 5/10, step 88/574 completed (loss: 0.5128151178359985, acc: 0.8737863898277283)
[2025-01-06 01:24:26,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:27,296][root][INFO] - Training Epoch: 5/10, step 89/574 completed (loss: 0.7086708545684814, acc: 0.8300970792770386)
[2025-01-06 01:24:27,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:28,111][root][INFO] - Training Epoch: 5/10, step 90/574 completed (loss: 0.8032523393630981, acc: 0.801075279712677)
[2025-01-06 01:24:28,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:28,913][root][INFO] - Training Epoch: 5/10, step 91/574 completed (loss: 0.6365634799003601, acc: 0.8103448152542114)
[2025-01-06 01:24:29,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:29,658][root][INFO] - Training Epoch: 5/10, step 92/574 completed (loss: 0.5502006411552429, acc: 0.8315789699554443)
[2025-01-06 01:24:29,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:30,649][root][INFO] - Training Epoch: 5/10, step 93/574 completed (loss: 0.6667263507843018, acc: 0.801980197429657)
[2025-01-06 01:24:30,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:31,008][root][INFO] - Training Epoch: 5/10, step 94/574 completed (loss: 0.478325217962265, acc: 0.8548387289047241)
[2025-01-06 01:24:31,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:31,387][root][INFO] - Training Epoch: 5/10, step 95/574 completed (loss: 0.3856801986694336, acc: 0.8695651888847351)
[2025-01-06 01:24:31,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:31,727][root][INFO] - Training Epoch: 5/10, step 96/574 completed (loss: 0.6777008175849915, acc: 0.7983193397521973)
[2025-01-06 01:24:31,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:32,094][root][INFO] - Training Epoch: 5/10, step 97/574 completed (loss: 0.6009306311607361, acc: 0.817307710647583)
[2025-01-06 01:24:32,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:32,509][root][INFO] - Training Epoch: 5/10, step 98/574 completed (loss: 0.8329910039901733, acc: 0.6934306621551514)
[2025-01-06 01:24:32,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:32,886][root][INFO] - Training Epoch: 5/10, step 99/574 completed (loss: 0.4786328077316284, acc: 0.8805969953536987)
[2025-01-06 01:24:32,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:33,236][root][INFO] - Training Epoch: 5/10, step 100/574 completed (loss: 0.054519880563020706, acc: 1.0)
[2025-01-06 01:24:33,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:33,582][root][INFO] - Training Epoch: 5/10, step 101/574 completed (loss: 0.009121282026171684, acc: 1.0)
[2025-01-06 01:24:33,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:33,924][root][INFO] - Training Epoch: 5/10, step 102/574 completed (loss: 0.05760223791003227, acc: 0.95652174949646)
[2025-01-06 01:24:33,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:34,261][root][INFO] - Training Epoch: 5/10, step 103/574 completed (loss: 0.0197436660528183, acc: 1.0)
[2025-01-06 01:24:34,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:34,620][root][INFO] - Training Epoch: 5/10, step 104/574 completed (loss: 0.10719139128923416, acc: 0.9655172228813171)
[2025-01-06 01:24:34,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:34,949][root][INFO] - Training Epoch: 5/10, step 105/574 completed (loss: 0.016512559726834297, acc: 1.0)
[2025-01-06 01:24:35,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:35,288][root][INFO] - Training Epoch: 5/10, step 106/574 completed (loss: 0.16944372653961182, acc: 0.9200000166893005)
[2025-01-06 01:24:35,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:35,619][root][INFO] - Training Epoch: 5/10, step 107/574 completed (loss: 0.004082603845745325, acc: 1.0)
[2025-01-06 01:24:35,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:35,959][root][INFO] - Training Epoch: 5/10, step 108/574 completed (loss: 0.031411804258823395, acc: 0.9615384340286255)
[2025-01-06 01:24:36,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:36,283][root][INFO] - Training Epoch: 5/10, step 109/574 completed (loss: 0.22110775113105774, acc: 0.9523809552192688)
[2025-01-06 01:24:36,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:36,647][root][INFO] - Training Epoch: 5/10, step 110/574 completed (loss: 0.05001487210392952, acc: 0.9846153855323792)
[2025-01-06 01:24:36,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:37,076][root][INFO] - Training Epoch: 5/10, step 111/574 completed (loss: 0.2808919847011566, acc: 0.8771929740905762)
[2025-01-06 01:24:37,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:37,448][root][INFO] - Training Epoch: 5/10, step 112/574 completed (loss: 0.3467675447463989, acc: 0.8947368264198303)
[2025-01-06 01:24:37,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:37,799][root][INFO] - Training Epoch: 5/10, step 113/574 completed (loss: 0.14315323531627655, acc: 0.9743589758872986)
[2025-01-06 01:24:37,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:38,199][root][INFO] - Training Epoch: 5/10, step 114/574 completed (loss: 0.14966316521167755, acc: 0.9591836929321289)
[2025-01-06 01:24:38,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:38,542][root][INFO] - Training Epoch: 5/10, step 115/574 completed (loss: 0.0024332546163350344, acc: 1.0)
[2025-01-06 01:24:38,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:38,909][root][INFO] - Training Epoch: 5/10, step 116/574 completed (loss: 0.23502126336097717, acc: 0.9523809552192688)
[2025-01-06 01:24:38,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:39,254][root][INFO] - Training Epoch: 5/10, step 117/574 completed (loss: 0.3754105567932129, acc: 0.8780487775802612)
[2025-01-06 01:24:39,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:39,632][root][INFO] - Training Epoch: 5/10, step 118/574 completed (loss: 0.16276666522026062, acc: 0.9354838728904724)
[2025-01-06 01:24:39,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:40,487][root][INFO] - Training Epoch: 5/10, step 119/574 completed (loss: 0.548801064491272, acc: 0.8403041958808899)
[2025-01-06 01:24:40,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:40,842][root][INFO] - Training Epoch: 5/10, step 120/574 completed (loss: 0.04651183262467384, acc: 1.0)
[2025-01-06 01:24:40,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:41,273][root][INFO] - Training Epoch: 5/10, step 121/574 completed (loss: 0.2479906678199768, acc: 0.942307710647583)
[2025-01-06 01:24:41,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:41,597][root][INFO] - Training Epoch: 5/10, step 122/574 completed (loss: 0.02578023076057434, acc: 1.0)
[2025-01-06 01:24:41,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:41,926][root][INFO] - Training Epoch: 5/10, step 123/574 completed (loss: 0.40915894508361816, acc: 0.8947368264198303)
[2025-01-06 01:24:42,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:42,310][root][INFO] - Training Epoch: 5/10, step 124/574 completed (loss: 0.6898718476295471, acc: 0.8159509301185608)
[2025-01-06 01:24:42,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:42,720][root][INFO] - Training Epoch: 5/10, step 125/574 completed (loss: 0.5420748591423035, acc: 0.8541666865348816)
[2025-01-06 01:24:42,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:43,141][root][INFO] - Training Epoch: 5/10, step 126/574 completed (loss: 0.5409641265869141, acc: 0.8333333134651184)
[2025-01-06 01:24:43,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:43,523][root][INFO] - Training Epoch: 5/10, step 127/574 completed (loss: 0.3827786147594452, acc: 0.875)
[2025-01-06 01:24:43,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:43,911][root][INFO] - Training Epoch: 5/10, step 128/574 completed (loss: 0.4923830032348633, acc: 0.8461538553237915)
[2025-01-06 01:24:44,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:44,347][root][INFO] - Training Epoch: 5/10, step 129/574 completed (loss: 0.5150810480117798, acc: 0.8088235259056091)
[2025-01-06 01:24:44,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:44,680][root][INFO] - Training Epoch: 5/10, step 130/574 completed (loss: 0.0825677216053009, acc: 0.9615384340286255)
[2025-01-06 01:24:44,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:45,014][root][INFO] - Training Epoch: 5/10, step 131/574 completed (loss: 0.027748649939894676, acc: 1.0)
[2025-01-06 01:24:45,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:45,353][root][INFO] - Training Epoch: 5/10, step 132/574 completed (loss: 0.06266089528799057, acc: 1.0)
[2025-01-06 01:24:45,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:45,689][root][INFO] - Training Epoch: 5/10, step 133/574 completed (loss: 0.10445203632116318, acc: 1.0)
[2025-01-06 01:24:45,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:46,039][root][INFO] - Training Epoch: 5/10, step 134/574 completed (loss: 0.27098047733306885, acc: 0.8857142925262451)
[2025-01-06 01:24:46,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:47,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:47,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:47,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:48,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:48,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:48,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:48,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:49,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:49,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:50,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:50,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:50,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:51,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:51,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:51,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:52,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:52,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:52,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:54,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:54,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:54,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:55,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:55,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:55,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:56,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:56,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:56,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:58,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:58,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:58,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:59,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:59,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:59,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:00,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:00,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:00,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:01,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:01,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:01,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:02,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:02,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:02,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:03,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:03,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:03,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:04,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:04,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:04,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:05,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:05,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:05,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:06,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:06,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:07,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:07,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:07,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:08,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:08,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:08,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:09,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:09,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:09,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:11,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:11,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:11,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:12,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:12,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:12,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:14,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:14,870][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0965, device='cuda:0') eval_epoch_loss=tensor(0.7403, device='cuda:0') eval_epoch_acc=tensor(0.8324, device='cuda:0')
[2025-01-06 01:25:14,871][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:25:14,871][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:25:15,123][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_135_loss_0.7402848601341248/model.pt
[2025-01-06 01:25:15,127][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:25:15,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:15,492][root][INFO] - Training Epoch: 5/10, step 135/574 completed (loss: 0.09948931634426117, acc: 1.0)
[2025-01-06 01:25:15,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:15,836][root][INFO] - Training Epoch: 5/10, step 136/574 completed (loss: 0.05125853046774864, acc: 1.0)
[2025-01-06 01:25:15,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:16,199][root][INFO] - Training Epoch: 5/10, step 137/574 completed (loss: 0.4606732726097107, acc: 0.8333333134651184)
[2025-01-06 01:25:16,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:16,543][root][INFO] - Training Epoch: 5/10, step 138/574 completed (loss: 0.25610655546188354, acc: 0.9130434989929199)
[2025-01-06 01:25:16,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:16,886][root][INFO] - Training Epoch: 5/10, step 139/574 completed (loss: 0.10828107595443726, acc: 0.9523809552192688)
[2025-01-06 01:25:16,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:17,199][root][INFO] - Training Epoch: 5/10, step 140/574 completed (loss: 0.10361329466104507, acc: 0.9615384340286255)
[2025-01-06 01:25:17,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:17,533][root][INFO] - Training Epoch: 5/10, step 141/574 completed (loss: 0.0863734781742096, acc: 0.9677419066429138)
[2025-01-06 01:25:17,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:17,872][root][INFO] - Training Epoch: 5/10, step 142/574 completed (loss: 0.342519074678421, acc: 0.9459459185600281)
[2025-01-06 01:25:17,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:18,404][root][INFO] - Training Epoch: 5/10, step 143/574 completed (loss: 0.4246760904788971, acc: 0.8684210777282715)
[2025-01-06 01:25:18,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:18,746][root][INFO] - Training Epoch: 5/10, step 144/574 completed (loss: 0.5570039749145508, acc: 0.8507462739944458)
[2025-01-06 01:25:18,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:19,089][root][INFO] - Training Epoch: 5/10, step 145/574 completed (loss: 0.24913369119167328, acc: 0.918367326259613)
[2025-01-06 01:25:19,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:19,522][root][INFO] - Training Epoch: 5/10, step 146/574 completed (loss: 0.49812227487564087, acc: 0.8297872543334961)
[2025-01-06 01:25:19,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:19,828][root][INFO] - Training Epoch: 5/10, step 147/574 completed (loss: 0.26053836941719055, acc: 0.9285714030265808)
[2025-01-06 01:25:19,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:20,122][root][INFO] - Training Epoch: 5/10, step 148/574 completed (loss: 0.06761227548122406, acc: 1.0)
[2025-01-06 01:25:20,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:20,402][root][INFO] - Training Epoch: 5/10, step 149/574 completed (loss: 0.17971469461917877, acc: 0.9130434989929199)
[2025-01-06 01:25:20,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:20,697][root][INFO] - Training Epoch: 5/10, step 150/574 completed (loss: 0.2619861662387848, acc: 0.8965517282485962)
[2025-01-06 01:25:20,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:20,993][root][INFO] - Training Epoch: 5/10, step 151/574 completed (loss: 0.3120729327201843, acc: 0.8695651888847351)
[2025-01-06 01:25:21,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:21,335][root][INFO] - Training Epoch: 5/10, step 152/574 completed (loss: 0.3291882574558258, acc: 0.9152542352676392)
[2025-01-06 01:25:21,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:21,647][root][INFO] - Training Epoch: 5/10, step 153/574 completed (loss: 0.12507693469524384, acc: 0.9649122953414917)
[2025-01-06 01:25:21,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:21,956][root][INFO] - Training Epoch: 5/10, step 154/574 completed (loss: 0.3522273898124695, acc: 0.8918918967247009)
[2025-01-06 01:25:22,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:22,249][root][INFO] - Training Epoch: 5/10, step 155/574 completed (loss: 0.05113545060157776, acc: 1.0)
[2025-01-06 01:25:22,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:22,548][root][INFO] - Training Epoch: 5/10, step 156/574 completed (loss: 0.056641194969415665, acc: 1.0)
[2025-01-06 01:25:22,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:22,837][root][INFO] - Training Epoch: 5/10, step 157/574 completed (loss: 0.4420989751815796, acc: 0.7894737124443054)
[2025-01-06 01:25:23,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:24,440][root][INFO] - Training Epoch: 5/10, step 158/574 completed (loss: 0.5278548002243042, acc: 0.8513513803482056)
[2025-01-06 01:25:24,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:24,727][root][INFO] - Training Epoch: 5/10, step 159/574 completed (loss: 0.5653854608535767, acc: 0.7777777910232544)
[2025-01-06 01:25:24,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:25,100][root][INFO] - Training Epoch: 5/10, step 160/574 completed (loss: 0.5162111520767212, acc: 0.8372092843055725)
[2025-01-06 01:25:25,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:25,682][root][INFO] - Training Epoch: 5/10, step 161/574 completed (loss: 0.6692789196968079, acc: 0.8117647171020508)
[2025-01-06 01:25:25,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:26,232][root][INFO] - Training Epoch: 5/10, step 162/574 completed (loss: 0.8138304948806763, acc: 0.8202247023582458)
[2025-01-06 01:25:26,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:26,537][root][INFO] - Training Epoch: 5/10, step 163/574 completed (loss: 0.12693685293197632, acc: 0.9545454382896423)
[2025-01-06 01:25:26,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:26,842][root][INFO] - Training Epoch: 5/10, step 164/574 completed (loss: 0.18714353442192078, acc: 0.9047619104385376)
[2025-01-06 01:25:26,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:27,112][root][INFO] - Training Epoch: 5/10, step 165/574 completed (loss: 0.1678994596004486, acc: 0.9655172228813171)
[2025-01-06 01:25:27,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:27,406][root][INFO] - Training Epoch: 5/10, step 166/574 completed (loss: 0.08652909845113754, acc: 0.9591836929321289)
[2025-01-06 01:25:27,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:27,710][root][INFO] - Training Epoch: 5/10, step 167/574 completed (loss: 0.0699460431933403, acc: 0.9800000190734863)
[2025-01-06 01:25:27,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:28,096][root][INFO] - Training Epoch: 5/10, step 168/574 completed (loss: 0.23883168399333954, acc: 0.9166666865348816)
[2025-01-06 01:25:28,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:28,416][root][INFO] - Training Epoch: 5/10, step 169/574 completed (loss: 0.6741688847541809, acc: 0.8235294222831726)
[2025-01-06 01:25:28,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:29,441][root][INFO] - Training Epoch: 5/10, step 170/574 completed (loss: 0.4832727015018463, acc: 0.8424657583236694)
[2025-01-06 01:25:29,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:29,721][root][INFO] - Training Epoch: 5/10, step 171/574 completed (loss: 0.24136106669902802, acc: 0.9166666865348816)
[2025-01-06 01:25:29,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:29,970][root][INFO] - Training Epoch: 5/10, step 172/574 completed (loss: 0.279418021440506, acc: 0.9259259104728699)
[2025-01-06 01:25:30,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:30,256][root][INFO] - Training Epoch: 5/10, step 173/574 completed (loss: 0.28241419792175293, acc: 0.8928571343421936)
[2025-01-06 01:25:30,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:30,790][root][INFO] - Training Epoch: 5/10, step 174/574 completed (loss: 0.725368857383728, acc: 0.8141592741012573)
[2025-01-06 01:25:30,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:31,087][root][INFO] - Training Epoch: 5/10, step 175/574 completed (loss: 0.31294044852256775, acc: 0.8985507488250732)
[2025-01-06 01:25:31,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:31,398][root][INFO] - Training Epoch: 5/10, step 176/574 completed (loss: 0.27678382396698, acc: 0.9090909361839294)
[2025-01-06 01:25:31,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:32,303][root][INFO] - Training Epoch: 5/10, step 177/574 completed (loss: 0.8894486427307129, acc: 0.7557252049446106)
[2025-01-06 01:25:32,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:32,982][root][INFO] - Training Epoch: 5/10, step 178/574 completed (loss: 0.6728252172470093, acc: 0.8074073791503906)
[2025-01-06 01:25:33,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:33,295][root][INFO] - Training Epoch: 5/10, step 179/574 completed (loss: 0.18929582834243774, acc: 0.9344262480735779)
[2025-01-06 01:25:33,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:33,587][root][INFO] - Training Epoch: 5/10, step 180/574 completed (loss: 0.02224622666835785, acc: 1.0)
[2025-01-06 01:25:33,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:33,904][root][INFO] - Training Epoch: 5/10, step 181/574 completed (loss: 0.14887766540050507, acc: 0.9599999785423279)
[2025-01-06 01:25:34,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:34,257][root][INFO] - Training Epoch: 5/10, step 182/574 completed (loss: 0.03602030500769615, acc: 1.0)
[2025-01-06 01:25:34,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:34,605][root][INFO] - Training Epoch: 5/10, step 183/574 completed (loss: 0.1246364414691925, acc: 0.9634146094322205)
[2025-01-06 01:25:34,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:34,977][root][INFO] - Training Epoch: 5/10, step 184/574 completed (loss: 0.4223092496395111, acc: 0.903323233127594)
[2025-01-06 01:25:35,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:35,321][root][INFO] - Training Epoch: 5/10, step 185/574 completed (loss: 0.3342105746269226, acc: 0.8991354703903198)
[2025-01-06 01:25:35,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:35,810][root][INFO] - Training Epoch: 5/10, step 186/574 completed (loss: 0.32903796434402466, acc: 0.90625)
[2025-01-06 01:25:35,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:36,347][root][INFO] - Training Epoch: 5/10, step 187/574 completed (loss: 0.4311161935329437, acc: 0.8818011283874512)
[2025-01-06 01:25:36,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:36,751][root][INFO] - Training Epoch: 5/10, step 188/574 completed (loss: 0.3561861515045166, acc: 0.9110320210456848)
[2025-01-06 01:25:36,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:37,081][root][INFO] - Training Epoch: 5/10, step 189/574 completed (loss: 0.16705404222011566, acc: 0.9599999785423279)
[2025-01-06 01:25:37,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:37,628][root][INFO] - Training Epoch: 5/10, step 190/574 completed (loss: 0.6014307737350464, acc: 0.7790697813034058)
[2025-01-06 01:25:37,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:38,426][root][INFO] - Training Epoch: 5/10, step 191/574 completed (loss: 0.9525665640830994, acc: 0.7222222089767456)
[2025-01-06 01:25:38,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:39,345][root][INFO] - Training Epoch: 5/10, step 192/574 completed (loss: 0.7344503998756409, acc: 0.7954545617103577)
[2025-01-06 01:25:39,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:40,100][root][INFO] - Training Epoch: 5/10, step 193/574 completed (loss: 0.471028596162796, acc: 0.8588235378265381)
[2025-01-06 01:25:40,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:41,173][root][INFO] - Training Epoch: 5/10, step 194/574 completed (loss: 0.689363956451416, acc: 0.790123462677002)
[2025-01-06 01:25:41,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:42,127][root][INFO] - Training Epoch: 5/10, step 195/574 completed (loss: 0.15424779057502747, acc: 0.9516128897666931)
[2025-01-06 01:25:42,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:42,437][root][INFO] - Training Epoch: 5/10, step 196/574 completed (loss: 0.017183156684041023, acc: 1.0)
[2025-01-06 01:25:42,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:42,736][root][INFO] - Training Epoch: 5/10, step 197/574 completed (loss: 0.22089970111846924, acc: 0.925000011920929)
[2025-01-06 01:25:42,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:43,074][root][INFO] - Training Epoch: 5/10, step 198/574 completed (loss: 0.312086820602417, acc: 0.9411764740943909)
[2025-01-06 01:25:43,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:43,391][root][INFO] - Training Epoch: 5/10, step 199/574 completed (loss: 0.704999566078186, acc: 0.8088235259056091)
[2025-01-06 01:25:43,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:43,712][root][INFO] - Training Epoch: 5/10, step 200/574 completed (loss: 0.4487472474575043, acc: 0.8644067645072937)
[2025-01-06 01:25:43,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:44,043][root][INFO] - Training Epoch: 5/10, step 201/574 completed (loss: 0.4792948365211487, acc: 0.8283582329750061)
[2025-01-06 01:25:44,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:44,388][root][INFO] - Training Epoch: 5/10, step 202/574 completed (loss: 0.450886070728302, acc: 0.8349514603614807)
[2025-01-06 01:25:44,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:44,737][root][INFO] - Training Epoch: 5/10, step 203/574 completed (loss: 0.2656056582927704, acc: 0.9365079402923584)
[2025-01-06 01:25:44,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:45,068][root][INFO] - Training Epoch: 5/10, step 204/574 completed (loss: 0.009846214205026627, acc: 1.0)
[2025-01-06 01:25:45,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:45,394][root][INFO] - Training Epoch: 5/10, step 205/574 completed (loss: 0.15028609335422516, acc: 0.9372197389602661)
[2025-01-06 01:25:45,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:45,791][root][INFO] - Training Epoch: 5/10, step 206/574 completed (loss: 0.3447166383266449, acc: 0.8976377844810486)
[2025-01-06 01:25:45,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:46,157][root][INFO] - Training Epoch: 5/10, step 207/574 completed (loss: 0.23373562097549438, acc: 0.9353448152542114)
[2025-01-06 01:25:46,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:46,505][root][INFO] - Training Epoch: 5/10, step 208/574 completed (loss: 0.2838641107082367, acc: 0.9311594367027283)
[2025-01-06 01:25:46,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:46,873][root][INFO] - Training Epoch: 5/10, step 209/574 completed (loss: 0.3022327125072479, acc: 0.9105058312416077)
[2025-01-06 01:25:46,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:47,214][root][INFO] - Training Epoch: 5/10, step 210/574 completed (loss: 0.07576348632574081, acc: 0.967391312122345)
[2025-01-06 01:25:47,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:47,527][root][INFO] - Training Epoch: 5/10, step 211/574 completed (loss: 0.020645905286073685, acc: 1.0)
[2025-01-06 01:25:47,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:47,825][root][INFO] - Training Epoch: 5/10, step 212/574 completed (loss: 0.022029781714081764, acc: 1.0)
[2025-01-06 01:25:47,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:48,193][root][INFO] - Training Epoch: 5/10, step 213/574 completed (loss: 0.048863485455513, acc: 0.978723406791687)
[2025-01-06 01:25:48,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:48,900][root][INFO] - Training Epoch: 5/10, step 214/574 completed (loss: 0.15887190401554108, acc: 0.9769230484962463)
[2025-01-06 01:25:48,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:49,228][root][INFO] - Training Epoch: 5/10, step 215/574 completed (loss: 0.10033900290727615, acc: 0.9729729890823364)
[2025-01-06 01:25:49,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:49,559][root][INFO] - Training Epoch: 5/10, step 216/574 completed (loss: 0.05745922774076462, acc: 0.9651162624359131)
[2025-01-06 01:25:49,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:50,093][root][INFO] - Training Epoch: 5/10, step 217/574 completed (loss: 0.12909477949142456, acc: 0.9459459185600281)
[2025-01-06 01:25:50,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:50,493][root][INFO] - Training Epoch: 5/10, step 218/574 completed (loss: 0.08868568390607834, acc: 0.9555555582046509)
[2025-01-06 01:25:50,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:50,813][root][INFO] - Training Epoch: 5/10, step 219/574 completed (loss: 0.013406667858362198, acc: 1.0)
[2025-01-06 01:25:50,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:51,133][root][INFO] - Training Epoch: 5/10, step 220/574 completed (loss: 0.0038380224723368883, acc: 1.0)
[2025-01-06 01:25:51,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:51,443][root][INFO] - Training Epoch: 5/10, step 221/574 completed (loss: 0.03670699894428253, acc: 1.0)
[2025-01-06 01:25:51,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:51,772][root][INFO] - Training Epoch: 5/10, step 222/574 completed (loss: 0.2398536056280136, acc: 0.8846153616905212)
[2025-01-06 01:25:51,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:52,529][root][INFO] - Training Epoch: 5/10, step 223/574 completed (loss: 0.3017447590827942, acc: 0.929347813129425)
[2025-01-06 01:25:52,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:53,075][root][INFO] - Training Epoch: 5/10, step 224/574 completed (loss: 0.39195144176483154, acc: 0.8977272510528564)
[2025-01-06 01:25:53,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:53,518][root][INFO] - Training Epoch: 5/10, step 225/574 completed (loss: 0.45899882912635803, acc: 0.8829787373542786)
[2025-01-06 01:25:53,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:53,889][root][INFO] - Training Epoch: 5/10, step 226/574 completed (loss: 0.1963333934545517, acc: 0.9245283007621765)
[2025-01-06 01:25:53,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:54,241][root][INFO] - Training Epoch: 5/10, step 227/574 completed (loss: 0.10307898372411728, acc: 0.9833333492279053)
[2025-01-06 01:25:54,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:54,572][root][INFO] - Training Epoch: 5/10, step 228/574 completed (loss: 0.1353224515914917, acc: 0.9534883499145508)
[2025-01-06 01:25:54,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:54,895][root][INFO] - Training Epoch: 5/10, step 229/574 completed (loss: 0.32998424768447876, acc: 0.9666666388511658)
[2025-01-06 01:25:55,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:55,273][root][INFO] - Training Epoch: 5/10, step 230/574 completed (loss: 0.9734005331993103, acc: 0.7473683953285217)
[2025-01-06 01:25:55,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:55,647][root][INFO] - Training Epoch: 5/10, step 231/574 completed (loss: 0.9347737431526184, acc: 0.699999988079071)
[2025-01-06 01:25:55,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:56,060][root][INFO] - Training Epoch: 5/10, step 232/574 completed (loss: 0.9875932335853577, acc: 0.6944444179534912)
[2025-01-06 01:25:56,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:56,546][root][INFO] - Training Epoch: 5/10, step 233/574 completed (loss: 1.3000097274780273, acc: 0.6238532066345215)
[2025-01-06 01:25:56,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:57,013][root][INFO] - Training Epoch: 5/10, step 234/574 completed (loss: 0.7758925557136536, acc: 0.7307692170143127)
[2025-01-06 01:25:57,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:57,311][root][INFO] - Training Epoch: 5/10, step 235/574 completed (loss: 0.22973023355007172, acc: 0.9473684430122375)
[2025-01-06 01:25:57,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:57,609][root][INFO] - Training Epoch: 5/10, step 236/574 completed (loss: 0.013069742359220982, acc: 1.0)
[2025-01-06 01:25:57,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:57,908][root][INFO] - Training Epoch: 5/10, step 237/574 completed (loss: 0.15884198248386383, acc: 0.9090909361839294)
[2025-01-06 01:25:57,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:58,199][root][INFO] - Training Epoch: 5/10, step 238/574 completed (loss: 0.36364421248435974, acc: 0.9259259104728699)
[2025-01-06 01:25:58,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:58,492][root][INFO] - Training Epoch: 5/10, step 239/574 completed (loss: 0.2088516503572464, acc: 0.9142857193946838)
[2025-01-06 01:25:58,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:58,830][root][INFO] - Training Epoch: 5/10, step 240/574 completed (loss: 0.5573582053184509, acc: 0.8636363744735718)
[2025-01-06 01:25:58,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:59,233][root][INFO] - Training Epoch: 5/10, step 241/574 completed (loss: 0.1920190453529358, acc: 0.9545454382896423)
[2025-01-06 01:25:59,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:59,841][root][INFO] - Training Epoch: 5/10, step 242/574 completed (loss: 0.5842257738113403, acc: 0.8387096524238586)
[2025-01-06 01:25:59,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:00,378][root][INFO] - Training Epoch: 5/10, step 243/574 completed (loss: 0.5107370615005493, acc: 0.7727272510528564)
[2025-01-06 01:26:00,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:00,705][root][INFO] - Training Epoch: 5/10, step 244/574 completed (loss: 0.003218521596863866, acc: 1.0)
[2025-01-06 01:26:00,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:01,018][root][INFO] - Training Epoch: 5/10, step 245/574 completed (loss: 0.07818812876939774, acc: 0.9615384340286255)
[2025-01-06 01:26:01,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:01,333][root][INFO] - Training Epoch: 5/10, step 246/574 completed (loss: 0.01687948778271675, acc: 1.0)
[2025-01-06 01:26:01,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:01,674][root][INFO] - Training Epoch: 5/10, step 247/574 completed (loss: 0.01949911192059517, acc: 1.0)
[2025-01-06 01:26:01,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:02,043][root][INFO] - Training Epoch: 5/10, step 248/574 completed (loss: 0.18603216111660004, acc: 0.9459459185600281)
[2025-01-06 01:26:02,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:02,389][root][INFO] - Training Epoch: 5/10, step 249/574 completed (loss: 0.2098017930984497, acc: 0.8918918967247009)
[2025-01-06 01:26:02,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:02,790][root][INFO] - Training Epoch: 5/10, step 250/574 completed (loss: 0.012388368137180805, acc: 1.0)
[2025-01-06 01:26:02,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:03,166][root][INFO] - Training Epoch: 5/10, step 251/574 completed (loss: 0.13696187734603882, acc: 0.970588207244873)
[2025-01-06 01:26:03,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:03,529][root][INFO] - Training Epoch: 5/10, step 252/574 completed (loss: 0.012535854242742062, acc: 1.0)
[2025-01-06 01:26:03,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:03,857][root][INFO] - Training Epoch: 5/10, step 253/574 completed (loss: 0.0045270840637385845, acc: 1.0)
[2025-01-06 01:26:03,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:04,196][root][INFO] - Training Epoch: 5/10, step 254/574 completed (loss: 0.0021776207722723484, acc: 1.0)
[2025-01-06 01:26:04,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:04,579][root][INFO] - Training Epoch: 5/10, step 255/574 completed (loss: 0.05031290277838707, acc: 1.0)
[2025-01-06 01:26:04,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:04,928][root][INFO] - Training Epoch: 5/10, step 256/574 completed (loss: 0.016140755265951157, acc: 1.0)
[2025-01-06 01:26:05,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:05,251][root][INFO] - Training Epoch: 5/10, step 257/574 completed (loss: 0.0980723425745964, acc: 0.9714285731315613)
[2025-01-06 01:26:05,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:05,556][root][INFO] - Training Epoch: 5/10, step 258/574 completed (loss: 0.06765143573284149, acc: 0.9736841917037964)
[2025-01-06 01:26:05,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:06,118][root][INFO] - Training Epoch: 5/10, step 259/574 completed (loss: 0.2562592923641205, acc: 0.9433962106704712)
[2025-01-06 01:26:06,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:06,698][root][INFO] - Training Epoch: 5/10, step 260/574 completed (loss: 0.19170226156711578, acc: 0.9416666626930237)
[2025-01-06 01:26:06,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:06,980][root][INFO] - Training Epoch: 5/10, step 261/574 completed (loss: 0.14048077166080475, acc: 0.9166666865348816)
[2025-01-06 01:26:07,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:07,313][root][INFO] - Training Epoch: 5/10, step 262/574 completed (loss: 0.32756277918815613, acc: 0.9677419066429138)
[2025-01-06 01:26:07,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:07,681][root][INFO] - Training Epoch: 5/10, step 263/574 completed (loss: 0.7636575102806091, acc: 0.8266666531562805)
[2025-01-06 01:26:07,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:08,006][root][INFO] - Training Epoch: 5/10, step 264/574 completed (loss: 0.35506999492645264, acc: 0.8333333134651184)
[2025-01-06 01:26:08,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:08,856][root][INFO] - Training Epoch: 5/10, step 265/574 completed (loss: 0.8403472304344177, acc: 0.7519999742507935)
[2025-01-06 01:26:08,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:09,192][root][INFO] - Training Epoch: 5/10, step 266/574 completed (loss: 0.8482652902603149, acc: 0.7191011309623718)
[2025-01-06 01:26:09,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:09,540][root][INFO] - Training Epoch: 5/10, step 267/574 completed (loss: 0.4745960235595703, acc: 0.8513513803482056)
[2025-01-06 01:26:09,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:09,999][root][INFO] - Training Epoch: 5/10, step 268/574 completed (loss: 0.30627503991127014, acc: 0.931034505367279)
[2025-01-06 01:26:10,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:10,354][root][INFO] - Training Epoch: 5/10, step 269/574 completed (loss: 0.033590421080589294, acc: 1.0)
[2025-01-06 01:26:10,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:10,682][root][INFO] - Training Epoch: 5/10, step 270/574 completed (loss: 0.044903457164764404, acc: 0.9545454382896423)
[2025-01-06 01:26:10,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:10,983][root][INFO] - Training Epoch: 5/10, step 271/574 completed (loss: 0.12447575479745865, acc: 0.9375)
[2025-01-06 01:26:11,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:11,311][root][INFO] - Training Epoch: 5/10, step 272/574 completed (loss: 0.10528530925512314, acc: 0.9666666388511658)
[2025-01-06 01:26:11,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:11,703][root][INFO] - Training Epoch: 5/10, step 273/574 completed (loss: 0.11632576584815979, acc: 0.9833333492279053)
[2025-01-06 01:26:11,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:12,034][root][INFO] - Training Epoch: 5/10, step 274/574 completed (loss: 0.21135729551315308, acc: 0.9375)
[2025-01-06 01:26:12,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:12,375][root][INFO] - Training Epoch: 5/10, step 275/574 completed (loss: 0.12337944656610489, acc: 0.9666666388511658)
[2025-01-06 01:26:12,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:12,702][root][INFO] - Training Epoch: 5/10, step 276/574 completed (loss: 0.18845978379249573, acc: 0.931034505367279)
[2025-01-06 01:26:12,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:13,024][root][INFO] - Training Epoch: 5/10, step 277/574 completed (loss: 0.011342717334628105, acc: 1.0)
[2025-01-06 01:26:13,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:14,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:14,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:14,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:15,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:15,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:15,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:17,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:17,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:17,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:18,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:18,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:18,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:20,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:20,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:21,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:21,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:21,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:23,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:23,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:23,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:24,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:24,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:24,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:26,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:26,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:26,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:27,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:27,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:27,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:28,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:28,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:28,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:29,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:29,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:29,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:30,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:30,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:30,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:31,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:31,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:31,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:33,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:33,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:33,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:34,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:34,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:34,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:35,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:35,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:36,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:36,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:36,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:38,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:38,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:38,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:39,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:39,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:39,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:40,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:40,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:40,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:41,509][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0981, device='cuda:0') eval_epoch_loss=tensor(0.7410, device='cuda:0') eval_epoch_acc=tensor(0.8224, device='cuda:0')
[2025-01-06 01:26:41,510][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:26:41,510][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:26:41,726][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_278_loss_0.7410483360290527/model.pt
[2025-01-06 01:26:41,729][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:26:41,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:42,061][root][INFO] - Training Epoch: 5/10, step 278/574 completed (loss: 0.09841760993003845, acc: 0.978723406791687)
[2025-01-06 01:26:42,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:42,381][root][INFO] - Training Epoch: 5/10, step 279/574 completed (loss: 0.21515560150146484, acc: 0.9166666865348816)
[2025-01-06 01:26:42,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:42,688][root][INFO] - Training Epoch: 5/10, step 280/574 completed (loss: 0.03076830692589283, acc: 1.0)
[2025-01-06 01:26:42,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:43,107][root][INFO] - Training Epoch: 5/10, step 281/574 completed (loss: 0.2155061960220337, acc: 0.9277108311653137)
[2025-01-06 01:26:43,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:43,469][root][INFO] - Training Epoch: 5/10, step 282/574 completed (loss: 0.43983158469200134, acc: 0.8611111044883728)
[2025-01-06 01:26:43,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:43,816][root][INFO] - Training Epoch: 5/10, step 283/574 completed (loss: 0.055443018674850464, acc: 0.9736841917037964)
[2025-01-06 01:26:43,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:44,150][root][INFO] - Training Epoch: 5/10, step 284/574 completed (loss: 0.07251576334238052, acc: 0.970588207244873)
[2025-01-06 01:26:44,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:44,482][root][INFO] - Training Epoch: 5/10, step 285/574 completed (loss: 0.03566237539052963, acc: 1.0)
[2025-01-06 01:26:44,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:44,824][root][INFO] - Training Epoch: 5/10, step 286/574 completed (loss: 0.2673368752002716, acc: 0.9140625)
[2025-01-06 01:26:44,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:45,170][root][INFO] - Training Epoch: 5/10, step 287/574 completed (loss: 0.2714102864265442, acc: 0.9279999732971191)
[2025-01-06 01:26:45,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:45,501][root][INFO] - Training Epoch: 5/10, step 288/574 completed (loss: 0.16167007386684418, acc: 0.9450549483299255)
[2025-01-06 01:26:45,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:45,831][root][INFO] - Training Epoch: 5/10, step 289/574 completed (loss: 0.28776121139526367, acc: 0.8944099545478821)
[2025-01-06 01:26:45,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:46,184][root][INFO] - Training Epoch: 5/10, step 290/574 completed (loss: 0.39486512541770935, acc: 0.876288652420044)
[2025-01-06 01:26:46,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:46,512][root][INFO] - Training Epoch: 5/10, step 291/574 completed (loss: 0.008603562600910664, acc: 1.0)
[2025-01-06 01:26:46,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:46,863][root][INFO] - Training Epoch: 5/10, step 292/574 completed (loss: 0.038914937525987625, acc: 1.0)
[2025-01-06 01:26:46,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:47,221][root][INFO] - Training Epoch: 5/10, step 293/574 completed (loss: 0.0989656001329422, acc: 0.982758641242981)
[2025-01-06 01:26:47,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:47,690][root][INFO] - Training Epoch: 5/10, step 294/574 completed (loss: 0.1423625946044922, acc: 0.9272727370262146)
[2025-01-06 01:26:47,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:48,239][root][INFO] - Training Epoch: 5/10, step 295/574 completed (loss: 0.35886701941490173, acc: 0.907216489315033)
[2025-01-06 01:26:48,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:48,539][root][INFO] - Training Epoch: 5/10, step 296/574 completed (loss: 0.1325782835483551, acc: 0.9482758641242981)
[2025-01-06 01:26:48,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:48,826][root][INFO] - Training Epoch: 5/10, step 297/574 completed (loss: 0.07159997522830963, acc: 0.9629629850387573)
[2025-01-06 01:26:48,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:49,136][root][INFO] - Training Epoch: 5/10, step 298/574 completed (loss: 0.04519377276301384, acc: 1.0)
[2025-01-06 01:26:49,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:49,431][root][INFO] - Training Epoch: 5/10, step 299/574 completed (loss: 0.03854674845933914, acc: 0.9821428656578064)
[2025-01-06 01:26:49,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:49,763][root][INFO] - Training Epoch: 5/10, step 300/574 completed (loss: 0.02324402704834938, acc: 1.0)
[2025-01-06 01:26:49,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:50,100][root][INFO] - Training Epoch: 5/10, step 301/574 completed (loss: 0.03246243670582771, acc: 1.0)
[2025-01-06 01:26:50,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:50,437][root][INFO] - Training Epoch: 5/10, step 302/574 completed (loss: 0.008988060988485813, acc: 1.0)
[2025-01-06 01:26:50,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:50,739][root][INFO] - Training Epoch: 5/10, step 303/574 completed (loss: 0.007685565855354071, acc: 1.0)
[2025-01-06 01:26:50,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51,036][root][INFO] - Training Epoch: 5/10, step 304/574 completed (loss: 0.04933689162135124, acc: 0.96875)
[2025-01-06 01:26:51,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51,349][root][INFO] - Training Epoch: 5/10, step 305/574 completed (loss: 0.14838124811649323, acc: 0.9344262480735779)
[2025-01-06 01:26:51,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51,646][root][INFO] - Training Epoch: 5/10, step 306/574 completed (loss: 0.08371438086032867, acc: 0.9666666388511658)
[2025-01-06 01:26:51,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51,999][root][INFO] - Training Epoch: 5/10, step 307/574 completed (loss: 0.012091594748198986, acc: 1.0)
[2025-01-06 01:26:52,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:52,343][root][INFO] - Training Epoch: 5/10, step 308/574 completed (loss: 0.10057847946882248, acc: 0.95652174949646)
[2025-01-06 01:26:52,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:52,767][root][INFO] - Training Epoch: 5/10, step 309/574 completed (loss: 0.0778363049030304, acc: 0.9722222089767456)
[2025-01-06 01:26:52,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:53,115][root][INFO] - Training Epoch: 5/10, step 310/574 completed (loss: 0.04195217415690422, acc: 1.0)
[2025-01-06 01:26:53,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:53,436][root][INFO] - Training Epoch: 5/10, step 311/574 completed (loss: 0.07264120876789093, acc: 0.9871794581413269)
[2025-01-06 01:26:53,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:53,793][root][INFO] - Training Epoch: 5/10, step 312/574 completed (loss: 0.05606301501393318, acc: 0.9795918464660645)
[2025-01-06 01:26:53,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:54,130][root][INFO] - Training Epoch: 5/10, step 313/574 completed (loss: 0.0021378439851105213, acc: 1.0)
[2025-01-06 01:26:54,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:54,451][root][INFO] - Training Epoch: 5/10, step 314/574 completed (loss: 0.004006546456366777, acc: 1.0)
[2025-01-06 01:26:54,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:54,755][root][INFO] - Training Epoch: 5/10, step 315/574 completed (loss: 0.007942024618387222, acc: 1.0)
[2025-01-06 01:26:54,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:55,059][root][INFO] - Training Epoch: 5/10, step 316/574 completed (loss: 0.2300058901309967, acc: 0.9032257795333862)
[2025-01-06 01:26:55,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:55,391][root][INFO] - Training Epoch: 5/10, step 317/574 completed (loss: 0.02409793622791767, acc: 1.0)
[2025-01-06 01:26:55,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:55,736][root][INFO] - Training Epoch: 5/10, step 318/574 completed (loss: 0.03777694329619408, acc: 0.9903846383094788)
[2025-01-06 01:26:55,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:56,114][root][INFO] - Training Epoch: 5/10, step 319/574 completed (loss: 0.054370563477277756, acc: 0.9777777791023254)
[2025-01-06 01:26:56,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:56,498][root][INFO] - Training Epoch: 5/10, step 320/574 completed (loss: 0.022442061454057693, acc: 1.0)
[2025-01-06 01:26:56,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:56,830][root][INFO] - Training Epoch: 5/10, step 321/574 completed (loss: 0.02042566053569317, acc: 0.9800000190734863)
[2025-01-06 01:26:56,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:57,204][root][INFO] - Training Epoch: 5/10, step 322/574 completed (loss: 0.3985764682292938, acc: 0.8518518805503845)
[2025-01-06 01:26:57,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:57,534][root][INFO] - Training Epoch: 5/10, step 323/574 completed (loss: 0.3829922080039978, acc: 0.8571428656578064)
[2025-01-06 01:26:57,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:57,857][root][INFO] - Training Epoch: 5/10, step 324/574 completed (loss: 0.1872507780790329, acc: 0.8717948794364929)
[2025-01-06 01:26:57,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:58,155][root][INFO] - Training Epoch: 5/10, step 325/574 completed (loss: 0.49379831552505493, acc: 0.8048780560493469)
[2025-01-06 01:26:58,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:58,457][root][INFO] - Training Epoch: 5/10, step 326/574 completed (loss: 0.3369772732257843, acc: 0.9210526347160339)
[2025-01-06 01:26:58,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:58,754][root][INFO] - Training Epoch: 5/10, step 327/574 completed (loss: 0.009022724814713001, acc: 1.0)
[2025-01-06 01:26:58,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:59,069][root][INFO] - Training Epoch: 5/10, step 328/574 completed (loss: 0.06346061080694199, acc: 0.9642857313156128)
[2025-01-06 01:26:59,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:59,368][root][INFO] - Training Epoch: 5/10, step 329/574 completed (loss: 0.002022427273914218, acc: 1.0)
[2025-01-06 01:26:59,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:59,666][root][INFO] - Training Epoch: 5/10, step 330/574 completed (loss: 0.013742838986217976, acc: 1.0)
[2025-01-06 01:26:59,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:59,980][root][INFO] - Training Epoch: 5/10, step 331/574 completed (loss: 0.10784904658794403, acc: 0.9838709831237793)
[2025-01-06 01:27:00,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:00,349][root][INFO] - Training Epoch: 5/10, step 332/574 completed (loss: 0.15693975985050201, acc: 0.9473684430122375)
[2025-01-06 01:27:00,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:00,646][root][INFO] - Training Epoch: 5/10, step 333/574 completed (loss: 0.030967257916927338, acc: 1.0)
[2025-01-06 01:27:00,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:00,981][root][INFO] - Training Epoch: 5/10, step 334/574 completed (loss: 0.01908538118004799, acc: 1.0)
[2025-01-06 01:27:01,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:01,368][root][INFO] - Training Epoch: 5/10, step 335/574 completed (loss: 0.004602448549121618, acc: 1.0)
[2025-01-06 01:27:01,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:01,706][root][INFO] - Training Epoch: 5/10, step 336/574 completed (loss: 0.29855266213417053, acc: 0.8799999952316284)
[2025-01-06 01:27:01,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:02,032][root][INFO] - Training Epoch: 5/10, step 337/574 completed (loss: 0.7991811037063599, acc: 0.7586206793785095)
[2025-01-06 01:27:02,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:02,369][root][INFO] - Training Epoch: 5/10, step 338/574 completed (loss: 0.8361055254936218, acc: 0.7446808218955994)
[2025-01-06 01:27:02,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:02,704][root][INFO] - Training Epoch: 5/10, step 339/574 completed (loss: 0.5481107831001282, acc: 0.8072289228439331)
[2025-01-06 01:27:02,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03,010][root][INFO] - Training Epoch: 5/10, step 340/574 completed (loss: 0.0021453124936670065, acc: 1.0)
[2025-01-06 01:27:03,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03,305][root][INFO] - Training Epoch: 5/10, step 341/574 completed (loss: 0.0531175397336483, acc: 0.9743589758872986)
[2025-01-06 01:27:03,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03,611][root][INFO] - Training Epoch: 5/10, step 342/574 completed (loss: 0.1995822936296463, acc: 0.9156626462936401)
[2025-01-06 01:27:03,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03,918][root][INFO] - Training Epoch: 5/10, step 343/574 completed (loss: 0.31471776962280273, acc: 0.9245283007621765)
[2025-01-06 01:27:04,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:04,245][root][INFO] - Training Epoch: 5/10, step 344/574 completed (loss: 0.18801189959049225, acc: 0.9620253443717957)
[2025-01-06 01:27:04,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:04,564][root][INFO] - Training Epoch: 5/10, step 345/574 completed (loss: 0.09130245447158813, acc: 0.9411764740943909)
[2025-01-06 01:27:04,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:04,864][root][INFO] - Training Epoch: 5/10, step 346/574 completed (loss: 0.1372736692428589, acc: 0.9253731369972229)
[2025-01-06 01:27:04,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:05,160][root][INFO] - Training Epoch: 5/10, step 347/574 completed (loss: 0.0011219799052923918, acc: 1.0)
[2025-01-06 01:27:05,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:05,469][root][INFO] - Training Epoch: 5/10, step 348/574 completed (loss: 0.04960617050528526, acc: 0.9599999785423279)
[2025-01-06 01:27:05,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:05,842][root][INFO] - Training Epoch: 5/10, step 349/574 completed (loss: 0.31888872385025024, acc: 0.8888888955116272)
[2025-01-06 01:27:05,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:06,139][root][INFO] - Training Epoch: 5/10, step 350/574 completed (loss: 0.23383863270282745, acc: 0.930232584476471)
[2025-01-06 01:27:06,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:06,459][root][INFO] - Training Epoch: 5/10, step 351/574 completed (loss: 0.1721477061510086, acc: 0.9230769276618958)
[2025-01-06 01:27:06,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:06,821][root][INFO] - Training Epoch: 5/10, step 352/574 completed (loss: 0.14895373582839966, acc: 0.9555555582046509)
[2025-01-06 01:27:06,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:07,151][root][INFO] - Training Epoch: 5/10, step 353/574 completed (loss: 0.005822019651532173, acc: 1.0)
[2025-01-06 01:27:07,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:07,474][root][INFO] - Training Epoch: 5/10, step 354/574 completed (loss: 0.053257569670677185, acc: 0.9615384340286255)
[2025-01-06 01:27:07,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:07,800][root][INFO] - Training Epoch: 5/10, step 355/574 completed (loss: 0.3198077976703644, acc: 0.901098906993866)
[2025-01-06 01:27:07,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:08,291][root][INFO] - Training Epoch: 5/10, step 356/574 completed (loss: 0.36088666319847107, acc: 0.904347836971283)
[2025-01-06 01:27:08,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:08,616][root][INFO] - Training Epoch: 5/10, step 357/574 completed (loss: 0.17115704715251923, acc: 0.945652186870575)
[2025-01-06 01:27:08,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:08,952][root][INFO] - Training Epoch: 5/10, step 358/574 completed (loss: 0.13603730499744415, acc: 0.9795918464660645)
[2025-01-06 01:27:09,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:09,280][root][INFO] - Training Epoch: 5/10, step 359/574 completed (loss: 0.0006189599516801536, acc: 1.0)
[2025-01-06 01:27:09,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:09,582][root][INFO] - Training Epoch: 5/10, step 360/574 completed (loss: 0.04232315346598625, acc: 1.0)
[2025-01-06 01:27:09,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:09,895][root][INFO] - Training Epoch: 5/10, step 361/574 completed (loss: 0.2859255075454712, acc: 0.9024389982223511)
[2025-01-06 01:27:09,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:10,218][root][INFO] - Training Epoch: 5/10, step 362/574 completed (loss: 0.18168912827968597, acc: 0.9555555582046509)
[2025-01-06 01:27:10,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:10,525][root][INFO] - Training Epoch: 5/10, step 363/574 completed (loss: 0.013363061472773552, acc: 1.0)
[2025-01-06 01:27:10,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:10,836][root][INFO] - Training Epoch: 5/10, step 364/574 completed (loss: 0.016092421486973763, acc: 1.0)
[2025-01-06 01:27:10,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:11,161][root][INFO] - Training Epoch: 5/10, step 365/574 completed (loss: 0.020817842334508896, acc: 1.0)
[2025-01-06 01:27:11,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:11,465][root][INFO] - Training Epoch: 5/10, step 366/574 completed (loss: 0.00029713966068811715, acc: 1.0)
[2025-01-06 01:27:11,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:11,764][root][INFO] - Training Epoch: 5/10, step 367/574 completed (loss: 0.00237796432338655, acc: 1.0)
[2025-01-06 01:27:11,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:12,063][root][INFO] - Training Epoch: 5/10, step 368/574 completed (loss: 0.01721382513642311, acc: 1.0)
[2025-01-06 01:27:12,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:12,360][root][INFO] - Training Epoch: 5/10, step 369/574 completed (loss: 0.027505645528435707, acc: 1.0)
[2025-01-06 01:27:12,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:12,954][root][INFO] - Training Epoch: 5/10, step 370/574 completed (loss: 0.43578556180000305, acc: 0.8787878751754761)
[2025-01-06 01:27:13,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:13,782][root][INFO] - Training Epoch: 5/10, step 371/574 completed (loss: 0.20952768623828888, acc: 0.9339622855186462)
[2025-01-06 01:27:13,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:14,111][root][INFO] - Training Epoch: 5/10, step 372/574 completed (loss: 0.10694176703691483, acc: 0.9777777791023254)
[2025-01-06 01:27:14,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:14,409][root][INFO] - Training Epoch: 5/10, step 373/574 completed (loss: 0.10039450228214264, acc: 0.9642857313156128)
[2025-01-06 01:27:14,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:14,724][root][INFO] - Training Epoch: 5/10, step 374/574 completed (loss: 0.12462795525789261, acc: 0.9714285731315613)
[2025-01-06 01:27:14,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:15,017][root][INFO] - Training Epoch: 5/10, step 375/574 completed (loss: 0.0021491909865289927, acc: 1.0)
[2025-01-06 01:27:15,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:15,281][root][INFO] - Training Epoch: 5/10, step 376/574 completed (loss: 0.001702394220046699, acc: 1.0)
[2025-01-06 01:27:15,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:15,593][root][INFO] - Training Epoch: 5/10, step 377/574 completed (loss: 0.06860318034887314, acc: 0.9791666865348816)
[2025-01-06 01:27:15,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:15,894][root][INFO] - Training Epoch: 5/10, step 378/574 completed (loss: 0.029737984761595726, acc: 0.9894737005233765)
[2025-01-06 01:27:16,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:16,460][root][INFO] - Training Epoch: 5/10, step 379/574 completed (loss: 0.3092438876628876, acc: 0.9221556782722473)
[2025-01-06 01:27:16,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:16,860][root][INFO] - Training Epoch: 5/10, step 380/574 completed (loss: 0.1914842575788498, acc: 0.9548872113227844)
[2025-01-06 01:27:17,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:17,919][root][INFO] - Training Epoch: 5/10, step 381/574 completed (loss: 0.3580690324306488, acc: 0.8823529481887817)
[2025-01-06 01:27:18,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:18,481][root][INFO] - Training Epoch: 5/10, step 382/574 completed (loss: 0.047440916299819946, acc: 0.9909909963607788)
[2025-01-06 01:27:18,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:18,771][root][INFO] - Training Epoch: 5/10, step 383/574 completed (loss: 0.10627783089876175, acc: 0.9285714030265808)
[2025-01-06 01:27:18,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19,066][root][INFO] - Training Epoch: 5/10, step 384/574 completed (loss: 0.0309599582105875, acc: 0.9642857313156128)
[2025-01-06 01:27:19,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19,358][root][INFO] - Training Epoch: 5/10, step 385/574 completed (loss: 0.07928617298603058, acc: 0.96875)
[2025-01-06 01:27:19,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19,651][root][INFO] - Training Epoch: 5/10, step 386/574 completed (loss: 0.0024190342519432306, acc: 1.0)
[2025-01-06 01:27:19,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19,952][root][INFO] - Training Epoch: 5/10, step 387/574 completed (loss: 0.0020858810748904943, acc: 1.0)
[2025-01-06 01:27:20,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:20,281][root][INFO] - Training Epoch: 5/10, step 388/574 completed (loss: 0.0012550875544548035, acc: 1.0)
[2025-01-06 01:27:20,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:20,600][root][INFO] - Training Epoch: 5/10, step 389/574 completed (loss: 0.0056808809749782085, acc: 1.0)
[2025-01-06 01:27:20,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:20,901][root][INFO] - Training Epoch: 5/10, step 390/574 completed (loss: 0.16318468749523163, acc: 0.9523809552192688)
[2025-01-06 01:27:20,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:21,276][root][INFO] - Training Epoch: 5/10, step 391/574 completed (loss: 0.2815544903278351, acc: 0.8703703880310059)
[2025-01-06 01:27:21,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:21,618][root][INFO] - Training Epoch: 5/10, step 392/574 completed (loss: 0.5557165741920471, acc: 0.844660222530365)
[2025-01-06 01:27:21,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:22,136][root][INFO] - Training Epoch: 5/10, step 393/574 completed (loss: 0.6656886339187622, acc: 0.8014705777168274)
[2025-01-06 01:27:22,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:22,506][root][INFO] - Training Epoch: 5/10, step 394/574 completed (loss: 0.30066901445388794, acc: 0.8999999761581421)
[2025-01-06 01:27:22,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:22,889][root][INFO] - Training Epoch: 5/10, step 395/574 completed (loss: 0.42622601985931396, acc: 0.8472222089767456)
[2025-01-06 01:27:22,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:23,226][root][INFO] - Training Epoch: 5/10, step 396/574 completed (loss: 0.23898516595363617, acc: 0.930232584476471)
[2025-01-06 01:27:23,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:23,546][root][INFO] - Training Epoch: 5/10, step 397/574 completed (loss: 0.007470321375876665, acc: 1.0)
[2025-01-06 01:27:23,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:23,886][root][INFO] - Training Epoch: 5/10, step 398/574 completed (loss: 0.07655225694179535, acc: 1.0)
[2025-01-06 01:27:24,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:24,245][root][INFO] - Training Epoch: 5/10, step 399/574 completed (loss: 0.013977003283798695, acc: 1.0)
[2025-01-06 01:27:24,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:24,778][root][INFO] - Training Epoch: 5/10, step 400/574 completed (loss: 0.16584360599517822, acc: 0.9558823704719543)
[2025-01-06 01:27:24,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:25,119][root][INFO] - Training Epoch: 5/10, step 401/574 completed (loss: 0.27220937609672546, acc: 0.9200000166893005)
[2025-01-06 01:27:25,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:25,459][root][INFO] - Training Epoch: 5/10, step 402/574 completed (loss: 0.14876987040042877, acc: 0.939393937587738)
[2025-01-06 01:27:25,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:25,776][root][INFO] - Training Epoch: 5/10, step 403/574 completed (loss: 0.12289396673440933, acc: 0.9696969985961914)
[2025-01-06 01:27:25,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:26,099][root][INFO] - Training Epoch: 5/10, step 404/574 completed (loss: 0.08876708894968033, acc: 0.9677419066429138)
[2025-01-06 01:27:26,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:26,420][root][INFO] - Training Epoch: 5/10, step 405/574 completed (loss: 0.002753455424681306, acc: 1.0)
[2025-01-06 01:27:26,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:26,735][root][INFO] - Training Epoch: 5/10, step 406/574 completed (loss: 0.02520223706960678, acc: 1.0)
[2025-01-06 01:27:26,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:27,072][root][INFO] - Training Epoch: 5/10, step 407/574 completed (loss: 0.02466546930372715, acc: 1.0)
[2025-01-06 01:27:27,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:27,387][root][INFO] - Training Epoch: 5/10, step 408/574 completed (loss: 0.009954012930393219, acc: 1.0)
[2025-01-06 01:27:27,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:27,706][root][INFO] - Training Epoch: 5/10, step 409/574 completed (loss: 0.015940317884087563, acc: 1.0)
[2025-01-06 01:27:27,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:28,028][root][INFO] - Training Epoch: 5/10, step 410/574 completed (loss: 0.08798902481794357, acc: 0.982758641242981)
[2025-01-06 01:27:28,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:28,340][root][INFO] - Training Epoch: 5/10, step 411/574 completed (loss: 0.005843082908540964, acc: 1.0)
[2025-01-06 01:27:28,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:28,664][root][INFO] - Training Epoch: 5/10, step 412/574 completed (loss: 0.02417730540037155, acc: 1.0)
[2025-01-06 01:27:28,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29,001][root][INFO] - Training Epoch: 5/10, step 413/574 completed (loss: 0.02283143624663353, acc: 1.0)
[2025-01-06 01:27:29,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29,336][root][INFO] - Training Epoch: 5/10, step 414/574 completed (loss: 0.0018240053905174136, acc: 1.0)
[2025-01-06 01:27:29,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29,669][root][INFO] - Training Epoch: 5/10, step 415/574 completed (loss: 0.05823388695716858, acc: 0.9803921580314636)
[2025-01-06 01:27:29,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29,993][root][INFO] - Training Epoch: 5/10, step 416/574 completed (loss: 0.034528426826000214, acc: 1.0)
[2025-01-06 01:27:30,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:30,352][root][INFO] - Training Epoch: 5/10, step 417/574 completed (loss: 0.23003429174423218, acc: 0.9444444179534912)
[2025-01-06 01:27:30,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:30,725][root][INFO] - Training Epoch: 5/10, step 418/574 completed (loss: 0.08626978099346161, acc: 0.9750000238418579)
[2025-01-06 01:27:30,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:31,024][root][INFO] - Training Epoch: 5/10, step 419/574 completed (loss: 0.013079049065709114, acc: 1.0)
[2025-01-06 01:27:31,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:31,327][root][INFO] - Training Epoch: 5/10, step 420/574 completed (loss: 0.011386449448764324, acc: 1.0)
[2025-01-06 01:27:32,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:32,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:32,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:32,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:34,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:34,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:34,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:34,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:35,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:35,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:38,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:38,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:38,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:39,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:39,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:39,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:39,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:42,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:42,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:42,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:42,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:43,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:43,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:43,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:44,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:44,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:44,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:45,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:45,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:45,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:47,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:47,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:47,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:48,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:48,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:48,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:49,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:49,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:49,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:51,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:51,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:52,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:52,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:52,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:53,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:53,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:53,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:55,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:55,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:55,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:56,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:56,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:56,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:57,336][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3771, device='cuda:0') eval_epoch_loss=tensor(0.8659, device='cuda:0') eval_epoch_acc=tensor(0.8090, device='cuda:0')
[2025-01-06 01:27:57,337][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:27:57,338][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:27:57,568][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_421_loss_0.8658795952796936/model.pt
[2025-01-06 01:27:57,573][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:27:57,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:57,948][root][INFO] - Training Epoch: 5/10, step 421/574 completed (loss: 0.05972534418106079, acc: 1.0)
[2025-01-06 01:27:58,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:58,315][root][INFO] - Training Epoch: 5/10, step 422/574 completed (loss: 0.32626286149024963, acc: 0.9375)
[2025-01-06 01:27:58,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:58,735][root][INFO] - Training Epoch: 5/10, step 423/574 completed (loss: 0.19198353588581085, acc: 0.9444444179534912)
[2025-01-06 01:27:58,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:59,069][root][INFO] - Training Epoch: 5/10, step 424/574 completed (loss: 0.1071435809135437, acc: 0.9629629850387573)
[2025-01-06 01:27:59,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:59,434][root][INFO] - Training Epoch: 5/10, step 425/574 completed (loss: 0.2531212866306305, acc: 0.9696969985961914)
[2025-01-06 01:27:59,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:59,777][root][INFO] - Training Epoch: 5/10, step 426/574 completed (loss: 1.0019358396530151, acc: 0.8695651888847351)
[2025-01-06 01:27:59,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:00,122][root][INFO] - Training Epoch: 5/10, step 427/574 completed (loss: 0.2059108465909958, acc: 0.9729729890823364)
[2025-01-06 01:28:00,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:00,428][root][INFO] - Training Epoch: 5/10, step 428/574 completed (loss: 0.006504349876195192, acc: 1.0)
[2025-01-06 01:28:00,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:00,729][root][INFO] - Training Epoch: 5/10, step 429/574 completed (loss: 0.08881250768899918, acc: 0.95652174949646)
[2025-01-06 01:28:00,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01,029][root][INFO] - Training Epoch: 5/10, step 430/574 completed (loss: 0.1860564798116684, acc: 0.9629629850387573)
[2025-01-06 01:28:01,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01,326][root][INFO] - Training Epoch: 5/10, step 431/574 completed (loss: 0.09227841347455978, acc: 0.9629629850387573)
[2025-01-06 01:28:01,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01,621][root][INFO] - Training Epoch: 5/10, step 432/574 completed (loss: 0.0058765895664691925, acc: 1.0)
[2025-01-06 01:28:01,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01,965][root][INFO] - Training Epoch: 5/10, step 433/574 completed (loss: 0.10948709398508072, acc: 0.9722222089767456)
[2025-01-06 01:28:02,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:02,278][root][INFO] - Training Epoch: 5/10, step 434/574 completed (loss: 0.0029089106246829033, acc: 1.0)
[2025-01-06 01:28:02,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:02,611][root][INFO] - Training Epoch: 5/10, step 435/574 completed (loss: 0.28545039892196655, acc: 0.9696969985961914)
[2025-01-06 01:28:02,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:02,911][root][INFO] - Training Epoch: 5/10, step 436/574 completed (loss: 0.30063608288764954, acc: 0.8888888955116272)
[2025-01-06 01:28:02,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:03,218][root][INFO] - Training Epoch: 5/10, step 437/574 completed (loss: 0.13650186359882355, acc: 0.9545454382896423)
[2025-01-06 01:28:03,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:03,506][root][INFO] - Training Epoch: 5/10, step 438/574 completed (loss: 0.2887611389160156, acc: 0.9047619104385376)
[2025-01-06 01:28:03,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:03,802][root][INFO] - Training Epoch: 5/10, step 439/574 completed (loss: 0.1396147608757019, acc: 0.9487179517745972)
[2025-01-06 01:28:03,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:04,266][root][INFO] - Training Epoch: 5/10, step 440/574 completed (loss: 0.08272864669561386, acc: 0.9545454382896423)
[2025-01-06 01:28:04,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:04,968][root][INFO] - Training Epoch: 5/10, step 441/574 completed (loss: 0.5819900631904602, acc: 0.7839999794960022)
[2025-01-06 01:28:05,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:05,367][root][INFO] - Training Epoch: 5/10, step 442/574 completed (loss: 0.4259142577648163, acc: 0.8870967626571655)
[2025-01-06 01:28:05,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:06,016][root][INFO] - Training Epoch: 5/10, step 443/574 completed (loss: 0.44142815470695496, acc: 0.8756219148635864)
[2025-01-06 01:28:06,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:06,307][root][INFO] - Training Epoch: 5/10, step 444/574 completed (loss: 0.13079889118671417, acc: 0.9622641801834106)
[2025-01-06 01:28:06,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:06,713][root][INFO] - Training Epoch: 5/10, step 445/574 completed (loss: 0.08375641703605652, acc: 0.9772727489471436)
[2025-01-06 01:28:06,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:07,020][root][INFO] - Training Epoch: 5/10, step 446/574 completed (loss: 0.054686423391103745, acc: 0.95652174949646)
[2025-01-06 01:28:07,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:07,306][root][INFO] - Training Epoch: 5/10, step 447/574 completed (loss: 0.1443122923374176, acc: 0.9230769276618958)
[2025-01-06 01:28:07,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:07,628][root][INFO] - Training Epoch: 5/10, step 448/574 completed (loss: 0.11397700756788254, acc: 0.9642857313156128)
[2025-01-06 01:28:07,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:07,956][root][INFO] - Training Epoch: 5/10, step 449/574 completed (loss: 0.14293000102043152, acc: 0.9552238583564758)
[2025-01-06 01:28:08,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:08,277][root][INFO] - Training Epoch: 5/10, step 450/574 completed (loss: 0.03458733856678009, acc: 1.0)
[2025-01-06 01:28:08,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:08,580][root][INFO] - Training Epoch: 5/10, step 451/574 completed (loss: 0.14014242589473724, acc: 0.95652174949646)
[2025-01-06 01:28:08,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:08,878][root][INFO] - Training Epoch: 5/10, step 452/574 completed (loss: 0.10566980391740799, acc: 0.9743589758872986)
[2025-01-06 01:28:08,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:09,247][root][INFO] - Training Epoch: 5/10, step 453/574 completed (loss: 0.1797410100698471, acc: 0.9605262875556946)
[2025-01-06 01:28:09,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:09,584][root][INFO] - Training Epoch: 5/10, step 454/574 completed (loss: 0.08395107090473175, acc: 0.9795918464660645)
[2025-01-06 01:28:09,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:09,891][root][INFO] - Training Epoch: 5/10, step 455/574 completed (loss: 0.03396297246217728, acc: 1.0)
[2025-01-06 01:28:09,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:10,205][root][INFO] - Training Epoch: 5/10, step 456/574 completed (loss: 0.29229748249053955, acc: 0.9278350472450256)
[2025-01-06 01:28:10,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:10,522][root][INFO] - Training Epoch: 5/10, step 457/574 completed (loss: 0.02803819440305233, acc: 1.0)
[2025-01-06 01:28:10,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:10,896][root][INFO] - Training Epoch: 5/10, step 458/574 completed (loss: 0.3107026517391205, acc: 0.9069767594337463)
[2025-01-06 01:28:11,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:11,255][root][INFO] - Training Epoch: 5/10, step 459/574 completed (loss: 0.15906789898872375, acc: 0.9285714030265808)
[2025-01-06 01:28:11,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:11,572][root][INFO] - Training Epoch: 5/10, step 460/574 completed (loss: 0.08479038625955582, acc: 0.9629629850387573)
[2025-01-06 01:28:11,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:11,870][root][INFO] - Training Epoch: 5/10, step 461/574 completed (loss: 0.05863267183303833, acc: 0.9722222089767456)
[2025-01-06 01:28:11,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:12,170][root][INFO] - Training Epoch: 5/10, step 462/574 completed (loss: 0.1249641552567482, acc: 0.96875)
[2025-01-06 01:28:12,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:12,461][root][INFO] - Training Epoch: 5/10, step 463/574 completed (loss: 0.16094334423542023, acc: 0.9615384340286255)
[2025-01-06 01:28:12,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:12,764][root][INFO] - Training Epoch: 5/10, step 464/574 completed (loss: 0.014277730137109756, acc: 1.0)
[2025-01-06 01:28:12,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:13,058][root][INFO] - Training Epoch: 5/10, step 465/574 completed (loss: 0.09794668853282928, acc: 0.9642857313156128)
[2025-01-06 01:28:13,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:13,360][root][INFO] - Training Epoch: 5/10, step 466/574 completed (loss: 0.25670114159584045, acc: 0.891566276550293)
[2025-01-06 01:28:13,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:13,691][root][INFO] - Training Epoch: 5/10, step 467/574 completed (loss: 0.21240496635437012, acc: 0.9369369149208069)
[2025-01-06 01:28:13,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14,024][root][INFO] - Training Epoch: 5/10, step 468/574 completed (loss: 0.22396527230739594, acc: 0.9223300814628601)
[2025-01-06 01:28:14,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14,347][root][INFO] - Training Epoch: 5/10, step 469/574 completed (loss: 0.26671740412712097, acc: 0.9105691313743591)
[2025-01-06 01:28:14,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14,680][root][INFO] - Training Epoch: 5/10, step 470/574 completed (loss: 0.018523069098591805, acc: 1.0)
[2025-01-06 01:28:14,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14,994][root][INFO] - Training Epoch: 5/10, step 471/574 completed (loss: 0.05224587395787239, acc: 0.9642857313156128)
[2025-01-06 01:28:15,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:15,398][root][INFO] - Training Epoch: 5/10, step 472/574 completed (loss: 0.18687044084072113, acc: 0.9313725233078003)
[2025-01-06 01:28:15,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:15,740][root][INFO] - Training Epoch: 5/10, step 473/574 completed (loss: 0.5569639205932617, acc: 0.8122270703315735)
[2025-01-06 01:28:15,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16,038][root][INFO] - Training Epoch: 5/10, step 474/574 completed (loss: 0.20565026998519897, acc: 0.9583333134651184)
[2025-01-06 01:28:16,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16,341][root][INFO] - Training Epoch: 5/10, step 475/574 completed (loss: 0.1756172925233841, acc: 0.9325153231620789)
[2025-01-06 01:28:16,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16,647][root][INFO] - Training Epoch: 5/10, step 476/574 completed (loss: 0.219427689909935, acc: 0.9136690497398376)
[2025-01-06 01:28:16,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16,997][root][INFO] - Training Epoch: 5/10, step 477/574 completed (loss: 0.3836537003517151, acc: 0.8944723606109619)
[2025-01-06 01:28:17,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:17,333][root][INFO] - Training Epoch: 5/10, step 478/574 completed (loss: 0.21776826679706573, acc: 0.9444444179534912)
[2025-01-06 01:28:17,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:17,638][root][INFO] - Training Epoch: 5/10, step 479/574 completed (loss: 0.054890722036361694, acc: 1.0)
[2025-01-06 01:28:17,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:17,942][root][INFO] - Training Epoch: 5/10, step 480/574 completed (loss: 0.055706318467855453, acc: 0.9629629850387573)
[2025-01-06 01:28:18,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:18,238][root][INFO] - Training Epoch: 5/10, step 481/574 completed (loss: 0.021400583907961845, acc: 1.0)
[2025-01-06 01:28:18,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:18,518][root][INFO] - Training Epoch: 5/10, step 482/574 completed (loss: 0.11448393017053604, acc: 1.0)
[2025-01-06 01:28:18,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:18,878][root][INFO] - Training Epoch: 5/10, step 483/574 completed (loss: 0.3671496510505676, acc: 0.931034505367279)
[2025-01-06 01:28:18,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:19,178][root][INFO] - Training Epoch: 5/10, step 484/574 completed (loss: 0.15750494599342346, acc: 0.9354838728904724)
[2025-01-06 01:28:19,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:19,473][root][INFO] - Training Epoch: 5/10, step 485/574 completed (loss: 0.04855506122112274, acc: 1.0)
[2025-01-06 01:28:19,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:19,778][root][INFO] - Training Epoch: 5/10, step 486/574 completed (loss: 0.08368300646543503, acc: 1.0)
[2025-01-06 01:28:19,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:20,078][root][INFO] - Training Epoch: 5/10, step 487/574 completed (loss: 0.11565173417329788, acc: 0.9523809552192688)
[2025-01-06 01:28:20,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:20,375][root][INFO] - Training Epoch: 5/10, step 488/574 completed (loss: 0.02106049656867981, acc: 1.0)
[2025-01-06 01:28:20,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:20,716][root][INFO] - Training Epoch: 5/10, step 489/574 completed (loss: 0.4538688361644745, acc: 0.800000011920929)
[2025-01-06 01:28:20,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:21,004][root][INFO] - Training Epoch: 5/10, step 490/574 completed (loss: 0.13854964077472687, acc: 0.9666666388511658)
[2025-01-06 01:28:21,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:21,296][root][INFO] - Training Epoch: 5/10, step 491/574 completed (loss: 0.031529948115348816, acc: 1.0)
[2025-01-06 01:28:21,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:21,591][root][INFO] - Training Epoch: 5/10, step 492/574 completed (loss: 0.16328838467597961, acc: 0.9607843160629272)
[2025-01-06 01:28:21,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:21,889][root][INFO] - Training Epoch: 5/10, step 493/574 completed (loss: 0.09819270670413971, acc: 0.9655172228813171)
[2025-01-06 01:28:21,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:22,181][root][INFO] - Training Epoch: 5/10, step 494/574 completed (loss: 0.04672465845942497, acc: 1.0)
[2025-01-06 01:28:22,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:22,519][root][INFO] - Training Epoch: 5/10, step 495/574 completed (loss: 0.046063680201768875, acc: 1.0)
[2025-01-06 01:28:22,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:22,877][root][INFO] - Training Epoch: 5/10, step 496/574 completed (loss: 0.4715263843536377, acc: 0.8839285969734192)
[2025-01-06 01:28:22,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:23,278][root][INFO] - Training Epoch: 5/10, step 497/574 completed (loss: 0.17339536547660828, acc: 0.932584285736084)
[2025-01-06 01:28:23,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:23,614][root][INFO] - Training Epoch: 5/10, step 498/574 completed (loss: 0.36544230580329895, acc: 0.8764045238494873)
[2025-01-06 01:28:23,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:23,961][root][INFO] - Training Epoch: 5/10, step 499/574 completed (loss: 0.8275190591812134, acc: 0.7517730593681335)
[2025-01-06 01:28:24,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:24,283][root][INFO] - Training Epoch: 5/10, step 500/574 completed (loss: 0.4007751941680908, acc: 0.8913043737411499)
[2025-01-06 01:28:24,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:24,610][root][INFO] - Training Epoch: 5/10, step 501/574 completed (loss: 0.0318145789206028, acc: 1.0)
[2025-01-06 01:28:24,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:24,925][root][INFO] - Training Epoch: 5/10, step 502/574 completed (loss: 0.041463810950517654, acc: 0.9615384340286255)
[2025-01-06 01:28:25,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:25,208][root][INFO] - Training Epoch: 5/10, step 503/574 completed (loss: 0.032365262508392334, acc: 1.0)
[2025-01-06 01:28:25,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:25,536][root][INFO] - Training Epoch: 5/10, step 504/574 completed (loss: 0.006329520605504513, acc: 1.0)
[2025-01-06 01:28:25,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:25,857][root][INFO] - Training Epoch: 5/10, step 505/574 completed (loss: 0.24671514332294464, acc: 0.9245283007621765)
[2025-01-06 01:28:25,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:26,140][root][INFO] - Training Epoch: 5/10, step 506/574 completed (loss: 0.2117394506931305, acc: 0.931034505367279)
[2025-01-06 01:28:26,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:26,721][root][INFO] - Training Epoch: 5/10, step 507/574 completed (loss: 0.6604424715042114, acc: 0.7747747898101807)
[2025-01-06 01:28:26,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:27,159][root][INFO] - Training Epoch: 5/10, step 508/574 completed (loss: 0.3685126006603241, acc: 0.9154929518699646)
[2025-01-06 01:28:27,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:27,456][root][INFO] - Training Epoch: 5/10, step 509/574 completed (loss: 0.018906155601143837, acc: 1.0)
[2025-01-06 01:28:27,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:27,763][root][INFO] - Training Epoch: 5/10, step 510/574 completed (loss: 0.00624294625595212, acc: 1.0)
[2025-01-06 01:28:27,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:28,056][root][INFO] - Training Epoch: 5/10, step 511/574 completed (loss: 0.007167731877416372, acc: 1.0)
[2025-01-06 01:28:29,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:30,436][root][INFO] - Training Epoch: 5/10, step 512/574 completed (loss: 0.7987813949584961, acc: 0.7642857432365417)
[2025-01-06 01:28:30,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:31,196][root][INFO] - Training Epoch: 5/10, step 513/574 completed (loss: 0.08580043911933899, acc: 0.9682539701461792)
[2025-01-06 01:28:31,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:31,469][root][INFO] - Training Epoch: 5/10, step 514/574 completed (loss: 0.025043707340955734, acc: 1.0)
[2025-01-06 01:28:31,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:31,767][root][INFO] - Training Epoch: 5/10, step 515/574 completed (loss: 0.008436575531959534, acc: 1.0)
[2025-01-06 01:28:31,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:32,454][root][INFO] - Training Epoch: 5/10, step 516/574 completed (loss: 0.3482547402381897, acc: 0.8888888955116272)
[2025-01-06 01:28:32,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:32,723][root][INFO] - Training Epoch: 5/10, step 517/574 completed (loss: 0.001753759104758501, acc: 1.0)
[2025-01-06 01:28:32,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:33,015][root][INFO] - Training Epoch: 5/10, step 518/574 completed (loss: 0.02795582078397274, acc: 1.0)
[2025-01-06 01:28:33,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:33,318][root][INFO] - Training Epoch: 5/10, step 519/574 completed (loss: 0.05600149184465408, acc: 1.0)
[2025-01-06 01:28:33,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:33,617][root][INFO] - Training Epoch: 5/10, step 520/574 completed (loss: 0.1651598960161209, acc: 0.8888888955116272)
[2025-01-06 01:28:33,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:34,592][root][INFO] - Training Epoch: 5/10, step 521/574 completed (loss: 0.4728981554508209, acc: 0.8644067645072937)
[2025-01-06 01:28:34,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:34,934][root][INFO] - Training Epoch: 5/10, step 522/574 completed (loss: 0.17295943200588226, acc: 0.9328358173370361)
[2025-01-06 01:28:35,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:35,264][root][INFO] - Training Epoch: 5/10, step 523/574 completed (loss: 0.22159600257873535, acc: 0.9416058659553528)
[2025-01-06 01:28:35,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:35,823][root][INFO] - Training Epoch: 5/10, step 524/574 completed (loss: 0.45327651500701904, acc: 0.8899999856948853)
[2025-01-06 01:28:35,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:36,128][root][INFO] - Training Epoch: 5/10, step 525/574 completed (loss: 0.011081267148256302, acc: 1.0)
[2025-01-06 01:28:36,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:36,433][root][INFO] - Training Epoch: 5/10, step 526/574 completed (loss: 0.1757577508687973, acc: 0.942307710647583)
[2025-01-06 01:28:36,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:36,730][root][INFO] - Training Epoch: 5/10, step 527/574 completed (loss: 0.4855863153934479, acc: 0.8571428656578064)
[2025-01-06 01:28:36,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37,053][root][INFO] - Training Epoch: 5/10, step 528/574 completed (loss: 0.47921648621559143, acc: 0.8196721076965332)
[2025-01-06 01:28:37,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37,354][root][INFO] - Training Epoch: 5/10, step 529/574 completed (loss: 0.05946027487516403, acc: 0.9830508232116699)
[2025-01-06 01:28:37,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37,652][root][INFO] - Training Epoch: 5/10, step 530/574 completed (loss: 0.3312564194202423, acc: 0.8604651093482971)
[2025-01-06 01:28:37,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37,956][root][INFO] - Training Epoch: 5/10, step 531/574 completed (loss: 0.12837056815624237, acc: 0.9318181872367859)
[2025-01-06 01:28:38,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:38,261][root][INFO] - Training Epoch: 5/10, step 532/574 completed (loss: 0.2443954348564148, acc: 0.9056603908538818)
[2025-01-06 01:28:38,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:38,566][root][INFO] - Training Epoch: 5/10, step 533/574 completed (loss: 0.23335400223731995, acc: 0.9318181872367859)
[2025-01-06 01:28:38,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:38,861][root][INFO] - Training Epoch: 5/10, step 534/574 completed (loss: 0.05668112635612488, acc: 1.0)
[2025-01-06 01:28:38,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:39,163][root][INFO] - Training Epoch: 5/10, step 535/574 completed (loss: 0.0570184662938118, acc: 1.0)
[2025-01-06 01:28:39,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:39,453][root][INFO] - Training Epoch: 5/10, step 536/574 completed (loss: 0.05597896873950958, acc: 0.9545454382896423)
[2025-01-06 01:28:39,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:39,838][root][INFO] - Training Epoch: 5/10, step 537/574 completed (loss: 0.2610642910003662, acc: 0.9230769276618958)
[2025-01-06 01:28:39,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:40,156][root][INFO] - Training Epoch: 5/10, step 538/574 completed (loss: 0.29422658681869507, acc: 0.921875)
[2025-01-06 01:28:40,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:40,538][root][INFO] - Training Epoch: 5/10, step 539/574 completed (loss: 0.18852569162845612, acc: 0.90625)
[2025-01-06 01:28:40,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:40,824][root][INFO] - Training Epoch: 5/10, step 540/574 completed (loss: 0.08925161510705948, acc: 1.0)
[2025-01-06 01:28:40,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:41,126][root][INFO] - Training Epoch: 5/10, step 541/574 completed (loss: 0.09762930870056152, acc: 0.9375)
[2025-01-06 01:28:41,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:41,430][root][INFO] - Training Epoch: 5/10, step 542/574 completed (loss: 0.002790506463497877, acc: 1.0)
[2025-01-06 01:28:41,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:41,718][root][INFO] - Training Epoch: 5/10, step 543/574 completed (loss: 0.00041984787094406784, acc: 1.0)
[2025-01-06 01:28:41,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:42,016][root][INFO] - Training Epoch: 5/10, step 544/574 completed (loss: 0.0213253702968359, acc: 1.0)
[2025-01-06 01:28:42,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:42,365][root][INFO] - Training Epoch: 5/10, step 545/574 completed (loss: 0.015063999220728874, acc: 1.0)
[2025-01-06 01:28:42,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:42,698][root][INFO] - Training Epoch: 5/10, step 546/574 completed (loss: 0.0033401392865926027, acc: 1.0)
[2025-01-06 01:28:42,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43,010][root][INFO] - Training Epoch: 5/10, step 547/574 completed (loss: 0.00243164598941803, acc: 1.0)
[2025-01-06 01:28:43,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43,308][root][INFO] - Training Epoch: 5/10, step 548/574 completed (loss: 0.008094551041722298, acc: 1.0)
[2025-01-06 01:28:43,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43,618][root][INFO] - Training Epoch: 5/10, step 549/574 completed (loss: 0.0011622058227658272, acc: 1.0)
[2025-01-06 01:28:43,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43,896][root][INFO] - Training Epoch: 5/10, step 550/574 completed (loss: 0.13783207535743713, acc: 0.9090909361839294)
[2025-01-06 01:28:43,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:44,197][root][INFO] - Training Epoch: 5/10, step 551/574 completed (loss: 0.0215439572930336, acc: 1.0)
[2025-01-06 01:28:44,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:44,502][root][INFO] - Training Epoch: 5/10, step 552/574 completed (loss: 0.11768998950719833, acc: 0.9571428298950195)
[2025-01-06 01:28:44,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:44,811][root][INFO] - Training Epoch: 5/10, step 553/574 completed (loss: 0.2207862138748169, acc: 0.9270073175430298)
[2025-01-06 01:28:44,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:45,132][root][INFO] - Training Epoch: 5/10, step 554/574 completed (loss: 0.1412343531847, acc: 0.951724112033844)
[2025-01-06 01:28:45,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:45,479][root][INFO] - Training Epoch: 5/10, step 555/574 completed (loss: 0.18571823835372925, acc: 0.9642857313156128)
[2025-01-06 01:28:45,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:45,825][root][INFO] - Training Epoch: 5/10, step 556/574 completed (loss: 0.3549996614456177, acc: 0.9139072895050049)
[2025-01-06 01:28:45,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:46,213][root][INFO] - Training Epoch: 5/10, step 557/574 completed (loss: 0.03925211727619171, acc: 0.9914529919624329)
[2025-01-06 01:28:46,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:46,541][root][INFO] - Training Epoch: 5/10, step 558/574 completed (loss: 0.1896653026342392, acc: 0.9200000166893005)
[2025-01-06 01:28:46,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:46,854][root][INFO] - Training Epoch: 5/10, step 559/574 completed (loss: 0.018321586772799492, acc: 1.0)
[2025-01-06 01:28:46,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:47,151][root][INFO] - Training Epoch: 5/10, step 560/574 completed (loss: 0.0023829187266528606, acc: 1.0)
[2025-01-06 01:28:47,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:47,449][root][INFO] - Training Epoch: 5/10, step 561/574 completed (loss: 0.034811411052942276, acc: 0.9743589758872986)
[2025-01-06 01:28:47,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:47,773][root][INFO] - Training Epoch: 5/10, step 562/574 completed (loss: 0.1748993843793869, acc: 0.9444444179534912)
[2025-01-06 01:28:47,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:48,059][root][INFO] - Training Epoch: 5/10, step 563/574 completed (loss: 0.19431708753108978, acc: 0.9610389471054077)
[2025-01-06 01:28:48,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:49,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:49,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:49,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:50,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:50,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:50,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:50,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:51,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:51,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:51,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:52,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:52,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:52,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:53,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:53,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:53,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:54,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:54,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:54,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:56,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:56,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:56,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:57,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:57,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:57,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:58,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:58,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:58,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:59,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:59,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:59,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:00,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:00,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:00,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:01,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:01,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:01,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:02,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:02,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:02,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:04,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:04,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:04,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:05,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:05,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:05,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:06,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:06,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:06,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:07,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:07,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:07,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:08,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:08,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:08,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:09,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:09,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:09,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:09,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:10,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:10,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:10,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:11,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:11,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:11,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:12,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:12,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:13,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:13,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:13,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:14,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:14,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:14,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:15,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:15,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:15,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:16,300][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2952, device='cuda:0') eval_epoch_loss=tensor(0.8308, device='cuda:0') eval_epoch_acc=tensor(0.8188, device='cuda:0')
[2025-01-06 01:29:16,302][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:29:16,302][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:29:16,610][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_564_loss_0.8308197259902954/model.pt
[2025-01-06 01:29:16,615][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:29:16,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:16,953][root][INFO] - Training Epoch: 5/10, step 564/574 completed (loss: 0.06216047331690788, acc: 0.9791666865348816)
[2025-01-06 01:29:17,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:17,292][root][INFO] - Training Epoch: 5/10, step 565/574 completed (loss: 0.04380826652050018, acc: 1.0)
[2025-01-06 01:29:17,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:17,622][root][INFO] - Training Epoch: 5/10, step 566/574 completed (loss: 0.11544660478830338, acc: 0.9642857313156128)
[2025-01-06 01:29:17,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:17,961][root][INFO] - Training Epoch: 5/10, step 567/574 completed (loss: 0.0118095763027668, acc: 1.0)
[2025-01-06 01:29:18,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:18,307][root][INFO] - Training Epoch: 5/10, step 568/574 completed (loss: 0.010140984319150448, acc: 1.0)
[2025-01-06 01:29:18,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:18,700][root][INFO] - Training Epoch: 5/10, step 569/574 completed (loss: 0.11945288628339767, acc: 0.9679144620895386)
[2025-01-06 01:29:18,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:19,031][root][INFO] - Training Epoch: 5/10, step 570/574 completed (loss: 0.012370442971587181, acc: 1.0)
[2025-01-06 01:29:19,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:19,399][root][INFO] - Training Epoch: 5/10, step 571/574 completed (loss: 0.08038023859262466, acc: 0.9743589758872986)
[2025-01-06 01:29:19,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:19,730][root][INFO] - Training Epoch: 5/10, step 572/574 completed (loss: 0.2993830144405365, acc: 0.9081632494926453)
[2025-01-06 01:29:19,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:20,092][root][INFO] - Training Epoch: 5/10, step 573/574 completed (loss: 0.14194442331790924, acc: 0.9433962106704712)
[2025-01-06 01:29:20,626][slam_llm.utils.train_utils][INFO] - Epoch 5: train_perplexity=1.2426, train_epoch_loss=0.2172, epoch time 329.9822268038988s
[2025-01-06 01:29:20,626][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:29:20,627][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:29:20,627][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:29:20,627][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 14
[2025-01-06 01:29:20,627][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:29:21,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:21,442][root][INFO] - Training Epoch: 6/10, step 0/574 completed (loss: 0.21888138353824615, acc: 0.9259259104728699)
[2025-01-06 01:29:21,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:21,737][root][INFO] - Training Epoch: 6/10, step 1/574 completed (loss: 0.05514974892139435, acc: 0.9599999785423279)
[2025-01-06 01:29:21,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:22,035][root][INFO] - Training Epoch: 6/10, step 2/574 completed (loss: 0.4016215205192566, acc: 0.837837815284729)
[2025-01-06 01:29:22,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:22,373][root][INFO] - Training Epoch: 6/10, step 3/574 completed (loss: 0.015138003043830395, acc: 1.0)
[2025-01-06 01:29:22,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:22,712][root][INFO] - Training Epoch: 6/10, step 4/574 completed (loss: 0.04636096581816673, acc: 0.9729729890823364)
[2025-01-06 01:29:22,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23,031][root][INFO] - Training Epoch: 6/10, step 5/574 completed (loss: 0.013430655933916569, acc: 1.0)
[2025-01-06 01:29:23,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23,356][root][INFO] - Training Epoch: 6/10, step 6/574 completed (loss: 0.1335483342409134, acc: 0.9795918464660645)
[2025-01-06 01:29:23,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23,656][root][INFO] - Training Epoch: 6/10, step 7/574 completed (loss: 0.0330473817884922, acc: 1.0)
[2025-01-06 01:29:23,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23,994][root][INFO] - Training Epoch: 6/10, step 8/574 completed (loss: 0.02420175075531006, acc: 1.0)
[2025-01-06 01:29:24,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24,312][root][INFO] - Training Epoch: 6/10, step 9/574 completed (loss: 0.002954151015728712, acc: 1.0)
[2025-01-06 01:29:24,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24,623][root][INFO] - Training Epoch: 6/10, step 10/574 completed (loss: 0.0028377408161759377, acc: 1.0)
[2025-01-06 01:29:24,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24,944][root][INFO] - Training Epoch: 6/10, step 11/574 completed (loss: 0.07534310966730118, acc: 0.9743589758872986)
[2025-01-06 01:29:25,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:25,241][root][INFO] - Training Epoch: 6/10, step 12/574 completed (loss: 0.025261929258704185, acc: 1.0)
[2025-01-06 01:29:25,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:25,571][root][INFO] - Training Epoch: 6/10, step 13/574 completed (loss: 0.09594035148620605, acc: 0.95652174949646)
[2025-01-06 01:29:25,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:25,888][root][INFO] - Training Epoch: 6/10, step 14/574 completed (loss: 0.01945626735687256, acc: 1.0)
[2025-01-06 01:29:25,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:26,205][root][INFO] - Training Epoch: 6/10, step 15/574 completed (loss: 0.07145198434591293, acc: 0.9795918464660645)
[2025-01-06 01:29:26,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:26,583][root][INFO] - Training Epoch: 6/10, step 16/574 completed (loss: 0.26921346783638, acc: 0.9473684430122375)
[2025-01-06 01:29:26,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:26,920][root][INFO] - Training Epoch: 6/10, step 17/574 completed (loss: 0.07026174664497375, acc: 0.9583333134651184)
[2025-01-06 01:29:27,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:27,241][root][INFO] - Training Epoch: 6/10, step 18/574 completed (loss: 0.13134869933128357, acc: 0.9444444179534912)
[2025-01-06 01:29:27,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:27,553][root][INFO] - Training Epoch: 6/10, step 19/574 completed (loss: 0.3274020254611969, acc: 0.9473684430122375)
[2025-01-06 01:29:27,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:27,886][root][INFO] - Training Epoch: 6/10, step 20/574 completed (loss: 0.020866453647613525, acc: 1.0)
[2025-01-06 01:29:27,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:28,227][root][INFO] - Training Epoch: 6/10, step 21/574 completed (loss: 0.008114096708595753, acc: 1.0)
[2025-01-06 01:29:28,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:28,576][root][INFO] - Training Epoch: 6/10, step 22/574 completed (loss: 0.39773234724998474, acc: 0.9200000166893005)
[2025-01-06 01:29:28,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:28,920][root][INFO] - Training Epoch: 6/10, step 23/574 completed (loss: 0.28470227122306824, acc: 0.9047619104385376)
[2025-01-06 01:29:29,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:29,253][root][INFO] - Training Epoch: 6/10, step 24/574 completed (loss: 0.0035589663311839104, acc: 1.0)
[2025-01-06 01:29:29,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:29,604][root][INFO] - Training Epoch: 6/10, step 25/574 completed (loss: 0.05961878225207329, acc: 1.0)
[2025-01-06 01:29:29,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:29,948][root][INFO] - Training Epoch: 6/10, step 26/574 completed (loss: 0.1453183889389038, acc: 0.9863013625144958)
[2025-01-06 01:29:30,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:31,159][root][INFO] - Training Epoch: 6/10, step 27/574 completed (loss: 0.5264865159988403, acc: 0.8695651888847351)
[2025-01-06 01:29:31,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:31,497][root][INFO] - Training Epoch: 6/10, step 28/574 completed (loss: 0.14593735337257385, acc: 0.930232584476471)
[2025-01-06 01:29:31,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:31,847][root][INFO] - Training Epoch: 6/10, step 29/574 completed (loss: 0.19018776714801788, acc: 0.9397590160369873)
[2025-01-06 01:29:31,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:32,201][root][INFO] - Training Epoch: 6/10, step 30/574 completed (loss: 0.15909940004348755, acc: 0.9259259104728699)
[2025-01-06 01:29:32,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:32,532][root][INFO] - Training Epoch: 6/10, step 31/574 completed (loss: 0.18064646422863007, acc: 0.9642857313156128)
[2025-01-06 01:29:32,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:32,872][root][INFO] - Training Epoch: 6/10, step 32/574 completed (loss: 0.07108241319656372, acc: 0.9629629850387573)
[2025-01-06 01:29:32,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33,237][root][INFO] - Training Epoch: 6/10, step 33/574 completed (loss: 0.11650685966014862, acc: 0.95652174949646)
[2025-01-06 01:29:33,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33,593][root][INFO] - Training Epoch: 6/10, step 34/574 completed (loss: 0.20998062193393707, acc: 0.9327731132507324)
[2025-01-06 01:29:33,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33,930][root][INFO] - Training Epoch: 6/10, step 35/574 completed (loss: 0.21726830303668976, acc: 0.9180327653884888)
[2025-01-06 01:29:34,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:34,297][root][INFO] - Training Epoch: 6/10, step 36/574 completed (loss: 0.2172539085149765, acc: 0.9523809552192688)
[2025-01-06 01:29:34,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:34,668][root][INFO] - Training Epoch: 6/10, step 37/574 completed (loss: 0.09994519501924515, acc: 0.9661017060279846)
[2025-01-06 01:29:34,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:35,017][root][INFO] - Training Epoch: 6/10, step 38/574 completed (loss: 0.14160001277923584, acc: 0.9655172228813171)
[2025-01-06 01:29:35,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:35,342][root][INFO] - Training Epoch: 6/10, step 39/574 completed (loss: 0.05276970565319061, acc: 1.0)
[2025-01-06 01:29:35,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:35,683][root][INFO] - Training Epoch: 6/10, step 40/574 completed (loss: 0.1278104931116104, acc: 0.9615384340286255)
[2025-01-06 01:29:35,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:36,065][root][INFO] - Training Epoch: 6/10, step 41/574 completed (loss: 0.19628725945949554, acc: 0.9594594836235046)
[2025-01-06 01:29:36,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:36,422][root][INFO] - Training Epoch: 6/10, step 42/574 completed (loss: 0.2649209201335907, acc: 0.9230769276618958)
[2025-01-06 01:29:36,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:36,845][root][INFO] - Training Epoch: 6/10, step 43/574 completed (loss: 0.24633866548538208, acc: 0.9191918969154358)
[2025-01-06 01:29:36,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:37,301][root][INFO] - Training Epoch: 6/10, step 44/574 completed (loss: 0.15117236971855164, acc: 0.938144326210022)
[2025-01-06 01:29:37,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:37,705][root][INFO] - Training Epoch: 6/10, step 45/574 completed (loss: 0.17515113949775696, acc: 0.9411764740943909)
[2025-01-06 01:29:37,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:38,012][root][INFO] - Training Epoch: 6/10, step 46/574 completed (loss: 0.015453724190592766, acc: 1.0)
[2025-01-06 01:29:38,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:38,300][root][INFO] - Training Epoch: 6/10, step 47/574 completed (loss: 0.004372906405478716, acc: 1.0)
[2025-01-06 01:29:38,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:38,654][root][INFO] - Training Epoch: 6/10, step 48/574 completed (loss: 0.056901317089796066, acc: 1.0)
[2025-01-06 01:29:38,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:38,988][root][INFO] - Training Epoch: 6/10, step 49/574 completed (loss: 0.005166663788259029, acc: 1.0)
[2025-01-06 01:29:39,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:39,294][root][INFO] - Training Epoch: 6/10, step 50/574 completed (loss: 0.2564646601676941, acc: 0.9122806787490845)
[2025-01-06 01:29:39,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:39,619][root][INFO] - Training Epoch: 6/10, step 51/574 completed (loss: 0.12657281756401062, acc: 0.9682539701461792)
[2025-01-06 01:29:39,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:39,977][root][INFO] - Training Epoch: 6/10, step 52/574 completed (loss: 0.1712491363286972, acc: 0.9436619877815247)
[2025-01-06 01:29:40,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:40,431][root][INFO] - Training Epoch: 6/10, step 53/574 completed (loss: 0.8467043042182922, acc: 0.7599999904632568)
[2025-01-06 01:29:40,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:40,748][root][INFO] - Training Epoch: 6/10, step 54/574 completed (loss: 0.12734416127204895, acc: 0.9729729890823364)
[2025-01-06 01:29:40,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:41,081][root][INFO] - Training Epoch: 6/10, step 55/574 completed (loss: 0.031512729823589325, acc: 1.0)
[2025-01-06 01:29:42,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:44,187][root][INFO] - Training Epoch: 6/10, step 56/574 completed (loss: 0.7419049739837646, acc: 0.7747440338134766)
[2025-01-06 01:29:44,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:45,355][root][INFO] - Training Epoch: 6/10, step 57/574 completed (loss: 0.9413402676582336, acc: 0.7298474907875061)
[2025-01-06 01:29:45,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:45,973][root][INFO] - Training Epoch: 6/10, step 58/574 completed (loss: 0.5607545375823975, acc: 0.8181818127632141)
[2025-01-06 01:29:46,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:46,540][root][INFO] - Training Epoch: 6/10, step 59/574 completed (loss: 0.19300802052021027, acc: 0.9191176295280457)
[2025-01-06 01:29:46,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:47,108][root][INFO] - Training Epoch: 6/10, step 60/574 completed (loss: 0.5093427896499634, acc: 0.8333333134651184)
[2025-01-06 01:29:47,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:47,520][root][INFO] - Training Epoch: 6/10, step 61/574 completed (loss: 0.29592952132225037, acc: 0.8999999761581421)
[2025-01-06 01:29:47,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:47,856][root][INFO] - Training Epoch: 6/10, step 62/574 completed (loss: 0.07938626408576965, acc: 0.970588207244873)
[2025-01-06 01:29:47,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:48,188][root][INFO] - Training Epoch: 6/10, step 63/574 completed (loss: 0.12015276402235031, acc: 0.9444444179534912)
[2025-01-06 01:29:48,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:48,514][root][INFO] - Training Epoch: 6/10, step 64/574 completed (loss: 0.05090772733092308, acc: 0.984375)
[2025-01-06 01:29:48,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:48,819][root][INFO] - Training Epoch: 6/10, step 65/574 completed (loss: 0.012652497738599777, acc: 1.0)
[2025-01-06 01:29:48,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:49,139][root][INFO] - Training Epoch: 6/10, step 66/574 completed (loss: 0.25892534852027893, acc: 0.9285714030265808)
[2025-01-06 01:29:49,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:49,464][root][INFO] - Training Epoch: 6/10, step 67/574 completed (loss: 0.15808187425136566, acc: 0.9666666388511658)
[2025-01-06 01:29:49,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:49,777][root][INFO] - Training Epoch: 6/10, step 68/574 completed (loss: 0.001129309879615903, acc: 1.0)
[2025-01-06 01:29:49,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:50,068][root][INFO] - Training Epoch: 6/10, step 69/574 completed (loss: 0.050104543566703796, acc: 0.9722222089767456)
[2025-01-06 01:29:50,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:50,367][root][INFO] - Training Epoch: 6/10, step 70/574 completed (loss: 0.22325457632541656, acc: 0.939393937587738)
[2025-01-06 01:29:50,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:50,692][root][INFO] - Training Epoch: 6/10, step 71/574 completed (loss: 0.4879559278488159, acc: 0.8676470518112183)
[2025-01-06 01:29:50,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:51,013][root][INFO] - Training Epoch: 6/10, step 72/574 completed (loss: 0.4293658435344696, acc: 0.8650793433189392)
[2025-01-06 01:29:51,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:51,329][root][INFO] - Training Epoch: 6/10, step 73/574 completed (loss: 0.8886668682098389, acc: 0.7538461685180664)
[2025-01-06 01:29:51,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:51,630][root][INFO] - Training Epoch: 6/10, step 74/574 completed (loss: 0.4135187864303589, acc: 0.9285714030265808)
[2025-01-06 01:29:51,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:51,964][root][INFO] - Training Epoch: 6/10, step 75/574 completed (loss: 0.6652464270591736, acc: 0.7910447716712952)
[2025-01-06 01:29:52,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:52,374][root][INFO] - Training Epoch: 6/10, step 76/574 completed (loss: 1.124248743057251, acc: 0.6788321137428284)
[2025-01-06 01:29:52,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:52,712][root][INFO] - Training Epoch: 6/10, step 77/574 completed (loss: 0.03660374879837036, acc: 1.0)
[2025-01-06 01:29:52,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:53,043][root][INFO] - Training Epoch: 6/10, step 78/574 completed (loss: 0.06017543748021126, acc: 0.9583333134651184)
[2025-01-06 01:29:53,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:53,374][root][INFO] - Training Epoch: 6/10, step 79/574 completed (loss: 0.016361601650714874, acc: 1.0)
[2025-01-06 01:29:53,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:53,701][root][INFO] - Training Epoch: 6/10, step 80/574 completed (loss: 0.13275542855262756, acc: 0.9615384340286255)
[2025-01-06 01:29:53,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:54,042][root][INFO] - Training Epoch: 6/10, step 81/574 completed (loss: 0.1366400420665741, acc: 0.9615384340286255)
[2025-01-06 01:29:54,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:54,382][root][INFO] - Training Epoch: 6/10, step 82/574 completed (loss: 0.09327682852745056, acc: 0.9807692170143127)
[2025-01-06 01:29:54,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:54,740][root][INFO] - Training Epoch: 6/10, step 83/574 completed (loss: 0.08058641850948334, acc: 0.96875)
[2025-01-06 01:29:54,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:55,099][root][INFO] - Training Epoch: 6/10, step 84/574 completed (loss: 0.08864345401525497, acc: 1.0)
[2025-01-06 01:29:55,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:55,449][root][INFO] - Training Epoch: 6/10, step 85/574 completed (loss: 0.13095945119857788, acc: 0.9800000190734863)
[2025-01-06 01:29:55,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:55,781][root][INFO] - Training Epoch: 6/10, step 86/574 completed (loss: 0.07905779033899307, acc: 0.95652174949646)
[2025-01-06 01:29:55,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:56,272][root][INFO] - Training Epoch: 6/10, step 87/574 completed (loss: 0.29912447929382324, acc: 0.8999999761581421)
[2025-01-06 01:29:56,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:56,627][root][INFO] - Training Epoch: 6/10, step 88/574 completed (loss: 0.4930056929588318, acc: 0.8737863898277283)
[2025-01-06 01:29:56,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:57,694][root][INFO] - Training Epoch: 6/10, step 89/574 completed (loss: 0.6258963346481323, acc: 0.8349514603614807)
[2025-01-06 01:29:57,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:58,514][root][INFO] - Training Epoch: 6/10, step 90/574 completed (loss: 0.7072142958641052, acc: 0.7903226017951965)
[2025-01-06 01:29:58,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:59,314][root][INFO] - Training Epoch: 6/10, step 91/574 completed (loss: 0.5805702209472656, acc: 0.8318965435028076)
[2025-01-06 01:29:59,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:00,056][root][INFO] - Training Epoch: 6/10, step 92/574 completed (loss: 0.3667292594909668, acc: 0.8947368264198303)
[2025-01-06 01:30:00,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01,042][root][INFO] - Training Epoch: 6/10, step 93/574 completed (loss: 0.6730954051017761, acc: 0.7821782231330872)
[2025-01-06 01:30:01,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01,345][root][INFO] - Training Epoch: 6/10, step 94/574 completed (loss: 0.3822387158870697, acc: 0.9032257795333862)
[2025-01-06 01:30:01,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01,681][root][INFO] - Training Epoch: 6/10, step 95/574 completed (loss: 0.1643986701965332, acc: 0.9710144996643066)
[2025-01-06 01:30:01,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01,988][root][INFO] - Training Epoch: 6/10, step 96/574 completed (loss: 0.45165979862213135, acc: 0.848739504814148)
[2025-01-06 01:30:02,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:02,310][root][INFO] - Training Epoch: 6/10, step 97/574 completed (loss: 0.41604140400886536, acc: 0.8653846383094788)
[2025-01-06 01:30:02,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:02,683][root][INFO] - Training Epoch: 6/10, step 98/574 completed (loss: 0.561677873134613, acc: 0.8175182342529297)
[2025-01-06 01:30:02,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:02,989][root][INFO] - Training Epoch: 6/10, step 99/574 completed (loss: 0.45078936219215393, acc: 0.8805969953536987)
[2025-01-06 01:30:03,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:03,310][root][INFO] - Training Epoch: 6/10, step 100/574 completed (loss: 0.09345052391290665, acc: 0.949999988079071)
[2025-01-06 01:30:03,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:03,614][root][INFO] - Training Epoch: 6/10, step 101/574 completed (loss: 0.005691051483154297, acc: 1.0)
[2025-01-06 01:30:03,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:03,912][root][INFO] - Training Epoch: 6/10, step 102/574 completed (loss: 0.025147324427962303, acc: 1.0)
[2025-01-06 01:30:03,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:04,226][root][INFO] - Training Epoch: 6/10, step 103/574 completed (loss: 0.023996680974960327, acc: 0.9772727489471436)
[2025-01-06 01:30:04,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:04,534][root][INFO] - Training Epoch: 6/10, step 104/574 completed (loss: 0.10244644433259964, acc: 0.9655172228813171)
[2025-01-06 01:30:04,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:04,872][root][INFO] - Training Epoch: 6/10, step 105/574 completed (loss: 0.013453254476189613, acc: 1.0)
[2025-01-06 01:30:04,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:05,202][root][INFO] - Training Epoch: 6/10, step 106/574 completed (loss: 0.07364095747470856, acc: 0.9599999785423279)
[2025-01-06 01:30:05,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:05,553][root][INFO] - Training Epoch: 6/10, step 107/574 completed (loss: 0.001755430013872683, acc: 1.0)
[2025-01-06 01:30:05,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:05,857][root][INFO] - Training Epoch: 6/10, step 108/574 completed (loss: 0.008136148564517498, acc: 1.0)
[2025-01-06 01:30:05,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:06,193][root][INFO] - Training Epoch: 6/10, step 109/574 completed (loss: 0.2572301924228668, acc: 0.976190447807312)
[2025-01-06 01:30:06,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:06,541][root][INFO] - Training Epoch: 6/10, step 110/574 completed (loss: 0.032333455979824066, acc: 1.0)
[2025-01-06 01:30:06,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:06,944][root][INFO] - Training Epoch: 6/10, step 111/574 completed (loss: 0.06297712028026581, acc: 0.9824561476707458)
[2025-01-06 01:30:07,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:07,317][root][INFO] - Training Epoch: 6/10, step 112/574 completed (loss: 0.17518103122711182, acc: 0.9122806787490845)
[2025-01-06 01:30:07,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:07,681][root][INFO] - Training Epoch: 6/10, step 113/574 completed (loss: 0.09128843247890472, acc: 0.9743589758872986)
[2025-01-06 01:30:07,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:08,054][root][INFO] - Training Epoch: 6/10, step 114/574 completed (loss: 0.04069137945771217, acc: 1.0)
[2025-01-06 01:30:08,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:08,360][root][INFO] - Training Epoch: 6/10, step 115/574 completed (loss: 0.006253094878047705, acc: 1.0)
[2025-01-06 01:30:08,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:08,717][root][INFO] - Training Epoch: 6/10, step 116/574 completed (loss: 0.11840491741895676, acc: 0.9523809552192688)
[2025-01-06 01:30:08,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:09,057][root][INFO] - Training Epoch: 6/10, step 117/574 completed (loss: 0.26047417521476746, acc: 0.9105691313743591)
[2025-01-06 01:30:09,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:09,391][root][INFO] - Training Epoch: 6/10, step 118/574 completed (loss: 0.09132316708564758, acc: 0.9677419066429138)
[2025-01-06 01:30:09,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:10,236][root][INFO] - Training Epoch: 6/10, step 119/574 completed (loss: 0.3970623314380646, acc: 0.9049429893493652)
[2025-01-06 01:30:10,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:10,578][root][INFO] - Training Epoch: 6/10, step 120/574 completed (loss: 0.10154648870229721, acc: 0.9466666579246521)
[2025-01-06 01:30:10,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:10,986][root][INFO] - Training Epoch: 6/10, step 121/574 completed (loss: 0.13179215788841248, acc: 0.9615384340286255)
[2025-01-06 01:30:11,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:11,332][root][INFO] - Training Epoch: 6/10, step 122/574 completed (loss: 0.054683852940797806, acc: 0.9583333134651184)
[2025-01-06 01:30:11,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:11,639][root][INFO] - Training Epoch: 6/10, step 123/574 completed (loss: 0.05686258152127266, acc: 0.9473684430122375)
[2025-01-06 01:30:11,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:11,962][root][INFO] - Training Epoch: 6/10, step 124/574 completed (loss: 0.44418784976005554, acc: 0.8650306463241577)
[2025-01-06 01:30:12,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:12,303][root][INFO] - Training Epoch: 6/10, step 125/574 completed (loss: 0.4036235213279724, acc: 0.9027777910232544)
[2025-01-06 01:30:12,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:12,624][root][INFO] - Training Epoch: 6/10, step 126/574 completed (loss: 0.46258053183555603, acc: 0.8833333253860474)
[2025-01-06 01:30:12,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:12,950][root][INFO] - Training Epoch: 6/10, step 127/574 completed (loss: 0.383063942193985, acc: 0.875)
[2025-01-06 01:30:13,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:13,253][root][INFO] - Training Epoch: 6/10, step 128/574 completed (loss: 0.4283006191253662, acc: 0.8717948794364929)
[2025-01-06 01:30:13,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:13,655][root][INFO] - Training Epoch: 6/10, step 129/574 completed (loss: 0.6536513566970825, acc: 0.8088235259056091)
[2025-01-06 01:30:13,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:13,971][root][INFO] - Training Epoch: 6/10, step 130/574 completed (loss: 0.0475090816617012, acc: 0.9615384340286255)
[2025-01-06 01:30:14,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:14,275][root][INFO] - Training Epoch: 6/10, step 131/574 completed (loss: 0.23761996626853943, acc: 0.9130434989929199)
[2025-01-06 01:30:14,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:14,615][root][INFO] - Training Epoch: 6/10, step 132/574 completed (loss: 0.038512591272592545, acc: 0.96875)
[2025-01-06 01:30:15,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:15,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:15,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:16,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:16,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:16,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:17,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:17,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:17,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:18,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:18,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:18,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:19,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:19,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:19,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:20,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:20,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:20,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:21,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:21,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:21,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:22,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:22,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:22,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:23,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:23,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:23,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:24,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:24,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:24,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:25,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:25,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:25,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:26,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:26,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:26,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:26,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:29,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:29,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:29,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:31,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:31,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:31,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:32,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:32,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:32,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:33,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:33,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:33,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:34,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:34,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:35,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:35,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:35,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:35,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:36,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:36,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:36,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:37,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:37,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:37,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:38,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:38,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:38,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:40,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:40,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:40,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:41,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:41,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:42,192][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1363, device='cuda:0') eval_epoch_loss=tensor(0.7591, device='cuda:0') eval_epoch_acc=tensor(0.8336, device='cuda:0')
[2025-01-06 01:30:42,193][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:30:42,193][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:30:42,445][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_133_loss_0.7590958476066589/model.pt
[2025-01-06 01:30:42,454][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:30:42,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:42,816][root][INFO] - Training Epoch: 6/10, step 133/574 completed (loss: 0.0779557079076767, acc: 0.95652174949646)
[2025-01-06 01:30:42,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:43,164][root][INFO] - Training Epoch: 6/10, step 134/574 completed (loss: 0.060113292187452316, acc: 1.0)
[2025-01-06 01:30:43,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:43,524][root][INFO] - Training Epoch: 6/10, step 135/574 completed (loss: 0.019597504287958145, acc: 1.0)
[2025-01-06 01:30:43,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:43,862][root][INFO] - Training Epoch: 6/10, step 136/574 completed (loss: 0.056328579783439636, acc: 0.976190447807312)
[2025-01-06 01:30:43,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:44,189][root][INFO] - Training Epoch: 6/10, step 137/574 completed (loss: 0.15717503428459167, acc: 0.9666666388511658)
[2025-01-06 01:30:44,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:44,493][root][INFO] - Training Epoch: 6/10, step 138/574 completed (loss: 0.014266898855566978, acc: 1.0)
[2025-01-06 01:30:44,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:44,802][root][INFO] - Training Epoch: 6/10, step 139/574 completed (loss: 0.006443979684263468, acc: 1.0)
[2025-01-06 01:30:44,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45,102][root][INFO] - Training Epoch: 6/10, step 140/574 completed (loss: 0.14067460596561432, acc: 0.9230769276618958)
[2025-01-06 01:30:45,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45,395][root][INFO] - Training Epoch: 6/10, step 141/574 completed (loss: 0.028109822422266006, acc: 1.0)
[2025-01-06 01:30:45,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45,688][root][INFO] - Training Epoch: 6/10, step 142/574 completed (loss: 0.26568999886512756, acc: 0.9459459185600281)
[2025-01-06 01:30:45,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:46,270][root][INFO] - Training Epoch: 6/10, step 143/574 completed (loss: 0.24433113634586334, acc: 0.9035087823867798)
[2025-01-06 01:30:46,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:46,627][root][INFO] - Training Epoch: 6/10, step 144/574 completed (loss: 0.38461771607398987, acc: 0.8731343150138855)
[2025-01-06 01:30:46,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:46,995][root][INFO] - Training Epoch: 6/10, step 145/574 completed (loss: 0.20780403912067413, acc: 0.9489796161651611)
[2025-01-06 01:30:47,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:47,456][root][INFO] - Training Epoch: 6/10, step 146/574 completed (loss: 0.5666846632957458, acc: 0.8191489577293396)
[2025-01-06 01:30:47,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:47,786][root][INFO] - Training Epoch: 6/10, step 147/574 completed (loss: 0.152830109000206, acc: 0.9714285731315613)
[2025-01-06 01:30:47,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:48,121][root][INFO] - Training Epoch: 6/10, step 148/574 completed (loss: 0.16028234362602234, acc: 0.9285714030265808)
[2025-01-06 01:30:48,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:48,478][root][INFO] - Training Epoch: 6/10, step 149/574 completed (loss: 0.43213507533073425, acc: 0.9130434989929199)
[2025-01-06 01:30:48,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:48,828][root][INFO] - Training Epoch: 6/10, step 150/574 completed (loss: 0.032732971012592316, acc: 1.0)
[2025-01-06 01:30:48,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:49,200][root][INFO] - Training Epoch: 6/10, step 151/574 completed (loss: 0.20479299128055573, acc: 0.9347826242446899)
[2025-01-06 01:30:49,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:49,549][root][INFO] - Training Epoch: 6/10, step 152/574 completed (loss: 0.27725839614868164, acc: 0.8983050584793091)
[2025-01-06 01:30:49,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:49,872][root][INFO] - Training Epoch: 6/10, step 153/574 completed (loss: 0.09957005828619003, acc: 0.9824561476707458)
[2025-01-06 01:30:49,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:50,210][root][INFO] - Training Epoch: 6/10, step 154/574 completed (loss: 0.387841135263443, acc: 0.8918918967247009)
[2025-01-06 01:30:50,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:50,531][root][INFO] - Training Epoch: 6/10, step 155/574 completed (loss: 0.041424866765737534, acc: 1.0)
[2025-01-06 01:30:50,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:50,854][root][INFO] - Training Epoch: 6/10, step 156/574 completed (loss: 0.047443147748708725, acc: 0.95652174949646)
[2025-01-06 01:30:50,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:51,228][root][INFO] - Training Epoch: 6/10, step 157/574 completed (loss: 0.4077311158180237, acc: 0.8947368264198303)
[2025-01-06 01:30:51,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:52,868][root][INFO] - Training Epoch: 6/10, step 158/574 completed (loss: 0.3826907277107239, acc: 0.8918918967247009)
[2025-01-06 01:30:52,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:53,207][root][INFO] - Training Epoch: 6/10, step 159/574 completed (loss: 0.515574038028717, acc: 0.8333333134651184)
[2025-01-06 01:30:53,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:53,640][root][INFO] - Training Epoch: 6/10, step 160/574 completed (loss: 0.442440003156662, acc: 0.8255813717842102)
[2025-01-06 01:30:53,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:54,252][root][INFO] - Training Epoch: 6/10, step 161/574 completed (loss: 0.6461131572723389, acc: 0.800000011920929)
[2025-01-06 01:30:54,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:54,841][root][INFO] - Training Epoch: 6/10, step 162/574 completed (loss: 0.5723920464515686, acc: 0.8426966071128845)
[2025-01-06 01:30:54,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:55,190][root][INFO] - Training Epoch: 6/10, step 163/574 completed (loss: 0.09172721207141876, acc: 0.9545454382896423)
[2025-01-06 01:30:55,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:55,475][root][INFO] - Training Epoch: 6/10, step 164/574 completed (loss: 0.036143794655799866, acc: 1.0)
[2025-01-06 01:30:55,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:55,778][root][INFO] - Training Epoch: 6/10, step 165/574 completed (loss: 0.15612515807151794, acc: 0.9655172228813171)
[2025-01-06 01:30:55,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:56,059][root][INFO] - Training Epoch: 6/10, step 166/574 completed (loss: 0.025370541960000992, acc: 1.0)
[2025-01-06 01:30:56,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:56,356][root][INFO] - Training Epoch: 6/10, step 167/574 completed (loss: 0.06586486101150513, acc: 0.9800000190734863)
[2025-01-06 01:30:56,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:56,750][root][INFO] - Training Epoch: 6/10, step 168/574 completed (loss: 0.175674706697464, acc: 0.9305555820465088)
[2025-01-06 01:30:56,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:57,059][root][INFO] - Training Epoch: 6/10, step 169/574 completed (loss: 0.5388196110725403, acc: 0.8039215803146362)
[2025-01-06 01:30:57,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:58,092][root][INFO] - Training Epoch: 6/10, step 170/574 completed (loss: 0.3688417077064514, acc: 0.8972602486610413)
[2025-01-06 01:30:58,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:58,389][root][INFO] - Training Epoch: 6/10, step 171/574 completed (loss: 0.1980840563774109, acc: 0.9166666865348816)
[2025-01-06 01:30:58,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:58,680][root][INFO] - Training Epoch: 6/10, step 172/574 completed (loss: 0.1026303842663765, acc: 0.9629629850387573)
[2025-01-06 01:30:58,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:59,059][root][INFO] - Training Epoch: 6/10, step 173/574 completed (loss: 0.14514590799808502, acc: 0.9642857313156128)
[2025-01-06 01:30:59,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:59,613][root][INFO] - Training Epoch: 6/10, step 174/574 completed (loss: 0.5314766764640808, acc: 0.8407079577445984)
[2025-01-06 01:30:59,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:59,910][root][INFO] - Training Epoch: 6/10, step 175/574 completed (loss: 0.19825021922588348, acc: 0.9710144996643066)
[2025-01-06 01:31:00,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:00,247][root][INFO] - Training Epoch: 6/10, step 176/574 completed (loss: 0.19403886795043945, acc: 0.9318181872367859)
[2025-01-06 01:31:00,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:01,158][root][INFO] - Training Epoch: 6/10, step 177/574 completed (loss: 0.5937432050704956, acc: 0.8091602921485901)
[2025-01-06 01:31:01,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:01,829][root][INFO] - Training Epoch: 6/10, step 178/574 completed (loss: 0.5934609770774841, acc: 0.8370370268821716)
[2025-01-06 01:31:01,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:02,198][root][INFO] - Training Epoch: 6/10, step 179/574 completed (loss: 0.1251375377178192, acc: 0.9672130942344666)
[2025-01-06 01:31:02,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:02,526][root][INFO] - Training Epoch: 6/10, step 180/574 completed (loss: 0.013229459524154663, acc: 1.0)
[2025-01-06 01:31:02,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:02,814][root][INFO] - Training Epoch: 6/10, step 181/574 completed (loss: 0.01606728695333004, acc: 1.0)
[2025-01-06 01:31:02,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:03,107][root][INFO] - Training Epoch: 6/10, step 182/574 completed (loss: 0.04009028896689415, acc: 1.0)
[2025-01-06 01:31:03,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:03,449][root][INFO] - Training Epoch: 6/10, step 183/574 completed (loss: 0.08607155829668045, acc: 0.9634146094322205)
[2025-01-06 01:31:03,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:03,781][root][INFO] - Training Epoch: 6/10, step 184/574 completed (loss: 0.29930615425109863, acc: 0.9154078364372253)
[2025-01-06 01:31:03,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:04,102][root][INFO] - Training Epoch: 6/10, step 185/574 completed (loss: 0.2777082324028015, acc: 0.9221901893615723)
[2025-01-06 01:31:04,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:04,599][root][INFO] - Training Epoch: 6/10, step 186/574 completed (loss: 0.27381616830825806, acc: 0.9156249761581421)
[2025-01-06 01:31:04,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:05,130][root][INFO] - Training Epoch: 6/10, step 187/574 completed (loss: 0.38136065006256104, acc: 0.8968105316162109)
[2025-01-06 01:31:05,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:05,529][root][INFO] - Training Epoch: 6/10, step 188/574 completed (loss: 0.3241918981075287, acc: 0.9145907759666443)
[2025-01-06 01:31:05,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:05,826][root][INFO] - Training Epoch: 6/10, step 189/574 completed (loss: 0.511033833026886, acc: 0.9599999785423279)
[2025-01-06 01:31:05,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:06,370][root][INFO] - Training Epoch: 6/10, step 190/574 completed (loss: 0.46623730659484863, acc: 0.8372092843055725)
[2025-01-06 01:31:06,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:07,168][root][INFO] - Training Epoch: 6/10, step 191/574 completed (loss: 0.804137110710144, acc: 0.7698412537574768)
[2025-01-06 01:31:07,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:08,083][root][INFO] - Training Epoch: 6/10, step 192/574 completed (loss: 0.6556093096733093, acc: 0.8181818127632141)
[2025-01-06 01:31:08,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:08,821][root][INFO] - Training Epoch: 6/10, step 193/574 completed (loss: 0.28295770287513733, acc: 0.9058823585510254)
[2025-01-06 01:31:09,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:09,892][root][INFO] - Training Epoch: 6/10, step 194/574 completed (loss: 0.6869308352470398, acc: 0.7839506268501282)
[2025-01-06 01:31:10,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:10,841][root][INFO] - Training Epoch: 6/10, step 195/574 completed (loss: 0.18959660828113556, acc: 0.9354838728904724)
[2025-01-06 01:31:10,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:11,116][root][INFO] - Training Epoch: 6/10, step 196/574 completed (loss: 0.035688288509845734, acc: 1.0)
[2025-01-06 01:31:11,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:11,459][root][INFO] - Training Epoch: 6/10, step 197/574 completed (loss: 0.2207464724779129, acc: 0.9750000238418579)
[2025-01-06 01:31:11,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:11,791][root][INFO] - Training Epoch: 6/10, step 198/574 completed (loss: 0.230453222990036, acc: 0.9411764740943909)
[2025-01-06 01:31:11,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:12,129][root][INFO] - Training Epoch: 6/10, step 199/574 completed (loss: 0.49096012115478516, acc: 0.875)
[2025-01-06 01:31:12,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:12,457][root][INFO] - Training Epoch: 6/10, step 200/574 completed (loss: 0.3457854092121124, acc: 0.9067796468734741)
[2025-01-06 01:31:12,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:12,799][root][INFO] - Training Epoch: 6/10, step 201/574 completed (loss: 0.47652608156204224, acc: 0.8283582329750061)
[2025-01-06 01:31:12,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:13,164][root][INFO] - Training Epoch: 6/10, step 202/574 completed (loss: 0.28190022706985474, acc: 0.9223300814628601)
[2025-01-06 01:31:13,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:13,461][root][INFO] - Training Epoch: 6/10, step 203/574 completed (loss: 0.16510623693466187, acc: 0.9523809552192688)
[2025-01-06 01:31:13,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:13,944][root][INFO] - Training Epoch: 6/10, step 204/574 completed (loss: 0.02987517975270748, acc: 0.9890109896659851)
[2025-01-06 01:31:14,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:14,284][root][INFO] - Training Epoch: 6/10, step 205/574 completed (loss: 0.15038608014583588, acc: 0.9506726264953613)
[2025-01-06 01:31:14,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:14,674][root][INFO] - Training Epoch: 6/10, step 206/574 completed (loss: 0.28491467237472534, acc: 0.9251968264579773)
[2025-01-06 01:31:14,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:15,012][root][INFO] - Training Epoch: 6/10, step 207/574 completed (loss: 0.1312829703092575, acc: 0.9612069129943848)
[2025-01-06 01:31:15,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:15,340][root][INFO] - Training Epoch: 6/10, step 208/574 completed (loss: 0.2637350559234619, acc: 0.9311594367027283)
[2025-01-06 01:31:15,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:15,701][root][INFO] - Training Epoch: 6/10, step 209/574 completed (loss: 0.21438539028167725, acc: 0.929961085319519)
[2025-01-06 01:31:15,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:16,048][root][INFO] - Training Epoch: 6/10, step 210/574 completed (loss: 0.042562711983919144, acc: 0.989130437374115)
[2025-01-06 01:31:16,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:16,383][root][INFO] - Training Epoch: 6/10, step 211/574 completed (loss: 0.0028831923846155405, acc: 1.0)
[2025-01-06 01:31:16,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:16,700][root][INFO] - Training Epoch: 6/10, step 212/574 completed (loss: 0.015155358240008354, acc: 1.0)
[2025-01-06 01:31:16,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:17,006][root][INFO] - Training Epoch: 6/10, step 213/574 completed (loss: 0.21261996030807495, acc: 0.957446813583374)
[2025-01-06 01:31:17,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:17,682][root][INFO] - Training Epoch: 6/10, step 214/574 completed (loss: 0.11410751193761826, acc: 0.9615384340286255)
[2025-01-06 01:31:17,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:18,022][root][INFO] - Training Epoch: 6/10, step 215/574 completed (loss: 0.06992745399475098, acc: 0.9864864945411682)
[2025-01-06 01:31:18,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:18,356][root][INFO] - Training Epoch: 6/10, step 216/574 completed (loss: 0.023017643019557, acc: 0.9883720874786377)
[2025-01-06 01:31:18,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:18,891][root][INFO] - Training Epoch: 6/10, step 217/574 completed (loss: 0.10327387601137161, acc: 0.9639639854431152)
[2025-01-06 01:31:18,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:19,306][root][INFO] - Training Epoch: 6/10, step 218/574 completed (loss: 0.05072517320513725, acc: 0.9777777791023254)
[2025-01-06 01:31:19,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:19,621][root][INFO] - Training Epoch: 6/10, step 219/574 completed (loss: 0.04657178744673729, acc: 0.9696969985961914)
[2025-01-06 01:31:19,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:19,949][root][INFO] - Training Epoch: 6/10, step 220/574 completed (loss: 0.003592920256778598, acc: 1.0)
[2025-01-06 01:31:20,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:20,303][root][INFO] - Training Epoch: 6/10, step 221/574 completed (loss: 0.005740860011428595, acc: 1.0)
[2025-01-06 01:31:20,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:20,639][root][INFO] - Training Epoch: 6/10, step 222/574 completed (loss: 0.16849538683891296, acc: 0.9230769276618958)
[2025-01-06 01:31:20,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:21,396][root][INFO] - Training Epoch: 6/10, step 223/574 completed (loss: 0.21051475405693054, acc: 0.9239130616188049)
[2025-01-06 01:31:21,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:21,941][root][INFO] - Training Epoch: 6/10, step 224/574 completed (loss: 0.32223525643348694, acc: 0.8806818127632141)
[2025-01-06 01:31:22,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:22,376][root][INFO] - Training Epoch: 6/10, step 225/574 completed (loss: 0.38848021626472473, acc: 0.8829787373542786)
[2025-01-06 01:31:22,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:22,745][root][INFO] - Training Epoch: 6/10, step 226/574 completed (loss: 0.11646464467048645, acc: 0.9622641801834106)
[2025-01-06 01:31:22,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:23,095][root][INFO] - Training Epoch: 6/10, step 227/574 completed (loss: 0.027816182002425194, acc: 1.0)
[2025-01-06 01:31:23,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:23,387][root][INFO] - Training Epoch: 6/10, step 228/574 completed (loss: 0.1285197138786316, acc: 0.9534883499145508)
[2025-01-06 01:31:23,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:23,711][root][INFO] - Training Epoch: 6/10, step 229/574 completed (loss: 0.15877015888690948, acc: 0.9333333373069763)
[2025-01-06 01:31:23,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:24,077][root][INFO] - Training Epoch: 6/10, step 230/574 completed (loss: 0.8282449245452881, acc: 0.7473683953285217)
[2025-01-06 01:31:24,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:24,385][root][INFO] - Training Epoch: 6/10, step 231/574 completed (loss: 0.8163486123085022, acc: 0.8222222328186035)
[2025-01-06 01:31:24,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:24,819][root][INFO] - Training Epoch: 6/10, step 232/574 completed (loss: 0.7376903891563416, acc: 0.7722222208976746)
[2025-01-06 01:31:24,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:25,314][root][INFO] - Training Epoch: 6/10, step 233/574 completed (loss: 1.1338272094726562, acc: 0.6880733966827393)
[2025-01-06 01:31:25,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:25,786][root][INFO] - Training Epoch: 6/10, step 234/574 completed (loss: 0.6762408018112183, acc: 0.807692289352417)
[2025-01-06 01:31:25,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:26,082][root][INFO] - Training Epoch: 6/10, step 235/574 completed (loss: 0.0035942625254392624, acc: 1.0)
[2025-01-06 01:31:26,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:26,383][root][INFO] - Training Epoch: 6/10, step 236/574 completed (loss: 0.26397934556007385, acc: 0.9583333134651184)
[2025-01-06 01:31:26,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:26,715][root][INFO] - Training Epoch: 6/10, step 237/574 completed (loss: 0.28278326988220215, acc: 0.9090909361839294)
[2025-01-06 01:31:26,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:27,036][root][INFO] - Training Epoch: 6/10, step 238/574 completed (loss: 0.20942114293575287, acc: 0.9259259104728699)
[2025-01-06 01:31:27,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:27,340][root][INFO] - Training Epoch: 6/10, step 239/574 completed (loss: 0.06354011595249176, acc: 1.0)
[2025-01-06 01:31:27,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:27,671][root][INFO] - Training Epoch: 6/10, step 240/574 completed (loss: 0.2127404361963272, acc: 0.9545454382896423)
[2025-01-06 01:31:27,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:27,970][root][INFO] - Training Epoch: 6/10, step 241/574 completed (loss: 0.2103433758020401, acc: 0.9318181872367859)
[2025-01-06 01:31:28,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:28,578][root][INFO] - Training Epoch: 6/10, step 242/574 completed (loss: 0.45572972297668457, acc: 0.8387096524238586)
[2025-01-06 01:31:28,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:29,107][root][INFO] - Training Epoch: 6/10, step 243/574 completed (loss: 0.2938190698623657, acc: 0.9318181872367859)
[2025-01-06 01:31:29,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:29,410][root][INFO] - Training Epoch: 6/10, step 244/574 completed (loss: 0.006969009060412645, acc: 1.0)
[2025-01-06 01:31:29,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:29,717][root][INFO] - Training Epoch: 6/10, step 245/574 completed (loss: 0.13091318309307098, acc: 0.9230769276618958)
[2025-01-06 01:31:29,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:30,028][root][INFO] - Training Epoch: 6/10, step 246/574 completed (loss: 0.016925601288676262, acc: 1.0)
[2025-01-06 01:31:30,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:30,413][root][INFO] - Training Epoch: 6/10, step 247/574 completed (loss: 0.06637770682573318, acc: 0.949999988079071)
[2025-01-06 01:31:30,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:30,795][root][INFO] - Training Epoch: 6/10, step 248/574 completed (loss: 0.031183596700429916, acc: 1.0)
[2025-01-06 01:31:30,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:31,152][root][INFO] - Training Epoch: 6/10, step 249/574 completed (loss: 0.028674107044935226, acc: 1.0)
[2025-01-06 01:31:31,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:31,497][root][INFO] - Training Epoch: 6/10, step 250/574 completed (loss: 0.06855552643537521, acc: 0.9729729890823364)
[2025-01-06 01:31:31,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:31,858][root][INFO] - Training Epoch: 6/10, step 251/574 completed (loss: 0.22927060723304749, acc: 0.970588207244873)
[2025-01-06 01:31:31,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:32,236][root][INFO] - Training Epoch: 6/10, step 252/574 completed (loss: 0.03812296688556671, acc: 0.9756097793579102)
[2025-01-06 01:31:32,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:32,570][root][INFO] - Training Epoch: 6/10, step 253/574 completed (loss: 0.006790949031710625, acc: 1.0)
[2025-01-06 01:31:32,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:32,889][root][INFO] - Training Epoch: 6/10, step 254/574 completed (loss: 0.0009361742995679379, acc: 1.0)
[2025-01-06 01:31:33,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:33,249][root][INFO] - Training Epoch: 6/10, step 255/574 completed (loss: 0.032426707446575165, acc: 1.0)
[2025-01-06 01:31:33,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:33,583][root][INFO] - Training Epoch: 6/10, step 256/574 completed (loss: 0.009874869138002396, acc: 1.0)
[2025-01-06 01:31:33,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:33,914][root][INFO] - Training Epoch: 6/10, step 257/574 completed (loss: 0.026600833982229233, acc: 1.0)
[2025-01-06 01:31:34,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:34,276][root][INFO] - Training Epoch: 6/10, step 258/574 completed (loss: 0.052565958350896835, acc: 0.9868420958518982)
[2025-01-06 01:31:34,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:34,844][root][INFO] - Training Epoch: 6/10, step 259/574 completed (loss: 0.1877337396144867, acc: 0.9433962106704712)
[2025-01-06 01:31:34,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:35,425][root][INFO] - Training Epoch: 6/10, step 260/574 completed (loss: 0.12243154644966125, acc: 0.9666666388511658)
[2025-01-06 01:31:35,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:35,748][root][INFO] - Training Epoch: 6/10, step 261/574 completed (loss: 0.02732921950519085, acc: 1.0)
[2025-01-06 01:31:35,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:36,111][root][INFO] - Training Epoch: 6/10, step 262/574 completed (loss: 0.05077493190765381, acc: 1.0)
[2025-01-06 01:31:36,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:36,472][root][INFO] - Training Epoch: 6/10, step 263/574 completed (loss: 0.3536781966686249, acc: 0.9066666960716248)
[2025-01-06 01:31:36,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:36,828][root][INFO] - Training Epoch: 6/10, step 264/574 completed (loss: 0.22232739627361298, acc: 0.9375)
[2025-01-06 01:31:37,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:37,680][root][INFO] - Training Epoch: 6/10, step 265/574 completed (loss: 0.7181040048599243, acc: 0.7760000228881836)
[2025-01-06 01:31:37,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:38,011][root][INFO] - Training Epoch: 6/10, step 266/574 completed (loss: 0.5372829437255859, acc: 0.7977527976036072)
[2025-01-06 01:31:38,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:38,386][root][INFO] - Training Epoch: 6/10, step 267/574 completed (loss: 0.23607346415519714, acc: 0.9189189076423645)
[2025-01-06 01:31:38,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:38,837][root][INFO] - Training Epoch: 6/10, step 268/574 completed (loss: 0.19720104336738586, acc: 0.9655172228813171)
[2025-01-06 01:31:38,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:39,153][root][INFO] - Training Epoch: 6/10, step 269/574 completed (loss: 0.007797488942742348, acc: 1.0)
[2025-01-06 01:31:39,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:39,481][root][INFO] - Training Epoch: 6/10, step 270/574 completed (loss: 0.005554614122956991, acc: 1.0)
[2025-01-06 01:31:39,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:39,813][root][INFO] - Training Epoch: 6/10, step 271/574 completed (loss: 0.1283840388059616, acc: 0.96875)
[2025-01-06 01:31:39,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:40,146][root][INFO] - Training Epoch: 6/10, step 272/574 completed (loss: 0.006357570644468069, acc: 1.0)
[2025-01-06 01:31:40,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:40,543][root][INFO] - Training Epoch: 6/10, step 273/574 completed (loss: 0.13909223675727844, acc: 0.9666666388511658)
[2025-01-06 01:31:40,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:40,897][root][INFO] - Training Epoch: 6/10, step 274/574 completed (loss: 0.03245265409350395, acc: 0.96875)
[2025-01-06 01:31:40,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:41,241][root][INFO] - Training Epoch: 6/10, step 275/574 completed (loss: 0.025295283645391464, acc: 1.0)
[2025-01-06 01:31:42,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:42,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:42,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:43,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:43,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:43,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:43,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:44,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:44,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:44,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:45,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:45,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:45,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:46,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:46,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:46,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:47,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:47,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:47,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:47,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:48,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:48,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:48,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:49,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:49,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:49,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:49,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:50,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:50,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:50,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:51,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:51,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:51,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:52,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:52,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:52,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:52,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:54,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:54,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:54,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:55,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:55,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:55,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:55,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:56,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:56,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:56,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:58,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:58,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:58,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:59,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:59,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:59,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:00,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:00,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:00,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:01,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:01,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:01,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:02,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:02,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:05,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:05,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:05,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:06,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:06,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:06,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:07,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:07,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:08,145][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2607, device='cuda:0') eval_epoch_loss=tensor(0.8157, device='cuda:0') eval_epoch_acc=tensor(0.8236, device='cuda:0')
[2025-01-06 01:32:08,147][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:32:08,147][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:32:08,498][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_276_loss_0.8156688809394836/model.pt
[2025-01-06 01:32:08,504][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:32:08,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:08,885][root][INFO] - Training Epoch: 6/10, step 276/574 completed (loss: 0.005763411521911621, acc: 1.0)
[2025-01-06 01:32:08,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:09,255][root][INFO] - Training Epoch: 6/10, step 277/574 completed (loss: 0.015476396307349205, acc: 1.0)
[2025-01-06 01:32:09,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:09,575][root][INFO] - Training Epoch: 6/10, step 278/574 completed (loss: 0.1768302172422409, acc: 0.936170220375061)
[2025-01-06 01:32:09,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:09,935][root][INFO] - Training Epoch: 6/10, step 279/574 completed (loss: 0.22318506240844727, acc: 0.9166666865348816)
[2025-01-06 01:32:10,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:10,276][root][INFO] - Training Epoch: 6/10, step 280/574 completed (loss: 0.01001564972102642, acc: 1.0)
[2025-01-06 01:32:10,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:10,703][root][INFO] - Training Epoch: 6/10, step 281/574 completed (loss: 0.1427396684885025, acc: 0.9759036302566528)
[2025-01-06 01:32:10,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:11,074][root][INFO] - Training Epoch: 6/10, step 282/574 completed (loss: 0.37845122814178467, acc: 0.8981481194496155)
[2025-01-06 01:32:11,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:11,392][root][INFO] - Training Epoch: 6/10, step 283/574 completed (loss: 0.03351602703332901, acc: 0.9736841917037964)
[2025-01-06 01:32:11,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:11,725][root][INFO] - Training Epoch: 6/10, step 284/574 completed (loss: 0.013815764337778091, acc: 1.0)
[2025-01-06 01:32:11,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:12,059][root][INFO] - Training Epoch: 6/10, step 285/574 completed (loss: 0.03370141237974167, acc: 1.0)
[2025-01-06 01:32:12,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:12,398][root][INFO] - Training Epoch: 6/10, step 286/574 completed (loss: 0.21082167327404022, acc: 0.9453125)
[2025-01-06 01:32:12,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:12,735][root][INFO] - Training Epoch: 6/10, step 287/574 completed (loss: 0.31449130177497864, acc: 0.9039999842643738)
[2025-01-06 01:32:12,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:13,071][root][INFO] - Training Epoch: 6/10, step 288/574 completed (loss: 0.1012192890048027, acc: 0.9780219793319702)
[2025-01-06 01:32:13,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:13,408][root][INFO] - Training Epoch: 6/10, step 289/574 completed (loss: 0.13336485624313354, acc: 0.9378882050514221)
[2025-01-06 01:32:13,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:13,767][root][INFO] - Training Epoch: 6/10, step 290/574 completed (loss: 0.35086125135421753, acc: 0.8865979313850403)
[2025-01-06 01:32:13,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:14,107][root][INFO] - Training Epoch: 6/10, step 291/574 completed (loss: 0.003912276588380337, acc: 1.0)
[2025-01-06 01:32:14,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:14,430][root][INFO] - Training Epoch: 6/10, step 292/574 completed (loss: 0.027593862265348434, acc: 1.0)
[2025-01-06 01:32:14,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:14,776][root][INFO] - Training Epoch: 6/10, step 293/574 completed (loss: 0.19372811913490295, acc: 0.9137930870056152)
[2025-01-06 01:32:14,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:15,241][root][INFO] - Training Epoch: 6/10, step 294/574 completed (loss: 0.17389222979545593, acc: 0.9272727370262146)
[2025-01-06 01:32:15,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:15,792][root][INFO] - Training Epoch: 6/10, step 295/574 completed (loss: 0.2477002888917923, acc: 0.9278350472450256)
[2025-01-06 01:32:15,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:16,108][root][INFO] - Training Epoch: 6/10, step 296/574 completed (loss: 0.17616969347000122, acc: 0.9655172228813171)
[2025-01-06 01:32:16,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:16,441][root][INFO] - Training Epoch: 6/10, step 297/574 completed (loss: 0.10144788026809692, acc: 0.9629629850387573)
[2025-01-06 01:32:16,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:16,778][root][INFO] - Training Epoch: 6/10, step 298/574 completed (loss: 0.03784778714179993, acc: 1.0)
[2025-01-06 01:32:16,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:17,093][root][INFO] - Training Epoch: 6/10, step 299/574 completed (loss: 0.005145073402673006, acc: 1.0)
[2025-01-06 01:32:17,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:17,408][root][INFO] - Training Epoch: 6/10, step 300/574 completed (loss: 0.003425614908337593, acc: 1.0)
[2025-01-06 01:32:17,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:17,769][root][INFO] - Training Epoch: 6/10, step 301/574 completed (loss: 0.03610197827219963, acc: 0.9811320900917053)
[2025-01-06 01:32:17,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18,112][root][INFO] - Training Epoch: 6/10, step 302/574 completed (loss: 0.004722012672573328, acc: 1.0)
[2025-01-06 01:32:18,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18,431][root][INFO] - Training Epoch: 6/10, step 303/574 completed (loss: 0.0033644018694758415, acc: 1.0)
[2025-01-06 01:32:18,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18,763][root][INFO] - Training Epoch: 6/10, step 304/574 completed (loss: 0.006991189904510975, acc: 1.0)
[2025-01-06 01:32:18,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:19,140][root][INFO] - Training Epoch: 6/10, step 305/574 completed (loss: 0.0957733541727066, acc: 0.9836065769195557)
[2025-01-06 01:32:19,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:19,495][root][INFO] - Training Epoch: 6/10, step 306/574 completed (loss: 0.04249138385057449, acc: 1.0)
[2025-01-06 01:32:19,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:19,809][root][INFO] - Training Epoch: 6/10, step 307/574 completed (loss: 0.002535092644393444, acc: 1.0)
[2025-01-06 01:32:19,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:20,133][root][INFO] - Training Epoch: 6/10, step 308/574 completed (loss: 0.09179080277681351, acc: 0.95652174949646)
[2025-01-06 01:32:20,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:20,551][root][INFO] - Training Epoch: 6/10, step 309/574 completed (loss: 0.04804740846157074, acc: 0.9861111044883728)
[2025-01-06 01:32:20,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:20,883][root][INFO] - Training Epoch: 6/10, step 310/574 completed (loss: 0.09222641587257385, acc: 0.9638554453849792)
[2025-01-06 01:32:20,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:21,228][root][INFO] - Training Epoch: 6/10, step 311/574 completed (loss: 0.10806500911712646, acc: 0.9358974099159241)
[2025-01-06 01:32:21,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:21,595][root][INFO] - Training Epoch: 6/10, step 312/574 completed (loss: 0.08888798952102661, acc: 0.9693877696990967)
[2025-01-06 01:32:21,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:21,918][root][INFO] - Training Epoch: 6/10, step 313/574 completed (loss: 0.0014229664811864495, acc: 1.0)
[2025-01-06 01:32:22,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:22,254][root][INFO] - Training Epoch: 6/10, step 314/574 completed (loss: 0.004773115273565054, acc: 1.0)
[2025-01-06 01:32:22,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:22,609][root][INFO] - Training Epoch: 6/10, step 315/574 completed (loss: 0.06744540482759476, acc: 0.9677419066429138)
[2025-01-06 01:32:22,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:22,923][root][INFO] - Training Epoch: 6/10, step 316/574 completed (loss: 0.2560786008834839, acc: 0.9354838728904724)
[2025-01-06 01:32:23,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:23,274][root][INFO] - Training Epoch: 6/10, step 317/574 completed (loss: 0.02808419056236744, acc: 0.9850746393203735)
[2025-01-06 01:32:23,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:23,657][root][INFO] - Training Epoch: 6/10, step 318/574 completed (loss: 0.01299215480685234, acc: 1.0)
[2025-01-06 01:32:23,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:24,018][root][INFO] - Training Epoch: 6/10, step 319/574 completed (loss: 0.007974879816174507, acc: 1.0)
[2025-01-06 01:32:24,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:24,357][root][INFO] - Training Epoch: 6/10, step 320/574 completed (loss: 0.012079799547791481, acc: 1.0)
[2025-01-06 01:32:24,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:24,729][root][INFO] - Training Epoch: 6/10, step 321/574 completed (loss: 0.013506009243428707, acc: 1.0)
[2025-01-06 01:32:24,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:25,052][root][INFO] - Training Epoch: 6/10, step 322/574 completed (loss: 0.5633297562599182, acc: 0.8518518805503845)
[2025-01-06 01:32:25,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:25,380][root][INFO] - Training Epoch: 6/10, step 323/574 completed (loss: 0.12609542906284332, acc: 0.9714285731315613)
[2025-01-06 01:32:25,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:25,748][root][INFO] - Training Epoch: 6/10, step 324/574 completed (loss: 0.22955884039402008, acc: 0.9487179517745972)
[2025-01-06 01:32:25,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:26,105][root][INFO] - Training Epoch: 6/10, step 325/574 completed (loss: 0.4156932830810547, acc: 0.8536585569381714)
[2025-01-06 01:32:26,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:26,434][root][INFO] - Training Epoch: 6/10, step 326/574 completed (loss: 0.1344403773546219, acc: 0.9736841917037964)
[2025-01-06 01:32:26,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:26,754][root][INFO] - Training Epoch: 6/10, step 327/574 completed (loss: 0.03269059956073761, acc: 1.0)
[2025-01-06 01:32:26,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:27,091][root][INFO] - Training Epoch: 6/10, step 328/574 completed (loss: 0.04411491006612778, acc: 0.9642857313156128)
[2025-01-06 01:32:27,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:27,396][root][INFO] - Training Epoch: 6/10, step 329/574 completed (loss: 0.1703193634748459, acc: 0.9629629850387573)
[2025-01-06 01:32:27,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:27,747][root][INFO] - Training Epoch: 6/10, step 330/574 completed (loss: 0.003447245806455612, acc: 1.0)
[2025-01-06 01:32:27,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:28,116][root][INFO] - Training Epoch: 6/10, step 331/574 completed (loss: 0.10775730013847351, acc: 0.9838709831237793)
[2025-01-06 01:32:28,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:28,493][root][INFO] - Training Epoch: 6/10, step 332/574 completed (loss: 0.023916255682706833, acc: 1.0)
[2025-01-06 01:32:28,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:28,860][root][INFO] - Training Epoch: 6/10, step 333/574 completed (loss: 0.0050874315202236176, acc: 1.0)
[2025-01-06 01:32:28,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:29,245][root][INFO] - Training Epoch: 6/10, step 334/574 completed (loss: 0.01674572005867958, acc: 1.0)
[2025-01-06 01:32:29,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:29,609][root][INFO] - Training Epoch: 6/10, step 335/574 completed (loss: 0.004517257213592529, acc: 1.0)
[2025-01-06 01:32:29,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:29,967][root][INFO] - Training Epoch: 6/10, step 336/574 completed (loss: 0.09665070474147797, acc: 0.9800000190734863)
[2025-01-06 01:32:30,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:30,328][root][INFO] - Training Epoch: 6/10, step 337/574 completed (loss: 0.5003653168678284, acc: 0.8275862336158752)
[2025-01-06 01:32:30,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:30,712][root][INFO] - Training Epoch: 6/10, step 338/574 completed (loss: 0.4610433876514435, acc: 0.8617021441459656)
[2025-01-06 01:32:30,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:31,052][root][INFO] - Training Epoch: 6/10, step 339/574 completed (loss: 0.4535311460494995, acc: 0.8674699068069458)
[2025-01-06 01:32:31,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:31,391][root][INFO] - Training Epoch: 6/10, step 340/574 completed (loss: 0.0018577385926619172, acc: 1.0)
[2025-01-06 01:32:31,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:31,717][root][INFO] - Training Epoch: 6/10, step 341/574 completed (loss: 0.01157098263502121, acc: 1.0)
[2025-01-06 01:32:31,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:32,052][root][INFO] - Training Epoch: 6/10, step 342/574 completed (loss: 0.09806327521800995, acc: 0.9397590160369873)
[2025-01-06 01:32:32,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:32,387][root][INFO] - Training Epoch: 6/10, step 343/574 completed (loss: 0.14356866478919983, acc: 0.9622641801834106)
[2025-01-06 01:32:32,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:32,720][root][INFO] - Training Epoch: 6/10, step 344/574 completed (loss: 0.041525643318891525, acc: 0.9873417615890503)
[2025-01-06 01:32:32,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:33,081][root][INFO] - Training Epoch: 6/10, step 345/574 completed (loss: 0.06908929347991943, acc: 0.9803921580314636)
[2025-01-06 01:32:33,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:33,425][root][INFO] - Training Epoch: 6/10, step 346/574 completed (loss: 0.011423836462199688, acc: 1.0)
[2025-01-06 01:32:33,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:33,762][root][INFO] - Training Epoch: 6/10, step 347/574 completed (loss: 0.0010790886590257287, acc: 1.0)
[2025-01-06 01:32:33,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:34,099][root][INFO] - Training Epoch: 6/10, step 348/574 completed (loss: 0.010681762360036373, acc: 1.0)
[2025-01-06 01:32:34,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:34,488][root][INFO] - Training Epoch: 6/10, step 349/574 completed (loss: 0.24386848509311676, acc: 0.9444444179534912)
[2025-01-06 01:32:34,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:34,817][root][INFO] - Training Epoch: 6/10, step 350/574 completed (loss: 0.0562131293118, acc: 1.0)
[2025-01-06 01:32:34,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:35,144][root][INFO] - Training Epoch: 6/10, step 351/574 completed (loss: 0.17146849632263184, acc: 0.9743589758872986)
[2025-01-06 01:32:35,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:35,517][root][INFO] - Training Epoch: 6/10, step 352/574 completed (loss: 0.10091692954301834, acc: 0.9777777791023254)
[2025-01-06 01:32:35,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:35,845][root][INFO] - Training Epoch: 6/10, step 353/574 completed (loss: 0.003162674605846405, acc: 1.0)
[2025-01-06 01:32:35,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:36,201][root][INFO] - Training Epoch: 6/10, step 354/574 completed (loss: 0.16553565859794617, acc: 0.9615384340286255)
[2025-01-06 01:32:36,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:36,541][root][INFO] - Training Epoch: 6/10, step 355/574 completed (loss: 0.30526354908943176, acc: 0.901098906993866)
[2025-01-06 01:32:36,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:37,042][root][INFO] - Training Epoch: 6/10, step 356/574 completed (loss: 0.2943090796470642, acc: 0.8608695864677429)
[2025-01-06 01:32:37,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:37,404][root][INFO] - Training Epoch: 6/10, step 357/574 completed (loss: 0.11500327289104462, acc: 0.97826087474823)
[2025-01-06 01:32:37,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:37,754][root][INFO] - Training Epoch: 6/10, step 358/574 completed (loss: 0.216099351644516, acc: 0.9387755393981934)
[2025-01-06 01:32:37,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38,099][root][INFO] - Training Epoch: 6/10, step 359/574 completed (loss: 0.0011403512908145785, acc: 1.0)
[2025-01-06 01:32:38,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38,436][root][INFO] - Training Epoch: 6/10, step 360/574 completed (loss: 0.01290606614202261, acc: 1.0)
[2025-01-06 01:32:38,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38,758][root][INFO] - Training Epoch: 6/10, step 361/574 completed (loss: 0.08943907916545868, acc: 0.9512194991111755)
[2025-01-06 01:32:38,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:39,063][root][INFO] - Training Epoch: 6/10, step 362/574 completed (loss: 0.14631672203540802, acc: 0.9777777791023254)
[2025-01-06 01:32:39,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:39,365][root][INFO] - Training Epoch: 6/10, step 363/574 completed (loss: 0.009582366794347763, acc: 1.0)
[2025-01-06 01:32:39,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:39,728][root][INFO] - Training Epoch: 6/10, step 364/574 completed (loss: 0.004893295466899872, acc: 1.0)
[2025-01-06 01:32:39,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:40,064][root][INFO] - Training Epoch: 6/10, step 365/574 completed (loss: 0.01092990767210722, acc: 1.0)
[2025-01-06 01:32:40,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:40,409][root][INFO] - Training Epoch: 6/10, step 366/574 completed (loss: 0.0006319808890111744, acc: 1.0)
[2025-01-06 01:32:40,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:40,746][root][INFO] - Training Epoch: 6/10, step 367/574 completed (loss: 0.0006503549520857632, acc: 1.0)
[2025-01-06 01:32:40,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:41,077][root][INFO] - Training Epoch: 6/10, step 368/574 completed (loss: 0.010306780226528645, acc: 1.0)
[2025-01-06 01:32:41,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:41,444][root][INFO] - Training Epoch: 6/10, step 369/574 completed (loss: 0.04824299365282059, acc: 1.0)
[2025-01-06 01:32:41,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:42,061][root][INFO] - Training Epoch: 6/10, step 370/574 completed (loss: 0.4249679148197174, acc: 0.8848484754562378)
[2025-01-06 01:32:42,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:42,912][root][INFO] - Training Epoch: 6/10, step 371/574 completed (loss: 0.12748026847839355, acc: 0.9622641801834106)
[2025-01-06 01:32:42,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:43,250][root][INFO] - Training Epoch: 6/10, step 372/574 completed (loss: 0.10234220325946808, acc: 0.9666666388511658)
[2025-01-06 01:32:43,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:43,584][root][INFO] - Training Epoch: 6/10, step 373/574 completed (loss: 0.029473092406988144, acc: 1.0)
[2025-01-06 01:32:43,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:43,910][root][INFO] - Training Epoch: 6/10, step 374/574 completed (loss: 0.12187951058149338, acc: 0.9428571462631226)
[2025-01-06 01:32:43,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:44,213][root][INFO] - Training Epoch: 6/10, step 375/574 completed (loss: 0.0024024026934057474, acc: 1.0)
[2025-01-06 01:32:44,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:44,549][root][INFO] - Training Epoch: 6/10, step 376/574 completed (loss: 0.0011791711440309882, acc: 1.0)
[2025-01-06 01:32:44,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:44,853][root][INFO] - Training Epoch: 6/10, step 377/574 completed (loss: 0.026005079969763756, acc: 1.0)
[2025-01-06 01:32:44,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:45,188][root][INFO] - Training Epoch: 6/10, step 378/574 completed (loss: 0.036788828670978546, acc: 0.9789473414421082)
[2025-01-06 01:32:45,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:45,778][root][INFO] - Training Epoch: 6/10, step 379/574 completed (loss: 0.22637973725795746, acc: 0.9221556782722473)
[2025-01-06 01:32:45,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:46,178][root][INFO] - Training Epoch: 6/10, step 380/574 completed (loss: 0.34817296266555786, acc: 0.9172932505607605)
[2025-01-06 01:32:46,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:47,423][root][INFO] - Training Epoch: 6/10, step 381/574 completed (loss: 0.6029040217399597, acc: 0.8395721912384033)
[2025-01-06 01:32:47,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:47,986][root][INFO] - Training Epoch: 6/10, step 382/574 completed (loss: 0.2258436381816864, acc: 0.954954981803894)
[2025-01-06 01:32:48,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:48,295][root][INFO] - Training Epoch: 6/10, step 383/574 completed (loss: 0.06360651552677155, acc: 1.0)
[2025-01-06 01:32:48,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:48,593][root][INFO] - Training Epoch: 6/10, step 384/574 completed (loss: 0.006333875935524702, acc: 1.0)
[2025-01-06 01:32:48,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:48,894][root][INFO] - Training Epoch: 6/10, step 385/574 completed (loss: 0.002457950497046113, acc: 1.0)
[2025-01-06 01:32:48,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:49,196][root][INFO] - Training Epoch: 6/10, step 386/574 completed (loss: 0.002728078281506896, acc: 1.0)
[2025-01-06 01:32:49,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:49,495][root][INFO] - Training Epoch: 6/10, step 387/574 completed (loss: 0.003034438006579876, acc: 1.0)
[2025-01-06 01:32:49,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:49,799][root][INFO] - Training Epoch: 6/10, step 388/574 completed (loss: 0.0013589821755886078, acc: 1.0)
[2025-01-06 01:32:49,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:50,126][root][INFO] - Training Epoch: 6/10, step 389/574 completed (loss: 0.0016884414944797754, acc: 1.0)
[2025-01-06 01:32:50,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:50,470][root][INFO] - Training Epoch: 6/10, step 390/574 completed (loss: 0.14206185936927795, acc: 0.9047619104385376)
[2025-01-06 01:32:50,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:50,806][root][INFO] - Training Epoch: 6/10, step 391/574 completed (loss: 0.18531154096126556, acc: 0.9259259104728699)
[2025-01-06 01:32:50,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:51,178][root][INFO] - Training Epoch: 6/10, step 392/574 completed (loss: 0.39421766996383667, acc: 0.8543689250946045)
[2025-01-06 01:32:51,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:51,717][root][INFO] - Training Epoch: 6/10, step 393/574 completed (loss: 0.5814769864082336, acc: 0.8529411554336548)
[2025-01-06 01:32:51,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:52,115][root][INFO] - Training Epoch: 6/10, step 394/574 completed (loss: 0.33364495635032654, acc: 0.9066666960716248)
[2025-01-06 01:32:52,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:52,524][root][INFO] - Training Epoch: 6/10, step 395/574 completed (loss: 0.41575613617897034, acc: 0.8888888955116272)
[2025-01-06 01:32:52,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:52,866][root][INFO] - Training Epoch: 6/10, step 396/574 completed (loss: 0.09238838404417038, acc: 0.9767441749572754)
[2025-01-06 01:32:52,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:53,198][root][INFO] - Training Epoch: 6/10, step 397/574 completed (loss: 0.1354547142982483, acc: 0.9583333134651184)
[2025-01-06 01:32:53,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:53,544][root][INFO] - Training Epoch: 6/10, step 398/574 completed (loss: 0.06422214210033417, acc: 0.9767441749572754)
[2025-01-06 01:32:53,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:53,869][root][INFO] - Training Epoch: 6/10, step 399/574 completed (loss: 0.03512624278664589, acc: 1.0)
[2025-01-06 01:32:53,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:54,416][root][INFO] - Training Epoch: 6/10, step 400/574 completed (loss: 0.06848934292793274, acc: 1.0)
[2025-01-06 01:32:54,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:54,768][root][INFO] - Training Epoch: 6/10, step 401/574 completed (loss: 0.14486336708068848, acc: 0.9466666579246521)
[2025-01-06 01:32:54,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:55,098][root][INFO] - Training Epoch: 6/10, step 402/574 completed (loss: 0.11121248453855515, acc: 0.939393937587738)
[2025-01-06 01:32:55,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:55,425][root][INFO] - Training Epoch: 6/10, step 403/574 completed (loss: 0.08614037185907364, acc: 0.939393937587738)
[2025-01-06 01:32:55,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:55,755][root][INFO] - Training Epoch: 6/10, step 404/574 completed (loss: 0.02276015281677246, acc: 1.0)
[2025-01-06 01:32:55,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:56,074][root][INFO] - Training Epoch: 6/10, step 405/574 completed (loss: 0.002504399511963129, acc: 1.0)
[2025-01-06 01:32:56,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:56,394][root][INFO] - Training Epoch: 6/10, step 406/574 completed (loss: 0.01828124187886715, acc: 1.0)
[2025-01-06 01:32:56,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:56,722][root][INFO] - Training Epoch: 6/10, step 407/574 completed (loss: 0.0025414940901100636, acc: 1.0)
[2025-01-06 01:32:56,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:57,061][root][INFO] - Training Epoch: 6/10, step 408/574 completed (loss: 0.003438895335420966, acc: 1.0)
[2025-01-06 01:32:57,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:57,416][root][INFO] - Training Epoch: 6/10, step 409/574 completed (loss: 0.001754772150889039, acc: 1.0)
[2025-01-06 01:32:57,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:57,768][root][INFO] - Training Epoch: 6/10, step 410/574 completed (loss: 0.019883345812559128, acc: 0.982758641242981)
[2025-01-06 01:32:57,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:58,090][root][INFO] - Training Epoch: 6/10, step 411/574 completed (loss: 0.012037183158099651, acc: 1.0)
[2025-01-06 01:32:58,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:58,423][root][INFO] - Training Epoch: 6/10, step 412/574 completed (loss: 0.0005653815460391343, acc: 1.0)
[2025-01-06 01:32:58,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:58,760][root][INFO] - Training Epoch: 6/10, step 413/574 completed (loss: 0.053919773548841476, acc: 0.9696969985961914)
[2025-01-06 01:32:58,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:59,178][root][INFO] - Training Epoch: 6/10, step 414/574 completed (loss: 0.10186150670051575, acc: 0.9090909361839294)
[2025-01-06 01:32:59,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:59,580][root][INFO] - Training Epoch: 6/10, step 415/574 completed (loss: 0.1357509046792984, acc: 0.9411764740943909)
[2025-01-06 01:32:59,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:59,891][root][INFO] - Training Epoch: 6/10, step 416/574 completed (loss: 0.029259197413921356, acc: 1.0)
[2025-01-06 01:32:59,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:00,231][root][INFO] - Training Epoch: 6/10, step 417/574 completed (loss: 0.014004609547555447, acc: 1.0)
[2025-01-06 01:33:00,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:00,582][root][INFO] - Training Epoch: 6/10, step 418/574 completed (loss: 0.04022429510951042, acc: 0.9750000238418579)
[2025-01-06 01:33:01,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:01,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:02,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:02,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:02,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:03,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:03,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:03,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:03,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:04,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:04,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:04,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:05,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:05,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:05,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:06,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:06,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:06,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:07,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:07,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:07,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:09,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:09,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:10,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:10,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:10,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:11,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:11,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:11,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:12,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:12,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:12,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:13,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:13,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:13,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:14,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:14,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:14,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:16,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:16,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:16,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:17,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:17,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:17,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:17,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:18,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:18,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:18,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:18,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:19,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:19,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:19,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:20,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:20,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:20,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:21,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:21,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:21,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:22,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:22,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:22,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:23,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:23,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:23,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:24,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:24,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:25,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:25,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:25,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:27,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:27,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:27,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:28,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:28,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:28,995][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2416, device='cuda:0') eval_epoch_loss=tensor(0.8072, device='cuda:0') eval_epoch_acc=tensor(0.8267, device='cuda:0')
[2025-01-06 01:33:28,997][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:33:28,997][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:33:29,366][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_419_loss_0.807183027267456/model.pt
[2025-01-06 01:33:29,371][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:33:29,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:29,707][root][INFO] - Training Epoch: 6/10, step 419/574 completed (loss: 0.010616546496748924, acc: 1.0)
[2025-01-06 01:33:29,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30,013][root][INFO] - Training Epoch: 6/10, step 420/574 completed (loss: 0.0018867915496230125, acc: 1.0)
[2025-01-06 01:33:30,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30,315][root][INFO] - Training Epoch: 6/10, step 421/574 completed (loss: 0.030319280922412872, acc: 1.0)
[2025-01-06 01:33:30,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30,610][root][INFO] - Training Epoch: 6/10, step 422/574 completed (loss: 0.03232948109507561, acc: 0.96875)
[2025-01-06 01:33:30,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30,930][root][INFO] - Training Epoch: 6/10, step 423/574 completed (loss: 0.0469970777630806, acc: 0.9722222089767456)
[2025-01-06 01:33:31,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:31,229][root][INFO] - Training Epoch: 6/10, step 424/574 completed (loss: 0.009267818182706833, acc: 1.0)
[2025-01-06 01:33:31,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:31,547][root][INFO] - Training Epoch: 6/10, step 425/574 completed (loss: 0.2311953902244568, acc: 0.939393937587738)
[2025-01-06 01:33:31,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:31,855][root][INFO] - Training Epoch: 6/10, step 426/574 completed (loss: 0.06312844902276993, acc: 0.95652174949646)
[2025-01-06 01:33:31,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:32,179][root][INFO] - Training Epoch: 6/10, step 427/574 completed (loss: 0.01834234595298767, acc: 1.0)
[2025-01-06 01:33:32,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:32,502][root][INFO] - Training Epoch: 6/10, step 428/574 completed (loss: 0.01301872730255127, acc: 1.0)
[2025-01-06 01:33:32,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:32,823][root][INFO] - Training Epoch: 6/10, step 429/574 completed (loss: 0.056513480842113495, acc: 0.95652174949646)
[2025-01-06 01:33:32,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:33,145][root][INFO] - Training Epoch: 6/10, step 430/574 completed (loss: 0.004747921135276556, acc: 1.0)
[2025-01-06 01:33:33,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:33,450][root][INFO] - Training Epoch: 6/10, step 431/574 completed (loss: 0.001115156919695437, acc: 1.0)
[2025-01-06 01:33:33,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:33,770][root][INFO] - Training Epoch: 6/10, step 432/574 completed (loss: 0.0006635895115323365, acc: 1.0)
[2025-01-06 01:33:33,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:34,127][root][INFO] - Training Epoch: 6/10, step 433/574 completed (loss: 0.21234025061130524, acc: 0.9722222089767456)
[2025-01-06 01:33:34,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:34,449][root][INFO] - Training Epoch: 6/10, step 434/574 completed (loss: 0.003720261389389634, acc: 1.0)
[2025-01-06 01:33:34,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:34,866][root][INFO] - Training Epoch: 6/10, step 435/574 completed (loss: 0.009692751802504063, acc: 1.0)
[2025-01-06 01:33:34,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:35,291][root][INFO] - Training Epoch: 6/10, step 436/574 completed (loss: 0.12499748170375824, acc: 0.9444444179534912)
[2025-01-06 01:33:35,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:35,671][root][INFO] - Training Epoch: 6/10, step 437/574 completed (loss: 0.028762727975845337, acc: 0.9772727489471436)
[2025-01-06 01:33:35,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:36,027][root][INFO] - Training Epoch: 6/10, step 438/574 completed (loss: 0.010404475033283234, acc: 1.0)
[2025-01-06 01:33:36,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:36,352][root][INFO] - Training Epoch: 6/10, step 439/574 completed (loss: 0.009487626142799854, acc: 1.0)
[2025-01-06 01:33:36,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:36,826][root][INFO] - Training Epoch: 6/10, step 440/574 completed (loss: 0.07950321584939957, acc: 0.9696969985961914)
[2025-01-06 01:33:37,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:37,536][root][INFO] - Training Epoch: 6/10, step 441/574 completed (loss: 0.603901743888855, acc: 0.8399999737739563)
[2025-01-06 01:33:37,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:37,960][root][INFO] - Training Epoch: 6/10, step 442/574 completed (loss: 0.4691774845123291, acc: 0.8629032373428345)
[2025-01-06 01:33:38,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:38,616][root][INFO] - Training Epoch: 6/10, step 443/574 completed (loss: 0.345438688993454, acc: 0.89552241563797)
[2025-01-06 01:33:38,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:38,943][root][INFO] - Training Epoch: 6/10, step 444/574 completed (loss: 0.05567646771669388, acc: 0.9622641801834106)
[2025-01-06 01:33:39,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:39,375][root][INFO] - Training Epoch: 6/10, step 445/574 completed (loss: 0.024734877049922943, acc: 1.0)
[2025-01-06 01:33:39,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:39,717][root][INFO] - Training Epoch: 6/10, step 446/574 completed (loss: 0.0027741771191358566, acc: 1.0)
[2025-01-06 01:33:39,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:40,101][root][INFO] - Training Epoch: 6/10, step 447/574 completed (loss: 0.07900816947221756, acc: 0.9615384340286255)
[2025-01-06 01:33:40,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:40,431][root][INFO] - Training Epoch: 6/10, step 448/574 completed (loss: 0.0008572966908104718, acc: 1.0)
[2025-01-06 01:33:40,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:40,764][root][INFO] - Training Epoch: 6/10, step 449/574 completed (loss: 0.08268040418624878, acc: 0.9552238583564758)
[2025-01-06 01:33:40,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41,101][root][INFO] - Training Epoch: 6/10, step 450/574 completed (loss: 0.02939728833734989, acc: 0.9861111044883728)
[2025-01-06 01:33:41,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41,422][root][INFO] - Training Epoch: 6/10, step 451/574 completed (loss: 0.010211379267275333, acc: 1.0)
[2025-01-06 01:33:41,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41,771][root][INFO] - Training Epoch: 6/10, step 452/574 completed (loss: 0.07107924669981003, acc: 0.9615384340286255)
[2025-01-06 01:33:41,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:42,104][root][INFO] - Training Epoch: 6/10, step 453/574 completed (loss: 0.18717704713344574, acc: 0.9736841917037964)
[2025-01-06 01:33:42,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:42,455][root][INFO] - Training Epoch: 6/10, step 454/574 completed (loss: 0.021258467808365822, acc: 1.0)
[2025-01-06 01:33:42,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:42,785][root][INFO] - Training Epoch: 6/10, step 455/574 completed (loss: 0.004541350528597832, acc: 1.0)
[2025-01-06 01:33:42,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:43,103][root][INFO] - Training Epoch: 6/10, step 456/574 completed (loss: 0.17082896828651428, acc: 0.9484536051750183)
[2025-01-06 01:33:43,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:43,424][root][INFO] - Training Epoch: 6/10, step 457/574 completed (loss: 0.018320199102163315, acc: 1.0)
[2025-01-06 01:33:43,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:43,816][root][INFO] - Training Epoch: 6/10, step 458/574 completed (loss: 0.14770573377609253, acc: 0.9476743936538696)
[2025-01-06 01:33:43,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:44,122][root][INFO] - Training Epoch: 6/10, step 459/574 completed (loss: 0.05257153511047363, acc: 0.9642857313156128)
[2025-01-06 01:33:44,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:44,486][root][INFO] - Training Epoch: 6/10, step 460/574 completed (loss: 0.02993132174015045, acc: 1.0)
[2025-01-06 01:33:44,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:44,845][root][INFO] - Training Epoch: 6/10, step 461/574 completed (loss: 0.005491858813911676, acc: 1.0)
[2025-01-06 01:33:44,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45,176][root][INFO] - Training Epoch: 6/10, step 462/574 completed (loss: 0.02607303485274315, acc: 1.0)
[2025-01-06 01:33:45,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45,486][root][INFO] - Training Epoch: 6/10, step 463/574 completed (loss: 0.0603378564119339, acc: 0.9615384340286255)
[2025-01-06 01:33:45,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45,782][root][INFO] - Training Epoch: 6/10, step 464/574 completed (loss: 0.011584766209125519, acc: 1.0)
[2025-01-06 01:33:45,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:46,079][root][INFO] - Training Epoch: 6/10, step 465/574 completed (loss: 0.07360707968473434, acc: 0.9642857313156128)
[2025-01-06 01:33:46,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:46,383][root][INFO] - Training Epoch: 6/10, step 466/574 completed (loss: 0.31737449765205383, acc: 0.9156626462936401)
[2025-01-06 01:33:46,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:46,717][root][INFO] - Training Epoch: 6/10, step 467/574 completed (loss: 0.0829029306769371, acc: 0.9819819927215576)
[2025-01-06 01:33:46,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47,029][root][INFO] - Training Epoch: 6/10, step 468/574 completed (loss: 0.21082670986652374, acc: 0.9611650705337524)
[2025-01-06 01:33:47,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47,339][root][INFO] - Training Epoch: 6/10, step 469/574 completed (loss: 0.175923153758049, acc: 0.9430894255638123)
[2025-01-06 01:33:47,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47,655][root][INFO] - Training Epoch: 6/10, step 470/574 completed (loss: 0.0026744662318378687, acc: 1.0)
[2025-01-06 01:33:47,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47,945][root][INFO] - Training Epoch: 6/10, step 471/574 completed (loss: 0.003870378015562892, acc: 1.0)
[2025-01-06 01:33:48,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:48,344][root][INFO] - Training Epoch: 6/10, step 472/574 completed (loss: 0.26681050658226013, acc: 0.9215686321258545)
[2025-01-06 01:33:48,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:48,738][root][INFO] - Training Epoch: 6/10, step 473/574 completed (loss: 0.40137985348701477, acc: 0.8777292370796204)
[2025-01-06 01:33:48,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:49,087][root][INFO] - Training Epoch: 6/10, step 474/574 completed (loss: 0.13319921493530273, acc: 0.9583333134651184)
[2025-01-06 01:33:49,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:49,421][root][INFO] - Training Epoch: 6/10, step 475/574 completed (loss: 0.18933561444282532, acc: 0.9386503100395203)
[2025-01-06 01:33:49,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:49,732][root][INFO] - Training Epoch: 6/10, step 476/574 completed (loss: 0.16108451783657074, acc: 0.9424460530281067)
[2025-01-06 01:33:49,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:50,067][root][INFO] - Training Epoch: 6/10, step 477/574 completed (loss: 0.31599050760269165, acc: 0.8944723606109619)
[2025-01-06 01:33:50,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:50,378][root][INFO] - Training Epoch: 6/10, step 478/574 completed (loss: 0.2539205253124237, acc: 0.8888888955116272)
[2025-01-06 01:33:50,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:50,705][root][INFO] - Training Epoch: 6/10, step 479/574 completed (loss: 0.053331948816776276, acc: 1.0)
[2025-01-06 01:33:50,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:51,019][root][INFO] - Training Epoch: 6/10, step 480/574 completed (loss: 0.005189498886466026, acc: 1.0)
[2025-01-06 01:33:51,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:51,331][root][INFO] - Training Epoch: 6/10, step 481/574 completed (loss: 0.023550357669591904, acc: 1.0)
[2025-01-06 01:33:51,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:51,652][root][INFO] - Training Epoch: 6/10, step 482/574 completed (loss: 0.04158687964081764, acc: 1.0)
[2025-01-06 01:33:51,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52,020][root][INFO] - Training Epoch: 6/10, step 483/574 completed (loss: 0.17082834243774414, acc: 0.9482758641242981)
[2025-01-06 01:33:52,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52,339][root][INFO] - Training Epoch: 6/10, step 484/574 completed (loss: 0.0013146803248673677, acc: 1.0)
[2025-01-06 01:33:52,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52,632][root][INFO] - Training Epoch: 6/10, step 485/574 completed (loss: 0.016828736290335655, acc: 1.0)
[2025-01-06 01:33:52,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52,926][root][INFO] - Training Epoch: 6/10, step 486/574 completed (loss: 0.10116251558065414, acc: 0.9629629850387573)
[2025-01-06 01:33:53,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53,225][root][INFO] - Training Epoch: 6/10, step 487/574 completed (loss: 0.024482110515236855, acc: 1.0)
[2025-01-06 01:33:53,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53,523][root][INFO] - Training Epoch: 6/10, step 488/574 completed (loss: 0.015758462250232697, acc: 1.0)
[2025-01-06 01:33:53,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53,861][root][INFO] - Training Epoch: 6/10, step 489/574 completed (loss: 0.27294617891311646, acc: 0.892307698726654)
[2025-01-06 01:33:53,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:54,160][root][INFO] - Training Epoch: 6/10, step 490/574 completed (loss: 0.06006141006946564, acc: 0.9666666388511658)
[2025-01-06 01:33:54,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:54,462][root][INFO] - Training Epoch: 6/10, step 491/574 completed (loss: 0.009046370163559914, acc: 1.0)
[2025-01-06 01:33:54,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:54,768][root][INFO] - Training Epoch: 6/10, step 492/574 completed (loss: 0.06803401559591293, acc: 0.9803921580314636)
[2025-01-06 01:33:54,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55,070][root][INFO] - Training Epoch: 6/10, step 493/574 completed (loss: 0.2953330874443054, acc: 0.931034505367279)
[2025-01-06 01:33:55,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55,378][root][INFO] - Training Epoch: 6/10, step 494/574 completed (loss: 0.026796404272317886, acc: 1.0)
[2025-01-06 01:33:55,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55,654][root][INFO] - Training Epoch: 6/10, step 495/574 completed (loss: 0.011508679017424583, acc: 1.0)
[2025-01-06 01:33:55,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55,996][root][INFO] - Training Epoch: 6/10, step 496/574 completed (loss: 0.39062389731407166, acc: 0.9107142686843872)
[2025-01-06 01:33:56,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:56,386][root][INFO] - Training Epoch: 6/10, step 497/574 completed (loss: 0.22615297138690948, acc: 0.9213483333587646)
[2025-01-06 01:33:56,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:56,694][root][INFO] - Training Epoch: 6/10, step 498/574 completed (loss: 0.24724359810352325, acc: 0.932584285736084)
[2025-01-06 01:33:56,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:57,002][root][INFO] - Training Epoch: 6/10, step 499/574 completed (loss: 0.6741212606430054, acc: 0.7943262457847595)
[2025-01-06 01:33:57,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:57,317][root][INFO] - Training Epoch: 6/10, step 500/574 completed (loss: 0.16968375444412231, acc: 0.945652186870575)
[2025-01-06 01:33:57,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:57,586][root][INFO] - Training Epoch: 6/10, step 501/574 completed (loss: 0.02928829938173294, acc: 1.0)
[2025-01-06 01:33:57,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:57,844][root][INFO] - Training Epoch: 6/10, step 502/574 completed (loss: 0.00034281532862223685, acc: 1.0)
[2025-01-06 01:33:57,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:58,171][root][INFO] - Training Epoch: 6/10, step 503/574 completed (loss: 0.015507025644183159, acc: 1.0)
[2025-01-06 01:33:58,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:58,505][root][INFO] - Training Epoch: 6/10, step 504/574 completed (loss: 0.024567551910877228, acc: 1.0)
[2025-01-06 01:33:58,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:58,858][root][INFO] - Training Epoch: 6/10, step 505/574 completed (loss: 0.3239670395851135, acc: 0.8867924809455872)
[2025-01-06 01:33:58,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:59,203][root][INFO] - Training Epoch: 6/10, step 506/574 completed (loss: 0.04711231216788292, acc: 0.9655172228813171)
[2025-01-06 01:33:59,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:59,848][root][INFO] - Training Epoch: 6/10, step 507/574 completed (loss: 0.8142815828323364, acc: 0.7747747898101807)
[2025-01-06 01:33:59,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:00,314][root][INFO] - Training Epoch: 6/10, step 508/574 completed (loss: 0.3378192186355591, acc: 0.8732394576072693)
[2025-01-06 01:34:00,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:00,641][root][INFO] - Training Epoch: 6/10, step 509/574 completed (loss: 0.0027735293842852116, acc: 1.0)
[2025-01-06 01:34:00,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:00,975][root][INFO] - Training Epoch: 6/10, step 510/574 completed (loss: 0.003914599772542715, acc: 1.0)
[2025-01-06 01:34:01,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:01,304][root][INFO] - Training Epoch: 6/10, step 511/574 completed (loss: 0.03872049227356911, acc: 0.9615384340286255)
[2025-01-06 01:34:02,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:03,688][root][INFO] - Training Epoch: 6/10, step 512/574 completed (loss: 0.5453023910522461, acc: 0.8785714507102966)
[2025-01-06 01:34:03,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:04,460][root][INFO] - Training Epoch: 6/10, step 513/574 completed (loss: 0.08605310320854187, acc: 0.976190447807312)
[2025-01-06 01:34:04,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:04,841][root][INFO] - Training Epoch: 6/10, step 514/574 completed (loss: 0.19505982100963593, acc: 0.9642857313156128)
[2025-01-06 01:34:04,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:05,210][root][INFO] - Training Epoch: 6/10, step 515/574 completed (loss: 0.06680455803871155, acc: 0.9833333492279053)
[2025-01-06 01:34:05,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:05,903][root][INFO] - Training Epoch: 6/10, step 516/574 completed (loss: 0.18831540644168854, acc: 0.9305555820465088)
[2025-01-06 01:34:05,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:06,242][root][INFO] - Training Epoch: 6/10, step 517/574 completed (loss: 0.0001894915767479688, acc: 1.0)
[2025-01-06 01:34:06,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:06,561][root][INFO] - Training Epoch: 6/10, step 518/574 completed (loss: 0.00784983765333891, acc: 1.0)
[2025-01-06 01:34:06,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:06,930][root][INFO] - Training Epoch: 6/10, step 519/574 completed (loss: 0.10656355321407318, acc: 0.949999988079071)
[2025-01-06 01:34:07,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:07,244][root][INFO] - Training Epoch: 6/10, step 520/574 completed (loss: 0.0030714343301951885, acc: 1.0)
[2025-01-06 01:34:07,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:08,234][root][INFO] - Training Epoch: 6/10, step 521/574 completed (loss: 0.5308884978294373, acc: 0.8432203531265259)
[2025-01-06 01:34:08,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:08,580][root][INFO] - Training Epoch: 6/10, step 522/574 completed (loss: 0.18745732307434082, acc: 0.9402984976768494)
[2025-01-06 01:34:08,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:08,950][root][INFO] - Training Epoch: 6/10, step 523/574 completed (loss: 0.14031729102134705, acc: 0.956204354763031)
[2025-01-06 01:34:09,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:09,508][root][INFO] - Training Epoch: 6/10, step 524/574 completed (loss: 0.3421371579170227, acc: 0.8999999761581421)
[2025-01-06 01:34:09,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:09,825][root][INFO] - Training Epoch: 6/10, step 525/574 completed (loss: 0.030333423987030983, acc: 1.0)
[2025-01-06 01:34:09,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:10,142][root][INFO] - Training Epoch: 6/10, step 526/574 completed (loss: 0.07727991789579391, acc: 0.9807692170143127)
[2025-01-06 01:34:10,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:10,455][root][INFO] - Training Epoch: 6/10, step 527/574 completed (loss: 0.0081782890483737, acc: 1.0)
[2025-01-06 01:34:10,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:10,807][root][INFO] - Training Epoch: 6/10, step 528/574 completed (loss: 0.2575919032096863, acc: 0.9672130942344666)
[2025-01-06 01:34:10,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:11,152][root][INFO] - Training Epoch: 6/10, step 529/574 completed (loss: 0.15430274605751038, acc: 0.9322034120559692)
[2025-01-06 01:34:11,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:11,486][root][INFO] - Training Epoch: 6/10, step 530/574 completed (loss: 0.4568032920360565, acc: 0.8372092843055725)
[2025-01-06 01:34:11,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:11,839][root][INFO] - Training Epoch: 6/10, step 531/574 completed (loss: 0.31241375207901, acc: 0.8636363744735718)
[2025-01-06 01:34:11,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:12,181][root][INFO] - Training Epoch: 6/10, step 532/574 completed (loss: 0.35009321570396423, acc: 0.9245283007621765)
[2025-01-06 01:34:12,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:12,491][root][INFO] - Training Epoch: 6/10, step 533/574 completed (loss: 0.04220636188983917, acc: 1.0)
[2025-01-06 01:34:12,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:12,797][root][INFO] - Training Epoch: 6/10, step 534/574 completed (loss: 0.03806332126259804, acc: 1.0)
[2025-01-06 01:34:12,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:13,134][root][INFO] - Training Epoch: 6/10, step 535/574 completed (loss: 0.05072270706295967, acc: 0.949999988079071)
[2025-01-06 01:34:13,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:13,433][root][INFO] - Training Epoch: 6/10, step 536/574 completed (loss: 0.003374268766492605, acc: 1.0)
[2025-01-06 01:34:13,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:13,822][root][INFO] - Training Epoch: 6/10, step 537/574 completed (loss: 0.15685094892978668, acc: 0.9538461565971375)
[2025-01-06 01:34:13,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:14,155][root][INFO] - Training Epoch: 6/10, step 538/574 completed (loss: 0.156529501080513, acc: 0.953125)
[2025-01-06 01:34:14,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:14,566][root][INFO] - Training Epoch: 6/10, step 539/574 completed (loss: 0.1716322898864746, acc: 0.90625)
[2025-01-06 01:34:14,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:14,915][root][INFO] - Training Epoch: 6/10, step 540/574 completed (loss: 0.5171963572502136, acc: 0.8484848737716675)
[2025-01-06 01:34:15,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:15,244][root][INFO] - Training Epoch: 6/10, step 541/574 completed (loss: 0.010688583366572857, acc: 1.0)
[2025-01-06 01:34:15,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:15,584][root][INFO] - Training Epoch: 6/10, step 542/574 completed (loss: 0.01519641000777483, acc: 1.0)
[2025-01-06 01:34:15,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:15,920][root][INFO] - Training Epoch: 6/10, step 543/574 completed (loss: 0.004670177586376667, acc: 1.0)
[2025-01-06 01:34:16,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:16,239][root][INFO] - Training Epoch: 6/10, step 544/574 completed (loss: 0.006316056475043297, acc: 1.0)
[2025-01-06 01:34:16,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:16,595][root][INFO] - Training Epoch: 6/10, step 545/574 completed (loss: 0.20214857161045074, acc: 0.9756097793579102)
[2025-01-06 01:34:16,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:16,933][root][INFO] - Training Epoch: 6/10, step 546/574 completed (loss: 0.01599574089050293, acc: 1.0)
[2025-01-06 01:34:17,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:17,241][root][INFO] - Training Epoch: 6/10, step 547/574 completed (loss: 0.004126070998609066, acc: 1.0)
[2025-01-06 01:34:17,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:17,545][root][INFO] - Training Epoch: 6/10, step 548/574 completed (loss: 0.02888917177915573, acc: 1.0)
[2025-01-06 01:34:17,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:17,862][root][INFO] - Training Epoch: 6/10, step 549/574 completed (loss: 0.0011078729294240475, acc: 1.0)
[2025-01-06 01:34:17,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:18,206][root][INFO] - Training Epoch: 6/10, step 550/574 completed (loss: 0.29548174142837524, acc: 0.8484848737716675)
[2025-01-06 01:34:18,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:18,560][root][INFO] - Training Epoch: 6/10, step 551/574 completed (loss: 0.08210422098636627, acc: 0.9750000238418579)
[2025-01-06 01:34:18,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:18,914][root][INFO] - Training Epoch: 6/10, step 552/574 completed (loss: 0.03925974667072296, acc: 0.9857142567634583)
[2025-01-06 01:34:19,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:19,261][root][INFO] - Training Epoch: 6/10, step 553/574 completed (loss: 0.13954895734786987, acc: 0.9635036587715149)
[2025-01-06 01:34:19,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:19,620][root][INFO] - Training Epoch: 6/10, step 554/574 completed (loss: 0.04128769040107727, acc: 0.9931034445762634)
[2025-01-06 01:34:19,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:19,962][root][INFO] - Training Epoch: 6/10, step 555/574 completed (loss: 0.1299060732126236, acc: 0.949999988079071)
[2025-01-06 01:34:20,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:20,313][root][INFO] - Training Epoch: 6/10, step 556/574 completed (loss: 0.17315785586833954, acc: 0.9602649211883545)
[2025-01-06 01:34:20,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:20,656][root][INFO] - Training Epoch: 6/10, step 557/574 completed (loss: 0.030606016516685486, acc: 1.0)
[2025-01-06 01:34:20,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:20,995][root][INFO] - Training Epoch: 6/10, step 558/574 completed (loss: 0.2112058848142624, acc: 0.9599999785423279)
[2025-01-06 01:34:21,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:21,345][root][INFO] - Training Epoch: 6/10, step 559/574 completed (loss: 0.22577546536922455, acc: 0.9230769276618958)
[2025-01-06 01:34:21,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:21,680][root][INFO] - Training Epoch: 6/10, step 560/574 completed (loss: 0.0010589960729703307, acc: 1.0)
[2025-01-06 01:34:21,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:21,981][root][INFO] - Training Epoch: 6/10, step 561/574 completed (loss: 0.0715101808309555, acc: 0.9743589758872986)
[2025-01-06 01:34:22,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:22,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:23,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:23,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:23,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:24,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:24,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:24,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:26,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:26,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:26,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:27,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:27,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:27,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:30,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:30,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:30,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:31,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:31,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:31,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:33,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:33,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:33,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:34,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:34,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:34,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:35,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:35,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:35,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:36,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:36,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:36,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:37,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:37,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:37,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:38,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:38,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:38,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:39,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:39,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:39,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:40,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:40,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:40,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:40,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:41,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:41,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:41,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:42,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:42,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:42,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:43,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:43,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:43,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:44,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:44,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:44,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:45,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:45,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:45,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:46,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:46,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:46,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:47,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:47,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:47,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:48,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:48,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:48,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:49,478][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3942, device='cuda:0') eval_epoch_loss=tensor(0.8730, device='cuda:0') eval_epoch_acc=tensor(0.8214, device='cuda:0')
[2025-01-06 01:34:49,479][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:34:49,480][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:34:49,774][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_562_loss_0.8730292916297913/model.pt
[2025-01-06 01:34:49,778][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:34:49,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:50,150][root][INFO] - Training Epoch: 6/10, step 562/574 completed (loss: 0.18322928249835968, acc: 0.9444444179534912)
[2025-01-06 01:34:50,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:50,473][root][INFO] - Training Epoch: 6/10, step 563/574 completed (loss: 0.1722952425479889, acc: 0.9350649118423462)
[2025-01-06 01:34:50,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:50,794][root][INFO] - Training Epoch: 6/10, step 564/574 completed (loss: 0.1499340683221817, acc: 0.9166666865348816)
[2025-01-06 01:34:50,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:51,092][root][INFO] - Training Epoch: 6/10, step 565/574 completed (loss: 0.01855316571891308, acc: 1.0)
[2025-01-06 01:34:51,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:51,432][root][INFO] - Training Epoch: 6/10, step 566/574 completed (loss: 0.09485635161399841, acc: 0.976190447807312)
[2025-01-06 01:34:51,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:51,808][root][INFO] - Training Epoch: 6/10, step 567/574 completed (loss: 0.009251946583390236, acc: 1.0)
[2025-01-06 01:34:51,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:52,140][root][INFO] - Training Epoch: 6/10, step 568/574 completed (loss: 0.06165238097310066, acc: 0.9629629850387573)
[2025-01-06 01:34:52,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:52,513][root][INFO] - Training Epoch: 6/10, step 569/574 completed (loss: 0.10810335725545883, acc: 0.9732620120048523)
[2025-01-06 01:34:52,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:52,837][root][INFO] - Training Epoch: 6/10, step 570/574 completed (loss: 0.013071156106889248, acc: 1.0)
[2025-01-06 01:34:52,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:53,198][root][INFO] - Training Epoch: 6/10, step 571/574 completed (loss: 0.08009839057922363, acc: 0.9572649598121643)
[2025-01-06 01:34:53,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:53,527][root][INFO] - Training Epoch: 6/10, step 572/574 completed (loss: 0.18555691838264465, acc: 0.954081654548645)
[2025-01-06 01:34:53,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:53,908][root][INFO] - Training Epoch: 6/10, step 573/574 completed (loss: 0.12100633233785629, acc: 0.9748427867889404)
[2025-01-06 01:34:54,354][slam_llm.utils.train_utils][INFO] - Epoch 6: train_perplexity=1.1797, train_epoch_loss=0.1653, epoch time 333.7258621901274s
[2025-01-06 01:34:54,354][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:34:54,354][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:34:54,355][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:34:54,355][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 17
[2025-01-06 01:34:54,355][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:34:54,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:55,178][root][INFO] - Training Epoch: 7/10, step 0/574 completed (loss: 0.008611262775957584, acc: 1.0)
[2025-01-06 01:34:55,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:55,471][root][INFO] - Training Epoch: 7/10, step 1/574 completed (loss: 0.04492209106683731, acc: 1.0)
[2025-01-06 01:34:55,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:55,782][root][INFO] - Training Epoch: 7/10, step 2/574 completed (loss: 0.13652461767196655, acc: 0.9459459185600281)
[2025-01-06 01:34:55,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:56,110][root][INFO] - Training Epoch: 7/10, step 3/574 completed (loss: 0.12535855174064636, acc: 0.9473684430122375)
[2025-01-06 01:34:56,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:56,420][root][INFO] - Training Epoch: 7/10, step 4/574 completed (loss: 0.05399925634264946, acc: 0.9729729890823364)
[2025-01-06 01:34:56,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:56,748][root][INFO] - Training Epoch: 7/10, step 5/574 completed (loss: 0.024929210543632507, acc: 1.0)
[2025-01-06 01:34:56,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57,066][root][INFO] - Training Epoch: 7/10, step 6/574 completed (loss: 0.16702358424663544, acc: 0.918367326259613)
[2025-01-06 01:34:57,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57,363][root][INFO] - Training Epoch: 7/10, step 7/574 completed (loss: 0.050304777920246124, acc: 1.0)
[2025-01-06 01:34:57,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57,672][root][INFO] - Training Epoch: 7/10, step 8/574 completed (loss: 0.001623420394025743, acc: 1.0)
[2025-01-06 01:34:57,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57,959][root][INFO] - Training Epoch: 7/10, step 9/574 completed (loss: 0.004372281953692436, acc: 1.0)
[2025-01-06 01:34:58,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:58,281][root][INFO] - Training Epoch: 7/10, step 10/574 completed (loss: 0.0030760338995605707, acc: 1.0)
[2025-01-06 01:34:58,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:58,590][root][INFO] - Training Epoch: 7/10, step 11/574 completed (loss: 0.04023264721035957, acc: 0.9743589758872986)
[2025-01-06 01:34:58,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:58,906][root][INFO] - Training Epoch: 7/10, step 12/574 completed (loss: 0.02077857032418251, acc: 1.0)
[2025-01-06 01:34:58,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:59,232][root][INFO] - Training Epoch: 7/10, step 13/574 completed (loss: 0.029862938448786736, acc: 0.97826087474823)
[2025-01-06 01:34:59,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:59,549][root][INFO] - Training Epoch: 7/10, step 14/574 completed (loss: 0.045195575803518295, acc: 0.9803921580314636)
[2025-01-06 01:34:59,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:59,861][root][INFO] - Training Epoch: 7/10, step 15/574 completed (loss: 0.042509160935878754, acc: 1.0)
[2025-01-06 01:34:59,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:00,161][root][INFO] - Training Epoch: 7/10, step 16/574 completed (loss: 0.009572403505444527, acc: 1.0)
[2025-01-06 01:35:00,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:00,462][root][INFO] - Training Epoch: 7/10, step 17/574 completed (loss: 0.003962119575589895, acc: 1.0)
[2025-01-06 01:35:00,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:00,776][root][INFO] - Training Epoch: 7/10, step 18/574 completed (loss: 0.08420254290103912, acc: 0.9722222089767456)
[2025-01-06 01:35:00,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:01,126][root][INFO] - Training Epoch: 7/10, step 19/574 completed (loss: 0.011062594130635262, acc: 1.0)
[2025-01-06 01:35:01,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:01,447][root][INFO] - Training Epoch: 7/10, step 20/574 completed (loss: 0.012713223695755005, acc: 1.0)
[2025-01-06 01:35:01,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:01,777][root][INFO] - Training Epoch: 7/10, step 21/574 completed (loss: 0.05400039255619049, acc: 0.9655172228813171)
[2025-01-06 01:35:01,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:02,131][root][INFO] - Training Epoch: 7/10, step 22/574 completed (loss: 0.047967538237571716, acc: 0.9599999785423279)
[2025-01-06 01:35:02,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:02,442][root][INFO] - Training Epoch: 7/10, step 23/574 completed (loss: 0.004906706977635622, acc: 1.0)
[2025-01-06 01:35:02,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:02,770][root][INFO] - Training Epoch: 7/10, step 24/574 completed (loss: 0.006794271059334278, acc: 1.0)
[2025-01-06 01:35:02,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:03,082][root][INFO] - Training Epoch: 7/10, step 25/574 completed (loss: 0.036085087805986404, acc: 0.9811320900917053)
[2025-01-06 01:35:03,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:03,403][root][INFO] - Training Epoch: 7/10, step 26/574 completed (loss: 0.14997388422489166, acc: 0.931506872177124)
[2025-01-06 01:35:03,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:04,679][root][INFO] - Training Epoch: 7/10, step 27/574 completed (loss: 0.5491489768028259, acc: 0.8221343755722046)
[2025-01-06 01:35:04,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:04,998][root][INFO] - Training Epoch: 7/10, step 28/574 completed (loss: 0.022655662149190903, acc: 1.0)
[2025-01-06 01:35:05,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:05,307][root][INFO] - Training Epoch: 7/10, step 29/574 completed (loss: 0.09424316138029099, acc: 0.9638554453849792)
[2025-01-06 01:35:05,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:05,646][root][INFO] - Training Epoch: 7/10, step 30/574 completed (loss: 0.046476107090711594, acc: 0.9753086566925049)
[2025-01-06 01:35:05,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:05,990][root][INFO] - Training Epoch: 7/10, step 31/574 completed (loss: 0.10567981749773026, acc: 0.9642857313156128)
[2025-01-06 01:35:06,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:06,316][root][INFO] - Training Epoch: 7/10, step 32/574 completed (loss: 0.0020309702958911657, acc: 1.0)
[2025-01-06 01:35:06,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:06,614][root][INFO] - Training Epoch: 7/10, step 33/574 completed (loss: 0.002814924344420433, acc: 1.0)
[2025-01-06 01:35:06,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:06,984][root][INFO] - Training Epoch: 7/10, step 34/574 completed (loss: 0.075599305331707, acc: 0.9747899174690247)
[2025-01-06 01:35:07,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:07,313][root][INFO] - Training Epoch: 7/10, step 35/574 completed (loss: 0.03883478790521622, acc: 0.9836065769195557)
[2025-01-06 01:35:07,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:07,666][root][INFO] - Training Epoch: 7/10, step 36/574 completed (loss: 0.05823792517185211, acc: 0.9841269850730896)
[2025-01-06 01:35:07,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:07,991][root][INFO] - Training Epoch: 7/10, step 37/574 completed (loss: 0.015441126190125942, acc: 1.0)
[2025-01-06 01:35:08,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:08,328][root][INFO] - Training Epoch: 7/10, step 38/574 completed (loss: 0.08139043301343918, acc: 0.9655172228813171)
[2025-01-06 01:35:08,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:08,619][root][INFO] - Training Epoch: 7/10, step 39/574 completed (loss: 0.24215716123580933, acc: 0.9523809552192688)
[2025-01-06 01:35:08,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:08,930][root][INFO] - Training Epoch: 7/10, step 40/574 completed (loss: 0.09184591472148895, acc: 0.9615384340286255)
[2025-01-06 01:35:09,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:09,297][root][INFO] - Training Epoch: 7/10, step 41/574 completed (loss: 0.09111896902322769, acc: 0.9594594836235046)
[2025-01-06 01:35:09,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:09,617][root][INFO] - Training Epoch: 7/10, step 42/574 completed (loss: 0.18494157493114471, acc: 0.9538461565971375)
[2025-01-06 01:35:09,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:10,008][root][INFO] - Training Epoch: 7/10, step 43/574 completed (loss: 0.170636385679245, acc: 0.9494949579238892)
[2025-01-06 01:35:10,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:10,422][root][INFO] - Training Epoch: 7/10, step 44/574 completed (loss: 0.10995933413505554, acc: 0.9484536051750183)
[2025-01-06 01:35:10,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:10,828][root][INFO] - Training Epoch: 7/10, step 45/574 completed (loss: 0.1081048771739006, acc: 0.970588207244873)
[2025-01-06 01:35:10,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:11,141][root][INFO] - Training Epoch: 7/10, step 46/574 completed (loss: 0.01087137870490551, acc: 1.0)
[2025-01-06 01:35:11,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:11,489][root][INFO] - Training Epoch: 7/10, step 47/574 completed (loss: 0.0025567507836967707, acc: 1.0)
[2025-01-06 01:35:11,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:11,835][root][INFO] - Training Epoch: 7/10, step 48/574 completed (loss: 0.0031841027084738016, acc: 1.0)
[2025-01-06 01:35:11,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:12,165][root][INFO] - Training Epoch: 7/10, step 49/574 completed (loss: 0.0030204772483557463, acc: 1.0)
[2025-01-06 01:35:12,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:12,489][root][INFO] - Training Epoch: 7/10, step 50/574 completed (loss: 0.15646547079086304, acc: 0.9473684430122375)
[2025-01-06 01:35:12,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:12,765][root][INFO] - Training Epoch: 7/10, step 51/574 completed (loss: 0.0811251550912857, acc: 0.9841269850730896)
[2025-01-06 01:35:12,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:13,076][root][INFO] - Training Epoch: 7/10, step 52/574 completed (loss: 0.1576269418001175, acc: 0.9295774698257446)
[2025-01-06 01:35:13,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:13,533][root][INFO] - Training Epoch: 7/10, step 53/574 completed (loss: 0.675944447517395, acc: 0.7666666507720947)
[2025-01-06 01:35:13,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:13,823][root][INFO] - Training Epoch: 7/10, step 54/574 completed (loss: 0.0695224180817604, acc: 0.9729729890823364)
[2025-01-06 01:35:13,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:14,132][root][INFO] - Training Epoch: 7/10, step 55/574 completed (loss: 0.013515312224626541, acc: 1.0)
[2025-01-06 01:35:15,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:17,092][root][INFO] - Training Epoch: 7/10, step 56/574 completed (loss: 0.7208430171012878, acc: 0.7952218651771545)
[2025-01-06 01:35:17,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:18,265][root][INFO] - Training Epoch: 7/10, step 57/574 completed (loss: 0.948214590549469, acc: 0.7254902124404907)
[2025-01-06 01:35:18,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:18,899][root][INFO] - Training Epoch: 7/10, step 58/574 completed (loss: 0.4078512489795685, acc: 0.8465909361839294)
[2025-01-06 01:35:19,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:19,470][root][INFO] - Training Epoch: 7/10, step 59/574 completed (loss: 0.12048635631799698, acc: 0.9485294222831726)
[2025-01-06 01:35:19,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:20,031][root][INFO] - Training Epoch: 7/10, step 60/574 completed (loss: 0.41678640246391296, acc: 0.8695651888847351)
[2025-01-06 01:35:20,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:20,435][root][INFO] - Training Epoch: 7/10, step 61/574 completed (loss: 0.17407551407814026, acc: 0.949999988079071)
[2025-01-06 01:35:20,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:20,766][root][INFO] - Training Epoch: 7/10, step 62/574 completed (loss: 0.016828546300530434, acc: 1.0)
[2025-01-06 01:35:20,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:21,116][root][INFO] - Training Epoch: 7/10, step 63/574 completed (loss: 0.044172197580337524, acc: 1.0)
[2025-01-06 01:35:21,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:21,480][root][INFO] - Training Epoch: 7/10, step 64/574 completed (loss: 0.2077743113040924, acc: 0.96875)
[2025-01-06 01:35:21,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:21,824][root][INFO] - Training Epoch: 7/10, step 65/574 completed (loss: 0.03916146978735924, acc: 1.0)
[2025-01-06 01:35:21,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:22,188][root][INFO] - Training Epoch: 7/10, step 66/574 completed (loss: 0.05443638190627098, acc: 0.9821428656578064)
[2025-01-06 01:35:22,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:22,589][root][INFO] - Training Epoch: 7/10, step 67/574 completed (loss: 0.07475534826517105, acc: 0.9666666388511658)
[2025-01-06 01:35:22,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:22,940][root][INFO] - Training Epoch: 7/10, step 68/574 completed (loss: 0.004897124134004116, acc: 1.0)
[2025-01-06 01:35:23,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:23,249][root][INFO] - Training Epoch: 7/10, step 69/574 completed (loss: 0.04406460374593735, acc: 1.0)
[2025-01-06 01:35:23,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:23,557][root][INFO] - Training Epoch: 7/10, step 70/574 completed (loss: 0.0611802376806736, acc: 0.9696969985961914)
[2025-01-06 01:35:23,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:23,879][root][INFO] - Training Epoch: 7/10, step 71/574 completed (loss: 0.36002057790756226, acc: 0.8676470518112183)
[2025-01-06 01:35:23,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:24,268][root][INFO] - Training Epoch: 7/10, step 72/574 completed (loss: 0.19169022142887115, acc: 0.9285714030265808)
[2025-01-06 01:35:24,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:24,618][root][INFO] - Training Epoch: 7/10, step 73/574 completed (loss: 0.6809706687927246, acc: 0.8307692408561707)
[2025-01-06 01:35:24,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:24,973][root][INFO] - Training Epoch: 7/10, step 74/574 completed (loss: 0.34492048621177673, acc: 0.9081632494926453)
[2025-01-06 01:35:25,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:25,294][root][INFO] - Training Epoch: 7/10, step 75/574 completed (loss: 0.3766469657421112, acc: 0.888059675693512)
[2025-01-06 01:35:25,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:25,672][root][INFO] - Training Epoch: 7/10, step 76/574 completed (loss: 0.9450035095214844, acc: 0.7518247961997986)
[2025-01-06 01:35:25,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:25,962][root][INFO] - Training Epoch: 7/10, step 77/574 completed (loss: 0.004178621806204319, acc: 1.0)
[2025-01-06 01:35:26,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:26,259][root][INFO] - Training Epoch: 7/10, step 78/574 completed (loss: 0.003068129299208522, acc: 1.0)
[2025-01-06 01:35:26,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:26,559][root][INFO] - Training Epoch: 7/10, step 79/574 completed (loss: 0.007396772038191557, acc: 1.0)
[2025-01-06 01:35:26,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:26,863][root][INFO] - Training Epoch: 7/10, step 80/574 completed (loss: 0.001419399632140994, acc: 1.0)
[2025-01-06 01:35:26,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:27,164][root][INFO] - Training Epoch: 7/10, step 81/574 completed (loss: 0.05226580426096916, acc: 1.0)
[2025-01-06 01:35:27,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:27,470][root][INFO] - Training Epoch: 7/10, step 82/574 completed (loss: 0.07510416209697723, acc: 0.9807692170143127)
[2025-01-06 01:35:27,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:27,778][root][INFO] - Training Epoch: 7/10, step 83/574 completed (loss: 0.0436381995677948, acc: 1.0)
[2025-01-06 01:35:27,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:28,094][root][INFO] - Training Epoch: 7/10, step 84/574 completed (loss: 0.0393863171339035, acc: 1.0)
[2025-01-06 01:35:28,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:28,404][root][INFO] - Training Epoch: 7/10, step 85/574 completed (loss: 0.09344083815813065, acc: 1.0)
[2025-01-06 01:35:28,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:28,705][root][INFO] - Training Epoch: 7/10, step 86/574 completed (loss: 0.007867999374866486, acc: 1.0)
[2025-01-06 01:35:28,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:29,156][root][INFO] - Training Epoch: 7/10, step 87/574 completed (loss: 0.21156752109527588, acc: 0.9599999785423279)
[2025-01-06 01:35:29,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:29,489][root][INFO] - Training Epoch: 7/10, step 88/574 completed (loss: 0.30888569355010986, acc: 0.893203854560852)
[2025-01-06 01:35:29,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:30,559][root][INFO] - Training Epoch: 7/10, step 89/574 completed (loss: 0.4925156831741333, acc: 0.8689320683479309)
[2025-01-06 01:35:30,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:31,374][root][INFO] - Training Epoch: 7/10, step 90/574 completed (loss: 0.4551818072795868, acc: 0.8709677457809448)
[2025-01-06 01:35:31,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:32,174][root][INFO] - Training Epoch: 7/10, step 91/574 completed (loss: 0.5213495492935181, acc: 0.8491379022598267)
[2025-01-06 01:35:32,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:32,915][root][INFO] - Training Epoch: 7/10, step 92/574 completed (loss: 0.38135334849357605, acc: 0.8842105269432068)
[2025-01-06 01:35:33,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:33,906][root][INFO] - Training Epoch: 7/10, step 93/574 completed (loss: 0.337309330701828, acc: 0.8712871074676514)
[2025-01-06 01:35:33,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:34,220][root][INFO] - Training Epoch: 7/10, step 94/574 completed (loss: 0.3079391419887543, acc: 0.8870967626571655)
[2025-01-06 01:35:34,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:34,550][root][INFO] - Training Epoch: 7/10, step 95/574 completed (loss: 0.21906302869319916, acc: 0.95652174949646)
[2025-01-06 01:35:34,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:34,865][root][INFO] - Training Epoch: 7/10, step 96/574 completed (loss: 0.3232272267341614, acc: 0.8907563090324402)
[2025-01-06 01:35:34,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:35,192][root][INFO] - Training Epoch: 7/10, step 97/574 completed (loss: 0.30108463764190674, acc: 0.8846153616905212)
[2025-01-06 01:35:35,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:35,569][root][INFO] - Training Epoch: 7/10, step 98/574 completed (loss: 0.38185954093933105, acc: 0.8686131238937378)
[2025-01-06 01:35:35,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:35,877][root][INFO] - Training Epoch: 7/10, step 99/574 completed (loss: 0.27770787477493286, acc: 0.9552238583564758)
[2025-01-06 01:35:35,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:36,182][root][INFO] - Training Epoch: 7/10, step 100/574 completed (loss: 0.0025447760708630085, acc: 1.0)
[2025-01-06 01:35:36,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:36,489][root][INFO] - Training Epoch: 7/10, step 101/574 completed (loss: 0.003563457168638706, acc: 1.0)
[2025-01-06 01:35:36,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:36,794][root][INFO] - Training Epoch: 7/10, step 102/574 completed (loss: 0.03735300898551941, acc: 1.0)
[2025-01-06 01:35:36,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:37,191][root][INFO] - Training Epoch: 7/10, step 103/574 completed (loss: 0.0033564353361725807, acc: 1.0)
[2025-01-06 01:35:37,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:37,545][root][INFO] - Training Epoch: 7/10, step 104/574 completed (loss: 0.06969104707241058, acc: 0.982758641242981)
[2025-01-06 01:35:37,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:37,896][root][INFO] - Training Epoch: 7/10, step 105/574 completed (loss: 0.0030898465774953365, acc: 1.0)
[2025-01-06 01:35:37,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:38,235][root][INFO] - Training Epoch: 7/10, step 106/574 completed (loss: 0.009862677194178104, acc: 1.0)
[2025-01-06 01:35:38,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:38,651][root][INFO] - Training Epoch: 7/10, step 107/574 completed (loss: 0.00813988782465458, acc: 1.0)
[2025-01-06 01:35:38,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:39,015][root][INFO] - Training Epoch: 7/10, step 108/574 completed (loss: 0.0007828667294234037, acc: 1.0)
[2025-01-06 01:35:39,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:39,351][root][INFO] - Training Epoch: 7/10, step 109/574 completed (loss: 0.004611083306372166, acc: 1.0)
[2025-01-06 01:35:39,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:39,714][root][INFO] - Training Epoch: 7/10, step 110/574 completed (loss: 0.03423811122775078, acc: 0.9846153855323792)
[2025-01-06 01:35:39,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:40,130][root][INFO] - Training Epoch: 7/10, step 111/574 completed (loss: 0.03766053915023804, acc: 1.0)
[2025-01-06 01:35:40,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:40,500][root][INFO] - Training Epoch: 7/10, step 112/574 completed (loss: 0.19344238936901093, acc: 0.9649122953414917)
[2025-01-06 01:35:40,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:40,852][root][INFO] - Training Epoch: 7/10, step 113/574 completed (loss: 0.04013235867023468, acc: 1.0)
[2025-01-06 01:35:40,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:41,242][root][INFO] - Training Epoch: 7/10, step 114/574 completed (loss: 0.17811378836631775, acc: 0.9591836929321289)
[2025-01-06 01:35:41,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:41,570][root][INFO] - Training Epoch: 7/10, step 115/574 completed (loss: 0.010346971452236176, acc: 1.0)
[2025-01-06 01:35:41,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:41,931][root][INFO] - Training Epoch: 7/10, step 116/574 completed (loss: 0.2491113245487213, acc: 0.9523809552192688)
[2025-01-06 01:35:42,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:42,299][root][INFO] - Training Epoch: 7/10, step 117/574 completed (loss: 0.1662766933441162, acc: 0.9430894255638123)
[2025-01-06 01:35:42,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:42,674][root][INFO] - Training Epoch: 7/10, step 118/574 completed (loss: 0.11246758699417114, acc: 0.9677419066429138)
[2025-01-06 01:35:42,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:43,510][root][INFO] - Training Epoch: 7/10, step 119/574 completed (loss: 0.3168603181838989, acc: 0.9201520681381226)
[2025-01-06 01:35:43,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:43,862][root][INFO] - Training Epoch: 7/10, step 120/574 completed (loss: 0.1910942792892456, acc: 0.9599999785423279)
[2025-01-06 01:35:43,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:44,271][root][INFO] - Training Epoch: 7/10, step 121/574 completed (loss: 0.0688038021326065, acc: 0.942307710647583)
[2025-01-06 01:35:44,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:44,616][root][INFO] - Training Epoch: 7/10, step 122/574 completed (loss: 0.007112998049706221, acc: 1.0)
[2025-01-06 01:35:44,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45,009][root][INFO] - Training Epoch: 7/10, step 123/574 completed (loss: 0.0029035997577011585, acc: 1.0)
[2025-01-06 01:35:45,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45,343][root][INFO] - Training Epoch: 7/10, step 124/574 completed (loss: 0.43226760625839233, acc: 0.8650306463241577)
[2025-01-06 01:35:45,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45,656][root][INFO] - Training Epoch: 7/10, step 125/574 completed (loss: 0.41424429416656494, acc: 0.8819444179534912)
[2025-01-06 01:35:45,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45,956][root][INFO] - Training Epoch: 7/10, step 126/574 completed (loss: 0.45941364765167236, acc: 0.8333333134651184)
[2025-01-06 01:35:46,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:46,280][root][INFO] - Training Epoch: 7/10, step 127/574 completed (loss: 0.24616625905036926, acc: 0.9226190447807312)
[2025-01-06 01:35:46,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:46,613][root][INFO] - Training Epoch: 7/10, step 128/574 completed (loss: 0.37599295377731323, acc: 0.9025641083717346)
[2025-01-06 01:35:46,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:47,025][root][INFO] - Training Epoch: 7/10, step 129/574 completed (loss: 0.3830764591693878, acc: 0.8970588445663452)
[2025-01-06 01:35:47,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:47,338][root][INFO] - Training Epoch: 7/10, step 130/574 completed (loss: 0.0185730941593647, acc: 1.0)
[2025-01-06 01:35:48,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:48,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:48,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:49,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:49,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:49,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:50,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:50,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:50,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:51,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:51,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:51,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:53,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:53,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:53,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:53,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:54,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:54,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:54,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:55,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:55,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:55,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:56,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:56,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:56,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:57,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:57,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:58,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:58,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:58,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:00,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:00,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:00,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:01,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:01,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:01,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:02,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:02,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:02,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:03,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:03,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:03,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:04,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:04,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:04,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:05,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:05,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:05,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:06,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:06,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:06,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:07,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:07,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:07,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:08,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:08,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:08,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:09,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:09,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:09,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:10,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:10,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:10,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:11,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:11,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:11,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:13,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:13,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:13,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:13,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:14,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:14,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:14,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:15,676][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2741, device='cuda:0') eval_epoch_loss=tensor(0.8216, device='cuda:0') eval_epoch_acc=tensor(0.8304, device='cuda:0')
[2025-01-06 01:36:15,678][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:36:15,678][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:36:15,922][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_131_loss_0.8215715289115906/model.pt
[2025-01-06 01:36:15,926][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:36:16,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:16,285][root][INFO] - Training Epoch: 7/10, step 131/574 completed (loss: 0.01643170416355133, acc: 1.0)
[2025-01-06 01:36:16,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:16,622][root][INFO] - Training Epoch: 7/10, step 132/574 completed (loss: 0.24116280674934387, acc: 0.875)
[2025-01-06 01:36:16,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:16,942][root][INFO] - Training Epoch: 7/10, step 133/574 completed (loss: 0.1345646232366562, acc: 0.95652174949646)
[2025-01-06 01:36:17,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:17,251][root][INFO] - Training Epoch: 7/10, step 134/574 completed (loss: 0.03838033601641655, acc: 0.9714285731315613)
[2025-01-06 01:36:17,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:17,556][root][INFO] - Training Epoch: 7/10, step 135/574 completed (loss: 0.01135534979403019, acc: 1.0)
[2025-01-06 01:36:17,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:17,845][root][INFO] - Training Epoch: 7/10, step 136/574 completed (loss: 0.0781264528632164, acc: 0.976190447807312)
[2025-01-06 01:36:17,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:18,190][root][INFO] - Training Epoch: 7/10, step 137/574 completed (loss: 0.11003577709197998, acc: 0.9666666388511658)
[2025-01-06 01:36:18,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:18,509][root][INFO] - Training Epoch: 7/10, step 138/574 completed (loss: 0.0163691695779562, acc: 1.0)
[2025-01-06 01:36:18,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:18,861][root][INFO] - Training Epoch: 7/10, step 139/574 completed (loss: 0.015326546505093575, acc: 1.0)
[2025-01-06 01:36:18,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:19,217][root][INFO] - Training Epoch: 7/10, step 140/574 completed (loss: 0.005089260172098875, acc: 1.0)
[2025-01-06 01:36:19,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:19,549][root][INFO] - Training Epoch: 7/10, step 141/574 completed (loss: 0.12333638966083527, acc: 0.9677419066429138)
[2025-01-06 01:36:19,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:19,865][root][INFO] - Training Epoch: 7/10, step 142/574 completed (loss: 0.3184846043586731, acc: 0.9459459185600281)
[2025-01-06 01:36:19,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:20,394][root][INFO] - Training Epoch: 7/10, step 143/574 completed (loss: 0.21254034340381622, acc: 0.9210526347160339)
[2025-01-06 01:36:20,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:20,745][root][INFO] - Training Epoch: 7/10, step 144/574 completed (loss: 0.4064680337905884, acc: 0.8507462739944458)
[2025-01-06 01:36:20,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:21,122][root][INFO] - Training Epoch: 7/10, step 145/574 completed (loss: 0.37581878900527954, acc: 0.8775510191917419)
[2025-01-06 01:36:21,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:21,569][root][INFO] - Training Epoch: 7/10, step 146/574 completed (loss: 0.29378801584243774, acc: 0.914893627166748)
[2025-01-06 01:36:21,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:21,883][root][INFO] - Training Epoch: 7/10, step 147/574 completed (loss: 0.20369461178779602, acc: 0.9285714030265808)
[2025-01-06 01:36:21,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:22,175][root][INFO] - Training Epoch: 7/10, step 148/574 completed (loss: 0.13071571290493011, acc: 0.9642857313156128)
[2025-01-06 01:36:22,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:22,471][root][INFO] - Training Epoch: 7/10, step 149/574 completed (loss: 0.15155360102653503, acc: 0.95652174949646)
[2025-01-06 01:36:22,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:22,785][root][INFO] - Training Epoch: 7/10, step 150/574 completed (loss: 0.05402267724275589, acc: 0.9655172228813171)
[2025-01-06 01:36:22,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:23,092][root][INFO] - Training Epoch: 7/10, step 151/574 completed (loss: 0.2280484288930893, acc: 0.95652174949646)
[2025-01-06 01:36:23,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:23,419][root][INFO] - Training Epoch: 7/10, step 152/574 completed (loss: 0.3245638608932495, acc: 0.9152542352676392)
[2025-01-06 01:36:23,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:23,717][root][INFO] - Training Epoch: 7/10, step 153/574 completed (loss: 0.0884610041975975, acc: 0.9824561476707458)
[2025-01-06 01:36:23,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24,039][root][INFO] - Training Epoch: 7/10, step 154/574 completed (loss: 0.1073869839310646, acc: 0.9594594836235046)
[2025-01-06 01:36:24,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24,367][root][INFO] - Training Epoch: 7/10, step 155/574 completed (loss: 0.01943925954401493, acc: 1.0)
[2025-01-06 01:36:24,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24,681][root][INFO] - Training Epoch: 7/10, step 156/574 completed (loss: 0.002422221004962921, acc: 1.0)
[2025-01-06 01:36:24,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24,970][root][INFO] - Training Epoch: 7/10, step 157/574 completed (loss: 0.31721460819244385, acc: 0.8947368264198303)
[2025-01-06 01:36:25,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:26,565][root][INFO] - Training Epoch: 7/10, step 158/574 completed (loss: 0.3330450654029846, acc: 0.8918918967247009)
[2025-01-06 01:36:26,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:26,906][root][INFO] - Training Epoch: 7/10, step 159/574 completed (loss: 0.3241678476333618, acc: 0.9074074029922485)
[2025-01-06 01:36:27,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:27,334][root][INFO] - Training Epoch: 7/10, step 160/574 completed (loss: 0.2905013859272003, acc: 0.9418604373931885)
[2025-01-06 01:36:27,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:27,943][root][INFO] - Training Epoch: 7/10, step 161/574 completed (loss: 0.45708218216896057, acc: 0.8470588326454163)
[2025-01-06 01:36:28,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:28,521][root][INFO] - Training Epoch: 7/10, step 162/574 completed (loss: 0.5151668190956116, acc: 0.8876404762268066)
[2025-01-06 01:36:28,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:28,866][root][INFO] - Training Epoch: 7/10, step 163/574 completed (loss: 0.10221819579601288, acc: 0.9772727489471436)
[2025-01-06 01:36:28,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:29,228][root][INFO] - Training Epoch: 7/10, step 164/574 completed (loss: 0.42192980647087097, acc: 0.9047619104385376)
[2025-01-06 01:36:29,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:29,597][root][INFO] - Training Epoch: 7/10, step 165/574 completed (loss: 0.21392059326171875, acc: 0.9655172228813171)
[2025-01-06 01:36:29,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:29,945][root][INFO] - Training Epoch: 7/10, step 166/574 completed (loss: 0.01514999009668827, acc: 1.0)
[2025-01-06 01:36:30,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:30,278][root][INFO] - Training Epoch: 7/10, step 167/574 completed (loss: 0.0622778981924057, acc: 0.9599999785423279)
[2025-01-06 01:36:30,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:30,700][root][INFO] - Training Epoch: 7/10, step 168/574 completed (loss: 0.15540280938148499, acc: 0.9444444179534912)
[2025-01-06 01:36:30,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:31,054][root][INFO] - Training Epoch: 7/10, step 169/574 completed (loss: 0.3630545735359192, acc: 0.9019607901573181)
[2025-01-06 01:36:31,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:32,080][root][INFO] - Training Epoch: 7/10, step 170/574 completed (loss: 0.2787523567676544, acc: 0.9178082346916199)
[2025-01-06 01:36:32,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:32,413][root][INFO] - Training Epoch: 7/10, step 171/574 completed (loss: 0.0035816107410937548, acc: 1.0)
[2025-01-06 01:36:32,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:32,745][root][INFO] - Training Epoch: 7/10, step 172/574 completed (loss: 0.1462947130203247, acc: 0.9259259104728699)
[2025-01-06 01:36:32,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:33,061][root][INFO] - Training Epoch: 7/10, step 173/574 completed (loss: 0.09986448287963867, acc: 0.9285714030265808)
[2025-01-06 01:36:33,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:33,618][root][INFO] - Training Epoch: 7/10, step 174/574 completed (loss: 0.4533800780773163, acc: 0.8584070801734924)
[2025-01-06 01:36:33,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:33,964][root][INFO] - Training Epoch: 7/10, step 175/574 completed (loss: 0.2868666648864746, acc: 0.9130434989929199)
[2025-01-06 01:36:34,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:34,319][root][INFO] - Training Epoch: 7/10, step 176/574 completed (loss: 0.10542468726634979, acc: 0.9772727489471436)
[2025-01-06 01:36:34,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:35,230][root][INFO] - Training Epoch: 7/10, step 177/574 completed (loss: 0.45361557602882385, acc: 0.8549618124961853)
[2025-01-06 01:36:35,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:35,910][root][INFO] - Training Epoch: 7/10, step 178/574 completed (loss: 0.3971753418445587, acc: 0.8962963223457336)
[2025-01-06 01:36:36,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:36,304][root][INFO] - Training Epoch: 7/10, step 179/574 completed (loss: 0.060721270740032196, acc: 0.9836065769195557)
[2025-01-06 01:36:36,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:36,658][root][INFO] - Training Epoch: 7/10, step 180/574 completed (loss: 0.011002481915056705, acc: 1.0)
[2025-01-06 01:36:36,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:37,004][root][INFO] - Training Epoch: 7/10, step 181/574 completed (loss: 0.0038733475375920534, acc: 1.0)
[2025-01-06 01:36:37,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:37,338][root][INFO] - Training Epoch: 7/10, step 182/574 completed (loss: 0.005207199603319168, acc: 1.0)
[2025-01-06 01:36:37,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:37,673][root][INFO] - Training Epoch: 7/10, step 183/574 completed (loss: 0.0433916300535202, acc: 0.9878048896789551)
[2025-01-06 01:36:37,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:38,033][root][INFO] - Training Epoch: 7/10, step 184/574 completed (loss: 0.23951898515224457, acc: 0.939577043056488)
[2025-01-06 01:36:38,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:38,391][root][INFO] - Training Epoch: 7/10, step 185/574 completed (loss: 0.23440389335155487, acc: 0.9250720739364624)
[2025-01-06 01:36:38,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:38,889][root][INFO] - Training Epoch: 7/10, step 186/574 completed (loss: 0.212742879986763, acc: 0.9125000238418579)
[2025-01-06 01:36:39,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:39,418][root][INFO] - Training Epoch: 7/10, step 187/574 completed (loss: 0.3399502635002136, acc: 0.9024389982223511)
[2025-01-06 01:36:39,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:39,843][root][INFO] - Training Epoch: 7/10, step 188/574 completed (loss: 0.24383263289928436, acc: 0.9252669215202332)
[2025-01-06 01:36:39,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:40,164][root][INFO] - Training Epoch: 7/10, step 189/574 completed (loss: 0.45733898878097534, acc: 0.8799999952316284)
[2025-01-06 01:36:40,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:40,730][root][INFO] - Training Epoch: 7/10, step 190/574 completed (loss: 0.25754037499427795, acc: 0.8720930218696594)
[2025-01-06 01:36:40,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:41,524][root][INFO] - Training Epoch: 7/10, step 191/574 completed (loss: 0.52273029088974, acc: 0.841269850730896)
[2025-01-06 01:36:41,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:42,441][root][INFO] - Training Epoch: 7/10, step 192/574 completed (loss: 0.46480557322502136, acc: 0.8712121248245239)
[2025-01-06 01:36:42,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:43,191][root][INFO] - Training Epoch: 7/10, step 193/574 completed (loss: 0.2984318137168884, acc: 0.929411768913269)
[2025-01-06 01:36:43,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:44,267][root][INFO] - Training Epoch: 7/10, step 194/574 completed (loss: 0.4645494520664215, acc: 0.8888888955116272)
[2025-01-06 01:36:44,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:45,221][root][INFO] - Training Epoch: 7/10, step 195/574 completed (loss: 0.1913248896598816, acc: 0.9354838728904724)
[2025-01-06 01:36:45,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:45,552][root][INFO] - Training Epoch: 7/10, step 196/574 completed (loss: 0.0295404102653265, acc: 1.0)
[2025-01-06 01:36:45,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:45,877][root][INFO] - Training Epoch: 7/10, step 197/574 completed (loss: 0.38598787784576416, acc: 0.9750000238418579)
[2025-01-06 01:36:45,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:46,209][root][INFO] - Training Epoch: 7/10, step 198/574 completed (loss: 0.19664731621742249, acc: 0.9411764740943909)
[2025-01-06 01:36:46,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:46,569][root][INFO] - Training Epoch: 7/10, step 199/574 completed (loss: 0.4027273654937744, acc: 0.875)
[2025-01-06 01:36:46,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:46,910][root][INFO] - Training Epoch: 7/10, step 200/574 completed (loss: 0.3227888345718384, acc: 0.8898305296897888)
[2025-01-06 01:36:46,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:47,246][root][INFO] - Training Epoch: 7/10, step 201/574 completed (loss: 0.35526448488235474, acc: 0.8656716346740723)
[2025-01-06 01:36:47,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:47,621][root][INFO] - Training Epoch: 7/10, step 202/574 completed (loss: 0.31312936544418335, acc: 0.893203854560852)
[2025-01-06 01:36:47,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:47,959][root][INFO] - Training Epoch: 7/10, step 203/574 completed (loss: 0.16978785395622253, acc: 0.9523809552192688)
[2025-01-06 01:36:48,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:48,285][root][INFO] - Training Epoch: 7/10, step 204/574 completed (loss: 0.0286257266998291, acc: 0.9890109896659851)
[2025-01-06 01:36:48,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:48,635][root][INFO] - Training Epoch: 7/10, step 205/574 completed (loss: 0.10138386487960815, acc: 0.9596412777900696)
[2025-01-06 01:36:48,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:49,070][root][INFO] - Training Epoch: 7/10, step 206/574 completed (loss: 0.1801345944404602, acc: 0.9488189220428467)
[2025-01-06 01:36:49,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:49,419][root][INFO] - Training Epoch: 7/10, step 207/574 completed (loss: 0.12298641353845596, acc: 0.9655172228813171)
[2025-01-06 01:36:49,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:49,797][root][INFO] - Training Epoch: 7/10, step 208/574 completed (loss: 0.19958843290805817, acc: 0.95652174949646)
[2025-01-06 01:36:49,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:50,194][root][INFO] - Training Epoch: 7/10, step 209/574 completed (loss: 0.137994185090065, acc: 0.957198441028595)
[2025-01-06 01:36:50,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:50,545][root][INFO] - Training Epoch: 7/10, step 210/574 completed (loss: 0.07592028379440308, acc: 0.967391312122345)
[2025-01-06 01:36:50,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:50,875][root][INFO] - Training Epoch: 7/10, step 211/574 completed (loss: 0.007457793224602938, acc: 1.0)
[2025-01-06 01:36:50,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:51,209][root][INFO] - Training Epoch: 7/10, step 212/574 completed (loss: 0.004288928117603064, acc: 1.0)
[2025-01-06 01:36:51,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:51,556][root][INFO] - Training Epoch: 7/10, step 213/574 completed (loss: 0.06255467981100082, acc: 0.978723406791687)
[2025-01-06 01:36:51,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:52,299][root][INFO] - Training Epoch: 7/10, step 214/574 completed (loss: 0.06998766958713531, acc: 0.9846153855323792)
[2025-01-06 01:36:52,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:52,605][root][INFO] - Training Epoch: 7/10, step 215/574 completed (loss: 0.11417503654956818, acc: 0.9729729890823364)
[2025-01-06 01:36:52,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:53,003][root][INFO] - Training Epoch: 7/10, step 216/574 completed (loss: 0.036987099796533585, acc: 0.9883720874786377)
[2025-01-06 01:36:53,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:53,536][root][INFO] - Training Epoch: 7/10, step 217/574 completed (loss: 0.12547707557678223, acc: 0.9639639854431152)
[2025-01-06 01:36:53,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:53,926][root][INFO] - Training Epoch: 7/10, step 218/574 completed (loss: 0.05314306169748306, acc: 0.9888888597488403)
[2025-01-06 01:36:54,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:54,257][root][INFO] - Training Epoch: 7/10, step 219/574 completed (loss: 0.06415921449661255, acc: 0.939393937587738)
[2025-01-06 01:36:54,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:54,572][root][INFO] - Training Epoch: 7/10, step 220/574 completed (loss: 0.0026775312144309282, acc: 1.0)
[2025-01-06 01:36:54,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:54,883][root][INFO] - Training Epoch: 7/10, step 221/574 completed (loss: 0.0027899830602109432, acc: 1.0)
[2025-01-06 01:36:54,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:55,216][root][INFO] - Training Epoch: 7/10, step 222/574 completed (loss: 0.2789788842201233, acc: 0.8653846383094788)
[2025-01-06 01:36:55,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:55,981][root][INFO] - Training Epoch: 7/10, step 223/574 completed (loss: 0.19748540222644806, acc: 0.9347826242446899)
[2025-01-06 01:36:56,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:56,519][root][INFO] - Training Epoch: 7/10, step 224/574 completed (loss: 0.24003607034683228, acc: 0.9204545617103577)
[2025-01-06 01:36:56,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:56,952][root][INFO] - Training Epoch: 7/10, step 225/574 completed (loss: 0.2836637794971466, acc: 0.914893627166748)
[2025-01-06 01:36:57,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:57,300][root][INFO] - Training Epoch: 7/10, step 226/574 completed (loss: 0.051245130598545074, acc: 1.0)
[2025-01-06 01:36:57,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:57,624][root][INFO] - Training Epoch: 7/10, step 227/574 completed (loss: 0.17994651198387146, acc: 0.949999988079071)
[2025-01-06 01:36:57,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:57,928][root][INFO] - Training Epoch: 7/10, step 228/574 completed (loss: 0.09437494724988937, acc: 0.9767441749572754)
[2025-01-06 01:36:58,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:58,256][root][INFO] - Training Epoch: 7/10, step 229/574 completed (loss: 0.16118206083774567, acc: 0.9333333373069763)
[2025-01-06 01:36:58,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:58,610][root][INFO] - Training Epoch: 7/10, step 230/574 completed (loss: 0.657036542892456, acc: 0.800000011920929)
[2025-01-06 01:36:58,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:58,905][root][INFO] - Training Epoch: 7/10, step 231/574 completed (loss: 0.38743358850479126, acc: 0.8444444537162781)
[2025-01-06 01:36:59,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:59,323][root][INFO] - Training Epoch: 7/10, step 232/574 completed (loss: 0.5330512523651123, acc: 0.8333333134651184)
[2025-01-06 01:36:59,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:59,817][root][INFO] - Training Epoch: 7/10, step 233/574 completed (loss: 0.9635981321334839, acc: 0.6972476840019226)
[2025-01-06 01:36:59,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:00,300][root][INFO] - Training Epoch: 7/10, step 234/574 completed (loss: 0.5343745946884155, acc: 0.8692307472229004)
[2025-01-06 01:37:00,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:00,668][root][INFO] - Training Epoch: 7/10, step 235/574 completed (loss: 0.008722317405045033, acc: 1.0)
[2025-01-06 01:37:00,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:00,993][root][INFO] - Training Epoch: 7/10, step 236/574 completed (loss: 0.004636658355593681, acc: 1.0)
[2025-01-06 01:37:01,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01,301][root][INFO] - Training Epoch: 7/10, step 237/574 completed (loss: 0.14544834196567535, acc: 0.9545454382896423)
[2025-01-06 01:37:01,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01,639][root][INFO] - Training Epoch: 7/10, step 238/574 completed (loss: 0.04004714637994766, acc: 1.0)
[2025-01-06 01:37:01,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01,953][root][INFO] - Training Epoch: 7/10, step 239/574 completed (loss: 0.036895908415317535, acc: 1.0)
[2025-01-06 01:37:02,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:02,296][root][INFO] - Training Epoch: 7/10, step 240/574 completed (loss: 0.22922629117965698, acc: 0.9090909361839294)
[2025-01-06 01:37:02,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:02,643][root][INFO] - Training Epoch: 7/10, step 241/574 completed (loss: 0.05580630525946617, acc: 0.9772727489471436)
[2025-01-06 01:37:02,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:03,221][root][INFO] - Training Epoch: 7/10, step 242/574 completed (loss: 0.3091917634010315, acc: 0.9032257795333862)
[2025-01-06 01:37:03,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:03,761][root][INFO] - Training Epoch: 7/10, step 243/574 completed (loss: 0.31784677505493164, acc: 0.9318181872367859)
[2025-01-06 01:37:03,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:04,098][root][INFO] - Training Epoch: 7/10, step 244/574 completed (loss: 0.0014583454467356205, acc: 1.0)
[2025-01-06 01:37:04,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:04,420][root][INFO] - Training Epoch: 7/10, step 245/574 completed (loss: 0.12459555268287659, acc: 0.9615384340286255)
[2025-01-06 01:37:04,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:04,760][root][INFO] - Training Epoch: 7/10, step 246/574 completed (loss: 0.0033767970744520426, acc: 1.0)
[2025-01-06 01:37:04,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:05,082][root][INFO] - Training Epoch: 7/10, step 247/574 completed (loss: 0.01118964422494173, acc: 1.0)
[2025-01-06 01:37:05,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:05,433][root][INFO] - Training Epoch: 7/10, step 248/574 completed (loss: 0.03641626983880997, acc: 1.0)
[2025-01-06 01:37:05,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:05,731][root][INFO] - Training Epoch: 7/10, step 249/574 completed (loss: 0.014736318960785866, acc: 1.0)
[2025-01-06 01:37:05,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:06,047][root][INFO] - Training Epoch: 7/10, step 250/574 completed (loss: 0.04697149619460106, acc: 0.9729729890823364)
[2025-01-06 01:37:06,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:06,384][root][INFO] - Training Epoch: 7/10, step 251/574 completed (loss: 0.014817750081419945, acc: 1.0)
[2025-01-06 01:37:06,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:06,735][root][INFO] - Training Epoch: 7/10, step 252/574 completed (loss: 0.011463929899036884, acc: 1.0)
[2025-01-06 01:37:06,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07,051][root][INFO] - Training Epoch: 7/10, step 253/574 completed (loss: 0.005001759622246027, acc: 1.0)
[2025-01-06 01:37:07,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07,352][root][INFO] - Training Epoch: 7/10, step 254/574 completed (loss: 0.0023169550113379955, acc: 1.0)
[2025-01-06 01:37:07,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07,676][root][INFO] - Training Epoch: 7/10, step 255/574 completed (loss: 0.0007837950834073126, acc: 1.0)
[2025-01-06 01:37:07,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07,980][root][INFO] - Training Epoch: 7/10, step 256/574 completed (loss: 0.014782757498323917, acc: 1.0)
[2025-01-06 01:37:08,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:08,283][root][INFO] - Training Epoch: 7/10, step 257/574 completed (loss: 0.08322738111019135, acc: 0.9714285731315613)
[2025-01-06 01:37:08,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:08,601][root][INFO] - Training Epoch: 7/10, step 258/574 completed (loss: 0.0216116551309824, acc: 1.0)
[2025-01-06 01:37:08,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:09,191][root][INFO] - Training Epoch: 7/10, step 259/574 completed (loss: 0.10413976013660431, acc: 0.9622641801834106)
[2025-01-06 01:37:09,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:09,777][root][INFO] - Training Epoch: 7/10, step 260/574 completed (loss: 0.14562971889972687, acc: 0.9583333134651184)
[2025-01-06 01:37:09,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:10,085][root][INFO] - Training Epoch: 7/10, step 261/574 completed (loss: 0.0020293465349823236, acc: 1.0)
[2025-01-06 01:37:10,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:10,380][root][INFO] - Training Epoch: 7/10, step 262/574 completed (loss: 0.011556466110050678, acc: 1.0)
[2025-01-06 01:37:10,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:10,721][root][INFO] - Training Epoch: 7/10, step 263/574 completed (loss: 0.2373652458190918, acc: 0.9333333373069763)
[2025-01-06 01:37:10,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:11,055][root][INFO] - Training Epoch: 7/10, step 264/574 completed (loss: 0.09172595292329788, acc: 0.9791666865348816)
[2025-01-06 01:37:11,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:11,910][root][INFO] - Training Epoch: 7/10, step 265/574 completed (loss: 0.6702795624732971, acc: 0.8240000009536743)
[2025-01-06 01:37:12,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:12,240][root][INFO] - Training Epoch: 7/10, step 266/574 completed (loss: 0.5444737076759338, acc: 0.8426966071128845)
[2025-01-06 01:37:12,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:12,577][root][INFO] - Training Epoch: 7/10, step 267/574 completed (loss: 0.1451970934867859, acc: 0.9324324131011963)
[2025-01-06 01:37:12,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13,033][root][INFO] - Training Epoch: 7/10, step 268/574 completed (loss: 0.14993537962436676, acc: 0.9482758641242981)
[2025-01-06 01:37:13,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13,323][root][INFO] - Training Epoch: 7/10, step 269/574 completed (loss: 0.0032659766729921103, acc: 1.0)
[2025-01-06 01:37:13,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13,628][root][INFO] - Training Epoch: 7/10, step 270/574 completed (loss: 0.1662442535161972, acc: 0.9545454382896423)
[2025-01-06 01:37:13,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13,924][root][INFO] - Training Epoch: 7/10, step 271/574 completed (loss: 0.01061232015490532, acc: 1.0)
[2025-01-06 01:37:14,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:14,262][root][INFO] - Training Epoch: 7/10, step 272/574 completed (loss: 0.0019970217254012823, acc: 1.0)
[2025-01-06 01:37:14,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:14,656][root][INFO] - Training Epoch: 7/10, step 273/574 completed (loss: 0.1482616364955902, acc: 0.9666666388511658)
[2025-01-06 01:37:15,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:15,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:15,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:16,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:16,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:16,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:17,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:17,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:17,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:18,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:18,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:18,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:19,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:19,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:21,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:21,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:21,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:22,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:22,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:22,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:23,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:23,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:23,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:24,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:24,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:25,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:25,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:25,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:25,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:26,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:26,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:26,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:27,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:27,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:28,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:28,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:28,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:29,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:29,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:29,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:30,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:30,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:30,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:30,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:31,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:31,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:31,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:32,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:32,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:32,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:33,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:33,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:34,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:34,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:34,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:34,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:35,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:35,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:36,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:36,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:37,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:37,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:37,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:38,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:38,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:38,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:39,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:39,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:39,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:41,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:41,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:41,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:42,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:42,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:42,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:42,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:43,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:43,853][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3881, device='cuda:0') eval_epoch_loss=tensor(0.8705, device='cuda:0') eval_epoch_acc=tensor(0.8280, device='cuda:0')
[2025-01-06 01:37:43,855][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:37:43,855][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:37:44,133][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_274_loss_0.8704822659492493/model.pt
[2025-01-06 01:37:44,156][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:37:44,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:44,560][root][INFO] - Training Epoch: 7/10, step 274/574 completed (loss: 0.038926105946302414, acc: 0.96875)
[2025-01-06 01:37:44,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:44,874][root][INFO] - Training Epoch: 7/10, step 275/574 completed (loss: 0.0010524929966777563, acc: 1.0)
[2025-01-06 01:37:44,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:45,172][root][INFO] - Training Epoch: 7/10, step 276/574 completed (loss: 0.02589508518576622, acc: 1.0)
[2025-01-06 01:37:45,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:45,492][root][INFO] - Training Epoch: 7/10, step 277/574 completed (loss: 0.007457309868186712, acc: 1.0)
[2025-01-06 01:37:45,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:45,799][root][INFO] - Training Epoch: 7/10, step 278/574 completed (loss: 0.04737091436982155, acc: 0.978723406791687)
[2025-01-06 01:37:45,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:46,139][root][INFO] - Training Epoch: 7/10, step 279/574 completed (loss: 0.13322947919368744, acc: 0.9583333134651184)
[2025-01-06 01:37:46,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:46,455][root][INFO] - Training Epoch: 7/10, step 280/574 completed (loss: 0.010564812459051609, acc: 1.0)
[2025-01-06 01:37:46,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:46,894][root][INFO] - Training Epoch: 7/10, step 281/574 completed (loss: 0.1746392697095871, acc: 0.9518072009086609)
[2025-01-06 01:37:46,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:47,264][root][INFO] - Training Epoch: 7/10, step 282/574 completed (loss: 0.4492625594139099, acc: 0.8703703880310059)
[2025-01-06 01:37:47,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:47,598][root][INFO] - Training Epoch: 7/10, step 283/574 completed (loss: 0.011017757467925549, acc: 1.0)
[2025-01-06 01:37:47,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:47,901][root][INFO] - Training Epoch: 7/10, step 284/574 completed (loss: 0.015371195040643215, acc: 1.0)
[2025-01-06 01:37:47,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:48,213][root][INFO] - Training Epoch: 7/10, step 285/574 completed (loss: 0.09239808470010757, acc: 0.9750000238418579)
[2025-01-06 01:37:48,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:48,526][root][INFO] - Training Epoch: 7/10, step 286/574 completed (loss: 0.11584127694368362, acc: 0.96875)
[2025-01-06 01:37:48,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:48,885][root][INFO] - Training Epoch: 7/10, step 287/574 completed (loss: 0.09765751659870148, acc: 0.9679999947547913)
[2025-01-06 01:37:48,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:49,228][root][INFO] - Training Epoch: 7/10, step 288/574 completed (loss: 0.06875767558813095, acc: 0.9890109896659851)
[2025-01-06 01:37:49,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:49,596][root][INFO] - Training Epoch: 7/10, step 289/574 completed (loss: 0.0610881932079792, acc: 0.9875776171684265)
[2025-01-06 01:37:49,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:50,011][root][INFO] - Training Epoch: 7/10, step 290/574 completed (loss: 0.18814000487327576, acc: 0.9226804375648499)
[2025-01-06 01:37:50,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:50,344][root][INFO] - Training Epoch: 7/10, step 291/574 completed (loss: 0.02668800950050354, acc: 1.0)
[2025-01-06 01:37:50,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:50,680][root][INFO] - Training Epoch: 7/10, step 292/574 completed (loss: 0.09542439132928848, acc: 0.976190447807312)
[2025-01-06 01:37:50,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:51,091][root][INFO] - Training Epoch: 7/10, step 293/574 completed (loss: 0.06358332931995392, acc: 0.982758641242981)
[2025-01-06 01:37:51,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:51,579][root][INFO] - Training Epoch: 7/10, step 294/574 completed (loss: 0.2334928661584854, acc: 0.9090909361839294)
[2025-01-06 01:37:51,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:52,147][root][INFO] - Training Epoch: 7/10, step 295/574 completed (loss: 0.18453122675418854, acc: 0.9329897165298462)
[2025-01-06 01:37:52,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:52,453][root][INFO] - Training Epoch: 7/10, step 296/574 completed (loss: 0.04105153679847717, acc: 1.0)
[2025-01-06 01:37:52,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:52,765][root][INFO] - Training Epoch: 7/10, step 297/574 completed (loss: 0.3157653510570526, acc: 0.9629629850387573)
[2025-01-06 01:37:52,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:53,074][root][INFO] - Training Epoch: 7/10, step 298/574 completed (loss: 0.023686086758971214, acc: 1.0)
[2025-01-06 01:37:53,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:53,412][root][INFO] - Training Epoch: 7/10, step 299/574 completed (loss: 0.0030399058014154434, acc: 1.0)
[2025-01-06 01:37:53,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:53,722][root][INFO] - Training Epoch: 7/10, step 300/574 completed (loss: 0.008016648702323437, acc: 1.0)
[2025-01-06 01:37:53,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:54,034][root][INFO] - Training Epoch: 7/10, step 301/574 completed (loss: 0.08740837872028351, acc: 0.9811320900917053)
[2025-01-06 01:37:54,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:54,365][root][INFO] - Training Epoch: 7/10, step 302/574 completed (loss: 0.016789529472589493, acc: 1.0)
[2025-01-06 01:37:54,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:54,696][root][INFO] - Training Epoch: 7/10, step 303/574 completed (loss: 0.011139913462102413, acc: 1.0)
[2025-01-06 01:37:54,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:55,007][root][INFO] - Training Epoch: 7/10, step 304/574 completed (loss: 0.02584584802389145, acc: 1.0)
[2025-01-06 01:37:55,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:55,388][root][INFO] - Training Epoch: 7/10, step 305/574 completed (loss: 0.04470890760421753, acc: 0.9836065769195557)
[2025-01-06 01:37:55,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:55,711][root][INFO] - Training Epoch: 7/10, step 306/574 completed (loss: 0.01399324182420969, acc: 1.0)
[2025-01-06 01:37:55,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:56,088][root][INFO] - Training Epoch: 7/10, step 307/574 completed (loss: 0.0005150206270627677, acc: 1.0)
[2025-01-06 01:37:56,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:56,460][root][INFO] - Training Epoch: 7/10, step 308/574 completed (loss: 0.11644606292247772, acc: 0.9710144996643066)
[2025-01-06 01:37:56,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:56,882][root][INFO] - Training Epoch: 7/10, step 309/574 completed (loss: 0.06772691011428833, acc: 0.9722222089767456)
[2025-01-06 01:37:56,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:57,255][root][INFO] - Training Epoch: 7/10, step 310/574 completed (loss: 0.03097519837319851, acc: 1.0)
[2025-01-06 01:37:57,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:57,626][root][INFO] - Training Epoch: 7/10, step 311/574 completed (loss: 0.09556501358747482, acc: 0.9615384340286255)
[2025-01-06 01:37:57,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:57,973][root][INFO] - Training Epoch: 7/10, step 312/574 completed (loss: 0.08701283484697342, acc: 0.9591836929321289)
[2025-01-06 01:37:58,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:58,295][root][INFO] - Training Epoch: 7/10, step 313/574 completed (loss: 0.001414785161614418, acc: 1.0)
[2025-01-06 01:37:58,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:58,678][root][INFO] - Training Epoch: 7/10, step 314/574 completed (loss: 0.012248971499502659, acc: 1.0)
[2025-01-06 01:37:58,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:59,043][root][INFO] - Training Epoch: 7/10, step 315/574 completed (loss: 0.361261248588562, acc: 0.9032257795333862)
[2025-01-06 01:37:59,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:59,338][root][INFO] - Training Epoch: 7/10, step 316/574 completed (loss: 0.3475467264652252, acc: 0.9354838728904724)
[2025-01-06 01:37:59,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:59,672][root][INFO] - Training Epoch: 7/10, step 317/574 completed (loss: 0.020893925800919533, acc: 1.0)
[2025-01-06 01:37:59,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:00,035][root][INFO] - Training Epoch: 7/10, step 318/574 completed (loss: 0.03055967204272747, acc: 0.9903846383094788)
[2025-01-06 01:38:00,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:00,363][root][INFO] - Training Epoch: 7/10, step 319/574 completed (loss: 0.003565158462151885, acc: 1.0)
[2025-01-06 01:38:00,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:00,668][root][INFO] - Training Epoch: 7/10, step 320/574 completed (loss: 0.015774080529808998, acc: 1.0)
[2025-01-06 01:38:00,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:01,034][root][INFO] - Training Epoch: 7/10, step 321/574 completed (loss: 0.0027496751863509417, acc: 1.0)
[2025-01-06 01:38:01,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:01,382][root][INFO] - Training Epoch: 7/10, step 322/574 completed (loss: 0.5074787735939026, acc: 0.8148148059844971)
[2025-01-06 01:38:01,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:01,721][root][INFO] - Training Epoch: 7/10, step 323/574 completed (loss: 0.16885706782341003, acc: 0.9714285731315613)
[2025-01-06 01:38:01,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:02,057][root][INFO] - Training Epoch: 7/10, step 324/574 completed (loss: 0.38449743390083313, acc: 0.8717948794364929)
[2025-01-06 01:38:02,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:02,433][root][INFO] - Training Epoch: 7/10, step 325/574 completed (loss: 0.25736886262893677, acc: 0.9024389982223511)
[2025-01-06 01:38:02,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:02,767][root][INFO] - Training Epoch: 7/10, step 326/574 completed (loss: 0.4639725387096405, acc: 0.8421052694320679)
[2025-01-06 01:38:02,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:03,091][root][INFO] - Training Epoch: 7/10, step 327/574 completed (loss: 0.00892880093306303, acc: 1.0)
[2025-01-06 01:38:03,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:03,425][root][INFO] - Training Epoch: 7/10, step 328/574 completed (loss: 0.0028261446859687567, acc: 1.0)
[2025-01-06 01:38:03,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:03,743][root][INFO] - Training Epoch: 7/10, step 329/574 completed (loss: 0.006417158525437117, acc: 1.0)
[2025-01-06 01:38:03,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:04,058][root][INFO] - Training Epoch: 7/10, step 330/574 completed (loss: 0.006465176120400429, acc: 1.0)
[2025-01-06 01:38:04,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:04,384][root][INFO] - Training Epoch: 7/10, step 331/574 completed (loss: 0.06879011541604996, acc: 0.9838709831237793)
[2025-01-06 01:38:04,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:04,740][root][INFO] - Training Epoch: 7/10, step 332/574 completed (loss: 0.036559686064720154, acc: 0.9824561476707458)
[2025-01-06 01:38:04,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:05,083][root][INFO] - Training Epoch: 7/10, step 333/574 completed (loss: 0.2405463010072708, acc: 0.9375)
[2025-01-06 01:38:05,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:05,426][root][INFO] - Training Epoch: 7/10, step 334/574 completed (loss: 0.025650683790445328, acc: 1.0)
[2025-01-06 01:38:05,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:05,729][root][INFO] - Training Epoch: 7/10, step 335/574 completed (loss: 0.006090047303587198, acc: 1.0)
[2025-01-06 01:38:05,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:06,064][root][INFO] - Training Epoch: 7/10, step 336/574 completed (loss: 0.15225572884082794, acc: 0.9399999976158142)
[2025-01-06 01:38:06,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:06,428][root][INFO] - Training Epoch: 7/10, step 337/574 completed (loss: 0.37422534823417664, acc: 0.8965517282485962)
[2025-01-06 01:38:06,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:06,758][root][INFO] - Training Epoch: 7/10, step 338/574 completed (loss: 0.43233174085617065, acc: 0.8297872543334961)
[2025-01-06 01:38:06,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:07,106][root][INFO] - Training Epoch: 7/10, step 339/574 completed (loss: 0.29486650228500366, acc: 0.9036144614219666)
[2025-01-06 01:38:07,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:07,451][root][INFO] - Training Epoch: 7/10, step 340/574 completed (loss: 0.008973948657512665, acc: 1.0)
[2025-01-06 01:38:07,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:07,756][root][INFO] - Training Epoch: 7/10, step 341/574 completed (loss: 0.008379094302654266, acc: 1.0)
[2025-01-06 01:38:07,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:08,078][root][INFO] - Training Epoch: 7/10, step 342/574 completed (loss: 0.08712157607078552, acc: 0.9879518151283264)
[2025-01-06 01:38:08,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:08,417][root][INFO] - Training Epoch: 7/10, step 343/574 completed (loss: 0.2479565441608429, acc: 0.9245283007621765)
[2025-01-06 01:38:08,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:08,732][root][INFO] - Training Epoch: 7/10, step 344/574 completed (loss: 0.03701899200677872, acc: 0.9873417615890503)
[2025-01-06 01:38:08,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:09,045][root][INFO] - Training Epoch: 7/10, step 345/574 completed (loss: 0.0016885651275515556, acc: 1.0)
[2025-01-06 01:38:09,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:09,343][root][INFO] - Training Epoch: 7/10, step 346/574 completed (loss: 0.11466087400913239, acc: 0.9701492786407471)
[2025-01-06 01:38:09,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:09,662][root][INFO] - Training Epoch: 7/10, step 347/574 completed (loss: 0.0015352300833910704, acc: 1.0)
[2025-01-06 01:38:09,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:09,973][root][INFO] - Training Epoch: 7/10, step 348/574 completed (loss: 0.0277949720621109, acc: 1.0)
[2025-01-06 01:38:10,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:10,371][root][INFO] - Training Epoch: 7/10, step 349/574 completed (loss: 0.29039114713668823, acc: 0.9166666865348816)
[2025-01-06 01:38:10,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:10,683][root][INFO] - Training Epoch: 7/10, step 350/574 completed (loss: 0.12747101485729218, acc: 0.9534883499145508)
[2025-01-06 01:38:10,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:11,011][root][INFO] - Training Epoch: 7/10, step 351/574 completed (loss: 0.023831067606806755, acc: 1.0)
[2025-01-06 01:38:11,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:11,383][root][INFO] - Training Epoch: 7/10, step 352/574 completed (loss: 0.23864910006523132, acc: 0.8888888955116272)
[2025-01-06 01:38:11,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:11,673][root][INFO] - Training Epoch: 7/10, step 353/574 completed (loss: 0.008806436322629452, acc: 1.0)
[2025-01-06 01:38:11,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:12,042][root][INFO] - Training Epoch: 7/10, step 354/574 completed (loss: 0.024983061477541924, acc: 1.0)
[2025-01-06 01:38:12,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:12,416][root][INFO] - Training Epoch: 7/10, step 355/574 completed (loss: 0.24193094670772552, acc: 0.901098906993866)
[2025-01-06 01:38:12,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:12,948][root][INFO] - Training Epoch: 7/10, step 356/574 completed (loss: 0.17892558872699738, acc: 0.947826087474823)
[2025-01-06 01:38:13,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:13,342][root][INFO] - Training Epoch: 7/10, step 357/574 completed (loss: 0.12078475207090378, acc: 0.9347826242446899)
[2025-01-06 01:38:13,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:13,706][root][INFO] - Training Epoch: 7/10, step 358/574 completed (loss: 0.09984544664621353, acc: 0.9591836929321289)
[2025-01-06 01:38:13,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14,018][root][INFO] - Training Epoch: 7/10, step 359/574 completed (loss: 0.0020606820471584797, acc: 1.0)
[2025-01-06 01:38:14,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14,320][root][INFO] - Training Epoch: 7/10, step 360/574 completed (loss: 0.0119114238768816, acc: 1.0)
[2025-01-06 01:38:14,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14,607][root][INFO] - Training Epoch: 7/10, step 361/574 completed (loss: 0.03795052319765091, acc: 1.0)
[2025-01-06 01:38:14,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14,928][root][INFO] - Training Epoch: 7/10, step 362/574 completed (loss: 0.13548098504543304, acc: 0.9555555582046509)
[2025-01-06 01:38:15,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:15,327][root][INFO] - Training Epoch: 7/10, step 363/574 completed (loss: 0.01574571244418621, acc: 1.0)
[2025-01-06 01:38:15,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:15,707][root][INFO] - Training Epoch: 7/10, step 364/574 completed (loss: 0.005388016812503338, acc: 1.0)
[2025-01-06 01:38:15,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:16,035][root][INFO] - Training Epoch: 7/10, step 365/574 completed (loss: 0.013072693720459938, acc: 1.0)
[2025-01-06 01:38:16,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:16,399][root][INFO] - Training Epoch: 7/10, step 366/574 completed (loss: 0.00023374044394586235, acc: 1.0)
[2025-01-06 01:38:16,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:16,743][root][INFO] - Training Epoch: 7/10, step 367/574 completed (loss: 0.0016938933404162526, acc: 1.0)
[2025-01-06 01:38:16,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:17,130][root][INFO] - Training Epoch: 7/10, step 368/574 completed (loss: 0.03158440440893173, acc: 0.9642857313156128)
[2025-01-06 01:38:17,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:17,437][root][INFO] - Training Epoch: 7/10, step 369/574 completed (loss: 0.046533919870853424, acc: 0.96875)
[2025-01-06 01:38:17,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:18,067][root][INFO] - Training Epoch: 7/10, step 370/574 completed (loss: 0.27015307545661926, acc: 0.939393937587738)
[2025-01-06 01:38:18,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:18,934][root][INFO] - Training Epoch: 7/10, step 371/574 completed (loss: 0.07194836437702179, acc: 0.9622641801834106)
[2025-01-06 01:38:19,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:19,291][root][INFO] - Training Epoch: 7/10, step 372/574 completed (loss: 0.1067931056022644, acc: 0.9777777791023254)
[2025-01-06 01:38:19,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:19,622][root][INFO] - Training Epoch: 7/10, step 373/574 completed (loss: 0.047390520572662354, acc: 0.9821428656578064)
[2025-01-06 01:38:19,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:19,939][root][INFO] - Training Epoch: 7/10, step 374/574 completed (loss: 0.03402157872915268, acc: 0.9714285731315613)
[2025-01-06 01:38:20,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:20,234][root][INFO] - Training Epoch: 7/10, step 375/574 completed (loss: 0.0015509299701079726, acc: 1.0)
[2025-01-06 01:38:20,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:20,535][root][INFO] - Training Epoch: 7/10, step 376/574 completed (loss: 0.005480203311890364, acc: 1.0)
[2025-01-06 01:38:20,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:20,810][root][INFO] - Training Epoch: 7/10, step 377/574 completed (loss: 0.012700878083705902, acc: 1.0)
[2025-01-06 01:38:20,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:21,135][root][INFO] - Training Epoch: 7/10, step 378/574 completed (loss: 0.004921074956655502, acc: 1.0)
[2025-01-06 01:38:21,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:21,758][root][INFO] - Training Epoch: 7/10, step 379/574 completed (loss: 0.08896220475435257, acc: 0.9640718698501587)
[2025-01-06 01:38:21,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:22,261][root][INFO] - Training Epoch: 7/10, step 380/574 completed (loss: 0.17268703877925873, acc: 0.9624060392379761)
[2025-01-06 01:38:22,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:23,393][root][INFO] - Training Epoch: 7/10, step 381/574 completed (loss: 0.2675914466381073, acc: 0.9251337051391602)
[2025-01-06 01:38:23,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:23,999][root][INFO] - Training Epoch: 7/10, step 382/574 completed (loss: 0.05664496496319771, acc: 0.9729729890823364)
[2025-01-06 01:38:24,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:24,338][root][INFO] - Training Epoch: 7/10, step 383/574 completed (loss: 0.15299741923809052, acc: 0.9642857313156128)
[2025-01-06 01:38:24,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:24,736][root][INFO] - Training Epoch: 7/10, step 384/574 completed (loss: 0.0007182666449807584, acc: 1.0)
[2025-01-06 01:38:24,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:25,072][root][INFO] - Training Epoch: 7/10, step 385/574 completed (loss: 0.0016329745994880795, acc: 1.0)
[2025-01-06 01:38:25,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:25,384][root][INFO] - Training Epoch: 7/10, step 386/574 completed (loss: 0.001223264029249549, acc: 1.0)
[2025-01-06 01:38:25,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:25,685][root][INFO] - Training Epoch: 7/10, step 387/574 completed (loss: 0.0014239007141441107, acc: 1.0)
[2025-01-06 01:38:25,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:26,058][root][INFO] - Training Epoch: 7/10, step 388/574 completed (loss: 0.00042504406883381307, acc: 1.0)
[2025-01-06 01:38:26,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:26,499][root][INFO] - Training Epoch: 7/10, step 389/574 completed (loss: 0.029691431671380997, acc: 1.0)
[2025-01-06 01:38:26,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:26,845][root][INFO] - Training Epoch: 7/10, step 390/574 completed (loss: 0.38588082790374756, acc: 0.9523809552192688)
[2025-01-06 01:38:26,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:27,181][root][INFO] - Training Epoch: 7/10, step 391/574 completed (loss: 0.20574434101581573, acc: 0.9444444179534912)
[2025-01-06 01:38:27,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:27,518][root][INFO] - Training Epoch: 7/10, step 392/574 completed (loss: 0.34017762541770935, acc: 0.8640776872634888)
[2025-01-06 01:38:27,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:28,069][root][INFO] - Training Epoch: 7/10, step 393/574 completed (loss: 0.34874799847602844, acc: 0.8897058963775635)
[2025-01-06 01:38:28,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:28,447][root][INFO] - Training Epoch: 7/10, step 394/574 completed (loss: 0.21154899895191193, acc: 0.9266666769981384)
[2025-01-06 01:38:28,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:28,839][root][INFO] - Training Epoch: 7/10, step 395/574 completed (loss: 0.25214850902557373, acc: 0.9375)
[2025-01-06 01:38:28,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:29,197][root][INFO] - Training Epoch: 7/10, step 396/574 completed (loss: 0.0174713134765625, acc: 1.0)
[2025-01-06 01:38:29,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:29,506][root][INFO] - Training Epoch: 7/10, step 397/574 completed (loss: 0.18801690638065338, acc: 0.9583333134651184)
[2025-01-06 01:38:29,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:29,855][root][INFO] - Training Epoch: 7/10, step 398/574 completed (loss: 0.1383812427520752, acc: 0.9534883499145508)
[2025-01-06 01:38:29,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:30,177][root][INFO] - Training Epoch: 7/10, step 399/574 completed (loss: 0.011574434116482735, acc: 1.0)
[2025-01-06 01:38:30,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:30,736][root][INFO] - Training Epoch: 7/10, step 400/574 completed (loss: 0.061109669506549835, acc: 0.9852941036224365)
[2025-01-06 01:38:30,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:31,103][root][INFO] - Training Epoch: 7/10, step 401/574 completed (loss: 0.051311150193214417, acc: 0.9866666793823242)
[2025-01-06 01:38:31,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:31,474][root][INFO] - Training Epoch: 7/10, step 402/574 completed (loss: 0.00777143519371748, acc: 1.0)
[2025-01-06 01:38:31,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:31,805][root][INFO] - Training Epoch: 7/10, step 403/574 completed (loss: 0.27552974224090576, acc: 0.939393937587738)
[2025-01-06 01:38:31,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:32,196][root][INFO] - Training Epoch: 7/10, step 404/574 completed (loss: 0.007743990980088711, acc: 1.0)
[2025-01-06 01:38:32,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:32,581][root][INFO] - Training Epoch: 7/10, step 405/574 completed (loss: 0.0006737704388797283, acc: 1.0)
[2025-01-06 01:38:32,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:32,912][root][INFO] - Training Epoch: 7/10, step 406/574 completed (loss: 0.0826185867190361, acc: 0.9599999785423279)
[2025-01-06 01:38:33,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:33,234][root][INFO] - Training Epoch: 7/10, step 407/574 completed (loss: 0.002069121226668358, acc: 1.0)
[2025-01-06 01:38:33,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:33,540][root][INFO] - Training Epoch: 7/10, step 408/574 completed (loss: 0.04401325434446335, acc: 1.0)
[2025-01-06 01:38:33,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:33,892][root][INFO] - Training Epoch: 7/10, step 409/574 completed (loss: 0.013003666885197163, acc: 1.0)
[2025-01-06 01:38:34,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:34,272][root][INFO] - Training Epoch: 7/10, step 410/574 completed (loss: 0.01118084229528904, acc: 1.0)
[2025-01-06 01:38:34,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:34,652][root][INFO] - Training Epoch: 7/10, step 411/574 completed (loss: 0.05120069533586502, acc: 0.9642857313156128)
[2025-01-06 01:38:34,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:34,994][root][INFO] - Training Epoch: 7/10, step 412/574 completed (loss: 0.0367853008210659, acc: 0.9666666388511658)
[2025-01-06 01:38:35,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:35,301][root][INFO] - Training Epoch: 7/10, step 413/574 completed (loss: 0.17997856438159943, acc: 0.939393937587738)
[2025-01-06 01:38:35,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:35,612][root][INFO] - Training Epoch: 7/10, step 414/574 completed (loss: 0.18947868049144745, acc: 0.9090909361839294)
[2025-01-06 01:38:35,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:35,926][root][INFO] - Training Epoch: 7/10, step 415/574 completed (loss: 0.040370356291532516, acc: 0.9803921580314636)
[2025-01-06 01:38:36,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:36,222][root][INFO] - Training Epoch: 7/10, step 416/574 completed (loss: 0.015810783952474594, acc: 1.0)
[2025-01-06 01:38:36,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:37,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:37,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:37,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:38,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:38,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:38,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:39,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:39,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:39,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:40,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:40,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:40,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:41,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:41,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:42,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:42,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:42,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:43,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:43,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:44,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:44,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:44,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:45,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:45,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:45,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:46,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:46,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:47,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:47,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:47,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:48,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:48,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:48,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:49,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:49,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:49,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:50,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:50,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:50,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:51,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:51,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:51,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:52,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:52,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:52,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:53,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:53,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:53,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:54,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:54,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:55,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:55,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:55,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:56,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:56,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:56,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:57,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:57,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:57,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:58,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:58,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:58,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:59,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:59,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:59,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:00,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:00,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:00,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:01,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:01,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:02,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:02,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:02,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:03,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:03,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:03,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:04,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:04,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:04,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:05,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:05,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:05,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:05,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:06,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:07,062][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2687, device='cuda:0') eval_epoch_loss=tensor(0.8192, device='cuda:0') eval_epoch_acc=tensor(0.8242, device='cuda:0')
[2025-01-06 01:39:07,064][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:39:07,065][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:39:07,411][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_417_loss_0.8192003965377808/model.pt
[2025-01-06 01:39:07,433][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:39:07,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:07,920][root][INFO] - Training Epoch: 7/10, step 417/574 completed (loss: 0.02430754154920578, acc: 1.0)
[2025-01-06 01:39:08,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:08,267][root][INFO] - Training Epoch: 7/10, step 418/574 completed (loss: 0.05182063579559326, acc: 0.9750000238418579)
[2025-01-06 01:39:08,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:08,652][root][INFO] - Training Epoch: 7/10, step 419/574 completed (loss: 0.0027610096149146557, acc: 1.0)
[2025-01-06 01:39:08,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:08,971][root][INFO] - Training Epoch: 7/10, step 420/574 completed (loss: 0.03377407416701317, acc: 1.0)
[2025-01-06 01:39:09,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:09,277][root][INFO] - Training Epoch: 7/10, step 421/574 completed (loss: 0.08945906907320023, acc: 0.9333333373069763)
[2025-01-06 01:39:09,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:09,579][root][INFO] - Training Epoch: 7/10, step 422/574 completed (loss: 0.025877060368657112, acc: 1.0)
[2025-01-06 01:39:09,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:09,909][root][INFO] - Training Epoch: 7/10, step 423/574 completed (loss: 0.05723774433135986, acc: 0.9722222089767456)
[2025-01-06 01:39:09,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:10,225][root][INFO] - Training Epoch: 7/10, step 424/574 completed (loss: 0.006575744599103928, acc: 1.0)
[2025-01-06 01:39:10,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:10,546][root][INFO] - Training Epoch: 7/10, step 425/574 completed (loss: 0.00894590001553297, acc: 1.0)
[2025-01-06 01:39:10,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:10,853][root][INFO] - Training Epoch: 7/10, step 426/574 completed (loss: 0.0014888247242197394, acc: 1.0)
[2025-01-06 01:39:10,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:11,170][root][INFO] - Training Epoch: 7/10, step 427/574 completed (loss: 0.056365907192230225, acc: 0.9729729890823364)
[2025-01-06 01:39:11,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:11,480][root][INFO] - Training Epoch: 7/10, step 428/574 completed (loss: 0.07487208396196365, acc: 0.9629629850387573)
[2025-01-06 01:39:11,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:11,814][root][INFO] - Training Epoch: 7/10, step 429/574 completed (loss: 0.055715981870889664, acc: 0.95652174949646)
[2025-01-06 01:39:11,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:12,183][root][INFO] - Training Epoch: 7/10, step 430/574 completed (loss: 0.00014396915503311902, acc: 1.0)
[2025-01-06 01:39:12,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:12,526][root][INFO] - Training Epoch: 7/10, step 431/574 completed (loss: 0.0009150683763436973, acc: 1.0)
[2025-01-06 01:39:12,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:12,876][root][INFO] - Training Epoch: 7/10, step 432/574 completed (loss: 0.0015245031099766493, acc: 1.0)
[2025-01-06 01:39:12,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:13,252][root][INFO] - Training Epoch: 7/10, step 433/574 completed (loss: 0.07178317755460739, acc: 0.9722222089767456)
[2025-01-06 01:39:13,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:13,567][root][INFO] - Training Epoch: 7/10, step 434/574 completed (loss: 0.0011615519179031253, acc: 1.0)
[2025-01-06 01:39:13,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:13,918][root][INFO] - Training Epoch: 7/10, step 435/574 completed (loss: 0.022792406380176544, acc: 0.9696969985961914)
[2025-01-06 01:39:14,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:14,265][root][INFO] - Training Epoch: 7/10, step 436/574 completed (loss: 0.0028675785288214684, acc: 1.0)
[2025-01-06 01:39:14,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:14,632][root][INFO] - Training Epoch: 7/10, step 437/574 completed (loss: 0.010672952979803085, acc: 1.0)
[2025-01-06 01:39:14,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:14,980][root][INFO] - Training Epoch: 7/10, step 438/574 completed (loss: 0.0019292045617476106, acc: 1.0)
[2025-01-06 01:39:15,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:15,323][root][INFO] - Training Epoch: 7/10, step 439/574 completed (loss: 0.14076213538646698, acc: 0.9487179517745972)
[2025-01-06 01:39:15,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:15,835][root][INFO] - Training Epoch: 7/10, step 440/574 completed (loss: 0.04261541739106178, acc: 0.9848484992980957)
[2025-01-06 01:39:16,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:16,560][root][INFO] - Training Epoch: 7/10, step 441/574 completed (loss: 0.43896010518074036, acc: 0.8799999952316284)
[2025-01-06 01:39:16,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:16,971][root][INFO] - Training Epoch: 7/10, step 442/574 completed (loss: 0.2338228076696396, acc: 0.9354838728904724)
[2025-01-06 01:39:17,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:17,626][root][INFO] - Training Epoch: 7/10, step 443/574 completed (loss: 0.18823586404323578, acc: 0.9402984976768494)
[2025-01-06 01:39:17,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:17,944][root][INFO] - Training Epoch: 7/10, step 444/574 completed (loss: 0.06557556986808777, acc: 0.9811320900917053)
[2025-01-06 01:39:18,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:18,386][root][INFO] - Training Epoch: 7/10, step 445/574 completed (loss: 0.04324403032660484, acc: 0.9772727489471436)
[2025-01-06 01:39:18,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:18,665][root][INFO] - Training Epoch: 7/10, step 446/574 completed (loss: 0.016078801825642586, acc: 1.0)
[2025-01-06 01:39:18,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:18,950][root][INFO] - Training Epoch: 7/10, step 447/574 completed (loss: 0.15601414442062378, acc: 0.9615384340286255)
[2025-01-06 01:39:19,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:19,232][root][INFO] - Training Epoch: 7/10, step 448/574 completed (loss: 0.07478725165128708, acc: 0.9642857313156128)
[2025-01-06 01:39:19,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:19,535][root][INFO] - Training Epoch: 7/10, step 449/574 completed (loss: 0.010830319486558437, acc: 1.0)
[2025-01-06 01:39:19,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:19,891][root][INFO] - Training Epoch: 7/10, step 450/574 completed (loss: 0.010986610315740108, acc: 1.0)
[2025-01-06 01:39:19,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:20,192][root][INFO] - Training Epoch: 7/10, step 451/574 completed (loss: 0.008812569081783295, acc: 1.0)
[2025-01-06 01:39:20,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:20,510][root][INFO] - Training Epoch: 7/10, step 452/574 completed (loss: 0.06623559445142746, acc: 0.9743589758872986)
[2025-01-06 01:39:20,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:20,837][root][INFO] - Training Epoch: 7/10, step 453/574 completed (loss: 0.1445326954126358, acc: 0.9605262875556946)
[2025-01-06 01:39:20,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:21,191][root][INFO] - Training Epoch: 7/10, step 454/574 completed (loss: 0.012575823813676834, acc: 1.0)
[2025-01-06 01:39:21,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:21,603][root][INFO] - Training Epoch: 7/10, step 455/574 completed (loss: 0.012621162459254265, acc: 1.0)
[2025-01-06 01:39:21,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:22,000][root][INFO] - Training Epoch: 7/10, step 456/574 completed (loss: 0.09823998808860779, acc: 0.9587628841400146)
[2025-01-06 01:39:22,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:22,358][root][INFO] - Training Epoch: 7/10, step 457/574 completed (loss: 0.06028330698609352, acc: 0.9857142567634583)
[2025-01-06 01:39:22,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:22,777][root][INFO] - Training Epoch: 7/10, step 458/574 completed (loss: 0.11305248737335205, acc: 0.9825581312179565)
[2025-01-06 01:39:22,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:23,129][root][INFO] - Training Epoch: 7/10, step 459/574 completed (loss: 0.021173149347305298, acc: 1.0)
[2025-01-06 01:39:23,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:23,531][root][INFO] - Training Epoch: 7/10, step 460/574 completed (loss: 0.027409465983510017, acc: 0.9876543283462524)
[2025-01-06 01:39:23,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:23,886][root][INFO] - Training Epoch: 7/10, step 461/574 completed (loss: 0.03905295580625534, acc: 1.0)
[2025-01-06 01:39:23,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:24,211][root][INFO] - Training Epoch: 7/10, step 462/574 completed (loss: 0.02136213146150112, acc: 1.0)
[2025-01-06 01:39:24,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:24,520][root][INFO] - Training Epoch: 7/10, step 463/574 completed (loss: 0.01988237164914608, acc: 1.0)
[2025-01-06 01:39:24,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:24,894][root][INFO] - Training Epoch: 7/10, step 464/574 completed (loss: 0.03485110029578209, acc: 0.97826087474823)
[2025-01-06 01:39:25,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:25,269][root][INFO] - Training Epoch: 7/10, step 465/574 completed (loss: 0.07535403221845627, acc: 0.976190447807312)
[2025-01-06 01:39:25,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:25,670][root][INFO] - Training Epoch: 7/10, step 466/574 completed (loss: 0.2803567051887512, acc: 0.9397590160369873)
[2025-01-06 01:39:25,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:26,055][root][INFO] - Training Epoch: 7/10, step 467/574 completed (loss: 0.027633678168058395, acc: 0.9909909963607788)
[2025-01-06 01:39:26,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:26,418][root][INFO] - Training Epoch: 7/10, step 468/574 completed (loss: 0.0982569232583046, acc: 0.9611650705337524)
[2025-01-06 01:39:26,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:26,742][root][INFO] - Training Epoch: 7/10, step 469/574 completed (loss: 0.16119255125522614, acc: 0.9674796462059021)
[2025-01-06 01:39:26,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:27,036][root][INFO] - Training Epoch: 7/10, step 470/574 completed (loss: 0.0037954242434352636, acc: 1.0)
[2025-01-06 01:39:27,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:27,338][root][INFO] - Training Epoch: 7/10, step 471/574 completed (loss: 0.019257059320807457, acc: 1.0)
[2025-01-06 01:39:27,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:27,761][root][INFO] - Training Epoch: 7/10, step 472/574 completed (loss: 0.22895070910453796, acc: 0.9313725233078003)
[2025-01-06 01:39:27,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:28,111][root][INFO] - Training Epoch: 7/10, step 473/574 completed (loss: 0.41869401931762695, acc: 0.8689956068992615)
[2025-01-06 01:39:28,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:28,408][root][INFO] - Training Epoch: 7/10, step 474/574 completed (loss: 0.13754205405712128, acc: 0.9479166865348816)
[2025-01-06 01:39:28,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:28,714][root][INFO] - Training Epoch: 7/10, step 475/574 completed (loss: 0.10049731284379959, acc: 0.9693251252174377)
[2025-01-06 01:39:28,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:29,022][root][INFO] - Training Epoch: 7/10, step 476/574 completed (loss: 0.1706475019454956, acc: 0.9496402740478516)
[2025-01-06 01:39:29,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:29,358][root][INFO] - Training Epoch: 7/10, step 477/574 completed (loss: 0.3163467049598694, acc: 0.8844221234321594)
[2025-01-06 01:39:29,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:29,727][root][INFO] - Training Epoch: 7/10, step 478/574 completed (loss: 0.06443988531827927, acc: 0.9722222089767456)
[2025-01-06 01:39:29,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:30,074][root][INFO] - Training Epoch: 7/10, step 479/574 completed (loss: 0.12623506784439087, acc: 0.939393937587738)
[2025-01-06 01:39:30,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:30,386][root][INFO] - Training Epoch: 7/10, step 480/574 completed (loss: 0.05278433859348297, acc: 1.0)
[2025-01-06 01:39:30,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:30,721][root][INFO] - Training Epoch: 7/10, step 481/574 completed (loss: 0.11466874182224274, acc: 0.949999988079071)
[2025-01-06 01:39:30,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:31,031][root][INFO] - Training Epoch: 7/10, step 482/574 completed (loss: 0.011088071390986443, acc: 1.0)
[2025-01-06 01:39:31,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:31,425][root][INFO] - Training Epoch: 7/10, step 483/574 completed (loss: 0.12186878174543381, acc: 0.9482758641242981)
[2025-01-06 01:39:31,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:31,784][root][INFO] - Training Epoch: 7/10, step 484/574 completed (loss: 0.004843076225370169, acc: 1.0)
[2025-01-06 01:39:31,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:32,088][root][INFO] - Training Epoch: 7/10, step 485/574 completed (loss: 0.13381646573543549, acc: 0.9473684430122375)
[2025-01-06 01:39:32,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:32,417][root][INFO] - Training Epoch: 7/10, step 486/574 completed (loss: 0.06674356758594513, acc: 1.0)
[2025-01-06 01:39:32,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:32,762][root][INFO] - Training Epoch: 7/10, step 487/574 completed (loss: 0.0016562881646677852, acc: 1.0)
[2025-01-06 01:39:32,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:33,116][root][INFO] - Training Epoch: 7/10, step 488/574 completed (loss: 0.04703845828771591, acc: 1.0)
[2025-01-06 01:39:33,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:33,479][root][INFO] - Training Epoch: 7/10, step 489/574 completed (loss: 0.12601742148399353, acc: 0.9384615421295166)
[2025-01-06 01:39:33,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:33,816][root][INFO] - Training Epoch: 7/10, step 490/574 completed (loss: 0.044530585408210754, acc: 0.9666666388511658)
[2025-01-06 01:39:33,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:34,171][root][INFO] - Training Epoch: 7/10, step 491/574 completed (loss: 0.009194308891892433, acc: 1.0)
[2025-01-06 01:39:34,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:34,517][root][INFO] - Training Epoch: 7/10, step 492/574 completed (loss: 0.25539761781692505, acc: 0.9215686321258545)
[2025-01-06 01:39:34,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:34,827][root][INFO] - Training Epoch: 7/10, step 493/574 completed (loss: 0.13381220400333405, acc: 0.9655172228813171)
[2025-01-06 01:39:34,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:35,205][root][INFO] - Training Epoch: 7/10, step 494/574 completed (loss: 0.1769278198480606, acc: 0.9473684430122375)
[2025-01-06 01:39:35,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:35,559][root][INFO] - Training Epoch: 7/10, step 495/574 completed (loss: 0.2552395164966583, acc: 0.8947368264198303)
[2025-01-06 01:39:35,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:36,020][root][INFO] - Training Epoch: 7/10, step 496/574 completed (loss: 0.37431758642196655, acc: 0.9107142686843872)
[2025-01-06 01:39:36,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:36,442][root][INFO] - Training Epoch: 7/10, step 497/574 completed (loss: 0.045059606432914734, acc: 1.0)
[2025-01-06 01:39:36,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:36,801][root][INFO] - Training Epoch: 7/10, step 498/574 completed (loss: 0.356868714094162, acc: 0.9213483333587646)
[2025-01-06 01:39:36,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:37,142][root][INFO] - Training Epoch: 7/10, step 499/574 completed (loss: 0.5889760255813599, acc: 0.8085106611251831)
[2025-01-06 01:39:37,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:37,548][root][INFO] - Training Epoch: 7/10, step 500/574 completed (loss: 0.16537804901599884, acc: 0.9347826242446899)
[2025-01-06 01:39:37,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:37,872][root][INFO] - Training Epoch: 7/10, step 501/574 completed (loss: 0.007495464291423559, acc: 1.0)
[2025-01-06 01:39:37,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:38,181][root][INFO] - Training Epoch: 7/10, step 502/574 completed (loss: 0.0007716961554251611, acc: 1.0)
[2025-01-06 01:39:38,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:38,540][root][INFO] - Training Epoch: 7/10, step 503/574 completed (loss: 0.10156777501106262, acc: 0.9629629850387573)
[2025-01-06 01:39:38,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:38,840][root][INFO] - Training Epoch: 7/10, step 504/574 completed (loss: 0.002506471937522292, acc: 1.0)
[2025-01-06 01:39:38,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:39,204][root][INFO] - Training Epoch: 7/10, step 505/574 completed (loss: 0.23970744013786316, acc: 0.9056603908538818)
[2025-01-06 01:39:39,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:39,555][root][INFO] - Training Epoch: 7/10, step 506/574 completed (loss: 0.18315771222114563, acc: 0.9655172228813171)
[2025-01-06 01:39:39,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:40,195][root][INFO] - Training Epoch: 7/10, step 507/574 completed (loss: 0.5728712677955627, acc: 0.8648648858070374)
[2025-01-06 01:39:40,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:40,637][root][INFO] - Training Epoch: 7/10, step 508/574 completed (loss: 0.23787151277065277, acc: 0.9436619877815247)
[2025-01-06 01:39:40,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:40,935][root][INFO] - Training Epoch: 7/10, step 509/574 completed (loss: 0.1925070583820343, acc: 0.949999988079071)
[2025-01-06 01:39:41,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:41,228][root][INFO] - Training Epoch: 7/10, step 510/574 completed (loss: 0.03211014345288277, acc: 0.9666666388511658)
[2025-01-06 01:39:41,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:41,613][root][INFO] - Training Epoch: 7/10, step 511/574 completed (loss: 0.22922572493553162, acc: 0.9230769276618958)
[2025-01-06 01:39:42,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:44,075][root][INFO] - Training Epoch: 7/10, step 512/574 completed (loss: 0.4075082540512085, acc: 0.8642857074737549)
[2025-01-06 01:39:44,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:44,852][root][INFO] - Training Epoch: 7/10, step 513/574 completed (loss: 0.09419792145490646, acc: 0.9603174328804016)
[2025-01-06 01:39:44,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:45,218][root][INFO] - Training Epoch: 7/10, step 514/574 completed (loss: 0.16109272837638855, acc: 0.9285714030265808)
[2025-01-06 01:39:45,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:45,555][root][INFO] - Training Epoch: 7/10, step 515/574 completed (loss: 0.003447932191193104, acc: 1.0)
[2025-01-06 01:39:45,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:46,300][root][INFO] - Training Epoch: 7/10, step 516/574 completed (loss: 0.1853807419538498, acc: 0.9444444179534912)
[2025-01-06 01:39:46,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:46,662][root][INFO] - Training Epoch: 7/10, step 517/574 completed (loss: 0.00023271415557246655, acc: 1.0)
[2025-01-06 01:39:46,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:46,959][root][INFO] - Training Epoch: 7/10, step 518/574 completed (loss: 0.013525388203561306, acc: 1.0)
[2025-01-06 01:39:47,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:47,264][root][INFO] - Training Epoch: 7/10, step 519/574 completed (loss: 0.06154017522931099, acc: 0.949999988079071)
[2025-01-06 01:39:47,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:47,569][root][INFO] - Training Epoch: 7/10, step 520/574 completed (loss: 0.025319255888462067, acc: 1.0)
[2025-01-06 01:39:47,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:48,625][root][INFO] - Training Epoch: 7/10, step 521/574 completed (loss: 0.3257550895214081, acc: 0.8983050584793091)
[2025-01-06 01:39:48,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:48,976][root][INFO] - Training Epoch: 7/10, step 522/574 completed (loss: 0.12728986144065857, acc: 0.9701492786407471)
[2025-01-06 01:39:49,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:49,342][root][INFO] - Training Epoch: 7/10, step 523/574 completed (loss: 0.14063704013824463, acc: 0.9635036587715149)
[2025-01-06 01:39:49,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:49,913][root][INFO] - Training Epoch: 7/10, step 524/574 completed (loss: 0.3719164729118347, acc: 0.9049999713897705)
[2025-01-06 01:39:49,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:50,222][root][INFO] - Training Epoch: 7/10, step 525/574 completed (loss: 0.003975576255470514, acc: 1.0)
[2025-01-06 01:39:50,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:50,528][root][INFO] - Training Epoch: 7/10, step 526/574 completed (loss: 0.08314640820026398, acc: 0.9807692170143127)
[2025-01-06 01:39:50,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:50,927][root][INFO] - Training Epoch: 7/10, step 527/574 completed (loss: 0.0273798368871212, acc: 1.0)
[2025-01-06 01:39:51,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:51,310][root][INFO] - Training Epoch: 7/10, step 528/574 completed (loss: 0.2587313652038574, acc: 0.9180327653884888)
[2025-01-06 01:39:51,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:51,636][root][INFO] - Training Epoch: 7/10, step 529/574 completed (loss: 0.03171389177441597, acc: 1.0)
[2025-01-06 01:39:51,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:51,962][root][INFO] - Training Epoch: 7/10, step 530/574 completed (loss: 0.5333653092384338, acc: 0.8604651093482971)
[2025-01-06 01:39:52,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:52,285][root][INFO] - Training Epoch: 7/10, step 531/574 completed (loss: 0.20691746473312378, acc: 0.9545454382896423)
[2025-01-06 01:39:52,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:52,630][root][INFO] - Training Epoch: 7/10, step 532/574 completed (loss: 0.04151274636387825, acc: 1.0)
[2025-01-06 01:39:52,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:52,970][root][INFO] - Training Epoch: 7/10, step 533/574 completed (loss: 0.1632836014032364, acc: 0.9772727489471436)
[2025-01-06 01:39:53,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:53,305][root][INFO] - Training Epoch: 7/10, step 534/574 completed (loss: 0.04709320887923241, acc: 1.0)
[2025-01-06 01:39:53,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:53,635][root][INFO] - Training Epoch: 7/10, step 535/574 completed (loss: 0.014911708422005177, acc: 1.0)
[2025-01-06 01:39:53,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:53,984][root][INFO] - Training Epoch: 7/10, step 536/574 completed (loss: 0.01140549685806036, acc: 1.0)
[2025-01-06 01:39:54,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:54,398][root][INFO] - Training Epoch: 7/10, step 537/574 completed (loss: 0.10966157913208008, acc: 0.9692307710647583)
[2025-01-06 01:39:54,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:54,732][root][INFO] - Training Epoch: 7/10, step 538/574 completed (loss: 0.18152324855327606, acc: 0.9375)
[2025-01-06 01:39:54,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:55,148][root][INFO] - Training Epoch: 7/10, step 539/574 completed (loss: 0.09117332845926285, acc: 1.0)
[2025-01-06 01:39:55,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:55,527][root][INFO] - Training Epoch: 7/10, step 540/574 completed (loss: 0.03396732360124588, acc: 1.0)
[2025-01-06 01:39:55,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:55,875][root][INFO] - Training Epoch: 7/10, step 541/574 completed (loss: 0.037553563714027405, acc: 1.0)
[2025-01-06 01:39:55,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:56,223][root][INFO] - Training Epoch: 7/10, step 542/574 completed (loss: 0.06722762435674667, acc: 0.9677419066429138)
[2025-01-06 01:39:56,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:56,652][root][INFO] - Training Epoch: 7/10, step 543/574 completed (loss: 0.002715194830670953, acc: 1.0)
[2025-01-06 01:39:56,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:57,003][root][INFO] - Training Epoch: 7/10, step 544/574 completed (loss: 0.03232460841536522, acc: 1.0)
[2025-01-06 01:39:57,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:57,345][root][INFO] - Training Epoch: 7/10, step 545/574 completed (loss: 0.14309900999069214, acc: 0.9512194991111755)
[2025-01-06 01:39:57,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:57,669][root][INFO] - Training Epoch: 7/10, step 546/574 completed (loss: 0.002651113085448742, acc: 1.0)
[2025-01-06 01:39:57,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:58,014][root][INFO] - Training Epoch: 7/10, step 547/574 completed (loss: 0.0039793215692043304, acc: 1.0)
[2025-01-06 01:39:58,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:58,346][root][INFO] - Training Epoch: 7/10, step 548/574 completed (loss: 0.007745253387838602, acc: 1.0)
[2025-01-06 01:39:58,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:58,706][root][INFO] - Training Epoch: 7/10, step 549/574 completed (loss: 0.0012849484337493777, acc: 1.0)
[2025-01-06 01:39:58,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:59,042][root][INFO] - Training Epoch: 7/10, step 550/574 completed (loss: 0.07086117565631866, acc: 0.939393937587738)
[2025-01-06 01:39:59,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:59,375][root][INFO] - Training Epoch: 7/10, step 551/574 completed (loss: 0.09715841710567474, acc: 0.9750000238418579)
[2025-01-06 01:39:59,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:59,715][root][INFO] - Training Epoch: 7/10, step 552/574 completed (loss: 0.15669263899326324, acc: 0.9571428298950195)
[2025-01-06 01:39:59,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:00,051][root][INFO] - Training Epoch: 7/10, step 553/574 completed (loss: 0.0783904418349266, acc: 0.956204354763031)
[2025-01-06 01:40:00,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:00,350][root][INFO] - Training Epoch: 7/10, step 554/574 completed (loss: 0.029421178624033928, acc: 0.9862068891525269)
[2025-01-06 01:40:00,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:00,680][root][INFO] - Training Epoch: 7/10, step 555/574 completed (loss: 0.12086974829435349, acc: 0.949999988079071)
[2025-01-06 01:40:00,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:01,020][root][INFO] - Training Epoch: 7/10, step 556/574 completed (loss: 0.15027207136154175, acc: 0.9735099077224731)
[2025-01-06 01:40:01,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:01,364][root][INFO] - Training Epoch: 7/10, step 557/574 completed (loss: 0.05085071921348572, acc: 0.9829059839248657)
[2025-01-06 01:40:01,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:01,682][root][INFO] - Training Epoch: 7/10, step 558/574 completed (loss: 0.0032025391701608896, acc: 1.0)
[2025-01-06 01:40:01,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:02,040][root][INFO] - Training Epoch: 7/10, step 559/574 completed (loss: 0.0979900062084198, acc: 0.9615384340286255)
[2025-01-06 01:40:02,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:03,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:03,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:03,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:04,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:04,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:04,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:06,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:06,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:07,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:07,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:09,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:09,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:09,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:10,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:10,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:10,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:11,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:11,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:12,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:12,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:12,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:13,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:13,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:13,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:14,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:14,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:14,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:15,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:15,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:15,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:17,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:17,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:17,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:18,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:18,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:20,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:20,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:20,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:21,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:21,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:21,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:22,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:22,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:22,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:23,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:23,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:23,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:24,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:24,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:24,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:25,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:25,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:25,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:26,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:26,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:26,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:27,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:27,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:27,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:28,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:28,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:28,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:29,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:29,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:29,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:30,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:30,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:30,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:31,542][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4268, device='cuda:0') eval_epoch_loss=tensor(0.8866, device='cuda:0') eval_epoch_acc=tensor(0.8266, device='cuda:0')
[2025-01-06 01:40:31,544][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:40:31,544][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:40:31,890][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_560_loss_0.8865754008293152/model.pt
[2025-01-06 01:40:31,903][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:40:32,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:32,305][root][INFO] - Training Epoch: 7/10, step 560/574 completed (loss: 0.0017870725132524967, acc: 1.0)
[2025-01-06 01:40:32,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:32,637][root][INFO] - Training Epoch: 7/10, step 561/574 completed (loss: 0.1821829229593277, acc: 0.9743589758872986)
[2025-01-06 01:40:32,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:32,996][root][INFO] - Training Epoch: 7/10, step 562/574 completed (loss: 0.18281981348991394, acc: 0.9666666388511658)
[2025-01-06 01:40:33,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:33,328][root][INFO] - Training Epoch: 7/10, step 563/574 completed (loss: 0.06237214058637619, acc: 0.9740259647369385)
[2025-01-06 01:40:33,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:33,638][root][INFO] - Training Epoch: 7/10, step 564/574 completed (loss: 0.14517821371555328, acc: 0.9583333134651184)
[2025-01-06 01:40:33,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:33,966][root][INFO] - Training Epoch: 7/10, step 565/574 completed (loss: 0.03420271724462509, acc: 0.982758641242981)
[2025-01-06 01:40:34,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:34,264][root][INFO] - Training Epoch: 7/10, step 566/574 completed (loss: 0.03653784468770027, acc: 0.988095223903656)
[2025-01-06 01:40:34,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:34,562][root][INFO] - Training Epoch: 7/10, step 567/574 completed (loss: 0.051581721752882004, acc: 0.9736841917037964)
[2025-01-06 01:40:34,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:34,856][root][INFO] - Training Epoch: 7/10, step 568/574 completed (loss: 0.0027295583859086037, acc: 1.0)
[2025-01-06 01:40:34,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:35,237][root][INFO] - Training Epoch: 7/10, step 569/574 completed (loss: 0.09639792144298553, acc: 0.9625668525695801)
[2025-01-06 01:40:35,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:35,555][root][INFO] - Training Epoch: 7/10, step 570/574 completed (loss: 0.006320232525467873, acc: 1.0)
[2025-01-06 01:40:35,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:35,875][root][INFO] - Training Epoch: 7/10, step 571/574 completed (loss: 0.03594614192843437, acc: 0.9914529919624329)
[2025-01-06 01:40:35,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:36,215][root][INFO] - Training Epoch: 7/10, step 572/574 completed (loss: 0.16131781041622162, acc: 0.9489796161651611)
[2025-01-06 01:40:36,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:36,557][root][INFO] - Training Epoch: 7/10, step 573/574 completed (loss: 0.07000480592250824, acc: 0.9874213933944702)
[2025-01-06 01:40:37,115][slam_llm.utils.train_utils][INFO] - Epoch 7: train_perplexity=1.1399, train_epoch_loss=0.1309, epoch time 342.75879050791264s
[2025-01-06 01:40:37,115][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:40:37,115][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 18 GB
[2025-01-06 01:40:37,115][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:40:37,115][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 19
[2025-01-06 01:40:37,116][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:40:37,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:37,964][root][INFO] - Training Epoch: 8/10, step 0/574 completed (loss: 0.024650931358337402, acc: 1.0)
[2025-01-06 01:40:38,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:38,259][root][INFO] - Training Epoch: 8/10, step 1/574 completed (loss: 0.050760265439748764, acc: 1.0)
[2025-01-06 01:40:38,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:38,607][root][INFO] - Training Epoch: 8/10, step 2/574 completed (loss: 0.24290455877780914, acc: 0.9459459185600281)
[2025-01-06 01:40:38,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:38,978][root][INFO] - Training Epoch: 8/10, step 3/574 completed (loss: 0.01974107138812542, acc: 1.0)
[2025-01-06 01:40:39,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:39,333][root][INFO] - Training Epoch: 8/10, step 4/574 completed (loss: 0.09718291461467743, acc: 0.9459459185600281)
[2025-01-06 01:40:39,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:39,705][root][INFO] - Training Epoch: 8/10, step 5/574 completed (loss: 0.032515257596969604, acc: 1.0)
[2025-01-06 01:40:39,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:40,038][root][INFO] - Training Epoch: 8/10, step 6/574 completed (loss: 0.04831629991531372, acc: 0.9795918464660645)
[2025-01-06 01:40:40,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:40,342][root][INFO] - Training Epoch: 8/10, step 7/574 completed (loss: 0.03772889822721481, acc: 0.9666666388511658)
[2025-01-06 01:40:40,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:40,648][root][INFO] - Training Epoch: 8/10, step 8/574 completed (loss: 0.2929380238056183, acc: 0.9545454382896423)
[2025-01-06 01:40:40,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:40,991][root][INFO] - Training Epoch: 8/10, step 9/574 completed (loss: 0.006488691549748182, acc: 1.0)
[2025-01-06 01:40:41,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:41,309][root][INFO] - Training Epoch: 8/10, step 10/574 completed (loss: 0.49928396940231323, acc: 0.9629629850387573)
[2025-01-06 01:40:41,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:41,591][root][INFO] - Training Epoch: 8/10, step 11/574 completed (loss: 0.13527970016002655, acc: 0.9487179517745972)
[2025-01-06 01:40:41,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:41,889][root][INFO] - Training Epoch: 8/10, step 12/574 completed (loss: 0.005344809498637915, acc: 1.0)
[2025-01-06 01:40:41,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:42,225][root][INFO] - Training Epoch: 8/10, step 13/574 completed (loss: 0.018739234656095505, acc: 1.0)
[2025-01-06 01:40:42,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:42,548][root][INFO] - Training Epoch: 8/10, step 14/574 completed (loss: 0.010526611469686031, acc: 1.0)
[2025-01-06 01:40:42,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:42,894][root][INFO] - Training Epoch: 8/10, step 15/574 completed (loss: 0.013876528479158878, acc: 1.0)
[2025-01-06 01:40:43,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:43,245][root][INFO] - Training Epoch: 8/10, step 16/574 completed (loss: 0.00758409732952714, acc: 1.0)
[2025-01-06 01:40:43,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:43,568][root][INFO] - Training Epoch: 8/10, step 17/574 completed (loss: 0.01152967382222414, acc: 1.0)
[2025-01-06 01:40:43,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:43,879][root][INFO] - Training Epoch: 8/10, step 18/574 completed (loss: 0.15421409904956818, acc: 0.9444444179534912)
[2025-01-06 01:40:43,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:44,190][root][INFO] - Training Epoch: 8/10, step 19/574 completed (loss: 0.05632776394486427, acc: 1.0)
[2025-01-06 01:40:44,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:44,493][root][INFO] - Training Epoch: 8/10, step 20/574 completed (loss: 0.01983962208032608, acc: 1.0)
[2025-01-06 01:40:44,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:44,789][root][INFO] - Training Epoch: 8/10, step 21/574 completed (loss: 0.08127110451459885, acc: 0.9655172228813171)
[2025-01-06 01:40:44,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:45,104][root][INFO] - Training Epoch: 8/10, step 22/574 completed (loss: 0.011005543172359467, acc: 1.0)
[2025-01-06 01:40:45,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:45,423][root][INFO] - Training Epoch: 8/10, step 23/574 completed (loss: 0.006926718167960644, acc: 1.0)
[2025-01-06 01:40:45,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:45,731][root][INFO] - Training Epoch: 8/10, step 24/574 completed (loss: 0.006737479940056801, acc: 1.0)
[2025-01-06 01:40:45,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:46,036][root][INFO] - Training Epoch: 8/10, step 25/574 completed (loss: 0.2322961837053299, acc: 0.9245283007621765)
[2025-01-06 01:40:46,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:46,357][root][INFO] - Training Epoch: 8/10, step 26/574 completed (loss: 0.10205422341823578, acc: 0.9726027250289917)
[2025-01-06 01:40:46,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:47,633][root][INFO] - Training Epoch: 8/10, step 27/574 completed (loss: 0.3980765640735626, acc: 0.8774703741073608)
[2025-01-06 01:40:47,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:47,970][root][INFO] - Training Epoch: 8/10, step 28/574 completed (loss: 0.0320376418530941, acc: 1.0)
[2025-01-06 01:40:48,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:48,325][root][INFO] - Training Epoch: 8/10, step 29/574 completed (loss: 0.08271926641464233, acc: 0.9638554453849792)
[2025-01-06 01:40:48,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:48,651][root][INFO] - Training Epoch: 8/10, step 30/574 completed (loss: 0.0378243587911129, acc: 1.0)
[2025-01-06 01:40:48,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:48,954][root][INFO] - Training Epoch: 8/10, step 31/574 completed (loss: 0.2628006935119629, acc: 0.9285714030265808)
[2025-01-06 01:40:49,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:49,258][root][INFO] - Training Epoch: 8/10, step 32/574 completed (loss: 0.01686270534992218, acc: 1.0)
[2025-01-06 01:40:49,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:49,562][root][INFO] - Training Epoch: 8/10, step 33/574 completed (loss: 0.003642983501777053, acc: 1.0)
[2025-01-06 01:40:49,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:49,886][root][INFO] - Training Epoch: 8/10, step 34/574 completed (loss: 0.0910114273428917, acc: 0.9663865566253662)
[2025-01-06 01:40:49,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:50,200][root][INFO] - Training Epoch: 8/10, step 35/574 completed (loss: 0.11572108417749405, acc: 0.9836065769195557)
[2025-01-06 01:40:50,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:50,555][root][INFO] - Training Epoch: 8/10, step 36/574 completed (loss: 0.09692147374153137, acc: 0.9682539701461792)
[2025-01-06 01:40:50,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:50,863][root][INFO] - Training Epoch: 8/10, step 37/574 completed (loss: 0.09280568361282349, acc: 0.9491525292396545)
[2025-01-06 01:40:50,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:51,190][root][INFO] - Training Epoch: 8/10, step 38/574 completed (loss: 0.07948688417673111, acc: 0.9655172228813171)
[2025-01-06 01:40:51,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:51,489][root][INFO] - Training Epoch: 8/10, step 39/574 completed (loss: 0.03676338866353035, acc: 1.0)
[2025-01-06 01:40:51,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:51,793][root][INFO] - Training Epoch: 8/10, step 40/574 completed (loss: 0.014878841117024422, acc: 1.0)
[2025-01-06 01:40:51,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:52,148][root][INFO] - Training Epoch: 8/10, step 41/574 completed (loss: 0.26559653878211975, acc: 0.8918918967247009)
[2025-01-06 01:40:52,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:52,485][root][INFO] - Training Epoch: 8/10, step 42/574 completed (loss: 0.17207838594913483, acc: 0.9384615421295166)
[2025-01-06 01:40:52,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:52,895][root][INFO] - Training Epoch: 8/10, step 43/574 completed (loss: 0.15140597522258759, acc: 0.9494949579238892)
[2025-01-06 01:40:52,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:53,303][root][INFO] - Training Epoch: 8/10, step 44/574 completed (loss: 0.14979976415634155, acc: 0.9484536051750183)
[2025-01-06 01:40:53,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:53,708][root][INFO] - Training Epoch: 8/10, step 45/574 completed (loss: 0.09391175955533981, acc: 0.9779411554336548)
[2025-01-06 01:40:53,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:54,028][root][INFO] - Training Epoch: 8/10, step 46/574 completed (loss: 0.037893153727054596, acc: 1.0)
[2025-01-06 01:40:54,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:54,334][root][INFO] - Training Epoch: 8/10, step 47/574 completed (loss: 0.1521957963705063, acc: 0.9629629850387573)
[2025-01-06 01:40:54,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:54,671][root][INFO] - Training Epoch: 8/10, step 48/574 completed (loss: 0.011204337701201439, acc: 1.0)
[2025-01-06 01:40:54,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:54,986][root][INFO] - Training Epoch: 8/10, step 49/574 completed (loss: 0.0030293345917016268, acc: 1.0)
[2025-01-06 01:40:55,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55,301][root][INFO] - Training Epoch: 8/10, step 50/574 completed (loss: 0.056967973709106445, acc: 1.0)
[2025-01-06 01:40:55,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55,635][root][INFO] - Training Epoch: 8/10, step 51/574 completed (loss: 0.11215487122535706, acc: 0.9682539701461792)
[2025-01-06 01:40:55,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55,937][root][INFO] - Training Epoch: 8/10, step 52/574 completed (loss: 0.24692277610301971, acc: 0.8873239159584045)
[2025-01-06 01:40:56,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:56,384][root][INFO] - Training Epoch: 8/10, step 53/574 completed (loss: 0.4940071702003479, acc: 0.8266666531562805)
[2025-01-06 01:40:56,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:56,691][root][INFO] - Training Epoch: 8/10, step 54/574 completed (loss: 0.23361964523792267, acc: 0.9729729890823364)
[2025-01-06 01:40:56,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:57,002][root][INFO] - Training Epoch: 8/10, step 55/574 completed (loss: 0.09206220507621765, acc: 0.9615384340286255)
[2025-01-06 01:40:58,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:59,966][root][INFO] - Training Epoch: 8/10, step 56/574 completed (loss: 0.4807559549808502, acc: 0.8532423377037048)
[2025-01-06 01:41:00,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:01,165][root][INFO] - Training Epoch: 8/10, step 57/574 completed (loss: 0.766803503036499, acc: 0.7625272274017334)
[2025-01-06 01:41:01,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:01,786][root][INFO] - Training Epoch: 8/10, step 58/574 completed (loss: 0.3257201910018921, acc: 0.9034090638160706)
[2025-01-06 01:41:01,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:02,359][root][INFO] - Training Epoch: 8/10, step 59/574 completed (loss: 0.11999915540218353, acc: 0.9632353186607361)
[2025-01-06 01:41:02,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:02,920][root][INFO] - Training Epoch: 8/10, step 60/574 completed (loss: 0.3996804356575012, acc: 0.8695651888847351)
[2025-01-06 01:41:03,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:03,333][root][INFO] - Training Epoch: 8/10, step 61/574 completed (loss: 0.1591884195804596, acc: 0.949999988079071)
[2025-01-06 01:41:03,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:03,679][root][INFO] - Training Epoch: 8/10, step 62/574 completed (loss: 0.03739231824874878, acc: 1.0)
[2025-01-06 01:41:03,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:04,002][root][INFO] - Training Epoch: 8/10, step 63/574 completed (loss: 0.03328123316168785, acc: 1.0)
[2025-01-06 01:41:04,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:04,344][root][INFO] - Training Epoch: 8/10, step 64/574 completed (loss: 0.01799294352531433, acc: 1.0)
[2025-01-06 01:41:04,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:04,646][root][INFO] - Training Epoch: 8/10, step 65/574 completed (loss: 0.044222306460142136, acc: 0.9655172228813171)
[2025-01-06 01:41:04,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:04,970][root][INFO] - Training Epoch: 8/10, step 66/574 completed (loss: 0.12267600744962692, acc: 0.9642857313156128)
[2025-01-06 01:41:05,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:05,300][root][INFO] - Training Epoch: 8/10, step 67/574 completed (loss: 0.12523190677165985, acc: 0.949999988079071)
[2025-01-06 01:41:05,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:05,616][root][INFO] - Training Epoch: 8/10, step 68/574 completed (loss: 0.004268779885023832, acc: 1.0)
[2025-01-06 01:41:05,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:05,921][root][INFO] - Training Epoch: 8/10, step 69/574 completed (loss: 0.03020535223186016, acc: 1.0)
[2025-01-06 01:41:05,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:06,224][root][INFO] - Training Epoch: 8/10, step 70/574 completed (loss: 0.12228167057037354, acc: 0.9696969985961914)
[2025-01-06 01:41:06,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:06,512][root][INFO] - Training Epoch: 8/10, step 71/574 completed (loss: 0.2834249436855316, acc: 0.9191176295280457)
[2025-01-06 01:41:06,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:06,832][root][INFO] - Training Epoch: 8/10, step 72/574 completed (loss: 0.17083531618118286, acc: 0.9682539701461792)
[2025-01-06 01:41:06,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:07,183][root][INFO] - Training Epoch: 8/10, step 73/574 completed (loss: 0.6844009160995483, acc: 0.8256410360336304)
[2025-01-06 01:41:07,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:07,488][root][INFO] - Training Epoch: 8/10, step 74/574 completed (loss: 0.3520187735557556, acc: 0.8877550959587097)
[2025-01-06 01:41:07,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:07,811][root][INFO] - Training Epoch: 8/10, step 75/574 completed (loss: 0.34010788798332214, acc: 0.89552241563797)
[2025-01-06 01:41:07,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:08,197][root][INFO] - Training Epoch: 8/10, step 76/574 completed (loss: 0.7362626194953918, acc: 0.7737226486206055)
[2025-01-06 01:41:08,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:08,495][root][INFO] - Training Epoch: 8/10, step 77/574 completed (loss: 0.007866952568292618, acc: 1.0)
[2025-01-06 01:41:08,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:08,816][root][INFO] - Training Epoch: 8/10, step 78/574 completed (loss: 0.00383321032859385, acc: 1.0)
[2025-01-06 01:41:08,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:09,117][root][INFO] - Training Epoch: 8/10, step 79/574 completed (loss: 0.006900681648403406, acc: 1.0)
[2025-01-06 01:41:09,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:09,434][root][INFO] - Training Epoch: 8/10, step 80/574 completed (loss: 0.24056309461593628, acc: 0.9230769276618958)
[2025-01-06 01:41:09,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:09,755][root][INFO] - Training Epoch: 8/10, step 81/574 completed (loss: 0.023477228358387947, acc: 1.0)
[2025-01-06 01:41:09,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:10,081][root][INFO] - Training Epoch: 8/10, step 82/574 completed (loss: 0.2856416702270508, acc: 0.9615384340286255)
[2025-01-06 01:41:10,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:10,389][root][INFO] - Training Epoch: 8/10, step 83/574 completed (loss: 0.04095231741666794, acc: 1.0)
[2025-01-06 01:41:10,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:10,726][root][INFO] - Training Epoch: 8/10, step 84/574 completed (loss: 0.06159322336316109, acc: 0.9855072498321533)
[2025-01-06 01:41:10,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:11,041][root][INFO] - Training Epoch: 8/10, step 85/574 completed (loss: 0.06481005996465683, acc: 0.9800000190734863)
[2025-01-06 01:41:11,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:11,351][root][INFO] - Training Epoch: 8/10, step 86/574 completed (loss: 0.0219110194593668, acc: 1.0)
[2025-01-06 01:41:11,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:11,813][root][INFO] - Training Epoch: 8/10, step 87/574 completed (loss: 0.2689208984375, acc: 0.9399999976158142)
[2025-01-06 01:41:11,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:12,150][root][INFO] - Training Epoch: 8/10, step 88/574 completed (loss: 0.24238230288028717, acc: 0.9320388436317444)
[2025-01-06 01:41:12,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:13,221][root][INFO] - Training Epoch: 8/10, step 89/574 completed (loss: 0.379447340965271, acc: 0.8737863898277283)
[2025-01-06 01:41:13,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:14,035][root][INFO] - Training Epoch: 8/10, step 90/574 completed (loss: 0.34783533215522766, acc: 0.8817204236984253)
[2025-01-06 01:41:14,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:14,835][root][INFO] - Training Epoch: 8/10, step 91/574 completed (loss: 0.35053837299346924, acc: 0.8965517282485962)
[2025-01-06 01:41:15,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:15,576][root][INFO] - Training Epoch: 8/10, step 92/574 completed (loss: 0.2709995210170746, acc: 0.9052631855010986)
[2025-01-06 01:41:15,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:16,562][root][INFO] - Training Epoch: 8/10, step 93/574 completed (loss: 0.361123651266098, acc: 0.9108911156654358)
[2025-01-06 01:41:16,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:16,854][root][INFO] - Training Epoch: 8/10, step 94/574 completed (loss: 0.1310790479183197, acc: 0.9838709831237793)
[2025-01-06 01:41:16,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:17,179][root][INFO] - Training Epoch: 8/10, step 95/574 completed (loss: 0.17726510763168335, acc: 0.9420289993286133)
[2025-01-06 01:41:17,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:17,512][root][INFO] - Training Epoch: 8/10, step 96/574 completed (loss: 0.22455421090126038, acc: 0.9411764740943909)
[2025-01-06 01:41:17,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:17,834][root][INFO] - Training Epoch: 8/10, step 97/574 completed (loss: 0.15735021233558655, acc: 0.9615384340286255)
[2025-01-06 01:41:17,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:18,206][root][INFO] - Training Epoch: 8/10, step 98/574 completed (loss: 0.393633633852005, acc: 0.8759124279022217)
[2025-01-06 01:41:18,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:18,602][root][INFO] - Training Epoch: 8/10, step 99/574 completed (loss: 0.25333061814308167, acc: 0.9104477763175964)
[2025-01-06 01:41:18,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:18,965][root][INFO] - Training Epoch: 8/10, step 100/574 completed (loss: 0.03602786362171173, acc: 1.0)
[2025-01-06 01:41:19,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:19,303][root][INFO] - Training Epoch: 8/10, step 101/574 completed (loss: 0.005032481625676155, acc: 1.0)
[2025-01-06 01:41:19,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:19,627][root][INFO] - Training Epoch: 8/10, step 102/574 completed (loss: 0.5152036547660828, acc: 0.9130434989929199)
[2025-01-06 01:41:19,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:19,954][root][INFO] - Training Epoch: 8/10, step 103/574 completed (loss: 0.02259145863354206, acc: 0.9772727489471436)
[2025-01-06 01:41:20,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:20,279][root][INFO] - Training Epoch: 8/10, step 104/574 completed (loss: 0.26770761609077454, acc: 0.982758641242981)
[2025-01-06 01:41:20,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:20,610][root][INFO] - Training Epoch: 8/10, step 105/574 completed (loss: 0.01817660965025425, acc: 1.0)
[2025-01-06 01:41:20,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:20,935][root][INFO] - Training Epoch: 8/10, step 106/574 completed (loss: 0.06426442414522171, acc: 0.9599999785423279)
[2025-01-06 01:41:21,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:21,262][root][INFO] - Training Epoch: 8/10, step 107/574 completed (loss: 0.0022600735537707806, acc: 1.0)
[2025-01-06 01:41:21,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:21,610][root][INFO] - Training Epoch: 8/10, step 108/574 completed (loss: 0.015891743823885918, acc: 1.0)
[2025-01-06 01:41:21,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:21,975][root][INFO] - Training Epoch: 8/10, step 109/574 completed (loss: 0.054543476551771164, acc: 0.9523809552192688)
[2025-01-06 01:41:22,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:22,330][root][INFO] - Training Epoch: 8/10, step 110/574 completed (loss: 0.022757943719625473, acc: 1.0)
[2025-01-06 01:41:22,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:22,743][root][INFO] - Training Epoch: 8/10, step 111/574 completed (loss: 0.034680016338825226, acc: 0.9824561476707458)
[2025-01-06 01:41:22,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:23,099][root][INFO] - Training Epoch: 8/10, step 112/574 completed (loss: 0.3918715715408325, acc: 0.9122806787490845)
[2025-01-06 01:41:23,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:23,479][root][INFO] - Training Epoch: 8/10, step 113/574 completed (loss: 0.02649998664855957, acc: 1.0)
[2025-01-06 01:41:23,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:23,840][root][INFO] - Training Epoch: 8/10, step 114/574 completed (loss: 0.08050717413425446, acc: 0.9591836929321289)
[2025-01-06 01:41:23,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:24,137][root][INFO] - Training Epoch: 8/10, step 115/574 completed (loss: 0.004632908385246992, acc: 1.0)
[2025-01-06 01:41:24,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:24,456][root][INFO] - Training Epoch: 8/10, step 116/574 completed (loss: 0.01611805334687233, acc: 1.0)
[2025-01-06 01:41:24,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:24,793][root][INFO] - Training Epoch: 8/10, step 117/574 completed (loss: 0.08393282443284988, acc: 0.9756097793579102)
[2025-01-06 01:41:24,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:25,207][root][INFO] - Training Epoch: 8/10, step 118/574 completed (loss: 0.022323567420244217, acc: 0.9838709831237793)
[2025-01-06 01:41:25,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:26,045][root][INFO] - Training Epoch: 8/10, step 119/574 completed (loss: 0.2935182750225067, acc: 0.9201520681381226)
[2025-01-06 01:41:26,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:26,374][root][INFO] - Training Epoch: 8/10, step 120/574 completed (loss: 0.04266061633825302, acc: 0.9733333587646484)
[2025-01-06 01:41:26,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:26,779][root][INFO] - Training Epoch: 8/10, step 121/574 completed (loss: 0.12144359201192856, acc: 0.9615384340286255)
[2025-01-06 01:41:26,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:27,075][root][INFO] - Training Epoch: 8/10, step 122/574 completed (loss: 0.004346610978245735, acc: 1.0)
[2025-01-06 01:41:27,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:27,377][root][INFO] - Training Epoch: 8/10, step 123/574 completed (loss: 0.008665991015732288, acc: 1.0)
[2025-01-06 01:41:27,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:27,693][root][INFO] - Training Epoch: 8/10, step 124/574 completed (loss: 0.23949608206748962, acc: 0.9325153231620789)
[2025-01-06 01:41:27,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:28,035][root][INFO] - Training Epoch: 8/10, step 125/574 completed (loss: 0.36680713295936584, acc: 0.8819444179534912)
[2025-01-06 01:41:28,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:28,350][root][INFO] - Training Epoch: 8/10, step 126/574 completed (loss: 0.3642045557498932, acc: 0.8916666507720947)
[2025-01-06 01:41:28,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:28,680][root][INFO] - Training Epoch: 8/10, step 127/574 completed (loss: 0.18115365505218506, acc: 0.9345238208770752)
[2025-01-06 01:41:28,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:29,014][root][INFO] - Training Epoch: 8/10, step 128/574 completed (loss: 0.30057671666145325, acc: 0.9076923131942749)
[2025-01-06 01:41:29,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:30,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:30,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:30,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:31,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:31,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:31,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:32,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:32,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:32,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:33,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:33,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:33,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:34,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:34,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:34,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:36,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:36,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:36,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:37,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:37,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:37,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:38,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:38,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:38,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:39,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:39,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:39,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:41,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:41,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:43,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:43,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:43,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:44,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:44,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:44,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:45,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:45,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:45,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:46,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:46,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:46,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:47,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:47,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:47,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:48,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:48,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:48,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:49,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:49,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:49,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:50,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:50,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:50,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:50,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:51,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:51,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:51,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:54,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:54,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:54,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:54,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:55,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:55,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:55,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:56,447][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3654, device='cuda:0') eval_epoch_loss=tensor(0.8609, device='cuda:0') eval_epoch_acc=tensor(0.8309, device='cuda:0')
[2025-01-06 01:41:56,448][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:41:56,449][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:41:56,734][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_129_loss_0.8609286546707153/model.pt
[2025-01-06 01:41:56,744][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:41:56,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:57,183][root][INFO] - Training Epoch: 8/10, step 129/574 completed (loss: 0.3849679231643677, acc: 0.8897058963775635)
[2025-01-06 01:41:57,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:57,497][root][INFO] - Training Epoch: 8/10, step 130/574 completed (loss: 0.10991000384092331, acc: 0.9615384340286255)
[2025-01-06 01:41:57,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:57,789][root][INFO] - Training Epoch: 8/10, step 131/574 completed (loss: 0.0582256056368351, acc: 0.95652174949646)
[2025-01-06 01:41:57,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:58,079][root][INFO] - Training Epoch: 8/10, step 132/574 completed (loss: 0.017022376880049706, acc: 1.0)
[2025-01-06 01:41:58,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:58,368][root][INFO] - Training Epoch: 8/10, step 133/574 completed (loss: 0.015200737863779068, acc: 1.0)
[2025-01-06 01:41:58,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:58,663][root][INFO] - Training Epoch: 8/10, step 134/574 completed (loss: 0.05525171756744385, acc: 1.0)
[2025-01-06 01:41:58,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:58,975][root][INFO] - Training Epoch: 8/10, step 135/574 completed (loss: 0.045774415135383606, acc: 1.0)
[2025-01-06 01:41:59,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:59,267][root][INFO] - Training Epoch: 8/10, step 136/574 completed (loss: 0.09125235676765442, acc: 0.976190447807312)
[2025-01-06 01:41:59,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:59,574][root][INFO] - Training Epoch: 8/10, step 137/574 completed (loss: 0.04885842278599739, acc: 1.0)
[2025-01-06 01:41:59,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:59,871][root][INFO] - Training Epoch: 8/10, step 138/574 completed (loss: 0.00647937273606658, acc: 1.0)
[2025-01-06 01:41:59,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:00,166][root][INFO] - Training Epoch: 8/10, step 139/574 completed (loss: 0.005421584006398916, acc: 1.0)
[2025-01-06 01:42:00,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:00,474][root][INFO] - Training Epoch: 8/10, step 140/574 completed (loss: 0.08052247017621994, acc: 0.9615384340286255)
[2025-01-06 01:42:00,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:00,780][root][INFO] - Training Epoch: 8/10, step 141/574 completed (loss: 0.028414985164999962, acc: 1.0)
[2025-01-06 01:42:00,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:01,073][root][INFO] - Training Epoch: 8/10, step 142/574 completed (loss: 0.26824864745140076, acc: 0.8918918967247009)
[2025-01-06 01:42:01,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:01,594][root][INFO] - Training Epoch: 8/10, step 143/574 completed (loss: 0.1910126507282257, acc: 0.9210526347160339)
[2025-01-06 01:42:01,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:01,956][root][INFO] - Training Epoch: 8/10, step 144/574 completed (loss: 0.24612028896808624, acc: 0.9626865386962891)
[2025-01-06 01:42:02,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:02,293][root][INFO] - Training Epoch: 8/10, step 145/574 completed (loss: 0.23483118414878845, acc: 0.9285714030265808)
[2025-01-06 01:42:02,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:02,741][root][INFO] - Training Epoch: 8/10, step 146/574 completed (loss: 0.32760095596313477, acc: 0.8617021441459656)
[2025-01-06 01:42:02,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03,041][root][INFO] - Training Epoch: 8/10, step 147/574 completed (loss: 0.09142474085092545, acc: 0.9571428298950195)
[2025-01-06 01:42:03,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03,334][root][INFO] - Training Epoch: 8/10, step 148/574 completed (loss: 0.1305272877216339, acc: 0.9285714030265808)
[2025-01-06 01:42:03,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03,626][root][INFO] - Training Epoch: 8/10, step 149/574 completed (loss: 0.044442400336265564, acc: 0.95652174949646)
[2025-01-06 01:42:03,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03,943][root][INFO] - Training Epoch: 8/10, step 150/574 completed (loss: 0.011084314435720444, acc: 1.0)
[2025-01-06 01:42:04,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:04,257][root][INFO] - Training Epoch: 8/10, step 151/574 completed (loss: 0.11734996736049652, acc: 0.95652174949646)
[2025-01-06 01:42:04,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:04,611][root][INFO] - Training Epoch: 8/10, step 152/574 completed (loss: 0.1696661114692688, acc: 0.9322034120559692)
[2025-01-06 01:42:04,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:04,915][root][INFO] - Training Epoch: 8/10, step 153/574 completed (loss: 0.14414052665233612, acc: 0.9298245906829834)
[2025-01-06 01:42:05,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:05,237][root][INFO] - Training Epoch: 8/10, step 154/574 completed (loss: 0.13951164484024048, acc: 0.9594594836235046)
[2025-01-06 01:42:05,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:05,531][root][INFO] - Training Epoch: 8/10, step 155/574 completed (loss: 0.0028845726046711206, acc: 1.0)
[2025-01-06 01:42:05,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:05,823][root][INFO] - Training Epoch: 8/10, step 156/574 completed (loss: 0.02275003492832184, acc: 1.0)
[2025-01-06 01:42:05,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:06,139][root][INFO] - Training Epoch: 8/10, step 157/574 completed (loss: 0.16385363042354584, acc: 0.9473684430122375)
[2025-01-06 01:42:06,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:07,755][root][INFO] - Training Epoch: 8/10, step 158/574 completed (loss: 0.1030183881521225, acc: 0.9594594836235046)
[2025-01-06 01:42:07,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:08,051][root][INFO] - Training Epoch: 8/10, step 159/574 completed (loss: 0.18249700963497162, acc: 0.9259259104728699)
[2025-01-06 01:42:08,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:08,447][root][INFO] - Training Epoch: 8/10, step 160/574 completed (loss: 0.1275772899389267, acc: 0.9534883499145508)
[2025-01-06 01:42:08,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:09,032][root][INFO] - Training Epoch: 8/10, step 161/574 completed (loss: 0.48159533739089966, acc: 0.8941176533699036)
[2025-01-06 01:42:09,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:09,590][root][INFO] - Training Epoch: 8/10, step 162/574 completed (loss: 0.41518285870552063, acc: 0.8876404762268066)
[2025-01-06 01:42:09,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:09,903][root][INFO] - Training Epoch: 8/10, step 163/574 completed (loss: 0.060063425451517105, acc: 0.9772727489471436)
[2025-01-06 01:42:09,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:10,196][root][INFO] - Training Epoch: 8/10, step 164/574 completed (loss: 0.16064903140068054, acc: 0.9523809552192688)
[2025-01-06 01:42:10,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:10,559][root][INFO] - Training Epoch: 8/10, step 165/574 completed (loss: 0.08398151397705078, acc: 0.9655172228813171)
[2025-01-06 01:42:10,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:10,902][root][INFO] - Training Epoch: 8/10, step 166/574 completed (loss: 0.033348940312862396, acc: 1.0)
[2025-01-06 01:42:10,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:11,218][root][INFO] - Training Epoch: 8/10, step 167/574 completed (loss: 0.17289398610591888, acc: 0.9200000166893005)
[2025-01-06 01:42:11,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:11,598][root][INFO] - Training Epoch: 8/10, step 168/574 completed (loss: 0.06524788588285446, acc: 0.9861111044883728)
[2025-01-06 01:42:11,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:12,013][root][INFO] - Training Epoch: 8/10, step 169/574 completed (loss: 0.3342537581920624, acc: 0.9313725233078003)
[2025-01-06 01:42:12,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:13,046][root][INFO] - Training Epoch: 8/10, step 170/574 completed (loss: 0.24660994112491608, acc: 0.9178082346916199)
[2025-01-06 01:42:13,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:13,409][root][INFO] - Training Epoch: 8/10, step 171/574 completed (loss: 0.03184181824326515, acc: 0.9583333134651184)
[2025-01-06 01:42:13,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:13,743][root][INFO] - Training Epoch: 8/10, step 172/574 completed (loss: 0.038590360432863235, acc: 1.0)
[2025-01-06 01:42:13,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:14,112][root][INFO] - Training Epoch: 8/10, step 173/574 completed (loss: 0.05037887021899223, acc: 0.9642857313156128)
[2025-01-06 01:42:14,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:14,654][root][INFO] - Training Epoch: 8/10, step 174/574 completed (loss: 0.3316101133823395, acc: 0.9026548862457275)
[2025-01-06 01:42:14,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:14,990][root][INFO] - Training Epoch: 8/10, step 175/574 completed (loss: 0.24169319868087769, acc: 0.9275362491607666)
[2025-01-06 01:42:15,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:15,343][root][INFO] - Training Epoch: 8/10, step 176/574 completed (loss: 0.05610295757651329, acc: 0.9772727489471436)
[2025-01-06 01:42:15,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:16,250][root][INFO] - Training Epoch: 8/10, step 177/574 completed (loss: 0.4228670597076416, acc: 0.885496199131012)
[2025-01-06 01:42:16,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:16,930][root][INFO] - Training Epoch: 8/10, step 178/574 completed (loss: 0.27843448519706726, acc: 0.9259259104728699)
[2025-01-06 01:42:17,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:17,287][root][INFO] - Training Epoch: 8/10, step 179/574 completed (loss: 0.06533005833625793, acc: 0.9836065769195557)
[2025-01-06 01:42:17,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:17,612][root][INFO] - Training Epoch: 8/10, step 180/574 completed (loss: 0.005081685725599527, acc: 1.0)
[2025-01-06 01:42:17,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:17,938][root][INFO] - Training Epoch: 8/10, step 181/574 completed (loss: 0.0004182456177659333, acc: 1.0)
[2025-01-06 01:42:18,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:18,281][root][INFO] - Training Epoch: 8/10, step 182/574 completed (loss: 0.02031443826854229, acc: 1.0)
[2025-01-06 01:42:18,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:18,652][root][INFO] - Training Epoch: 8/10, step 183/574 completed (loss: 0.03344447910785675, acc: 1.0)
[2025-01-06 01:42:18,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:19,036][root][INFO] - Training Epoch: 8/10, step 184/574 completed (loss: 0.24062174558639526, acc: 0.9335347414016724)
[2025-01-06 01:42:19,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:19,391][root][INFO] - Training Epoch: 8/10, step 185/574 completed (loss: 0.18315403163433075, acc: 0.9365994334220886)
[2025-01-06 01:42:19,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:19,911][root][INFO] - Training Epoch: 8/10, step 186/574 completed (loss: 0.15710128843784332, acc: 0.953125)
[2025-01-06 01:42:20,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:20,438][root][INFO] - Training Epoch: 8/10, step 187/574 completed (loss: 0.25296762585639954, acc: 0.9193245768547058)
[2025-01-06 01:42:20,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:20,877][root][INFO] - Training Epoch: 8/10, step 188/574 completed (loss: 0.27638664841651917, acc: 0.918149471282959)
[2025-01-06 01:42:20,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:21,244][root][INFO] - Training Epoch: 8/10, step 189/574 completed (loss: 0.2854507863521576, acc: 0.9599999785423279)
[2025-01-06 01:42:21,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:21,790][root][INFO] - Training Epoch: 8/10, step 190/574 completed (loss: 0.14251424372196198, acc: 0.9534883499145508)
[2025-01-06 01:42:22,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:22,591][root][INFO] - Training Epoch: 8/10, step 191/574 completed (loss: 0.40436726808547974, acc: 0.8571428656578064)
[2025-01-06 01:42:22,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:23,504][root][INFO] - Training Epoch: 8/10, step 192/574 completed (loss: 0.33113381266593933, acc: 0.8939393758773804)
[2025-01-06 01:42:23,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:24,243][root][INFO] - Training Epoch: 8/10, step 193/574 completed (loss: 0.158001109957695, acc: 0.929411768913269)
[2025-01-06 01:42:24,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:25,315][root][INFO] - Training Epoch: 8/10, step 194/574 completed (loss: 0.47775721549987793, acc: 0.8888888955116272)
[2025-01-06 01:42:25,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:26,267][root][INFO] - Training Epoch: 8/10, step 195/574 completed (loss: 0.04732285067439079, acc: 0.9838709831237793)
[2025-01-06 01:42:26,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:26,623][root][INFO] - Training Epoch: 8/10, step 196/574 completed (loss: 0.003712752368301153, acc: 1.0)
[2025-01-06 01:42:26,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:26,947][root][INFO] - Training Epoch: 8/10, step 197/574 completed (loss: 0.01710064709186554, acc: 1.0)
[2025-01-06 01:42:27,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:27,249][root][INFO] - Training Epoch: 8/10, step 198/574 completed (loss: 0.10972170531749725, acc: 0.970588207244873)
[2025-01-06 01:42:27,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:27,570][root][INFO] - Training Epoch: 8/10, step 199/574 completed (loss: 0.31630071997642517, acc: 0.8897058963775635)
[2025-01-06 01:42:27,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:27,947][root][INFO] - Training Epoch: 8/10, step 200/574 completed (loss: 0.10611085593700409, acc: 0.9745762944221497)
[2025-01-06 01:42:28,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:28,287][root][INFO] - Training Epoch: 8/10, step 201/574 completed (loss: 0.18975886702537537, acc: 0.9328358173370361)
[2025-01-06 01:42:28,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:28,627][root][INFO] - Training Epoch: 8/10, step 202/574 completed (loss: 0.11512531340122223, acc: 0.9417475461959839)
[2025-01-06 01:42:28,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:28,950][root][INFO] - Training Epoch: 8/10, step 203/574 completed (loss: 0.03659132868051529, acc: 1.0)
[2025-01-06 01:42:29,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:29,289][root][INFO] - Training Epoch: 8/10, step 204/574 completed (loss: 0.007752655074000359, acc: 1.0)
[2025-01-06 01:42:29,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:29,626][root][INFO] - Training Epoch: 8/10, step 205/574 completed (loss: 0.09307248145341873, acc: 0.9506726264953613)
[2025-01-06 01:42:29,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:30,017][root][INFO] - Training Epoch: 8/10, step 206/574 completed (loss: 0.14783787727355957, acc: 0.9527559280395508)
[2025-01-06 01:42:30,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:30,353][root][INFO] - Training Epoch: 8/10, step 207/574 completed (loss: 0.09971287846565247, acc: 0.9698275923728943)
[2025-01-06 01:42:30,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:30,722][root][INFO] - Training Epoch: 8/10, step 208/574 completed (loss: 0.23044291138648987, acc: 0.9275362491607666)
[2025-01-06 01:42:30,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31,071][root][INFO] - Training Epoch: 8/10, step 209/574 completed (loss: 0.1094890758395195, acc: 0.9533073902130127)
[2025-01-06 01:42:31,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31,395][root][INFO] - Training Epoch: 8/10, step 210/574 completed (loss: 0.03839975595474243, acc: 0.97826087474823)
[2025-01-06 01:42:31,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31,691][root][INFO] - Training Epoch: 8/10, step 211/574 completed (loss: 0.0091606630012393, acc: 1.0)
[2025-01-06 01:42:31,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31,972][root][INFO] - Training Epoch: 8/10, step 212/574 completed (loss: 0.05004812404513359, acc: 0.9642857313156128)
[2025-01-06 01:42:32,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:32,274][root][INFO] - Training Epoch: 8/10, step 213/574 completed (loss: 0.0277533121407032, acc: 1.0)
[2025-01-06 01:42:32,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:32,986][root][INFO] - Training Epoch: 8/10, step 214/574 completed (loss: 0.07642871141433716, acc: 0.9692307710647583)
[2025-01-06 01:42:33,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:33,311][root][INFO] - Training Epoch: 8/10, step 215/574 completed (loss: 0.05386996641755104, acc: 0.9864864945411682)
[2025-01-06 01:42:33,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:33,617][root][INFO] - Training Epoch: 8/10, step 216/574 completed (loss: 0.006723704747855663, acc: 1.0)
[2025-01-06 01:42:33,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34,148][root][INFO] - Training Epoch: 8/10, step 217/574 completed (loss: 0.13269801437854767, acc: 0.9639639854431152)
[2025-01-06 01:42:34,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34,533][root][INFO] - Training Epoch: 8/10, step 218/574 completed (loss: 0.04397976025938988, acc: 0.9777777791023254)
[2025-01-06 01:42:34,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34,821][root][INFO] - Training Epoch: 8/10, step 219/574 completed (loss: 0.032058145850896835, acc: 1.0)
[2025-01-06 01:42:34,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:35,119][root][INFO] - Training Epoch: 8/10, step 220/574 completed (loss: 0.0005277091986499727, acc: 1.0)
[2025-01-06 01:42:35,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:35,414][root][INFO] - Training Epoch: 8/10, step 221/574 completed (loss: 0.011461309157311916, acc: 1.0)
[2025-01-06 01:42:35,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:35,737][root][INFO] - Training Epoch: 8/10, step 222/574 completed (loss: 0.044275227934122086, acc: 0.9807692170143127)
[2025-01-06 01:42:35,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:36,525][root][INFO] - Training Epoch: 8/10, step 223/574 completed (loss: 0.1021774560213089, acc: 0.9728260636329651)
[2025-01-06 01:42:36,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:37,066][root][INFO] - Training Epoch: 8/10, step 224/574 completed (loss: 0.17797024548053741, acc: 0.9488636255264282)
[2025-01-06 01:42:37,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:37,495][root][INFO] - Training Epoch: 8/10, step 225/574 completed (loss: 0.1999196857213974, acc: 0.9468085169792175)
[2025-01-06 01:42:37,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:37,848][root][INFO] - Training Epoch: 8/10, step 226/574 completed (loss: 0.08776164799928665, acc: 0.9622641801834106)
[2025-01-06 01:42:37,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:38,200][root][INFO] - Training Epoch: 8/10, step 227/574 completed (loss: 0.1660614013671875, acc: 0.9666666388511658)
[2025-01-06 01:42:38,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:38,535][root][INFO] - Training Epoch: 8/10, step 228/574 completed (loss: 0.12453290075063705, acc: 0.9534883499145508)
[2025-01-06 01:42:38,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:38,868][root][INFO] - Training Epoch: 8/10, step 229/574 completed (loss: 0.47861555218696594, acc: 0.8999999761581421)
[2025-01-06 01:42:38,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:39,224][root][INFO] - Training Epoch: 8/10, step 230/574 completed (loss: 0.5375162959098816, acc: 0.8526315689086914)
[2025-01-06 01:42:39,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:39,537][root][INFO] - Training Epoch: 8/10, step 231/574 completed (loss: 0.2739321291446686, acc: 0.9222221970558167)
[2025-01-06 01:42:39,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:39,946][root][INFO] - Training Epoch: 8/10, step 232/574 completed (loss: 0.541994571685791, acc: 0.8222222328186035)
[2025-01-06 01:42:40,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:40,435][root][INFO] - Training Epoch: 8/10, step 233/574 completed (loss: 0.7920323014259338, acc: 0.7706422209739685)
[2025-01-06 01:42:40,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:40,903][root][INFO] - Training Epoch: 8/10, step 234/574 completed (loss: 0.36243465542793274, acc: 0.892307698726654)
[2025-01-06 01:42:40,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:41,217][root][INFO] - Training Epoch: 8/10, step 235/574 completed (loss: 0.005952051375061274, acc: 1.0)
[2025-01-06 01:42:41,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:41,509][root][INFO] - Training Epoch: 8/10, step 236/574 completed (loss: 0.006194393616169691, acc: 1.0)
[2025-01-06 01:42:41,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:41,811][root][INFO] - Training Epoch: 8/10, step 237/574 completed (loss: 0.03253305330872536, acc: 1.0)
[2025-01-06 01:42:41,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:42,120][root][INFO] - Training Epoch: 8/10, step 238/574 completed (loss: 0.16671635210514069, acc: 0.9629629850387573)
[2025-01-06 01:42:42,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:42,412][root][INFO] - Training Epoch: 8/10, step 239/574 completed (loss: 0.02603250928223133, acc: 1.0)
[2025-01-06 01:42:42,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:42,735][root][INFO] - Training Epoch: 8/10, step 240/574 completed (loss: 0.13511845469474792, acc: 0.9545454382896423)
[2025-01-06 01:42:42,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:43,033][root][INFO] - Training Epoch: 8/10, step 241/574 completed (loss: 0.19903001189231873, acc: 0.9772727489471436)
[2025-01-06 01:42:43,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:43,612][root][INFO] - Training Epoch: 8/10, step 242/574 completed (loss: 0.12125416100025177, acc: 0.9838709831237793)
[2025-01-06 01:42:43,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:44,148][root][INFO] - Training Epoch: 8/10, step 243/574 completed (loss: 0.11962330341339111, acc: 0.9772727489471436)
[2025-01-06 01:42:44,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:44,509][root][INFO] - Training Epoch: 8/10, step 244/574 completed (loss: 0.0004790894454345107, acc: 1.0)
[2025-01-06 01:42:44,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:44,853][root][INFO] - Training Epoch: 8/10, step 245/574 completed (loss: 0.10266898572444916, acc: 0.9615384340286255)
[2025-01-06 01:42:44,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:45,187][root][INFO] - Training Epoch: 8/10, step 246/574 completed (loss: 0.013061102479696274, acc: 1.0)
[2025-01-06 01:42:45,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:45,523][root][INFO] - Training Epoch: 8/10, step 247/574 completed (loss: 0.012060213834047318, acc: 1.0)
[2025-01-06 01:42:45,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:45,873][root][INFO] - Training Epoch: 8/10, step 248/574 completed (loss: 0.021615153178572655, acc: 1.0)
[2025-01-06 01:42:45,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:46,188][root][INFO] - Training Epoch: 8/10, step 249/574 completed (loss: 0.020049266517162323, acc: 1.0)
[2025-01-06 01:42:46,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:46,511][root][INFO] - Training Epoch: 8/10, step 250/574 completed (loss: 0.0045240200124681, acc: 1.0)
[2025-01-06 01:42:46,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:46,860][root][INFO] - Training Epoch: 8/10, step 251/574 completed (loss: 0.03893671929836273, acc: 0.9852941036224365)
[2025-01-06 01:42:46,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47,218][root][INFO] - Training Epoch: 8/10, step 252/574 completed (loss: 0.006977087818086147, acc: 1.0)
[2025-01-06 01:42:47,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47,531][root][INFO] - Training Epoch: 8/10, step 253/574 completed (loss: 0.004838174674659967, acc: 1.0)
[2025-01-06 01:42:47,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47,830][root][INFO] - Training Epoch: 8/10, step 254/574 completed (loss: 0.00053030886920169, acc: 1.0)
[2025-01-06 01:42:47,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:48,144][root][INFO] - Training Epoch: 8/10, step 255/574 completed (loss: 0.024779560044407845, acc: 1.0)
[2025-01-06 01:42:48,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:48,454][root][INFO] - Training Epoch: 8/10, step 256/574 completed (loss: 0.0020464416593313217, acc: 1.0)
[2025-01-06 01:42:48,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:48,754][root][INFO] - Training Epoch: 8/10, step 257/574 completed (loss: 0.14156872034072876, acc: 0.9714285731315613)
[2025-01-06 01:42:48,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:49,065][root][INFO] - Training Epoch: 8/10, step 258/574 completed (loss: 0.007149716839194298, acc: 1.0)
[2025-01-06 01:42:49,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:49,642][root][INFO] - Training Epoch: 8/10, step 259/574 completed (loss: 0.11561931669712067, acc: 0.9528301954269409)
[2025-01-06 01:42:49,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:50,221][root][INFO] - Training Epoch: 8/10, step 260/574 completed (loss: 0.08496116101741791, acc: 0.9750000238418579)
[2025-01-06 01:42:50,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:50,526][root][INFO] - Training Epoch: 8/10, step 261/574 completed (loss: 0.007569076493382454, acc: 1.0)
[2025-01-06 01:42:50,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:50,823][root][INFO] - Training Epoch: 8/10, step 262/574 completed (loss: 0.06283717602491379, acc: 0.9677419066429138)
[2025-01-06 01:42:50,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:51,220][root][INFO] - Training Epoch: 8/10, step 263/574 completed (loss: 0.14175420999526978, acc: 0.9466666579246521)
[2025-01-06 01:42:51,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:51,608][root][INFO] - Training Epoch: 8/10, step 264/574 completed (loss: 0.08009471744298935, acc: 0.9791666865348816)
[2025-01-06 01:42:51,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:52,475][root][INFO] - Training Epoch: 8/10, step 265/574 completed (loss: 0.4268088936805725, acc: 0.8320000171661377)
[2025-01-06 01:42:52,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:52,811][root][INFO] - Training Epoch: 8/10, step 266/574 completed (loss: 0.4793510138988495, acc: 0.898876428604126)
[2025-01-06 01:42:52,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:53,149][root][INFO] - Training Epoch: 8/10, step 267/574 completed (loss: 0.12340471148490906, acc: 0.9594594836235046)
[2025-01-06 01:42:53,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:53,599][root][INFO] - Training Epoch: 8/10, step 268/574 completed (loss: 0.1592126488685608, acc: 0.9137930870056152)
[2025-01-06 01:42:53,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:53,891][root][INFO] - Training Epoch: 8/10, step 269/574 completed (loss: 0.0036744363605976105, acc: 1.0)
[2025-01-06 01:42:53,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:54,194][root][INFO] - Training Epoch: 8/10, step 270/574 completed (loss: 0.023534482344985008, acc: 1.0)
[2025-01-06 01:42:54,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:54,492][root][INFO] - Training Epoch: 8/10, step 271/574 completed (loss: 0.0037943816278129816, acc: 1.0)
[2025-01-06 01:42:55,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:55,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:55,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:57,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:57,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:57,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:58,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:58,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:58,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:59,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:59,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:59,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:00,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:00,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:00,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:02,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:02,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:02,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:03,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:03,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:03,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:04,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:04,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:04,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:05,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:05,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:05,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:06,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:06,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:06,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:07,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:07,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:08,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:08,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:08,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:09,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:09,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:09,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:10,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:10,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:10,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:11,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:11,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:11,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:13,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:13,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:13,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:14,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:14,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:14,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:15,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:15,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:16,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:16,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:16,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:17,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:17,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:17,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:18,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:18,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:18,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:19,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:19,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:19,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:20,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:20,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:20,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:21,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:21,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:21,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:22,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:22,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:22,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:23,355][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3515, device='cuda:0') eval_epoch_loss=tensor(0.8551, device='cuda:0') eval_epoch_acc=tensor(0.8251, device='cuda:0')
[2025-01-06 01:43:23,356][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:43:23,356][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:43:23,621][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_272_loss_0.8550664782524109/model.pt
[2025-01-06 01:43:23,626][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:43:23,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:23,983][root][INFO] - Training Epoch: 8/10, step 272/574 completed (loss: 0.0006515998393297195, acc: 1.0)
[2025-01-06 01:43:24,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:24,373][root][INFO] - Training Epoch: 8/10, step 273/574 completed (loss: 0.09177563339471817, acc: 0.9666666388511658)
[2025-01-06 01:43:24,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:24,676][root][INFO] - Training Epoch: 8/10, step 274/574 completed (loss: 0.027822226285934448, acc: 1.0)
[2025-01-06 01:43:24,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:25,016][root][INFO] - Training Epoch: 8/10, step 275/574 completed (loss: 0.06854072213172913, acc: 0.9666666388511658)
[2025-01-06 01:43:25,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:25,354][root][INFO] - Training Epoch: 8/10, step 276/574 completed (loss: 0.07102590799331665, acc: 0.9655172228813171)
[2025-01-06 01:43:25,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:25,801][root][INFO] - Training Epoch: 8/10, step 277/574 completed (loss: 0.015490638092160225, acc: 1.0)
[2025-01-06 01:43:25,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:26,195][root][INFO] - Training Epoch: 8/10, step 278/574 completed (loss: 0.01346651278436184, acc: 1.0)
[2025-01-06 01:43:26,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:26,515][root][INFO] - Training Epoch: 8/10, step 279/574 completed (loss: 0.12021932750940323, acc: 0.9583333134651184)
[2025-01-06 01:43:26,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:26,848][root][INFO] - Training Epoch: 8/10, step 280/574 completed (loss: 0.04517355188727379, acc: 0.9772727489471436)
[2025-01-06 01:43:26,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:27,290][root][INFO] - Training Epoch: 8/10, step 281/574 completed (loss: 0.23913443088531494, acc: 0.9156626462936401)
[2025-01-06 01:43:27,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:27,647][root][INFO] - Training Epoch: 8/10, step 282/574 completed (loss: 0.41727617383003235, acc: 0.9259259104728699)
[2025-01-06 01:43:27,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:27,952][root][INFO] - Training Epoch: 8/10, step 283/574 completed (loss: 0.0020147126633673906, acc: 1.0)
[2025-01-06 01:43:28,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:28,250][root][INFO] - Training Epoch: 8/10, step 284/574 completed (loss: 0.03145173564553261, acc: 1.0)
[2025-01-06 01:43:28,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:28,572][root][INFO] - Training Epoch: 8/10, step 285/574 completed (loss: 0.044633232057094574, acc: 0.9750000238418579)
[2025-01-06 01:43:28,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:28,883][root][INFO] - Training Epoch: 8/10, step 286/574 completed (loss: 0.09787125140428543, acc: 0.96875)
[2025-01-06 01:43:28,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:29,241][root][INFO] - Training Epoch: 8/10, step 287/574 completed (loss: 0.09705469012260437, acc: 0.9599999785423279)
[2025-01-06 01:43:29,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:29,553][root][INFO] - Training Epoch: 8/10, step 288/574 completed (loss: 0.042497240006923676, acc: 0.9890109896659851)
[2025-01-06 01:43:29,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:29,880][root][INFO] - Training Epoch: 8/10, step 289/574 completed (loss: 0.05851555988192558, acc: 0.9689440727233887)
[2025-01-06 01:43:29,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:30,214][root][INFO] - Training Epoch: 8/10, step 290/574 completed (loss: 0.179770365357399, acc: 0.9587628841400146)
[2025-01-06 01:43:30,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:30,568][root][INFO] - Training Epoch: 8/10, step 291/574 completed (loss: 0.001776452292688191, acc: 1.0)
[2025-01-06 01:43:30,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:30,907][root][INFO] - Training Epoch: 8/10, step 292/574 completed (loss: 0.012304826639592648, acc: 1.0)
[2025-01-06 01:43:31,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:31,260][root][INFO] - Training Epoch: 8/10, step 293/574 completed (loss: 0.05550353229045868, acc: 0.982758641242981)
[2025-01-06 01:43:31,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:31,740][root][INFO] - Training Epoch: 8/10, step 294/574 completed (loss: 0.09512338787317276, acc: 0.9636363387107849)
[2025-01-06 01:43:31,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:32,295][root][INFO] - Training Epoch: 8/10, step 295/574 completed (loss: 0.20896457135677338, acc: 0.9432989954948425)
[2025-01-06 01:43:32,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:32,593][root][INFO] - Training Epoch: 8/10, step 296/574 completed (loss: 0.02978205680847168, acc: 1.0)
[2025-01-06 01:43:32,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:32,916][root][INFO] - Training Epoch: 8/10, step 297/574 completed (loss: 0.00850393995642662, acc: 1.0)
[2025-01-06 01:43:33,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:33,290][root][INFO] - Training Epoch: 8/10, step 298/574 completed (loss: 0.010642990469932556, acc: 1.0)
[2025-01-06 01:43:33,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:33,624][root][INFO] - Training Epoch: 8/10, step 299/574 completed (loss: 0.006509265396744013, acc: 1.0)
[2025-01-06 01:43:33,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:34,017][root][INFO] - Training Epoch: 8/10, step 300/574 completed (loss: 0.008511669933795929, acc: 1.0)
[2025-01-06 01:43:34,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:34,375][root][INFO] - Training Epoch: 8/10, step 301/574 completed (loss: 0.03154073655605316, acc: 1.0)
[2025-01-06 01:43:34,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:34,722][root][INFO] - Training Epoch: 8/10, step 302/574 completed (loss: 0.019794290885329247, acc: 1.0)
[2025-01-06 01:43:34,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:35,023][root][INFO] - Training Epoch: 8/10, step 303/574 completed (loss: 0.035710737109184265, acc: 0.970588207244873)
[2025-01-06 01:43:35,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:35,342][root][INFO] - Training Epoch: 8/10, step 304/574 completed (loss: 0.04018844664096832, acc: 0.96875)
[2025-01-06 01:43:35,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:35,685][root][INFO] - Training Epoch: 8/10, step 305/574 completed (loss: 0.08784795552492142, acc: 0.9672130942344666)
[2025-01-06 01:43:35,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:36,020][root][INFO] - Training Epoch: 8/10, step 306/574 completed (loss: 0.0020657412242144346, acc: 1.0)
[2025-01-06 01:43:36,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:36,328][root][INFO] - Training Epoch: 8/10, step 307/574 completed (loss: 0.0005579013377428055, acc: 1.0)
[2025-01-06 01:43:36,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:36,623][root][INFO] - Training Epoch: 8/10, step 308/574 completed (loss: 0.12216682732105255, acc: 0.9710144996643066)
[2025-01-06 01:43:36,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:37,056][root][INFO] - Training Epoch: 8/10, step 309/574 completed (loss: 0.04133713245391846, acc: 0.9722222089767456)
[2025-01-06 01:43:37,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:37,381][root][INFO] - Training Epoch: 8/10, step 310/574 completed (loss: 0.011366677470505238, acc: 1.0)
[2025-01-06 01:43:37,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:37,701][root][INFO] - Training Epoch: 8/10, step 311/574 completed (loss: 0.035618364810943604, acc: 0.9871794581413269)
[2025-01-06 01:43:37,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:38,068][root][INFO] - Training Epoch: 8/10, step 312/574 completed (loss: 0.004934428259730339, acc: 1.0)
[2025-01-06 01:43:38,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:38,379][root][INFO] - Training Epoch: 8/10, step 313/574 completed (loss: 0.0004294599639251828, acc: 1.0)
[2025-01-06 01:43:38,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:38,717][root][INFO] - Training Epoch: 8/10, step 314/574 completed (loss: 0.0015623763902112842, acc: 1.0)
[2025-01-06 01:43:38,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:39,056][root][INFO] - Training Epoch: 8/10, step 315/574 completed (loss: 0.05105762183666229, acc: 0.9677419066429138)
[2025-01-06 01:43:39,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:39,388][root][INFO] - Training Epoch: 8/10, step 316/574 completed (loss: 0.19400601089000702, acc: 0.9677419066429138)
[2025-01-06 01:43:39,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:39,728][root][INFO] - Training Epoch: 8/10, step 317/574 completed (loss: 0.014615724794566631, acc: 1.0)
[2025-01-06 01:43:39,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:40,050][root][INFO] - Training Epoch: 8/10, step 318/574 completed (loss: 0.024163778871297836, acc: 0.9903846383094788)
[2025-01-06 01:43:40,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:40,381][root][INFO] - Training Epoch: 8/10, step 319/574 completed (loss: 0.06277136504650116, acc: 0.9555555582046509)
[2025-01-06 01:43:40,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:40,714][root][INFO] - Training Epoch: 8/10, step 320/574 completed (loss: 0.008830498903989792, acc: 1.0)
[2025-01-06 01:43:40,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41,025][root][INFO] - Training Epoch: 8/10, step 321/574 completed (loss: 0.009697643108665943, acc: 1.0)
[2025-01-06 01:43:41,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41,343][root][INFO] - Training Epoch: 8/10, step 322/574 completed (loss: 0.0339345820248127, acc: 1.0)
[2025-01-06 01:43:41,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41,655][root][INFO] - Training Epoch: 8/10, step 323/574 completed (loss: 0.09869298338890076, acc: 0.9142857193946838)
[2025-01-06 01:43:41,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41,998][root][INFO] - Training Epoch: 8/10, step 324/574 completed (loss: 0.12093450874090195, acc: 1.0)
[2025-01-06 01:43:42,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:42,294][root][INFO] - Training Epoch: 8/10, step 325/574 completed (loss: 0.23275557160377502, acc: 0.9024389982223511)
[2025-01-06 01:43:42,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:42,602][root][INFO] - Training Epoch: 8/10, step 326/574 completed (loss: 0.18818853795528412, acc: 0.9473684430122375)
[2025-01-06 01:43:42,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:42,933][root][INFO] - Training Epoch: 8/10, step 327/574 completed (loss: 0.046854302287101746, acc: 1.0)
[2025-01-06 01:43:43,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:43,261][root][INFO] - Training Epoch: 8/10, step 328/574 completed (loss: 0.0005373828462325037, acc: 1.0)
[2025-01-06 01:43:43,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:43,611][root][INFO] - Training Epoch: 8/10, step 329/574 completed (loss: 0.023003557696938515, acc: 1.0)
[2025-01-06 01:43:43,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:43,955][root][INFO] - Training Epoch: 8/10, step 330/574 completed (loss: 0.09334233403205872, acc: 0.96875)
[2025-01-06 01:43:44,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:44,268][root][INFO] - Training Epoch: 8/10, step 331/574 completed (loss: 0.022227060049772263, acc: 1.0)
[2025-01-06 01:43:44,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:44,641][root][INFO] - Training Epoch: 8/10, step 332/574 completed (loss: 0.04546278342604637, acc: 0.9824561476707458)
[2025-01-06 01:43:44,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:44,956][root][INFO] - Training Epoch: 8/10, step 333/574 completed (loss: 0.017215630039572716, acc: 1.0)
[2025-01-06 01:43:45,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:45,283][root][INFO] - Training Epoch: 8/10, step 334/574 completed (loss: 0.0011172355152666569, acc: 1.0)
[2025-01-06 01:43:45,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:45,596][root][INFO] - Training Epoch: 8/10, step 335/574 completed (loss: 0.0006876468542031944, acc: 1.0)
[2025-01-06 01:43:45,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:45,973][root][INFO] - Training Epoch: 8/10, step 336/574 completed (loss: 0.05351201072335243, acc: 0.9800000190734863)
[2025-01-06 01:43:46,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:46,385][root][INFO] - Training Epoch: 8/10, step 337/574 completed (loss: 0.17982877790927887, acc: 0.931034505367279)
[2025-01-06 01:43:46,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:46,750][root][INFO] - Training Epoch: 8/10, step 338/574 completed (loss: 0.22087684273719788, acc: 0.8829787373542786)
[2025-01-06 01:43:46,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:47,075][root][INFO] - Training Epoch: 8/10, step 339/574 completed (loss: 0.2607513666152954, acc: 0.9277108311653137)
[2025-01-06 01:43:47,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:47,371][root][INFO] - Training Epoch: 8/10, step 340/574 completed (loss: 0.0009600886260159314, acc: 1.0)
[2025-01-06 01:43:47,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:47,725][root][INFO] - Training Epoch: 8/10, step 341/574 completed (loss: 0.022765086963772774, acc: 1.0)
[2025-01-06 01:43:47,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:48,105][root][INFO] - Training Epoch: 8/10, step 342/574 completed (loss: 0.29022088646888733, acc: 0.9277108311653137)
[2025-01-06 01:43:48,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:48,443][root][INFO] - Training Epoch: 8/10, step 343/574 completed (loss: 0.046528469771146774, acc: 0.9811320900917053)
[2025-01-06 01:43:48,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:48,770][root][INFO] - Training Epoch: 8/10, step 344/574 completed (loss: 0.0056954496540129185, acc: 1.0)
[2025-01-06 01:43:48,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:49,086][root][INFO] - Training Epoch: 8/10, step 345/574 completed (loss: 0.002555759157985449, acc: 1.0)
[2025-01-06 01:43:49,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:49,461][root][INFO] - Training Epoch: 8/10, step 346/574 completed (loss: 0.04482788220047951, acc: 0.9850746393203735)
[2025-01-06 01:43:49,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:49,804][root][INFO] - Training Epoch: 8/10, step 347/574 completed (loss: 0.38232332468032837, acc: 0.949999988079071)
[2025-01-06 01:43:49,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:50,139][root][INFO] - Training Epoch: 8/10, step 348/574 completed (loss: 0.003446233458817005, acc: 1.0)
[2025-01-06 01:43:50,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:50,545][root][INFO] - Training Epoch: 8/10, step 349/574 completed (loss: 0.17171329259872437, acc: 0.9166666865348816)
[2025-01-06 01:43:50,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:50,911][root][INFO] - Training Epoch: 8/10, step 350/574 completed (loss: 0.16989606618881226, acc: 0.9767441749572754)
[2025-01-06 01:43:51,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:51,276][root][INFO] - Training Epoch: 8/10, step 351/574 completed (loss: 0.0031643761321902275, acc: 1.0)
[2025-01-06 01:43:51,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:51,655][root][INFO] - Training Epoch: 8/10, step 352/574 completed (loss: 0.0652906522154808, acc: 0.9555555582046509)
[2025-01-06 01:43:51,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:51,963][root][INFO] - Training Epoch: 8/10, step 353/574 completed (loss: 0.0037087840028107166, acc: 1.0)
[2025-01-06 01:43:52,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:52,268][root][INFO] - Training Epoch: 8/10, step 354/574 completed (loss: 0.017571208998560905, acc: 1.0)
[2025-01-06 01:43:52,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:52,604][root][INFO] - Training Epoch: 8/10, step 355/574 completed (loss: 0.12127593159675598, acc: 0.9670329689979553)
[2025-01-06 01:43:52,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:53,106][root][INFO] - Training Epoch: 8/10, step 356/574 completed (loss: 0.10596702992916107, acc: 0.95652174949646)
[2025-01-06 01:43:53,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:53,441][root][INFO] - Training Epoch: 8/10, step 357/574 completed (loss: 0.08839039504528046, acc: 0.95652174949646)
[2025-01-06 01:43:53,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:53,787][root][INFO] - Training Epoch: 8/10, step 358/574 completed (loss: 0.017104635015130043, acc: 1.0)
[2025-01-06 01:43:53,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:54,090][root][INFO] - Training Epoch: 8/10, step 359/574 completed (loss: 0.0009854643139988184, acc: 1.0)
[2025-01-06 01:43:54,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:54,443][root][INFO] - Training Epoch: 8/10, step 360/574 completed (loss: 0.006582377012819052, acc: 1.0)
[2025-01-06 01:43:54,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:54,825][root][INFO] - Training Epoch: 8/10, step 361/574 completed (loss: 0.07277669757604599, acc: 0.9756097793579102)
[2025-01-06 01:43:54,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:55,180][root][INFO] - Training Epoch: 8/10, step 362/574 completed (loss: 0.14718832075595856, acc: 0.9777777791023254)
[2025-01-06 01:43:55,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:55,571][root][INFO] - Training Epoch: 8/10, step 363/574 completed (loss: 0.015062997117638588, acc: 1.0)
[2025-01-06 01:43:55,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:55,968][root][INFO] - Training Epoch: 8/10, step 364/574 completed (loss: 0.2338636964559555, acc: 0.9268292784690857)
[2025-01-06 01:43:56,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:56,307][root][INFO] - Training Epoch: 8/10, step 365/574 completed (loss: 0.027227139100432396, acc: 1.0)
[2025-01-06 01:43:56,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:56,607][root][INFO] - Training Epoch: 8/10, step 366/574 completed (loss: 0.018694687634706497, acc: 1.0)
[2025-01-06 01:43:56,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:56,907][root][INFO] - Training Epoch: 8/10, step 367/574 completed (loss: 0.0016755079850554466, acc: 1.0)
[2025-01-06 01:43:56,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:57,203][root][INFO] - Training Epoch: 8/10, step 368/574 completed (loss: 0.012827148661017418, acc: 1.0)
[2025-01-06 01:43:57,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:57,520][root][INFO] - Training Epoch: 8/10, step 369/574 completed (loss: 0.004325381480157375, acc: 1.0)
[2025-01-06 01:43:57,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:58,163][root][INFO] - Training Epoch: 8/10, step 370/574 completed (loss: 0.26740702986717224, acc: 0.9090909361839294)
[2025-01-06 01:43:58,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:59,021][root][INFO] - Training Epoch: 8/10, step 371/574 completed (loss: 0.06643573939800262, acc: 0.9716981053352356)
[2025-01-06 01:43:59,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:59,404][root][INFO] - Training Epoch: 8/10, step 372/574 completed (loss: 0.026720557361841202, acc: 0.9888888597488403)
[2025-01-06 01:43:59,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:59,745][root][INFO] - Training Epoch: 8/10, step 373/574 completed (loss: 0.07776711136102676, acc: 0.9821428656578064)
[2025-01-06 01:43:59,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:00,077][root][INFO] - Training Epoch: 8/10, step 374/574 completed (loss: 0.02193385176360607, acc: 0.9714285731315613)
[2025-01-06 01:44:00,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:00,391][root][INFO] - Training Epoch: 8/10, step 375/574 completed (loss: 0.011318780481815338, acc: 1.0)
[2025-01-06 01:44:00,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:00,706][root][INFO] - Training Epoch: 8/10, step 376/574 completed (loss: 0.0005035865469835699, acc: 1.0)
[2025-01-06 01:44:00,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:01,093][root][INFO] - Training Epoch: 8/10, step 377/574 completed (loss: 0.014661087654531002, acc: 1.0)
[2025-01-06 01:44:01,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:01,466][root][INFO] - Training Epoch: 8/10, step 378/574 completed (loss: 0.024779262021183968, acc: 0.9894737005233765)
[2025-01-06 01:44:01,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:02,059][root][INFO] - Training Epoch: 8/10, step 379/574 completed (loss: 0.09616132080554962, acc: 0.976047933101654)
[2025-01-06 01:44:02,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:02,473][root][INFO] - Training Epoch: 8/10, step 380/574 completed (loss: 0.12678015232086182, acc: 0.9624060392379761)
[2025-01-06 01:44:02,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:03,788][root][INFO] - Training Epoch: 8/10, step 381/574 completed (loss: 0.17245809733867645, acc: 0.9465240836143494)
[2025-01-06 01:44:03,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:04,355][root][INFO] - Training Epoch: 8/10, step 382/574 completed (loss: 0.02205885760486126, acc: 0.9909909963607788)
[2025-01-06 01:44:04,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:04,713][root][INFO] - Training Epoch: 8/10, step 383/574 completed (loss: 0.010326902382075787, acc: 1.0)
[2025-01-06 01:44:04,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:05,116][root][INFO] - Training Epoch: 8/10, step 384/574 completed (loss: 0.0024580468889325857, acc: 1.0)
[2025-01-06 01:44:05,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:05,461][root][INFO] - Training Epoch: 8/10, step 385/574 completed (loss: 0.01523963175714016, acc: 1.0)
[2025-01-06 01:44:05,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:05,801][root][INFO] - Training Epoch: 8/10, step 386/574 completed (loss: 0.00043740789988078177, acc: 1.0)
[2025-01-06 01:44:05,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:06,136][root][INFO] - Training Epoch: 8/10, step 387/574 completed (loss: 0.014054905623197556, acc: 1.0)
[2025-01-06 01:44:06,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:06,461][root][INFO] - Training Epoch: 8/10, step 388/574 completed (loss: 0.0004145036218687892, acc: 1.0)
[2025-01-06 01:44:06,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:06,831][root][INFO] - Training Epoch: 8/10, step 389/574 completed (loss: 0.002900910098105669, acc: 1.0)
[2025-01-06 01:44:06,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:07,146][root][INFO] - Training Epoch: 8/10, step 390/574 completed (loss: 0.06288546323776245, acc: 0.9523809552192688)
[2025-01-06 01:44:07,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:07,463][root][INFO] - Training Epoch: 8/10, step 391/574 completed (loss: 0.0701407864689827, acc: 1.0)
[2025-01-06 01:44:07,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:07,807][root][INFO] - Training Epoch: 8/10, step 392/574 completed (loss: 0.2431638240814209, acc: 0.9223300814628601)
[2025-01-06 01:44:07,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:08,364][root][INFO] - Training Epoch: 8/10, step 393/574 completed (loss: 0.25913214683532715, acc: 0.8823529481887817)
[2025-01-06 01:44:08,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:08,757][root][INFO] - Training Epoch: 8/10, step 394/574 completed (loss: 0.24515947699546814, acc: 0.9200000166893005)
[2025-01-06 01:44:08,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:09,137][root][INFO] - Training Epoch: 8/10, step 395/574 completed (loss: 0.1423952579498291, acc: 0.9513888955116272)
[2025-01-06 01:44:09,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:09,446][root][INFO] - Training Epoch: 8/10, step 396/574 completed (loss: 0.04373763129115105, acc: 0.9767441749572754)
[2025-01-06 01:44:09,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:09,741][root][INFO] - Training Epoch: 8/10, step 397/574 completed (loss: 0.01368799526244402, acc: 1.0)
[2025-01-06 01:44:09,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10,061][root][INFO] - Training Epoch: 8/10, step 398/574 completed (loss: 0.018073678016662598, acc: 1.0)
[2025-01-06 01:44:10,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10,360][root][INFO] - Training Epoch: 8/10, step 399/574 completed (loss: 0.0008414802723564208, acc: 1.0)
[2025-01-06 01:44:10,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10,940][root][INFO] - Training Epoch: 8/10, step 400/574 completed (loss: 0.024734975770115852, acc: 1.0)
[2025-01-06 01:44:11,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:11,345][root][INFO] - Training Epoch: 8/10, step 401/574 completed (loss: 0.0583343431353569, acc: 0.9866666793823242)
[2025-01-06 01:44:11,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:11,699][root][INFO] - Training Epoch: 8/10, step 402/574 completed (loss: 0.004743139259517193, acc: 1.0)
[2025-01-06 01:44:11,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12,022][root][INFO] - Training Epoch: 8/10, step 403/574 completed (loss: 0.21513113379478455, acc: 0.9696969985961914)
[2025-01-06 01:44:12,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12,355][root][INFO] - Training Epoch: 8/10, step 404/574 completed (loss: 0.023579007014632225, acc: 1.0)
[2025-01-06 01:44:12,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12,747][root][INFO] - Training Epoch: 8/10, step 405/574 completed (loss: 0.0005718565080314875, acc: 1.0)
[2025-01-06 01:44:12,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:13,073][root][INFO] - Training Epoch: 8/10, step 406/574 completed (loss: 0.00037468699156306684, acc: 1.0)
[2025-01-06 01:44:13,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:13,389][root][INFO] - Training Epoch: 8/10, step 407/574 completed (loss: 0.0007783980690874159, acc: 1.0)
[2025-01-06 01:44:13,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:13,692][root][INFO] - Training Epoch: 8/10, step 408/574 completed (loss: 0.004143433645367622, acc: 1.0)
[2025-01-06 01:44:13,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:14,003][root][INFO] - Training Epoch: 8/10, step 409/574 completed (loss: 0.0013068022672086954, acc: 1.0)
[2025-01-06 01:44:14,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:14,374][root][INFO] - Training Epoch: 8/10, step 410/574 completed (loss: 0.005224870517849922, acc: 1.0)
[2025-01-06 01:44:14,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:14,761][root][INFO] - Training Epoch: 8/10, step 411/574 completed (loss: 0.00621033413335681, acc: 1.0)
[2025-01-06 01:44:14,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:15,121][root][INFO] - Training Epoch: 8/10, step 412/574 completed (loss: 0.00062104023527354, acc: 1.0)
[2025-01-06 01:44:15,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:15,496][root][INFO] - Training Epoch: 8/10, step 413/574 completed (loss: 0.016679586842656136, acc: 1.0)
[2025-01-06 01:44:15,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:15,818][root][INFO] - Training Epoch: 8/10, step 414/574 completed (loss: 0.0019784022588282824, acc: 1.0)
[2025-01-06 01:44:16,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:16,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:18,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:18,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:18,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:19,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:19,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:19,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:20,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:20,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:20,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:21,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:21,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:22,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:22,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:22,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:23,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:23,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:23,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:24,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:24,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:26,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:26,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:26,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:27,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:27,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:27,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:28,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:28,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:28,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:29,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:29,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:29,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:30,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:30,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:31,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:31,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:31,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:32,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:32,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:32,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:33,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:33,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:36,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:36,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:37,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:37,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:38,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:38,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:38,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:39,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:39,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:39,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:40,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:40,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:40,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:41,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:41,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:41,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:42,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:42,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:42,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:43,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:43,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:43,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:44,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:44,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:44,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:45,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:45,865][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4004, device='cuda:0') eval_epoch_loss=tensor(0.8756, device='cuda:0') eval_epoch_acc=tensor(0.8333, device='cuda:0')
[2025-01-06 01:44:45,866][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:44:45,866][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:44:46,126][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_415_loss_0.875615119934082/model.pt
[2025-01-06 01:44:46,137][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:44:46,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:46,587][root][INFO] - Training Epoch: 8/10, step 415/574 completed (loss: 0.05253737419843674, acc: 0.9803921580314636)
[2025-01-06 01:44:46,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:46,933][root][INFO] - Training Epoch: 8/10, step 416/574 completed (loss: 0.01548805832862854, acc: 1.0)
[2025-01-06 01:44:47,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:47,307][root][INFO] - Training Epoch: 8/10, step 417/574 completed (loss: 0.010564562864601612, acc: 1.0)
[2025-01-06 01:44:47,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:47,673][root][INFO] - Training Epoch: 8/10, step 418/574 completed (loss: 0.003922495059669018, acc: 1.0)
[2025-01-06 01:44:47,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:48,013][root][INFO] - Training Epoch: 8/10, step 419/574 completed (loss: 0.004616984631866217, acc: 1.0)
[2025-01-06 01:44:48,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:48,331][root][INFO] - Training Epoch: 8/10, step 420/574 completed (loss: 0.013904799707233906, acc: 1.0)
[2025-01-06 01:44:48,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:48,669][root][INFO] - Training Epoch: 8/10, step 421/574 completed (loss: 0.0031020641326904297, acc: 1.0)
[2025-01-06 01:44:48,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:49,024][root][INFO] - Training Epoch: 8/10, step 422/574 completed (loss: 0.10857438296079636, acc: 0.96875)
[2025-01-06 01:44:49,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:49,393][root][INFO] - Training Epoch: 8/10, step 423/574 completed (loss: 0.028919700533151627, acc: 1.0)
[2025-01-06 01:44:49,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:49,731][root][INFO] - Training Epoch: 8/10, step 424/574 completed (loss: 0.005413890816271305, acc: 1.0)
[2025-01-06 01:44:49,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:50,093][root][INFO] - Training Epoch: 8/10, step 425/574 completed (loss: 0.007981711998581886, acc: 1.0)
[2025-01-06 01:44:50,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:50,458][root][INFO] - Training Epoch: 8/10, step 426/574 completed (loss: 0.0015813293866813183, acc: 1.0)
[2025-01-06 01:44:50,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:50,815][root][INFO] - Training Epoch: 8/10, step 427/574 completed (loss: 0.12966422736644745, acc: 0.9729729890823364)
[2025-01-06 01:44:50,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:51,162][root][INFO] - Training Epoch: 8/10, step 428/574 completed (loss: 0.009132716804742813, acc: 1.0)
[2025-01-06 01:44:51,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:51,521][root][INFO] - Training Epoch: 8/10, step 429/574 completed (loss: 0.0019295752281323075, acc: 1.0)
[2025-01-06 01:44:51,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:51,860][root][INFO] - Training Epoch: 8/10, step 430/574 completed (loss: 0.0001374860294163227, acc: 1.0)
[2025-01-06 01:44:51,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:52,204][root][INFO] - Training Epoch: 8/10, step 431/574 completed (loss: 0.0003038463764823973, acc: 1.0)
[2025-01-06 01:44:52,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:52,558][root][INFO] - Training Epoch: 8/10, step 432/574 completed (loss: 0.0008451014873571694, acc: 1.0)
[2025-01-06 01:44:52,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:52,930][root][INFO] - Training Epoch: 8/10, step 433/574 completed (loss: 0.017629636451601982, acc: 1.0)
[2025-01-06 01:44:53,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:53,237][root][INFO] - Training Epoch: 8/10, step 434/574 completed (loss: 0.000318007922032848, acc: 1.0)
[2025-01-06 01:44:53,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:53,587][root][INFO] - Training Epoch: 8/10, step 435/574 completed (loss: 0.004199683200567961, acc: 1.0)
[2025-01-06 01:44:53,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:53,929][root][INFO] - Training Epoch: 8/10, step 436/574 completed (loss: 0.007366574369370937, acc: 1.0)
[2025-01-06 01:44:54,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:54,278][root][INFO] - Training Epoch: 8/10, step 437/574 completed (loss: 0.001219998230226338, acc: 1.0)
[2025-01-06 01:44:54,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:54,606][root][INFO] - Training Epoch: 8/10, step 438/574 completed (loss: 0.0003927531943190843, acc: 1.0)
[2025-01-06 01:44:54,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:54,955][root][INFO] - Training Epoch: 8/10, step 439/574 completed (loss: 0.008834217675030231, acc: 1.0)
[2025-01-06 01:44:55,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:55,438][root][INFO] - Training Epoch: 8/10, step 440/574 completed (loss: 0.12383202463388443, acc: 0.939393937587738)
[2025-01-06 01:44:55,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:56,137][root][INFO] - Training Epoch: 8/10, step 441/574 completed (loss: 0.1869276762008667, acc: 0.9279999732971191)
[2025-01-06 01:44:56,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:56,556][root][INFO] - Training Epoch: 8/10, step 442/574 completed (loss: 0.18176525831222534, acc: 0.9354838728904724)
[2025-01-06 01:44:56,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:57,235][root][INFO] - Training Epoch: 8/10, step 443/574 completed (loss: 0.21001669764518738, acc: 0.9452736377716064)
[2025-01-06 01:44:57,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:57,557][root][INFO] - Training Epoch: 8/10, step 444/574 completed (loss: 0.012197710573673248, acc: 1.0)
[2025-01-06 01:44:57,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:57,977][root][INFO] - Training Epoch: 8/10, step 445/574 completed (loss: 0.010095751844346523, acc: 1.0)
[2025-01-06 01:44:58,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:58,263][root][INFO] - Training Epoch: 8/10, step 446/574 completed (loss: 0.0009599950863048434, acc: 1.0)
[2025-01-06 01:44:58,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:58,556][root][INFO] - Training Epoch: 8/10, step 447/574 completed (loss: 0.0019823883194476366, acc: 1.0)
[2025-01-06 01:44:58,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:58,858][root][INFO] - Training Epoch: 8/10, step 448/574 completed (loss: 0.0024998653680086136, acc: 1.0)
[2025-01-06 01:44:58,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:59,187][root][INFO] - Training Epoch: 8/10, step 449/574 completed (loss: 0.043217483907938004, acc: 0.9850746393203735)
[2025-01-06 01:44:59,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:59,512][root][INFO] - Training Epoch: 8/10, step 450/574 completed (loss: 0.013749532401561737, acc: 1.0)
[2025-01-06 01:44:59,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:59,854][root][INFO] - Training Epoch: 8/10, step 451/574 completed (loss: 0.004992376081645489, acc: 1.0)
[2025-01-06 01:44:59,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:00,227][root][INFO] - Training Epoch: 8/10, step 452/574 completed (loss: 0.009993808344006538, acc: 1.0)
[2025-01-06 01:45:00,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:00,577][root][INFO] - Training Epoch: 8/10, step 453/574 completed (loss: 0.014624533243477345, acc: 1.0)
[2025-01-06 01:45:00,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:00,888][root][INFO] - Training Epoch: 8/10, step 454/574 completed (loss: 0.020113827660679817, acc: 0.9795918464660645)
[2025-01-06 01:45:00,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:01,175][root][INFO] - Training Epoch: 8/10, step 455/574 completed (loss: 0.013709623366594315, acc: 1.0)
[2025-01-06 01:45:01,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:01,505][root][INFO] - Training Epoch: 8/10, step 456/574 completed (loss: 0.1374206393957138, acc: 0.969072163105011)
[2025-01-06 01:45:01,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:01,854][root][INFO] - Training Epoch: 8/10, step 457/574 completed (loss: 0.002549816621467471, acc: 1.0)
[2025-01-06 01:45:01,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:02,251][root][INFO] - Training Epoch: 8/10, step 458/574 completed (loss: 0.06214716657996178, acc: 0.9883720874786377)
[2025-01-06 01:45:02,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:02,550][root][INFO] - Training Epoch: 8/10, step 459/574 completed (loss: 0.027819517999887466, acc: 0.9821428656578064)
[2025-01-06 01:45:02,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:02,869][root][INFO] - Training Epoch: 8/10, step 460/574 completed (loss: 0.019974209368228912, acc: 1.0)
[2025-01-06 01:45:02,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:03,185][root][INFO] - Training Epoch: 8/10, step 461/574 completed (loss: 0.02112283557653427, acc: 1.0)
[2025-01-06 01:45:03,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:03,496][root][INFO] - Training Epoch: 8/10, step 462/574 completed (loss: 0.0010306001640856266, acc: 1.0)
[2025-01-06 01:45:03,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:03,811][root][INFO] - Training Epoch: 8/10, step 463/574 completed (loss: 0.0010599142406135798, acc: 1.0)
[2025-01-06 01:45:03,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:04,157][root][INFO] - Training Epoch: 8/10, step 464/574 completed (loss: 0.002950676716864109, acc: 1.0)
[2025-01-06 01:45:04,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:04,477][root][INFO] - Training Epoch: 8/10, step 465/574 completed (loss: 0.01738390140235424, acc: 0.988095223903656)
[2025-01-06 01:45:04,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:04,799][root][INFO] - Training Epoch: 8/10, step 466/574 completed (loss: 0.13364413380622864, acc: 0.9518072009086609)
[2025-01-06 01:45:04,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:05,137][root][INFO] - Training Epoch: 8/10, step 467/574 completed (loss: 0.016371294856071472, acc: 1.0)
[2025-01-06 01:45:05,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:05,475][root][INFO] - Training Epoch: 8/10, step 468/574 completed (loss: 0.13412582874298096, acc: 0.9611650705337524)
[2025-01-06 01:45:05,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:05,794][root][INFO] - Training Epoch: 8/10, step 469/574 completed (loss: 0.04342063143849373, acc: 0.9918699264526367)
[2025-01-06 01:45:05,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:06,100][root][INFO] - Training Epoch: 8/10, step 470/574 completed (loss: 0.008276172913610935, acc: 1.0)
[2025-01-06 01:45:06,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:06,523][root][INFO] - Training Epoch: 8/10, step 471/574 completed (loss: 0.17591747641563416, acc: 0.9285714030265808)
[2025-01-06 01:45:06,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:06,954][root][INFO] - Training Epoch: 8/10, step 472/574 completed (loss: 0.14273284375667572, acc: 0.9509803652763367)
[2025-01-06 01:45:07,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:07,316][root][INFO] - Training Epoch: 8/10, step 473/574 completed (loss: 0.21040169894695282, acc: 0.9388646483421326)
[2025-01-06 01:45:07,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:07,627][root][INFO] - Training Epoch: 8/10, step 474/574 completed (loss: 0.06255025416612625, acc: 0.9791666865348816)
[2025-01-06 01:45:07,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:07,943][root][INFO] - Training Epoch: 8/10, step 475/574 completed (loss: 0.05578870698809624, acc: 0.9815950989723206)
[2025-01-06 01:45:08,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:08,253][root][INFO] - Training Epoch: 8/10, step 476/574 completed (loss: 0.07703353464603424, acc: 0.971222996711731)
[2025-01-06 01:45:08,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:08,600][root][INFO] - Training Epoch: 8/10, step 477/574 completed (loss: 0.16522136330604553, acc: 0.9396985173225403)
[2025-01-06 01:45:08,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:08,906][root][INFO] - Training Epoch: 8/10, step 478/574 completed (loss: 0.11545904725790024, acc: 0.9722222089767456)
[2025-01-06 01:45:08,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:09,219][root][INFO] - Training Epoch: 8/10, step 479/574 completed (loss: 0.024220174178481102, acc: 1.0)
[2025-01-06 01:45:09,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:09,589][root][INFO] - Training Epoch: 8/10, step 480/574 completed (loss: 0.2488476037979126, acc: 0.9629629850387573)
[2025-01-06 01:45:09,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:09,992][root][INFO] - Training Epoch: 8/10, step 481/574 completed (loss: 0.10975410789251328, acc: 0.949999988079071)
[2025-01-06 01:45:10,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:10,339][root][INFO] - Training Epoch: 8/10, step 482/574 completed (loss: 0.08501529693603516, acc: 1.0)
[2025-01-06 01:45:10,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:10,780][root][INFO] - Training Epoch: 8/10, step 483/574 completed (loss: 0.3634440302848816, acc: 0.9482758641242981)
[2025-01-06 01:45:10,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:11,161][root][INFO] - Training Epoch: 8/10, step 484/574 completed (loss: 0.0023318594321608543, acc: 1.0)
[2025-01-06 01:45:11,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:11,517][root][INFO] - Training Epoch: 8/10, step 485/574 completed (loss: 0.010384238325059414, acc: 1.0)
[2025-01-06 01:45:11,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:11,835][root][INFO] - Training Epoch: 8/10, step 486/574 completed (loss: 0.0029422107618302107, acc: 1.0)
[2025-01-06 01:45:11,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:12,145][root][INFO] - Training Epoch: 8/10, step 487/574 completed (loss: 0.06781740486621857, acc: 0.9523809552192688)
[2025-01-06 01:45:12,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:12,453][root][INFO] - Training Epoch: 8/10, step 488/574 completed (loss: 0.04446737840771675, acc: 1.0)
[2025-01-06 01:45:12,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:12,820][root][INFO] - Training Epoch: 8/10, step 489/574 completed (loss: 0.17782460153102875, acc: 0.9692307710647583)
[2025-01-06 01:45:12,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:13,170][root][INFO] - Training Epoch: 8/10, step 490/574 completed (loss: 0.0041846176609396935, acc: 1.0)
[2025-01-06 01:45:13,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:13,489][root][INFO] - Training Epoch: 8/10, step 491/574 completed (loss: 0.0087058050557971, acc: 1.0)
[2025-01-06 01:45:13,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:13,797][root][INFO] - Training Epoch: 8/10, step 492/574 completed (loss: 0.07996616512537003, acc: 0.9607843160629272)
[2025-01-06 01:45:13,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:14,131][root][INFO] - Training Epoch: 8/10, step 493/574 completed (loss: 0.026258382946252823, acc: 1.0)
[2025-01-06 01:45:14,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:14,451][root][INFO] - Training Epoch: 8/10, step 494/574 completed (loss: 0.4085206091403961, acc: 0.9473684430122375)
[2025-01-06 01:45:14,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:14,749][root][INFO] - Training Epoch: 8/10, step 495/574 completed (loss: 0.013150992803275585, acc: 1.0)
[2025-01-06 01:45:14,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:15,122][root][INFO] - Training Epoch: 8/10, step 496/574 completed (loss: 0.22549569606781006, acc: 0.9285714030265808)
[2025-01-06 01:45:15,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:15,491][root][INFO] - Training Epoch: 8/10, step 497/574 completed (loss: 0.12532104551792145, acc: 0.9438202381134033)
[2025-01-06 01:45:15,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:15,828][root][INFO] - Training Epoch: 8/10, step 498/574 completed (loss: 0.1041581779718399, acc: 0.966292142868042)
[2025-01-06 01:45:15,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:16,197][root][INFO] - Training Epoch: 8/10, step 499/574 completed (loss: 0.3128999173641205, acc: 0.9078013896942139)
[2025-01-06 01:45:16,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:16,523][root][INFO] - Training Epoch: 8/10, step 500/574 completed (loss: 0.11902116984128952, acc: 0.967391312122345)
[2025-01-06 01:45:16,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:16,844][root][INFO] - Training Epoch: 8/10, step 501/574 completed (loss: 0.001201135921292007, acc: 1.0)
[2025-01-06 01:45:16,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:17,189][root][INFO] - Training Epoch: 8/10, step 502/574 completed (loss: 0.04206934943795204, acc: 0.9615384340286255)
[2025-01-06 01:45:17,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:17,512][root][INFO] - Training Epoch: 8/10, step 503/574 completed (loss: 0.015259206295013428, acc: 1.0)
[2025-01-06 01:45:17,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:17,822][root][INFO] - Training Epoch: 8/10, step 504/574 completed (loss: 0.002767583355307579, acc: 1.0)
[2025-01-06 01:45:17,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:18,170][root][INFO] - Training Epoch: 8/10, step 505/574 completed (loss: 0.12816829979419708, acc: 0.9433962106704712)
[2025-01-06 01:45:18,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:18,489][root][INFO] - Training Epoch: 8/10, step 506/574 completed (loss: 0.15372127294540405, acc: 0.9655172228813171)
[2025-01-06 01:45:18,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:19,130][root][INFO] - Training Epoch: 8/10, step 507/574 completed (loss: 0.31892523169517517, acc: 0.9009009003639221)
[2025-01-06 01:45:19,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:19,592][root][INFO] - Training Epoch: 8/10, step 508/574 completed (loss: 0.1436275690793991, acc: 0.9718309640884399)
[2025-01-06 01:45:19,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:19,903][root][INFO] - Training Epoch: 8/10, step 509/574 completed (loss: 0.05543253570795059, acc: 0.949999988079071)
[2025-01-06 01:45:20,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:20,241][root][INFO] - Training Epoch: 8/10, step 510/574 completed (loss: 0.5240922570228577, acc: 0.9333333373069763)
[2025-01-06 01:45:20,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:20,597][root][INFO] - Training Epoch: 8/10, step 511/574 completed (loss: 0.17516930401325226, acc: 0.9230769276618958)
[2025-01-06 01:45:21,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:23,197][root][INFO] - Training Epoch: 8/10, step 512/574 completed (loss: 0.2799805998802185, acc: 0.9142857193946838)
[2025-01-06 01:45:23,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:23,996][root][INFO] - Training Epoch: 8/10, step 513/574 completed (loss: 0.030923688784241676, acc: 0.976190447807312)
[2025-01-06 01:45:24,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:24,321][root][INFO] - Training Epoch: 8/10, step 514/574 completed (loss: 0.020678410306572914, acc: 1.0)
[2025-01-06 01:45:24,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:24,635][root][INFO] - Training Epoch: 8/10, step 515/574 completed (loss: 0.020132310688495636, acc: 0.9833333492279053)
[2025-01-06 01:45:24,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:25,329][root][INFO] - Training Epoch: 8/10, step 516/574 completed (loss: 0.07733353227376938, acc: 0.9722222089767456)
[2025-01-06 01:45:25,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:25,642][root][INFO] - Training Epoch: 8/10, step 517/574 completed (loss: 0.008541987277567387, acc: 1.0)
[2025-01-06 01:45:25,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:25,948][root][INFO] - Training Epoch: 8/10, step 518/574 completed (loss: 0.01050260104238987, acc: 1.0)
[2025-01-06 01:45:26,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:26,283][root][INFO] - Training Epoch: 8/10, step 519/574 completed (loss: 0.028481388464570045, acc: 1.0)
[2025-01-06 01:45:26,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:26,596][root][INFO] - Training Epoch: 8/10, step 520/574 completed (loss: 0.1226106509566307, acc: 0.9259259104728699)
[2025-01-06 01:45:26,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:27,661][root][INFO] - Training Epoch: 8/10, step 521/574 completed (loss: 0.29503133893013, acc: 0.8813559412956238)
[2025-01-06 01:45:27,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:28,009][root][INFO] - Training Epoch: 8/10, step 522/574 completed (loss: 0.08530739694833755, acc: 0.9850746393203735)
[2025-01-06 01:45:28,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:28,365][root][INFO] - Training Epoch: 8/10, step 523/574 completed (loss: 0.06849198788404465, acc: 0.9781022071838379)
[2025-01-06 01:45:28,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:28,921][root][INFO] - Training Epoch: 8/10, step 524/574 completed (loss: 0.2560397982597351, acc: 0.9300000071525574)
[2025-01-06 01:45:28,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:29,229][root][INFO] - Training Epoch: 8/10, step 525/574 completed (loss: 0.0060165049508214, acc: 1.0)
[2025-01-06 01:45:29,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:29,562][root][INFO] - Training Epoch: 8/10, step 526/574 completed (loss: 0.05098313093185425, acc: 0.9807692170143127)
[2025-01-06 01:45:29,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:29,894][root][INFO] - Training Epoch: 8/10, step 527/574 completed (loss: 0.004581585060805082, acc: 1.0)
[2025-01-06 01:45:29,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:30,217][root][INFO] - Training Epoch: 8/10, step 528/574 completed (loss: 0.1627453863620758, acc: 0.9508196711540222)
[2025-01-06 01:45:30,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:30,516][root][INFO] - Training Epoch: 8/10, step 529/574 completed (loss: 0.09836050868034363, acc: 0.9830508232116699)
[2025-01-06 01:45:30,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:30,815][root][INFO] - Training Epoch: 8/10, step 530/574 completed (loss: 0.14237593114376068, acc: 0.9534883499145508)
[2025-01-06 01:45:30,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:31,109][root][INFO] - Training Epoch: 8/10, step 531/574 completed (loss: 0.20280112326145172, acc: 0.9772727489471436)
[2025-01-06 01:45:31,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:31,407][root][INFO] - Training Epoch: 8/10, step 532/574 completed (loss: 0.30319318175315857, acc: 0.9245283007621765)
[2025-01-06 01:45:31,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:31,709][root][INFO] - Training Epoch: 8/10, step 533/574 completed (loss: 0.08980674296617508, acc: 0.9545454382896423)
[2025-01-06 01:45:31,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:32,016][root][INFO] - Training Epoch: 8/10, step 534/574 completed (loss: 0.006846800912171602, acc: 1.0)
[2025-01-06 01:45:32,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:32,313][root][INFO] - Training Epoch: 8/10, step 535/574 completed (loss: 0.07740315794944763, acc: 0.949999988079071)
[2025-01-06 01:45:32,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:32,626][root][INFO] - Training Epoch: 8/10, step 536/574 completed (loss: 0.3508436977863312, acc: 0.9545454382896423)
[2025-01-06 01:45:32,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:33,083][root][INFO] - Training Epoch: 8/10, step 537/574 completed (loss: 0.08808094263076782, acc: 0.9384615421295166)
[2025-01-06 01:45:33,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:33,482][root][INFO] - Training Epoch: 8/10, step 538/574 completed (loss: 0.05409912392497063, acc: 0.984375)
[2025-01-06 01:45:33,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:33,883][root][INFO] - Training Epoch: 8/10, step 539/574 completed (loss: 0.020626122131943703, acc: 1.0)
[2025-01-06 01:45:33,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:34,179][root][INFO] - Training Epoch: 8/10, step 540/574 completed (loss: 0.04479951038956642, acc: 1.0)
[2025-01-06 01:45:34,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:34,495][root][INFO] - Training Epoch: 8/10, step 541/574 completed (loss: 0.002290884265676141, acc: 1.0)
[2025-01-06 01:45:34,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:34,839][root][INFO] - Training Epoch: 8/10, step 542/574 completed (loss: 0.0014477769145742059, acc: 1.0)
[2025-01-06 01:45:34,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:35,139][root][INFO] - Training Epoch: 8/10, step 543/574 completed (loss: 0.002150175627321005, acc: 1.0)
[2025-01-06 01:45:35,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:35,437][root][INFO] - Training Epoch: 8/10, step 544/574 completed (loss: 0.008452219888567924, acc: 1.0)
[2025-01-06 01:45:35,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:35,760][root][INFO] - Training Epoch: 8/10, step 545/574 completed (loss: 0.03425387293100357, acc: 1.0)
[2025-01-06 01:45:35,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36,057][root][INFO] - Training Epoch: 8/10, step 546/574 completed (loss: 0.017639964818954468, acc: 1.0)
[2025-01-06 01:45:36,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36,356][root][INFO] - Training Epoch: 8/10, step 547/574 completed (loss: 0.0028531362768262625, acc: 1.0)
[2025-01-06 01:45:36,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36,661][root][INFO] - Training Epoch: 8/10, step 548/574 completed (loss: 0.003444151720032096, acc: 1.0)
[2025-01-06 01:45:36,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36,945][root][INFO] - Training Epoch: 8/10, step 549/574 completed (loss: 0.08821287006139755, acc: 0.9599999785423279)
[2025-01-06 01:45:37,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:37,296][root][INFO] - Training Epoch: 8/10, step 550/574 completed (loss: 0.013781487941741943, acc: 1.0)
[2025-01-06 01:45:37,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:37,636][root][INFO] - Training Epoch: 8/10, step 551/574 completed (loss: 0.03047911450266838, acc: 0.9750000238418579)
[2025-01-06 01:45:37,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:37,978][root][INFO] - Training Epoch: 8/10, step 552/574 completed (loss: 0.023470452055335045, acc: 0.9857142567634583)
[2025-01-06 01:45:38,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:38,329][root][INFO] - Training Epoch: 8/10, step 553/574 completed (loss: 0.03693694621324539, acc: 0.985401451587677)
[2025-01-06 01:45:38,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:38,682][root][INFO] - Training Epoch: 8/10, step 554/574 completed (loss: 0.02932824194431305, acc: 0.9931034445762634)
[2025-01-06 01:45:38,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:38,993][root][INFO] - Training Epoch: 8/10, step 555/574 completed (loss: 0.07431233674287796, acc: 0.9714285731315613)
[2025-01-06 01:45:39,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:39,305][root][INFO] - Training Epoch: 8/10, step 556/574 completed (loss: 0.17117930948734283, acc: 0.9470198750495911)
[2025-01-06 01:45:39,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:39,610][root][INFO] - Training Epoch: 8/10, step 557/574 completed (loss: 0.016205325722694397, acc: 0.9914529919624329)
[2025-01-06 01:45:40,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:40,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:40,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:41,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:41,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:41,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:42,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:42,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:42,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:43,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:43,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:43,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:44,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:44,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:44,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:45,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:45,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:45,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:46,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:46,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:48,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:48,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:48,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:49,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:49,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:49,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:51,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:51,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:52,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:52,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:52,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:53,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:53,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:53,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:54,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:54,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:54,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:55,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:55,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:55,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:56,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:56,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:56,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:57,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:57,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:57,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:58,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:58,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:00,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:00,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:01,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:01,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:02,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:02,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:02,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:02,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:03,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:03,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:04,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:04,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:04,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:05,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:05,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:05,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:06,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:06,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:06,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:08,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:08,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:09,169][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3869, device='cuda:0') eval_epoch_loss=tensor(0.8700, device='cuda:0') eval_epoch_acc=tensor(0.8252, device='cuda:0')
[2025-01-06 01:46:09,170][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:46:09,170][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:46:09,409][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_558_loss_0.8699989318847656/model.pt
[2025-01-06 01:46:09,414][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:46:09,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:09,818][root][INFO] - Training Epoch: 8/10, step 558/574 completed (loss: 0.22078262269496918, acc: 0.9599999785423279)
[2025-01-06 01:46:09,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:10,149][root][INFO] - Training Epoch: 8/10, step 559/574 completed (loss: 0.09316980838775635, acc: 0.9615384340286255)
[2025-01-06 01:46:10,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:10,444][root][INFO] - Training Epoch: 8/10, step 560/574 completed (loss: 0.0015077551361173391, acc: 1.0)
[2025-01-06 01:46:10,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:10,754][root][INFO] - Training Epoch: 8/10, step 561/574 completed (loss: 0.025434641167521477, acc: 1.0)
[2025-01-06 01:46:10,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:11,115][root][INFO] - Training Epoch: 8/10, step 562/574 completed (loss: 0.09363455325365067, acc: 0.9666666388511658)
[2025-01-06 01:46:11,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:11,439][root][INFO] - Training Epoch: 8/10, step 563/574 completed (loss: 0.02273234724998474, acc: 1.0)
[2025-01-06 01:46:11,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:11,754][root][INFO] - Training Epoch: 8/10, step 564/574 completed (loss: 0.053603217005729675, acc: 0.9791666865348816)
[2025-01-06 01:46:11,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12,044][root][INFO] - Training Epoch: 8/10, step 565/574 completed (loss: 0.17835168540477753, acc: 0.9655172228813171)
[2025-01-06 01:46:12,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12,365][root][INFO] - Training Epoch: 8/10, step 566/574 completed (loss: 0.017618227750062943, acc: 1.0)
[2025-01-06 01:46:12,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12,666][root][INFO] - Training Epoch: 8/10, step 567/574 completed (loss: 0.0023814064916223288, acc: 1.0)
[2025-01-06 01:46:12,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12,958][root][INFO] - Training Epoch: 8/10, step 568/574 completed (loss: 0.014460033737123013, acc: 1.0)
[2025-01-06 01:46:13,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:13,334][root][INFO] - Training Epoch: 8/10, step 569/574 completed (loss: 0.1058628037571907, acc: 0.9732620120048523)
[2025-01-06 01:46:13,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:13,633][root][INFO] - Training Epoch: 8/10, step 570/574 completed (loss: 0.030436189845204353, acc: 0.9838709831237793)
[2025-01-06 01:46:13,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:13,951][root][INFO] - Training Epoch: 8/10, step 571/574 completed (loss: 0.0064058150164783, acc: 1.0)
[2025-01-06 01:46:14,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:14,258][root][INFO] - Training Epoch: 8/10, step 572/574 completed (loss: 0.11485200375318527, acc: 0.9642857313156128)
[2025-01-06 01:46:14,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:14,615][root][INFO] - Training Epoch: 8/10, step 573/574 completed (loss: 0.12422963231801987, acc: 0.9496855139732361)
[2025-01-06 01:46:15,141][slam_llm.utils.train_utils][INFO] - Epoch 8: train_perplexity=1.1084, train_epoch_loss=0.1029, epoch time 338.0241457410157s
[2025-01-06 01:46:15,141][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:46:15,142][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:46:15,142][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:46:15,142][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 22
[2025-01-06 01:46:15,142][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:46:15,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:15,954][root][INFO] - Training Epoch: 9/10, step 0/574 completed (loss: 0.03730924427509308, acc: 1.0)
[2025-01-06 01:46:16,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:16,258][root][INFO] - Training Epoch: 9/10, step 1/574 completed (loss: 0.0319896936416626, acc: 1.0)
[2025-01-06 01:46:16,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:16,573][root][INFO] - Training Epoch: 9/10, step 2/574 completed (loss: 0.02973325550556183, acc: 1.0)
[2025-01-06 01:46:16,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:16,929][root][INFO] - Training Epoch: 9/10, step 3/574 completed (loss: 0.0347835011780262, acc: 0.9736841917037964)
[2025-01-06 01:46:17,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17,248][root][INFO] - Training Epoch: 9/10, step 4/574 completed (loss: 0.032957158982753754, acc: 1.0)
[2025-01-06 01:46:17,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17,573][root][INFO] - Training Epoch: 9/10, step 5/574 completed (loss: 0.013520709238946438, acc: 1.0)
[2025-01-06 01:46:17,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17,893][root][INFO] - Training Epoch: 9/10, step 6/574 completed (loss: 0.08898144960403442, acc: 0.9591836929321289)
[2025-01-06 01:46:17,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:18,201][root][INFO] - Training Epoch: 9/10, step 7/574 completed (loss: 0.10831990838050842, acc: 0.9666666388511658)
[2025-01-06 01:46:18,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:18,588][root][INFO] - Training Epoch: 9/10, step 8/574 completed (loss: 0.00352409016340971, acc: 1.0)
[2025-01-06 01:46:18,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:18,967][root][INFO] - Training Epoch: 9/10, step 9/574 completed (loss: 0.0026145498268306255, acc: 1.0)
[2025-01-06 01:46:19,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:19,310][root][INFO] - Training Epoch: 9/10, step 10/574 completed (loss: 0.002656396944075823, acc: 1.0)
[2025-01-06 01:46:19,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:19,632][root][INFO] - Training Epoch: 9/10, step 11/574 completed (loss: 0.006779918447136879, acc: 1.0)
[2025-01-06 01:46:19,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:19,944][root][INFO] - Training Epoch: 9/10, step 12/574 completed (loss: 0.006834798492491245, acc: 1.0)
[2025-01-06 01:46:20,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:20,262][root][INFO] - Training Epoch: 9/10, step 13/574 completed (loss: 0.013865218497812748, acc: 1.0)
[2025-01-06 01:46:20,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:20,586][root][INFO] - Training Epoch: 9/10, step 14/574 completed (loss: 0.008055884391069412, acc: 1.0)
[2025-01-06 01:46:20,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:20,889][root][INFO] - Training Epoch: 9/10, step 15/574 completed (loss: 0.021174980327486992, acc: 1.0)
[2025-01-06 01:46:20,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:21,187][root][INFO] - Training Epoch: 9/10, step 16/574 completed (loss: 0.0017022539395838976, acc: 1.0)
[2025-01-06 01:46:21,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:21,490][root][INFO] - Training Epoch: 9/10, step 17/574 completed (loss: 0.04435183107852936, acc: 0.9583333134651184)
[2025-01-06 01:46:21,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:21,805][root][INFO] - Training Epoch: 9/10, step 18/574 completed (loss: 0.04859549552202225, acc: 1.0)
[2025-01-06 01:46:21,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:22,161][root][INFO] - Training Epoch: 9/10, step 19/574 completed (loss: 0.0005866107530891895, acc: 1.0)
[2025-01-06 01:46:22,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:22,492][root][INFO] - Training Epoch: 9/10, step 20/574 completed (loss: 0.020416323095560074, acc: 1.0)
[2025-01-06 01:46:22,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:22,812][root][INFO] - Training Epoch: 9/10, step 21/574 completed (loss: 0.004257923923432827, acc: 1.0)
[2025-01-06 01:46:22,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:23,120][root][INFO] - Training Epoch: 9/10, step 22/574 completed (loss: 0.03255883604288101, acc: 1.0)
[2025-01-06 01:46:23,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:23,425][root][INFO] - Training Epoch: 9/10, step 23/574 completed (loss: 0.013089385814964771, acc: 1.0)
[2025-01-06 01:46:23,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:23,732][root][INFO] - Training Epoch: 9/10, step 24/574 completed (loss: 0.012575939297676086, acc: 1.0)
[2025-01-06 01:46:23,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:24,053][root][INFO] - Training Epoch: 9/10, step 25/574 completed (loss: 0.22368718683719635, acc: 0.9622641801834106)
[2025-01-06 01:46:24,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:24,374][root][INFO] - Training Epoch: 9/10, step 26/574 completed (loss: 0.04846687987446785, acc: 0.9863013625144958)
[2025-01-06 01:46:24,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:25,708][root][INFO] - Training Epoch: 9/10, step 27/574 completed (loss: 0.273049920797348, acc: 0.8853754997253418)
[2025-01-06 01:46:25,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:26,019][root][INFO] - Training Epoch: 9/10, step 28/574 completed (loss: 0.11171254515647888, acc: 0.9534883499145508)
[2025-01-06 01:46:26,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:26,353][root][INFO] - Training Epoch: 9/10, step 29/574 completed (loss: 0.08692032098770142, acc: 0.9759036302566528)
[2025-01-06 01:46:26,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:26,703][root][INFO] - Training Epoch: 9/10, step 30/574 completed (loss: 0.09950103610754013, acc: 0.9753086566925049)
[2025-01-06 01:46:26,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27,011][root][INFO] - Training Epoch: 9/10, step 31/574 completed (loss: 0.0287090502679348, acc: 1.0)
[2025-01-06 01:46:27,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27,327][root][INFO] - Training Epoch: 9/10, step 32/574 completed (loss: 0.06250554323196411, acc: 0.9629629850387573)
[2025-01-06 01:46:27,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27,652][root][INFO] - Training Epoch: 9/10, step 33/574 completed (loss: 0.008959085680544376, acc: 1.0)
[2025-01-06 01:46:27,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27,984][root][INFO] - Training Epoch: 9/10, step 34/574 completed (loss: 0.08232340961694717, acc: 0.9663865566253662)
[2025-01-06 01:46:28,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:28,309][root][INFO] - Training Epoch: 9/10, step 35/574 completed (loss: 0.08843110501766205, acc: 0.9672130942344666)
[2025-01-06 01:46:28,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:28,657][root][INFO] - Training Epoch: 9/10, step 36/574 completed (loss: 0.03856136277318001, acc: 0.9841269850730896)
[2025-01-06 01:46:28,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:28,980][root][INFO] - Training Epoch: 9/10, step 37/574 completed (loss: 0.009550128132104874, acc: 1.0)
[2025-01-06 01:46:29,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:29,346][root][INFO] - Training Epoch: 9/10, step 38/574 completed (loss: 0.07365070283412933, acc: 0.977011501789093)
[2025-01-06 01:46:29,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:29,636][root][INFO] - Training Epoch: 9/10, step 39/574 completed (loss: 0.02732616290450096, acc: 1.0)
[2025-01-06 01:46:29,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:30,021][root][INFO] - Training Epoch: 9/10, step 40/574 completed (loss: 0.0883350819349289, acc: 0.9230769276618958)
[2025-01-06 01:46:30,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:30,397][root][INFO] - Training Epoch: 9/10, step 41/574 completed (loss: 0.20758621394634247, acc: 0.9459459185600281)
[2025-01-06 01:46:30,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:30,760][root][INFO] - Training Epoch: 9/10, step 42/574 completed (loss: 0.13176201283931732, acc: 0.9538461565971375)
[2025-01-06 01:46:30,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:31,190][root][INFO] - Training Epoch: 9/10, step 43/574 completed (loss: 0.0861232653260231, acc: 0.9595959782600403)
[2025-01-06 01:46:31,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:31,638][root][INFO] - Training Epoch: 9/10, step 44/574 completed (loss: 0.09138700366020203, acc: 0.969072163105011)
[2025-01-06 01:46:31,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:32,037][root][INFO] - Training Epoch: 9/10, step 45/574 completed (loss: 0.07367419451475143, acc: 0.9779411554336548)
[2025-01-06 01:46:32,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:32,341][root][INFO] - Training Epoch: 9/10, step 46/574 completed (loss: 0.0039873202331364155, acc: 1.0)
[2025-01-06 01:46:32,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:32,633][root][INFO] - Training Epoch: 9/10, step 47/574 completed (loss: 0.08886586874723434, acc: 0.9629629850387573)
[2025-01-06 01:46:32,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:32,933][root][INFO] - Training Epoch: 9/10, step 48/574 completed (loss: 0.005023537669330835, acc: 1.0)
[2025-01-06 01:46:33,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:33,233][root][INFO] - Training Epoch: 9/10, step 49/574 completed (loss: 0.16469168663024902, acc: 0.9722222089767456)
[2025-01-06 01:46:33,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:33,546][root][INFO] - Training Epoch: 9/10, step 50/574 completed (loss: 0.1327718049287796, acc: 0.9473684430122375)
[2025-01-06 01:46:33,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:33,859][root][INFO] - Training Epoch: 9/10, step 51/574 completed (loss: 0.1081632524728775, acc: 0.9365079402923584)
[2025-01-06 01:46:33,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:34,196][root][INFO] - Training Epoch: 9/10, step 52/574 completed (loss: 0.2905113995075226, acc: 0.9014084339141846)
[2025-01-06 01:46:34,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:34,640][root][INFO] - Training Epoch: 9/10, step 53/574 completed (loss: 0.516643762588501, acc: 0.8333333134651184)
[2025-01-06 01:46:34,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:34,942][root][INFO] - Training Epoch: 9/10, step 54/574 completed (loss: 0.1228170394897461, acc: 0.9729729890823364)
[2025-01-06 01:46:35,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:35,231][root][INFO] - Training Epoch: 9/10, step 55/574 completed (loss: 0.0022825351916253567, acc: 1.0)
[2025-01-06 01:46:36,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:38,268][root][INFO] - Training Epoch: 9/10, step 56/574 completed (loss: 0.4045547544956207, acc: 0.8532423377037048)
[2025-01-06 01:46:38,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:39,566][root][INFO] - Training Epoch: 9/10, step 57/574 completed (loss: 0.7825831770896912, acc: 0.7973856329917908)
[2025-01-06 01:46:39,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:40,201][root][INFO] - Training Epoch: 9/10, step 58/574 completed (loss: 0.25134825706481934, acc: 0.8977272510528564)
[2025-01-06 01:46:40,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:40,771][root][INFO] - Training Epoch: 9/10, step 59/574 completed (loss: 0.08115781843662262, acc: 0.9779411554336548)
[2025-01-06 01:46:40,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:41,358][root][INFO] - Training Epoch: 9/10, step 60/574 completed (loss: 0.22564280033111572, acc: 0.9202898740768433)
[2025-01-06 01:46:41,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:41,776][root][INFO] - Training Epoch: 9/10, step 61/574 completed (loss: 0.10662670433521271, acc: 0.9624999761581421)
[2025-01-06 01:46:41,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:42,090][root][INFO] - Training Epoch: 9/10, step 62/574 completed (loss: 0.008022364228963852, acc: 1.0)
[2025-01-06 01:46:42,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:42,414][root][INFO] - Training Epoch: 9/10, step 63/574 completed (loss: 0.010211491957306862, acc: 1.0)
[2025-01-06 01:46:42,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:42,762][root][INFO] - Training Epoch: 9/10, step 64/574 completed (loss: 0.1581019163131714, acc: 0.96875)
[2025-01-06 01:46:42,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:43,106][root][INFO] - Training Epoch: 9/10, step 65/574 completed (loss: 0.003000300144776702, acc: 1.0)
[2025-01-06 01:46:43,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:43,454][root][INFO] - Training Epoch: 9/10, step 66/574 completed (loss: 0.11327844858169556, acc: 0.9464285969734192)
[2025-01-06 01:46:43,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:43,827][root][INFO] - Training Epoch: 9/10, step 67/574 completed (loss: 0.07494962215423584, acc: 0.9666666388511658)
[2025-01-06 01:46:43,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:44,205][root][INFO] - Training Epoch: 9/10, step 68/574 completed (loss: 0.001955389278009534, acc: 1.0)
[2025-01-06 01:46:44,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:44,565][root][INFO] - Training Epoch: 9/10, step 69/574 completed (loss: 0.1523786187171936, acc: 0.9722222089767456)
[2025-01-06 01:46:44,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:44,913][root][INFO] - Training Epoch: 9/10, step 70/574 completed (loss: 0.05865238606929779, acc: 1.0)
[2025-01-06 01:46:44,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:45,261][root][INFO] - Training Epoch: 9/10, step 71/574 completed (loss: 0.24901247024536133, acc: 0.9264705777168274)
[2025-01-06 01:46:45,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:45,605][root][INFO] - Training Epoch: 9/10, step 72/574 completed (loss: 0.25808286666870117, acc: 0.9523809552192688)
[2025-01-06 01:46:45,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:45,953][root][INFO] - Training Epoch: 9/10, step 73/574 completed (loss: 0.6382179260253906, acc: 0.7897436022758484)
[2025-01-06 01:46:46,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:46,299][root][INFO] - Training Epoch: 9/10, step 74/574 completed (loss: 0.22226141393184662, acc: 0.918367326259613)
[2025-01-06 01:46:46,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:46,661][root][INFO] - Training Epoch: 9/10, step 75/574 completed (loss: 0.3032158315181732, acc: 0.9253731369972229)
[2025-01-06 01:46:46,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:47,086][root][INFO] - Training Epoch: 9/10, step 76/574 completed (loss: 0.6757789254188538, acc: 0.8138686418533325)
[2025-01-06 01:46:47,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:47,427][root][INFO] - Training Epoch: 9/10, step 77/574 completed (loss: 0.0010923473164439201, acc: 1.0)
[2025-01-06 01:46:47,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:47,784][root][INFO] - Training Epoch: 9/10, step 78/574 completed (loss: 0.06791345775127411, acc: 0.9583333134651184)
[2025-01-06 01:46:47,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:48,156][root][INFO] - Training Epoch: 9/10, step 79/574 completed (loss: 0.018732022494077682, acc: 1.0)
[2025-01-06 01:46:48,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:48,517][root][INFO] - Training Epoch: 9/10, step 80/574 completed (loss: 0.0009962237672880292, acc: 1.0)
[2025-01-06 01:46:48,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:48,862][root][INFO] - Training Epoch: 9/10, step 81/574 completed (loss: 0.038174450397491455, acc: 0.9807692170143127)
[2025-01-06 01:46:48,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:49,232][root][INFO] - Training Epoch: 9/10, step 82/574 completed (loss: 0.06661369651556015, acc: 0.9807692170143127)
[2025-01-06 01:46:49,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:49,596][root][INFO] - Training Epoch: 9/10, step 83/574 completed (loss: 0.016311844810843468, acc: 1.0)
[2025-01-06 01:46:49,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:49,971][root][INFO] - Training Epoch: 9/10, step 84/574 completed (loss: 0.12095089256763458, acc: 0.95652174949646)
[2025-01-06 01:46:50,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:50,343][root][INFO] - Training Epoch: 9/10, step 85/574 completed (loss: 0.038242071866989136, acc: 0.9800000190734863)
[2025-01-06 01:46:50,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:50,712][root][INFO] - Training Epoch: 9/10, step 86/574 completed (loss: 0.07727295905351639, acc: 0.95652174949646)
[2025-01-06 01:46:50,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:51,208][root][INFO] - Training Epoch: 9/10, step 87/574 completed (loss: 0.06722473353147507, acc: 0.9800000190734863)
[2025-01-06 01:46:51,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:51,573][root][INFO] - Training Epoch: 9/10, step 88/574 completed (loss: 0.16082410514354706, acc: 0.9223300814628601)
[2025-01-06 01:46:51,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:52,658][root][INFO] - Training Epoch: 9/10, step 89/574 completed (loss: 0.32791706919670105, acc: 0.8834951519966125)
[2025-01-06 01:46:52,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:53,480][root][INFO] - Training Epoch: 9/10, step 90/574 completed (loss: 0.38495874404907227, acc: 0.8870967626571655)
[2025-01-06 01:46:53,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:54,283][root][INFO] - Training Epoch: 9/10, step 91/574 completed (loss: 0.2950054109096527, acc: 0.9051724076271057)
[2025-01-06 01:46:54,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:55,029][root][INFO] - Training Epoch: 9/10, step 92/574 completed (loss: 0.14695870876312256, acc: 0.9368420839309692)
[2025-01-06 01:46:55,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:56,044][root][INFO] - Training Epoch: 9/10, step 93/574 completed (loss: 0.19272591173648834, acc: 0.9504950642585754)
[2025-01-06 01:46:56,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:56,372][root][INFO] - Training Epoch: 9/10, step 94/574 completed (loss: 0.1310328096151352, acc: 0.9516128897666931)
[2025-01-06 01:46:56,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:56,726][root][INFO] - Training Epoch: 9/10, step 95/574 completed (loss: 0.15903019905090332, acc: 0.95652174949646)
[2025-01-06 01:46:56,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:57,066][root][INFO] - Training Epoch: 9/10, step 96/574 completed (loss: 0.2388136386871338, acc: 0.9411764740943909)
[2025-01-06 01:46:57,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:57,411][root][INFO] - Training Epoch: 9/10, step 97/574 completed (loss: 0.2824530303478241, acc: 0.9230769276618958)
[2025-01-06 01:46:57,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:57,816][root][INFO] - Training Epoch: 9/10, step 98/574 completed (loss: 0.250299334526062, acc: 0.9343065619468689)
[2025-01-06 01:46:57,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:58,151][root][INFO] - Training Epoch: 9/10, step 99/574 completed (loss: 0.3022834360599518, acc: 0.8805969953536987)
[2025-01-06 01:46:58,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:58,481][root][INFO] - Training Epoch: 9/10, step 100/574 completed (loss: 0.004724212922155857, acc: 1.0)
[2025-01-06 01:46:58,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:58,811][root][INFO] - Training Epoch: 9/10, step 101/574 completed (loss: 0.010127994231879711, acc: 1.0)
[2025-01-06 01:46:58,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59,157][root][INFO] - Training Epoch: 9/10, step 102/574 completed (loss: 0.01078624464571476, acc: 1.0)
[2025-01-06 01:46:59,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59,492][root][INFO] - Training Epoch: 9/10, step 103/574 completed (loss: 0.01404072716832161, acc: 1.0)
[2025-01-06 01:46:59,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59,851][root][INFO] - Training Epoch: 9/10, step 104/574 completed (loss: 0.07792455703020096, acc: 0.9482758641242981)
[2025-01-06 01:46:59,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:00,222][root][INFO] - Training Epoch: 9/10, step 105/574 completed (loss: 0.01254297699779272, acc: 1.0)
[2025-01-06 01:47:00,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:00,569][root][INFO] - Training Epoch: 9/10, step 106/574 completed (loss: 0.04951851814985275, acc: 1.0)
[2025-01-06 01:47:00,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:00,911][root][INFO] - Training Epoch: 9/10, step 107/574 completed (loss: 0.0012913485988974571, acc: 1.0)
[2025-01-06 01:47:00,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:01,243][root][INFO] - Training Epoch: 9/10, step 108/574 completed (loss: 0.018290013074874878, acc: 1.0)
[2025-01-06 01:47:01,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:01,568][root][INFO] - Training Epoch: 9/10, step 109/574 completed (loss: 0.1401011049747467, acc: 0.9523809552192688)
[2025-01-06 01:47:01,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:01,912][root][INFO] - Training Epoch: 9/10, step 110/574 completed (loss: 0.20256778597831726, acc: 0.9538461565971375)
[2025-01-06 01:47:02,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:02,348][root][INFO] - Training Epoch: 9/10, step 111/574 completed (loss: 0.15650363266468048, acc: 0.9298245906829834)
[2025-01-06 01:47:02,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:02,732][root][INFO] - Training Epoch: 9/10, step 112/574 completed (loss: 0.06661728024482727, acc: 1.0)
[2025-01-06 01:47:02,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:03,085][root][INFO] - Training Epoch: 9/10, step 113/574 completed (loss: 0.04862198606133461, acc: 1.0)
[2025-01-06 01:47:03,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:03,493][root][INFO] - Training Epoch: 9/10, step 114/574 completed (loss: 0.11452563107013702, acc: 0.9795918464660645)
[2025-01-06 01:47:03,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:03,915][root][INFO] - Training Epoch: 9/10, step 115/574 completed (loss: 0.0006754989153705537, acc: 1.0)
[2025-01-06 01:47:04,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:04,296][root][INFO] - Training Epoch: 9/10, step 116/574 completed (loss: 0.032407987862825394, acc: 1.0)
[2025-01-06 01:47:04,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:04,648][root][INFO] - Training Epoch: 9/10, step 117/574 completed (loss: 0.08256760239601135, acc: 0.9593495726585388)
[2025-01-06 01:47:04,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:05,045][root][INFO] - Training Epoch: 9/10, step 118/574 completed (loss: 0.020856443792581558, acc: 0.9838709831237793)
[2025-01-06 01:47:05,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:05,885][root][INFO] - Training Epoch: 9/10, step 119/574 completed (loss: 0.3592732548713684, acc: 0.8935361504554749)
[2025-01-06 01:47:05,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:06,215][root][INFO] - Training Epoch: 9/10, step 120/574 completed (loss: 0.026855656877160072, acc: 1.0)
[2025-01-06 01:47:06,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:06,622][root][INFO] - Training Epoch: 9/10, step 121/574 completed (loss: 0.3298376500606537, acc: 0.942307710647583)
[2025-01-06 01:47:06,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:06,992][root][INFO] - Training Epoch: 9/10, step 122/574 completed (loss: 0.009143687784671783, acc: 1.0)
[2025-01-06 01:47:07,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:07,326][root][INFO] - Training Epoch: 9/10, step 123/574 completed (loss: 0.029828393831849098, acc: 1.0)
[2025-01-06 01:47:07,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:07,666][root][INFO] - Training Epoch: 9/10, step 124/574 completed (loss: 0.2715388238430023, acc: 0.9141104221343994)
[2025-01-06 01:47:07,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:08,013][root][INFO] - Training Epoch: 9/10, step 125/574 completed (loss: 0.2986984848976135, acc: 0.8958333134651184)
[2025-01-06 01:47:08,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:08,329][root][INFO] - Training Epoch: 9/10, step 126/574 completed (loss: 0.17155471444129944, acc: 0.9750000238418579)
[2025-01-06 01:47:09,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:09,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:09,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:10,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:10,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:10,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:11,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:11,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:13,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:13,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:13,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:16,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:16,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:16,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:17,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:17,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:17,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:18,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:18,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:18,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:19,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:19,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:19,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:20,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:20,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:20,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:21,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:21,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:21,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:23,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:23,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:23,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:24,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:24,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:24,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:25,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:25,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:25,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:25,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:26,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:26,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:26,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:27,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:27,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:27,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:29,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:29,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:29,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:30,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:30,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:31,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:31,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:31,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:33,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:33,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:33,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:35,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:35,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:36,423][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4904, device='cuda:0') eval_epoch_loss=tensor(0.9124, device='cuda:0') eval_epoch_acc=tensor(0.8245, device='cuda:0')
[2025-01-06 01:47:36,424][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:47:36,425][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:47:36,717][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_127_loss_0.9124407768249512/model.pt
[2025-01-06 01:47:36,722][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:47:36,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:37,206][root][INFO] - Training Epoch: 9/10, step 127/574 completed (loss: 0.15495184063911438, acc: 0.9404761791229248)
[2025-01-06 01:47:37,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:37,604][root][INFO] - Training Epoch: 9/10, step 128/574 completed (loss: 0.21429917216300964, acc: 0.9333333373069763)
[2025-01-06 01:47:37,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38,005][root][INFO] - Training Epoch: 9/10, step 129/574 completed (loss: 0.20413319766521454, acc: 0.9191176295280457)
[2025-01-06 01:47:38,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38,323][root][INFO] - Training Epoch: 9/10, step 130/574 completed (loss: 0.0052903336472809315, acc: 1.0)
[2025-01-06 01:47:38,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38,606][root][INFO] - Training Epoch: 9/10, step 131/574 completed (loss: 0.043946344405412674, acc: 1.0)
[2025-01-06 01:47:38,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38,914][root][INFO] - Training Epoch: 9/10, step 132/574 completed (loss: 0.03748499974608421, acc: 0.96875)
[2025-01-06 01:47:38,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:39,220][root][INFO] - Training Epoch: 9/10, step 133/574 completed (loss: 0.06942372769117355, acc: 0.95652174949646)
[2025-01-06 01:47:39,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:39,523][root][INFO] - Training Epoch: 9/10, step 134/574 completed (loss: 0.22732731699943542, acc: 0.9428571462631226)
[2025-01-06 01:47:39,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:39,820][root][INFO] - Training Epoch: 9/10, step 135/574 completed (loss: 0.009632022120058537, acc: 1.0)
[2025-01-06 01:47:39,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:40,155][root][INFO] - Training Epoch: 9/10, step 136/574 completed (loss: 0.02295033074915409, acc: 0.976190447807312)
[2025-01-06 01:47:40,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:40,472][root][INFO] - Training Epoch: 9/10, step 137/574 completed (loss: 0.11129564791917801, acc: 0.9333333373069763)
[2025-01-06 01:47:40,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:40,769][root][INFO] - Training Epoch: 9/10, step 138/574 completed (loss: 0.058950480073690414, acc: 0.95652174949646)
[2025-01-06 01:47:40,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:41,120][root][INFO] - Training Epoch: 9/10, step 139/574 completed (loss: 0.0019478531321510673, acc: 1.0)
[2025-01-06 01:47:41,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:41,439][root][INFO] - Training Epoch: 9/10, step 140/574 completed (loss: 0.05371994152665138, acc: 1.0)
[2025-01-06 01:47:41,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:41,734][root][INFO] - Training Epoch: 9/10, step 141/574 completed (loss: 0.040349122136831284, acc: 0.9677419066429138)
[2025-01-06 01:47:41,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:42,037][root][INFO] - Training Epoch: 9/10, step 142/574 completed (loss: 0.28265368938446045, acc: 0.9189189076423645)
[2025-01-06 01:47:42,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:42,558][root][INFO] - Training Epoch: 9/10, step 143/574 completed (loss: 0.16323857009410858, acc: 0.9385964870452881)
[2025-01-06 01:47:42,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:42,889][root][INFO] - Training Epoch: 9/10, step 144/574 completed (loss: 0.1932765394449234, acc: 0.9402984976768494)
[2025-01-06 01:47:42,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:43,227][root][INFO] - Training Epoch: 9/10, step 145/574 completed (loss: 0.20429284870624542, acc: 0.9489796161651611)
[2025-01-06 01:47:43,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:43,663][root][INFO] - Training Epoch: 9/10, step 146/574 completed (loss: 0.12745703756809235, acc: 0.957446813583374)
[2025-01-06 01:47:43,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:43,970][root][INFO] - Training Epoch: 9/10, step 147/574 completed (loss: 0.22562289237976074, acc: 0.9571428298950195)
[2025-01-06 01:47:44,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:44,250][root][INFO] - Training Epoch: 9/10, step 148/574 completed (loss: 0.017756149172782898, acc: 1.0)
[2025-01-06 01:47:44,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:44,546][root][INFO] - Training Epoch: 9/10, step 149/574 completed (loss: 0.005525671411305666, acc: 1.0)
[2025-01-06 01:47:44,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:44,834][root][INFO] - Training Epoch: 9/10, step 150/574 completed (loss: 0.005056342575699091, acc: 1.0)
[2025-01-06 01:47:44,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:45,148][root][INFO] - Training Epoch: 9/10, step 151/574 completed (loss: 0.04918980970978737, acc: 0.97826087474823)
[2025-01-06 01:47:45,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:45,458][root][INFO] - Training Epoch: 9/10, step 152/574 completed (loss: 0.07168181240558624, acc: 0.9830508232116699)
[2025-01-06 01:47:45,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:45,758][root][INFO] - Training Epoch: 9/10, step 153/574 completed (loss: 0.12413059920072556, acc: 0.9649122953414917)
[2025-01-06 01:47:45,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:46,074][root][INFO] - Training Epoch: 9/10, step 154/574 completed (loss: 0.16532683372497559, acc: 0.9729729890823364)
[2025-01-06 01:47:46,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:46,382][root][INFO] - Training Epoch: 9/10, step 155/574 completed (loss: 0.014588266611099243, acc: 1.0)
[2025-01-06 01:47:46,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:46,728][root][INFO] - Training Epoch: 9/10, step 156/574 completed (loss: 0.07669320702552795, acc: 0.95652174949646)
[2025-01-06 01:47:46,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:47,073][root][INFO] - Training Epoch: 9/10, step 157/574 completed (loss: 0.21494179964065552, acc: 0.9473684430122375)
[2025-01-06 01:47:47,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:48,713][root][INFO] - Training Epoch: 9/10, step 158/574 completed (loss: 0.7090420126914978, acc: 0.8243243098258972)
[2025-01-06 01:47:48,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:49,030][root][INFO] - Training Epoch: 9/10, step 159/574 completed (loss: 1.044543743133545, acc: 0.7777777910232544)
[2025-01-06 01:47:49,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:49,431][root][INFO] - Training Epoch: 9/10, step 160/574 completed (loss: 0.31208422780036926, acc: 0.8720930218696594)
[2025-01-06 01:47:49,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:50,019][root][INFO] - Training Epoch: 9/10, step 161/574 completed (loss: 0.13661697506904602, acc: 0.9764705896377563)
[2025-01-06 01:47:50,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:50,572][root][INFO] - Training Epoch: 9/10, step 162/574 completed (loss: 0.5358391404151917, acc: 0.8426966071128845)
[2025-01-06 01:47:50,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:50,883][root][INFO] - Training Epoch: 9/10, step 163/574 completed (loss: 0.03628089651465416, acc: 1.0)
[2025-01-06 01:47:50,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:51,180][root][INFO] - Training Epoch: 9/10, step 164/574 completed (loss: 0.002822066191583872, acc: 1.0)
[2025-01-06 01:47:51,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:51,485][root][INFO] - Training Epoch: 9/10, step 165/574 completed (loss: 0.12818729877471924, acc: 0.931034505367279)
[2025-01-06 01:47:51,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:51,826][root][INFO] - Training Epoch: 9/10, step 166/574 completed (loss: 0.010053071193397045, acc: 1.0)
[2025-01-06 01:47:51,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:52,148][root][INFO] - Training Epoch: 9/10, step 167/574 completed (loss: 0.1043315902352333, acc: 0.9599999785423279)
[2025-01-06 01:47:52,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:52,546][root][INFO] - Training Epoch: 9/10, step 168/574 completed (loss: 0.17381040751934052, acc: 0.9583333134651184)
[2025-01-06 01:47:52,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:52,869][root][INFO] - Training Epoch: 9/10, step 169/574 completed (loss: 0.28590282797813416, acc: 0.9215686321258545)
[2025-01-06 01:47:53,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:53,904][root][INFO] - Training Epoch: 9/10, step 170/574 completed (loss: 0.2389741986989975, acc: 0.931506872177124)
[2025-01-06 01:47:54,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:54,263][root][INFO] - Training Epoch: 9/10, step 171/574 completed (loss: 0.0017197391716763377, acc: 1.0)
[2025-01-06 01:47:54,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:54,597][root][INFO] - Training Epoch: 9/10, step 172/574 completed (loss: 0.001575378468260169, acc: 1.0)
[2025-01-06 01:47:54,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:54,898][root][INFO] - Training Epoch: 9/10, step 173/574 completed (loss: 0.010582885704934597, acc: 1.0)
[2025-01-06 01:47:55,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:55,441][root][INFO] - Training Epoch: 9/10, step 174/574 completed (loss: 0.41538307070732117, acc: 0.8761062026023865)
[2025-01-06 01:47:55,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:55,751][root][INFO] - Training Epoch: 9/10, step 175/574 completed (loss: 0.17180465161800385, acc: 0.9420289993286133)
[2025-01-06 01:47:55,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:56,062][root][INFO] - Training Epoch: 9/10, step 176/574 completed (loss: 0.1732996255159378, acc: 0.9431818127632141)
[2025-01-06 01:47:56,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:56,967][root][INFO] - Training Epoch: 9/10, step 177/574 completed (loss: 0.24090923368930817, acc: 0.9236640930175781)
[2025-01-06 01:47:57,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:57,638][root][INFO] - Training Epoch: 9/10, step 178/574 completed (loss: 0.38341522216796875, acc: 0.8814814686775208)
[2025-01-06 01:47:57,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:57,949][root][INFO] - Training Epoch: 9/10, step 179/574 completed (loss: 0.06260107457637787, acc: 0.9508196711540222)
[2025-01-06 01:47:58,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:58,232][root][INFO] - Training Epoch: 9/10, step 180/574 completed (loss: 0.005518214777112007, acc: 1.0)
[2025-01-06 01:47:58,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:58,539][root][INFO] - Training Epoch: 9/10, step 181/574 completed (loss: 0.0011832095915451646, acc: 1.0)
[2025-01-06 01:47:58,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:58,853][root][INFO] - Training Epoch: 9/10, step 182/574 completed (loss: 0.011319617740809917, acc: 1.0)
[2025-01-06 01:47:58,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:59,145][root][INFO] - Training Epoch: 9/10, step 183/574 completed (loss: 0.023521585389971733, acc: 1.0)
[2025-01-06 01:47:59,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:59,475][root][INFO] - Training Epoch: 9/10, step 184/574 completed (loss: 0.18229468166828156, acc: 0.9516616463661194)
[2025-01-06 01:47:59,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:59,821][root][INFO] - Training Epoch: 9/10, step 185/574 completed (loss: 0.22749055922031403, acc: 0.9221901893615723)
[2025-01-06 01:47:59,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:00,321][root][INFO] - Training Epoch: 9/10, step 186/574 completed (loss: 0.16336564719676971, acc: 0.9468749761581421)
[2025-01-06 01:48:00,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:00,855][root][INFO] - Training Epoch: 9/10, step 187/574 completed (loss: 0.2299875169992447, acc: 0.9380863308906555)
[2025-01-06 01:48:00,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:01,291][root][INFO] - Training Epoch: 9/10, step 188/574 completed (loss: 0.20257830619812012, acc: 0.9501779079437256)
[2025-01-06 01:48:01,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:01,614][root][INFO] - Training Epoch: 9/10, step 189/574 completed (loss: 0.22015707194805145, acc: 0.8799999952316284)
[2025-01-06 01:48:01,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:02,183][root][INFO] - Training Epoch: 9/10, step 190/574 completed (loss: 0.22174522280693054, acc: 0.930232584476471)
[2025-01-06 01:48:02,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:02,987][root][INFO] - Training Epoch: 9/10, step 191/574 completed (loss: 0.2908616364002228, acc: 0.89682537317276)
[2025-01-06 01:48:03,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:03,905][root][INFO] - Training Epoch: 9/10, step 192/574 completed (loss: 0.27352142333984375, acc: 0.9242424368858337)
[2025-01-06 01:48:04,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:04,647][root][INFO] - Training Epoch: 9/10, step 193/574 completed (loss: 0.09892155975103378, acc: 0.9882352948188782)
[2025-01-06 01:48:04,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:05,728][root][INFO] - Training Epoch: 9/10, step 194/574 completed (loss: 0.4053444564342499, acc: 0.8827160596847534)
[2025-01-06 01:48:05,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:06,687][root][INFO] - Training Epoch: 9/10, step 195/574 completed (loss: 0.07325991243124008, acc: 0.9677419066429138)
[2025-01-06 01:48:06,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:06,999][root][INFO] - Training Epoch: 9/10, step 196/574 completed (loss: 0.06029237061738968, acc: 0.9642857313156128)
[2025-01-06 01:48:07,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:07,369][root][INFO] - Training Epoch: 9/10, step 197/574 completed (loss: 0.10090933740139008, acc: 0.949999988079071)
[2025-01-06 01:48:07,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:07,716][root][INFO] - Training Epoch: 9/10, step 198/574 completed (loss: 0.18845416605472565, acc: 0.9558823704719543)
[2025-01-06 01:48:07,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:08,034][root][INFO] - Training Epoch: 9/10, step 199/574 completed (loss: 0.20685355365276337, acc: 0.9338235259056091)
[2025-01-06 01:48:08,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:08,356][root][INFO] - Training Epoch: 9/10, step 200/574 completed (loss: 0.21322238445281982, acc: 0.9237288236618042)
[2025-01-06 01:48:08,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:08,666][root][INFO] - Training Epoch: 9/10, step 201/574 completed (loss: 0.12708614766597748, acc: 0.9701492786407471)
[2025-01-06 01:48:08,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:09,002][root][INFO] - Training Epoch: 9/10, step 202/574 completed (loss: 0.11235000193119049, acc: 0.9514563083648682)
[2025-01-06 01:48:09,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:09,304][root][INFO] - Training Epoch: 9/10, step 203/574 completed (loss: 0.02961895987391472, acc: 1.0)
[2025-01-06 01:48:09,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:09,601][root][INFO] - Training Epoch: 9/10, step 204/574 completed (loss: 0.005414175800979137, acc: 1.0)
[2025-01-06 01:48:09,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:09,913][root][INFO] - Training Epoch: 9/10, step 205/574 completed (loss: 0.08157932013273239, acc: 0.9775784611701965)
[2025-01-06 01:48:10,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:10,303][root][INFO] - Training Epoch: 9/10, step 206/574 completed (loss: 0.11219470947980881, acc: 0.9685039520263672)
[2025-01-06 01:48:10,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:10,626][root][INFO] - Training Epoch: 9/10, step 207/574 completed (loss: 0.11138926446437836, acc: 0.9784482717514038)
[2025-01-06 01:48:10,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:10,970][root][INFO] - Training Epoch: 9/10, step 208/574 completed (loss: 0.18200495839118958, acc: 0.945652186870575)
[2025-01-06 01:48:11,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:11,323][root][INFO] - Training Epoch: 9/10, step 209/574 completed (loss: 0.09856713563203812, acc: 0.9610894918441772)
[2025-01-06 01:48:11,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:11,654][root][INFO] - Training Epoch: 9/10, step 210/574 completed (loss: 0.050104472786188126, acc: 0.97826087474823)
[2025-01-06 01:48:11,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:11,961][root][INFO] - Training Epoch: 9/10, step 211/574 completed (loss: 0.007754837162792683, acc: 1.0)
[2025-01-06 01:48:12,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:12,262][root][INFO] - Training Epoch: 9/10, step 212/574 completed (loss: 0.008579655550420284, acc: 1.0)
[2025-01-06 01:48:12,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:12,565][root][INFO] - Training Epoch: 9/10, step 213/574 completed (loss: 0.0076357596553862095, acc: 1.0)
[2025-01-06 01:48:12,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:13,243][root][INFO] - Training Epoch: 9/10, step 214/574 completed (loss: 0.0318545438349247, acc: 0.9846153855323792)
[2025-01-06 01:48:13,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:13,561][root][INFO] - Training Epoch: 9/10, step 215/574 completed (loss: 0.03377619758248329, acc: 0.9729729890823364)
[2025-01-06 01:48:13,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:13,875][root][INFO] - Training Epoch: 9/10, step 216/574 completed (loss: 0.059209950268268585, acc: 0.9883720874786377)
[2025-01-06 01:48:14,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:14,405][root][INFO] - Training Epoch: 9/10, step 217/574 completed (loss: 0.08912704885005951, acc: 0.9819819927215576)
[2025-01-06 01:48:14,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:14,787][root][INFO] - Training Epoch: 9/10, step 218/574 completed (loss: 0.036172136664390564, acc: 0.9777777791023254)
[2025-01-06 01:48:14,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:15,076][root][INFO] - Training Epoch: 9/10, step 219/574 completed (loss: 0.0023221485316753387, acc: 1.0)
[2025-01-06 01:48:15,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:15,362][root][INFO] - Training Epoch: 9/10, step 220/574 completed (loss: 0.04941648617386818, acc: 0.9629629850387573)
[2025-01-06 01:48:15,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:15,637][root][INFO] - Training Epoch: 9/10, step 221/574 completed (loss: 0.00712280860170722, acc: 1.0)
[2025-01-06 01:48:15,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:15,913][root][INFO] - Training Epoch: 9/10, step 222/574 completed (loss: 0.07344119250774384, acc: 0.9807692170143127)
[2025-01-06 01:48:16,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:16,674][root][INFO] - Training Epoch: 9/10, step 223/574 completed (loss: 0.08634795248508453, acc: 0.967391312122345)
[2025-01-06 01:48:16,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:17,213][root][INFO] - Training Epoch: 9/10, step 224/574 completed (loss: 0.11515757441520691, acc: 0.9715909361839294)
[2025-01-06 01:48:17,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:17,644][root][INFO] - Training Epoch: 9/10, step 225/574 completed (loss: 0.06700345128774643, acc: 0.9893617033958435)
[2025-01-06 01:48:17,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:17,986][root][INFO] - Training Epoch: 9/10, step 226/574 completed (loss: 0.03761034458875656, acc: 1.0)
[2025-01-06 01:48:18,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:18,357][root][INFO] - Training Epoch: 9/10, step 227/574 completed (loss: 0.04231591522693634, acc: 1.0)
[2025-01-06 01:48:18,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:18,720][root][INFO] - Training Epoch: 9/10, step 228/574 completed (loss: 0.026810402050614357, acc: 1.0)
[2025-01-06 01:48:18,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:19,023][root][INFO] - Training Epoch: 9/10, step 229/574 completed (loss: 0.08755362033843994, acc: 0.9666666388511658)
[2025-01-06 01:48:19,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:19,379][root][INFO] - Training Epoch: 9/10, step 230/574 completed (loss: 0.3972838819026947, acc: 0.8947368264198303)
[2025-01-06 01:48:19,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:19,683][root][INFO] - Training Epoch: 9/10, step 231/574 completed (loss: 0.43114030361175537, acc: 0.8444444537162781)
[2025-01-06 01:48:19,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:20,094][root][INFO] - Training Epoch: 9/10, step 232/574 completed (loss: 0.44466620683670044, acc: 0.8722222447395325)
[2025-01-06 01:48:20,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:20,589][root][INFO] - Training Epoch: 9/10, step 233/574 completed (loss: 0.8061618208885193, acc: 0.747706413269043)
[2025-01-06 01:48:20,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:21,063][root][INFO] - Training Epoch: 9/10, step 234/574 completed (loss: 0.35951393842697144, acc: 0.9076923131942749)
[2025-01-06 01:48:21,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:21,362][root][INFO] - Training Epoch: 9/10, step 235/574 completed (loss: 0.023803314194083214, acc: 1.0)
[2025-01-06 01:48:21,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:21,663][root][INFO] - Training Epoch: 9/10, step 236/574 completed (loss: 0.006193581968545914, acc: 1.0)
[2025-01-06 01:48:21,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:21,961][root][INFO] - Training Epoch: 9/10, step 237/574 completed (loss: 0.044847723096609116, acc: 1.0)
[2025-01-06 01:48:22,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:22,260][root][INFO] - Training Epoch: 9/10, step 238/574 completed (loss: 0.15910175442695618, acc: 0.9629629850387573)
[2025-01-06 01:48:22,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:22,558][root][INFO] - Training Epoch: 9/10, step 239/574 completed (loss: 0.03855368122458458, acc: 1.0)
[2025-01-06 01:48:22,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:22,885][root][INFO] - Training Epoch: 9/10, step 240/574 completed (loss: 0.2471858561038971, acc: 0.9545454382896423)
[2025-01-06 01:48:22,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:23,182][root][INFO] - Training Epoch: 9/10, step 241/574 completed (loss: 0.06224854290485382, acc: 1.0)
[2025-01-06 01:48:23,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:23,758][root][INFO] - Training Epoch: 9/10, step 242/574 completed (loss: 0.11718442291021347, acc: 0.9516128897666931)
[2025-01-06 01:48:23,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:24,287][root][INFO] - Training Epoch: 9/10, step 243/574 completed (loss: 0.1566675305366516, acc: 0.9545454382896423)
[2025-01-06 01:48:24,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:24,580][root][INFO] - Training Epoch: 9/10, step 244/574 completed (loss: 0.017433037981390953, acc: 1.0)
[2025-01-06 01:48:24,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:24,885][root][INFO] - Training Epoch: 9/10, step 245/574 completed (loss: 0.2580822706222534, acc: 0.9615384340286255)
[2025-01-06 01:48:24,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:25,187][root][INFO] - Training Epoch: 9/10, step 246/574 completed (loss: 0.14228682219982147, acc: 0.9677419066429138)
[2025-01-06 01:48:25,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:25,486][root][INFO] - Training Epoch: 9/10, step 247/574 completed (loss: 0.004656163044273853, acc: 1.0)
[2025-01-06 01:48:25,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:25,826][root][INFO] - Training Epoch: 9/10, step 248/574 completed (loss: 0.017399946227669716, acc: 1.0)
[2025-01-06 01:48:25,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:26,128][root][INFO] - Training Epoch: 9/10, step 249/574 completed (loss: 0.036253832280635834, acc: 0.9729729890823364)
[2025-01-06 01:48:26,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:26,433][root][INFO] - Training Epoch: 9/10, step 250/574 completed (loss: 0.002996870782226324, acc: 1.0)
[2025-01-06 01:48:26,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:26,764][root][INFO] - Training Epoch: 9/10, step 251/574 completed (loss: 0.028757674619555473, acc: 1.0)
[2025-01-06 01:48:26,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:27,107][root][INFO] - Training Epoch: 9/10, step 252/574 completed (loss: 0.058482006192207336, acc: 0.9756097793579102)
[2025-01-06 01:48:27,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:27,418][root][INFO] - Training Epoch: 9/10, step 253/574 completed (loss: 0.015833009034395218, acc: 1.0)
[2025-01-06 01:48:27,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:27,710][root][INFO] - Training Epoch: 9/10, step 254/574 completed (loss: 0.0001600670802872628, acc: 1.0)
[2025-01-06 01:48:27,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28,012][root][INFO] - Training Epoch: 9/10, step 255/574 completed (loss: 0.0010151425376534462, acc: 1.0)
[2025-01-06 01:48:28,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28,339][root][INFO] - Training Epoch: 9/10, step 256/574 completed (loss: 0.007225082255899906, acc: 1.0)
[2025-01-06 01:48:28,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28,671][root][INFO] - Training Epoch: 9/10, step 257/574 completed (loss: 0.006562829948961735, acc: 1.0)
[2025-01-06 01:48:28,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28,983][root][INFO] - Training Epoch: 9/10, step 258/574 completed (loss: 0.12475623935461044, acc: 0.9605262875556946)
[2025-01-06 01:48:29,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:29,565][root][INFO] - Training Epoch: 9/10, step 259/574 completed (loss: 0.02138112671673298, acc: 1.0)
[2025-01-06 01:48:29,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:30,147][root][INFO] - Training Epoch: 9/10, step 260/574 completed (loss: 0.037408795207738876, acc: 0.9916666746139526)
[2025-01-06 01:48:30,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:30,437][root][INFO] - Training Epoch: 9/10, step 261/574 completed (loss: 0.013069923967123032, acc: 1.0)
[2025-01-06 01:48:30,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:30,738][root][INFO] - Training Epoch: 9/10, step 262/574 completed (loss: 0.041982393711805344, acc: 0.9677419066429138)
[2025-01-06 01:48:30,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:31,072][root][INFO] - Training Epoch: 9/10, step 263/574 completed (loss: 0.18293648958206177, acc: 0.9466666579246521)
[2025-01-06 01:48:31,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:31,377][root][INFO] - Training Epoch: 9/10, step 264/574 completed (loss: 0.09115774184465408, acc: 0.9791666865348816)
[2025-01-06 01:48:31,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:32,222][root][INFO] - Training Epoch: 9/10, step 265/574 completed (loss: 0.28887706995010376, acc: 0.9200000166893005)
[2025-01-06 01:48:32,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:32,542][root][INFO] - Training Epoch: 9/10, step 266/574 completed (loss: 0.26280421018600464, acc: 0.9438202381134033)
[2025-01-06 01:48:32,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:32,877][root][INFO] - Training Epoch: 9/10, step 267/574 completed (loss: 0.07786864787340164, acc: 0.9864864945411682)
[2025-01-06 01:48:32,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:33,336][root][INFO] - Training Epoch: 9/10, step 268/574 completed (loss: 0.03185277059674263, acc: 1.0)
[2025-01-06 01:48:33,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:33,637][root][INFO] - Training Epoch: 9/10, step 269/574 completed (loss: 0.002837412292137742, acc: 1.0)
[2025-01-06 01:48:34,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:34,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:35,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:35,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:35,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:37,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:37,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:37,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:38,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:38,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:39,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:39,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:39,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:39,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:40,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:40,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:40,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:41,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:41,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:41,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:43,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:43,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:43,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:43,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:45,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:45,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:45,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:48,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:48,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:48,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:49,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:49,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:50,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:50,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:50,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:51,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:51,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:51,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:52,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:52,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:52,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:53,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:53,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:53,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:54,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:54,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:54,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:55,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:55,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:56,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:56,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:56,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:57,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:57,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:57,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:59,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:59,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:59,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:00,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:00,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:00,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:01,503][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5092, device='cuda:0') eval_epoch_loss=tensor(0.9199, device='cuda:0') eval_epoch_acc=tensor(0.8232, device='cuda:0')
[2025-01-06 01:49:01,505][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:49:01,505][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:49:01,751][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_270_loss_0.9199445843696594/model.pt
[2025-01-06 01:49:01,762][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:49:01,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:02,152][root][INFO] - Training Epoch: 9/10, step 270/574 completed (loss: 0.0033226122613996267, acc: 1.0)
[2025-01-06 01:49:02,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:02,452][root][INFO] - Training Epoch: 9/10, step 271/574 completed (loss: 0.034376125782728195, acc: 0.96875)
[2025-01-06 01:49:02,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:02,753][root][INFO] - Training Epoch: 9/10, step 272/574 completed (loss: 0.014521409757435322, acc: 1.0)
[2025-01-06 01:49:02,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:03,136][root][INFO] - Training Epoch: 9/10, step 273/574 completed (loss: 0.21866287291049957, acc: 0.9166666865348816)
[2025-01-06 01:49:03,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:03,432][root][INFO] - Training Epoch: 9/10, step 274/574 completed (loss: 0.03229515999555588, acc: 1.0)
[2025-01-06 01:49:03,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:03,775][root][INFO] - Training Epoch: 9/10, step 275/574 completed (loss: 0.001950367004610598, acc: 1.0)
[2025-01-06 01:49:03,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:04,116][root][INFO] - Training Epoch: 9/10, step 276/574 completed (loss: 0.0013889132533222437, acc: 1.0)
[2025-01-06 01:49:04,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:04,520][root][INFO] - Training Epoch: 9/10, step 277/574 completed (loss: 0.0026577108073979616, acc: 1.0)
[2025-01-06 01:49:04,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:04,868][root][INFO] - Training Epoch: 9/10, step 278/574 completed (loss: 0.009314077906310558, acc: 1.0)
[2025-01-06 01:49:04,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:05,201][root][INFO] - Training Epoch: 9/10, step 279/574 completed (loss: 0.015233281068503857, acc: 1.0)
[2025-01-06 01:49:05,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:05,523][root][INFO] - Training Epoch: 9/10, step 280/574 completed (loss: 0.007322547025978565, acc: 1.0)
[2025-01-06 01:49:05,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:05,976][root][INFO] - Training Epoch: 9/10, step 281/574 completed (loss: 0.21733742952346802, acc: 0.9397590160369873)
[2025-01-06 01:49:06,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:06,345][root][INFO] - Training Epoch: 9/10, step 282/574 completed (loss: 0.13373495638370514, acc: 0.9444444179534912)
[2025-01-06 01:49:06,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:06,686][root][INFO] - Training Epoch: 9/10, step 283/574 completed (loss: 0.2564164102077484, acc: 0.9473684430122375)
[2025-01-06 01:49:06,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:06,986][root][INFO] - Training Epoch: 9/10, step 284/574 completed (loss: 0.182519793510437, acc: 0.970588207244873)
[2025-01-06 01:49:07,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:07,321][root][INFO] - Training Epoch: 9/10, step 285/574 completed (loss: 0.00564234796911478, acc: 1.0)
[2025-01-06 01:49:07,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:07,673][root][INFO] - Training Epoch: 9/10, step 286/574 completed (loss: 0.06661907583475113, acc: 0.984375)
[2025-01-06 01:49:07,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:08,034][root][INFO] - Training Epoch: 9/10, step 287/574 completed (loss: 0.11254991590976715, acc: 0.9520000219345093)
[2025-01-06 01:49:08,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:08,351][root][INFO] - Training Epoch: 9/10, step 288/574 completed (loss: 0.19480526447296143, acc: 0.9670329689979553)
[2025-01-06 01:49:08,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:08,668][root][INFO] - Training Epoch: 9/10, step 289/574 completed (loss: 0.07154016196727753, acc: 0.9689440727233887)
[2025-01-06 01:49:08,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:09,035][root][INFO] - Training Epoch: 9/10, step 290/574 completed (loss: 0.14933398365974426, acc: 0.9639175534248352)
[2025-01-06 01:49:09,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:09,341][root][INFO] - Training Epoch: 9/10, step 291/574 completed (loss: 0.0041465419344604015, acc: 1.0)
[2025-01-06 01:49:09,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:09,642][root][INFO] - Training Epoch: 9/10, step 292/574 completed (loss: 0.008162298239767551, acc: 1.0)
[2025-01-06 01:49:09,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:10,005][root][INFO] - Training Epoch: 9/10, step 293/574 completed (loss: 0.002707928419113159, acc: 1.0)
[2025-01-06 01:49:10,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:10,507][root][INFO] - Training Epoch: 9/10, step 294/574 completed (loss: 0.019225023686885834, acc: 1.0)
[2025-01-06 01:49:10,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11,068][root][INFO] - Training Epoch: 9/10, step 295/574 completed (loss: 0.16114337742328644, acc: 0.9639175534248352)
[2025-01-06 01:49:11,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11,327][root][INFO] - Training Epoch: 9/10, step 296/574 completed (loss: 0.03266581892967224, acc: 1.0)
[2025-01-06 01:49:11,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11,647][root][INFO] - Training Epoch: 9/10, step 297/574 completed (loss: 0.003024518257007003, acc: 1.0)
[2025-01-06 01:49:11,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11,947][root][INFO] - Training Epoch: 9/10, step 298/574 completed (loss: 0.0101160304620862, acc: 1.0)
[2025-01-06 01:49:12,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:12,248][root][INFO] - Training Epoch: 9/10, step 299/574 completed (loss: 0.1142028197646141, acc: 0.9821428656578064)
[2025-01-06 01:49:12,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:12,548][root][INFO] - Training Epoch: 9/10, step 300/574 completed (loss: 0.020174624398350716, acc: 1.0)
[2025-01-06 01:49:12,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:12,923][root][INFO] - Training Epoch: 9/10, step 301/574 completed (loss: 0.014420125633478165, acc: 1.0)
[2025-01-06 01:49:13,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:13,317][root][INFO] - Training Epoch: 9/10, step 302/574 completed (loss: 0.0057917372323572636, acc: 1.0)
[2025-01-06 01:49:13,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:13,645][root][INFO] - Training Epoch: 9/10, step 303/574 completed (loss: 0.0010357660939916968, acc: 1.0)
[2025-01-06 01:49:13,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:13,952][root][INFO] - Training Epoch: 9/10, step 304/574 completed (loss: 0.028772631660103798, acc: 1.0)
[2025-01-06 01:49:14,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:14,280][root][INFO] - Training Epoch: 9/10, step 305/574 completed (loss: 0.07502831518650055, acc: 0.9836065769195557)
[2025-01-06 01:49:14,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:14,629][root][INFO] - Training Epoch: 9/10, step 306/574 completed (loss: 0.009022674523293972, acc: 1.0)
[2025-01-06 01:49:14,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:14,967][root][INFO] - Training Epoch: 9/10, step 307/574 completed (loss: 0.000489480618853122, acc: 1.0)
[2025-01-06 01:49:15,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:15,297][root][INFO] - Training Epoch: 9/10, step 308/574 completed (loss: 0.03181818127632141, acc: 1.0)
[2025-01-06 01:49:15,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:15,745][root][INFO] - Training Epoch: 9/10, step 309/574 completed (loss: 0.04436575248837471, acc: 0.9861111044883728)
[2025-01-06 01:49:15,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:16,058][root][INFO] - Training Epoch: 9/10, step 310/574 completed (loss: 0.09303351491689682, acc: 0.9759036302566528)
[2025-01-06 01:49:16,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:16,382][root][INFO] - Training Epoch: 9/10, step 311/574 completed (loss: 0.027325797826051712, acc: 0.9871794581413269)
[2025-01-06 01:49:16,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:16,729][root][INFO] - Training Epoch: 9/10, step 312/574 completed (loss: 0.05070962756872177, acc: 0.9795918464660645)
[2025-01-06 01:49:16,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:17,040][root][INFO] - Training Epoch: 9/10, step 313/574 completed (loss: 0.0018906063633039594, acc: 1.0)
[2025-01-06 01:49:17,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:17,463][root][INFO] - Training Epoch: 9/10, step 314/574 completed (loss: 0.057928476482629776, acc: 0.9583333134651184)
[2025-01-06 01:49:17,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:17,813][root][INFO] - Training Epoch: 9/10, step 315/574 completed (loss: 0.0027484381571412086, acc: 1.0)
[2025-01-06 01:49:17,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:18,150][root][INFO] - Training Epoch: 9/10, step 316/574 completed (loss: 0.16962219774723053, acc: 0.9677419066429138)
[2025-01-06 01:49:18,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:18,505][root][INFO] - Training Epoch: 9/10, step 317/574 completed (loss: 0.02991306222975254, acc: 0.9850746393203735)
[2025-01-06 01:49:18,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:18,841][root][INFO] - Training Epoch: 9/10, step 318/574 completed (loss: 0.0034330817870795727, acc: 1.0)
[2025-01-06 01:49:18,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:19,192][root][INFO] - Training Epoch: 9/10, step 319/574 completed (loss: 0.006345882546156645, acc: 1.0)
[2025-01-06 01:49:19,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:19,533][root][INFO] - Training Epoch: 9/10, step 320/574 completed (loss: 0.043688077479600906, acc: 0.9838709831237793)
[2025-01-06 01:49:19,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:19,856][root][INFO] - Training Epoch: 9/10, step 321/574 completed (loss: 0.0005384897231124341, acc: 1.0)
[2025-01-06 01:49:19,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:20,208][root][INFO] - Training Epoch: 9/10, step 322/574 completed (loss: 0.08204015344381332, acc: 1.0)
[2025-01-06 01:49:20,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:20,575][root][INFO] - Training Epoch: 9/10, step 323/574 completed (loss: 0.1135789081454277, acc: 0.9714285731315613)
[2025-01-06 01:49:20,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:20,920][root][INFO] - Training Epoch: 9/10, step 324/574 completed (loss: 0.07925249636173248, acc: 0.9487179517745972)
[2025-01-06 01:49:21,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:21,230][root][INFO] - Training Epoch: 9/10, step 325/574 completed (loss: 0.13615530729293823, acc: 0.9268292784690857)
[2025-01-06 01:49:21,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:21,547][root][INFO] - Training Epoch: 9/10, step 326/574 completed (loss: 0.2026943415403366, acc: 0.9736841917037964)
[2025-01-06 01:49:21,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:21,874][root][INFO] - Training Epoch: 9/10, step 327/574 completed (loss: 0.009997415356338024, acc: 1.0)
[2025-01-06 01:49:21,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:22,195][root][INFO] - Training Epoch: 9/10, step 328/574 completed (loss: 0.028846191242337227, acc: 1.0)
[2025-01-06 01:49:22,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:22,514][root][INFO] - Training Epoch: 9/10, step 329/574 completed (loss: 0.056901175528764725, acc: 0.9629629850387573)
[2025-01-06 01:49:22,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:22,834][root][INFO] - Training Epoch: 9/10, step 330/574 completed (loss: 0.05504335090517998, acc: 0.96875)
[2025-01-06 01:49:22,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:23,179][root][INFO] - Training Epoch: 9/10, step 331/574 completed (loss: 0.026153435930609703, acc: 0.9838709831237793)
[2025-01-06 01:49:23,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:23,541][root][INFO] - Training Epoch: 9/10, step 332/574 completed (loss: 0.004630747716873884, acc: 1.0)
[2025-01-06 01:49:23,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:23,853][root][INFO] - Training Epoch: 9/10, step 333/574 completed (loss: 0.006191552616655827, acc: 1.0)
[2025-01-06 01:49:23,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:24,201][root][INFO] - Training Epoch: 9/10, step 334/574 completed (loss: 0.04429163411259651, acc: 1.0)
[2025-01-06 01:49:24,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:24,514][root][INFO] - Training Epoch: 9/10, step 335/574 completed (loss: 0.0022742259316146374, acc: 1.0)
[2025-01-06 01:49:24,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:24,830][root][INFO] - Training Epoch: 9/10, step 336/574 completed (loss: 0.04283278062939644, acc: 1.0)
[2025-01-06 01:49:24,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:25,166][root][INFO] - Training Epoch: 9/10, step 337/574 completed (loss: 0.12853188812732697, acc: 0.9655172228813171)
[2025-01-06 01:49:25,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:25,504][root][INFO] - Training Epoch: 9/10, step 338/574 completed (loss: 0.24005214869976044, acc: 0.9468085169792175)
[2025-01-06 01:49:25,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:25,831][root][INFO] - Training Epoch: 9/10, step 339/574 completed (loss: 0.3953699767589569, acc: 0.9156626462936401)
[2025-01-06 01:49:25,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:26,164][root][INFO] - Training Epoch: 9/10, step 340/574 completed (loss: 0.00422151992097497, acc: 1.0)
[2025-01-06 01:49:26,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:26,483][root][INFO] - Training Epoch: 9/10, step 341/574 completed (loss: 0.009356711059808731, acc: 1.0)
[2025-01-06 01:49:26,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:26,820][root][INFO] - Training Epoch: 9/10, step 342/574 completed (loss: 0.14295239746570587, acc: 0.9638554453849792)
[2025-01-06 01:49:26,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:27,125][root][INFO] - Training Epoch: 9/10, step 343/574 completed (loss: 0.09119585901498795, acc: 0.9433962106704712)
[2025-01-06 01:49:27,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:27,435][root][INFO] - Training Epoch: 9/10, step 344/574 completed (loss: 0.02001519687473774, acc: 1.0)
[2025-01-06 01:49:27,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:27,789][root][INFO] - Training Epoch: 9/10, step 345/574 completed (loss: 0.0019303846638649702, acc: 1.0)
[2025-01-06 01:49:27,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:28,101][root][INFO] - Training Epoch: 9/10, step 346/574 completed (loss: 0.030088087543845177, acc: 0.9850746393203735)
[2025-01-06 01:49:28,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:28,456][root][INFO] - Training Epoch: 9/10, step 347/574 completed (loss: 0.0003096677246503532, acc: 1.0)
[2025-01-06 01:49:28,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:28,792][root][INFO] - Training Epoch: 9/10, step 348/574 completed (loss: 0.003622096963226795, acc: 1.0)
[2025-01-06 01:49:28,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:29,221][root][INFO] - Training Epoch: 9/10, step 349/574 completed (loss: 0.17726004123687744, acc: 0.9166666865348816)
[2025-01-06 01:49:29,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:29,565][root][INFO] - Training Epoch: 9/10, step 350/574 completed (loss: 0.48583173751831055, acc: 0.9069767594337463)
[2025-01-06 01:49:29,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:29,921][root][INFO] - Training Epoch: 9/10, step 351/574 completed (loss: 0.007488385774195194, acc: 1.0)
[2025-01-06 01:49:30,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:30,297][root][INFO] - Training Epoch: 9/10, step 352/574 completed (loss: 0.07972703129053116, acc: 0.9555555582046509)
[2025-01-06 01:49:30,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:30,634][root][INFO] - Training Epoch: 9/10, step 353/574 completed (loss: 0.015060567297041416, acc: 1.0)
[2025-01-06 01:49:30,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:31,018][root][INFO] - Training Epoch: 9/10, step 354/574 completed (loss: 0.0072332420386374, acc: 1.0)
[2025-01-06 01:49:31,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:31,426][root][INFO] - Training Epoch: 9/10, step 355/574 completed (loss: 0.13288284838199615, acc: 0.9450549483299255)
[2025-01-06 01:49:31,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:31,949][root][INFO] - Training Epoch: 9/10, step 356/574 completed (loss: 0.07379597425460815, acc: 0.9739130139350891)
[2025-01-06 01:49:32,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:32,278][root][INFO] - Training Epoch: 9/10, step 357/574 completed (loss: 0.06390725076198578, acc: 0.97826087474823)
[2025-01-06 01:49:32,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:32,630][root][INFO] - Training Epoch: 9/10, step 358/574 completed (loss: 0.04644423723220825, acc: 0.9795918464660645)
[2025-01-06 01:49:32,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:32,927][root][INFO] - Training Epoch: 9/10, step 359/574 completed (loss: 0.002019907347857952, acc: 1.0)
[2025-01-06 01:49:33,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:33,227][root][INFO] - Training Epoch: 9/10, step 360/574 completed (loss: 0.007172172423452139, acc: 1.0)
[2025-01-06 01:49:33,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:33,521][root][INFO] - Training Epoch: 9/10, step 361/574 completed (loss: 0.007049932144582272, acc: 1.0)
[2025-01-06 01:49:33,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:33,856][root][INFO] - Training Epoch: 9/10, step 362/574 completed (loss: 0.022161297500133514, acc: 1.0)
[2025-01-06 01:49:33,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:34,199][root][INFO] - Training Epoch: 9/10, step 363/574 completed (loss: 0.052681803703308105, acc: 0.9736841917037964)
[2025-01-06 01:49:34,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:34,540][root][INFO] - Training Epoch: 9/10, step 364/574 completed (loss: 0.03726160526275635, acc: 0.9756097793579102)
[2025-01-06 01:49:34,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:34,838][root][INFO] - Training Epoch: 9/10, step 365/574 completed (loss: 0.41651296615600586, acc: 0.939393937587738)
[2025-01-06 01:49:34,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:35,137][root][INFO] - Training Epoch: 9/10, step 366/574 completed (loss: 0.00010611756442813203, acc: 1.0)
[2025-01-06 01:49:35,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:35,439][root][INFO] - Training Epoch: 9/10, step 367/574 completed (loss: 0.0041528199799358845, acc: 1.0)
[2025-01-06 01:49:35,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:35,762][root][INFO] - Training Epoch: 9/10, step 368/574 completed (loss: 0.032702427357435226, acc: 1.0)
[2025-01-06 01:49:35,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:36,040][root][INFO] - Training Epoch: 9/10, step 369/574 completed (loss: 0.008341647684574127, acc: 1.0)
[2025-01-06 01:49:36,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:36,673][root][INFO] - Training Epoch: 9/10, step 370/574 completed (loss: 0.12891097366809845, acc: 0.9575757384300232)
[2025-01-06 01:49:36,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:37,525][root][INFO] - Training Epoch: 9/10, step 371/574 completed (loss: 0.04758840054273605, acc: 0.9811320900917053)
[2025-01-06 01:49:37,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:37,873][root][INFO] - Training Epoch: 9/10, step 372/574 completed (loss: 0.041297025978565216, acc: 0.9888888597488403)
[2025-01-06 01:49:37,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:38,222][root][INFO] - Training Epoch: 9/10, step 373/574 completed (loss: 0.009746755473315716, acc: 1.0)
[2025-01-06 01:49:38,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:38,553][root][INFO] - Training Epoch: 9/10, step 374/574 completed (loss: 0.009459763765335083, acc: 1.0)
[2025-01-06 01:49:38,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:38,874][root][INFO] - Training Epoch: 9/10, step 375/574 completed (loss: 0.00017834268510341644, acc: 1.0)
[2025-01-06 01:49:38,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:39,180][root][INFO] - Training Epoch: 9/10, step 376/574 completed (loss: 0.0003672648163046688, acc: 1.0)
[2025-01-06 01:49:39,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:39,497][root][INFO] - Training Epoch: 9/10, step 377/574 completed (loss: 0.011914008297026157, acc: 1.0)
[2025-01-06 01:49:39,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:39,813][root][INFO] - Training Epoch: 9/10, step 378/574 completed (loss: 0.0019803636241704226, acc: 1.0)
[2025-01-06 01:49:39,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:40,425][root][INFO] - Training Epoch: 9/10, step 379/574 completed (loss: 0.13162513077259064, acc: 0.9520958065986633)
[2025-01-06 01:49:40,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:40,837][root][INFO] - Training Epoch: 9/10, step 380/574 completed (loss: 0.0745573565363884, acc: 0.9774436354637146)
[2025-01-06 01:49:41,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:42,116][root][INFO] - Training Epoch: 9/10, step 381/574 completed (loss: 0.16890084743499756, acc: 0.9251337051391602)
[2025-01-06 01:49:42,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:42,698][root][INFO] - Training Epoch: 9/10, step 382/574 completed (loss: 0.09595734626054764, acc: 0.9819819927215576)
[2025-01-06 01:49:42,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:43,022][root][INFO] - Training Epoch: 9/10, step 383/574 completed (loss: 0.01706506311893463, acc: 1.0)
[2025-01-06 01:49:43,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:43,346][root][INFO] - Training Epoch: 9/10, step 384/574 completed (loss: 0.0016062009381130338, acc: 1.0)
[2025-01-06 01:49:43,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:43,750][root][INFO] - Training Epoch: 9/10, step 385/574 completed (loss: 0.005911483895033598, acc: 1.0)
[2025-01-06 01:49:43,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:44,106][root][INFO] - Training Epoch: 9/10, step 386/574 completed (loss: 0.009740266017615795, acc: 1.0)
[2025-01-06 01:49:44,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:44,504][root][INFO] - Training Epoch: 9/10, step 387/574 completed (loss: 0.0022641511168330908, acc: 1.0)
[2025-01-06 01:49:44,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:44,836][root][INFO] - Training Epoch: 9/10, step 388/574 completed (loss: 0.00031112474971450865, acc: 1.0)
[2025-01-06 01:49:44,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45,122][root][INFO] - Training Epoch: 9/10, step 389/574 completed (loss: 0.0003006465267390013, acc: 1.0)
[2025-01-06 01:49:45,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45,424][root][INFO] - Training Epoch: 9/10, step 390/574 completed (loss: 0.06704557687044144, acc: 0.9523809552192688)
[2025-01-06 01:49:45,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45,829][root][INFO] - Training Epoch: 9/10, step 391/574 completed (loss: 0.14115045964717865, acc: 0.9629629850387573)
[2025-01-06 01:49:46,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:46,264][root][INFO] - Training Epoch: 9/10, step 392/574 completed (loss: 0.23486587405204773, acc: 0.893203854560852)
[2025-01-06 01:49:46,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:46,814][root][INFO] - Training Epoch: 9/10, step 393/574 completed (loss: 0.41142481565475464, acc: 0.8602941036224365)
[2025-01-06 01:49:46,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:47,215][root][INFO] - Training Epoch: 9/10, step 394/574 completed (loss: 0.199760302901268, acc: 0.9466666579246521)
[2025-01-06 01:49:47,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:47,617][root][INFO] - Training Epoch: 9/10, step 395/574 completed (loss: 0.3295583724975586, acc: 0.9027777910232544)
[2025-01-06 01:49:47,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:48,030][root][INFO] - Training Epoch: 9/10, step 396/574 completed (loss: 0.0390082485973835, acc: 0.9767441749572754)
[2025-01-06 01:49:48,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:48,376][root][INFO] - Training Epoch: 9/10, step 397/574 completed (loss: 0.007235990837216377, acc: 1.0)
[2025-01-06 01:49:48,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:48,777][root][INFO] - Training Epoch: 9/10, step 398/574 completed (loss: 0.04048573598265648, acc: 1.0)
[2025-01-06 01:49:48,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:49,199][root][INFO] - Training Epoch: 9/10, step 399/574 completed (loss: 0.14490878582000732, acc: 0.9599999785423279)
[2025-01-06 01:49:49,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:49,790][root][INFO] - Training Epoch: 9/10, step 400/574 completed (loss: 0.0605289451777935, acc: 0.970588207244873)
[2025-01-06 01:49:49,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:50,094][root][INFO] - Training Epoch: 9/10, step 401/574 completed (loss: 0.03690589219331741, acc: 0.9866666793823242)
[2025-01-06 01:49:50,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:50,430][root][INFO] - Training Epoch: 9/10, step 402/574 completed (loss: 0.0033190613612532616, acc: 1.0)
[2025-01-06 01:49:50,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:50,760][root][INFO] - Training Epoch: 9/10, step 403/574 completed (loss: 0.007591390050947666, acc: 1.0)
[2025-01-06 01:49:50,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:51,041][root][INFO] - Training Epoch: 9/10, step 404/574 completed (loss: 0.018361886963248253, acc: 1.0)
[2025-01-06 01:49:51,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:51,256][root][INFO] - Training Epoch: 9/10, step 405/574 completed (loss: 0.0019573408644646406, acc: 1.0)
[2025-01-06 01:49:51,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:51,488][root][INFO] - Training Epoch: 9/10, step 406/574 completed (loss: 0.0014949525939300656, acc: 1.0)
[2025-01-06 01:49:51,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:51,809][root][INFO] - Training Epoch: 9/10, step 407/574 completed (loss: 0.006712730508297682, acc: 1.0)
[2025-01-06 01:49:51,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:52,189][root][INFO] - Training Epoch: 9/10, step 408/574 completed (loss: 0.04193837568163872, acc: 0.9629629850387573)
[2025-01-06 01:49:52,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:52,554][root][INFO] - Training Epoch: 9/10, step 409/574 completed (loss: 0.004213918931782246, acc: 1.0)
[2025-01-06 01:49:52,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:52,922][root][INFO] - Training Epoch: 9/10, step 410/574 completed (loss: 0.004652881994843483, acc: 1.0)
[2025-01-06 01:49:53,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:53,235][root][INFO] - Training Epoch: 9/10, step 411/574 completed (loss: 0.00830634031444788, acc: 1.0)
[2025-01-06 01:49:53,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:53,600][root][INFO] - Training Epoch: 9/10, step 412/574 completed (loss: 0.06095750629901886, acc: 0.9666666388511658)
[2025-01-06 01:49:54,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:54,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:55,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:55,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:55,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:56,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:56,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:56,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:57,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:57,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:57,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:58,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:58,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:59,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:59,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:59,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:02,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:02,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:02,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:03,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:03,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:03,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:05,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:05,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:05,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:06,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:06,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:06,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:07,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:07,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:07,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:08,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:08,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:08,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:10,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:10,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:10,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:12,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:12,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:13,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:13,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:13,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:14,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:14,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:15,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:15,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:15,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:16,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:16,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:17,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:17,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:17,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:18,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:18,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:18,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:20,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:20,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:20,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:21,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:21,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:21,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:22,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:23,023][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5254, device='cuda:0') eval_epoch_loss=tensor(0.9264, device='cuda:0') eval_epoch_acc=tensor(0.8239, device='cuda:0')
[2025-01-06 01:50:23,025][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:50:23,025][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:50:23,324][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_413_loss_0.9263991713523865/model.pt
[2025-01-06 01:50:23,336][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:50:23,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:23,783][root][INFO] - Training Epoch: 9/10, step 413/574 completed (loss: 0.0022779677528887987, acc: 1.0)
[2025-01-06 01:50:23,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24,163][root][INFO] - Training Epoch: 9/10, step 414/574 completed (loss: 0.006961076054722071, acc: 1.0)
[2025-01-06 01:50:24,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24,502][root][INFO] - Training Epoch: 9/10, step 415/574 completed (loss: 0.09160646796226501, acc: 0.9607843160629272)
[2025-01-06 01:50:24,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24,834][root][INFO] - Training Epoch: 9/10, step 416/574 completed (loss: 0.16216722130775452, acc: 0.9230769276618958)
[2025-01-06 01:50:24,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:25,180][root][INFO] - Training Epoch: 9/10, step 417/574 completed (loss: 0.028828339651226997, acc: 1.0)
[2025-01-06 01:50:25,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:25,555][root][INFO] - Training Epoch: 9/10, step 418/574 completed (loss: 0.028244774788618088, acc: 0.9750000238418579)
[2025-01-06 01:50:25,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:25,896][root][INFO] - Training Epoch: 9/10, step 419/574 completed (loss: 0.007939593866467476, acc: 1.0)
[2025-01-06 01:50:26,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:26,248][root][INFO] - Training Epoch: 9/10, step 420/574 completed (loss: 0.0009848015615716577, acc: 1.0)
[2025-01-06 01:50:26,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:26,601][root][INFO] - Training Epoch: 9/10, step 421/574 completed (loss: 0.11926867812871933, acc: 0.9666666388511658)
[2025-01-06 01:50:26,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:26,925][root][INFO] - Training Epoch: 9/10, step 422/574 completed (loss: 0.47740408778190613, acc: 0.96875)
[2025-01-06 01:50:27,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:27,266][root][INFO] - Training Epoch: 9/10, step 423/574 completed (loss: 0.045279283076524734, acc: 0.9722222089767456)
[2025-01-06 01:50:27,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:27,607][root][INFO] - Training Epoch: 9/10, step 424/574 completed (loss: 0.002748813247308135, acc: 1.0)
[2025-01-06 01:50:27,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:27,947][root][INFO] - Training Epoch: 9/10, step 425/574 completed (loss: 0.011101865209639072, acc: 1.0)
[2025-01-06 01:50:28,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:28,300][root][INFO] - Training Epoch: 9/10, step 426/574 completed (loss: 0.0007808012305758893, acc: 1.0)
[2025-01-06 01:50:28,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:28,638][root][INFO] - Training Epoch: 9/10, step 427/574 completed (loss: 0.008383318781852722, acc: 1.0)
[2025-01-06 01:50:28,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:28,960][root][INFO] - Training Epoch: 9/10, step 428/574 completed (loss: 0.02958701364696026, acc: 0.9629629850387573)
[2025-01-06 01:50:29,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:29,324][root][INFO] - Training Epoch: 9/10, step 429/574 completed (loss: 0.05428589880466461, acc: 0.95652174949646)
[2025-01-06 01:50:29,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:29,644][root][INFO] - Training Epoch: 9/10, step 430/574 completed (loss: 0.002190006896853447, acc: 1.0)
[2025-01-06 01:50:29,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:29,962][root][INFO] - Training Epoch: 9/10, step 431/574 completed (loss: 0.0010860210750252008, acc: 1.0)
[2025-01-06 01:50:30,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:30,297][root][INFO] - Training Epoch: 9/10, step 432/574 completed (loss: 0.0030077307019382715, acc: 1.0)
[2025-01-06 01:50:30,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:30,670][root][INFO] - Training Epoch: 9/10, step 433/574 completed (loss: 0.027545427903532982, acc: 1.0)
[2025-01-06 01:50:30,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:30,994][root][INFO] - Training Epoch: 9/10, step 434/574 completed (loss: 0.0015246898401528597, acc: 1.0)
[2025-01-06 01:50:31,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:31,334][root][INFO] - Training Epoch: 9/10, step 435/574 completed (loss: 0.0037833205424249172, acc: 1.0)
[2025-01-06 01:50:31,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:31,650][root][INFO] - Training Epoch: 9/10, step 436/574 completed (loss: 0.04871132969856262, acc: 0.9722222089767456)
[2025-01-06 01:50:31,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:31,996][root][INFO] - Training Epoch: 9/10, step 437/574 completed (loss: 0.04770375415682793, acc: 0.9772727489471436)
[2025-01-06 01:50:32,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:32,331][root][INFO] - Training Epoch: 9/10, step 438/574 completed (loss: 0.0003087612276431173, acc: 1.0)
[2025-01-06 01:50:32,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:32,683][root][INFO] - Training Epoch: 9/10, step 439/574 completed (loss: 0.06352224200963974, acc: 0.9743589758872986)
[2025-01-06 01:50:32,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:33,182][root][INFO] - Training Epoch: 9/10, step 440/574 completed (loss: 0.0975685715675354, acc: 0.9696969985961914)
[2025-01-06 01:50:33,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:33,906][root][INFO] - Training Epoch: 9/10, step 441/574 completed (loss: 0.20863068103790283, acc: 0.9359999895095825)
[2025-01-06 01:50:34,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:34,329][root][INFO] - Training Epoch: 9/10, step 442/574 completed (loss: 0.11566136032342911, acc: 0.9274193644523621)
[2025-01-06 01:50:34,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:34,998][root][INFO] - Training Epoch: 9/10, step 443/574 completed (loss: 0.16983933746814728, acc: 0.9402984976768494)
[2025-01-06 01:50:35,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:35,354][root][INFO] - Training Epoch: 9/10, step 444/574 completed (loss: 0.031821850687265396, acc: 0.9811320900917053)
[2025-01-06 01:50:35,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:35,782][root][INFO] - Training Epoch: 9/10, step 445/574 completed (loss: 0.011796051636338234, acc: 1.0)
[2025-01-06 01:50:35,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:36,114][root][INFO] - Training Epoch: 9/10, step 446/574 completed (loss: 0.003954008687287569, acc: 1.0)
[2025-01-06 01:50:36,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:36,431][root][INFO] - Training Epoch: 9/10, step 447/574 completed (loss: 0.0071325963363051414, acc: 1.0)
[2025-01-06 01:50:36,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:36,712][root][INFO] - Training Epoch: 9/10, step 448/574 completed (loss: 0.0021663832012563944, acc: 1.0)
[2025-01-06 01:50:36,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:37,120][root][INFO] - Training Epoch: 9/10, step 449/574 completed (loss: 0.04763694480061531, acc: 0.9850746393203735)
[2025-01-06 01:50:37,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:37,495][root][INFO] - Training Epoch: 9/10, step 450/574 completed (loss: 0.021924959495663643, acc: 0.9861111044883728)
[2025-01-06 01:50:37,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:37,866][root][INFO] - Training Epoch: 9/10, step 451/574 completed (loss: 0.026148172095417976, acc: 0.989130437374115)
[2025-01-06 01:50:37,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:38,256][root][INFO] - Training Epoch: 9/10, step 452/574 completed (loss: 0.023901253938674927, acc: 0.9871794581413269)
[2025-01-06 01:50:38,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:38,603][root][INFO] - Training Epoch: 9/10, step 453/574 completed (loss: 0.0641958937048912, acc: 0.9736841917037964)
[2025-01-06 01:50:38,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:38,961][root][INFO] - Training Epoch: 9/10, step 454/574 completed (loss: 0.7900601029396057, acc: 0.9387755393981934)
[2025-01-06 01:50:39,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:39,277][root][INFO] - Training Epoch: 9/10, step 455/574 completed (loss: 0.0065656546503305435, acc: 1.0)
[2025-01-06 01:50:39,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:39,632][root][INFO] - Training Epoch: 9/10, step 456/574 completed (loss: 0.0834830105304718, acc: 0.9793814420700073)
[2025-01-06 01:50:39,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:39,982][root][INFO] - Training Epoch: 9/10, step 457/574 completed (loss: 0.005965917371213436, acc: 1.0)
[2025-01-06 01:50:40,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:40,364][root][INFO] - Training Epoch: 9/10, step 458/574 completed (loss: 0.07007963955402374, acc: 0.9767441749572754)
[2025-01-06 01:50:40,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:40,698][root][INFO] - Training Epoch: 9/10, step 459/574 completed (loss: 0.008266710676252842, acc: 1.0)
[2025-01-06 01:50:40,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:41,022][root][INFO] - Training Epoch: 9/10, step 460/574 completed (loss: 0.02637396939098835, acc: 1.0)
[2025-01-06 01:50:41,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:41,375][root][INFO] - Training Epoch: 9/10, step 461/574 completed (loss: 0.10779157280921936, acc: 0.9722222089767456)
[2025-01-06 01:50:41,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:41,768][root][INFO] - Training Epoch: 9/10, step 462/574 completed (loss: 0.021210718899965286, acc: 1.0)
[2025-01-06 01:50:41,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:42,148][root][INFO] - Training Epoch: 9/10, step 463/574 completed (loss: 0.008815483190119267, acc: 1.0)
[2025-01-06 01:50:42,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:42,488][root][INFO] - Training Epoch: 9/10, step 464/574 completed (loss: 0.036324676126241684, acc: 0.97826087474823)
[2025-01-06 01:50:42,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:42,853][root][INFO] - Training Epoch: 9/10, step 465/574 completed (loss: 0.14960525929927826, acc: 0.9523809552192688)
[2025-01-06 01:50:42,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:43,224][root][INFO] - Training Epoch: 9/10, step 466/574 completed (loss: 0.09715685993432999, acc: 0.9759036302566528)
[2025-01-06 01:50:43,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:43,621][root][INFO] - Training Epoch: 9/10, step 467/574 completed (loss: 0.02505825087428093, acc: 0.9909909963607788)
[2025-01-06 01:50:43,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:44,001][root][INFO] - Training Epoch: 9/10, step 468/574 completed (loss: 0.06136972829699516, acc: 0.9805825352668762)
[2025-01-06 01:50:44,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:44,379][root][INFO] - Training Epoch: 9/10, step 469/574 completed (loss: 0.13473092019557953, acc: 0.9593495726585388)
[2025-01-06 01:50:44,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:44,719][root][INFO] - Training Epoch: 9/10, step 470/574 completed (loss: 0.007322714198380709, acc: 1.0)
[2025-01-06 01:50:44,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:45,059][root][INFO] - Training Epoch: 9/10, step 471/574 completed (loss: 0.10049986094236374, acc: 0.9642857313156128)
[2025-01-06 01:50:45,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:45,500][root][INFO] - Training Epoch: 9/10, step 472/574 completed (loss: 0.3353343904018402, acc: 0.9117646813392639)
[2025-01-06 01:50:45,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:45,901][root][INFO] - Training Epoch: 9/10, step 473/574 completed (loss: 0.2623966634273529, acc: 0.9082969427108765)
[2025-01-06 01:50:46,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:46,329][root][INFO] - Training Epoch: 9/10, step 474/574 completed (loss: 0.055600568652153015, acc: 0.9895833134651184)
[2025-01-06 01:50:46,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:46,742][root][INFO] - Training Epoch: 9/10, step 475/574 completed (loss: 0.0752723291516304, acc: 0.9754601120948792)
[2025-01-06 01:50:46,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:47,101][root][INFO] - Training Epoch: 9/10, step 476/574 completed (loss: 0.05152662470936775, acc: 0.971222996711731)
[2025-01-06 01:50:47,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:47,462][root][INFO] - Training Epoch: 9/10, step 477/574 completed (loss: 0.20346127450466156, acc: 0.9396985173225403)
[2025-01-06 01:50:47,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:47,839][root][INFO] - Training Epoch: 9/10, step 478/574 completed (loss: 0.028422756120562553, acc: 1.0)
[2025-01-06 01:50:47,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:48,208][root][INFO] - Training Epoch: 9/10, step 479/574 completed (loss: 0.08974802494049072, acc: 0.9696969985961914)
[2025-01-06 01:50:48,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:48,563][root][INFO] - Training Epoch: 9/10, step 480/574 completed (loss: 0.007896014489233494, acc: 1.0)
[2025-01-06 01:50:48,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:48,918][root][INFO] - Training Epoch: 9/10, step 481/574 completed (loss: 0.07775597274303436, acc: 0.949999988079071)
[2025-01-06 01:50:49,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:49,272][root][INFO] - Training Epoch: 9/10, step 482/574 completed (loss: 0.1270350068807602, acc: 0.949999988079071)
[2025-01-06 01:50:49,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:49,697][root][INFO] - Training Epoch: 9/10, step 483/574 completed (loss: 0.44434377551078796, acc: 0.8965517282485962)
[2025-01-06 01:50:49,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:50,043][root][INFO] - Training Epoch: 9/10, step 484/574 completed (loss: 0.006906169932335615, acc: 1.0)
[2025-01-06 01:50:50,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:50,380][root][INFO] - Training Epoch: 9/10, step 485/574 completed (loss: 0.0033248686231672764, acc: 1.0)
[2025-01-06 01:50:50,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:50,715][root][INFO] - Training Epoch: 9/10, step 486/574 completed (loss: 0.08616122603416443, acc: 0.9629629850387573)
[2025-01-06 01:50:50,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:51,058][root][INFO] - Training Epoch: 9/10, step 487/574 completed (loss: 0.051786672323942184, acc: 1.0)
[2025-01-06 01:50:51,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:51,442][root][INFO] - Training Epoch: 9/10, step 488/574 completed (loss: 0.027958063408732414, acc: 1.0)
[2025-01-06 01:50:51,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:51,804][root][INFO] - Training Epoch: 9/10, step 489/574 completed (loss: 0.18380078673362732, acc: 0.9230769276618958)
[2025-01-06 01:50:51,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:52,131][root][INFO] - Training Epoch: 9/10, step 490/574 completed (loss: 0.010791878215968609, acc: 1.0)
[2025-01-06 01:50:52,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:52,464][root][INFO] - Training Epoch: 9/10, step 491/574 completed (loss: 0.19138798117637634, acc: 0.931034505367279)
[2025-01-06 01:50:52,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:52,794][root][INFO] - Training Epoch: 9/10, step 492/574 completed (loss: 0.052229490131139755, acc: 0.9803921580314636)
[2025-01-06 01:50:52,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:53,116][root][INFO] - Training Epoch: 9/10, step 493/574 completed (loss: 0.04936307296156883, acc: 0.9655172228813171)
[2025-01-06 01:50:53,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:53,449][root][INFO] - Training Epoch: 9/10, step 494/574 completed (loss: 0.04939533397555351, acc: 1.0)
[2025-01-06 01:50:53,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:53,774][root][INFO] - Training Epoch: 9/10, step 495/574 completed (loss: 0.0024947163183242083, acc: 1.0)
[2025-01-06 01:50:53,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:54,163][root][INFO] - Training Epoch: 9/10, step 496/574 completed (loss: 0.1629745215177536, acc: 0.9375)
[2025-01-06 01:50:54,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:54,580][root][INFO] - Training Epoch: 9/10, step 497/574 completed (loss: 0.09570563584566116, acc: 0.9550561904907227)
[2025-01-06 01:50:54,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:54,928][root][INFO] - Training Epoch: 9/10, step 498/574 completed (loss: 0.20530788600444794, acc: 0.9101123809814453)
[2025-01-06 01:50:55,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:55,280][root][INFO] - Training Epoch: 9/10, step 499/574 completed (loss: 0.23510222136974335, acc: 0.9219858050346375)
[2025-01-06 01:50:55,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:55,637][root][INFO] - Training Epoch: 9/10, step 500/574 completed (loss: 0.16433799266815186, acc: 0.9347826242446899)
[2025-01-06 01:50:55,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:55,964][root][INFO] - Training Epoch: 9/10, step 501/574 completed (loss: 0.0022139602806419134, acc: 1.0)
[2025-01-06 01:50:56,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:56,334][root][INFO] - Training Epoch: 9/10, step 502/574 completed (loss: 0.0006281250389292836, acc: 1.0)
[2025-01-06 01:50:56,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:56,657][root][INFO] - Training Epoch: 9/10, step 503/574 completed (loss: 0.0021063191816210747, acc: 1.0)
[2025-01-06 01:50:56,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:56,963][root][INFO] - Training Epoch: 9/10, step 504/574 completed (loss: 0.015586423687636852, acc: 1.0)
[2025-01-06 01:50:57,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:57,263][root][INFO] - Training Epoch: 9/10, step 505/574 completed (loss: 0.12184742093086243, acc: 0.9433962106704712)
[2025-01-06 01:50:57,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:57,575][root][INFO] - Training Epoch: 9/10, step 506/574 completed (loss: 0.01988268457353115, acc: 1.0)
[2025-01-06 01:50:57,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:58,190][root][INFO] - Training Epoch: 9/10, step 507/574 completed (loss: 0.3294852375984192, acc: 0.8918918967247009)
[2025-01-06 01:50:58,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:58,649][root][INFO] - Training Epoch: 9/10, step 508/574 completed (loss: 0.29141414165496826, acc: 0.8873239159584045)
[2025-01-06 01:50:58,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:58,974][root][INFO] - Training Epoch: 9/10, step 509/574 completed (loss: 0.2239793837070465, acc: 0.8999999761581421)
[2025-01-06 01:50:59,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:59,346][root][INFO] - Training Epoch: 9/10, step 510/574 completed (loss: 0.05881012603640556, acc: 0.9666666388511658)
[2025-01-06 01:50:59,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:59,680][root][INFO] - Training Epoch: 9/10, step 511/574 completed (loss: 0.0035750193055719137, acc: 1.0)
[2025-01-06 01:51:00,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:02,133][root][INFO] - Training Epoch: 9/10, step 512/574 completed (loss: 0.3144038915634155, acc: 0.9071428775787354)
[2025-01-06 01:51:02,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:02,940][root][INFO] - Training Epoch: 9/10, step 513/574 completed (loss: 0.036923084408044815, acc: 0.9920634627342224)
[2025-01-06 01:51:03,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:03,256][root][INFO] - Training Epoch: 9/10, step 514/574 completed (loss: 0.05551266297698021, acc: 0.9642857313156128)
[2025-01-06 01:51:03,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:03,577][root][INFO] - Training Epoch: 9/10, step 515/574 completed (loss: 0.006471381522715092, acc: 1.0)
[2025-01-06 01:51:03,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:04,270][root][INFO] - Training Epoch: 9/10, step 516/574 completed (loss: 0.04279400408267975, acc: 0.9861111044883728)
[2025-01-06 01:51:04,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:04,638][root][INFO] - Training Epoch: 9/10, step 517/574 completed (loss: 0.0007216455996967852, acc: 1.0)
[2025-01-06 01:51:04,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:04,973][root][INFO] - Training Epoch: 9/10, step 518/574 completed (loss: 0.29992184042930603, acc: 0.9677419066429138)
[2025-01-06 01:51:05,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:05,303][root][INFO] - Training Epoch: 9/10, step 519/574 completed (loss: 0.04758646711707115, acc: 1.0)
[2025-01-06 01:51:05,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:05,617][root][INFO] - Training Epoch: 9/10, step 520/574 completed (loss: 0.15520793199539185, acc: 0.9629629850387573)
[2025-01-06 01:51:05,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:06,697][root][INFO] - Training Epoch: 9/10, step 521/574 completed (loss: 0.22454401850700378, acc: 0.9194915294647217)
[2025-01-06 01:51:06,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:07,063][root][INFO] - Training Epoch: 9/10, step 522/574 completed (loss: 0.05352320894598961, acc: 0.9776119589805603)
[2025-01-06 01:51:07,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:07,439][root][INFO] - Training Epoch: 9/10, step 523/574 completed (loss: 0.09243744611740112, acc: 0.9489051103591919)
[2025-01-06 01:51:07,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:08,017][root][INFO] - Training Epoch: 9/10, step 524/574 completed (loss: 0.24407584965229034, acc: 0.925000011920929)
[2025-01-06 01:51:08,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:08,343][root][INFO] - Training Epoch: 9/10, step 525/574 completed (loss: 0.006838236469775438, acc: 1.0)
[2025-01-06 01:51:08,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:08,721][root][INFO] - Training Epoch: 9/10, step 526/574 completed (loss: 0.0743292048573494, acc: 0.9807692170143127)
[2025-01-06 01:51:08,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:09,000][root][INFO] - Training Epoch: 9/10, step 527/574 completed (loss: 0.038201406598091125, acc: 1.0)
[2025-01-06 01:51:09,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:09,337][root][INFO] - Training Epoch: 9/10, step 528/574 completed (loss: 0.28325462341308594, acc: 0.8852459192276001)
[2025-01-06 01:51:09,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:09,689][root][INFO] - Training Epoch: 9/10, step 529/574 completed (loss: 0.08494538813829422, acc: 0.9661017060279846)
[2025-01-06 01:51:09,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:10,078][root][INFO] - Training Epoch: 9/10, step 530/574 completed (loss: 0.1391192525625229, acc: 0.9767441749572754)
[2025-01-06 01:51:10,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:10,420][root][INFO] - Training Epoch: 9/10, step 531/574 completed (loss: 0.03929637372493744, acc: 0.9772727489471436)
[2025-01-06 01:51:10,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:10,755][root][INFO] - Training Epoch: 9/10, step 532/574 completed (loss: 0.16368624567985535, acc: 0.9433962106704712)
[2025-01-06 01:51:10,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:11,062][root][INFO] - Training Epoch: 9/10, step 533/574 completed (loss: 0.0957125723361969, acc: 0.9545454382896423)
[2025-01-06 01:51:11,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:11,377][root][INFO] - Training Epoch: 9/10, step 534/574 completed (loss: 0.11928479373455048, acc: 0.9599999785423279)
[2025-01-06 01:51:11,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:11,691][root][INFO] - Training Epoch: 9/10, step 535/574 completed (loss: 0.00963643379509449, acc: 1.0)
[2025-01-06 01:51:11,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:11,916][root][INFO] - Training Epoch: 9/10, step 536/574 completed (loss: 0.10987844318151474, acc: 0.9545454382896423)
[2025-01-06 01:51:12,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:12,315][root][INFO] - Training Epoch: 9/10, step 537/574 completed (loss: 0.034607212990522385, acc: 1.0)
[2025-01-06 01:51:12,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:12,683][root][INFO] - Training Epoch: 9/10, step 538/574 completed (loss: 0.12044724076986313, acc: 0.953125)
[2025-01-06 01:51:12,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:13,076][root][INFO] - Training Epoch: 9/10, step 539/574 completed (loss: 0.29247188568115234, acc: 0.96875)
[2025-01-06 01:51:13,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:13,405][root][INFO] - Training Epoch: 9/10, step 540/574 completed (loss: 0.012203403748571873, acc: 1.0)
[2025-01-06 01:51:13,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:13,756][root][INFO] - Training Epoch: 9/10, step 541/574 completed (loss: 0.01114597823470831, acc: 1.0)
[2025-01-06 01:51:13,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:14,100][root][INFO] - Training Epoch: 9/10, step 542/574 completed (loss: 0.0011466349242255092, acc: 1.0)
[2025-01-06 01:51:14,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:14,449][root][INFO] - Training Epoch: 9/10, step 543/574 completed (loss: 0.00201849895529449, acc: 1.0)
[2025-01-06 01:51:14,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:14,784][root][INFO] - Training Epoch: 9/10, step 544/574 completed (loss: 0.006485192105174065, acc: 1.0)
[2025-01-06 01:51:14,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15,139][root][INFO] - Training Epoch: 9/10, step 545/574 completed (loss: 0.014250234700739384, acc: 1.0)
[2025-01-06 01:51:15,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15,487][root][INFO] - Training Epoch: 9/10, step 546/574 completed (loss: 0.002946473192423582, acc: 1.0)
[2025-01-06 01:51:15,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15,851][root][INFO] - Training Epoch: 9/10, step 547/574 completed (loss: 0.06054137274622917, acc: 0.9736841917037964)
[2025-01-06 01:51:15,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:16,193][root][INFO] - Training Epoch: 9/10, step 548/574 completed (loss: 0.0013463551877066493, acc: 1.0)
[2025-01-06 01:51:16,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:16,550][root][INFO] - Training Epoch: 9/10, step 549/574 completed (loss: 0.02395135723054409, acc: 1.0)
[2025-01-06 01:51:16,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:16,940][root][INFO] - Training Epoch: 9/10, step 550/574 completed (loss: 0.0016810602974146605, acc: 1.0)
[2025-01-06 01:51:17,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:17,274][root][INFO] - Training Epoch: 9/10, step 551/574 completed (loss: 0.1773119866847992, acc: 0.9750000238418579)
[2025-01-06 01:51:17,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:17,620][root][INFO] - Training Epoch: 9/10, step 552/574 completed (loss: 0.010686351917684078, acc: 1.0)
[2025-01-06 01:51:17,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:17,930][root][INFO] - Training Epoch: 9/10, step 553/574 completed (loss: 0.04717741906642914, acc: 0.985401451587677)
[2025-01-06 01:51:18,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:18,201][root][INFO] - Training Epoch: 9/10, step 554/574 completed (loss: 0.027950406074523926, acc: 0.9931034445762634)
[2025-01-06 01:51:18,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:18,521][root][INFO] - Training Epoch: 9/10, step 555/574 completed (loss: 0.18519286811351776, acc: 0.9571428298950195)
[2025-01-06 01:51:19,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:19,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:19,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:21,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:21,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:21,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:22,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:22,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:23,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:23,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:23,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:24,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:24,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:24,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:25,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:25,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:25,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:26,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:26,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:26,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:27,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:27,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:27,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:28,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:28,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:28,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:29,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:29,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:29,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:30,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:30,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:30,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:31,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:31,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:31,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:33,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:33,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:33,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:34,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:34,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:34,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:34,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:36,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:36,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:36,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:37,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:37,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:37,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:37,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:38,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:38,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:38,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:39,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:39,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:40,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:40,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:40,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:41,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:41,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:42,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:42,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:42,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:43,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:43,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:43,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:44,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:44,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:44,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:45,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:45,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:45,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:46,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:46,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:46,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:47,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:47,839][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5681, device='cuda:0') eval_epoch_loss=tensor(0.9432, device='cuda:0') eval_epoch_acc=tensor(0.8211, device='cuda:0')
[2025-01-06 01:51:47,841][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:51:47,841][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:51:48,342][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_556_loss_0.9431648850440979/model.pt
[2025-01-06 01:51:48,356][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:51:48,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:48,736][root][INFO] - Training Epoch: 9/10, step 556/574 completed (loss: 0.14410777390003204, acc: 0.9735099077224731)
[2025-01-06 01:51:48,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:49,101][root][INFO] - Training Epoch: 9/10, step 557/574 completed (loss: 0.02405565045773983, acc: 0.9914529919624329)
[2025-01-06 01:51:49,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:49,435][root][INFO] - Training Epoch: 9/10, step 558/574 completed (loss: 0.0008561814320273697, acc: 1.0)
[2025-01-06 01:51:49,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:49,810][root][INFO] - Training Epoch: 9/10, step 559/574 completed (loss: 0.013955062255263329, acc: 1.0)
[2025-01-06 01:51:49,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:50,134][root][INFO] - Training Epoch: 9/10, step 560/574 completed (loss: 0.0019524513045325875, acc: 1.0)
[2025-01-06 01:51:50,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:50,458][root][INFO] - Training Epoch: 9/10, step 561/574 completed (loss: 0.02129381150007248, acc: 1.0)
[2025-01-06 01:51:50,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:50,796][root][INFO] - Training Epoch: 9/10, step 562/574 completed (loss: 0.13203229010105133, acc: 0.9777777791023254)
[2025-01-06 01:51:50,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:51,146][root][INFO] - Training Epoch: 9/10, step 563/574 completed (loss: 0.07974859327077866, acc: 0.9740259647369385)
[2025-01-06 01:51:51,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:51,476][root][INFO] - Training Epoch: 9/10, step 564/574 completed (loss: 0.025776132941246033, acc: 1.0)
[2025-01-06 01:51:51,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:51,771][root][INFO] - Training Epoch: 9/10, step 565/574 completed (loss: 0.06329319626092911, acc: 0.982758641242981)
[2025-01-06 01:51:51,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:52,073][root][INFO] - Training Epoch: 9/10, step 566/574 completed (loss: 0.11691033095121384, acc: 0.9404761791229248)
[2025-01-06 01:51:52,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:52,366][root][INFO] - Training Epoch: 9/10, step 567/574 completed (loss: 0.0022535643074661493, acc: 1.0)
[2025-01-06 01:51:52,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:52,672][root][INFO] - Training Epoch: 9/10, step 568/574 completed (loss: 0.05184226483106613, acc: 0.9629629850387573)
[2025-01-06 01:51:52,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:53,059][root][INFO] - Training Epoch: 9/10, step 569/574 completed (loss: 0.09701031446456909, acc: 0.9679144620895386)
[2025-01-06 01:51:53,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:53,468][root][INFO] - Training Epoch: 9/10, step 570/574 completed (loss: 0.014742698520421982, acc: 0.9838709831237793)
[2025-01-06 01:51:53,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:53,833][root][INFO] - Training Epoch: 9/10, step 571/574 completed (loss: 0.017617426812648773, acc: 0.9914529919624329)
[2025-01-06 01:51:53,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:54,172][root][INFO] - Training Epoch: 9/10, step 572/574 completed (loss: 0.08994581550359726, acc: 0.9744898080825806)
[2025-01-06 01:51:54,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:54,525][root][INFO] - Training Epoch: 9/10, step 573/574 completed (loss: 0.055712878704071045, acc: 0.9811320900917053)
[2025-01-06 01:51:55,145][slam_llm.utils.train_utils][INFO] - Epoch 9: train_perplexity=1.1013, train_epoch_loss=0.0965, epoch time 340.00066924095154s
[2025-01-06 01:51:55,146][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:51:55,146][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2025-01-06 01:51:55,146][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:51:55,146][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 25
[2025-01-06 01:51:55,146][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:51:55,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:56,256][root][INFO] - Training Epoch: 10/10, step 0/574 completed (loss: 0.003817417658865452, acc: 1.0)
[2025-01-06 01:51:56,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:56,565][root][INFO] - Training Epoch: 10/10, step 1/574 completed (loss: 0.030667442828416824, acc: 1.0)
[2025-01-06 01:51:56,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:56,888][root][INFO] - Training Epoch: 10/10, step 2/574 completed (loss: 0.0446324460208416, acc: 0.9729729890823364)
[2025-01-06 01:51:56,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:57,229][root][INFO] - Training Epoch: 10/10, step 3/574 completed (loss: 0.006421281024813652, acc: 1.0)
[2025-01-06 01:51:57,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:57,521][root][INFO] - Training Epoch: 10/10, step 4/574 completed (loss: 0.032710276544094086, acc: 1.0)
[2025-01-06 01:51:57,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:57,856][root][INFO] - Training Epoch: 10/10, step 5/574 completed (loss: 0.005741732660681009, acc: 1.0)
[2025-01-06 01:51:57,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:58,251][root][INFO] - Training Epoch: 10/10, step 6/574 completed (loss: 0.014209314249455929, acc: 1.0)
[2025-01-06 01:51:58,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:58,593][root][INFO] - Training Epoch: 10/10, step 7/574 completed (loss: 0.004341354593634605, acc: 1.0)
[2025-01-06 01:51:58,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:58,923][root][INFO] - Training Epoch: 10/10, step 8/574 completed (loss: 0.005412768106907606, acc: 1.0)
[2025-01-06 01:51:58,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:59,229][root][INFO] - Training Epoch: 10/10, step 9/574 completed (loss: 0.00374178821220994, acc: 1.0)
[2025-01-06 01:51:59,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:59,558][root][INFO] - Training Epoch: 10/10, step 10/574 completed (loss: 0.007820110768079758, acc: 1.0)
[2025-01-06 01:51:59,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:59,881][root][INFO] - Training Epoch: 10/10, step 11/574 completed (loss: 0.04409017786383629, acc: 0.9743589758872986)
[2025-01-06 01:51:59,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:00,229][root][INFO] - Training Epoch: 10/10, step 12/574 completed (loss: 0.01653021015226841, acc: 1.0)
[2025-01-06 01:52:00,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:00,629][root][INFO] - Training Epoch: 10/10, step 13/574 completed (loss: 0.04300805181264877, acc: 0.95652174949646)
[2025-01-06 01:52:00,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:00,990][root][INFO] - Training Epoch: 10/10, step 14/574 completed (loss: 0.004821576178073883, acc: 1.0)
[2025-01-06 01:52:01,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:01,342][root][INFO] - Training Epoch: 10/10, step 15/574 completed (loss: 0.10419268161058426, acc: 0.9795918464660645)
[2025-01-06 01:52:01,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:01,692][root][INFO] - Training Epoch: 10/10, step 16/574 completed (loss: 0.016155367717146873, acc: 1.0)
[2025-01-06 01:52:01,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02,025][root][INFO] - Training Epoch: 10/10, step 17/574 completed (loss: 0.005869880318641663, acc: 1.0)
[2025-01-06 01:52:02,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02,361][root][INFO] - Training Epoch: 10/10, step 18/574 completed (loss: 0.04990755394101143, acc: 0.9722222089767456)
[2025-01-06 01:52:02,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02,679][root][INFO] - Training Epoch: 10/10, step 19/574 completed (loss: 0.0216143187135458, acc: 1.0)
[2025-01-06 01:52:02,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02,989][root][INFO] - Training Epoch: 10/10, step 20/574 completed (loss: 0.004785494413226843, acc: 1.0)
[2025-01-06 01:52:03,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:03,424][root][INFO] - Training Epoch: 10/10, step 21/574 completed (loss: 0.010196588933467865, acc: 1.0)
[2025-01-06 01:52:03,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:03,809][root][INFO] - Training Epoch: 10/10, step 22/574 completed (loss: 0.01785830222070217, acc: 1.0)
[2025-01-06 01:52:03,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:04,161][root][INFO] - Training Epoch: 10/10, step 23/574 completed (loss: 0.04640844464302063, acc: 0.9523809552192688)
[2025-01-06 01:52:04,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:04,491][root][INFO] - Training Epoch: 10/10, step 24/574 completed (loss: 0.07731453329324722, acc: 0.9375)
[2025-01-06 01:52:04,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:04,857][root][INFO] - Training Epoch: 10/10, step 25/574 completed (loss: 0.3574341833591461, acc: 0.9433962106704712)
[2025-01-06 01:52:04,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:05,227][root][INFO] - Training Epoch: 10/10, step 26/574 completed (loss: 0.03253665566444397, acc: 1.0)
[2025-01-06 01:52:05,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:06,510][root][INFO] - Training Epoch: 10/10, step 27/574 completed (loss: 0.3113143742084503, acc: 0.9090909361839294)
[2025-01-06 01:52:06,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:06,877][root][INFO] - Training Epoch: 10/10, step 28/574 completed (loss: 0.004146228544414043, acc: 1.0)
[2025-01-06 01:52:06,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:07,239][root][INFO] - Training Epoch: 10/10, step 29/574 completed (loss: 0.10839460790157318, acc: 0.9518072009086609)
[2025-01-06 01:52:07,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:07,551][root][INFO] - Training Epoch: 10/10, step 30/574 completed (loss: 0.036989737302064896, acc: 1.0)
[2025-01-06 01:52:07,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:07,860][root][INFO] - Training Epoch: 10/10, step 31/574 completed (loss: 0.001309012295678258, acc: 1.0)
[2025-01-06 01:52:07,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:08,195][root][INFO] - Training Epoch: 10/10, step 32/574 completed (loss: 0.014692266471683979, acc: 1.0)
[2025-01-06 01:52:08,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:08,528][root][INFO] - Training Epoch: 10/10, step 33/574 completed (loss: 0.0024062504526227713, acc: 1.0)
[2025-01-06 01:52:08,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:08,870][root][INFO] - Training Epoch: 10/10, step 34/574 completed (loss: 0.07388797402381897, acc: 0.9747899174690247)
[2025-01-06 01:52:08,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:09,211][root][INFO] - Training Epoch: 10/10, step 35/574 completed (loss: 0.028985487297177315, acc: 0.9836065769195557)
[2025-01-06 01:52:09,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:09,548][root][INFO] - Training Epoch: 10/10, step 36/574 completed (loss: 0.0648711547255516, acc: 0.9523809552192688)
[2025-01-06 01:52:09,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:09,879][root][INFO] - Training Epoch: 10/10, step 37/574 completed (loss: 0.05151297152042389, acc: 0.9830508232116699)
[2025-01-06 01:52:09,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:10,238][root][INFO] - Training Epoch: 10/10, step 38/574 completed (loss: 0.10531816631555557, acc: 0.954023003578186)
[2025-01-06 01:52:10,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:10,553][root][INFO] - Training Epoch: 10/10, step 39/574 completed (loss: 0.04063151404261589, acc: 1.0)
[2025-01-06 01:52:10,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:10,831][root][INFO] - Training Epoch: 10/10, step 40/574 completed (loss: 0.06320558488368988, acc: 0.9615384340286255)
[2025-01-06 01:52:10,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:11,229][root][INFO] - Training Epoch: 10/10, step 41/574 completed (loss: 0.2325502336025238, acc: 0.9594594836235046)
[2025-01-06 01:52:11,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:11,579][root][INFO] - Training Epoch: 10/10, step 42/574 completed (loss: 0.06559684872627258, acc: 0.9692307710647583)
[2025-01-06 01:52:11,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:12,041][root][INFO] - Training Epoch: 10/10, step 43/574 completed (loss: 0.03452355042099953, acc: 1.0)
[2025-01-06 01:52:12,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:12,489][root][INFO] - Training Epoch: 10/10, step 44/574 completed (loss: 0.0429309606552124, acc: 0.9896907210350037)
[2025-01-06 01:52:12,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:12,902][root][INFO] - Training Epoch: 10/10, step 45/574 completed (loss: 0.10541146248579025, acc: 0.9632353186607361)
[2025-01-06 01:52:12,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:13,240][root][INFO] - Training Epoch: 10/10, step 46/574 completed (loss: 0.06261304020881653, acc: 1.0)
[2025-01-06 01:52:13,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:13,571][root][INFO] - Training Epoch: 10/10, step 47/574 completed (loss: 0.0012266597477719188, acc: 1.0)
[2025-01-06 01:52:13,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:13,915][root][INFO] - Training Epoch: 10/10, step 48/574 completed (loss: 0.0006920580635778606, acc: 1.0)
[2025-01-06 01:52:14,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:14,234][root][INFO] - Training Epoch: 10/10, step 49/574 completed (loss: 0.02172750048339367, acc: 1.0)
[2025-01-06 01:52:14,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:14,580][root][INFO] - Training Epoch: 10/10, step 50/574 completed (loss: 0.07623951882123947, acc: 0.9649122953414917)
[2025-01-06 01:52:14,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:14,962][root][INFO] - Training Epoch: 10/10, step 51/574 completed (loss: 0.020186929032206535, acc: 1.0)
[2025-01-06 01:52:15,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:15,259][root][INFO] - Training Epoch: 10/10, step 52/574 completed (loss: 0.20756591856479645, acc: 0.9436619877815247)
[2025-01-06 01:52:15,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:15,709][root][INFO] - Training Epoch: 10/10, step 53/574 completed (loss: 0.35100531578063965, acc: 0.8799999952316284)
[2025-01-06 01:52:15,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:16,016][root][INFO] - Training Epoch: 10/10, step 54/574 completed (loss: 0.06509733945131302, acc: 0.9729729890823364)
[2025-01-06 01:52:16,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:16,322][root][INFO] - Training Epoch: 10/10, step 55/574 completed (loss: 0.00041347125079482794, acc: 1.0)
[2025-01-06 01:52:17,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:19,417][root][INFO] - Training Epoch: 10/10, step 56/574 completed (loss: 0.4284701943397522, acc: 0.8634812235832214)
[2025-01-06 01:52:19,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:20,589][root][INFO] - Training Epoch: 10/10, step 57/574 completed (loss: 0.7049659490585327, acc: 0.7995642423629761)
[2025-01-06 01:52:20,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:21,211][root][INFO] - Training Epoch: 10/10, step 58/574 completed (loss: 0.26039519906044006, acc: 0.9090909361839294)
[2025-01-06 01:52:21,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:21,778][root][INFO] - Training Epoch: 10/10, step 59/574 completed (loss: 0.07133016735315323, acc: 0.970588207244873)
[2025-01-06 01:52:21,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:22,348][root][INFO] - Training Epoch: 10/10, step 60/574 completed (loss: 0.25783008337020874, acc: 0.9202898740768433)
[2025-01-06 01:52:22,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:22,787][root][INFO] - Training Epoch: 10/10, step 61/574 completed (loss: 0.10825717449188232, acc: 0.9750000238418579)
[2025-01-06 01:52:22,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:23,123][root][INFO] - Training Epoch: 10/10, step 62/574 completed (loss: 0.030448345467448235, acc: 1.0)
[2025-01-06 01:52:23,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:23,454][root][INFO] - Training Epoch: 10/10, step 63/574 completed (loss: 0.010226748883724213, acc: 1.0)
[2025-01-06 01:52:23,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:23,819][root][INFO] - Training Epoch: 10/10, step 64/574 completed (loss: 0.009304523468017578, acc: 1.0)
[2025-01-06 01:52:23,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:24,165][root][INFO] - Training Epoch: 10/10, step 65/574 completed (loss: 0.026504892855882645, acc: 0.9655172228813171)
[2025-01-06 01:52:24,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:24,501][root][INFO] - Training Epoch: 10/10, step 66/574 completed (loss: 0.07975289970636368, acc: 0.9821428656578064)
[2025-01-06 01:52:24,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:24,845][root][INFO] - Training Epoch: 10/10, step 67/574 completed (loss: 0.052774712443351746, acc: 0.9833333492279053)
[2025-01-06 01:52:24,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:25,627][root][INFO] - Training Epoch: 10/10, step 68/574 completed (loss: 0.0017782412469387054, acc: 1.0)
[2025-01-06 01:52:25,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:25,972][root][INFO] - Training Epoch: 10/10, step 69/574 completed (loss: 0.6090773344039917, acc: 0.8888888955116272)
[2025-01-06 01:52:26,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:26,306][root][INFO] - Training Epoch: 10/10, step 70/574 completed (loss: 0.017206665128469467, acc: 1.0)
[2025-01-06 01:52:26,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:26,655][root][INFO] - Training Epoch: 10/10, step 71/574 completed (loss: 0.16334308683872223, acc: 0.9485294222831726)
[2025-01-06 01:52:26,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:26,982][root][INFO] - Training Epoch: 10/10, step 72/574 completed (loss: 0.2119714617729187, acc: 0.920634925365448)
[2025-01-06 01:52:27,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:27,306][root][INFO] - Training Epoch: 10/10, step 73/574 completed (loss: 0.4169222414493561, acc: 0.8564102649688721)
[2025-01-06 01:52:27,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:27,580][root][INFO] - Training Epoch: 10/10, step 74/574 completed (loss: 0.1573115736246109, acc: 0.9285714030265808)
[2025-01-06 01:52:27,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:27,893][root][INFO] - Training Epoch: 10/10, step 75/574 completed (loss: 0.2253594547510147, acc: 0.9179104566574097)
[2025-01-06 01:52:28,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:28,277][root][INFO] - Training Epoch: 10/10, step 76/574 completed (loss: 0.5332384705543518, acc: 0.8284671306610107)
[2025-01-06 01:52:28,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:28,582][root][INFO] - Training Epoch: 10/10, step 77/574 completed (loss: 0.000696299655828625, acc: 1.0)
[2025-01-06 01:52:28,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:28,882][root][INFO] - Training Epoch: 10/10, step 78/574 completed (loss: 0.004382019396871328, acc: 1.0)
[2025-01-06 01:52:28,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:29,248][root][INFO] - Training Epoch: 10/10, step 79/574 completed (loss: 0.06517574191093445, acc: 0.9696969985961914)
[2025-01-06 01:52:29,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:29,598][root][INFO] - Training Epoch: 10/10, step 80/574 completed (loss: 0.0026802148204296827, acc: 1.0)
[2025-01-06 01:52:29,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:29,941][root][INFO] - Training Epoch: 10/10, step 81/574 completed (loss: 0.020856481045484543, acc: 1.0)
[2025-01-06 01:52:30,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:30,279][root][INFO] - Training Epoch: 10/10, step 82/574 completed (loss: 0.02115444280207157, acc: 1.0)
[2025-01-06 01:52:30,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:30,618][root][INFO] - Training Epoch: 10/10, step 83/574 completed (loss: 0.007653599604964256, acc: 1.0)
[2025-01-06 01:52:30,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:30,965][root][INFO] - Training Epoch: 10/10, step 84/574 completed (loss: 0.04119795933365822, acc: 1.0)
[2025-01-06 01:52:31,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:31,310][root][INFO] - Training Epoch: 10/10, step 85/574 completed (loss: 0.06528039276599884, acc: 0.9800000190734863)
[2025-01-06 01:52:31,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:31,676][root][INFO] - Training Epoch: 10/10, step 86/574 completed (loss: 0.10681913048028946, acc: 0.95652174949646)
[2025-01-06 01:52:31,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:32,164][root][INFO] - Training Epoch: 10/10, step 87/574 completed (loss: 0.052402883768081665, acc: 1.0)
[2025-01-06 01:52:32,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:32,520][root][INFO] - Training Epoch: 10/10, step 88/574 completed (loss: 0.095291867852211, acc: 0.9611650705337524)
[2025-01-06 01:52:32,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:33,598][root][INFO] - Training Epoch: 10/10, step 89/574 completed (loss: 0.2710117995738983, acc: 0.9029126167297363)
[2025-01-06 01:52:33,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:34,417][root][INFO] - Training Epoch: 10/10, step 90/574 completed (loss: 0.4038931429386139, acc: 0.8870967626571655)
[2025-01-06 01:52:34,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:35,220][root][INFO] - Training Epoch: 10/10, step 91/574 completed (loss: 0.2385319620370865, acc: 0.9267241358757019)
[2025-01-06 01:52:35,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:35,967][root][INFO] - Training Epoch: 10/10, step 92/574 completed (loss: 0.1411653310060501, acc: 0.9578947424888611)
[2025-01-06 01:52:36,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:36,957][root][INFO] - Training Epoch: 10/10, step 93/574 completed (loss: 0.2795782685279846, acc: 0.9009901285171509)
[2025-01-06 01:52:37,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:37,333][root][INFO] - Training Epoch: 10/10, step 94/574 completed (loss: 0.08702971041202545, acc: 0.9838709831237793)
[2025-01-06 01:52:37,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:37,673][root][INFO] - Training Epoch: 10/10, step 95/574 completed (loss: 0.11270437389612198, acc: 0.9710144996643066)
[2025-01-06 01:52:37,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:38,000][root][INFO] - Training Epoch: 10/10, step 96/574 completed (loss: 0.20558109879493713, acc: 0.9327731132507324)
[2025-01-06 01:52:38,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:38,363][root][INFO] - Training Epoch: 10/10, step 97/574 completed (loss: 0.2731117606163025, acc: 0.9134615659713745)
[2025-01-06 01:52:38,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:38,740][root][INFO] - Training Epoch: 10/10, step 98/574 completed (loss: 0.183450385928154, acc: 0.9416058659553528)
[2025-01-06 01:52:38,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:39,044][root][INFO] - Training Epoch: 10/10, step 99/574 completed (loss: 0.1987525075674057, acc: 0.9253731369972229)
[2025-01-06 01:52:39,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:39,402][root][INFO] - Training Epoch: 10/10, step 100/574 completed (loss: 0.018648816272616386, acc: 1.0)
[2025-01-06 01:52:39,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:39,728][root][INFO] - Training Epoch: 10/10, step 101/574 completed (loss: 0.0015536946011707187, acc: 1.0)
[2025-01-06 01:52:39,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:40,042][root][INFO] - Training Epoch: 10/10, step 102/574 completed (loss: 0.0014432872412726283, acc: 1.0)
[2025-01-06 01:52:40,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:40,411][root][INFO] - Training Epoch: 10/10, step 103/574 completed (loss: 0.047563586384058, acc: 0.9772727489471436)
[2025-01-06 01:52:40,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:40,749][root][INFO] - Training Epoch: 10/10, step 104/574 completed (loss: 0.09618422389030457, acc: 0.9655172228813171)
[2025-01-06 01:52:40,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41,072][root][INFO] - Training Epoch: 10/10, step 105/574 completed (loss: 0.018483392894268036, acc: 1.0)
[2025-01-06 01:52:41,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41,374][root][INFO] - Training Epoch: 10/10, step 106/574 completed (loss: 0.0011304154759272933, acc: 1.0)
[2025-01-06 01:52:41,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41,668][root][INFO] - Training Epoch: 10/10, step 107/574 completed (loss: 0.0006513636326417327, acc: 1.0)
[2025-01-06 01:52:41,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41,968][root][INFO] - Training Epoch: 10/10, step 108/574 completed (loss: 0.0002846403804142028, acc: 1.0)
[2025-01-06 01:52:42,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:42,268][root][INFO] - Training Epoch: 10/10, step 109/574 completed (loss: 0.003940212074667215, acc: 1.0)
[2025-01-06 01:52:42,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:42,598][root][INFO] - Training Epoch: 10/10, step 110/574 completed (loss: 0.02207338437438011, acc: 1.0)
[2025-01-06 01:52:42,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:42,992][root][INFO] - Training Epoch: 10/10, step 111/574 completed (loss: 0.21766360104084015, acc: 0.9473684430122375)
[2025-01-06 01:52:43,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:43,330][root][INFO] - Training Epoch: 10/10, step 112/574 completed (loss: 0.10790599882602692, acc: 0.9649122953414917)
[2025-01-06 01:52:43,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:43,668][root][INFO] - Training Epoch: 10/10, step 113/574 completed (loss: 0.11174452304840088, acc: 0.9487179517745972)
[2025-01-06 01:52:43,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:44,028][root][INFO] - Training Epoch: 10/10, step 114/574 completed (loss: 0.16531071066856384, acc: 0.9591836929321289)
[2025-01-06 01:52:44,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:44,328][root][INFO] - Training Epoch: 10/10, step 115/574 completed (loss: 0.003741685301065445, acc: 1.0)
[2025-01-06 01:52:44,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:44,649][root][INFO] - Training Epoch: 10/10, step 116/574 completed (loss: 0.04632866010069847, acc: 0.9841269850730896)
[2025-01-06 01:52:44,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:44,954][root][INFO] - Training Epoch: 10/10, step 117/574 completed (loss: 0.05160781368613243, acc: 0.9918699264526367)
[2025-01-06 01:52:45,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:45,265][root][INFO] - Training Epoch: 10/10, step 118/574 completed (loss: 0.04386235773563385, acc: 0.9838709831237793)
[2025-01-06 01:52:45,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:46,121][root][INFO] - Training Epoch: 10/10, step 119/574 completed (loss: 0.3468726873397827, acc: 0.9087452292442322)
[2025-01-06 01:52:46,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:46,462][root][INFO] - Training Epoch: 10/10, step 120/574 completed (loss: 0.05130112171173096, acc: 0.9733333587646484)
[2025-01-06 01:52:46,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:46,860][root][INFO] - Training Epoch: 10/10, step 121/574 completed (loss: 0.0108101861551404, acc: 1.0)
[2025-01-06 01:52:46,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:47,153][root][INFO] - Training Epoch: 10/10, step 122/574 completed (loss: 0.0009707671706564724, acc: 1.0)
[2025-01-06 01:52:47,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:47,453][root][INFO] - Training Epoch: 10/10, step 123/574 completed (loss: 0.05547238513827324, acc: 1.0)
[2025-01-06 01:52:47,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:47,745][root][INFO] - Training Epoch: 10/10, step 124/574 completed (loss: 0.22091437876224518, acc: 0.907975435256958)
[2025-01-06 01:52:48,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:48,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:50,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:50,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:51,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:51,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:51,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:52,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:52,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:52,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:53,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:53,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:53,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:55,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:55,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:55,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:56,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:56,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:56,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:57,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:57,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:57,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:58,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:58,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:58,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:59,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:59,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:00,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:00,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:00,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:01,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:01,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:01,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:03,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:03,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:04,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:04,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:04,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:05,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:05,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:07,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:07,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:07,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:08,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:08,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:08,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:09,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:09,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:10,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:10,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:10,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:11,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:11,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:11,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:12,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:12,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:13,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:13,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:13,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:13,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:14,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:14,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:15,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:15,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:15,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:17,650][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3489, device='cuda:0') eval_epoch_loss=tensor(0.8539, device='cuda:0') eval_epoch_acc=tensor(0.8315, device='cuda:0')
[2025-01-06 01:53:17,651][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:53:17,651][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:53:17,892][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_125_loss_0.8539376854896545/model.pt
[2025-01-06 01:53:17,896][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:53:17,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:18,245][root][INFO] - Training Epoch: 10/10, step 125/574 completed (loss: 0.23181165754795074, acc: 0.9513888955116272)
[2025-01-06 01:53:18,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:18,548][root][INFO] - Training Epoch: 10/10, step 126/574 completed (loss: 0.21959072351455688, acc: 0.8999999761581421)
[2025-01-06 01:53:18,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:18,898][root][INFO] - Training Epoch: 10/10, step 127/574 completed (loss: 0.15337613224983215, acc: 0.9464285969734192)
[2025-01-06 01:53:18,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:19,253][root][INFO] - Training Epoch: 10/10, step 128/574 completed (loss: 0.1616414338350296, acc: 0.9589743614196777)
[2025-01-06 01:53:19,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:19,683][root][INFO] - Training Epoch: 10/10, step 129/574 completed (loss: 0.19169548153877258, acc: 0.9485294222831726)
[2025-01-06 01:53:19,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:20,020][root][INFO] - Training Epoch: 10/10, step 130/574 completed (loss: 0.006777138449251652, acc: 1.0)
[2025-01-06 01:53:20,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:20,312][root][INFO] - Training Epoch: 10/10, step 131/574 completed (loss: 0.015708204358816147, acc: 1.0)
[2025-01-06 01:53:20,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:20,612][root][INFO] - Training Epoch: 10/10, step 132/574 completed (loss: 0.09781084209680557, acc: 0.9375)
[2025-01-06 01:53:20,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:20,914][root][INFO] - Training Epoch: 10/10, step 133/574 completed (loss: 0.09886274486780167, acc: 0.95652174949646)
[2025-01-06 01:53:20,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:21,215][root][INFO] - Training Epoch: 10/10, step 134/574 completed (loss: 0.0142797427251935, acc: 1.0)
[2025-01-06 01:53:21,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:21,525][root][INFO] - Training Epoch: 10/10, step 135/574 completed (loss: 0.005969928577542305, acc: 1.0)
[2025-01-06 01:53:21,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:21,838][root][INFO] - Training Epoch: 10/10, step 136/574 completed (loss: 0.013569601811468601, acc: 1.0)
[2025-01-06 01:53:21,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:22,133][root][INFO] - Training Epoch: 10/10, step 137/574 completed (loss: 0.15081140398979187, acc: 0.9666666388511658)
[2025-01-06 01:53:22,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:22,435][root][INFO] - Training Epoch: 10/10, step 138/574 completed (loss: 0.023412736132740974, acc: 1.0)
[2025-01-06 01:53:22,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:22,737][root][INFO] - Training Epoch: 10/10, step 139/574 completed (loss: 0.009865380823612213, acc: 1.0)
[2025-01-06 01:53:22,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:23,043][root][INFO] - Training Epoch: 10/10, step 140/574 completed (loss: 0.01695379614830017, acc: 1.0)
[2025-01-06 01:53:23,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:23,341][root][INFO] - Training Epoch: 10/10, step 141/574 completed (loss: 0.07725439965724945, acc: 0.9677419066429138)
[2025-01-06 01:53:23,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:23,644][root][INFO] - Training Epoch: 10/10, step 142/574 completed (loss: 0.1502417027950287, acc: 0.9729729890823364)
[2025-01-06 01:53:23,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:24,171][root][INFO] - Training Epoch: 10/10, step 143/574 completed (loss: 0.18142522871494293, acc: 0.9473684430122375)
[2025-01-06 01:53:24,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:24,506][root][INFO] - Training Epoch: 10/10, step 144/574 completed (loss: 0.15436863899230957, acc: 0.9552238583564758)
[2025-01-06 01:53:24,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:24,847][root][INFO] - Training Epoch: 10/10, step 145/574 completed (loss: 0.13552075624465942, acc: 0.9693877696990967)
[2025-01-06 01:53:24,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:25,277][root][INFO] - Training Epoch: 10/10, step 146/574 completed (loss: 0.11777279525995255, acc: 0.957446813583374)
[2025-01-06 01:53:25,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:25,592][root][INFO] - Training Epoch: 10/10, step 147/574 completed (loss: 0.4412004053592682, acc: 0.9285714030265808)
[2025-01-06 01:53:25,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:25,884][root][INFO] - Training Epoch: 10/10, step 148/574 completed (loss: 0.022220997139811516, acc: 1.0)
[2025-01-06 01:53:25,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:26,193][root][INFO] - Training Epoch: 10/10, step 149/574 completed (loss: 0.004843032453209162, acc: 1.0)
[2025-01-06 01:53:26,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:26,532][root][INFO] - Training Epoch: 10/10, step 150/574 completed (loss: 0.004113806411623955, acc: 1.0)
[2025-01-06 01:53:26,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:26,860][root][INFO] - Training Epoch: 10/10, step 151/574 completed (loss: 0.15280620753765106, acc: 0.97826087474823)
[2025-01-06 01:53:26,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:27,196][root][INFO] - Training Epoch: 10/10, step 152/574 completed (loss: 0.07895714044570923, acc: 0.9661017060279846)
[2025-01-06 01:53:27,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:27,499][root][INFO] - Training Epoch: 10/10, step 153/574 completed (loss: 0.05968476086854935, acc: 0.9649122953414917)
[2025-01-06 01:53:27,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:27,820][root][INFO] - Training Epoch: 10/10, step 154/574 completed (loss: 0.10597417503595352, acc: 0.9729729890823364)
[2025-01-06 01:53:27,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:28,139][root][INFO] - Training Epoch: 10/10, step 155/574 completed (loss: 0.010820678435266018, acc: 1.0)
[2025-01-06 01:53:28,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:28,476][root][INFO] - Training Epoch: 10/10, step 156/574 completed (loss: 0.0028445085044950247, acc: 1.0)
[2025-01-06 01:53:28,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:28,822][root][INFO] - Training Epoch: 10/10, step 157/574 completed (loss: 0.264160692691803, acc: 0.8947368264198303)
[2025-01-06 01:53:29,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:30,479][root][INFO] - Training Epoch: 10/10, step 158/574 completed (loss: 0.3174183964729309, acc: 0.9054054021835327)
[2025-01-06 01:53:30,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:30,824][root][INFO] - Training Epoch: 10/10, step 159/574 completed (loss: 0.22497698664665222, acc: 0.9629629850387573)
[2025-01-06 01:53:30,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:31,212][root][INFO] - Training Epoch: 10/10, step 160/574 completed (loss: 0.23756073415279388, acc: 0.9534883499145508)
[2025-01-06 01:53:31,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:31,800][root][INFO] - Training Epoch: 10/10, step 161/574 completed (loss: 0.1978272795677185, acc: 0.9411764740943909)
[2025-01-06 01:53:31,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:32,354][root][INFO] - Training Epoch: 10/10, step 162/574 completed (loss: 0.19096526503562927, acc: 0.9101123809814453)
[2025-01-06 01:53:32,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:32,713][root][INFO] - Training Epoch: 10/10, step 163/574 completed (loss: 0.03226327523589134, acc: 1.0)
[2025-01-06 01:53:32,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:33,099][root][INFO] - Training Epoch: 10/10, step 164/574 completed (loss: 0.010486537590622902, acc: 1.0)
[2025-01-06 01:53:33,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:33,429][root][INFO] - Training Epoch: 10/10, step 165/574 completed (loss: 0.1082039475440979, acc: 0.9655172228813171)
[2025-01-06 01:53:33,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:33,750][root][INFO] - Training Epoch: 10/10, step 166/574 completed (loss: 0.009363319724798203, acc: 1.0)
[2025-01-06 01:53:33,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:34,051][root][INFO] - Training Epoch: 10/10, step 167/574 completed (loss: 0.030724814161658287, acc: 0.9800000190734863)
[2025-01-06 01:53:34,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:34,451][root][INFO] - Training Epoch: 10/10, step 168/574 completed (loss: 0.2710230350494385, acc: 0.9166666865348816)
[2025-01-06 01:53:34,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:34,786][root][INFO] - Training Epoch: 10/10, step 169/574 completed (loss: 0.14425447583198547, acc: 0.9803921580314636)
[2025-01-06 01:53:35,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:35,889][root][INFO] - Training Epoch: 10/10, step 170/574 completed (loss: 0.16990354657173157, acc: 0.965753436088562)
[2025-01-06 01:53:35,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:36,174][root][INFO] - Training Epoch: 10/10, step 171/574 completed (loss: 0.005470253061503172, acc: 1.0)
[2025-01-06 01:53:36,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:36,463][root][INFO] - Training Epoch: 10/10, step 172/574 completed (loss: 0.1973331719636917, acc: 0.9629629850387573)
[2025-01-06 01:53:36,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:36,770][root][INFO] - Training Epoch: 10/10, step 173/574 completed (loss: 0.014948160387575626, acc: 1.0)
[2025-01-06 01:53:36,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:37,310][root][INFO] - Training Epoch: 10/10, step 174/574 completed (loss: 0.24473394453525543, acc: 0.9026548862457275)
[2025-01-06 01:53:37,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:37,611][root][INFO] - Training Epoch: 10/10, step 175/574 completed (loss: 0.17281846702098846, acc: 0.9420289993286133)
[2025-01-06 01:53:37,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:37,938][root][INFO] - Training Epoch: 10/10, step 176/574 completed (loss: 0.07446009665727615, acc: 0.9772727489471436)
[2025-01-06 01:53:38,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:38,846][root][INFO] - Training Epoch: 10/10, step 177/574 completed (loss: 0.1566278338432312, acc: 0.9618320465087891)
[2025-01-06 01:53:39,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:39,514][root][INFO] - Training Epoch: 10/10, step 178/574 completed (loss: 0.246014803647995, acc: 0.9259259104728699)
[2025-01-06 01:53:39,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:39,811][root][INFO] - Training Epoch: 10/10, step 179/574 completed (loss: 0.054983995854854584, acc: 0.9836065769195557)
[2025-01-06 01:53:39,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:40,101][root][INFO] - Training Epoch: 10/10, step 180/574 completed (loss: 0.09896694868803024, acc: 0.9583333134651184)
[2025-01-06 01:53:40,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:40,384][root][INFO] - Training Epoch: 10/10, step 181/574 completed (loss: 0.0005254211137071252, acc: 1.0)
[2025-01-06 01:53:40,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:40,686][root][INFO] - Training Epoch: 10/10, step 182/574 completed (loss: 0.0037628503050655127, acc: 1.0)
[2025-01-06 01:53:40,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:40,991][root][INFO] - Training Epoch: 10/10, step 183/574 completed (loss: 0.01167871430516243, acc: 1.0)
[2025-01-06 01:53:41,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:41,323][root][INFO] - Training Epoch: 10/10, step 184/574 completed (loss: 0.1251111626625061, acc: 0.9607250690460205)
[2025-01-06 01:53:41,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:41,638][root][INFO] - Training Epoch: 10/10, step 185/574 completed (loss: 0.10966311395168304, acc: 0.9596541523933411)
[2025-01-06 01:53:41,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:42,112][root][INFO] - Training Epoch: 10/10, step 186/574 completed (loss: 0.11737243086099625, acc: 0.9468749761581421)
[2025-01-06 01:53:42,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:42,637][root][INFO] - Training Epoch: 10/10, step 187/574 completed (loss: 0.2259722799062729, acc: 0.9268292784690857)
[2025-01-06 01:53:42,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:43,035][root][INFO] - Training Epoch: 10/10, step 188/574 completed (loss: 0.13139501214027405, acc: 0.9537366628646851)
[2025-01-06 01:53:43,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:43,329][root][INFO] - Training Epoch: 10/10, step 189/574 completed (loss: 0.03395995497703552, acc: 1.0)
[2025-01-06 01:53:43,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:43,883][root][INFO] - Training Epoch: 10/10, step 190/574 completed (loss: 0.26043450832366943, acc: 0.930232584476471)
[2025-01-06 01:53:44,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:44,685][root][INFO] - Training Epoch: 10/10, step 191/574 completed (loss: 0.3288562297821045, acc: 0.9126983880996704)
[2025-01-06 01:53:44,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:45,602][root][INFO] - Training Epoch: 10/10, step 192/574 completed (loss: 0.22860205173492432, acc: 0.939393937587738)
[2025-01-06 01:53:45,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:46,350][root][INFO] - Training Epoch: 10/10, step 193/574 completed (loss: 0.09297024458646774, acc: 0.9764705896377563)
[2025-01-06 01:53:46,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:47,429][root][INFO] - Training Epoch: 10/10, step 194/574 completed (loss: 0.26489025354385376, acc: 0.9259259104728699)
[2025-01-06 01:53:47,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:48,382][root][INFO] - Training Epoch: 10/10, step 195/574 completed (loss: 0.08090926706790924, acc: 0.9677419066429138)
[2025-01-06 01:53:48,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:48,686][root][INFO] - Training Epoch: 10/10, step 196/574 completed (loss: 0.0009974415879696608, acc: 1.0)
[2025-01-06 01:53:48,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49,019][root][INFO] - Training Epoch: 10/10, step 197/574 completed (loss: 0.06604646146297455, acc: 0.9750000238418579)
[2025-01-06 01:53:49,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49,341][root][INFO] - Training Epoch: 10/10, step 198/574 completed (loss: 0.08469010144472122, acc: 0.970588207244873)
[2025-01-06 01:53:49,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49,670][root][INFO] - Training Epoch: 10/10, step 199/574 completed (loss: 0.15125153958797455, acc: 0.9485294222831726)
[2025-01-06 01:53:49,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49,997][root][INFO] - Training Epoch: 10/10, step 200/574 completed (loss: 0.1631798893213272, acc: 0.9322034120559692)
[2025-01-06 01:53:50,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:50,304][root][INFO] - Training Epoch: 10/10, step 201/574 completed (loss: 0.1527126282453537, acc: 0.9328358173370361)
[2025-01-06 01:53:50,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:50,654][root][INFO] - Training Epoch: 10/10, step 202/574 completed (loss: 0.22095970809459686, acc: 0.9223300814628601)
[2025-01-06 01:53:50,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:50,956][root][INFO] - Training Epoch: 10/10, step 203/574 completed (loss: 0.0780513733625412, acc: 0.9523809552192688)
[2025-01-06 01:53:51,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:51,255][root][INFO] - Training Epoch: 10/10, step 204/574 completed (loss: 0.044441863894462585, acc: 0.9890109896659851)
[2025-01-06 01:53:51,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:51,575][root][INFO] - Training Epoch: 10/10, step 205/574 completed (loss: 0.03739798441529274, acc: 0.9910314083099365)
[2025-01-06 01:53:51,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:51,965][root][INFO] - Training Epoch: 10/10, step 206/574 completed (loss: 0.1427232325077057, acc: 0.960629940032959)
[2025-01-06 01:53:52,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:52,290][root][INFO] - Training Epoch: 10/10, step 207/574 completed (loss: 0.040588997304439545, acc: 0.9870689511299133)
[2025-01-06 01:53:52,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:52,626][root][INFO] - Training Epoch: 10/10, step 208/574 completed (loss: 0.1066877543926239, acc: 0.9601449370384216)
[2025-01-06 01:53:52,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:52,981][root][INFO] - Training Epoch: 10/10, step 209/574 completed (loss: 0.09720294177532196, acc: 0.9766536951065063)
[2025-01-06 01:53:53,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:53,306][root][INFO] - Training Epoch: 10/10, step 210/574 completed (loss: 0.02013726532459259, acc: 1.0)
[2025-01-06 01:53:53,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:53,608][root][INFO] - Training Epoch: 10/10, step 211/574 completed (loss: 0.0027512097731232643, acc: 1.0)
[2025-01-06 01:53:53,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:53,907][root][INFO] - Training Epoch: 10/10, step 212/574 completed (loss: 0.00254674069583416, acc: 1.0)
[2025-01-06 01:53:53,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:54,216][root][INFO] - Training Epoch: 10/10, step 213/574 completed (loss: 0.012003383599221706, acc: 1.0)
[2025-01-06 01:53:54,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:54,894][root][INFO] - Training Epoch: 10/10, step 214/574 completed (loss: 0.05592238903045654, acc: 0.9769230484962463)
[2025-01-06 01:53:54,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:55,213][root][INFO] - Training Epoch: 10/10, step 215/574 completed (loss: 0.0664779469370842, acc: 0.9729729890823364)
[2025-01-06 01:53:55,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:55,544][root][INFO] - Training Epoch: 10/10, step 216/574 completed (loss: 0.09044839441776276, acc: 0.9883720874786377)
[2025-01-06 01:53:55,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:56,124][root][INFO] - Training Epoch: 10/10, step 217/574 completed (loss: 0.03728214651346207, acc: 0.9819819927215576)
[2025-01-06 01:53:56,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:56,511][root][INFO] - Training Epoch: 10/10, step 218/574 completed (loss: 0.02411808632314205, acc: 1.0)
[2025-01-06 01:53:56,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:56,822][root][INFO] - Training Epoch: 10/10, step 219/574 completed (loss: 0.003968869801610708, acc: 1.0)
[2025-01-06 01:53:56,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:57,167][root][INFO] - Training Epoch: 10/10, step 220/574 completed (loss: 0.018989643082022667, acc: 1.0)
[2025-01-06 01:53:57,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:57,471][root][INFO] - Training Epoch: 10/10, step 221/574 completed (loss: 0.00521797826513648, acc: 1.0)
[2025-01-06 01:53:57,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:57,786][root][INFO] - Training Epoch: 10/10, step 222/574 completed (loss: 0.03537282347679138, acc: 0.9807692170143127)
[2025-01-06 01:53:57,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:58,556][root][INFO] - Training Epoch: 10/10, step 223/574 completed (loss: 0.11345580220222473, acc: 0.9619565010070801)
[2025-01-06 01:53:58,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:59,095][root][INFO] - Training Epoch: 10/10, step 224/574 completed (loss: 0.14001686871051788, acc: 0.9431818127632141)
[2025-01-06 01:53:59,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:59,527][root][INFO] - Training Epoch: 10/10, step 225/574 completed (loss: 0.0875762403011322, acc: 0.9680851101875305)
[2025-01-06 01:53:59,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:59,871][root][INFO] - Training Epoch: 10/10, step 226/574 completed (loss: 0.08814442902803421, acc: 0.9811320900917053)
[2025-01-06 01:53:59,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:00,207][root][INFO] - Training Epoch: 10/10, step 227/574 completed (loss: 0.03226375952363014, acc: 0.9833333492279053)
[2025-01-06 01:54:00,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:00,550][root][INFO] - Training Epoch: 10/10, step 228/574 completed (loss: 0.010597793385386467, acc: 1.0)
[2025-01-06 01:54:00,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:00,851][root][INFO] - Training Epoch: 10/10, step 229/574 completed (loss: 0.12837310135364532, acc: 0.9333333373069763)
[2025-01-06 01:54:00,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:01,262][root][INFO] - Training Epoch: 10/10, step 230/574 completed (loss: 0.3118368089199066, acc: 0.9157894849777222)
[2025-01-06 01:54:01,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:01,603][root][INFO] - Training Epoch: 10/10, step 231/574 completed (loss: 0.26398032903671265, acc: 0.9111111164093018)
[2025-01-06 01:54:01,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:02,018][root][INFO] - Training Epoch: 10/10, step 232/574 completed (loss: 0.2508622109889984, acc: 0.9111111164093018)
[2025-01-06 01:54:02,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:02,507][root][INFO] - Training Epoch: 10/10, step 233/574 completed (loss: 0.6110925674438477, acc: 0.8211008906364441)
[2025-01-06 01:54:02,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:02,980][root][INFO] - Training Epoch: 10/10, step 234/574 completed (loss: 0.2679115831851959, acc: 0.9153845906257629)
[2025-01-06 01:54:03,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:03,277][root][INFO] - Training Epoch: 10/10, step 235/574 completed (loss: 0.0035276212729513645, acc: 1.0)
[2025-01-06 01:54:03,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:03,566][root][INFO] - Training Epoch: 10/10, step 236/574 completed (loss: 0.00604877807199955, acc: 1.0)
[2025-01-06 01:54:03,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:03,863][root][INFO] - Training Epoch: 10/10, step 237/574 completed (loss: 0.04681262746453285, acc: 1.0)
[2025-01-06 01:54:03,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:04,161][root][INFO] - Training Epoch: 10/10, step 238/574 completed (loss: 0.018152417615056038, acc: 1.0)
[2025-01-06 01:54:04,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:04,467][root][INFO] - Training Epoch: 10/10, step 239/574 completed (loss: 0.4216828942298889, acc: 0.9142857193946838)
[2025-01-06 01:54:04,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:04,821][root][INFO] - Training Epoch: 10/10, step 240/574 completed (loss: 0.06898539513349533, acc: 0.9772727489471436)
[2025-01-06 01:54:04,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:05,158][root][INFO] - Training Epoch: 10/10, step 241/574 completed (loss: 0.012701147235929966, acc: 1.0)
[2025-01-06 01:54:05,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:05,736][root][INFO] - Training Epoch: 10/10, step 242/574 completed (loss: 0.10067229717969894, acc: 0.9838709831237793)
[2025-01-06 01:54:05,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:06,262][root][INFO] - Training Epoch: 10/10, step 243/574 completed (loss: 0.3171849846839905, acc: 0.9318181872367859)
[2025-01-06 01:54:06,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:06,551][root][INFO] - Training Epoch: 10/10, step 244/574 completed (loss: 0.00021518695575650781, acc: 1.0)
[2025-01-06 01:54:06,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:06,851][root][INFO] - Training Epoch: 10/10, step 245/574 completed (loss: 0.09245322644710541, acc: 0.9615384340286255)
[2025-01-06 01:54:06,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:07,146][root][INFO] - Training Epoch: 10/10, step 246/574 completed (loss: 0.021839434280991554, acc: 1.0)
[2025-01-06 01:54:07,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:07,439][root][INFO] - Training Epoch: 10/10, step 247/574 completed (loss: 0.001511573907919228, acc: 1.0)
[2025-01-06 01:54:07,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:07,816][root][INFO] - Training Epoch: 10/10, step 248/574 completed (loss: 0.03996927663683891, acc: 1.0)
[2025-01-06 01:54:07,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:08,179][root][INFO] - Training Epoch: 10/10, step 249/574 completed (loss: 0.0867520421743393, acc: 0.9729729890823364)
[2025-01-06 01:54:08,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:08,539][root][INFO] - Training Epoch: 10/10, step 250/574 completed (loss: 0.0031029940582811832, acc: 1.0)
[2025-01-06 01:54:08,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:08,889][root][INFO] - Training Epoch: 10/10, step 251/574 completed (loss: 0.05223897472023964, acc: 0.9852941036224365)
[2025-01-06 01:54:08,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:09,241][root][INFO] - Training Epoch: 10/10, step 252/574 completed (loss: 0.04289090633392334, acc: 0.9756097793579102)
[2025-01-06 01:54:09,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:09,605][root][INFO] - Training Epoch: 10/10, step 253/574 completed (loss: 0.0018644691444933414, acc: 1.0)
[2025-01-06 01:54:09,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:09,966][root][INFO] - Training Epoch: 10/10, step 254/574 completed (loss: 0.0002071126364171505, acc: 1.0)
[2025-01-06 01:54:10,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:10,348][root][INFO] - Training Epoch: 10/10, step 255/574 completed (loss: 0.0019167247228324413, acc: 1.0)
[2025-01-06 01:54:10,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:10,723][root][INFO] - Training Epoch: 10/10, step 256/574 completed (loss: 0.008823375217616558, acc: 1.0)
[2025-01-06 01:54:10,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:11,058][root][INFO] - Training Epoch: 10/10, step 257/574 completed (loss: 0.05124467983841896, acc: 0.9714285731315613)
[2025-01-06 01:54:11,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:11,393][root][INFO] - Training Epoch: 10/10, step 258/574 completed (loss: 0.0054073818027973175, acc: 1.0)
[2025-01-06 01:54:11,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:11,980][root][INFO] - Training Epoch: 10/10, step 259/574 completed (loss: 0.02324472926557064, acc: 1.0)
[2025-01-06 01:54:12,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:12,570][root][INFO] - Training Epoch: 10/10, step 260/574 completed (loss: 0.0590260811150074, acc: 0.9750000238418579)
[2025-01-06 01:54:12,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:12,931][root][INFO] - Training Epoch: 10/10, step 261/574 completed (loss: 0.006166630890220404, acc: 1.0)
[2025-01-06 01:54:13,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:13,239][root][INFO] - Training Epoch: 10/10, step 262/574 completed (loss: 0.026631999760866165, acc: 1.0)
[2025-01-06 01:54:13,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:13,564][root][INFO] - Training Epoch: 10/10, step 263/574 completed (loss: 0.20297956466674805, acc: 0.9200000166893005)
[2025-01-06 01:54:13,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:13,870][root][INFO] - Training Epoch: 10/10, step 264/574 completed (loss: 0.0260633435100317, acc: 1.0)
[2025-01-06 01:54:14,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:14,721][root][INFO] - Training Epoch: 10/10, step 265/574 completed (loss: 0.24728496372699738, acc: 0.9200000166893005)
[2025-01-06 01:54:14,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:15,046][root][INFO] - Training Epoch: 10/10, step 266/574 completed (loss: 0.1668594479560852, acc: 0.932584285736084)
[2025-01-06 01:54:15,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:15,382][root][INFO] - Training Epoch: 10/10, step 267/574 completed (loss: 0.2011512666940689, acc: 0.9459459185600281)
[2025-01-06 01:54:16,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:16,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:16,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:17,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:17,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:17,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:18,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:18,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:18,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:19,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:19,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:19,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:20,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:20,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:20,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:21,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:21,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:21,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:22,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:22,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:22,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:23,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:23,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:23,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:24,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:24,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:24,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:25,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:25,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:25,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:25,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:26,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:26,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:26,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:28,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:28,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:28,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:29,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:29,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:29,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:30,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:30,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:30,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:31,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:31,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:31,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:32,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:32,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:32,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:34,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:34,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:34,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:35,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:35,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:35,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:36,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:36,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:36,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:37,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:37,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:37,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:38,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:38,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:38,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:39,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:39,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:39,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:41,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:41,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:41,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:42,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:42,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:42,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:43,546][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4545, device='cuda:0') eval_epoch_loss=tensor(0.8979, device='cuda:0') eval_epoch_acc=tensor(0.8272, device='cuda:0')
[2025-01-06 01:54:43,547][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:54:43,548][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:54:43,787][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_268_loss_0.8979087471961975/model.pt
[2025-01-06 01:54:43,792][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:54:43,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:44,387][root][INFO] - Training Epoch: 10/10, step 268/574 completed (loss: 0.0138937309384346, acc: 1.0)
[2025-01-06 01:54:44,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:44,735][root][INFO] - Training Epoch: 10/10, step 269/574 completed (loss: 0.004369869362562895, acc: 1.0)
[2025-01-06 01:54:44,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:45,039][root][INFO] - Training Epoch: 10/10, step 270/574 completed (loss: 0.005492184776812792, acc: 1.0)
[2025-01-06 01:54:45,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:45,335][root][INFO] - Training Epoch: 10/10, step 271/574 completed (loss: 0.0014612630475312471, acc: 1.0)
[2025-01-06 01:54:45,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:45,660][root][INFO] - Training Epoch: 10/10, step 272/574 completed (loss: 0.001147949369624257, acc: 1.0)
[2025-01-06 01:54:45,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:46,008][root][INFO] - Training Epoch: 10/10, step 273/574 completed (loss: 0.25909677147865295, acc: 0.949999988079071)
[2025-01-06 01:54:46,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:46,308][root][INFO] - Training Epoch: 10/10, step 274/574 completed (loss: 0.004296892788261175, acc: 1.0)
[2025-01-06 01:54:46,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:46,659][root][INFO] - Training Epoch: 10/10, step 275/574 completed (loss: 0.0006052662502042949, acc: 1.0)
[2025-01-06 01:54:46,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:46,963][root][INFO] - Training Epoch: 10/10, step 276/574 completed (loss: 0.0008211668464355171, acc: 1.0)
[2025-01-06 01:54:47,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:47,308][root][INFO] - Training Epoch: 10/10, step 277/574 completed (loss: 0.003979371394962072, acc: 1.0)
[2025-01-06 01:54:47,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:47,692][root][INFO] - Training Epoch: 10/10, step 278/574 completed (loss: 0.0166175439953804, acc: 1.0)
[2025-01-06 01:54:47,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:48,018][root][INFO] - Training Epoch: 10/10, step 279/574 completed (loss: 0.008725925348699093, acc: 1.0)
[2025-01-06 01:54:48,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:48,338][root][INFO] - Training Epoch: 10/10, step 280/574 completed (loss: 0.004164222162216902, acc: 1.0)
[2025-01-06 01:54:48,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:48,765][root][INFO] - Training Epoch: 10/10, step 281/574 completed (loss: 0.10814622044563293, acc: 0.9759036302566528)
[2025-01-06 01:54:48,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:49,117][root][INFO] - Training Epoch: 10/10, step 282/574 completed (loss: 0.31132087111473083, acc: 0.9166666865348816)
[2025-01-06 01:54:49,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:49,422][root][INFO] - Training Epoch: 10/10, step 283/574 completed (loss: 0.09834109246730804, acc: 0.9736841917037964)
[2025-01-06 01:54:49,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:49,702][root][INFO] - Training Epoch: 10/10, step 284/574 completed (loss: 0.020310774445533752, acc: 1.0)
[2025-01-06 01:54:49,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:50,008][root][INFO] - Training Epoch: 10/10, step 285/574 completed (loss: 0.015782300382852554, acc: 1.0)
[2025-01-06 01:54:50,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:50,325][root][INFO] - Training Epoch: 10/10, step 286/574 completed (loss: 0.06972289830446243, acc: 0.96875)
[2025-01-06 01:54:50,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:50,684][root][INFO] - Training Epoch: 10/10, step 287/574 completed (loss: 0.0644063949584961, acc: 0.9760000109672546)
[2025-01-06 01:54:50,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:51,009][root][INFO] - Training Epoch: 10/10, step 288/574 completed (loss: 0.40223944187164307, acc: 0.9340659379959106)
[2025-01-06 01:54:51,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:51,317][root][INFO] - Training Epoch: 10/10, step 289/574 completed (loss: 0.05083048716187477, acc: 0.9813664555549622)
[2025-01-06 01:54:51,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:51,672][root][INFO] - Training Epoch: 10/10, step 290/574 completed (loss: 0.104835644364357, acc: 0.9639175534248352)
[2025-01-06 01:54:51,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:51,982][root][INFO] - Training Epoch: 10/10, step 291/574 completed (loss: 0.0007908565457910299, acc: 1.0)
[2025-01-06 01:54:52,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:52,286][root][INFO] - Training Epoch: 10/10, step 292/574 completed (loss: 0.01203838735818863, acc: 1.0)
[2025-01-06 01:54:52,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:52,616][root][INFO] - Training Epoch: 10/10, step 293/574 completed (loss: 0.022648204118013382, acc: 1.0)
[2025-01-06 01:54:52,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:53,115][root][INFO] - Training Epoch: 10/10, step 294/574 completed (loss: 0.3358578383922577, acc: 0.9636363387107849)
[2025-01-06 01:54:53,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:53,664][root][INFO] - Training Epoch: 10/10, step 295/574 completed (loss: 0.12656646966934204, acc: 0.9742268323898315)
[2025-01-06 01:54:53,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:53,965][root][INFO] - Training Epoch: 10/10, step 296/574 completed (loss: 0.01921319216489792, acc: 1.0)
[2025-01-06 01:54:54,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:54,260][root][INFO] - Training Epoch: 10/10, step 297/574 completed (loss: 0.012054918333888054, acc: 1.0)
[2025-01-06 01:54:54,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:54,568][root][INFO] - Training Epoch: 10/10, step 298/574 completed (loss: 0.018414286896586418, acc: 1.0)
[2025-01-06 01:54:54,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:54,869][root][INFO] - Training Epoch: 10/10, step 299/574 completed (loss: 0.006886411923915148, acc: 1.0)
[2025-01-06 01:54:54,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:55,174][root][INFO] - Training Epoch: 10/10, step 300/574 completed (loss: 0.0020124120637774467, acc: 1.0)
[2025-01-06 01:54:55,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:55,499][root][INFO] - Training Epoch: 10/10, step 301/574 completed (loss: 0.00538622448220849, acc: 1.0)
[2025-01-06 01:54:55,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:55,819][root][INFO] - Training Epoch: 10/10, step 302/574 completed (loss: 0.015115195885300636, acc: 1.0)
[2025-01-06 01:54:55,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:56,121][root][INFO] - Training Epoch: 10/10, step 303/574 completed (loss: 0.0008552666404284537, acc: 1.0)
[2025-01-06 01:54:56,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:56,418][root][INFO] - Training Epoch: 10/10, step 304/574 completed (loss: 0.0006909049698151648, acc: 1.0)
[2025-01-06 01:54:56,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:56,775][root][INFO] - Training Epoch: 10/10, step 305/574 completed (loss: 0.15778948366641998, acc: 0.9672130942344666)
[2025-01-06 01:54:56,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:57,098][root][INFO] - Training Epoch: 10/10, step 306/574 completed (loss: 0.025607379153370857, acc: 0.9666666388511658)
[2025-01-06 01:54:57,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:57,423][root][INFO] - Training Epoch: 10/10, step 307/574 completed (loss: 0.0004543899849522859, acc: 1.0)
[2025-01-06 01:54:57,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:57,765][root][INFO] - Training Epoch: 10/10, step 308/574 completed (loss: 0.07556329667568207, acc: 0.9855072498321533)
[2025-01-06 01:54:57,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:58,262][root][INFO] - Training Epoch: 10/10, step 309/574 completed (loss: 0.003596734721213579, acc: 1.0)
[2025-01-06 01:54:58,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:58,641][root][INFO] - Training Epoch: 10/10, step 310/574 completed (loss: 0.039346739649772644, acc: 0.9879518151283264)
[2025-01-06 01:54:58,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:58,984][root][INFO] - Training Epoch: 10/10, step 311/574 completed (loss: 0.014809470623731613, acc: 1.0)
[2025-01-06 01:54:59,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:59,352][root][INFO] - Training Epoch: 10/10, step 312/574 completed (loss: 0.014972114935517311, acc: 1.0)
[2025-01-06 01:54:59,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:59,674][root][INFO] - Training Epoch: 10/10, step 313/574 completed (loss: 0.006188841070979834, acc: 1.0)
[2025-01-06 01:54:59,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:00,007][root][INFO] - Training Epoch: 10/10, step 314/574 completed (loss: 0.000785829673986882, acc: 1.0)
[2025-01-06 01:55:00,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:00,354][root][INFO] - Training Epoch: 10/10, step 315/574 completed (loss: 0.012693346478044987, acc: 1.0)
[2025-01-06 01:55:00,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:00,672][root][INFO] - Training Epoch: 10/10, step 316/574 completed (loss: 0.06357283890247345, acc: 0.9677419066429138)
[2025-01-06 01:55:00,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:01,094][root][INFO] - Training Epoch: 10/10, step 317/574 completed (loss: 0.03103059157729149, acc: 0.9701492786407471)
[2025-01-06 01:55:01,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:01,490][root][INFO] - Training Epoch: 10/10, step 318/574 completed (loss: 0.015414949506521225, acc: 0.9903846383094788)
[2025-01-06 01:55:01,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:01,873][root][INFO] - Training Epoch: 10/10, step 319/574 completed (loss: 0.030995558947324753, acc: 0.9777777791023254)
[2025-01-06 01:55:01,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02,250][root][INFO] - Training Epoch: 10/10, step 320/574 completed (loss: 0.005656094290316105, acc: 1.0)
[2025-01-06 01:55:02,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02,624][root][INFO] - Training Epoch: 10/10, step 321/574 completed (loss: 0.0004580017412081361, acc: 1.0)
[2025-01-06 01:55:02,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02,994][root][INFO] - Training Epoch: 10/10, step 322/574 completed (loss: 0.03597478196024895, acc: 1.0)
[2025-01-06 01:55:03,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:03,330][root][INFO] - Training Epoch: 10/10, step 323/574 completed (loss: 0.1479877233505249, acc: 0.9714285731315613)
[2025-01-06 01:55:03,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:03,681][root][INFO] - Training Epoch: 10/10, step 324/574 completed (loss: 0.021657973527908325, acc: 1.0)
[2025-01-06 01:55:03,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:04,055][root][INFO] - Training Epoch: 10/10, step 325/574 completed (loss: 0.16170896589756012, acc: 0.9512194991111755)
[2025-01-06 01:55:04,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:04,423][root][INFO] - Training Epoch: 10/10, step 326/574 completed (loss: 0.27642521262168884, acc: 0.9210526347160339)
[2025-01-06 01:55:04,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:04,788][root][INFO] - Training Epoch: 10/10, step 327/574 completed (loss: 0.11615663021802902, acc: 0.9473684430122375)
[2025-01-06 01:55:04,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:05,135][root][INFO] - Training Epoch: 10/10, step 328/574 completed (loss: 0.043734222650527954, acc: 1.0)
[2025-01-06 01:55:05,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:05,499][root][INFO] - Training Epoch: 10/10, step 329/574 completed (loss: 0.013664423488080502, acc: 1.0)
[2025-01-06 01:55:05,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:05,842][root][INFO] - Training Epoch: 10/10, step 330/574 completed (loss: 0.0006986599764786661, acc: 1.0)
[2025-01-06 01:55:05,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:06,234][root][INFO] - Training Epoch: 10/10, step 331/574 completed (loss: 0.08819244056940079, acc: 0.9838709831237793)
[2025-01-06 01:55:06,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:06,651][root][INFO] - Training Epoch: 10/10, step 332/574 completed (loss: 0.011557579971849918, acc: 1.0)
[2025-01-06 01:55:06,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:06,992][root][INFO] - Training Epoch: 10/10, step 333/574 completed (loss: 0.09544223546981812, acc: 0.96875)
[2025-01-06 01:55:07,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:07,333][root][INFO] - Training Epoch: 10/10, step 334/574 completed (loss: 0.003336310852319002, acc: 1.0)
[2025-01-06 01:55:07,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:07,627][root][INFO] - Training Epoch: 10/10, step 335/574 completed (loss: 0.0651099681854248, acc: 0.9473684430122375)
[2025-01-06 01:55:07,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:07,977][root][INFO] - Training Epoch: 10/10, step 336/574 completed (loss: 0.06084883585572243, acc: 0.9800000190734863)
[2025-01-06 01:55:08,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:08,289][root][INFO] - Training Epoch: 10/10, step 337/574 completed (loss: 0.16445417702198029, acc: 0.9425287246704102)
[2025-01-06 01:55:08,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:08,681][root][INFO] - Training Epoch: 10/10, step 338/574 completed (loss: 0.1326507329940796, acc: 0.978723406791687)
[2025-01-06 01:55:08,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:09,029][root][INFO] - Training Epoch: 10/10, step 339/574 completed (loss: 0.1657623052597046, acc: 0.9277108311653137)
[2025-01-06 01:55:09,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:09,342][root][INFO] - Training Epoch: 10/10, step 340/574 completed (loss: 0.000755057786591351, acc: 1.0)
[2025-01-06 01:55:09,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:09,661][root][INFO] - Training Epoch: 10/10, step 341/574 completed (loss: 0.020332328975200653, acc: 1.0)
[2025-01-06 01:55:09,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10,008][root][INFO] - Training Epoch: 10/10, step 342/574 completed (loss: 0.03878398984670639, acc: 0.9879518151283264)
[2025-01-06 01:55:10,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10,336][root][INFO] - Training Epoch: 10/10, step 343/574 completed (loss: 0.04624544084072113, acc: 0.9811320900917053)
[2025-01-06 01:55:10,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10,683][root][INFO] - Training Epoch: 10/10, step 344/574 completed (loss: 0.019220968708395958, acc: 0.9873417615890503)
[2025-01-06 01:55:10,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:11,023][root][INFO] - Training Epoch: 10/10, step 345/574 completed (loss: 0.010705864988267422, acc: 1.0)
[2025-01-06 01:55:11,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:11,331][root][INFO] - Training Epoch: 10/10, step 346/574 completed (loss: 0.014242012985050678, acc: 1.0)
[2025-01-06 01:55:11,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:11,625][root][INFO] - Training Epoch: 10/10, step 347/574 completed (loss: 0.0006427918560802937, acc: 1.0)
[2025-01-06 01:55:11,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:11,923][root][INFO] - Training Epoch: 10/10, step 348/574 completed (loss: 0.011037877760827541, acc: 1.0)
[2025-01-06 01:55:12,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:12,306][root][INFO] - Training Epoch: 10/10, step 349/574 completed (loss: 0.10335138440132141, acc: 0.9444444179534912)
[2025-01-06 01:55:12,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:12,607][root][INFO] - Training Epoch: 10/10, step 350/574 completed (loss: 0.0799228847026825, acc: 0.9534883499145508)
[2025-01-06 01:55:12,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:12,924][root][INFO] - Training Epoch: 10/10, step 351/574 completed (loss: 0.06276217848062515, acc: 0.9743589758872986)
[2025-01-06 01:55:13,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:13,282][root][INFO] - Training Epoch: 10/10, step 352/574 completed (loss: 0.04025936871767044, acc: 1.0)
[2025-01-06 01:55:13,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:13,578][root][INFO] - Training Epoch: 10/10, step 353/574 completed (loss: 0.5019205212593079, acc: 0.95652174949646)
[2025-01-06 01:55:13,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:13,864][root][INFO] - Training Epoch: 10/10, step 354/574 completed (loss: 0.030620107427239418, acc: 1.0)
[2025-01-06 01:55:13,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:14,214][root][INFO] - Training Epoch: 10/10, step 355/574 completed (loss: 0.25160813331604004, acc: 0.9340659379959106)
[2025-01-06 01:55:14,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:14,717][root][INFO] - Training Epoch: 10/10, step 356/574 completed (loss: 0.14294148981571198, acc: 0.9739130139350891)
[2025-01-06 01:55:14,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:15,069][root][INFO] - Training Epoch: 10/10, step 357/574 completed (loss: 0.11227109283208847, acc: 0.967391312122345)
[2025-01-06 01:55:15,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:15,425][root][INFO] - Training Epoch: 10/10, step 358/574 completed (loss: 0.10971644520759583, acc: 0.9387755393981934)
[2025-01-06 01:55:15,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:15,763][root][INFO] - Training Epoch: 10/10, step 359/574 completed (loss: 0.36208924651145935, acc: 0.9583333134651184)
[2025-01-06 01:55:15,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:16,095][root][INFO] - Training Epoch: 10/10, step 360/574 completed (loss: 0.007788127288222313, acc: 1.0)
[2025-01-06 01:55:16,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:16,432][root][INFO] - Training Epoch: 10/10, step 361/574 completed (loss: 0.01577867940068245, acc: 1.0)
[2025-01-06 01:55:16,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:16,779][root][INFO] - Training Epoch: 10/10, step 362/574 completed (loss: 0.021898098289966583, acc: 0.9777777791023254)
[2025-01-06 01:55:16,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:17,116][root][INFO] - Training Epoch: 10/10, step 363/574 completed (loss: 0.015677565708756447, acc: 1.0)
[2025-01-06 01:55:17,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:17,458][root][INFO] - Training Epoch: 10/10, step 364/574 completed (loss: 0.005696884356439114, acc: 1.0)
[2025-01-06 01:55:17,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:17,803][root][INFO] - Training Epoch: 10/10, step 365/574 completed (loss: 0.5257164835929871, acc: 0.9090909361839294)
[2025-01-06 01:55:17,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:18,127][root][INFO] - Training Epoch: 10/10, step 366/574 completed (loss: 0.00024106947239488363, acc: 1.0)
[2025-01-06 01:55:18,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:18,504][root][INFO] - Training Epoch: 10/10, step 367/574 completed (loss: 0.08859841525554657, acc: 0.95652174949646)
[2025-01-06 01:55:18,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:18,865][root][INFO] - Training Epoch: 10/10, step 368/574 completed (loss: 0.0006399091216735542, acc: 1.0)
[2025-01-06 01:55:18,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:19,209][root][INFO] - Training Epoch: 10/10, step 369/574 completed (loss: 0.03079815022647381, acc: 0.96875)
[2025-01-06 01:55:19,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:19,856][root][INFO] - Training Epoch: 10/10, step 370/574 completed (loss: 0.12374231964349747, acc: 0.9575757384300232)
[2025-01-06 01:55:20,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:20,733][root][INFO] - Training Epoch: 10/10, step 371/574 completed (loss: 0.060270484536886215, acc: 0.9905660152435303)
[2025-01-06 01:55:20,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:21,109][root][INFO] - Training Epoch: 10/10, step 372/574 completed (loss: 0.021383265033364296, acc: 0.9888888597488403)
[2025-01-06 01:55:21,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:21,465][root][INFO] - Training Epoch: 10/10, step 373/574 completed (loss: 0.006984509527683258, acc: 1.0)
[2025-01-06 01:55:21,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:21,820][root][INFO] - Training Epoch: 10/10, step 374/574 completed (loss: 0.004067061934620142, acc: 1.0)
[2025-01-06 01:55:21,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:22,213][root][INFO] - Training Epoch: 10/10, step 375/574 completed (loss: 0.0035384008660912514, acc: 1.0)
[2025-01-06 01:55:22,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:22,593][root][INFO] - Training Epoch: 10/10, step 376/574 completed (loss: 0.016278821974992752, acc: 1.0)
[2025-01-06 01:55:22,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:23,026][root][INFO] - Training Epoch: 10/10, step 377/574 completed (loss: 0.0039995573461055756, acc: 1.0)
[2025-01-06 01:55:23,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:23,423][root][INFO] - Training Epoch: 10/10, step 378/574 completed (loss: 0.004809204023331404, acc: 1.0)
[2025-01-06 01:55:23,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:24,054][root][INFO] - Training Epoch: 10/10, step 379/574 completed (loss: 0.09237967431545258, acc: 0.976047933101654)
[2025-01-06 01:55:24,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:24,493][root][INFO] - Training Epoch: 10/10, step 380/574 completed (loss: 0.07998200505971909, acc: 0.9849624037742615)
[2025-01-06 01:55:24,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:25,773][root][INFO] - Training Epoch: 10/10, step 381/574 completed (loss: 0.13465042412281036, acc: 0.9411764740943909)
[2025-01-06 01:55:25,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:26,371][root][INFO] - Training Epoch: 10/10, step 382/574 completed (loss: 0.025512922555208206, acc: 0.9909909963607788)
[2025-01-06 01:55:26,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:26,761][root][INFO] - Training Epoch: 10/10, step 383/574 completed (loss: 0.027012387290596962, acc: 1.0)
[2025-01-06 01:55:26,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:27,091][root][INFO] - Training Epoch: 10/10, step 384/574 completed (loss: 0.001211080583743751, acc: 1.0)
[2025-01-06 01:55:27,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:27,447][root][INFO] - Training Epoch: 10/10, step 385/574 completed (loss: 0.035005372017621994, acc: 1.0)
[2025-01-06 01:55:27,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:27,806][root][INFO] - Training Epoch: 10/10, step 386/574 completed (loss: 0.0010224414290860295, acc: 1.0)
[2025-01-06 01:55:27,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:28,174][root][INFO] - Training Epoch: 10/10, step 387/574 completed (loss: 0.017376506701111794, acc: 1.0)
[2025-01-06 01:55:28,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:28,524][root][INFO] - Training Epoch: 10/10, step 388/574 completed (loss: 0.0007137765642255545, acc: 1.0)
[2025-01-06 01:55:28,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:28,883][root][INFO] - Training Epoch: 10/10, step 389/574 completed (loss: 0.0011289836838841438, acc: 1.0)
[2025-01-06 01:55:28,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:29,266][root][INFO] - Training Epoch: 10/10, step 390/574 completed (loss: 0.05079558491706848, acc: 1.0)
[2025-01-06 01:55:29,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:29,635][root][INFO] - Training Epoch: 10/10, step 391/574 completed (loss: 0.1286470741033554, acc: 0.9629629850387573)
[2025-01-06 01:55:29,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:30,043][root][INFO] - Training Epoch: 10/10, step 392/574 completed (loss: 0.21363845467567444, acc: 0.9223300814628601)
[2025-01-06 01:55:30,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:30,625][root][INFO] - Training Epoch: 10/10, step 393/574 completed (loss: 0.31845319271087646, acc: 0.904411792755127)
[2025-01-06 01:55:30,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:31,031][root][INFO] - Training Epoch: 10/10, step 394/574 completed (loss: 0.2057441920042038, acc: 0.9200000166893005)
[2025-01-06 01:55:31,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:31,451][root][INFO] - Training Epoch: 10/10, step 395/574 completed (loss: 0.11852872371673584, acc: 0.9513888955116272)
[2025-01-06 01:55:31,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:31,833][root][INFO] - Training Epoch: 10/10, step 396/574 completed (loss: 0.020680774003267288, acc: 1.0)
[2025-01-06 01:55:31,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:32,209][root][INFO] - Training Epoch: 10/10, step 397/574 completed (loss: 0.031018784269690514, acc: 1.0)
[2025-01-06 01:55:32,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:32,603][root][INFO] - Training Epoch: 10/10, step 398/574 completed (loss: 0.011737529188394547, acc: 1.0)
[2025-01-06 01:55:32,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:32,967][root][INFO] - Training Epoch: 10/10, step 399/574 completed (loss: 0.0018222479848191142, acc: 1.0)
[2025-01-06 01:55:33,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:33,542][root][INFO] - Training Epoch: 10/10, step 400/574 completed (loss: 0.042258646339178085, acc: 0.9852941036224365)
[2025-01-06 01:55:33,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:33,896][root][INFO] - Training Epoch: 10/10, step 401/574 completed (loss: 0.021641064435243607, acc: 1.0)
[2025-01-06 01:55:33,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:34,229][root][INFO] - Training Epoch: 10/10, step 402/574 completed (loss: 0.014623326249420643, acc: 1.0)
[2025-01-06 01:55:34,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:34,592][root][INFO] - Training Epoch: 10/10, step 403/574 completed (loss: 0.0890992134809494, acc: 0.939393937587738)
[2025-01-06 01:55:34,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:34,937][root][INFO] - Training Epoch: 10/10, step 404/574 completed (loss: 0.08726204931735992, acc: 0.9677419066429138)
[2025-01-06 01:55:35,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:35,275][root][INFO] - Training Epoch: 10/10, step 405/574 completed (loss: 0.0016316147521138191, acc: 1.0)
[2025-01-06 01:55:35,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:35,698][root][INFO] - Training Epoch: 10/10, step 406/574 completed (loss: 0.006104819942265749, acc: 1.0)
[2025-01-06 01:55:35,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:36,080][root][INFO] - Training Epoch: 10/10, step 407/574 completed (loss: 0.0007221458363346756, acc: 1.0)
[2025-01-06 01:55:36,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:36,449][root][INFO] - Training Epoch: 10/10, step 408/574 completed (loss: 0.006749085616320372, acc: 1.0)
[2025-01-06 01:55:36,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:36,760][root][INFO] - Training Epoch: 10/10, step 409/574 completed (loss: 0.013005981221795082, acc: 1.0)
[2025-01-06 01:55:36,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:37,083][root][INFO] - Training Epoch: 10/10, step 410/574 completed (loss: 0.05918871611356735, acc: 0.9655172228813171)
[2025-01-06 01:55:37,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:39,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:39,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:39,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:40,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:40,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:40,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:41,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:41,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:42,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:42,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:42,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:43,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:43,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:43,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:44,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:44,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:44,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:45,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:45,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:45,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:46,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:46,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:46,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:46,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:48,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:48,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:48,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:50,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:50,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:50,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:51,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:51,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:51,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:52,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:52,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:52,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:54,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:54,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:54,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:55,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:55,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:55,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:57,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:57,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:57,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:58,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:58,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:58,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:59,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:59,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:01,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:01,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:01,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:02,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:02,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:02,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:03,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:03,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:03,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:04,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:04,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:05,326][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5081, device='cuda:0') eval_epoch_loss=tensor(0.9195, device='cuda:0') eval_epoch_acc=tensor(0.8136, device='cuda:0')
[2025-01-06 01:56:05,327][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:56:05,327][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:56:05,575][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_411_loss_0.9195123314857483/model.pt
[2025-01-06 01:56:05,599][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:56:05,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:05,984][root][INFO] - Training Epoch: 10/10, step 411/574 completed (loss: 0.18451867997646332, acc: 0.9642857313156128)
[2025-01-06 01:56:06,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:06,363][root][INFO] - Training Epoch: 10/10, step 412/574 completed (loss: 0.001244276762008667, acc: 1.0)
[2025-01-06 01:56:06,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:06,698][root][INFO] - Training Epoch: 10/10, step 413/574 completed (loss: 0.007978950627148151, acc: 1.0)
[2025-01-06 01:56:06,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:07,023][root][INFO] - Training Epoch: 10/10, step 414/574 completed (loss: 0.003507158951833844, acc: 1.0)
[2025-01-06 01:56:07,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:07,374][root][INFO] - Training Epoch: 10/10, step 415/574 completed (loss: 0.17161285877227783, acc: 0.9607843160629272)
[2025-01-06 01:56:07,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:07,685][root][INFO] - Training Epoch: 10/10, step 416/574 completed (loss: 0.27790161967277527, acc: 0.9230769276618958)
[2025-01-06 01:56:07,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:08,029][root][INFO] - Training Epoch: 10/10, step 417/574 completed (loss: 0.1131029948592186, acc: 0.9444444179534912)
[2025-01-06 01:56:08,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:08,357][root][INFO] - Training Epoch: 10/10, step 418/574 completed (loss: 0.028782326728105545, acc: 1.0)
[2025-01-06 01:56:08,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:08,661][root][INFO] - Training Epoch: 10/10, step 419/574 completed (loss: 0.028659483417868614, acc: 1.0)
[2025-01-06 01:56:08,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:09,001][root][INFO] - Training Epoch: 10/10, step 420/574 completed (loss: 0.0015793743077665567, acc: 1.0)
[2025-01-06 01:56:09,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:09,336][root][INFO] - Training Epoch: 10/10, step 421/574 completed (loss: 0.0013978219358250499, acc: 1.0)
[2025-01-06 01:56:09,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:09,630][root][INFO] - Training Epoch: 10/10, step 422/574 completed (loss: 0.09826447069644928, acc: 0.96875)
[2025-01-06 01:56:09,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:09,969][root][INFO] - Training Epoch: 10/10, step 423/574 completed (loss: 0.012519057840108871, acc: 1.0)
[2025-01-06 01:56:10,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:10,271][root][INFO] - Training Epoch: 10/10, step 424/574 completed (loss: 0.0038503168616443872, acc: 1.0)
[2025-01-06 01:56:10,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:10,582][root][INFO] - Training Epoch: 10/10, step 425/574 completed (loss: 0.018657390028238297, acc: 1.0)
[2025-01-06 01:56:10,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:10,899][root][INFO] - Training Epoch: 10/10, step 426/574 completed (loss: 0.001017970615066588, acc: 1.0)
[2025-01-06 01:56:10,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:11,213][root][INFO] - Training Epoch: 10/10, step 427/574 completed (loss: 0.007695780601352453, acc: 1.0)
[2025-01-06 01:56:11,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:11,547][root][INFO] - Training Epoch: 10/10, step 428/574 completed (loss: 0.005044391844421625, acc: 1.0)
[2025-01-06 01:56:11,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:11,908][root][INFO] - Training Epoch: 10/10, step 429/574 completed (loss: 0.0015657087787985802, acc: 1.0)
[2025-01-06 01:56:12,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:12,271][root][INFO] - Training Epoch: 10/10, step 430/574 completed (loss: 0.00027974124532192945, acc: 1.0)
[2025-01-06 01:56:12,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:12,661][root][INFO] - Training Epoch: 10/10, step 431/574 completed (loss: 0.0075985523872077465, acc: 1.0)
[2025-01-06 01:56:12,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:13,220][root][INFO] - Training Epoch: 10/10, step 432/574 completed (loss: 0.00604929169639945, acc: 1.0)
[2025-01-06 01:56:13,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:13,581][root][INFO] - Training Epoch: 10/10, step 433/574 completed (loss: 0.03326382488012314, acc: 1.0)
[2025-01-06 01:56:13,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:13,932][root][INFO] - Training Epoch: 10/10, step 434/574 completed (loss: 0.010414049960672855, acc: 1.0)
[2025-01-06 01:56:14,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:14,238][root][INFO] - Training Epoch: 10/10, step 435/574 completed (loss: 0.2005184441804886, acc: 0.9696969985961914)
[2025-01-06 01:56:14,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:14,537][root][INFO] - Training Epoch: 10/10, step 436/574 completed (loss: 0.011847566813230515, acc: 1.0)
[2025-01-06 01:56:14,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:14,844][root][INFO] - Training Epoch: 10/10, step 437/574 completed (loss: 0.030076952651143074, acc: 0.9772727489471436)
[2025-01-06 01:56:14,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:15,165][root][INFO] - Training Epoch: 10/10, step 438/574 completed (loss: 0.01086769625544548, acc: 1.0)
[2025-01-06 01:56:15,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:15,500][root][INFO] - Training Epoch: 10/10, step 439/574 completed (loss: 0.05325588211417198, acc: 0.9743589758872986)
[2025-01-06 01:56:15,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:15,965][root][INFO] - Training Epoch: 10/10, step 440/574 completed (loss: 0.1828332096338272, acc: 0.9848484992980957)
[2025-01-06 01:56:16,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:16,681][root][INFO] - Training Epoch: 10/10, step 441/574 completed (loss: 0.25562289357185364, acc: 0.9279999732971191)
[2025-01-06 01:56:16,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:17,088][root][INFO] - Training Epoch: 10/10, step 442/574 completed (loss: 0.1419522911310196, acc: 0.9596773982048035)
[2025-01-06 01:56:17,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:17,740][root][INFO] - Training Epoch: 10/10, step 443/574 completed (loss: 0.1735547035932541, acc: 0.9353233575820923)
[2025-01-06 01:56:17,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:18,057][root][INFO] - Training Epoch: 10/10, step 444/574 completed (loss: 0.03299744799733162, acc: 0.9811320900917053)
[2025-01-06 01:56:18,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:18,475][root][INFO] - Training Epoch: 10/10, step 445/574 completed (loss: 0.03302169591188431, acc: 0.9772727489471436)
[2025-01-06 01:56:18,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:18,792][root][INFO] - Training Epoch: 10/10, step 446/574 completed (loss: 0.005735873710364103, acc: 1.0)
[2025-01-06 01:56:18,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:19,097][root][INFO] - Training Epoch: 10/10, step 447/574 completed (loss: 0.040324218571186066, acc: 1.0)
[2025-01-06 01:56:19,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:19,407][root][INFO] - Training Epoch: 10/10, step 448/574 completed (loss: 0.01728280819952488, acc: 1.0)
[2025-01-06 01:56:19,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:19,738][root][INFO] - Training Epoch: 10/10, step 449/574 completed (loss: 0.040708400309085846, acc: 0.9850746393203735)
[2025-01-06 01:56:19,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:20,091][root][INFO] - Training Epoch: 10/10, step 450/574 completed (loss: 0.0144288819283247, acc: 1.0)
[2025-01-06 01:56:20,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:20,450][root][INFO] - Training Epoch: 10/10, step 451/574 completed (loss: 0.01201546285301447, acc: 1.0)
[2025-01-06 01:56:20,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:20,774][root][INFO] - Training Epoch: 10/10, step 452/574 completed (loss: 0.019741173833608627, acc: 1.0)
[2025-01-06 01:56:20,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:21,101][root][INFO] - Training Epoch: 10/10, step 453/574 completed (loss: 0.12924370169639587, acc: 0.9473684430122375)
[2025-01-06 01:56:21,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:21,379][root][INFO] - Training Epoch: 10/10, step 454/574 completed (loss: 0.01945592276751995, acc: 1.0)
[2025-01-06 01:56:21,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:21,695][root][INFO] - Training Epoch: 10/10, step 455/574 completed (loss: 0.029885856434702873, acc: 1.0)
[2025-01-06 01:56:21,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:22,047][root][INFO] - Training Epoch: 10/10, step 456/574 completed (loss: 0.07815586030483246, acc: 0.9896907210350037)
[2025-01-06 01:56:22,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:22,457][root][INFO] - Training Epoch: 10/10, step 457/574 completed (loss: 0.021196147426962852, acc: 0.9857142567634583)
[2025-01-06 01:56:22,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:22,848][root][INFO] - Training Epoch: 10/10, step 458/574 completed (loss: 0.11808202415704727, acc: 0.9651162624359131)
[2025-01-06 01:56:22,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:23,182][root][INFO] - Training Epoch: 10/10, step 459/574 completed (loss: 0.05621980503201485, acc: 0.9821428656578064)
[2025-01-06 01:56:23,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:23,527][root][INFO] - Training Epoch: 10/10, step 460/574 completed (loss: 0.01512508187443018, acc: 1.0)
[2025-01-06 01:56:23,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:23,874][root][INFO] - Training Epoch: 10/10, step 461/574 completed (loss: 0.0023687700740993023, acc: 1.0)
[2025-01-06 01:56:23,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:24,215][root][INFO] - Training Epoch: 10/10, step 462/574 completed (loss: 0.0047901044599711895, acc: 1.0)
[2025-01-06 01:56:24,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:24,584][root][INFO] - Training Epoch: 10/10, step 463/574 completed (loss: 0.005559317767620087, acc: 1.0)
[2025-01-06 01:56:24,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:24,933][root][INFO] - Training Epoch: 10/10, step 464/574 completed (loss: 0.019072147086262703, acc: 1.0)
[2025-01-06 01:56:25,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:25,292][root][INFO] - Training Epoch: 10/10, step 465/574 completed (loss: 0.032244641333818436, acc: 0.988095223903656)
[2025-01-06 01:56:25,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:25,623][root][INFO] - Training Epoch: 10/10, step 466/574 completed (loss: 0.11540864408016205, acc: 0.9638554453849792)
[2025-01-06 01:56:25,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:25,984][root][INFO] - Training Epoch: 10/10, step 467/574 completed (loss: 0.011096267960965633, acc: 1.0)
[2025-01-06 01:56:26,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:26,342][root][INFO] - Training Epoch: 10/10, step 468/574 completed (loss: 0.09044354408979416, acc: 0.9611650705337524)
[2025-01-06 01:56:26,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:26,728][root][INFO] - Training Epoch: 10/10, step 469/574 completed (loss: 0.020242126658558846, acc: 1.0)
[2025-01-06 01:56:26,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:27,109][root][INFO] - Training Epoch: 10/10, step 470/574 completed (loss: 0.025998413562774658, acc: 1.0)
[2025-01-06 01:56:27,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:27,437][root][INFO] - Training Epoch: 10/10, step 471/574 completed (loss: 0.3300933837890625, acc: 0.9285714030265808)
[2025-01-06 01:56:27,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:27,844][root][INFO] - Training Epoch: 10/10, step 472/574 completed (loss: 0.27049848437309265, acc: 0.9215686321258545)
[2025-01-06 01:56:27,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:28,201][root][INFO] - Training Epoch: 10/10, step 473/574 completed (loss: 0.25868120789527893, acc: 0.9213973879814148)
[2025-01-06 01:56:28,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:28,550][root][INFO] - Training Epoch: 10/10, step 474/574 completed (loss: 0.039398193359375, acc: 0.9895833134651184)
[2025-01-06 01:56:28,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:28,918][root][INFO] - Training Epoch: 10/10, step 475/574 completed (loss: 0.09426474571228027, acc: 0.9447852969169617)
[2025-01-06 01:56:29,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:29,263][root][INFO] - Training Epoch: 10/10, step 476/574 completed (loss: 0.026705531403422356, acc: 1.0)
[2025-01-06 01:56:29,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:29,628][root][INFO] - Training Epoch: 10/10, step 477/574 completed (loss: 0.1347474455833435, acc: 0.9497487545013428)
[2025-01-06 01:56:29,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:29,971][root][INFO] - Training Epoch: 10/10, step 478/574 completed (loss: 0.06855777651071548, acc: 0.9444444179534912)
[2025-01-06 01:56:30,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:30,286][root][INFO] - Training Epoch: 10/10, step 479/574 completed (loss: 0.024219060316681862, acc: 1.0)
[2025-01-06 01:56:30,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:30,607][root][INFO] - Training Epoch: 10/10, step 480/574 completed (loss: 0.020885927602648735, acc: 1.0)
[2025-01-06 01:56:30,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:30,929][root][INFO] - Training Epoch: 10/10, step 481/574 completed (loss: 0.0038783657364547253, acc: 1.0)
[2025-01-06 01:56:31,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:31,257][root][INFO] - Training Epoch: 10/10, step 482/574 completed (loss: 0.13649742305278778, acc: 0.949999988079071)
[2025-01-06 01:56:31,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:31,633][root][INFO] - Training Epoch: 10/10, step 483/574 completed (loss: 0.09034910053014755, acc: 0.9482758641242981)
[2025-01-06 01:56:31,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:31,941][root][INFO] - Training Epoch: 10/10, step 484/574 completed (loss: 0.035569705069065094, acc: 1.0)
[2025-01-06 01:56:32,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:32,286][root][INFO] - Training Epoch: 10/10, step 485/574 completed (loss: 0.01090280432254076, acc: 1.0)
[2025-01-06 01:56:32,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:32,601][root][INFO] - Training Epoch: 10/10, step 486/574 completed (loss: 0.04968878626823425, acc: 1.0)
[2025-01-06 01:56:32,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:32,897][root][INFO] - Training Epoch: 10/10, step 487/574 completed (loss: 0.05974661558866501, acc: 0.9523809552192688)
[2025-01-06 01:56:33,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:33,227][root][INFO] - Training Epoch: 10/10, step 488/574 completed (loss: 0.12405648827552795, acc: 0.9545454382896423)
[2025-01-06 01:56:33,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:33,560][root][INFO] - Training Epoch: 10/10, step 489/574 completed (loss: 0.07774417847394943, acc: 0.9692307710647583)
[2025-01-06 01:56:33,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:33,863][root][INFO] - Training Epoch: 10/10, step 490/574 completed (loss: 0.005311909597367048, acc: 1.0)
[2025-01-06 01:56:33,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:34,164][root][INFO] - Training Epoch: 10/10, step 491/574 completed (loss: 0.29063740372657776, acc: 0.9655172228813171)
[2025-01-06 01:56:34,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:34,469][root][INFO] - Training Epoch: 10/10, step 492/574 completed (loss: 0.04560975357890129, acc: 0.9803921580314636)
[2025-01-06 01:56:34,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:34,785][root][INFO] - Training Epoch: 10/10, step 493/574 completed (loss: 0.01795397698879242, acc: 1.0)
[2025-01-06 01:56:34,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:35,104][root][INFO] - Training Epoch: 10/10, step 494/574 completed (loss: 0.013676249422132969, acc: 1.0)
[2025-01-06 01:56:35,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:35,423][root][INFO] - Training Epoch: 10/10, step 495/574 completed (loss: 0.02530744858086109, acc: 1.0)
[2025-01-06 01:56:35,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:35,780][root][INFO] - Training Epoch: 10/10, step 496/574 completed (loss: 0.12633323669433594, acc: 0.9732142686843872)
[2025-01-06 01:56:35,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:36,177][root][INFO] - Training Epoch: 10/10, step 497/574 completed (loss: 0.14512403309345245, acc: 0.966292142868042)
[2025-01-06 01:56:36,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:36,497][root][INFO] - Training Epoch: 10/10, step 498/574 completed (loss: 0.07726941257715225, acc: 0.9775280952453613)
[2025-01-06 01:56:36,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:36,814][root][INFO] - Training Epoch: 10/10, step 499/574 completed (loss: 0.27392876148223877, acc: 0.9078013896942139)
[2025-01-06 01:56:36,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:37,128][root][INFO] - Training Epoch: 10/10, step 500/574 completed (loss: 0.14631405472755432, acc: 0.945652186870575)
[2025-01-06 01:56:37,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:37,424][root][INFO] - Training Epoch: 10/10, step 501/574 completed (loss: 0.01250426098704338, acc: 1.0)
[2025-01-06 01:56:37,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:37,720][root][INFO] - Training Epoch: 10/10, step 502/574 completed (loss: 0.0012305793352425098, acc: 1.0)
[2025-01-06 01:56:37,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38,012][root][INFO] - Training Epoch: 10/10, step 503/574 completed (loss: 0.3762723207473755, acc: 0.9629629850387573)
[2025-01-06 01:56:38,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38,307][root][INFO] - Training Epoch: 10/10, step 504/574 completed (loss: 0.030666057020425797, acc: 1.0)
[2025-01-06 01:56:38,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38,604][root][INFO] - Training Epoch: 10/10, step 505/574 completed (loss: 0.0489119254052639, acc: 0.9622641801834106)
[2025-01-06 01:56:38,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38,897][root][INFO] - Training Epoch: 10/10, step 506/574 completed (loss: 0.022535081952810287, acc: 1.0)
[2025-01-06 01:56:39,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:39,494][root][INFO] - Training Epoch: 10/10, step 507/574 completed (loss: 0.13569524884223938, acc: 0.9819819927215576)
[2025-01-06 01:56:39,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:39,938][root][INFO] - Training Epoch: 10/10, step 508/574 completed (loss: 0.16514089703559875, acc: 0.9577465057373047)
[2025-01-06 01:56:40,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:40,256][root][INFO] - Training Epoch: 10/10, step 509/574 completed (loss: 0.1821666657924652, acc: 0.949999988079071)
[2025-01-06 01:56:40,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:40,583][root][INFO] - Training Epoch: 10/10, step 510/574 completed (loss: 0.000563219771720469, acc: 1.0)
[2025-01-06 01:56:40,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:40,887][root][INFO] - Training Epoch: 10/10, step 511/574 completed (loss: 0.023373078554868698, acc: 1.0)
[2025-01-06 01:56:42,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:43,340][root][INFO] - Training Epoch: 10/10, step 512/574 completed (loss: 0.19799166917800903, acc: 0.949999988079071)
[2025-01-06 01:56:43,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:44,135][root][INFO] - Training Epoch: 10/10, step 513/574 completed (loss: 0.09968572854995728, acc: 0.9523809552192688)
[2025-01-06 01:56:44,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:44,560][root][INFO] - Training Epoch: 10/10, step 514/574 completed (loss: 0.03074568137526512, acc: 1.0)
[2025-01-06 01:56:44,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:44,930][root][INFO] - Training Epoch: 10/10, step 515/574 completed (loss: 0.03145505487918854, acc: 0.9833333492279053)
[2025-01-06 01:56:45,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:45,624][root][INFO] - Training Epoch: 10/10, step 516/574 completed (loss: 0.06643521785736084, acc: 0.9861111044883728)
[2025-01-06 01:56:45,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:45,945][root][INFO] - Training Epoch: 10/10, step 517/574 completed (loss: 0.0016926816897466779, acc: 1.0)
[2025-01-06 01:56:46,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:46,262][root][INFO] - Training Epoch: 10/10, step 518/574 completed (loss: 0.012689149007201195, acc: 1.0)
[2025-01-06 01:56:46,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:46,626][root][INFO] - Training Epoch: 10/10, step 519/574 completed (loss: 0.001419594045728445, acc: 1.0)
[2025-01-06 01:56:46,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:47,011][root][INFO] - Training Epoch: 10/10, step 520/574 completed (loss: 0.2082298994064331, acc: 0.9259259104728699)
[2025-01-06 01:56:47,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:48,033][root][INFO] - Training Epoch: 10/10, step 521/574 completed (loss: 0.21741966903209686, acc: 0.9364407062530518)
[2025-01-06 01:56:48,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:48,432][root][INFO] - Training Epoch: 10/10, step 522/574 completed (loss: 0.05190269276499748, acc: 0.9925373196601868)
[2025-01-06 01:56:48,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:48,820][root][INFO] - Training Epoch: 10/10, step 523/574 completed (loss: 0.049583278596401215, acc: 0.9781022071838379)
[2025-01-06 01:56:48,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:49,391][root][INFO] - Training Epoch: 10/10, step 524/574 completed (loss: 0.2142442762851715, acc: 0.9350000023841858)
[2025-01-06 01:56:49,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:49,735][root][INFO] - Training Epoch: 10/10, step 525/574 completed (loss: 0.04350357875227928, acc: 0.9814814925193787)
[2025-01-06 01:56:49,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:50,061][root][INFO] - Training Epoch: 10/10, step 526/574 completed (loss: 0.30956414341926575, acc: 0.9807692170143127)
[2025-01-06 01:56:50,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:50,377][root][INFO] - Training Epoch: 10/10, step 527/574 completed (loss: 0.03570469841361046, acc: 1.0)
[2025-01-06 01:56:50,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:50,725][root][INFO] - Training Epoch: 10/10, step 528/574 completed (loss: 0.36068087816238403, acc: 0.8852459192276001)
[2025-01-06 01:56:50,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51,030][root][INFO] - Training Epoch: 10/10, step 529/574 completed (loss: 0.026688717305660248, acc: 1.0)
[2025-01-06 01:56:51,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51,330][root][INFO] - Training Epoch: 10/10, step 530/574 completed (loss: 0.17607001960277557, acc: 0.9534883499145508)
[2025-01-06 01:56:51,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51,656][root][INFO] - Training Epoch: 10/10, step 531/574 completed (loss: 0.1335829645395279, acc: 0.9545454382896423)
[2025-01-06 01:56:51,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51,963][root][INFO] - Training Epoch: 10/10, step 532/574 completed (loss: 0.12336279451847076, acc: 0.9622641801834106)
[2025-01-06 01:56:52,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:52,268][root][INFO] - Training Epoch: 10/10, step 533/574 completed (loss: 0.17130865156650543, acc: 0.9318181872367859)
[2025-01-06 01:56:52,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:52,566][root][INFO] - Training Epoch: 10/10, step 534/574 completed (loss: 0.03115815483033657, acc: 1.0)
[2025-01-06 01:56:52,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:52,876][root][INFO] - Training Epoch: 10/10, step 535/574 completed (loss: 0.0018660681089386344, acc: 1.0)
[2025-01-06 01:56:52,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:53,183][root][INFO] - Training Epoch: 10/10, step 536/574 completed (loss: 0.003932999912649393, acc: 1.0)
[2025-01-06 01:56:53,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:53,576][root][INFO] - Training Epoch: 10/10, step 537/574 completed (loss: 0.03283492475748062, acc: 1.0)
[2025-01-06 01:56:53,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:53,896][root][INFO] - Training Epoch: 10/10, step 538/574 completed (loss: 0.16252736747264862, acc: 0.96875)
[2025-01-06 01:56:53,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:54,280][root][INFO] - Training Epoch: 10/10, step 539/574 completed (loss: 0.09487929940223694, acc: 0.9375)
[2025-01-06 01:56:54,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:54,586][root][INFO] - Training Epoch: 10/10, step 540/574 completed (loss: 0.07817808538675308, acc: 0.9696969985961914)
[2025-01-06 01:56:54,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:54,885][root][INFO] - Training Epoch: 10/10, step 541/574 completed (loss: 0.09425029158592224, acc: 0.9375)
[2025-01-06 01:56:54,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:55,184][root][INFO] - Training Epoch: 10/10, step 542/574 completed (loss: 0.00493236817419529, acc: 1.0)
[2025-01-06 01:56:55,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:55,478][root][INFO] - Training Epoch: 10/10, step 543/574 completed (loss: 0.005399125162512064, acc: 1.0)
[2025-01-06 01:56:55,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:55,774][root][INFO] - Training Epoch: 10/10, step 544/574 completed (loss: 0.009137260727584362, acc: 1.0)
[2025-01-06 01:56:55,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:56,091][root][INFO] - Training Epoch: 10/10, step 545/574 completed (loss: 0.004357474856078625, acc: 1.0)
[2025-01-06 01:56:56,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:56,406][root][INFO] - Training Epoch: 10/10, step 546/574 completed (loss: 0.0019031282281503081, acc: 1.0)
[2025-01-06 01:56:56,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:56,724][root][INFO] - Training Epoch: 10/10, step 547/574 completed (loss: 0.00040794885717332363, acc: 1.0)
[2025-01-06 01:56:56,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:57,076][root][INFO] - Training Epoch: 10/10, step 548/574 completed (loss: 0.00419752299785614, acc: 1.0)
[2025-01-06 01:56:57,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:57,416][root][INFO] - Training Epoch: 10/10, step 549/574 completed (loss: 0.00047594140050932765, acc: 1.0)
[2025-01-06 01:56:57,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:57,766][root][INFO] - Training Epoch: 10/10, step 550/574 completed (loss: 0.02038729004561901, acc: 1.0)
[2025-01-06 01:56:57,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:58,095][root][INFO] - Training Epoch: 10/10, step 551/574 completed (loss: 0.0012247228296473622, acc: 1.0)
[2025-01-06 01:56:58,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:58,419][root][INFO] - Training Epoch: 10/10, step 552/574 completed (loss: 0.07515456527471542, acc: 0.9857142567634583)
[2025-01-06 01:56:58,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:58,756][root][INFO] - Training Epoch: 10/10, step 553/574 completed (loss: 0.03811318799853325, acc: 0.985401451587677)
[2025-01-06 01:56:59,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:59,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:01,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:01,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:02,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:02,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:02,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:03,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:03,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:03,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:04,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:04,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:05,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:05,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:05,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:05,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:06,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:06,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:06,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:07,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:07,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:07,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:09,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:09,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:09,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:10,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:10,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:10,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:11,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:11,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:12,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:12,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:12,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:13,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:13,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:13,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:13,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:14,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:14,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:15,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:15,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:15,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:16,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:16,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:16,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:16,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:17,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:17,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:18,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:18,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:18,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:19,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:19,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:20,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:20,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:20,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:21,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:21,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:21,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:22,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:22,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:22,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:23,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:23,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:25,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:25,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:25,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:27,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:27,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:28,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:28,744][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5755, device='cuda:0') eval_epoch_loss=tensor(0.9460, device='cuda:0') eval_epoch_acc=tensor(0.8212, device='cuda:0')
[2025-01-06 01:57:28,746][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:57:28,746][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:57:29,097][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_554_loss_0.9460422396659851/model.pt
[2025-01-06 01:57:29,108][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:57:29,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:29,519][root][INFO] - Training Epoch: 10/10, step 554/574 completed (loss: 0.059103503823280334, acc: 0.9724137783050537)
[2025-01-06 01:57:29,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:29,831][root][INFO] - Training Epoch: 10/10, step 555/574 completed (loss: 0.0789204090833664, acc: 0.9785714149475098)
[2025-01-06 01:57:29,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:30,145][root][INFO] - Training Epoch: 10/10, step 556/574 completed (loss: 0.06326834857463837, acc: 0.9867549538612366)
[2025-01-06 01:57:30,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:30,464][root][INFO] - Training Epoch: 10/10, step 557/574 completed (loss: 0.016167787835001945, acc: 1.0)
[2025-01-06 01:57:30,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:30,782][root][INFO] - Training Epoch: 10/10, step 558/574 completed (loss: 0.00135647167917341, acc: 1.0)
[2025-01-06 01:57:30,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:31,090][root][INFO] - Training Epoch: 10/10, step 559/574 completed (loss: 0.007840147241950035, acc: 1.0)
[2025-01-06 01:57:31,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:31,407][root][INFO] - Training Epoch: 10/10, step 560/574 completed (loss: 0.0023139948025345802, acc: 1.0)
[2025-01-06 01:57:31,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:31,727][root][INFO] - Training Epoch: 10/10, step 561/574 completed (loss: 0.0018500704318284988, acc: 1.0)
[2025-01-06 01:57:31,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32,051][root][INFO] - Training Epoch: 10/10, step 562/574 completed (loss: 0.10426461696624756, acc: 0.9444444179534912)
[2025-01-06 01:57:32,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32,356][root][INFO] - Training Epoch: 10/10, step 563/574 completed (loss: 0.04002651572227478, acc: 0.9870129823684692)
[2025-01-06 01:57:32,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32,653][root][INFO] - Training Epoch: 10/10, step 564/574 completed (loss: 0.06151224672794342, acc: 0.9791666865348816)
[2025-01-06 01:57:32,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32,961][root][INFO] - Training Epoch: 10/10, step 565/574 completed (loss: 0.039465393871068954, acc: 0.9655172228813171)
[2025-01-06 01:57:33,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:33,289][root][INFO] - Training Epoch: 10/10, step 566/574 completed (loss: 0.009441657923161983, acc: 1.0)
[2025-01-06 01:57:33,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:33,621][root][INFO] - Training Epoch: 10/10, step 567/574 completed (loss: 0.0036802683025598526, acc: 1.0)
[2025-01-06 01:57:33,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:33,932][root][INFO] - Training Epoch: 10/10, step 568/574 completed (loss: 0.021918579936027527, acc: 1.0)
[2025-01-06 01:57:34,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:34,310][root][INFO] - Training Epoch: 10/10, step 569/574 completed (loss: 0.044884759932756424, acc: 0.9839572310447693)
[2025-01-06 01:57:34,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:34,625][root][INFO] - Training Epoch: 10/10, step 570/574 completed (loss: 0.0013453500578179955, acc: 1.0)
[2025-01-06 01:57:34,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:34,927][root][INFO] - Training Epoch: 10/10, step 571/574 completed (loss: 0.010372593998908997, acc: 1.0)
[2025-01-06 01:57:35,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:35,234][root][INFO] - Training Epoch: 10/10, step 572/574 completed (loss: 0.12877157330513, acc: 0.9642857313156128)
[2025-01-06 01:57:35,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:35,584][root][INFO] - Training Epoch: 10/10, step 573/574 completed (loss: 0.06368528306484222, acc: 0.9748427867889404)
[2025-01-06 01:57:36,146][slam_llm.utils.train_utils][INFO] - Epoch 10: train_perplexity=1.0859, train_epoch_loss=0.0824, epoch time 340.9976419173181s
[2025-01-06 01:57:36,147][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:57:36,147][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 20 GB
[2025-01-06 01:57:36,147][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:57:36,147][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 28
[2025-01-06 01:57:36,147][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:57:36,154][root][INFO] - Key: avg_train_prep, Value: 1.5500922203063965
[2025-01-06 01:57:36,155][root][INFO] - Key: avg_train_loss, Value: 0.3510302007198334
[2025-01-06 01:57:36,156][root][INFO] - Key: avg_train_acc, Value: 0.9069973230361938
[2025-01-06 01:57:36,156][root][INFO] - Key: avg_eval_prep, Value: 2.2773172855377197
[2025-01-06 01:57:36,156][root][INFO] - Key: avg_eval_loss, Value: 0.8162105679512024
[2025-01-06 01:57:36,156][root][INFO] - Key: avg_eval_acc, Value: 0.8183140754699707
[2025-01-06 01:57:36,156][root][INFO] - Key: avg_epoch_time, Value: 346.2848273772746
[2025-01-06 01:57:36,156][root][INFO] - Key: avg_checkpoint_time, Value: 0.2840960045345128
